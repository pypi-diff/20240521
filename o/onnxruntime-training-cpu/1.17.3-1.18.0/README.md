# Comparing `tmp/onnxruntime_training_cpu-1.17.3-cp39-cp39-win_amd64.whl.zip` & `tmp/onnxruntime_training_cpu-1.18.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,414 +1,425 @@
-Zip file size: 7011364 bytes, number of entries: 412
--rw-rw-rw-  2.0 fat     1094 b- defN 24-Apr-12 21:16 onnxruntime/LICENSE
--rw-rw-rw-  2.0 fat     2490 b- defN 24-Apr-12 21:16 onnxruntime/Privacy.md
--rw-rw-rw-  2.0 fat   345046 b- defN 24-Apr-12 21:16 onnxruntime/ThirdPartyNotices.txt
--rw-rw-rw-  2.0 fat     4367 b- defN 24-Apr-12 21:16 onnxruntime/__init__.py
--rw-rw-rw-  2.0 fat      334 b- defN 24-Apr-12 21:16 onnxruntime/backend/__init__.py
--rw-rw-rw-  2.0 fat     8121 b- defN 24-Apr-12 21:16 onnxruntime/backend/backend.py
--rw-rw-rw-  2.0 fat     1821 b- defN 24-Apr-12 21:16 onnxruntime/backend/backend_rep.py
--rw-rw-rw-  2.0 fat      251 b- defN 24-Apr-12 21:16 onnxruntime/capi/__init__.py
--rw-rw-rw-  2.0 fat      413 b- defN 24-Apr-12 21:16 onnxruntime/capi/_ld_preload.py
--rw-rw-rw-  2.0 fat     1533 b- defN 24-Apr-12 21:16 onnxruntime/capi/_pybind_state.py
--rw-rw-rw-  2.0 fat       67 b- defN 24-Apr-12 21:48 onnxruntime/capi/build_and_package_info.py
--rw-rw-rw-  2.0 fat     4068 b- defN 24-Apr-12 21:16 onnxruntime/capi/onnxruntime_collect_build_info.py
--rw-rw-rw-  2.0 fat    42503 b- defN 24-Apr-12 21:16 onnxruntime/capi/onnxruntime_inference_collection.py
--rw-rw-rw-  2.0 fat    21936 b- defN 24-Apr-12 21:48 onnxruntime/capi/onnxruntime_providers_shared.dll
--rw-rw-rw-  2.0 fat 16793632 b- defN 24-Apr-12 21:48 onnxruntime/capi/onnxruntime_pybind11_state.pyd
--rw-rw-rw-  2.0 fat     6394 b- defN 24-Apr-12 21:16 onnxruntime/capi/onnxruntime_validation.py
--rw-rw-rw-  2.0 fat     2024 b- defN 24-Apr-12 21:16 onnxruntime/capi/pt_patch.py
--rw-rw-rw-  2.0 fat       34 b- defN 24-Apr-12 21:16 onnxruntime/capi/version_info.py
--rw-rw-rw-  2.0 fat      471 b- defN 24-Apr-12 21:16 onnxruntime/datasets/__init__.py
--rw-rw-rw-  2.0 fat      670 b- defN 24-Apr-12 21:16 onnxruntime/datasets/logreg_iris.onnx
--rw-rw-rw-  2.0 fat      130 b- defN 24-Apr-12 21:16 onnxruntime/datasets/mul_1.onnx
--rw-rw-rw-  2.0 fat      103 b- defN 24-Apr-12 21:16 onnxruntime/datasets/sigmoid.onnx
--rw-rw-rw-  2.0 fat      686 b- defN 24-Apr-12 21:16 onnxruntime/quantization/__init__.py
--rw-rw-rw-  2.0 fat    50946 b- defN 24-Apr-12 21:16 onnxruntime/quantization/calibrate.py
--rw-rw-rw-  2.0 fat    16364 b- defN 24-Apr-12 21:16 onnxruntime/quantization/matmul_4bits_quantizer.py
--rw-rw-rw-  2.0 fat     9307 b- defN 24-Apr-12 21:16 onnxruntime/quantization/matmul_bnb4_quantizer.py
--rw-rw-rw-  2.0 fat    22628 b- defN 24-Apr-12 21:16 onnxruntime/quantization/onnx_model.py
--rw-rw-rw-  2.0 fat    66463 b- defN 24-Apr-12 21:16 onnxruntime/quantization/onnx_quantizer.py
--rw-rw-rw-  2.0 fat     5045 b- defN 24-Apr-12 21:16 onnxruntime/quantization/preprocess.py
--rw-rw-rw-  2.0 fat    15887 b- defN 24-Apr-12 21:16 onnxruntime/quantization/qdq_loss_debug.py
--rw-rw-rw-  2.0 fat    24144 b- defN 24-Apr-12 21:16 onnxruntime/quantization/qdq_quantizer.py
--rw-rw-rw-  2.0 fat    28779 b- defN 24-Apr-12 21:16 onnxruntime/quantization/quant_utils.py
--rw-rw-rw-  2.0 fat    38218 b- defN 24-Apr-12 21:16 onnxruntime/quantization/quantize.py
--rw-rw-rw-  2.0 fat     3696 b- defN 24-Apr-12 21:16 onnxruntime/quantization/registry.py
--rw-rw-rw-  2.0 fat     6835 b- defN 24-Apr-12 21:16 onnxruntime/quantization/shape_inference.py
--rw-rw-rw-  2.0 fat     2250 b- defN 24-Apr-12 21:16 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
--rw-rw-rw-  2.0 fat     2665 b- defN 24-Apr-12 21:16 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
--rw-rw-rw-  2.0 fat      120 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/__init__.py
--rw-rw-rw-  2.0 fat     5224 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py
--rw-rw-rw-  2.0 fat     1992 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/preprocess.py
--rw-rw-rw-  2.0 fat     4123 b- defN 24-Apr-12 21:16 onnxruntime/quantization/execution_providers/qnn/quant_config.py
--rw-rw-rw-  2.0 fat      163 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/__init__.py
--rw-rw-rw-  2.0 fat    11494 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/fusion.py
--rw-rw-rw-  2.0 fat    10637 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/fusion_gelu.py
--rw-rw-rw-  2.0 fat     5256 b- defN 24-Apr-12 21:16 onnxruntime/quantization/fusions/fusion_layernorm.py
--rw-rw-rw-  2.0 fat       85 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/__init__.py
--rw-rw-rw-  2.0 fat     4463 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/activation.py
--rw-rw-rw-  2.0 fat      589 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/argmax.py
--rw-rw-rw-  2.0 fat     2637 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/attention.py
--rw-rw-rw-  2.0 fat     1118 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/base_operator.py
--rw-rw-rw-  2.0 fat     2544 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/binary_op.py
--rw-rw-rw-  2.0 fat     2149 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/concat.py
--rw-rw-rw-  2.0 fat     9986 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/conv.py
--rw-rw-rw-  2.0 fat     3350 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/direct_q8.py
--rw-rw-rw-  2.0 fat     4058 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/embed_layernorm.py
--rw-rw-rw-  2.0 fat     2166 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/gather.py
--rw-rw-rw-  2.0 fat     2445 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/gavgpool.py
--rw-rw-rw-  2.0 fat     6119 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/gemm.py
--rw-rw-rw-  2.0 fat     5114 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/lstm.py
--rw-rw-rw-  2.0 fat     8395 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/matmul.py
--rw-rw-rw-  2.0 fat      961 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/maxpool.py
--rw-rw-rw-  2.0 fat     1545 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/norm.py
--rw-rw-rw-  2.0 fat     4852 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/pad.py
--rw-rw-rw-  2.0 fat     2285 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/pooling.py
--rw-rw-rw-  2.0 fat      823 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/qdq_base_operator.py
--rw-rw-rw-  2.0 fat      962 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/resize.py
--rw-rw-rw-  2.0 fat     4269 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/softmax.py
--rw-rw-rw-  2.0 fat     2244 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/split.py
--rw-rw-rw-  2.0 fat     3127 b- defN 24-Apr-12 21:16 onnxruntime/quantization/operators/where.py
--rw-rw-rw-  2.0 fat      528 b- defN 24-Apr-12 21:16 onnxruntime/tools/__init__.py
--rw-rw-rw-  2.0 fat     2871 b- defN 24-Apr-12 21:16 onnxruntime/tools/check_onnx_model_mobile_usability.py
--rw-rw-rw-  2.0 fat    16900 b- defN 24-Apr-12 21:16 onnxruntime/tools/convert_onnx_models_to_ort.py
--rw-rw-rw-  2.0 fat     1569 b- defN 24-Apr-12 21:16 onnxruntime/tools/file_utils.py
--rw-rw-rw-  2.0 fat      333 b- defN 24-Apr-12 21:16 onnxruntime/tools/logger.py
--rw-rw-rw-  2.0 fat     2608 b- defN 24-Apr-12 21:16 onnxruntime/tools/make_dynamic_shape_fixed.py
--rw-rw-rw-  2.0 fat     6380 b- defN 24-Apr-12 21:16 onnxruntime/tools/offline_tuning.py
--rw-rw-rw-  2.0 fat    16692 b- defN 24-Apr-12 21:16 onnxruntime/tools/onnx_model_utils.py
--rw-rw-rw-  2.0 fat     3361 b- defN 24-Apr-12 21:16 onnxruntime/tools/onnx_randomizer.py
--rw-rw-rw-  2.0 fat     5770 b- defN 24-Apr-12 21:16 onnxruntime/tools/onnxruntime_test.py
--rw-rw-rw-  2.0 fat     1969 b- defN 24-Apr-12 21:16 onnxruntime/tools/optimize_onnx_model.py
--rw-rw-rw-  2.0 fat     4091 b- defN 24-Apr-12 21:16 onnxruntime/tools/pytorch_export_contrib_ops.py
--rw-rw-rw-  2.0 fat     5971 b- defN 24-Apr-12 21:16 onnxruntime/tools/pytorch_export_helpers.py
--rw-rw-rw-  2.0 fat    10137 b- defN 24-Apr-12 21:16 onnxruntime/tools/reduced_build_config_parser.py
--rw-rw-rw-  2.0 fat   138389 b- defN 24-Apr-12 21:16 onnxruntime/tools/symbolic_shape_infer.py
--rw-rw-rw-  2.0 fat     1182 b- defN 24-Apr-12 21:16 onnxruntime/tools/update_onnx_opset.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/__init__.py
--rw-rw-rw-  2.0 fat    12648 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
--rw-rw-rw-  2.0 fat     1796 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
--rw-rw-rw-  2.0 fat     3069 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
--rw-rw-rw-  2.0 fat     2385 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
--rw-rw-rw-  2.0 fat    25977 b- defN 24-Apr-12 21:16 onnxruntime/tools/mobile_helpers/usability_checker.py
--rw-rw-rw-  2.0 fat     1378 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/__init__.py
--rw-rw-rw-  2.0 fat    27375 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
--rw-rw-rw-  2.0 fat     4484 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_model_processor.py
--rw-rw-rw-  2.0 fat     4466 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/types.py
--rw-rw-rw-  2.0 fat     2604 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
--rw-rw-rw-  2.0 fat      149 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
--rw-rw-rw-  2.0 fat     1611 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
--rw-rw-rw-  2.0 fat     9310 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
--rw-rw-rw-  2.0 fat      348 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
--rw-rw-rw-  2.0 fat     3491 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py
--rw-rw-rw-  2.0 fat     3754 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
--rw-rw-rw-  2.0 fat     1924 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
--rw-rw-rw-  2.0 fat     2939 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
--rw-rw-rw-  2.0 fat     2112 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
--rw-rw-rw-  2.0 fat     1792 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
--rw-rw-rw-  2.0 fat     1988 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
--rw-rw-rw-  2.0 fat      176 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
--rw-rw-rw-  2.0 fat     1076 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
--rw-rw-rw-  2.0 fat     1613 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py
--rw-rw-rw-  2.0 fat     9000 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
--rw-rw-rw-  2.0 fat     2477 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
--rw-rw-rw-  2.0 fat     1583 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py
--rw-rw-rw-  2.0 fat     2532 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
--rw-rw-rw-  2.0 fat     2244 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
--rw-rw-rw-  2.0 fat     1728 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
--rw-rw-rw-  2.0 fat     6145 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
--rw-rw-rw-  2.0 fat     3177 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py
--rw-rw-rw-  2.0 fat     8635 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
--rw-rw-rw-  2.0 fat     3339 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
--rw-rw-rw-  2.0 fat      153 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
--rw-rw-rw-  2.0 fat     4785 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
--rw-rw-rw-  2.0 fat     2664 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
--rw-rw-rw-  2.0 fat     1621 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
--rw-rw-rw-  2.0 fat     3244 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py
--rw-rw-rw-  2.0 fat     2538 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py
--rw-rw-rw-  2.0 fat     4182 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py
--rw-rw-rw-  2.0 fat     3194 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
--rw-rw-rw-  2.0 fat     2954 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
--rw-rw-rw-  2.0 fat     2253 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
--rw-rw-rw-  2.0 fat     1437 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
--rw-rw-rw-  2.0 fat     1889 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
--rw-rw-rw-  2.0 fat     3133 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
--rw-rw-rw-  2.0 fat     1644 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py
--rw-rw-rw-  2.0 fat     1673 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
--rw-rw-rw-  2.0 fat     5144 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
--rw-rw-rw-  2.0 fat      502 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
--rw-rw-rw-  2.0 fat     1828 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
--rw-rw-rw-  2.0 fat     2039 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
--rw-rw-rw-  2.0 fat      200 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
--rw-rw-rw-  2.0 fat     2118 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
--rw-rw-rw-  2.0 fat      251 b- defN 24-Apr-12 21:16 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/tools/qdq_helpers/__init__.py
--rw-rw-rw-  2.0 fat     1279 b- defN 24-Apr-12 21:16 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
--rw-rw-rw-  2.0 fat      880 b- defN 24-Apr-12 21:16 onnxruntime/training/__init__.py
--rw-rw-rw-  2.0 fat     6754 b- defN 24-Apr-12 21:16 onnxruntime/training/_utils.py
--rw-rw-rw-  2.0 fat    10222 b- defN 24-Apr-12 21:16 onnxruntime/training/artifacts.py
--rw-rw-rw-  2.0 fat       70 b- defN 24-Apr-12 21:16 onnxruntime/training/amp/__init__.py
--rw-rw-rw-  2.0 fat     4774 b- defN 24-Apr-12 21:16 onnxruntime/training/amp/loss_scaler.py
--rw-rw-rw-  2.0 fat      449 b- defN 24-Apr-12 21:16 onnxruntime/training/api/__init__.py
--rw-rw-rw-  2.0 fat     8621 b- defN 24-Apr-12 21:16 onnxruntime/training/api/checkpoint_state.py
--rw-rw-rw-  2.0 fat     1398 b- defN 24-Apr-12 21:16 onnxruntime/training/api/lr_scheduler.py
--rw-rw-rw-  2.0 fat     7795 b- defN 24-Apr-12 21:16 onnxruntime/training/api/module.py
--rw-rw-rw-  2.0 fat     1619 b- defN 24-Apr-12 21:16 onnxruntime/training/api/optimizer.py
--rw-rw-rw-  2.0 fat       87 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/__init__.py
--rw-rw-rw-  2.0 fat      863 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/exporter.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/gradient_graph/__init__.py
--rw-rw-rw-  2.0 fat     3472 b- defN 24-Apr-12 21:16 onnxruntime/training/experimental/gradient_graph/_gradient_graph_tools.py
--rw-rw-rw-  2.0 fat      904 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/__init__.py
--rw-rw-rw-  2.0 fat     3010 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/_graph_utils.py
--rw-rw-rw-  2.0 fat     9940 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/_training_graph_utils.py
--rw-rw-rw-  2.0 fat    15827 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/blocks.py
--rw-rw-rw-  2.0 fat     1698 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/checkpoint_utils.py
--rw-rw-rw-  2.0 fat     5113 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/model_accessor.py
--rw-rw-rw-  2.0 fat     9101 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/onnxblock.py
--rw-rw-rw-  2.0 fat      281 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/loss/__init__.py
--rw-rw-rw-  2.0 fat    10050 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/loss/loss.py
--rw-rw-rw-  2.0 fat      225 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/optim/__init__.py
--rw-rw-rw-  2.0 fat    10580 b- defN 24-Apr-12 21:16 onnxruntime/training/onnxblock/optim/optim.py
--rw-rw-rw-  2.0 fat      519 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/__init__.py
--rw-rw-rw-  2.0 fat     6555 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_apex_amp_modifier.py
--rw-rw-rw-  2.0 fat     3775 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_ds_code_store.py
--rw-rw-rw-  2.0 fat    12775 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_ds_modifier.py
--rw-rw-rw-  2.0 fat     4182 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_megatron_modifier.py
--rw-rw-rw-  2.0 fat     6796 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_modifier.py
--rw-rw-rw-  2.0 fat     2574 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_modifier_registry.py
--rw-rw-rw-  2.0 fat      562 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/_multi_tensor_apply.py
--rw-rw-rw-  2.0 fat    12622 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/config.py
--rw-rw-rw-  2.0 fat     3961 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/fp16_optimizer.py
--rw-rw-rw-  2.0 fat     8105 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/fused_adam.py
--rw-rw-rw-  2.0 fat    12986 b- defN 24-Apr-12 21:16 onnxruntime/training/optim/lr_scheduler.py
--rw-rw-rw-  2.0 fat     1508 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/__init__.py
--rw-rw-rw-  2.0 fat     2525 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_cache.py
--rw-rw-rw-  2.0 fat    25508 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_codegen.py
--rw-rw-rw-  2.0 fat    10032 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_common.py
--rw-rw-rw-  2.0 fat    19133 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_decompose.py
--rw-rw-rw-  2.0 fat    17634 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_ir.py
--rw-rw-rw-  2.0 fat    26876 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_lowering.py
--rw-rw-rw-  2.0 fat     3367 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_op_config.py
--rw-rw-rw-  2.0 fat    10600 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_sorted_graph.py
--rw-rw-rw-  2.0 fat     1046 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_sympy_utils.py
--rw-rw-rw-  2.0 fat     5223 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/_utils.py
--rw-rw-rw-  2.0 fat     6215 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/triton_op_executor.py
--rw-rw-rw-  2.0 fat     1024 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/__init__.py
--rw-rw-rw-  2.0 fat    46891 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/_flash_attn.py
--rw-rw-rw-  2.0 fat    16930 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/_mm.py
--rw-rw-rw-  2.0 fat    14688 b- defN 24-Apr-12 21:16 onnxruntime/training/ort_triton/kernel/_slice_scel.py
--rw-rw-rw-  2.0 fat     5283 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/__init__.py
--rw-rw-rw-  2.0 fat     3866 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_autograd_function.py
--rw-rw-rw-  2.0 fat    19507 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py
--rw-rw-rw-  2.0 fat    11582 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_gradient_registry.py
--rw-rw-rw-  2.0 fat    36602 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py
--rw-rw-rw-  2.0 fat     7835 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_execution_agent.py
--rw-rw-rw-  2.0 fat     8208 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_fallback.py
--rw-rw-rw-  2.0 fat     2486 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_fallback_exceptions.py
--rw-rw-rw-  2.0 fat     4203 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_gradient_accumulation_manager.py
--rw-rw-rw-  2.0 fat     1121 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_graph_execution_interface.py
--rw-rw-rw-  2.0 fat    43718 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_graph_execution_manager.py
--rw-rw-rw-  2.0 fat     1155 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_graph_execution_manager_factory.py
--rw-rw-rw-  2.0 fat    11512 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_inference_manager.py
--rw-rw-rw-  2.0 fat    27801 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_io.py
--rw-rw-rw-  2.0 fat    11040 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_logger.py
--rw-rw-rw-  2.0 fat     9416 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_mem_efficient_grad_mgmt.py
--rw-rw-rw-  2.0 fat     1947 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_onnx_models.py
--rw-rw-rw-  2.0 fat     9619 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_pythonop_helper.py
--rw-rw-rw-  2.0 fat    32071 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_runtime_inspector.py
--rw-rw-rw-  2.0 fat      579 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_factory.py
--rw-rw-rw-  2.0 fat     4607 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_interface.py
--rw-rw-rw-  2.0 fat     8465 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_ort.py
--rw-rw-rw-  2.0 fat     3832 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_torch_module_pytorch.py
--rw-rw-rw-  2.0 fat    27572 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_training_manager.py
--rw-rw-rw-  2.0 fat    20478 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_utils.py
--rw-rw-rw-  2.0 fat    19011 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/_zero_stage3_compatibility.py
--rw-rw-rw-  2.0 fat     2020 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizer_registry.py
--rw-rw-rw-  2.0 fat    20382 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/options.py
--rw-rw-rw-  2.0 fat    16392 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/ortmodule.py
--rw-rw-rw-  2.0 fat      111 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/__init__.py
--rw-rw-rw-  2.0 fat      187 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/__init__.py
--rw-rw-rw-  2.0 fat    13162 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py
--rw-rw-rw-  2.0 fat      280 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/json_config/__init__.py
--rw-rw-rw-  2.0 fat    13274 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py
--rw-rw-rw-  2.0 fat      742 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizers/__init__.py
--rw-rw-rw-  2.0 fat    15778 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py
--rw-rw-rw-  2.0 fat     8012 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/graph_optimizers/utils.py
--rw-rw-rw-  2.0 fat     1875 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py
--rw-rw-rw-  2.0 fat     4493 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/install.py
--rw-rw-rw-  2.0 fat     1187 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py
--rw-rw-rw-  2.0 fat    10153 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc
--rw-rw-rw-  2.0 fat      604 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py
--rw-rw-rw-  2.0 fat      353 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py
--rw-rw-rw-  2.0 fat      873 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.cc
--rw-rw-rw-  2.0 fat     5216 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.h
--rw-rw-rw-  2.0 fat     7452 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc
--rw-rw-rw-  2.0 fat      968 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.h
--rw-rw-rw-  2.0 fat    21395 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc
--rw-rw-rw-  2.0 fat      966 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.h
--rw-rw-rw-  2.0 fat    11685 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.cc
--rw-rw-rw-  2.0 fat     4550 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.h
--rw-rw-rw-  2.0 fat      484 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/fake_ctx.py
--rw-rw-rw-  2.0 fat     1166 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py
--rw-rw-rw-  2.0 fat      824 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc
--rw-rw-rw-  2.0 fat    10114 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp
--rw-rw-rw-  2.0 fat     7268 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_adam.cu
--rw-rw-rw-  2.0 fat     5768 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_apply.cuh
--rw-rw-rw-  2.0 fat     5063 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_axpby_kernel.cu
--rw-rw-rw-  2.0 fat     6377 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_l2norm_kernel.cu
--rw-rw-rw-  2.0 fat     4498 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_scale_kernel.cu
--rw-rw-rw-  2.0 fat     1317 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/setup.py
--rw-rw-rw-  2.0 fat     2828 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/type_shim.h
--rw-rw-rw-  2.0 fat     1624 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py
--rw-rw-rw-  2.0 fat     1454 b- defN 24-Apr-12 21:16 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc
--rw-rw-rw-  2.0 fat     1100 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/__init__.py
--rw-rw-rw-  2.0 fat     2807 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/ptable.py
--rw-rw-rw-  2.0 fat    13279 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/torch_io_helper.py
--rw-rw-rw-  2.0 fat      835 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/torch_profile_utils.py
--rw-rw-rw-  2.0 fat     3033 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/torch_type_map.py
--rw-rw-rw-  2.0 fat      219 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/data/__init__.py
--rw-rw-rw-  2.0 fat    17709 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/data/sampler.py
--rw-rw-rw-  2.0 fat     1425 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/__init__.py
--rw-rw-rw-  2.0 fat    12847 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_statistics_subscriber.py
--rw-rw-rw-  2.0 fat     9347 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_subscriber_base.py
--rw-rw-rw-  2.0 fat    14135 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_subscriber_manager.py
--rw-rw-rw-  2.0 fat    29148 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/_zero_offload_subscriber.py
--rw-rw-rw-  2.0 fat     5691 b- defN 24-Apr-12 21:16 onnxruntime/training/utils/hooks/merge_activation_summary.py
--rw-rw-rw-  2.0 fat      321 b- defN 24-Apr-12 21:16 onnxruntime/transformers/__init__.py
--rw-rw-rw-  2.0 fat     1442 b- defN 24-Apr-12 21:16 onnxruntime/transformers/affinity_helper.py
--rw-rw-rw-  2.0 fat    33192 b- defN 24-Apr-12 21:16 onnxruntime/transformers/benchmark.py
--rw-rw-rw-  2.0 fat    23121 b- defN 24-Apr-12 21:16 onnxruntime/transformers/benchmark_helper.py
--rw-rw-rw-  2.0 fat    21005 b- defN 24-Apr-12 21:16 onnxruntime/transformers/bert_perf_test.py
--rw-rw-rw-  2.0 fat    23531 b- defN 24-Apr-12 21:16 onnxruntime/transformers/bert_test_data.py
--rw-rw-rw-  2.0 fat     8086 b- defN 24-Apr-12 21:16 onnxruntime/transformers/compare_bert_results.py
--rw-rw-rw-  2.0 fat     1143 b- defN 24-Apr-12 21:16 onnxruntime/transformers/constants.py
--rw-rw-rw-  2.0 fat   127465 b- defN 24-Apr-12 21:16 onnxruntime/transformers/convert_generation.py
--rw-rw-rw-  2.0 fat     6705 b- defN 24-Apr-12 21:16 onnxruntime/transformers/convert_tf_models_to_pytorch.py
--rw-rw-rw-  2.0 fat    16909 b- defN 24-Apr-12 21:16 onnxruntime/transformers/convert_to_packing_mode.py
--rw-rw-rw-  2.0 fat    24591 b- defN 24-Apr-12 21:16 onnxruntime/transformers/float16.py
--rw-rw-rw-  2.0 fat    52559 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention.py
--rw-rw-rw-  2.0 fat     8722 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention_clip.py
--rw-rw-rw-  2.0 fat    57000 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention_unet.py
--rw-rw-rw-  2.0 fat    12418 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_attention_vae.py
--rw-rw-rw-  2.0 fat    29437 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_bart_attention.py
--rw-rw-rw-  2.0 fat     5870 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_base.py
--rw-rw-rw-  2.0 fat     2066 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_bias_add.py
--rw-rw-rw-  2.0 fat     2300 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_biasgelu.py
--rw-rw-rw-  2.0 fat     4516 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_biassplitgelu.py
--rw-rw-rw-  2.0 fat     5021 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_conformer_attention.py
--rw-rw-rw-  2.0 fat    36892 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_embedlayer.py
--rw-rw-rw-  2.0 fat    13324 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_fastgelu.py
--rw-rw-rw-  2.0 fat    10180 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gelu.py
--rw-rw-rw-  2.0 fat     1029 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gelu_approximation.py
--rw-rw-rw-  2.0 fat     4258 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gemmfastgelu.py
--rw-rw-rw-  2.0 fat    22508 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gpt_attention.py
--rw-rw-rw-  2.0 fat    13639 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gpt_attention_megatron.py
--rw-rw-rw-  2.0 fat    10794 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_gpt_attention_no_past.py
--rw-rw-rw-  2.0 fat     7604 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_group_norm.py
--rw-rw-rw-  2.0 fat    12217 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_layernorm.py
--rw-rw-rw-  2.0 fat     3973 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_nhwc_conv.py
--rw-rw-rw-  2.0 fat    12086 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_options.py
--rw-rw-rw-  2.0 fat    17163 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_attention.py
--rw-rw-rw-  2.0 fat     4393 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_gelu.py
--rw-rw-rw-  2.0 fat     4915 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_layernorm.py
--rw-rw-rw-  2.0 fat     8566 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_qordered_matmul.py
--rw-rw-rw-  2.0 fat     6403 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_reshape.py
--rw-rw-rw-  2.0 fat    59307 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_rotary_attention.py
--rw-rw-rw-  2.0 fat     3813 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_shape.py
--rw-rw-rw-  2.0 fat     6554 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_simplified_layernorm.py
--rw-rw-rw-  2.0 fat    10918 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_skip_group_norm.py
--rw-rw-rw-  2.0 fat     8639 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_skiplayernorm.py
--rw-rw-rw-  2.0 fat     7035 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_transpose.py
--rw-rw-rw-  2.0 fat    12775 b- defN 24-Apr-12 21:16 onnxruntime/transformers/fusion_utils.py
--rw-rw-rw-  2.0 fat     9130 b- defN 24-Apr-12 21:16 onnxruntime/transformers/huggingface_models.py
--rw-rw-rw-  2.0 fat      651 b- defN 24-Apr-12 21:16 onnxruntime/transformers/import_utils.py
--rw-rw-rw-  2.0 fat    12734 b- defN 24-Apr-12 21:16 onnxruntime/transformers/io_binding_helper.py
--rw-rw-rw-  2.0 fat    15444 b- defN 24-Apr-12 21:16 onnxruntime/transformers/large_model_exporter.py
--rw-rw-rw-  2.0 fat     7282 b- defN 24-Apr-12 21:16 onnxruntime/transformers/machine_info.py
--rw-rw-rw-  2.0 fat     5327 b- defN 24-Apr-12 21:16 onnxruntime/transformers/metrics.py
--rw-rw-rw-  2.0 fat    25320 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_exporter.py
--rw-rw-rw-  2.0 fat    64879 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model.py
--rw-rw-rw-  2.0 fat     5579 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bart.py
--rw-rw-rw-  2.0 fat    19974 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bert.py
--rw-rw-rw-  2.0 fat    18940 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bert_keras.py
--rw-rw-rw-  2.0 fat    25433 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_bert_tf.py
--rw-rw-rw-  2.0 fat     1297 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_clip.py
--rw-rw-rw-  2.0 fat     1444 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_conformer.py
--rw-rw-rw-  2.0 fat     3913 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_gpt2.py
--rw-rw-rw-  2.0 fat    28931 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_t5.py
--rw-rw-rw-  2.0 fat     8436 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_tnlr.py
--rw-rw-rw-  2.0 fat     9520 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_unet.py
--rw-rw-rw-  2.0 fat     1545 b- defN 24-Apr-12 21:16 onnxruntime/transformers/onnx_model_vae.py
--rw-rw-rw-  2.0 fat    23667 b- defN 24-Apr-12 21:16 onnxruntime/transformers/optimizer.py
--rw-rw-rw-  2.0 fat    25009 b- defN 24-Apr-12 21:16 onnxruntime/transformers/profiler.py
--rw-rw-rw-  2.0 fat     2885 b- defN 24-Apr-12 21:16 onnxruntime/transformers/quantize_helper.py
--rw-rw-rw-  2.0 fat     4591 b- defN 24-Apr-12 21:16 onnxruntime/transformers/shape_infer_helper.py
--rw-rw-rw-  2.0 fat    15575 b- defN 24-Apr-12 21:16 onnxruntime/transformers/shape_optimizer.py
--rw-rw-rw-  2.0 fat     2575 b- defN 24-Apr-12 21:16 onnxruntime/transformers/torch_onnx_export_helper.py
--rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bart/__init__.py
--rw-rw-rw-  2.0 fat     4285 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bart/export.py
--rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bert/__init__.py
--rw-rw-rw-  2.0 fat    12395 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/bert/eval_squad.py
--rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/__init__.py
--rw-rw-rw-  2.0 fat    15916 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
--rw-rw-rw-  2.0 fat    20593 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    41373 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/gpt2_helper.py
--rw-rw-rw-  2.0 fat    18239 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/gpt2_parity.py
--rw-rw-rw-  2.0 fat    20078 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/gpt2_tester.py
--rw-rw-rw-  2.0 fat     5806 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/gpt2/parity_check_helper.py
--rw-rw-rw-  2.0 fat      490 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/__init__.py
--rw-rw-rw-  2.0 fat    27321 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/benchmark.py
--rw-rw-rw-  2.0 fat    15834 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/benchmark_all.py
--rw-rw-rw-  2.0 fat    24221 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/benchmark_e2e.py
--rw-rw-rw-  2.0 fat    43368 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     1636 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/dist_settings.py
--rw-rw-rw-  2.0 fat    20898 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/llama_inputs.py
--rw-rw-rw-  2.0 fat     9000 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/llama_parity.py
--rw-rw-rw-  2.0 fat     1665 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/llama_torch.py
--rw-rw-rw-  2.0 fat     4959 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/llama/quant_kv_dataloader.py
--rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/__init__.py
--rw-rw-rw-  2.0 fat    30284 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/benchmark_longformer.py
--rw-rw-rw-  2.0 fat    15219 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     9964 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/generate_test_data.py
--rw-rw-rw-  2.0 fat     3180 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/longformer/longformer_helper.py
--rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/__init__.py
--rw-rw-rw-  2.0 fat    48065 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/benchmark.py
--rw-rw-rw-  2.0 fat    13253 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py
--rw-rw-rw-  2.0 fat     3142 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py
--rw-rw-rw-  2.0 fat     9914 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py
--rw-rw-rw-  2.0 fat    28609 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/demo_utils.py
--rw-rw-rw-  2.0 fat    51724 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/diffusion_models.py
--rw-rw-rw-  2.0 fat    49538 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/diffusion_schedulers.py
--rw-rw-rw-  2.0 fat    11889 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder.py
--rw-rw-rw-  2.0 fat    15150 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py
--rw-rw-rw-  2.0 fat    11451 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_trt.py
--rw-rw-rw-  2.0 fat    15999 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_tensorrt.py
--rw-rw-rw-  2.0 fat     4289 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/engine_builder_torch.py
--rw-rw-rw-  2.0 fat    12881 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
--rw-rw-rw-  2.0 fat     5836 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/ort_optimizer.py
--rw-rw-rw-  2.0 fat    33667 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py
--rw-rw-rw-  2.0 fat      432 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/stable_diffusion/trt_utilities.py
--rw-rw-rw-  2.0 fat      495 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/__init__.py
--rw-rw-rw-  2.0 fat     9010 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/convert_to_onnx.py
--rw-rw-rw-  2.0 fat     6987 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/past_helper.py
--rw-rw-rw-  2.0 fat    17262 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_decoder.py
--rw-rw-rw-  2.0 fat     6295 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_encoder.py
--rw-rw-rw-  2.0 fat    12273 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
--rw-rw-rw-  2.0 fat    11032 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/t5/t5_helper.py
--rw-rw-rw-  2.0 fat      490 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/__init__.py
--rw-rw-rw-  2.0 fat    23329 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/benchmark.py
--rw-rw-rw-  2.0 fat    19461 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/benchmark_all.py
--rw-rw-rw-  2.0 fat    18396 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/convert_to_onnx.py
--rw-rw-rw-  2.0 fat    14910 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_chain.py
--rw-rw-rw-  2.0 fat    16021 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_decoder.py
--rw-rw-rw-  2.0 fat     5740 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_encoder.py
--rw-rw-rw-  2.0 fat    12723 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
--rw-rw-rw-  2.0 fat    23487 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_helper.py
--rw-rw-rw-  2.0 fat     3272 b- defN 24-Apr-12 21:16 onnxruntime/transformers/models/whisper/whisper_openai_helper.py
--rw-rw-rw-  2.0 fat     4712 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       77 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    45729 b- defN 24-Apr-12 21:48 onnxruntime_training_cpu-1.17.3.dist-info/RECORD
-412 files, 21153179 bytes uncompressed, 6935532 bytes compressed:  67.2%
+Zip file size: 7071216 bytes, number of entries: 423
+-rw-rw-rw-  2.0 fat     1094 b- defN 24-May-15 08:31 onnxruntime/LICENSE
+-rw-rw-rw-  2.0 fat     2490 b- defN 24-May-15 08:31 onnxruntime/Privacy.md
+-rw-rw-rw-  2.0 fat   345046 b- defN 24-May-15 08:31 onnxruntime/ThirdPartyNotices.txt
+-rw-rw-rw-  2.0 fat     4367 b- defN 24-May-15 08:31 onnxruntime/__init__.py
+-rw-rw-rw-  2.0 fat      334 b- defN 24-May-15 08:31 onnxruntime/backend/__init__.py
+-rw-rw-rw-  2.0 fat     8121 b- defN 24-May-15 08:31 onnxruntime/backend/backend.py
+-rw-rw-rw-  2.0 fat     1821 b- defN 24-May-15 08:31 onnxruntime/backend/backend_rep.py
+-rw-rw-rw-  2.0 fat      251 b- defN 24-May-15 08:31 onnxruntime/capi/__init__.py
+-rw-rw-rw-  2.0 fat      413 b- defN 24-May-15 08:31 onnxruntime/capi/_ld_preload.py
+-rw-rw-rw-  2.0 fat     1533 b- defN 24-May-15 08:31 onnxruntime/capi/_pybind_state.py
+-rw-rw-rw-  2.0 fat       67 b- defN 24-May-15 08:37 onnxruntime/capi/build_and_package_info.py
+-rw-rw-rw-  2.0 fat     4068 b- defN 24-May-15 08:31 onnxruntime/capi/onnxruntime_collect_build_info.py
+-rw-rw-rw-  2.0 fat    42452 b- defN 24-May-15 08:31 onnxruntime/capi/onnxruntime_inference_collection.py
+-rw-rw-rw-  2.0 fat    22048 b- defN 24-May-15 08:37 onnxruntime/capi/onnxruntime_providers_shared.dll
+-rw-rw-rw-  2.0 fat 16732192 b- defN 24-May-15 08:37 onnxruntime/capi/onnxruntime_pybind11_state.pyd
+-rw-rw-rw-  2.0 fat     6394 b- defN 24-May-15 08:31 onnxruntime/capi/onnxruntime_validation.py
+-rw-rw-rw-  2.0 fat     2024 b- defN 24-May-15 08:31 onnxruntime/capi/pt_patch.py
+-rw-rw-rw-  2.0 fat       34 b- defN 24-May-15 08:31 onnxruntime/capi/version_info.py
+-rw-rw-rw-  2.0 fat      471 b- defN 24-May-15 08:31 onnxruntime/datasets/__init__.py
+-rw-rw-rw-  2.0 fat      670 b- defN 24-May-15 08:31 onnxruntime/datasets/logreg_iris.onnx
+-rw-rw-rw-  2.0 fat      130 b- defN 24-May-15 08:31 onnxruntime/datasets/mul_1.onnx
+-rw-rw-rw-  2.0 fat      103 b- defN 24-May-15 08:31 onnxruntime/datasets/sigmoid.onnx
+-rw-rw-rw-  2.0 fat      686 b- defN 24-May-15 08:32 onnxruntime/quantization/__init__.py
+-rw-rw-rw-  2.0 fat    24351 b- defN 24-May-15 08:32 onnxruntime/quantization/base_quantizer.py
+-rw-rw-rw-  2.0 fat    50272 b- defN 24-May-15 08:32 onnxruntime/quantization/calibrate.py
+-rw-rw-rw-  2.0 fat    28348 b- defN 24-May-15 08:32 onnxruntime/quantization/matmul_4bits_quantizer.py
+-rw-rw-rw-  2.0 fat     9300 b- defN 24-May-15 08:32 onnxruntime/quantization/matmul_bnb4_quantizer.py
+-rw-rw-rw-  2.0 fat    23689 b- defN 24-May-15 08:32 onnxruntime/quantization/onnx_model.py
+-rw-rw-rw-  2.0 fat    43942 b- defN 24-May-15 08:32 onnxruntime/quantization/onnx_quantizer.py
+-rw-rw-rw-  2.0 fat     5045 b- defN 24-May-15 08:32 onnxruntime/quantization/preprocess.py
+-rw-rw-rw-  2.0 fat    15887 b- defN 24-May-15 08:32 onnxruntime/quantization/qdq_loss_debug.py
+-rw-rw-rw-  2.0 fat    54690 b- defN 24-May-15 08:32 onnxruntime/quantization/qdq_quantizer.py
+-rw-rw-rw-  2.0 fat    30255 b- defN 24-May-15 08:32 onnxruntime/quantization/quant_utils.py
+-rw-rw-rw-  2.0 fat    39055 b- defN 24-May-15 08:32 onnxruntime/quantization/quantize.py
+-rw-rw-rw-  2.0 fat     3656 b- defN 24-May-15 08:32 onnxruntime/quantization/registry.py
+-rw-rw-rw-  2.0 fat     8711 b- defN 24-May-15 08:32 onnxruntime/quantization/shape_inference.py
+-rw-rw-rw-  2.0 fat    21101 b- defN 24-May-15 08:32 onnxruntime/quantization/tensor_quant_overrides.py
+-rw-rw-rw-  2.0 fat     2250 b- defN 24-May-15 08:32 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
+-rw-rw-rw-  2.0 fat     2665 b- defN 24-May-15 08:32 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-15 08:32 onnxruntime/quantization/CalTableFlatBuffers/__init__.py
+-rw-rw-rw-  2.0 fat      120 b- defN 24-May-15 08:32 onnxruntime/quantization/execution_providers/qnn/__init__.py
+-rw-rw-rw-  2.0 fat     5327 b- defN 24-May-15 08:32 onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py
+-rw-rw-rw-  2.0 fat    19000 b- defN 24-May-15 08:32 onnxruntime/quantization/execution_providers/qnn/mixed_precision_overrides_utils.py
+-rw-rw-rw-  2.0 fat    14192 b- defN 24-May-15 08:32 onnxruntime/quantization/execution_providers/qnn/preprocess.py
+-rw-rw-rw-  2.0 fat    17829 b- defN 24-May-15 08:32 onnxruntime/quantization/execution_providers/qnn/quant_config.py
+-rw-rw-rw-  2.0 fat      163 b- defN 24-May-15 08:32 onnxruntime/quantization/fusions/__init__.py
+-rw-rw-rw-  2.0 fat    12088 b- defN 24-May-15 08:32 onnxruntime/quantization/fusions/fusion.py
+-rw-rw-rw-  2.0 fat    10647 b- defN 24-May-15 08:32 onnxruntime/quantization/fusions/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     5306 b- defN 24-May-15 08:32 onnxruntime/quantization/fusions/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat       85 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/__init__.py
+-rw-rw-rw-  2.0 fat     4463 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/activation.py
+-rw-rw-rw-  2.0 fat      589 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/argmax.py
+-rw-rw-rw-  2.0 fat     2637 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/attention.py
+-rw-rw-rw-  2.0 fat     1118 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/base_operator.py
+-rw-rw-rw-  2.0 fat     2544 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/binary_op.py
+-rw-rw-rw-  2.0 fat     2143 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/concat.py
+-rw-rw-rw-  2.0 fat    10168 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/conv.py
+-rw-rw-rw-  2.0 fat     3388 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/direct_q8.py
+-rw-rw-rw-  2.0 fat     4058 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/embed_layernorm.py
+-rw-rw-rw-  2.0 fat     2194 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/gather.py
+-rw-rw-rw-  2.0 fat     2445 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/gavgpool.py
+-rw-rw-rw-  2.0 fat     6252 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/gemm.py
+-rw-rw-rw-  2.0 fat     5184 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/lstm.py
+-rw-rw-rw-  2.0 fat     8305 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/matmul.py
+-rw-rw-rw-  2.0 fat      961 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/maxpool.py
+-rw-rw-rw-  2.0 fat     1643 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/norm.py
+-rw-rw-rw-  2.0 fat     4914 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/pad.py
+-rw-rw-rw-  2.0 fat     2285 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/pooling.py
+-rw-rw-rw-  2.0 fat      823 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/qdq_base_operator.py
+-rw-rw-rw-  2.0 fat      962 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/resize.py
+-rw-rw-rw-  2.0 fat     2714 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/softmax.py
+-rw-rw-rw-  2.0 fat     2258 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/split.py
+-rw-rw-rw-  2.0 fat     3127 b- defN 24-May-15 08:32 onnxruntime/quantization/operators/where.py
+-rw-rw-rw-  2.0 fat      528 b- defN 24-May-15 08:32 onnxruntime/tools/__init__.py
+-rw-rw-rw-  2.0 fat     2871 b- defN 24-May-15 08:31 onnxruntime/tools/check_onnx_model_mobile_usability.py
+-rw-rw-rw-  2.0 fat    16810 b- defN 24-May-15 08:31 onnxruntime/tools/convert_onnx_models_to_ort.py
+-rw-rw-rw-  2.0 fat     1569 b- defN 24-May-15 08:31 onnxruntime/tools/file_utils.py
+-rw-rw-rw-  2.0 fat      333 b- defN 24-May-15 08:31 onnxruntime/tools/logger.py
+-rw-rw-rw-  2.0 fat     2608 b- defN 24-May-15 08:31 onnxruntime/tools/make_dynamic_shape_fixed.py
+-rw-rw-rw-  2.0 fat     6380 b- defN 24-May-15 08:31 onnxruntime/tools/offline_tuning.py
+-rw-rw-rw-  2.0 fat    16692 b- defN 24-May-15 08:31 onnxruntime/tools/onnx_model_utils.py
+-rw-rw-rw-  2.0 fat     3361 b- defN 24-May-15 08:31 onnxruntime/tools/onnx_randomizer.py
+-rw-rw-rw-  2.0 fat     5770 b- defN 24-May-15 08:31 onnxruntime/tools/onnxruntime_test.py
+-rw-rw-rw-  2.0 fat     1969 b- defN 24-May-15 08:31 onnxruntime/tools/optimize_onnx_model.py
+-rw-rw-rw-  2.0 fat     4091 b- defN 24-May-15 08:31 onnxruntime/tools/pytorch_export_contrib_ops.py
+-rw-rw-rw-  2.0 fat     5971 b- defN 24-May-15 08:31 onnxruntime/tools/pytorch_export_helpers.py
+-rw-rw-rw-  2.0 fat    10137 b- defN 24-May-15 08:31 onnxruntime/tools/reduced_build_config_parser.py
+-rw-rw-rw-  2.0 fat   141288 b- defN 24-May-15 08:31 onnxruntime/tools/symbolic_shape_infer.py
+-rw-rw-rw-  2.0 fat     1182 b- defN 24-May-15 08:32 onnxruntime/tools/update_onnx_opset.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-15 08:32 onnxruntime/tools/mobile_helpers/__init__.py
+-rw-rw-rw-  2.0 fat    12642 b- defN 24-May-15 08:32 onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py
+-rw-rw-rw-  2.0 fat     1796 b- defN 24-May-15 08:32 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md
+-rw-rw-rw-  2.0 fat     3069 b- defN 24-May-15 08:32 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config
+-rw-rw-rw-  2.0 fat     2385 b- defN 24-May-15 08:32 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md
+-rw-rw-rw-  2.0 fat    25977 b- defN 24-May-15 08:32 onnxruntime/tools/mobile_helpers/usability_checker.py
+-rw-rw-rw-  2.0 fat     1378 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/__init__.py
+-rw-rw-rw-  2.0 fat    27211 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/operator_type_usage_processors.py
+-rw-rw-rw-  2.0 fat     4472 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_model_processor.py
+-rw-rw-rw-  2.0 fat     4466 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/types.py
+-rw-rw-rw-  2.0 fat     2604 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py
+-rw-rw-rw-  2.0 fat      147 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py
+-rw-rw-rw-  2.0 fat     2093 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py
+-rw-rw-rw-  2.0 fat    11187 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py
+-rw-rw-rw-  2.0 fat      346 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py
+-rw-rw-rw-  2.0 fat     4342 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py
+-rw-rw-rw-  2.0 fat     4648 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py
+-rw-rw-rw-  2.0 fat     2526 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py
+-rw-rw-rw-  2.0 fat     3678 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py
+-rw-rw-rw-  2.0 fat     2682 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py
+-rw-rw-rw-  2.0 fat     2262 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py
+-rw-rw-rw-  2.0 fat     2574 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py
+-rw-rw-rw-  2.0 fat      174 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py
+-rw-rw-rw-  2.0 fat     1137 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py
+-rw-rw-rw-  2.0 fat     2075 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py
+-rw-rw-rw-  2.0 fat    11039 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py
+-rw-rw-rw-  2.0 fat     3125 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py
+-rw-rw-rw-  2.0 fat     2037 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py
+-rw-rw-rw-  2.0 fat     3193 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     2867 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py
+-rw-rw-rw-  2.0 fat     2194 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py
+-rw-rw-rw-  2.0 fat     7663 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py
+-rw-rw-rw-  2.0 fat     4994 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py
+-rw-rw-rw-  2.0 fat    10718 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py
+-rw-rw-rw-  2.0 fat     4183 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py
+-rw-rw-rw-  2.0 fat      151 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py
+-rw-rw-rw-  2.0 fat     6144 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py
+-rw-rw-rw-  2.0 fat     3387 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py
+-rw-rw-rw-  2.0 fat     2099 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py
+-rw-rw-rw-  2.0 fat     4135 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py
+-rw-rw-rw-  2.0 fat     3218 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py
+-rw-rw-rw-  2.0 fat     5099 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py
+-rw-rw-rw-  2.0 fat     4067 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py
+-rw-rw-rw-  2.0 fat     3832 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py
+-rw-rw-rw-  2.0 fat     2800 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py
+-rw-rw-rw-  2.0 fat     1829 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py
+-rw-rw-rw-  2.0 fat     2352 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py
+-rw-rw-rw-  2.0 fat     3806 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py
+-rw-rw-rw-  2.0 fat     2110 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py
+-rw-rw-rw-  2.0 fat     2147 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py
+-rw-rw-rw-  2.0 fat     6802 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py
+-rw-rw-rw-  2.0 fat      500 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py
+-rw-rw-rw-  2.0 fat     2326 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py
+-rw-rw-rw-  2.0 fat     2599 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py
+-rw-rw-rw-  2.0 fat      198 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py
+-rw-rw-rw-  2.0 fat     2655 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py
+-rw-rw-rw-  2.0 fat      251 b- defN 24-May-15 08:32 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-15 08:32 onnxruntime/tools/qdq_helpers/__init__.py
+-rw-rw-rw-  2.0 fat     1279 b- defN 24-May-15 08:32 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py
+-rw-rw-rw-  2.0 fat      863 b- defN 24-May-15 08:32 onnxruntime/training/__init__.py
+-rw-rw-rw-  2.0 fat     6754 b- defN 24-May-15 08:32 onnxruntime/training/_utils.py
+-rw-rw-rw-  2.0 fat    11789 b- defN 24-May-15 08:32 onnxruntime/training/artifacts.py
+-rw-rw-rw-  2.0 fat       70 b- defN 24-May-15 08:32 onnxruntime/training/amp/__init__.py
+-rw-rw-rw-  2.0 fat     4774 b- defN 24-May-15 08:32 onnxruntime/training/amp/loss_scaler.py
+-rw-rw-rw-  2.0 fat      449 b- defN 24-May-15 08:32 onnxruntime/training/api/__init__.py
+-rw-rw-rw-  2.0 fat     8717 b- defN 24-May-15 08:32 onnxruntime/training/api/checkpoint_state.py
+-rw-rw-rw-  2.0 fat     1398 b- defN 24-May-15 08:32 onnxruntime/training/api/lr_scheduler.py
+-rw-rw-rw-  2.0 fat     7974 b- defN 24-May-15 08:32 onnxruntime/training/api/module.py
+-rw-rw-rw-  2.0 fat     1619 b- defN 24-May-15 08:32 onnxruntime/training/api/optimizer.py
+-rw-rw-rw-  2.0 fat       87 b- defN 24-May-15 08:32 onnxruntime/training/experimental/__init__.py
+-rw-rw-rw-  2.0 fat      863 b- defN 24-May-15 08:32 onnxruntime/training/experimental/exporter.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-15 08:32 onnxruntime/training/experimental/gradient_graph/__init__.py
+-rw-rw-rw-  2.0 fat     3472 b- defN 24-May-15 08:32 onnxruntime/training/experimental/gradient_graph/_gradient_graph_tools.py
+-rw-rw-rw-  2.0 fat      904 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/__init__.py
+-rw-rw-rw-  2.0 fat     3010 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/_graph_utils.py
+-rw-rw-rw-  2.0 fat     9940 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/_training_graph_utils.py
+-rw-rw-rw-  2.0 fat    15827 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/blocks.py
+-rw-rw-rw-  2.0 fat     1826 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/checkpoint_utils.py
+-rw-rw-rw-  2.0 fat     5113 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/model_accessor.py
+-rw-rw-rw-  2.0 fat     9101 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/onnxblock.py
+-rw-rw-rw-  2.0 fat      281 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/loss/__init__.py
+-rw-rw-rw-  2.0 fat    10050 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/loss/loss.py
+-rw-rw-rw-  2.0 fat      225 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/optim/__init__.py
+-rw-rw-rw-  2.0 fat    10580 b- defN 24-May-15 08:32 onnxruntime/training/onnxblock/optim/optim.py
+-rw-rw-rw-  2.0 fat      519 b- defN 24-May-15 08:32 onnxruntime/training/optim/__init__.py
+-rw-rw-rw-  2.0 fat     6541 b- defN 24-May-15 08:32 onnxruntime/training/optim/_apex_amp_modifier.py
+-rw-rw-rw-  2.0 fat     3775 b- defN 24-May-15 08:32 onnxruntime/training/optim/_ds_code_store.py
+-rw-rw-rw-  2.0 fat    12775 b- defN 24-May-15 08:32 onnxruntime/training/optim/_ds_modifier.py
+-rw-rw-rw-  2.0 fat     4182 b- defN 24-May-15 08:32 onnxruntime/training/optim/_megatron_modifier.py
+-rw-rw-rw-  2.0 fat     6796 b- defN 24-May-15 08:32 onnxruntime/training/optim/_modifier.py
+-rw-rw-rw-  2.0 fat     2574 b- defN 24-May-15 08:32 onnxruntime/training/optim/_modifier_registry.py
+-rw-rw-rw-  2.0 fat      562 b- defN 24-May-15 08:32 onnxruntime/training/optim/_multi_tensor_apply.py
+-rw-rw-rw-  2.0 fat    12622 b- defN 24-May-15 08:32 onnxruntime/training/optim/config.py
+-rw-rw-rw-  2.0 fat     3961 b- defN 24-May-15 08:32 onnxruntime/training/optim/fp16_optimizer.py
+-rw-rw-rw-  2.0 fat     8105 b- defN 24-May-15 08:32 onnxruntime/training/optim/fused_adam.py
+-rw-rw-rw-  2.0 fat    12986 b- defN 24-May-15 08:32 onnxruntime/training/optim/lr_scheduler.py
+-rw-rw-rw-  2.0 fat     1578 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/__init__.py
+-rw-rw-rw-  2.0 fat     2586 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_cache.py
+-rw-rw-rw-  2.0 fat    25502 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_codegen.py
+-rw-rw-rw-  2.0 fat    10003 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_common.py
+-rw-rw-rw-  2.0 fat    19050 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_decompose.py
+-rw-rw-rw-  2.0 fat    17774 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_ir.py
+-rw-rw-rw-  2.0 fat    26882 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_lowering.py
+-rw-rw-rw-  2.0 fat     3367 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_op_config.py
+-rw-rw-rw-  2.0 fat    10513 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_sorted_graph.py
+-rw-rw-rw-  2.0 fat     1046 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_sympy_utils.py
+-rw-rw-rw-  2.0 fat     5776 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/_utils.py
+-rw-rw-rw-  2.0 fat     8336 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/triton_op_executor.py
+-rw-rw-rw-  2.0 fat     1024 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/kernel/__init__.py
+-rw-rw-rw-  2.0 fat    46888 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/kernel/_flash_attn.py
+-rw-rw-rw-  2.0 fat    16805 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/kernel/_mm.py
+-rw-rw-rw-  2.0 fat    14695 b- defN 24-May-15 08:32 onnxruntime/training/ort_triton/kernel/_slice_scel.py
+-rw-rw-rw-  2.0 fat     9635 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/__init__.py
+-rw-rw-rw-  2.0 fat     3866 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_custom_autograd_function.py
+-rw-rw-rw-  2.0 fat    21495 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_custom_gradient_registry.py
+-rw-rw-rw-  2.0 fat    41814 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py
+-rw-rw-rw-  2.0 fat     7835 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_execution_agent.py
+-rw-rw-rw-  2.0 fat     8208 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_fallback.py
+-rw-rw-rw-  2.0 fat     2414 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_fallback_exceptions.py
+-rw-rw-rw-  2.0 fat     4203 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_gradient_accumulation_manager.py
+-rw-rw-rw-  2.0 fat     1121 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_graph_execution_interface.py
+-rw-rw-rw-  2.0 fat    50719 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_graph_execution_manager.py
+-rw-rw-rw-  2.0 fat     1155 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_graph_execution_manager_factory.py
+-rw-rw-rw-  2.0 fat    11484 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_inference_manager.py
+-rw-rw-rw-  2.0 fat    27801 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_io.py
+-rw-rw-rw-  2.0 fat    11098 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_logger.py
+-rw-rw-rw-  2.0 fat     9416 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_mem_efficient_grad_mgmt.py
+-rw-rw-rw-  2.0 fat     1947 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_onnx_models.py
+-rw-rw-rw-  2.0 fat     9619 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_pythonop_helper.py
+-rw-rw-rw-  2.0 fat    33194 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_runtime_inspector.py
+-rw-rw-rw-  2.0 fat      579 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_torch_module_factory.py
+-rw-rw-rw-  2.0 fat     4607 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_torch_module_interface.py
+-rw-rw-rw-  2.0 fat     8465 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_torch_module_ort.py
+-rw-rw-rw-  2.0 fat     3832 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_torch_module_pytorch.py
+-rw-rw-rw-  2.0 fat    27645 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_training_manager.py
+-rw-rw-rw-  2.0 fat    20475 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_utils.py
+-rw-rw-rw-  2.0 fat    19011 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/_zero_stage3_compatibility.py
+-rw-rw-rw-  2.0 fat     2020 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/graph_optimizer_registry.py
+-rw-rw-rw-  2.0 fat    21056 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/options.py
+-rw-rw-rw-  2.0 fat    16392 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/ortmodule.py
+-rw-rw-rw-  2.0 fat      111 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/__init__.py
+-rw-rw-rw-  2.0 fat      187 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/__init__.py
+-rw-rw-rw-  2.0 fat    13117 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py
+-rw-rw-rw-  2.0 fat      280 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/json_config/__init__.py
+-rw-rw-rw-  2.0 fat    13274 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py
+-rw-rw-rw-  2.0 fat      320 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/pipe/__init__.py
+-rw-rw-rw-  2.0 fat     8314 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/experimental/pipe/_ort_pipeline_module.py
+-rw-rw-rw-  2.0 fat      742 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/graph_optimizers/__init__.py
+-rw-rw-rw-  2.0 fat    10842 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py
+-rw-rw-rw-  2.0 fat     8012 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/graph_optimizers/utils.py
+-rw-rw-rw-  2.0 fat     1875 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py
+-rw-rw-rw-  2.0 fat     4493 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/install.py
+-rw-rw-rw-  2.0 fat     1190 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py
+-rw-rw-rw-  2.0 fat    10165 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc
+-rw-rw-rw-  2.0 fat      604 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py
+-rw-rw-rw-  2.0 fat      353 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py
+-rw-rw-rw-  2.0 fat      873 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.cc
+-rw-rw-rw-  2.0 fat     5216 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.h
+-rw-rw-rw-  2.0 fat     7533 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc
+-rw-rw-rw-  2.0 fat      968 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.h
+-rw-rw-rw-  2.0 fat    21364 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc
+-rw-rw-rw-  2.0 fat      966 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.h
+-rw-rw-rw-  2.0 fat    11685 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.cc
+-rw-rw-rw-  2.0 fat     4550 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.h
+-rw-rw-rw-  2.0 fat      484 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/fake_ctx.py
+-rw-rw-rw-  2.0 fat     1166 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py
+-rw-rw-rw-  2.0 fat      824 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc
+-rw-rw-rw-  2.0 fat    10114 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp
+-rw-rw-rw-  2.0 fat     7268 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_adam.cu
+-rw-rw-rw-  2.0 fat     5768 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_apply.cuh
+-rw-rw-rw-  2.0 fat     5063 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_axpby_kernel.cu
+-rw-rw-rw-  2.0 fat     6377 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_l2norm_kernel.cu
+-rw-rw-rw-  2.0 fat     4498 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_scale_kernel.cu
+-rw-rw-rw-  2.0 fat     1317 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/setup.py
+-rw-rw-rw-  2.0 fat     2828 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/type_shim.h
+-rw-rw-rw-  2.0 fat     1624 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py
+-rw-rw-rw-  2.0 fat     1454 b- defN 24-May-15 08:32 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc
+-rw-rw-rw-  2.0 fat     1148 b- defN 24-May-15 08:32 onnxruntime/training/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2807 b- defN 24-May-15 08:32 onnxruntime/training/utils/ptable.py
+-rw-rw-rw-  2.0 fat    13279 b- defN 24-May-15 08:32 onnxruntime/training/utils/torch_io_helper.py
+-rw-rw-rw-  2.0 fat     4073 b- defN 24-May-15 08:32 onnxruntime/training/utils/torch_profile_utils.py
+-rw-rw-rw-  2.0 fat     3033 b- defN 24-May-15 08:32 onnxruntime/training/utils/torch_type_map.py
+-rw-rw-rw-  2.0 fat      219 b- defN 24-May-15 08:32 onnxruntime/training/utils/data/__init__.py
+-rw-rw-rw-  2.0 fat    17709 b- defN 24-May-15 08:32 onnxruntime/training/utils/data/sampler.py
+-rw-rw-rw-  2.0 fat     1425 b- defN 24-May-15 08:32 onnxruntime/training/utils/hooks/__init__.py
+-rw-rw-rw-  2.0 fat    13100 b- defN 24-May-15 08:32 onnxruntime/training/utils/hooks/_statistics_subscriber.py
+-rw-rw-rw-  2.0 fat     9347 b- defN 24-May-15 08:32 onnxruntime/training/utils/hooks/_subscriber_base.py
+-rw-rw-rw-  2.0 fat    14135 b- defN 24-May-15 08:32 onnxruntime/training/utils/hooks/_subscriber_manager.py
+-rw-rw-rw-  2.0 fat    29148 b- defN 24-May-15 08:32 onnxruntime/training/utils/hooks/_zero_offload_subscriber.py
+-rw-rw-rw-  2.0 fat     5691 b- defN 24-May-15 08:32 onnxruntime/training/utils/hooks/merge_activation_summary.py
+-rw-rw-rw-  2.0 fat      321 b- defN 24-May-15 08:32 onnxruntime/transformers/__init__.py
+-rw-rw-rw-  2.0 fat     1442 b- defN 24-May-15 08:32 onnxruntime/transformers/affinity_helper.py
+-rw-rw-rw-  2.0 fat    33741 b- defN 24-May-15 08:32 onnxruntime/transformers/benchmark.py
+-rw-rw-rw-  2.0 fat    23245 b- defN 24-May-15 08:32 onnxruntime/transformers/benchmark_helper.py
+-rw-rw-rw-  2.0 fat    20995 b- defN 24-May-15 08:32 onnxruntime/transformers/bert_perf_test.py
+-rw-rw-rw-  2.0 fat    23516 b- defN 24-May-15 08:32 onnxruntime/transformers/bert_test_data.py
+-rw-rw-rw-  2.0 fat     7908 b- defN 24-May-15 08:32 onnxruntime/transformers/compare_bert_results.py
+-rw-rw-rw-  2.0 fat     1143 b- defN 24-May-15 08:32 onnxruntime/transformers/constants.py
+-rw-rw-rw-  2.0 fat   127566 b- defN 24-May-15 08:32 onnxruntime/transformers/convert_generation.py
+-rw-rw-rw-  2.0 fat     6705 b- defN 24-May-15 08:32 onnxruntime/transformers/convert_tf_models_to_pytorch.py
+-rw-rw-rw-  2.0 fat    16909 b- defN 24-May-15 08:32 onnxruntime/transformers/convert_to_packing_mode.py
+-rw-rw-rw-  2.0 fat     3797 b- defN 24-May-15 08:32 onnxruntime/transformers/dynamo_onnx_helper.py
+-rw-rw-rw-  2.0 fat    24691 b- defN 24-May-15 08:32 onnxruntime/transformers/float16.py
+-rw-rw-rw-  2.0 fat    52559 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_attention.py
+-rw-rw-rw-  2.0 fat     8722 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_attention_clip.py
+-rw-rw-rw-  2.0 fat    56932 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_attention_unet.py
+-rw-rw-rw-  2.0 fat    12418 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_attention_vae.py
+-rw-rw-rw-  2.0 fat    29437 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_bart_attention.py
+-rw-rw-rw-  2.0 fat     5870 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_base.py
+-rw-rw-rw-  2.0 fat     2066 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_bias_add.py
+-rw-rw-rw-  2.0 fat     2300 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_biasgelu.py
+-rw-rw-rw-  2.0 fat     4516 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_biassplitgelu.py
+-rw-rw-rw-  2.0 fat     5021 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_conformer_attention.py
+-rw-rw-rw-  2.0 fat    36750 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_embedlayer.py
+-rw-rw-rw-  2.0 fat    13324 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_fastgelu.py
+-rw-rw-rw-  2.0 fat    10180 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_gelu.py
+-rw-rw-rw-  2.0 fat     1029 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_gelu_approximation.py
+-rw-rw-rw-  2.0 fat     4258 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_gemmfastgelu.py
+-rw-rw-rw-  2.0 fat    22508 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_gpt_attention.py
+-rw-rw-rw-  2.0 fat    13639 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_gpt_attention_megatron.py
+-rw-rw-rw-  2.0 fat    10794 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_gpt_attention_no_past.py
+-rw-rw-rw-  2.0 fat     7604 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_group_norm.py
+-rw-rw-rw-  2.0 fat    12217 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_layernorm.py
+-rw-rw-rw-  2.0 fat     3973 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_nhwc_conv.py
+-rw-rw-rw-  2.0 fat    12704 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_options.py
+-rw-rw-rw-  2.0 fat    17163 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_qordered_attention.py
+-rw-rw-rw-  2.0 fat     4435 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_qordered_gelu.py
+-rw-rw-rw-  2.0 fat     4957 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_qordered_layernorm.py
+-rw-rw-rw-  2.0 fat     8566 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_qordered_matmul.py
+-rw-rw-rw-  2.0 fat     6403 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_reshape.py
+-rw-rw-rw-  2.0 fat    68261 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_rotary_attention.py
+-rw-rw-rw-  2.0 fat     3813 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_shape.py
+-rw-rw-rw-  2.0 fat     6554 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_simplified_layernorm.py
+-rw-rw-rw-  2.0 fat    10880 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_skip_group_norm.py
+-rw-rw-rw-  2.0 fat     8639 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_skiplayernorm.py
+-rw-rw-rw-  2.0 fat     7035 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_transpose.py
+-rw-rw-rw-  2.0 fat    12775 b- defN 24-May-15 08:32 onnxruntime/transformers/fusion_utils.py
+-rw-rw-rw-  2.0 fat     9130 b- defN 24-May-15 08:32 onnxruntime/transformers/huggingface_models.py
+-rw-rw-rw-  2.0 fat      651 b- defN 24-May-15 08:32 onnxruntime/transformers/import_utils.py
+-rw-rw-rw-  2.0 fat    17543 b- defN 24-May-15 08:32 onnxruntime/transformers/io_binding_helper.py
+-rw-rw-rw-  2.0 fat    15310 b- defN 24-May-15 08:32 onnxruntime/transformers/large_model_exporter.py
+-rw-rw-rw-  2.0 fat     7282 b- defN 24-May-15 08:32 onnxruntime/transformers/machine_info.py
+-rw-rw-rw-  2.0 fat     5327 b- defN 24-May-15 08:32 onnxruntime/transformers/metrics.py
+-rw-rw-rw-  2.0 fat    25176 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_exporter.py
+-rw-rw-rw-  2.0 fat    64986 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model.py
+-rw-rw-rw-  2.0 fat     5579 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_bart.py
+-rw-rw-rw-  2.0 fat    19974 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_bert.py
+-rw-rw-rw-  2.0 fat    18940 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_bert_keras.py
+-rw-rw-rw-  2.0 fat    25433 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_bert_tf.py
+-rw-rw-rw-  2.0 fat     1297 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_clip.py
+-rw-rw-rw-  2.0 fat     1444 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_conformer.py
+-rw-rw-rw-  2.0 fat     3913 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_gpt2.py
+-rw-rw-rw-  2.0 fat    36377 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_phi.py
+-rw-rw-rw-  2.0 fat    28931 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_t5.py
+-rw-rw-rw-  2.0 fat     8436 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_tnlr.py
+-rw-rw-rw-  2.0 fat     9517 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_unet.py
+-rw-rw-rw-  2.0 fat     1545 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_model_vae.py
+-rw-rw-rw-  2.0 fat     2161 b- defN 24-May-15 08:32 onnxruntime/transformers/onnx_utils.py
+-rw-rw-rw-  2.0 fat    25221 b- defN 24-May-15 08:32 onnxruntime/transformers/optimizer.py
+-rw-rw-rw-  2.0 fat    24891 b- defN 24-May-15 08:32 onnxruntime/transformers/profiler.py
+-rw-rw-rw-  2.0 fat     2885 b- defN 24-May-15 08:32 onnxruntime/transformers/quantize_helper.py
+-rw-rw-rw-  2.0 fat     4591 b- defN 24-May-15 08:32 onnxruntime/transformers/shape_infer_helper.py
+-rw-rw-rw-  2.0 fat    15505 b- defN 24-May-15 08:32 onnxruntime/transformers/shape_optimizer.py
+-rw-rw-rw-  2.0 fat     2575 b- defN 24-May-15 08:32 onnxruntime/transformers/torch_onnx_export_helper.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-May-15 08:32 onnxruntime/transformers/models/bart/__init__.py
+-rw-rw-rw-  2.0 fat     4285 b- defN 24-May-15 08:32 onnxruntime/transformers/models/bart/export.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-May-15 08:32 onnxruntime/transformers/models/bert/__init__.py
+-rw-rw-rw-  2.0 fat    12378 b- defN 24-May-15 08:32 onnxruntime/transformers/models/bert/eval_squad.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/__init__.py
+-rw-rw-rw-  2.0 fat    15930 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/benchmark_gpt2.py
+-rw-rw-rw-  2.0 fat    20593 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    41381 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/gpt2_helper.py
+-rw-rw-rw-  2.0 fat    18238 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/gpt2_parity.py
+-rw-rw-rw-  2.0 fat    20020 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/gpt2_tester.py
+-rw-rw-rw-  2.0 fat     5806 b- defN 24-May-15 08:32 onnxruntime/transformers/models/gpt2/parity_check_helper.py
+-rw-rw-rw-  2.0 fat      490 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/__init__.py
+-rw-rw-rw-  2.0 fat    27262 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/benchmark.py
+-rw-rw-rw-  2.0 fat    15851 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/benchmark_all.py
+-rw-rw-rw-  2.0 fat    24221 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/benchmark_e2e.py
+-rw-rw-rw-  2.0 fat    43492 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     1636 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/dist_settings.py
+-rw-rw-rw-  2.0 fat    21005 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/llama_inputs.py
+-rw-rw-rw-  2.0 fat    10236 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/llama_parity.py
+-rw-rw-rw-  2.0 fat     1665 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/llama_torch.py
+-rw-rw-rw-  2.0 fat     4959 b- defN 24-May-15 08:32 onnxruntime/transformers/models/llama/quant_kv_dataloader.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-May-15 08:32 onnxruntime/transformers/models/longformer/__init__.py
+-rw-rw-rw-  2.0 fat    30250 b- defN 24-May-15 08:32 onnxruntime/transformers/models/longformer/benchmark_longformer.py
+-rw-rw-rw-  2.0 fat    15219 b- defN 24-May-15 08:32 onnxruntime/transformers/models/longformer/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     9964 b- defN 24-May-15 08:32 onnxruntime/transformers/models/longformer/generate_test_data.py
+-rw-rw-rw-  2.0 fat     3180 b- defN 24-May-15 08:32 onnxruntime/transformers/models/longformer/longformer_helper.py
+-rw-rw-rw-  2.0 fat      490 b- defN 24-May-15 08:32 onnxruntime/transformers/models/phi2/__init__.py
+-rw-rw-rw-  2.0 fat    20376 b- defN 24-May-15 08:32 onnxruntime/transformers/models/phi2/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    17700 b- defN 24-May-15 08:32 onnxruntime/transformers/models/phi2/inference_example.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/__init__.py
+-rw-rw-rw-  2.0 fat    48353 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/benchmark.py
+-rw-rw-rw-  2.0 fat    13253 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py
+-rw-rw-rw-  2.0 fat     3394 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py
+-rw-rw-rw-  2.0 fat    10179 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py
+-rw-rw-rw-  2.0 fat    29367 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/demo_utils.py
+-rw-rw-rw-  2.0 fat    51710 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/diffusion_models.py
+-rw-rw-rw-  2.0 fat    49538 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/diffusion_schedulers.py
+-rw-rw-rw-  2.0 fat    11980 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/engine_builder.py
+-rw-rw-rw-  2.0 fat    16294 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py
+-rw-rw-rw-  2.0 fat    11451 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_trt.py
+-rw-rw-rw-  2.0 fat    15999 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/engine_builder_tensorrt.py
+-rw-rw-rw-  2.0 fat     4289 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/engine_builder_torch.py
+-rw-rw-rw-  2.0 fat    12881 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py
+-rw-rw-rw-  2.0 fat     5836 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/ort_optimizer.py
+-rw-rw-rw-  2.0 fat    33995 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py
+-rw-rw-rw-  2.0 fat      432 b- defN 24-May-15 08:32 onnxruntime/transformers/models/stable_diffusion/trt_utilities.py
+-rw-rw-rw-  2.0 fat      495 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/__init__.py
+-rw-rw-rw-  2.0 fat     9010 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat     6987 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/past_helper.py
+-rw-rw-rw-  2.0 fat    17262 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/t5_decoder.py
+-rw-rw-rw-  2.0 fat     6295 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/t5_encoder.py
+-rw-rw-rw-  2.0 fat    12273 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    11032 b- defN 24-May-15 08:32 onnxruntime/transformers/models/t5/t5_helper.py
+-rw-rw-rw-  2.0 fat      490 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/__init__.py
+-rw-rw-rw-  2.0 fat    23376 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/benchmark.py
+-rw-rw-rw-  2.0 fat    19461 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/benchmark_all.py
+-rw-rw-rw-  2.0 fat    18404 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/convert_to_onnx.py
+-rw-rw-rw-  2.0 fat    14910 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/whisper_chain.py
+-rw-rw-rw-  2.0 fat    16021 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/whisper_decoder.py
+-rw-rw-rw-  2.0 fat     5740 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/whisper_encoder.py
+-rw-rw-rw-  2.0 fat    12723 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py
+-rw-rw-rw-  2.0 fat    23487 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/whisper_helper.py
+-rw-rw-rw-  2.0 fat     3272 b- defN 24-May-15 08:32 onnxruntime/transformers/models/whisper/whisper_openai_helper.py
+-rw-rw-rw-  2.0 fat     4514 b- defN 24-May-15 08:38 onnxruntime_training_cpu-1.18.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-May-15 08:38 onnxruntime_training_cpu-1.18.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       77 b- defN 24-May-15 08:38 onnxruntime_training_cpu-1.18.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 24-May-15 08:38 onnxruntime_training_cpu-1.18.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    46962 b- defN 24-May-15 08:38 onnxruntime_training_cpu-1.18.0.dist-info/RECORD
+423 files, 21375551 bytes uncompressed, 6993362 bytes compressed:  67.3%
```

## zipnote {}

```diff
@@ -63,14 +63,17 @@
 
 Filename: onnxruntime/datasets/sigmoid.onnx
 Comment: 
 
 Filename: onnxruntime/quantization/__init__.py
 Comment: 
 
+Filename: onnxruntime/quantization/base_quantizer.py
+Comment: 
+
 Filename: onnxruntime/quantization/calibrate.py
 Comment: 
 
 Filename: onnxruntime/quantization/matmul_4bits_quantizer.py
 Comment: 
 
 Filename: onnxruntime/quantization/matmul_bnb4_quantizer.py
@@ -99,14 +102,17 @@
 
 Filename: onnxruntime/quantization/registry.py
 Comment: 
 
 Filename: onnxruntime/quantization/shape_inference.py
 Comment: 
 
+Filename: onnxruntime/quantization/tensor_quant_overrides.py
+Comment: 
+
 Filename: onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py
 Comment: 
 
 Filename: onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py
 Comment: 
 
 Filename: onnxruntime/quantization/CalTableFlatBuffers/__init__.py
@@ -114,14 +120,17 @@
 
 Filename: onnxruntime/quantization/execution_providers/qnn/__init__.py
 Comment: 
 
 Filename: onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py
 Comment: 
 
+Filename: onnxruntime/quantization/execution_providers/qnn/mixed_precision_overrides_utils.py
+Comment: 
+
 Filename: onnxruntime/quantization/execution_providers/qnn/preprocess.py
 Comment: 
 
 Filename: onnxruntime/quantization/execution_providers/qnn/quant_config.py
 Comment: 
 
 Filename: onnxruntime/quantization/fusions/__init__.py
@@ -693,14 +702,20 @@
 
 Filename: onnxruntime/training/ortmodule/experimental/json_config/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py
 Comment: 
 
+Filename: onnxruntime/training/ortmodule/experimental/pipe/__init__.py
+Comment: 
+
+Filename: onnxruntime/training/ortmodule/experimental/pipe/_ort_pipeline_module.py
+Comment: 
+
 Filename: onnxruntime/training/ortmodule/graph_optimizers/__init__.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py
 Comment: 
 
 Filename: onnxruntime/training/ortmodule/graph_optimizers/utils.py
@@ -855,14 +870,17 @@
 
 Filename: onnxruntime/transformers/convert_tf_models_to_pytorch.py
 Comment: 
 
 Filename: onnxruntime/transformers/convert_to_packing_mode.py
 Comment: 
 
+Filename: onnxruntime/transformers/dynamo_onnx_helper.py
+Comment: 
+
 Filename: onnxruntime/transformers/float16.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_attention.py
 Comment: 
 
 Filename: onnxruntime/transformers/fusion_attention_clip.py
@@ -1005,26 +1023,32 @@
 
 Filename: onnxruntime/transformers/onnx_model_conformer.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_gpt2.py
 Comment: 
 
+Filename: onnxruntime/transformers/onnx_model_phi.py
+Comment: 
+
 Filename: onnxruntime/transformers/onnx_model_t5.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_tnlr.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_unet.py
 Comment: 
 
 Filename: onnxruntime/transformers/onnx_model_vae.py
 Comment: 
 
+Filename: onnxruntime/transformers/onnx_utils.py
+Comment: 
+
 Filename: onnxruntime/transformers/optimizer.py
 Comment: 
 
 Filename: onnxruntime/transformers/profiler.py
 Comment: 
 
 Filename: onnxruntime/transformers/quantize_helper.py
@@ -1113,14 +1137,23 @@
 
 Filename: onnxruntime/transformers/models/longformer/generate_test_data.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/longformer/longformer_helper.py
 Comment: 
 
+Filename: onnxruntime/transformers/models/phi2/__init__.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/phi2/convert_to_onnx.py
+Comment: 
+
+Filename: onnxruntime/transformers/models/phi2/inference_example.py
+Comment: 
+
 Filename: onnxruntime/transformers/models/stable_diffusion/__init__.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/stable_diffusion/benchmark.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py
@@ -1215,23 +1248,23 @@
 
 Filename: onnxruntime/transformers/models/whisper/whisper_helper.py
 Comment: 
 
 Filename: onnxruntime/transformers/models/whisper/whisper_openai_helper.py
 Comment: 
 
-Filename: onnxruntime_training_cpu-1.17.3.dist-info/METADATA
+Filename: onnxruntime_training_cpu-1.18.0.dist-info/METADATA
 Comment: 
 
-Filename: onnxruntime_training_cpu-1.17.3.dist-info/WHEEL
+Filename: onnxruntime_training_cpu-1.18.0.dist-info/WHEEL
 Comment: 
 
-Filename: onnxruntime_training_cpu-1.17.3.dist-info/entry_points.txt
+Filename: onnxruntime_training_cpu-1.18.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: onnxruntime_training_cpu-1.17.3.dist-info/top_level.txt
+Filename: onnxruntime_training_cpu-1.18.0.dist-info/top_level.txt
 Comment: 
 
-Filename: onnxruntime_training_cpu-1.17.3.dist-info/RECORD
+Filename: onnxruntime_training_cpu-1.18.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## onnxruntime/ThirdPartyNotices.txt

```diff
@@ -1825,15 +1825,15 @@
 Stripe, Inc.
 Yixuan Qiu <yixuanq@gmail.com>
 Yusuke Suzuki <utatane.tea@gmail.com>
 Zbigniew Skowron <zbychs@gmail.com>
 
 _____
 
-HalidelR
+HalideIR
 
 Copyright (c) 2016 HalideIR contributors
 Copyright (c) 2012-2014 MIT CSAIL, Google Inc., and other contributors
 HalideIR is derived from the Halide project.
 
 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
```

## onnxruntime/__init__.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 """
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_
 or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 """
-__version__ = "1.17.3"
+__version__ = "1.18.0"
 __author__ = "Microsoft"
 
 # we need to do device version validation (for example to check Cuda version for an onnxruntime-training package).
 # in order to know whether the onnxruntime package is for training it needs
 # to do import onnxruntime.training.ortmodule first.
 # onnxruntime.capi._pybind_state is required before import onnxruntime.training.ortmodule.
 # however, import onnxruntime.capi._pybind_state will already raise an exception if a required Cuda version
```

## onnxruntime/capi/build_and_package_info.py

```diff
@@ -1,2 +1,2 @@
 package_name = 'onnxruntime-training-cpu'
-__version__ = '1.17.3'
+__version__ = '1.18.0'
```

## onnxruntime/capi/onnxruntime_inference_collection.py

```diff
@@ -354,15 +354,15 @@
     """
     This is the main class used to run a model.
     """
 
     def __init__(
         self,
         path_or_bytes: str | bytes | os.PathLike,
-        sess_options: Sequence[onnxruntime.SessionOptions] | None = None,
+        sess_options: onnxruntime.SessionOptions | None = None,
         providers: Sequence[str | tuple[str, dict[Any, Any]]] | None = None,
         provider_options: Sequence[dict[Any, Any]] | None = None,
         **kwargs,
     ) -> None:
         """
         :param path_or_bytes: Filename or serialized ONNX or ORT format model in a byte string.
         :param sess_options: Session options.
@@ -409,15 +409,15 @@
         self._enable_fallback = True
         if "read_config_from_model" in kwargs:
             self._read_config_from_model = int(kwargs["read_config_from_model"]) == 1
         else:
             self._read_config_from_model = os.environ.get("ORT_LOAD_CONFIG_FROM_MODEL") == "1"
 
         # internal parameters that we don't expect to be used in general so aren't documented
-        disabled_optimizers = kwargs["disabled_optimizers"] if "disabled_optimizers" in kwargs else None
+        disabled_optimizers = kwargs.get("disabled_optimizers")
 
         try:
             self._create_inference_session(providers, provider_options, disabled_optimizers)
         except (ValueError, RuntimeError) as e:
             if self._enable_fallback:
                 try:
                     print("*************** EP Error ***************")
```

## onnxruntime/capi/onnxruntime_providers_shared.dll

### objdump

```diff
@@ -4,15 +4,15 @@
 start address 0x0000000180001370
 
 Characteristics 0x2022
 	executable
 	large address aware
 	DLL
 
-Time/Date		Fri Apr 12 20:49:05 2024
+Time/Date		Wed May 15 08:17:59 2024
 Magic			020b	(PE32+)
 MajorLinkerVersion	14
 MinorLinkerVersion	37
 SizeOfCode		0000000000001000
 SizeOfInitializedData	0000000000002000
 SizeOfUninitializedData	0000000000000000
 AddressOfEntryPoint	0000000000001370
@@ -25,15 +25,15 @@
 MajorImageVersion	0
 MinorImageVersion	0
 MajorSubsystemVersion	6
 MinorSubsystemVersion	0
 Win32Version		00000000
 SizeOfImage		00007000
 SizeOfHeaders		00000400
-CheckSum		0000a12a
+CheckSum		0000615f
 Subsystem		00000003	(Windows CUI)
 DllCharacteristics	00004160
 					HIGH_ENTROPY_VA
 					DYNAMIC_BASE
 					NX_COMPAT
 					GUARD_CF
 SizeOfStackReserve	0000000000100000
@@ -44,15 +44,15 @@
 NumberOfRvaAndSizes	00000010
 
 The Data Directory
 Entry 0 0000000000002860 00000080 Export Directory [.edata (or where ever we found it)]
 Entry 1 00000000000028e0 00000050 Import Directory [parts of .idata]
 Entry 2 0000000000005000 000005c8 Resource Directory [.rsrc]
 Entry 3 0000000000004000 000001b0 Exception Directory [.pdata]
-Entry 4 0000000000002e00 000027b0 Security Directory
+Entry 4 0000000000002e00 00002820 Security Directory
 Entry 5 0000000000006000 0000002c Base Relocation Directory [.reloc]
 Entry 6 0000000000002300 00000054 Debug Directory
 Entry 7 0000000000000000 00000000 Description Directory
 Entry 8 0000000000000000 00000000 Special Directory
 Entry 9 0000000000000000 00000000 Thread Storage Directory [.tls]
 Entry a 00000000000021c0 00000140 Load Configuration Directory
 Entry b 0000000000000000 00000000 Bound Import Directory
@@ -433,15 +433,15 @@
 	reloc   16 offset  2f0 [22f0] DIR64
 	reloc   17 offset  2f8 [22f8] DIR64
 
 There is a debug directory in .rdata at 0x180002300
 
 Type                Size     Rva      Offset
   2        CodeView 00000066 000023e4 000017e4
-(format RSDS signature 849d0465f9e94d048ea0af066341ce19 age 1 pdb C:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
+(format RSDS signature c885d358fc4d428e82e14263a37222e2 age 1 pdb C:\a\_work\1\b\RelWithDebInfo\RelWithDebInfo\onnxruntime_providers_shared.pdb)
  13         CoffGrp 00000268 0000244c 0000184c
  20         Unknown 00000004 000026b4 00001ab4
 
 The .rsrc Resource Directory section:
 000  Type Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 2
 010   Entry: ID: 0x000010, Value: 0x80000020
 020    Name Table: Char: 0, Time: 00000000, Ver: 0/0, Num Names: 0, IDs: 1
@@ -1735,38 +1735,39 @@
    1800022ea:	add    %al,0x1(%rax)
    1800022f0:	sbb    %ah,(%rcx)
    1800022f2:	add    %al,0x1(%rax)
    1800022f8:	and    %ah,(%rcx)
    1800022fa:	add    %al,0x1(%rax)
    180002300:	add    %al,(%rax)
    180002302:	add    %al,(%rax)
-   180002304:	rex.B sahf
-   180002306:	sbb    %esp,0x0(%rsi)
-   180002309:	add    %al,(%rax)
-   18000230b:	add    %al,(%rdx)
-   18000230d:	add    %al,(%rax)
-   18000230f:	add    %ah,0x0(%rsi)
-   180002312:	add    %al,(%rax)
-   180002314:	in     $0x23,%al
-   180002316:	add    %al,(%rax)
-   180002318:	in     $0x17,%al
+   180002304:	mov    $0x6f,%bh
+   180002306:	rex.R
+   180002307:	data16 add %al,(%rax)
+   18000230a:	add    %al,(%rax)
+   18000230c:	add    (%rax),%al
+   18000230e:	add    %al,(%rax)
+   180002310:	data16 add %al,(%rax)
+   180002313:	add    %ah,%ah
+   180002315:	and    (%rax),%eax
+   180002317:	add    %ah,%ah
+   180002319:	(bad)
    18000231a:	add    %al,(%rax)
    18000231c:	add    %al,(%rax)
    18000231e:	add    %al,(%rax)
-   180002320:	rex.B sahf
-   180002322:	sbb    %esp,0x0(%rsi)
-   180002325:	add    %al,(%rax)
-   180002327:	add    %cl,0x68000000(%rip)        # 0x1e800232d
+   180002320:	mov    $0x6f,%bh
+   180002322:	rex.R
+   180002323:	data16 add %al,(%rax)
+   180002326:	add    %al,(%rax)
+   180002328:	or     $0x68000000,%eax
    18000232d:	add    (%rax),%al
    18000232f:	add    %cl,0x0(%rsp)
    180002333:	add    %cl,0x0(%rax,%rbx,1)
    180002337:	add    %al,(%rax)
    180002339:	add    %al,(%rax)
-   18000233b:	add    %al,-0x62(%rcx)
-   18000233e:	sbb    %esp,0x0(%rsi)
+   18000233b:	add    %dh,0x66446f(%rdi)
    180002341:	add    %al,(%rax)
    180002343:	add    %dl,(%rax,%rax,1)
    180002346:	add    %al,(%rax)
    180002348:	add    $0x0,%al
    18000234a:	add    %al,(%rax)
    18000234c:	mov    $0x26,%ah
    18000234e:	add    %al,(%rax)
@@ -1810,21 +1811,19 @@
    1800023da:	add    %al,(%rax)
    1800023dc:	mov    $0x1d,%dh
    1800023de:	add    %al,(%rax)
    1800023e0:	xchg   %eax,%edx
    1800023e1:	add    %al,(%rax)
    1800023e3:	add    %dl,0x53(%rdx)
    1800023e6:	rex.R push %rbx
-   1800023e8:	gs add $0x9d,%al
-   1800023eb:	test   %ch,%cl
-   1800023ed:	stc
-   1800023ee:	add    $0x4d,%al
-   1800023f0:	mov    0x416306af(%rax),%fs
-   1800023f6:	(bad)
-   1800023f7:	sbb    %eax,(%rcx)
+   1800023e8:	pop    %rax
+   1800023e9:	roll   %cl,-0x7103b238(%rbp)
+   1800023ef:	rex.X (bad)
+   1800023f1:	loope  0x180002435
+   1800023f3:	movsxd 0x1e22272(%rbx),%esp
    1800023f9:	add    %al,(%rax)
    1800023fb:	add    %al,0x3a(%rbx)
    1800023fe:	pop    %rsp
    1800023ff:	(bad)
    180002400:	pop    %rsp
    180002401:	pop    %rdi
    180002402:	ja     0x180002473
@@ -2997,20 +2996,22 @@
    1800050ba:	pop    %rdi
    1800050bb:	add    %cl,0x0(%rcx)
    1800050be:	rex.WRX add %r8b,0x0(%rsi)
    1800050c2:	rex.WRXB add %r8b,(%r8)
    1800050c5:	add    %al,(%rax)
    1800050c7:	add    %bh,0xfeef04(%rbp)
    1800050cd:	add    %al,(%rcx)
-   1800050cf:	add    %dl,(%rcx)
+   1800050cf:	add    %dl,(%rdx)
    1800050d1:	add    %al,(%rcx)
-   1800050d3:	add    %bl,0x110018(%rcx,%rax,1)
-   1800050da:	add    %eax,(%rax)
-   1800050dc:	pushf
-   1800050dd:	add    %ebx,(%rax)
+   1800050d3:	add    %al,(%rbx)
+   1800050d5:	add    (%rax),%bl
+   1800050d7:	add    %dl,(%rdx)
+   1800050d9:	add    %al,(%rcx)
+   1800050db:	add    %al,(%rbx)
+   1800050dd:	add    (%rax),%bl
    1800050df:	add    %bh,(%rdi)
    1800050e1:	add    %al,(%rax)
    1800050e3:	add    %al,(%rax)
    1800050e5:	add    %al,(%rax)
    1800050e7:	add    %al,(%rax,%rax,1)
    1800050ea:	add    %al,(%rax)
    1800050ec:	add    (%rax),%al
@@ -3108,31 +3109,30 @@
    1800051da:	jb     0x1800051dc
    1800051dc:	jae    0x1800051de
    1800051de:	imul   $0x6e006f,(%rax),%eax
    1800051e4:	add    %al,(%rax)
    1800051e6:	add    %al,(%rax)
    1800051e8:	xor    %eax,(%rax)
    1800051ea:	cs add %dh,(%rcx)
-   1800051ed:	add    %dh,(%rdi)
+   1800051ed:	add    %bh,(%rax)
    1800051ef:	add    %ch,(%rsi)
    1800051f1:	add    %dh,(%rdx)
    1800051f3:	add    %dh,(%rax)
    1800051f5:	add    %dh,(%rdx)
    1800051f7:	add    %dh,(%rax,%rax,1)
    1800051fa:	xor    %al,(%rax)
-   1800051fc:	xor    $0x0,%al
-   1800051fe:	xor    %eax,(%rax)
-   180005200:	xor    (%rax),%al
-   180005202:	cs add %dh,(%rdx)
+   1800051fc:	xor    $0x35003100,%eax
+   180005201:	add    %ch,(%rsi)
+   180005203:	add    %dh,(%rcx)
    180005205:	add    %ch,(%rsi)
-   180005207:	add    %dh,0x62003600(%rip)        # 0x1e200880d
-   18000520d:	add    %dh,(%rsi)
-   18000520f:	add    %dh,(%rsi)
-   180005211:	add    %dh,(%rax)
-   180005213:	add    %ah,0x0(%rsi)
+   180005207:	add    %dh,(%rax,%rax,1)
+   18000520a:	xor    $0x33003700,%eax
+   18000520f:	add    %dh,(%rdi)
+   180005211:	add    %dh,(%rax,%rax,1)
+   180005214:	xor    %al,(%rax)
    180005216:	add    %al,(%rax)
    180005218:	cmp    (%rax),%al
    18000521a:	or     $0x49000100,%eax
    18000521f:	add    %ch,0x0(%rsi)
    180005222:	je     0x180005224
    180005224:	add    %dh,%gs:0x0(%rdx)
    180005228:	outsb  %ds:(%rsi),(%dx)
@@ -3296,31 +3296,30 @@
    1800053c1:	add    %ah,0x0(%rbp)
    1800053c4:	jb     0x1800053c6
    1800053c6:	jae    0x1800053c8
    1800053c8:	imul   $0x6e006f,(%rax),%eax
    1800053ce:	add    %al,(%rax)
    1800053d0:	xor    %eax,(%rax)
    1800053d2:	cs add %dh,(%rcx)
-   1800053d5:	add    %dh,(%rdi)
+   1800053d5:	add    %bh,(%rax)
    1800053d7:	add    %ch,(%rsi)
    1800053d9:	add    %dh,(%rdx)
    1800053db:	add    %dh,(%rax)
    1800053dd:	add    %dh,(%rdx)
    1800053df:	add    %dh,(%rax,%rax,1)
    1800053e2:	xor    %al,(%rax)
-   1800053e4:	xor    $0x0,%al
-   1800053e6:	xor    %eax,(%rax)
-   1800053e8:	xor    (%rax),%al
-   1800053ea:	cs add %dh,(%rdx)
+   1800053e4:	xor    $0x35003100,%eax
+   1800053e9:	add    %ch,(%rsi)
+   1800053eb:	add    %dh,(%rcx)
    1800053ed:	add    %ch,(%rsi)
-   1800053ef:	add    %dh,0x62003600(%rip)        # 0x1e20089f5
-   1800053f5:	add    %dh,(%rsi)
-   1800053f7:	add    %dh,(%rsi)
-   1800053f9:	add    %dh,(%rax)
-   1800053fb:	add    %ah,0x0(%rsi)
+   1800053ef:	add    %dh,(%rax,%rax,1)
+   1800053f2:	xor    $0x33003700,%eax
+   1800053f7:	add    %dh,(%rdi)
+   1800053f9:	add    %dh,(%rax,%rax,1)
+   1800053fc:	xor    %al,(%rax)
    1800053fe:	add    %al,(%rax)
    180005400:	add    %r8b,(%rax)
    180005403:	add    %al,(%rcx)
    180005405:	add    %dl,0x0(%rsi)
    180005408:	(bad)
    180005409:	add    %dh,0x0(%rdx)
    18000540c:	rex.RX add %r13b,0x0(%rcx)
```

## onnxruntime/quantization/calibrate.py

```diff
@@ -364,15 +364,14 @@
             if not inputs:
                 break
             self.intermediate_outputs.append(self.infer_session.run(None, inputs))
             if (
                 self.max_intermediate_outputs is not None
                 and len(self.intermediate_outputs) == self.max_intermediate_outputs
             ):
-                self.compute_range()
                 self.clear_collected_data()
 
         if len(self.intermediate_outputs) == 0 and self.calibrate_tensors_range is None:
             raise ValueError("No data is collected.")
 
         t = self.compute_data()
         if not isinstance(t, TensorsData):
@@ -730,21 +729,19 @@
     def collect_absolute_value(self, name_to_arr):
         """
         Collect histogram on absolute value
         """
         for tensor, data_arr in name_to_arr.items():
             if isinstance(data_arr, list):
                 for arr in data_arr:
-                    if not isinstance(arr, np.ndarray):
-                        raise ValueError(f"Unexpected type {type(arr)} for tensor={tensor!r}")
-                dtypes = set(a.dtype for a in arr)
-                if len(dtypes) != 1:
-                    raise ValueError(
-                        f"The calibration expects only one element type but got {dtypes} for tensor={tensor!r}"
-                    )
+                    assert isinstance(arr, np.ndarray), f"Unexpected type {type(arr)} for tensor={tensor!r}"
+                dtypes = set(a.dtype for a in data_arr)
+                assert (
+                    len(dtypes) == 1
+                ), f"The calibration expects only one element type but got {dtypes} for tensor={tensor!r}"
                 data_arr_np = np.asarray(data_arr)
             elif not isinstance(data_arr, np.ndarray):
                 raise ValueError(f"Unexpected type {type(data_arr)} for tensor={tensor!r}")
             else:
                 data_arr_np = data_arr
             data_arr_np = data_arr_np.flatten()
             if data_arr_np.size > 0:
@@ -914,19 +911,15 @@
     def compute_entropy(self):
         histogram_dict = self.histogram_dict
         num_quantized_bins = self.num_quantized_bins
 
         thresholds_dict = {}  # per tensor thresholds
 
         print(f"Number of tensors : {len(histogram_dict)}")
-        print(
-            "Number of histogram bins : {} (The number may increase depends on the data it collects)".format(
-                self.num_bins
-            )
-        )
+        print(f"Number of histogram bins : {self.num_bins} (The number may increase depends on the data it collects)")
         print(f"Number of quantized bins : {self.num_quantized_bins}")
 
         for tensor, histogram in histogram_dict.items():
             optimal_threshold = self.get_entropy_threshold(histogram, num_quantized_bins)
             thresholds_dict[tensor] = optimal_threshold
             thresholds_dict[tensor] = (*optimal_threshold, *histogram[:2])
 
@@ -1096,63 +1089,61 @@
     calibrate_method=CalibrationMethod.MinMax,
     use_external_data_format=False,
     extra_options={},  # noqa: B006
 ):
     calibrator = None
     if calibrate_method == CalibrationMethod.MinMax:
         # default settings for min-max algorithm
-        symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
-        moving_average = False if "moving_average" not in extra_options else extra_options["moving_average"]
-        averaging_constant = 0.01 if "averaging_constant" not in extra_options else extra_options["averaging_constant"]
-        max_intermediate_outputs = (
-            None if "max_intermediate_outputs" not in extra_options else extra_options["max_intermediate_outputs"]
-        )
+        symmetric = extra_options.get("symmetric", False)
+        moving_average = extra_options.get("moving_average", False)
+        averaging_constant = extra_options.get("averaging_constant", 0.01)
+        max_intermediate_outputs = extra_options.get("max_intermediate_outputs", None)
         calibrator = MinMaxCalibrater(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             moving_average=moving_average,
             averaging_constant=averaging_constant,
             max_intermediate_outputs=max_intermediate_outputs,
         )
     elif calibrate_method == CalibrationMethod.Entropy:
         # default settings for entropy algorithm
-        num_bins = 128 if "num_bins" not in extra_options else extra_options["num_bins"]
-        num_quantized_bins = 128 if "num_quantized_bins" not in extra_options else extra_options["num_quantized_bins"]
-        symmetric = False if "symmetric" not in extra_options else extra_options["symmetric"]
+        num_bins = extra_options.get("num_bins", 128)
+        num_quantized_bins = extra_options.get("num_quantized_bins", 128)
+        symmetric = extra_options.get("symmetric", False)
         calibrator = EntropyCalibrater(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             num_bins=num_bins,
             num_quantized_bins=num_quantized_bins,
         )
     elif calibrate_method == CalibrationMethod.Percentile:
         # default settings for percentile algorithm
-        num_bins = 2048 if "num_bins" not in extra_options else extra_options["num_bins"]
-        percentile = 99.999 if "percentile" not in extra_options else extra_options["percentile"]
-        symmetric = True if "symmetric" not in extra_options else extra_options["symmetric"]
+        num_bins = extra_options.get("num_bins", 2048)
+        percentile = extra_options.get("percentile", 99.999)
+        symmetric = extra_options.get("symmetric", True)
         calibrator = PercentileCalibrater(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format=use_external_data_format,
             symmetric=symmetric,
             num_bins=num_bins,
             percentile=percentile,
         )
 
     elif calibrate_method == CalibrationMethod.Distribution:
         # default settings for percentile algorithm
-        num_bins = 2048 if "num_bins" not in extra_options else extra_options["num_bins"]
-        scenario = "same" if "scenario" not in extra_options else extra_options["scenario"]
+        num_bins = extra_options.get("num_bins", 2048)
+        scenario = extra_options.get("scenario", "same")
 
         calibrator = DistributionCalibrater(
             model,
             op_types_to_calibrate,
             augmented_model_path,
             use_external_data_format=use_external_data_format,
             num_bins=num_bins,
```

## onnxruntime/quantization/matmul_4bits_quantizer.py

```diff
@@ -61,115 +61,363 @@
 
 
 class GPTQWeightOnlyQuantConfig(WeightOnlyQuantConfig):
     def __init__(
         self,
         calibration_data_reader: CalibrationDataReader,
         percdamp=0.01,
-        blocksize=128,
+        block_size=128,
         actorder=False,
         mse=False,
         perchannel=True,
     ):
         """
         This is a class for GPTQ algorithm Weight Only Quant Configuration.
         GPTQ algorithm provides more accurate quantization but requires more computational resources.
 
         Args:
             calibration_data_reader:
                 a calibration data reader. It enumerates calibration data and generates inputs for the original model.
             percdamp:
                 percent of the average Hessian diagonal to use for dampening.
-            blocksize (int, optional):
+            block_size (int, optional):
                 channel number in one block to execute a GPTQ quantization iteration.
             actorder (bool, optional):
                 whether rearrange Hessian matrix considering the diag's value.
             mse (bool, optional):
                 whether get scale and zero point with mse error.
             perchannel (bool, optional):
                 whether quantize weight per-channel.
         """
         super().__init__(
             algorithm="GPTQ",
         )
         self.calibration_data_reader = calibration_data_reader
         self.percdamp = percdamp
-        self.blocksize = blocksize
+        self.block_size = block_size
         self.actorder = actorder
         self.mse = mse
         self.perchannel = perchannel
 
 
-class MatMul4BitsQuantizer:
-    """Perform 4b quantization of constant MatMul weights"""
+class HQQWeightOnlyQuantConfig(WeightOnlyQuantConfig):
+    def __init__(
+        self,
+        block_size=128,
+        bits=4,
+        axis=1,
+    ):
+        """
+        This is a class for HQQ algorithm Weight Only Quant Configuration.
+        HQQ algorithm quant weight without needing calibrate data.
 
+        Args:
+            block_size (int, optional):
+                channel number in one block to execute a GPTQ quantization iteration.
+            bits (int, optional):
+                how many bits to represent weight.
+            axis (int, optional):
+                0 or 1. which axis to quantize. https://arxiv.org/pdf/2309.15531.pdf
+        """
+        super().__init__(
+            algorithm="HQQ",
+        )
+        self.block_size = block_size
+        self.bits = bits
+        self.axis = axis
+
+
+class DefaultWeightOnlyQuantConfig(WeightOnlyQuantConfig):
     def __init__(
         self,
-        model: ModelProto | str,
-        block_size: int,
-        is_symmetric: bool,
+        block_size: int = 128,
+        is_symmetric: bool = False,
         accuracy_level: int | None = None,
-        nodes_to_exclude=None,
-        algo_config: WeightOnlyQuantConfig = None,
     ):
-        if nodes_to_exclude is None:
-            nodes_to_exclude = []
-        self.model = ONNXModel(onnx.load(model)) if isinstance(model, str) else ONNXModel(model)
-        self.model_path = model if isinstance(model, str) else None
+        super().__init__(algorithm="DEFAULT")
         self.block_size = block_size
         self.is_symmetric = is_symmetric
+        self.bits = 4
         self.accuracy_level = accuracy_level
-        self.nodes_to_exclude = set(nodes_to_exclude)
-        self.algo_config = algo_config
 
+
+def is_divisible(val1, val2):
+    return int(val2 * np.ceil(val1 / val2)) == val1
+
+
+class HQQWeightOnlyQuantizer:
+    def __init__(
+        self,
+        config: HQQWeightOnlyQuantConfig,
+    ):
+        self.config = config
+
+    # Proximal solver || weight - dequantize(quantize(weight))||_p^p
     @staticmethod
-    def __get_initializer(name, graph_path: list[GraphProto]) -> tuple[TensorProto, GraphProto]:
-        for gid in range(len(graph_path) - 1, -1, -1):
-            graph = graph_path[gid]
-            for tensor in graph.initializer:
-                if tensor.name == name:
-                    return tensor, graph
-        return None, None
+    def optimize_weights(
+        tensor,
+        scale,
+        zero,
+        min_max: list[int],
+        axis: int = 0,
+        opt_params: dict = None,  # noqa: RUF013
+        verbose=False,
+    ):
+        import torch
+
+        opt_params = {"lp_norm": 0.7, "beta": 1e1, "kappa": 1.01, "iters": 20} if opt_params is None else opt_params
+        lp_norm, beta, kappa, iters = (
+            opt_params["lp_norm"],
+            opt_params["beta"],
+            opt_params["kappa"],
+            opt_params["iters"],
+        )
+
+        dtype = torch.float16 if tensor.is_cuda else torch.float32
+        w_f = tensor.to(dtype)
+        scale = scale.to(dtype)
+        zero = zero.to(dtype)
+
+        if lp_norm == 1:
+
+            def shrink_op(x, beta):
+                return torch.sign(x) * torch.nn.functional.relu(torch.abs(x) - 1.0 / beta)
+
+        else:
+
+            def shrink_op(x, beta, p=lp_norm):
+                return torch.sign(x) * torch.nn.functional.relu(
+                    torch.abs(x) - (1.0 / beta) * torch.pow(torch.abs(x) + 1e-8, p - 1)
+                )
+
+        best_error = 1e4
+        for i in range(iters):
+            w_q = torch.round(w_f * scale + zero).clamp(min_max[0], min_max[1])
+            w_r = (w_q - zero) / scale
+            w_e = shrink_op(w_f - w_r, beta)
+            zero = torch.mean(w_q - (w_f - w_e) * scale, axis=axis, keepdim=True)
+            beta *= kappa
+
+            current_error = float(torch.abs(w_f - w_r).mean())
+            if verbose:
+                print(i, np.round(current_error, 6))
+            if current_error < best_error:
+                best_error = current_error
+            else:
+                break
+
+        del w_f, w_q, w_r, w_e
+
+        return scale, zero
+
+    @staticmethod
+    def pack_on_row_fast_248bit(pack_tensor, ori_int_tensor, bits):
+        if pack_tensor.shape[0] == ori_int_tensor.shape[0]:
+            ori_int_tensor = ori_int_tensor.T
+            pack_tensor = pack_tensor.T
+        if bits in [2, 4, 8]:
+            compress_ratio = pack_tensor.element_size() * 8 // bits
+            for j in range(compress_ratio):
+                pack_tensor[0:] |= ori_int_tensor[j::compress_ratio] << (bits * (j))
+        else:
+            raise NotImplementedError("Only 2,4,8 bits are supported.")
+
+    # from Official implementation of Half-Quadratic Quantization (HQQ)
+    def quantize_internal(
+        self, tensor, bits=4, channel_wise=True, group_size=64, optimize=True, round_zero=True, axis=1
+    ):
+        import torch
+
+        weight = tensor.float()
+        ori_shape = weight.shape
+
+        pad_len = (group_size - ori_shape[axis] % group_size) % group_size
+        if axis == 1:
+            weight = torch.nn.functional.pad(weight, (0, pad_len), "constant", 0)
+        else:
+            weight = torch.nn.functional.pad(weight, (0, 0, 0, pad_len), "constant", 0)
+        shape = weight.shape
+
+        # Reshape for grouping
+        if (group_size is not None) and channel_wise:
+            weight = weight.reshape([-1, group_size]) if (axis == 1) else weight.reshape([group_size, -1])
+
+        # Get min/max values
+        if channel_wise is False:
+            _min, _max = weight.min(), weight.max()
+            optimize = False
+        else:
+            _min = weight.min(axis=axis, keepdim=True)[0]
+            _max = weight.max(axis=axis, keepdim=True)[0]
+
+        max_v = 2**bits - 1
+        min_v = 0
+        min_max = [min_v, max_v]
+
+        # Note: here we work with the inverse of the scale to avoid division and quantize instead via weight*scale + zero, the scale is inverted later on.
+        # clamp to avoid half-precision problems
+        scale = (max_v / (_max - _min)).clamp(max=2e4)
+        #!!!!!!!!!!!!!!!
+        min_max_axis = _max - _min
+        if (min_max_axis == 0).sum().item() > 0:
+            min_max_axis[min_max_axis == 0] = max_v
+            scale = (max_v / min_max_axis).clamp(max=2e4)
+        zero = -_min * scale
+
+        if round_zero:
+            zero = torch.round(zero)
+
+        # Fine-tune weights
+        if optimize:
+            scale, zero = self.optimize_weights(tensor=weight, scale=scale, zero=zero, min_max=min_max, axis=axis)
+
+        # Quantize
+        # Necessary for fake quantization backprop
+        w_q = torch.round(weight * scale + zero).clamp(min_max[0], min_max[1])
+        w_q = w_q.reshape(shape).int()
+
+        scale = 1.0 / scale
+        if axis == 1:
+            scale = scale.reshape(shape[0], -1)
+            zero = zero.reshape(shape[0], -1)
+        else:
+            scale = scale.reshape(-1, shape[-1])
+            zero = zero.reshape(-1, shape[-1])
+        # cleanup
+        del weight, _min, _max
+
+        return w_q, scale.to(tensor.dtype), zero.to(tensor.dtype)
+
+    def quantize(self, node: NodeProto, graph_stack: list[GraphProto]):
+        """If the node is MatMul with fp32 const weight, quantize the weight with int4, and return the new node"""
+        if node.op_type != "MatMul":
+            return node  # only care about MatMul for now
+        import torch
+
+        logger.info(f"start to quantize {node.name} ...")
+        inputB = node.input[1]  # noqa: N806
+        b_pb, bs_graph = get_initializer(inputB, graph_stack)
+        if b_pb is None:
+            logger.info("MatMul doesn't have const weight. Skip to quantize")
+            return node  # only care about constant weight
+
+        b_array = onnx.numpy_helper.to_array(b_pb)
+        if len(b_array.shape) != 2:
+            logger.info("MatMul weight is not 2D. Skip to quantize")
+            return node  # can only process 2-D matrix
+        b_array_torch = torch.from_numpy(b_array)
+        if torch.cuda.is_available():
+            b_array_torch = b_array_torch.cuda()
+        quant_weight_torch, scales_torch, zero_points_torch = self.quantize_internal(
+            b_array_torch.T, bits=self.config.bits, group_size=self.config.block_size
+        )
+        quant_weight_torch = quant_weight_torch.contiguous()
+        scales_torch = scales_torch.contiguous()
+        zero_points_torch = zero_points_torch.contiguous()
+
+        packed_torch = torch.zeros(
+            (quant_weight_torch.shape[0], quant_weight_torch.shape[1] // 2),
+            dtype=torch.uint8,
+            device=quant_weight_torch.device,
+        )
+        self.pack_on_row_fast_248bit(packed_torch, quant_weight_torch, self.config.bits)
+        scales = scales_torch.cpu().numpy()
+        zero_points = zero_points_torch.cpu().numpy()
+        # reshape to the predefined shape in MatmulNbits
+        scales = scales.reshape(-1)
+        zero_points = zero_points.reshape(-1)
+        rows, cols = b_array_torch.shape
+        block_size = self.config.block_size
+        blob_size = block_size // 2
+        k_blocks = (rows + block_size - 1) // block_size
+        packed_torch = packed_torch.reshape(cols, k_blocks, blob_size)
+
+        b_quant = onnx.numpy_helper.from_array(packed_torch.cpu().numpy())
+        b_quant.name = b_pb.name + "_Q4"
+        for input in bs_graph.input:
+            if input.name == inputB:
+                bs_graph.input.remove(input)
+                break
+
+        scales_tensor = onnx.numpy_helper.from_array(scales)
+        scales_tensor.name = b_pb.name + "_scales"
+        bs_graph.initializer.extend([b_quant, scales_tensor])
+
+        input_names = [node.input[0], b_quant.name, scales_tensor.name]
+        zp_tensor = onnx.numpy_helper.from_array(zero_points)
+        zp_tensor.name = b_pb.name + "_zero_points"
+        bs_graph.initializer.extend([zp_tensor])
+        input_names.append(zp_tensor.name)
+
+        kwargs = {}
+        rows, cols = b_array.shape
+        kwargs["K"] = rows
+        kwargs["N"] = cols
+        kwargs["bits"] = self.config.bits
+        kwargs["block_size"] = self.config.block_size
+
+        matmul_q4_node = onnx.helper.make_node(
+            "MatMulNBits",
+            inputs=input_names,
+            outputs=[node.output[0]],
+            name=node.name + "_Q4" if node.name else "",
+            domain="com.microsoft",
+            **kwargs,
+        )
+
+        logger.info(f"complete quantization of {node.name} ...")
+
+        return matmul_q4_node
+
+
+def get_initializer(name, graph_path: list[GraphProto]) -> tuple[TensorProto, GraphProto]:
+    for gid in range(len(graph_path) - 1, -1, -1):
+        graph = graph_path[gid]
+        for tensor in graph.initializer:
+            if tensor.name == name:
+                return tensor, graph
+    return None, None
+
+
+class DefaultWeightOnlyQuantizer:
+    def __init__(self, config: DefaultWeightOnlyQuantConfig):
+        self.config = config
 
     def int4_block_quant(self, fp32weight: npt.ArrayLike) -> np.ndarray:
         """4b quantize fp32 weight to a blob"""
 
         if len(fp32weight.shape) != 2:
             raise ValueError("Current int4 block quantization only supports 2D tensors!")
         rows, cols = fp32weight.shape
 
-        block_size = self.block_size
+        block_size = self.config.block_size
         blob_size = block_size // 2
         k_blocks = (rows + block_size - 1) // block_size
         padded_rows = k_blocks * block_size
         pad_len = padded_rows - rows
         if pad_len > 0:
             fp32weight = np.pad(fp32weight, ((0, pad_len), (0, 0)), "constant")
 
         # block wise quantization, each block comes from a single column
         packed = np.zeros((cols, k_blocks, blob_size), dtype="uint8")
         scales = np.zeros((cols * k_blocks), dtype=fp32weight.dtype)
         zero_point = np.zeros(cols * ((k_blocks + 1) // 2), dtype="uint8")
-        quantize_matmul_4bits(packed, fp32weight, scales, zero_point, block_size, cols, rows, self.is_symmetric)
+        quantize_matmul_4bits(packed, fp32weight, scales, zero_point, block_size, cols, rows, self.config.is_symmetric)
 
         return (packed, scales, zero_point)
 
-    def _q4_matmul_node_weight(self, node: NodeProto, graph_stack: list[GraphProto]) -> NodeProto:
+    def quantize(self, node: NodeProto, graph_stack: list[GraphProto]) -> NodeProto:
         """If the node is MatMul with fp32 const weight, quantize the weight with int4, and return the new node"""
 
         if node.op_type != "MatMul":
             return node  # only care about MatMul for now
 
         logger.info(f"start to quantize {node.name} ...")
-        if node.name in self.nodes_to_exclude:
-            logger.info(f"exclude to quantize {node.name} as specified by nodes_to_exclude...")
-            return node
-
         inputB = node.input[1]  # noqa: N806
-        B, Bs_graph = MatMul4BitsQuantizer.__get_initializer(inputB, graph_stack)  # noqa: N806
+        B, Bs_graph = get_initializer(inputB, graph_stack)  # noqa: N806
         if B is None:
             logger.info("MatMul doesn't have const weight. Skip to quantize")
             return node  # only care about constant weight
 
         B_array = onnx.numpy_helper.to_array(B)  # noqa: N806
         if len(B_array.shape) != 2:
             logger.info("MatMul weight is not 2D. Skip to quantize")
@@ -184,42 +432,74 @@
                 break
 
         scales_tensor = onnx.numpy_helper.from_array(scales)
         scales_tensor.name = B.name + "_scales"
         Bs_graph.initializer.extend([B_quant, scales_tensor])
 
         input_names = [node.input[0], B_quant.name, scales_tensor.name]
-        if not self.is_symmetric:
+        if not self.config.is_symmetric:
             zp_tensor = onnx.numpy_helper.from_array(zero_points)
             zp_tensor.name = B.name + "_zero_points"
             Bs_graph.initializer.extend([zp_tensor])
             input_names.append(zp_tensor.name)
 
         kwargs = {}
         rows, cols = B_array.shape
         kwargs["K"] = rows
         kwargs["N"] = cols
         kwargs["bits"] = 4
-        kwargs["block_size"] = self.block_size
-        if self.accuracy_level is not None:
-            kwargs["accuracy_level"] = self.accuracy_level
+        kwargs["block_size"] = self.config.block_size
+        if self.config.accuracy_level is not None:
+            kwargs["accuracy_level"] = self.config.accuracy_level
 
         matmul_q4_node = onnx.helper.make_node(
             "MatMulNBits",
             inputs=input_names,
             outputs=[node.output[0]],
             name=node.name + "_Q4" if node.name else "",
             domain="com.microsoft",
             **kwargs,
         )
 
         logger.info(f"complete quantization of {node.name} ...")
 
         return matmul_q4_node
 
+
+class MatMul4BitsQuantizer:
+    """Perform 4b quantization of constant MatMul weights"""
+
+    def __init__(
+        self,
+        model: ModelProto | str,
+        block_size: int = 128,
+        is_symmetric: bool = False,
+        accuracy_level: int | None = None,
+        nodes_to_exclude=None,
+        algo_config: WeightOnlyQuantConfig = None,
+    ):
+        if nodes_to_exclude is None:
+            nodes_to_exclude = []
+        self.model = ONNXModel(onnx.load(model)) if isinstance(model, str) else ONNXModel(model)
+        self.model_path = model if isinstance(model, str) else None
+        self.block_size = block_size
+        self.is_symmetric = is_symmetric
+        self.accuracy_level = accuracy_level
+        self.nodes_to_exclude = set(nodes_to_exclude)
+        self.node_quantizer = None
+        if algo_config is None:
+            algo_config = DefaultWeightOnlyQuantConfig(
+                block_size=block_size, is_symmetric=is_symmetric, accuracy_level=accuracy_level
+            )
+        self.algo_config = algo_config
+        if algo_config.algorithm == "HQQ":
+            self.node_quantizer = HQQWeightOnlyQuantizer(self.algo_config)
+        elif algo_config.algorithm == "DEFAULT":
+            self.node_quantizer = DefaultWeightOnlyQuantizer(self.algo_config)
+
     def _process_subgraph(self, graph_stack: list[GraphProto]):
         new_nodes = []
         graph = graph_stack[-1]
 
         for node in graph.node:
             graph_attrs = [
                 attr
@@ -242,16 +522,23 @@
                         kv = {attr.name: value}
                     else:
                         kv = attribute_to_kwarg(attr)
                     kwargs.update(kv)
                 node = onnx.helper.make_node(  # noqa: PLW2901
                     node.op_type, node.input, node.output, name=node.name, **kwargs
                 )
-
-            new_nodes.append(self._q4_matmul_node_weight(node, graph_stack))
+            out_node = None
+            if node.name in self.nodes_to_exclude:
+                logger.info(f"exclude to quantize {node.name} as specified by nodes_to_exclude...")
+                out_node = node
+            elif self.algo_config is not None and self.algo_config.algorithm == "HQQ":
+                out_node = self.node_quantizer.quantize(node, graph_stack)
+            else:
+                out_node = self.node_quantizer.quantize(node, graph_stack)
+            new_nodes.append(out_node)
 
         graph.ClearField("node")
         graph.node.extend(new_nodes)
         graph_stack.pop()
         return graph
 
     def _generate_q4_node_config(self):
@@ -296,15 +583,15 @@
                 weight_config=weight_only_node_config,
                 **kwargs,
             )
         elif algorithm == "GPTQ":
             from neural_compressor.adaptor.ox_utils.weight_only import gptq_quantize
 
             kwargs["percdamp"] = self.algo_config.percdamp
-            kwargs["blocksize"] = self.algo_config.blocksize
+            kwargs["blocksize"] = self.algo_config.block_size
             kwargs["actorder"] = self.algo_config.actorder
             kwargs["mse"] = self.algo_config.mse
             kwargs["perchannel"] = self.algo_config.perchannel
             kwargs["n_samples"] = -1
             dataloader = inc_dataloader()
 
             self.model = gptq_quantize(
@@ -312,26 +599,25 @@
                 weight_config=weight_only_node_config,
                 dataloader=dataloader,
                 **kwargs,
             )
         logger.info(f"complete quantization of model with {algorithm} algorithm.")
 
     def process(self):
-        if self.algo_config is None:
+        if self.algo_config.algorithm in ["HQQ", "DEFAULT"]:
             # use a stack to keep track of sub-graphs
             graph_stack = [self.model.graph()]
             opset_import = self.model.opset_import()
 
             has_ms_domain = False
             for opset in opset_import:
                 if opset.domain == "com.microsoft":
                     has_ms_domain = True
             if not has_ms_domain:
                 opset_import.extend([onnx.helper.make_opsetid("com.microsoft", 1)])
-
             self._process_subgraph(graph_stack)
             self.model.clean_initializers()
         else:
             # use Intel® Neural Compressor for RTN or GPTQ weight-only quantize algorithm
             try:
                 importlib.import_module("neural_compressor")
             except Exception as e:
@@ -363,22 +649,30 @@
 """
     )
 
     parser.add_argument("--input_model", required=True, help="Path to the input model file")
     parser.add_argument("--output_model", required=True, help="Path to the output model file")
     parser.add_argument("--block_size", required=False, default=32, type=int, help="Block size for quantization")
     parser.add_argument(
+        "--quant_method",
+        default="default",
+        type=str,
+        choices=["default", "hqq", "rtn", "gptq"],
+        help="the algorithm used to quantize weight, \nrtn and gptq leverage Intel® Neural Compressor",
+    )
+    parser.add_argument("--bits", default=4, type=int, help="the target bits to represent weight")
+    parser.add_argument(
         "--symmetric",
         required=False,
         default=True,
         const=True,
         nargs="?",
         type=ort_convert_str_to_bool,
         choices=[True, False],
-        help="Indicate whether to quantize the model symmetrically",
+        help="Indicate whether to quantize the model symmetrically, symmetric is not supported by hqq",
     )
     parser.add_argument(
         "--accuracy_level",
         required=False,
         type=int,
         help="Accuracy level of the 4-bit quantized MatMul computation. "
         "Refer to the MatMulNBits contrib op's 'accuracy_level' attribute for details "
@@ -406,17 +700,33 @@
     input_model_path = args.input_model
     output_model_path = args.output_model
 
     if os.path.exists(output_model_path):
         logger.error(f"file {output_model_path} already exists")
         raise Exception(f"file {output_model_path} already exists")
 
+    if args.symmetric and args.quant_method == "hqq":
+        logger.warning("symmetric is not supportted by hqq, will force to symmetric=False")
+        args.symmetric = False
+
     model = onnx.load(input_model_path)
+    if args.quant_method == "hqq":
+        quant_config = HQQWeightOnlyQuantConfig(block_size=args.block_size, bits=args.bits)
+    elif args.quant_method == "default":
+        quant_config = DefaultWeightOnlyQuantConfig(
+            block_size=args.block_size, is_symmetric=args.symmetric, accuracy_level=args.accuracy_level
+        )
+    elif args.quant_method == "rtn":
+        quant_config = RTNWeightOnlyQuantConfig()
+    elif args.quant_method == "gptq":
+        quant_config = GPTQWeightOnlyQuantConfig(block_size=args.block_size)
+    else:
+        raise ValueError(f"Unsupported quantization method: {args.quant_method}")
+
     quant = MatMul4BitsQuantizer(
         model=model,
-        block_size=args.block_size,
-        is_symmetric=args.symmetric,
         accuracy_level=args.accuracy_level,
         nodes_to_exclude=args.nodes_to_exclude,
+        algo_config=quant_config,
     )
     quant.process()
     quant.model.save_model_to_file(output_model_path, True)
```

## onnxruntime/quantization/matmul_bnb4_quantizer.py

```diff
@@ -195,22 +195,22 @@
 
     parser.add_argument("--input_model", required=True, help="Path to the input model file")
     parser.add_argument("--output_model", required=True, help="Path to the output model file")
     parser.add_argument(
         "--quant_type",
         required=False,
         default=1,
-        options=[MatMulBnb4Quantizer.FP4, MatMulBnb4Quantizer.NF4],
+        choices=[MatMulBnb4Quantizer.FP4, MatMulBnb4Quantizer.NF4],
         help="Quantization data type. 0: FP4, 1: NF4",
     )
     parser.add_argument(
         "--block_size",
         required=False,
         default=64,
-        description="Block size for blockwise quantization. Note: bnb.nn.Linear4bit only uses block_size=64",
+        help="Block size for blockwise quantization. Note: bnb.nn.Linear4bit only uses block_size=64",
     )
     parser.add_argument("-v", "--verbose", required=False, action="store_true")
     parser.set_defaults(verbose=False)
     parser.add_argument(
         "--nodes_to_exclude",
         nargs="+",
         type=str,
```

## onnxruntime/quantization/onnx_model.py

```diff
@@ -75,19 +75,15 @@
     for initializer in unused_initializer:
         graph.initializer.remove(initializer)
         if initializer.name in name_to_input:
             try:
                 graph.input.remove(name_to_input[initializer.name])
             except StopIteration:
                 if model.ir_version < 4:
-                    print(
-                        "Warning: invalid weight name {} found in the graph (not a graph input)".format(
-                            initializer.name
-                        )
-                    )
+                    print(f"Warning: invalid weight name {initializer.name} found in the graph (not a graph input)")
 
     requesting_tensor_names.difference_update(input.name for input in graph.input)
 
     return graph, requesting_tensor_names
 
 
 class ONNXModel:
@@ -279,14 +275,31 @@
             The node found or None.
         """
         graph_nodes_list = list(graph.node)  # deep copy
         graph_nodes_list.extend(new_nodes_list)
         node = find_by_name(node_name, graph_nodes_list)
         return node
 
+    def get_largest_node_name_suffix(self, node_name_prefix):
+        """
+        Gets the largest node name (int) suffix for all node names that begin with `node_name_prefix`.
+        Example: for nodes my_prefix_0 and my_prefix_3, this method returns 3.
+        """
+        suffix = -1
+
+        for node in self.model.graph.node:
+            if node.name and node.name.startswith(node_name_prefix):
+                try:
+                    index = int(node.name[len(node_name_prefix) :])
+                    suffix = max(index, suffix)
+                except ValueError:
+                    continue
+
+        return suffix
+
     def find_nodes_by_initializer(self, graph, initializer):
         """
         Find all nodes with given initializer as an input.
         """
         nodes = []
         for node in graph.node:
             for node_input in node.input:
@@ -424,25 +437,35 @@
             if node.input[j] == old_input_name:
                 node.input[j] = new_input_name
 
     def replace_input_of_all_nodes(self, old_input_name, new_input_name):
         for node in self.model.graph.node:
             ONNXModel.replace_node_input(node, old_input_name, new_input_name)
 
+    def replace_input_of_nodes(self, old_input_name, new_input_name, node_names_set):
+        for node in self.model.graph.node:
+            if node.name in node_names_set:
+                ONNXModel.replace_node_input(node, old_input_name, new_input_name)
+
     @staticmethod
     def replace_node_output(node, old_output_name, new_output_name):
         assert isinstance(old_output_name, str) and isinstance(new_output_name, str)
         for j in range(len(node.output)):
             if node.output[j] == old_output_name:
                 node.output[j] = new_output_name
 
     def replace_output_of_all_nodes(self, old_output_name, new_output_name):
         for node in self.model.graph.node:
             ONNXModel.replace_node_output(node, old_output_name, new_output_name)
 
+    def replace_output_of_nodes(self, old_output_name, new_output_name, node_names_set):
+        for node in self.model.graph.node:
+            if node.name in node_names_set:
+                ONNXModel.replace_node_output(node, old_output_name, new_output_name)
+
     def remove_unused_constant(self):
         input_name_to_nodes = self.input_name_to_nodes()
 
         # remove unused constant
         unused_nodes = []
         nodes = self.nodes()
         for node in nodes:
```

## onnxruntime/quantization/onnx_quantizer.py

```diff
@@ -1,77 +1,44 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import logging
-from typing import Any, Dict
 
 import numpy as np
 import onnx
 import onnx.numpy_helper
 from onnx import onnx_pb as onnx_proto
 
-try:
-    from onnx.reference.op_run import to_array_extended
-except ImportError:
-    # old version of onnx.
-    to_array_extended = None
-
+from .base_quantizer import BaseQuantizer, QuantizationParams
 from .calibrate import TensorData
 from .onnx_model import ONNXModel
 from .quant_utils import (
-    ONNX_TYPE_TO_NP_TYPE,
     TENSOR_NAME_QUANT_SUFFIX,
     QuantizationMode,
     QuantizedValue,
     QuantizedValueType,
-    QuantType,
     __producer__,
     __version__,
     add_infer_metadata,
     attribute_to_kwarg,
     compute_scale_zp,
     compute_scale_zp_float8,
     find_by_name,
     get_qmin_qmax_for_qType,
     get_qrange_for_qType,
-    model_has_infer_metadata,
     ms_domain,
-    quantize_data,
-    quantize_nparray,
     save_and_reload_model_with_shape_infer,
     tensor_proto_to_array,
 )
 from .registry import CreateOpQuantizer
 
 
-class QuantizationParams:
-    def __init__(self, **data: Dict[str, Any]):
-        self.data = {}
-        for k, v in data.items():
-            if not isinstance(k, str):
-                raise TypeError(f"Keys must be strings not {type(k)} for k={k!r}.")
-            if not isinstance(v, (int, str, np.ndarray)):
-                raise TypeError(f"Values must be numpy arrays, int, float, str not {type(v)} for k={k!r}.")
-            if k == "scale" and v.dtype not in (np.float32, np.float16):
-                raise ValueError(f"scale must a float32 or float16 numpy element but is {v.dtype} for k={k!r}")
-            self.data[k] = v
-
-    def __iter__(self):
-        yield from self.data
-
-    def __getitem__(self, key):
-        return self.data[key]
-
-    def __len__(self):
-        return len(self.data)
-
-
-class ONNXQuantizer:
+class ONNXQuantizer(BaseQuantizer):
     def __init__(
         self,
         model,
         per_channel,
         reduce_range,
         mode,
         static,
@@ -79,90 +46,54 @@
         activation_qType,
         tensors_range,
         nodes_to_quantize,
         nodes_to_exclude,
         op_types_to_quantize,
         extra_options=None,
     ):
-        if not model_has_infer_metadata(model):
-            model = save_and_reload_model_with_shape_infer(model)
-        self.value_infos = {vi.name: vi for vi in model.graph.value_info}
-        self.value_infos.update({ot.name: ot for ot in model.graph.output})
-        self.value_infos.update({it.name: it for it in model.graph.input})
+        BaseQuantizer.__init__(
+            self,
+            model,
+            per_channel,
+            reduce_range,
+            weight_qType,
+            activation_qType,
+            tensors_range,
+            nodes_to_quantize,
+            nodes_to_exclude,
+            op_types_to_quantize,
+            extra_options,
+        )
 
-        self.model = ONNXModel(model)
         if not static:
             self.model.replace_gemm_with_matmul()
             # We need to update value_infos.
             model = save_and_reload_model_with_shape_infer(self.model.model)
             self.value_infos = {vi.name: vi for vi in model.graph.value_info}
             self.value_infos.update({ot.name: ot for ot in model.graph.output})
             self.value_infos.update({it.name: it for it in model.graph.input})
             self.model = ONNXModel(model)
 
-        self.per_channel = per_channel  # weight-pack per channel
-        self.reduce_range = reduce_range
         self.mode = mode  # QuantizationMode.Value
         self.static = static  # use static quantization for inputs.
-        self.fuse_dynamic_quant = False
+        self.fuse_dynamic_quant = self.opset_version > 10
 
-        self.extra_options = extra_options if extra_options else {}
-        self.enable_subgraph_quantization = (
-            "EnableSubgraph" in self.extra_options and self.extra_options["EnableSubgraph"]
-        )
-        self.force_quantize_no_input_check = (
-            "ForceQuantizeNoInputCheck" in self.extra_options and self.extra_options["ForceQuantizeNoInputCheck"]
-        )
         self.q_matmul_const_b_only = "MatMulConstBOnly" in self.extra_options and self.extra_options["MatMulConstBOnly"]
-        self.is_weight_symmetric = (
-            weight_qType in (QuantType.QInt8, QuantType.QInt16, QuantType.QFLOAT8E4M3FN)
-            if "WeightSymmetric" not in self.extra_options
-            else self.extra_options["WeightSymmetric"]
-        )
-        self.is_activation_symmetric = (
-            False if "ActivationSymmetric" not in self.extra_options else self.extra_options["ActivationSymmetric"]
-        )
-        self.min_real_range = self.extra_options.get("MinimumRealRange")
-
-        self.activation_qType = getattr(activation_qType, "tensor_type", activation_qType)
-        self.weight_qType = getattr(weight_qType, "tensor_type", weight_qType)
-        """
-            Dictionary specifying the min and max values for tensors. It has following format:
-                {
-                    "param_name": [min, max]
-                }
-            example:
-                {
-                    'Conv_3:0': [np.float32(0), np.float32(0.5)],
-                    'Conv_4:0': [np.float32(1), np.float32(3.5)]
-                }
-        """
-        if tensors_range is not None and any(map(lambda t: not isinstance(t, TensorData), tensors_range.values())):
-            raise TypeError(
-                f"tensors_range contains unexpected types {set(type(v) for v in tensors_range.values())}, not TensorData."
-            )
-        self.tensors_range = tensors_range
-        self.nodes_to_quantize = nodes_to_quantize  # specific nodes to quantize
-        self.nodes_to_exclude = nodes_to_exclude  # specific nodes to exclude
-        self.op_types_to_quantize = op_types_to_quantize
+
         self.new_nodes = []
-        self.parent = None
         self.graph_scope = "/"  # for human readable debug information
         self.tensor_names = {}  # in case the shape inference not totally working
         self.tensor_names.update({ot.name: 1 for ot in model.graph.output})
         self.tensor_names.update({it.name: 1 for it in model.graph.input})
         for node in self.model.model.graph.node:
             self.tensor_names.update({output_name: 1 for output_name in node.output})
 
-        self.opset_version = self.check_opset_version()
-
         if self.mode not in QuantizationMode:
             raise ValueError(f"unsupported quantization mode {self.mode}")
 
-        self.tensor_quant_overrides = self._get_and_check_tensor_quant_overrides()
         self.quantization_params = self.calculate_quantization_params()
 
         # QuantizeRange tensor name and zero tensor name for scale and zero point calculation.
         # Used when static is False
         self.fixed_qrange_uint8_name = "fixed_quantization_range_uint8"
         self.fixed_qrange_int8_name = "fixed_quantization_range_int8"
         # For uint8 data-type, to compute zero point, we subtract rmin from 0 (represented by fixed_zero_name tensor)
@@ -171,97 +102,14 @@
         self.fixed_zero_zp_name = "fixed_zero_zp"
 
         # Map of all original value names to quantized value names
         self.quantized_value_map = {}
         # some output from nodes will be quantized, yet itself should be treat as existing so
         # no dequantized will be applied when needed later
         self.generated_value_names = self.model.get_non_initializer_inputs()
-        # to store specified scale and zeropoint instead of calculated value, tensor_name->(scale, zeropoint)
-        self.used_scale_zp_map = {}
-
-    def _get_and_check_tensor_quant_overrides(self):
-        """
-        Get tensor quantization overrides and check correctness.
-        """
-        tensor_quant_overrides = self.extra_options.get("TensorQuantOverrides", {})
-
-        # Validate that compatible/valid overrides are provided.
-        if tensor_quant_overrides:
-            initializer_names = self.model.get_initializer_name_set()
-            value_info_names = set(self.value_infos.keys())
-            keys_unsupported_with_scale_zp = {"symmetric", "reduce_range", "rmax", "rmin"}
-
-            for tensor_name, quant_overrides_list in tensor_quant_overrides.items():
-                if tensor_name not in initializer_names and tensor_name not in value_info_names:
-                    raise ValueError(f"Tensor '{tensor_name}' in TensorQuantOverrides is not present in the model")
-
-                if not isinstance(quant_overrides_list, list):
-                    raise ValueError(f"Tensor quantization overrides for '{tensor_name}' are not in a list")
-
-                is_initializer = tensor_name in initializer_names
-                if not is_initializer and len(quant_overrides_list) > 1:
-                    raise ValueError(
-                        f"Tensor '{tensor_name}' has a list of per-channel overrides, but is not an initializer"
-                    )
-
-                quant_type = None
-                for index, quant_overrides in enumerate(quant_overrides_list):
-                    if not isinstance(quant_overrides, dict):
-                        raise ValueError(
-                            f"Tensor quantization overrides at index {index} for '{tensor_name}' are not in a dict"
-                        )
-
-                    # For per-channel quantization, all channels must use the same quantization type.
-                    # Therefore, if the user tries to override the quant_type for a channel, it must match in all
-                    # other channels.
-                    if index == 0:
-                        quant_type = quant_overrides.get("quant_type")
-                    elif quant_type != quant_overrides.get("quant_type"):
-                        raise ValueError(
-                            "Channel quantization types for tensor '{tensor_name}' do not match at index {index}."
-                        )
-
-                    has_scale = "scale" in quant_overrides
-                    has_zero_point = "zero_point" in quant_overrides
-
-                    if (has_scale and not has_zero_point) or (has_zero_point and not has_scale):
-                        raise ValueError(
-                            "Must provide both 'scale' and 'zero_point' if one of the overrides is provided"
-                        )
-
-                    if has_scale:
-                        for key in keys_unsupported_with_scale_zp:
-                            if key in quant_overrides:
-                                raise ValueError(
-                                    f"Tensor override option '{key}' is invalid with 'scale' and 'zero_point'"
-                                )
-
-        return tensor_quant_overrides
-
-    def get_per_tensor_quant_overrides(self, tensor_name):
-        quant_overrides_list = self.tensor_quant_overrides.get(tensor_name, [{}])
-        num_overrides = len(quant_overrides_list)
-        if num_overrides > 1:
-            raise ValueError(
-                f"Expected tensor '{tensor_name}' to use per-tensor quantization overrides, "
-                f"but found {num_overrides} per-channel overrides."
-            )
-
-        return quant_overrides_list[0] if num_overrides > 0 else {}
-
-    def get_per_channel_quant_overrides(self, tensor_name, num_channels):
-        quant_overrides_list = self.tensor_quant_overrides.get(tensor_name, [{} for i in range(num_channels)])
-
-        if len(quant_overrides_list) != num_channels:
-            raise ValueError(
-                f"Expected tensor '{tensor_name}' to have {num_channels} per-channel quantization overrides, "
-                f"but found {len(quant_overrides_list)} instead."
-            )
-
-        return quant_overrides_list
 
     # routines for subgraph support
     def quantize_subgraph(self, subgraph, graph_key):
         """
         generate submodel for the subgraph, so that we re-utilize current quantization implementation.
         quantize the submodel
         update subgraph and set it back to node
@@ -321,54 +169,14 @@
                     )
                 kv = {attr.name: value}
             else:
                 kv = attribute_to_kwarg(attr)
             kwargs.update(kv)
         return onnx.helper.make_node(node.op_type, node.input, node.output, name=node.name, **kwargs)
 
-    def check_opset_version(self):
-        ai_onnx_domain = [
-            opset for opset in self.model.model.opset_import if not opset.domain or opset.domain == "ai.onnx"
-        ]
-        if len(ai_onnx_domain) != 1:
-            raise ValueError("Failed to find proper ai.onnx domain")
-        opset_version = ai_onnx_domain[0].version
-
-        if opset_version == 10:
-            logging.warning(
-                "The original model opset version is {}, which does not support node fusions. Please update the model to opset >= 11 for better performance.".format(
-                    opset_version
-                )
-            )
-            return 10
-
-        if opset_version < 10:
-            logging.warning(
-                "The original model opset version is {}, which does not support quantization. Please update the model to opset >= 11. Updating the model automatically to opset 11. Please verify the quantized model.".format(
-                    opset_version
-                )
-            )
-            self.model.model.opset_import.remove(ai_onnx_domain[0])
-            self.model.model.opset_import.extend([onnx.helper.make_opsetid("", 11)])
-            opset_version = 11
-
-        if opset_version < 19 and self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-            logging.warning(
-                "The original model opset version is {}, which does not support quantization to float 8. "
-                "Please update the model to opset >= 19. Updating the model automatically to opset 19. "
-                "Please verify the quantized model.".format(opset_version)
-            )
-            self.model.model.opset_import.remove(ai_onnx_domain[0])
-            self.model.model.opset_import.extend([onnx.helper.make_opsetid("", 19)])
-            self.model.model.ir_version = 9
-            opset_version = 19
-
-        self.fuse_dynamic_quant = True
-        return opset_version
-
     def has_QDQ_nodes(self):  # noqa: N802
         """
         Detect if model already has QuantizeLinear or DequantizeLinear.
         """
         return any(
             node.op_type == "QuantizeLinear" or node.op_type == "DequantizeLinear" for node in self.model.nodes()
         )
@@ -427,29 +235,14 @@
             if ms_nodes:
                 opset = self.model.model.opset_import.add()
                 opset.version = 1
                 opset.domain = ms_domain
 
         return self.model.model
 
-    def is_input_a_initializer(self, input_name):
-        initializer = find_by_name(input_name, self.model.initializer())
-        return initializer is not None
-
-    def is_per_channel(self):
-        return self.per_channel
-
-    def is_valid_quantize_weight(self, weight_name):
-        weight = find_by_name(weight_name, self.model.initializer())
-        if weight is not None:
-            return weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16)
-        if (not self.enable_subgraph_quantization) or (self.parent is None):
-            return False
-        return self.parent.is_valid_quantize_weight(weight_name)
-
     def _get_default_tensor_type(self, tensor_name):
         if "DefaultTensorType" in self.extra_options:
             logging.info(
                 "get_tensor_type returns DefaultTensorType for tensor name %r, use %d",
                 tensor_name,
                 self.extra_options["DefaultTensorType"],
             )
@@ -509,44 +302,27 @@
 
         logging.warning(
             f"Failed to infer data type of tensor: {tensor_name!r}. Please add data type info for this tensor "
             f"if your model has customized operators."
         )
         return False
 
-    def should_quantize_node(self, node):
-        if (
-            self.nodes_to_quantize is not None
-            and len(self.nodes_to_quantize) != 0
-            and node.name not in self.nodes_to_quantize
-        ):
-            return False
-
-        if node.op_type not in self.op_types_to_quantize:
-            return False
-
-        if self.nodes_to_exclude is not None and node.name in self.nodes_to_exclude:
-            return False
-
-        return True
-
-    def _get_dynamic_input_quantization_params(self, input_name, nodes_list, qType):
+    def _get_dynamic_input_quantization_params(self, input_name, nodes_list, qType, initial_type):
         """
         Create nodes for dynamic quantization of input and add them to nodes_list.
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
             parameter qType: type to quantize to.
+            parameter initial_type: type to quantize from
             return: scale_name, zero_point_name, scale_shape, zero_point_shape.
         """
         if qType == onnx_proto.TensorProto.INT8:
-            return self._get_dynamic_input_quantization_params_int8(input_name, nodes_list)
+            return self._get_dynamic_input_quantization_params_int8(input_name, nodes_list, initial_type)
         if qType == onnx_proto.TensorProto.UINT8:
-            return self._get_dynamic_input_quantization_params_uint8(input_name, nodes_list)
-        if qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-            return self._get_dynamic_input_quantization_params_float8e4m3fn(input_name, nodes_list)
+            return self._get_dynamic_input_quantization_params_uint8(input_name, nodes_list, initial_type)
         raise ValueError(f"Unexpected value for qType={qType}.")
 
     def _get_dynamic_input_quantization_params_int8(self, input_name, nodes_list, initial_type):
         """
         Create nodes for dynamic quantization of input to int8 and add them to nodes_list
             parameter input_name: Name of the input.
             parameter nodes_list: new nodes are appended to this list.
@@ -747,16 +523,15 @@
                 )
 
             zero_point_values = np.array([params["zero_point"]])
             if not hasattr(params["scale"], "dtype") or params["scale"].dtype not in (np.float32, np.float16):
                 raise ValueError(f"Unexpected type {type(params['scale'])} and param_name={param_name!r}")
             scale_values = np.array([params["scale"]])
             assert scale_values.dtype != np.float64
-            # zero_point_type = params["quant_type"]
-            assert zero_point_type == params["quant_type"]
+            zero_point_type = params["quant_type"]
         else:
             zero_point_values = np.array([use_zeropoint])
             scale_values = np.array([use_scale])
             params = self.quantization_params[param_name]
             if "scale" in params:
                 dtype = params["scale"].dtype
                 scale_values = scale_values.astype(dtype)
@@ -779,26 +554,29 @@
         else:
             raise ValueError(f"Unexpected dtype={scale_values.dtype} for param_name={param_name!r}")
         init_scale = onnx.helper.make_tensor(scale_name, scale_type, scale_shape, scale_values.reshape((-1,)).tolist())
         self.model.add_initializer(init_scale)
 
         return True, scale_name, zero_point_name, scale_shape, zero_point_shape
 
-    def _get_quantize_input_nodes(self, node, input_index, qType, given_scale_name=None, given_zp_name=None):
+    def _get_quantize_input_nodes(
+        self, node, input_index, qType, given_scale_name=None, given_zp_name=None, initial_type=None
+    ):
         """
         Given an input for a node (which is not a initializer), this function
 
         - add nodes to compute zero point and scale for this input if they don't exist.
         - add new QuantizeLinear node to quantize the input.
 
         :param node: node being quantized in NodeProto format.
         :param input_index: index of input in node.input.
         :param qType: type to quantize to.
         :param given_scale_name: if those inputs need to be quanitzed using this scale tensor.
         :param given_zp_name: if those inputs to be quantized using this zeropoint tensor.
+        :param initial_type: type of the weight to quantize
         :return: List of newly created nodes in NodeProto format.
         """
         input_name = node.input[input_index]
         assert input_name != "", "Cannot access undefined variable in graph."
         output_name = input_name + TENSOR_NAME_QUANT_SUFFIX
         ql_node_name = input_name + "_QuantizeLinear"
 
@@ -826,43 +604,34 @@
                 qlinear_node = onnx.helper.make_node(
                     "DynamicQuantizeLinear",
                     [input_name],
                     [output_name, scale_name, zp_name],
                     ql_node_name,
                 )
             else:
+                assert initial_type is not None, (
+                    f"Cannot quantize input without knowing the initial type, "
+                    f"input_name={input_name!r}, input_index={input_index}, qType={qType}, node={node}"
+                )
                 (
                     scale_name,
                     zp_name,
                     scale_shape,
                     zp_shape,
-                ) = self._get_dynamic_input_quantization_params(input_name, nodes, qType)
+                ) = self._get_dynamic_input_quantization_params(input_name, nodes, qType, initial_type=initial_type)
                 qlinear_node = onnx.helper.make_node(
                     "QuantizeLinear",
                     [input_name, scale_name, zp_name],
                     [output_name],
                     ql_node_name,
                 )
 
         self.quantized_value_map[input_name] = QuantizedValue(input_name, output_name, scale_name, zp_name, qType)
         return [*nodes, qlinear_node]
 
-    def set_quant_scale_zp(self, tensor_name, value):
-        assert isinstance(value, tuple) and len(value) == 2, "value must be scale(float or float16) and zeropoint"
-        assert hasattr(value[0], "dtype")
-        assert tensor_name not in self.used_scale_zp_map, f"{tensor_name} has been setted before"
-        self.used_scale_zp_map[tensor_name] = value
-
-    def find_quant_scale_zp(self, input_name):
-        if input_name in self.used_scale_zp_map:
-            return self.used_scale_zp_map[input_name]
-        if self.parent is not None:
-            return self.parent.find_quantized_value(input_name)
-        return (None, None)
-
     def find_quantized_value(self, input_name):
         if input_name in self.quantized_value_map:
             return self.quantized_value_map[input_name]
         if self.parent is not None:
             return self.parent.find_quantized_value(input_name)
         return None
 
@@ -876,80 +645,33 @@
             return self.quantized_value_map[bias_name].q_name
 
         # get scale for weight
         weight_scale_name = self.quantized_value_map[weight_name].scale_name
         weight_initializer = find_by_name(weight_scale_name, self.model.initializer())
         weight_scale = tensor_proto_to_array(weight_initializer)
 
-        # get bias
-        bias_initializer = find_by_name(bias_name, self.model.initializer())
-        bias_data = tensor_proto_to_array(bias_initializer)
-        quantized_bias_name = bias_name + TENSOR_NAME_QUANT_SUFFIX
-
         # get scale for input
         if input_name in self.quantized_value_map:
             input_scale_name = self.quantized_value_map[input_name].scale_name
         elif input_name in self.quantization_params:
             _, input_scale_name, _, _, _ = self._get_quantization_params(input_name)
         else:
             raise ValueError(f"Expected {input_name} to be in quantized value map for static quantization")
 
         inputscale_initializer = find_by_name(input_scale_name, self.model.initializer())
         input_scale = tensor_proto_to_array(inputscale_initializer)
 
-        # quantize bias
-        if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-            data = np.asarray(bias_data)
-            if data.dtype == np.float16:
-                node_qtype = onnx.TensorProto.FLOAT16
-            elif data.dtype == np.float32:
-                node_qtype = onnx.TensorProto.FLOAT
-            else:
-                raise TypeError(f"Only float16 or float32 are supported with float 8 but bias dtype is {data.dtype}.")
-            quantized_data = data.astype(np.float32)
-            bias_scale = np.array([1], dtype=quantized_data.dtype)
-            bias_scale_data = bias_scale.reshape(-1)
-            packed_bias_initializer = onnx.numpy_helper.from_array(quantized_data, quantized_bias_name)
-            self.model.initializer_extend([packed_bias_initializer])
-            node_type = "Cast"
-        else:
-            # calculate scale for bias
-            # TODO: This formula should be explained including why the scale is not estimated for the bias as well.
-            bias_scale = input_scale * weight_scale * beta
-
-            quantized_data = (np.asarray(bias_data) / bias_scale).round().astype(np.int32)
-
-            # update bias initializer
-            bias_np_data = np.asarray(quantized_data, dtype=np.int32).reshape(bias_initializer.dims)
-            packed_bias_initializer = onnx.numpy_helper.from_array(bias_np_data, quantized_bias_name)
-            self.model.initializer_extend([packed_bias_initializer])
-            bias_scale_data = np.asarray(bias_scale, dtype=np.float32).reshape(-1)
-            node_type = "DequantizeLinear"
-            node_qtype = self.weight_qType
-
-        # update scale initializer
-        quantized_bias_scale_name = quantized_bias_name + "_scale"
-        packed_bias_scale_initializer = onnx.numpy_helper.from_array(bias_scale_data, quantized_bias_scale_name)
-        self.model.initializer_extend([packed_bias_scale_initializer])
-
-        # update zero initializer
-        if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-            tensor_type = self.weight_qType
-        else:
-            tensor_type = onnx_proto.TensorProto.INT32
-
-        quantized_bias_zp_name = quantized_bias_name + "_zero_point"
-        if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-            packed_bias_zp_initializer = onnx.helper.make_tensor(quantized_bias_zp_name, self.weight_qType, [1], [0.0])
-        elif self.is_per_channel():
-            bias_zp_data = np.zeros(bias_scale.shape, dtype=np.int32).reshape(-1)
-            packed_bias_zp_initializer = onnx.numpy_helper.from_array(bias_zp_data, quantized_bias_zp_name)
-        else:
-            packed_bias_zp_initializer = onnx.helper.make_tensor(quantized_bias_zp_name, tensor_type, [], [0])
-        self.model.initializer_extend([packed_bias_zp_initializer])
+        (
+            quantized_bias_name,
+            quantized_bias_scale_name,
+            quantized_bias_zp_name,
+            bias_scale_data,
+            node_type,
+            node_qtype,
+        ) = self.quantize_bias_static_impl(bias_name, input_scale, weight_scale, beta)
 
         assert bias_name not in self.quantized_value_map
         quantized_value = QuantizedValue(
             bias_name,
             quantized_bias_name,
             quantized_bias_scale_name,
             quantized_bias_zp_name,
@@ -1074,15 +796,31 @@
                 scale_names.append(scale_name)
             elif self.contains_tensor(node_input):
                 # Add QuantizeLinear node.
                 qlinear_node = self.model.find_node_by_name(
                     node_input + "_QuantizeLinear", self.new_nodes, self.model.graph()
                 )
                 if qlinear_node is None:
-                    quantize_input_nodes = self._get_quantize_input_nodes(node, input_index, self.activation_qType)
+                    input_name = node.input[input_index]
+                    if input_name in self.value_infos:
+                        value_info = self.value_infos[input_name]
+                        assert value_info.HasField("type"), f"value_info={value_info} has no type."
+                        assert value_info.type.HasField("tensor_type"), f"value_info={value_info} is not a tensor."
+                        initial_type = value_info.type.tensor_type.elem_type
+                    else:
+                        # Shape inference failed. Fallback to self.tensor_names.
+                        assert input_name in self.tensor_names, (
+                            f"shape inference failed for {input_name!r} and "
+                            f"attribute 'tensor_names' does not have any value for "
+                            f"this tensor."
+                        )
+                        initial_type = self.tensor_names[input_name]
+                    quantize_input_nodes = self._get_quantize_input_nodes(
+                        node, input_index, self.activation_qType, initial_type=initial_type
+                    )
                     if quantize_input_nodes is None:
                         return (None, None, None, None)
                     if from_subgraph:
                         self.add_new_nodes(quantize_input_nodes)
                     else:
                         nodes.extend(quantize_input_nodes)
                     qlinear_node = quantize_input_nodes[-1]
@@ -1132,80 +870,17 @@
             quantized_value = self.quantized_value_map[weight.name]
             return (
                 quantized_value.q_name,
                 quantized_value.zp_name,
                 quantized_value.scale_name,
             )
 
-        q_weight_name = weight.name + TENSOR_NAME_QUANT_SUFFIX
-        zp_name = weight.name + "_zero_point"
-        scale_name = weight.name + "_scale"
-
-        # Quantize weight data. Use quantization overrides if provided by the user.
-        weight_data = tensor_proto_to_array(weight)
-        quant_overrides = self.get_per_tensor_quant_overrides(weight.name)
-        if "quant_type" in quant_overrides:
-            qType = quant_overrides["quant_type"].tensor_type  # noqa: N806
-
-        if "scale" in quant_overrides and "zero_point" in quant_overrides:
-            zero_point = np.array(quant_overrides["zero_point"], dtype=ONNX_TYPE_TO_NP_TYPE[qType])
-            scale = np.array(quant_overrides["scale"])
-            q_weight_data = quantize_nparray(qType, weight_data.flatten(), scale, zero_point)
-            assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
-            assert (
-                zero_point.dtype != np.float32 and zero_point.dtype != np.float16
-            ), f"Unexpected dtype {zero_point.dtype}"
-            assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
-
-        else:
-            _, _, zero_point, scale, q_weight_data = quantize_data(
-                weight_data.flatten(),
-                qType,
-                quant_overrides.get("symmetric", self.is_weight_symmetric),
-                reduce_range=quant_overrides.get("reduce_range", self.reduce_range and reduce_range),
-                min_real_range=self.min_real_range,
-                rmin_override=quant_overrides.get("rmin"),
-                rmax_override=quant_overrides.get("rmax"),
-            )
-
-            assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
-            assert (
-                zero_point.dtype != np.float32 and zero_point.dtype != np.float16
-            ), f"Unexpected dtype {zero_point.dtype}"
-            assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
-
-        scale_dtype = weight.data_type
-        scale_initializer = onnx.helper.make_tensor(scale_name, scale_dtype, [], scale.reshape((-1,)).tolist())
-        zero_initializer = onnx.helper.make_tensor(zp_name, qType, [], zero_point.reshape((-1,)).tolist())
-        self.model.initializer_extend([scale_initializer, zero_initializer])
-
-        if not keep_float_weight:
-            if self.weight_qType == onnx_proto.TensorProto.FLOAT8E4M3FN:
-                q_weight_initializer = onnx.TensorProto()
-                q_weight_initializer.data_type = self.weight_qType
-                q_weight_initializer.dims.extend(weight.dims)
-                q_weight_initializer.name = q_weight_name
-                # Do not remove .flatten().copy() numpy is not clear about data persistence.
-                q_weight_initializer.raw_data = q_weight_data.flatten().copy().tobytes()
-                if to_array_extended is not None:
-                    # This test should not be needed but it helped catch some issues
-                    # with data persistence and tobytes.
-                    check = to_array_extended(q_weight_initializer)
-                    if check.shape != weight_data.shape or check.tobytes() != q_weight_data.tobytes():
-                        raise RuntimeError(
-                            f"The initializer of shape {weight_data.shape} could not be created, expecting "
-                            f"{q_weight_data.tobytes()[:10]}, got {check.tobytes()[:10]} and shape={weight.shape}"
-                            f"\nraw={str(q_weight_initializer)[:200]}."
-                        )
-            else:
-                q_weight_data = np.asarray(q_weight_data, dtype=onnx.helper.tensor_dtype_to_np_dtype(qType)).reshape(
-                    weight.dims
-                )
-                q_weight_initializer = onnx.numpy_helper.from_array(q_weight_data, q_weight_name)
-            self.model.initializer_extend([q_weight_initializer])
+        q_weight_name, zp_name, scale_name = self.quantize_initializer_impl(
+            weight, qType, reduce_range, keep_float_weight
+        )
 
         # Log entry for this quantized weight
         quantized_value = QuantizedValue(
             weight.name,
             q_weight_name,
             scale_name,
             zp_name,
@@ -1228,121 +903,27 @@
             quantized_value = self.quantized_value_map[weight_name]
             return (
                 quantized_value.q_name,
                 quantized_value.zp_name,
                 quantized_value.scale_name,
             )
 
-        initializer = find_by_name(weight_name, self.model.initializer())
-        if initializer is None:
-            raise ValueError("{} is not an initializer", weight_name)
-
-        weights = tensor_proto_to_array(initializer)
-        channel_count = weights.shape[channel_axis]
-        quant_overrides_for_channels = self.get_per_channel_quant_overrides(weight_name, channel_count)
-
-        # If user provides per-channel quantization overrides, all channels must use the same quantization type.
-        # So, just use the first channel's type.
-        if "quant_type" in quant_overrides_for_channels[0]:
-            weight_qType = quant_overrides_for_channels[0]["quant_type"].tensor_type  # noqa: N806
-
-        zero_point_list = []
-        scale_list = []
-        quantized_per_channel_data_list = []
-        for i in range(channel_count):
-            per_channel_data = weights.take(i, channel_axis)
-            channel_quant_overrides = quant_overrides_for_channels[i]
-
-            if "scale" in channel_quant_overrides and "zero_point" in channel_quant_overrides:
-                zero_point = np.array(channel_quant_overrides["zero_point"], dtype=ONNX_TYPE_TO_NP_TYPE[weight_qType])
-                scale = np.array(channel_quant_overrides["scale"])
-                quantized_per_channel_data = quantize_nparray(
-                    weight_qType, per_channel_data.flatten(), scale, zero_point
-                )
-                assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
-                assert (
-                    zero_point.dtype != np.float32 and zero_point.dtype != np.float16
-                ), f"Unexpected dtype {zero_point.dtype}"
-                assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
-                assert isinstance(
-                    quantized_per_channel_data, np.ndarray
-                ), f"Unexpected type {type(quantized_per_channel_data)}"
-
-            else:
-                symmetric = channel_quant_overrides.get(
-                    "symmetric",
-                    (
-                        self.is_weight_symmetric
-                        or weight_qType in (onnx_proto.TensorProto.INT8, onnx_proto.TensorProto.FLOAT8E4M3FN)
-                    ),
-                )
-                _, _, zero_point, scale, quantized_per_channel_data = quantize_data(
-                    per_channel_data.flatten(),
-                    weight_qType,
-                    symmetric,
-                    reduce_range=channel_quant_overrides.get("reduce_range", self.reduce_range and reduce_range),
-                    min_real_range=self.min_real_range,
-                    rmin_override=channel_quant_overrides.get("rmin"),
-                    rmax_override=channel_quant_overrides.get("rmax"),
-                )
-
-                assert isinstance(zero_point, np.ndarray), f"Unexpected type {type(zero_point)}"
-                assert (
-                    zero_point.dtype != np.float32 and zero_point.dtype != np.float16
-                ), f"Unexpected dtype {zero_point.dtype}"
-                assert isinstance(scale, np.ndarray), f"Unexpected type {type(scale)}"
-                assert isinstance(
-                    quantized_per_channel_data, np.ndarray
-                ), f"Unexpected type {type(quantized_per_channel_data)}"
-
-            zero_point_list.append(zero_point)
-            scale_list.append(scale)
-            quantized_per_channel_data_list.append(quantized_per_channel_data)
-
-        # combine per_channel_data into one
-        reshape_dims = list(weights.shape)  # deep copy
-        reshape_dims[channel_axis] = 1  # only one per channel for reshape
-        quantized_weights = np.asarray(quantized_per_channel_data_list[0]).reshape(reshape_dims)
-        for i in range(1, len(quantized_per_channel_data_list)):
-            channel_weights = np.asarray(quantized_per_channel_data_list[i]).reshape(reshape_dims)
-            quantized_weights = np.concatenate((quantized_weights, channel_weights), channel_axis)
-
-        q_weight_name = weight_name + TENSOR_NAME_QUANT_SUFFIX
-        zp_name = weight_name + "_zero_point"
-        scale_name = weight_name + "_scale"
-
+        q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel_impl(
+            weight_name, weight_qType, channel_axis, reduce_range, keep_float_weight
+        )
         quantized_value = QuantizedValue(
             weight_name,
             q_weight_name,
             scale_name,
             zp_name,
             QuantizedValueType.Initializer,
             None,
         )
         self.quantized_value_map[weight_name] = quantized_value
 
-        # Update packed weight, zero point, and scale initializers
-        zero_scale_shape = [initializer.dims[channel_axis]]
-        scale_initializer = onnx.helper.make_tensor(
-            scale_name, initializer.data_type, zero_scale_shape, np.hstack(scale_list).tolist()
-        )
-        zero_initializer = onnx.helper.make_tensor(
-            zp_name, weight_qType, zero_scale_shape, np.hstack(zero_point_list).tolist()
-        )
-
-        self.model.initializer_extend([scale_initializer, zero_initializer])
-
-        if not keep_float_weight:
-            quantized_weights = np.asarray(
-                quantized_weights,
-                dtype=onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[weight_qType],
-            ).reshape(initializer.dims)
-            q_weight_initializer = onnx.numpy_helper.from_array(quantized_weights, q_weight_name)
-            self.model.initializer_extend([q_weight_initializer])
-
         return q_weight_name, zp_name, scale_name
 
     def _dequantize_value(self, value_name):
         """
         Given a value (input/output) which is quantized, add a DequantizeLinear node to dequantize
         it back to float32 or float16
             parameter value_name: value to dequantize
@@ -1390,40 +971,25 @@
         for output in self.model.graph().output:
             dequantize_node = self._dequantize_value(output.name)
             if dequantize_node is not None:
                 self.new_nodes.append(dequantize_node)
 
     def calculate_quantization_params(self):
         if self.tensors_range is None:
-            return
+            return None
 
-        # adjust tensor_ranges for input of Clip and Relu node
-        for node in self.model.nodes():
-            if node.op_type not in ["Clip", "Relu"]:
-                continue
-            if self.is_activation_symmetric:
-                continue
-            if not self.should_quantize_node(node):
-                continue
-            if len(self.model.input_name_to_nodes()[node.input[0]]) != 1:
-                continue
-            if node.input[0] not in self.tensors_range or node.output[0] not in self.tensors_range:
-                continue
-            td = self.tensors_range[node.output[0]]
-            if not isinstance(td, TensorData):
-                raise TypeError(f"Unexpected type {type(td)} for {node.output[0]!r}.")
-            self.tensors_range[node.input[0]] = td
+        self.adjust_tensor_ranges()
 
         quantization_params = {}
         for tensor_name in self.tensors_range:
             td = self.tensors_range[tensor_name]
             if not isinstance(td, TensorData):
                 raise TypeError(f"Unexpected type {type(td)} for {tensor_name!r}.")
 
-            quant_overrides = self.get_per_tensor_quant_overrides(tensor_name)
+            quant_overrides = self.tensor_quant_overrides.get_per_tensor_overrides(tensor_name, default_val={})
 
             quant_type = self.activation_qType
             if "quant_type" in quant_overrides:
                 quant_type = quant_overrides["quant_type"].tensor_type
 
             if "scale" in quant_overrides and "zero_point" in quant_overrides:
                 zero, scale = quant_overrides["zero_point"], quant_overrides["scale"]
```

## onnxruntime/quantization/qdq_quantizer.py

```diff
@@ -1,144 +1,217 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
+from __future__ import annotations
+
 import logging
+from dataclasses import dataclass
 from enum import Enum
+from typing import Any
 
+import numpy as np
 import onnx
 import onnx.numpy_helper
 from onnx import TensorProto
 from onnx import onnx_pb as onnx_proto
 
-from .onnx_quantizer import ONNXQuantizer
+from .base_quantizer import BaseQuantizer, QuantizationParams
+from .calibrate import TensorData
 from .quant_utils import (
     DEQUANT_OP_NAME,
     QUANT_OP_NAME,
     QuantizedValue,
     QuantizedValueType,
     __producer__,
     __version__,
     add_dequant_output_suffix,
     add_dequant_suffix,
     add_quant_input_suffix,
     add_quant_output_suffix,
     add_quant_suffix,
+    compute_scale_zp,
+    compute_scale_zp_float8,
     find_by_name,
+    get_qmin_qmax_for_qType,
     ms_domain,
+    normalize_axis,
+    tensor_proto_to_array,
 )
 from .registry import CreateQDQQuantizer
 
 
 class QDQQuantTensorType(Enum):
     ACTIVATION = 0
     WEIGHT = 1
     BIAS = 2
 
 
+# Holds the name of the node input from which a node output will share the
+# same quantization param initializers (zero-point and scale initializers).
+# Ex: A Transpose node's output will use the same quant param initializers used at the input.
+@dataclass
+class QDQQuantParamProvider:
+    input_name: str
+    node_name: str
+
+
+# Holds information for tensors that have been marked for quantization by operator quantizers.
+# Does not hold information for bias tensors.
 class QDQTensorQuantInfo:
     def __init__(self, tensor_type=QDQQuantTensorType.ACTIVATION, quant_para_provider=None, axis=None, data_type=None):
         self.tensor_type = tensor_type
         self.quant_para_provider = quant_para_provider
         self.axis = axis
         self.is_shared = quant_para_provider is not None
         assert data_type is not None
         self.data_type = data_type
 
 
-class QDQQuantizer(ONNXQuantizer):
+# Holds information for bias tensors that have been marked for quantization by operator quantizers.
+@dataclass
+class QDQBiasQuantInfo:
+    node_name: str
+    input_name: str
+    weight_name: str
+    beta: float
+
+
+# Holds quantization parameter values (scale, zp) for a tensor.
+# A tensor typically has a one set of quantization parameters, unless the tensor is
+# at a "mixed-precision" boundary where the activation quantization type changes (e.g., from uint8 to uint16).
+@dataclass
+class QDQTensorQuantParams:
+    original: QuantizationParams  # Generated by producer node.
+    converted: QuantizationParams | None  # Converted type consumed by some (or all/none) consumer nodes.
+    converted_recv_nodes: set[str] | None  # The name of nodes that consume the converted type.
+
+
+# Holds scale and zero_point initializer TensorProtos.
+@dataclass
+class QDQScaleZpInitializers:
+    scale: TensorProto
+    zero_point: TensorProto
+
+
+# Holds all scale and zero-point initializers for a tensor.
+# A tensor typically has a one set of quantization parameters, unless the tensor is
+# at a "mixed-precision" boundary where the activation quantization type changes (e.g., from uint8 to uint16).
+@dataclass
+class QDQTensorScaleZpInitializers:
+    original: QDQScaleZpInitializers
+    converted: QDQScaleZpInitializers | None
+    converted_recv_nodes: set[str] | None
+
+
+# Holds cached information of a tensor's quantized values (types, zp/scale initializer names, etc.).
+# A tensor typically has a one set of quantization parameters, unless the tensor is
+# at a "mixed-precision" boundary where the activation quantization type changes (e.g., from uint8 to uint16).
+@dataclass
+class QDQTensorQuantizedValue:
+    original: QuantizedValue
+    converted: QuantizedValue | None
+    converted_recv_nodes: set[str] | None
+
+    def get_for_consumer(self, consumer_node_name) -> QuantizedValue:
+        if self.converted is None:  # Quantized value is not converted, return original
+            return self.original
+
+        if self.converted_recv_nodes is None:  # All consumers receive the converted value
+            return self.converted
+
+        # Check if consumer node name is in the list of nodes that
+        # receive the converted quantization value. If not, return the original value generated
+        # by the tensor's producer.
+        return self.converted if (consumer_node_name in self.converted_recv_nodes) else self.original
+
+
+class QDQQuantizer(BaseQuantizer):
     def __init__(
         self,
         model,
         per_channel,
         reduce_range,
-        mode,
-        static,
         weight_qType,
         activation_qType,
         tensors_range,
         nodes_to_quantize,
         nodes_to_exclude,
         op_types_to_quantize,
         extra_options=None,
     ):
-        ONNXQuantizer.__init__(
+        BaseQuantizer.__init__(
             self,
             model,
             per_channel,
             reduce_range,
-            mode,
-            static,
             weight_qType,
             activation_qType,
             tensors_range,
             nodes_to_quantize,
             nodes_to_exclude,
             op_types_to_quantize,
             extra_options,
         )
         self.tensors_to_quantize = {}
-        self.bias_to_quantize = []
+        self.bias_to_quantize = {}
 
         self.nodes_to_remove = []
 
         # Specific op types to exclude qdq quantization for their outputs.
         # In TRT, it's not recommended to quantize outputs for weighted ops such as Conv, Matmul, Gemm
         # because those ops may be followed by nodes that require high resolution inputs.
         # Adding QDQ for those ops' output may end up with worse accuracy.
         # So, we don't recommend to add QDQ to node's output under such condition.
-        self.op_types_to_exclude_output_quantization = (
-            []
-            if "OpTypesToExcludeOutputQuantization" not in extra_options
-            else extra_options["OpTypesToExcludeOutputQuantization"]
-        )
+        self.op_types_to_exclude_output_quantization = extra_options.get("OpTypesToExcludeOutputQuantization", [])
 
         # We do quantization on Dequantizelinear's input to remove Quantizelinear for weight as an optimization.
         # In some cases, for example QDQ BERT model for TensorRT, QDQ should always appear as a pair.
         # Therefore, we need to disable this optimization and add qdq pair to weight.
-        self.add_qdq_pair_to_weight = (
-            False if "AddQDQPairToWeight" not in extra_options else extra_options["AddQDQPairToWeight"]
-        )
+        self.add_qdq_pair_to_weight = extra_options.get("AddQDQPairToWeight", False)
 
         # Some scenarios do not need the bias quantized. For example, in the case of Quantization Aware Training,
         # quantizing the bias is not needed. This is because in QAT, all model parameters are expected to be in
         # floating point format. To that end, we can use the FakeQuant operator for weights and activations that
         # can always have QDQ pairs (by using AddQDQPairToWeight). But for biases in a quantized model, we can't use
         # FakeQuant because it only ever appears before a DQ (since it is quantized as int32).
-        self.quantize_bias = True if "QuantizeBias" not in extra_options else extra_options["QuantizeBias"]
+        self.quantize_bias = extra_options.get("QuantizeBias", True)
 
         # The default behavior is that multiple nodes can share a QDQ pair as their inputs.
         # In TRT, QDQ pair can`t be shared between nodes, so it will create dedicated QDQ pairs for each node.
-        self.dedicated_qdq_pair = (
-            False if "DedicatedQDQPair" not in extra_options else extra_options["DedicatedQDQPair"]
-        )
-        if self.dedicated_qdq_pair:
-            self.tensor_to_its_receiving_nodes = {}
+        self.dedicated_qdq_pair = extra_options.get("DedicatedQDQPair", False)
+        self.tensor_to_its_receiving_nodes = {}
 
         # Let user set channel axis for specific op type and it's effective only when per channel quantization is supported and per_channel is True.
-        self.qdq_op_type_per_channel_support_to_axis = (
-            {}
-            if "QDQOpTypePerChannelSupportToAxis" not in extra_options
-            else extra_options["QDQOpTypePerChannelSupportToAxis"]
-        )
+        self.qdq_op_type_per_channel_support_to_axis = extra_options.get("QDQOpTypePerChannelSupportToAxis", {})
 
         self.qdq_op_domain = ms_domain if extra_options.get("UseQDQContribOps", False) else None
 
-        # The ONNX spec does not yet support 16-bit Q/DQ ops. So, must override the Q/DQ op domain to 'com.microsoft'
-        # if the activation or weight types are 16-bit integers.
-        # TODO: Remove this override (and use only the 'UseQDQContribOps' option) if/when ONNX adds 16-bit support.
-        int16_types = (TensorProto.UINT16, TensorProto.INT16)
-        if not self.qdq_op_domain and (self.activation_qType in int16_types or self.weight_qType in int16_types):
-            logging.warning(
-                "ONNX QuantizeLinear and DequantizeLinear operators do not support 16-bit integer quantization types. "
-                f"The domain of QuantizeLinear and DequantizeLinear operators will be set to '{ms_domain}' to "
-                "enable support."
-            )
-            self.qdq_op_domain = ms_domain
+        # The ONNX spec did not support 16-bit Q/DQ ops before opset 21.
+        # So, may have to override the Q/DQ op domain to 'com.microsoft' if the activation or weight types
+        # are 16-bit integers.
+        if self.opset_version < 21:
+            int16_types = (TensorProto.UINT16, TensorProto.INT16)
+            overrides_have_int16 = any(t.tensor_type in int16_types for t in self.tensor_quant_override_qtypes)
+            if not self.qdq_op_domain and (
+                self.activation_qType in int16_types or self.weight_qType in int16_types or overrides_have_int16
+            ):
+                logging.warning(
+                    "ONNX QuantizeLinear and DequantizeLinear operators do not support "
+                    "16-bit integer quantization types prior to opset 21. "
+                    f"The domain of QuantizeLinear and DequantizeLinear operators will be set to '{ms_domain}' to "
+                    "enable support."
+                )
+                self.qdq_op_domain = ms_domain
+
+        self.quantization_params = self.calc_graph_quant_params()
+
+        # Map of all original value names to quantized value names
+        self.quantized_value_map = {}
 
     def _get_tensor_type(self, tensor_name):
         """
         Check if tensor can be quantized
         """
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight is not None:
@@ -162,87 +235,127 @@
             if vi.type.HasField("tensor_type") and vi.type.tensor_type.elem_type in (
                 TensorProto.FLOAT,
                 TensorProto.FLOAT16,
             ):
                 return True
         else:
             logging.warning(
-                "failed to infer the type of tensor: {}. Skip to quantize it. Please check if it is expected.".format(
-                    tensor_name
-                )
+                f"failed to infer the type of tensor: {tensor_name}. Skip to quantize it. Please check if it is expected."
             )
 
         return False
 
-    def __quantize_tensor(self, tensor_name, quant_sharing_param=None, tensor_type=QDQQuantTensorType.ACTIVATION):
+    def __quantize_tensor(self, tensor_name, quant_sharing_provider=None, tensor_type=QDQQuantTensorType.ACTIVATION):
         """
-        Quantize tensors. If quant_param_tensor is not None, tensor with name tensor_name will be quantized with same
-        quantization parameters as tensor quant_param_tensor
+        Adds a tensor to the list (actually a dict) of tensors to quantize. Called indirectly by op quantizers that
+        want to quantize a tensor (i.e., "mark" a tensor for quantization).
+
+        If quant_sharing_provider is not None, tensor with name tensor_name will be quantized with the same
+        quantization parameters as the node input specified in quant_sharing_provider. Ex: A Tranpose node's output
+        will typically use the same quantization parameter initializers used at the Transpose node's input.
 
         Args:
             tensor_name: name of the tensor to quantize
-            quant_sharing_param: name of the tensor that provides quantization parameter
+            quant_sharing_provider: name of the tensor and node that provides quantization parameter
             tensor_type: QDQQuantTensorType default ACTIVATION
         """
         if self._is_tensor_quantizable(tensor_name):
-            if quant_sharing_param:
+            if quant_sharing_provider:
+                if not isinstance(quant_sharing_provider, QDQQuantParamProvider):
+                    raise TypeError(
+                        f"quant_sharing_provider must be of type QDQQuantParamProvider, not {type(quant_sharing_provider)}."
+                    )
+
                 data_type = self._get_tensor_type(tensor_name)
                 self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
-                    tensor_type=tensor_type, quant_para_provider=quant_sharing_param, data_type=data_type
+                    tensor_type=tensor_type, quant_para_provider=quant_sharing_provider, data_type=data_type
                 )
             elif tensor_name not in self.tensors_to_quantize:
                 data_type = self._get_tensor_type(tensor_name)
                 self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=tensor_type, data_type=data_type)
 
-    def quantize_activation_tensor(self, tensor_name, quant_sharing_param=None):
+    def quantize_activation_tensor(self, tensor_name: str):
         """
-        Quantize Activation Tensor
+        Adds a tensor to the list of tensors to quantize. Called by op quantizers that
+        want to quantize a tensor (i.e., "mark" a tensor for quantization).
+
         Args:
             tensor_name: name of the tensor to quantize
-            quant_sharing_param: name of the tensor that provides quantization parameter
-
         """
-        return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.ACTIVATION)
+        return self.__quantize_tensor(tensor_name, None, QDQQuantTensorType.ACTIVATION)
 
-    def quantize_weight_tensor(self, tensor_name, quant_sharing_param=None):
+    def quantize_output_same_as_input(self, output_name: str, input_name: str, node_name: str):
         """
-        Quantize Weight Tensor
+        Adds a tensor to the list of tensors to quantize. Called by op quantizers that
+        want to quantize an output tensor using the same quantization parameters as one of the node's inputs.
+
+        Ex: A Tranpose node's output will typically use the same quantization parameter initializers used at
+        the Transpose node's input.
+
         Args:
-            tensor_name: name of the tensor to quantize
-            quant_sharing_param: name of the tensor that provides quantization parameter
+            output_name: name of the node output to quantize so that it uses the same quantization params as an input.
+            input_name: name of the node input from which the output tensor will get its quantization params.
+            node_name: name of the node that consumes `input_name`.
+        """
+        return self.__quantize_tensor(
+            output_name, QDQQuantParamProvider(input_name, node_name), QDQQuantTensorType.ACTIVATION
+        )
 
+    def quantize_weight_tensor(self, tensor_name: str):
         """
-        return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.WEIGHT)
+        Adds a tensor to the list of weight tensors to quantize. Called by op quantizers that
+        want to quantize a weight (i.e., "mark" a weight for quantization).
+
+        Args:
+            tensor_name: name of the weight to quantize
+        """
+        return self.__quantize_tensor(tensor_name, None, QDQQuantTensorType.WEIGHT)
 
     def quantize_weight_tensor_per_channel(self, tensor_name, axis):
         weight = find_by_name(tensor_name, self.model.initializer())
         if weight:
             if weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
                 self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(
                     tensor_type=QDQQuantTensorType.WEIGHT, axis=axis, data_type=weight.data_type
                 )
         else:
             logging.warning(f"only support per-channel quantization on weight. Tensor: {tensor_name} is not quantized.")
 
-    def quantize_bias_tensor(self, bias_name, input_name, weight_name, beta=1.0):
+    def quantize_bias_tensor(self, node_name, bias_name, input_name, weight_name, beta=1.0):
+        """
+        Adds a bias tensor to the list of bias tensors to quantize. Called by op quantizers that
+        want to quantize a bias with bias_zero_point = 0 and bias_scale = input_scale * weight_scale * beta.
+        TODO: Explain the reasoning for using this formula.
+
+        Args:
+            node_name: name of the node that consumes the bias, input, and weight tensors.
+            bias_name: name of the bias tensor to quantize.
+            input_name: name of the input tensor whose scale is used to compute the bias's scale.
+            weight_name: name of the weight tensor whose scale is used to compute the bias's scale.
+            beta: Multiplier used to compute the bias's scale.
+        """
         # If the user provided quantization overrides for this tensor, treat it as a regular weight.
         if self.tensor_quant_overrides.get(bias_name):
             logging.info(
                 f"Quantizing bias tensor '{bias_name}' as a weight due to the presence of user-specified overrides"
             )
-            if self.per_channel:
-                self.quantize_weight_tensor_per_channel(bias_name, 0)
+            is_per_channel, axis = self.is_tensor_per_channel(bias_name, default_axis=0)
+            if is_per_channel:
+                self.quantize_weight_tensor_per_channel(bias_name, axis)
             else:
                 self.quantize_weight_tensor(bias_name)
             return
 
         weight = find_by_name(bias_name, self.model.initializer())
         if weight is not None:
             if weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
-                self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
+                if bias_name not in self.bias_to_quantize:
+                    self.bias_to_quantize[bias_name] = QDQBiasQuantInfo(node_name, input_name, weight_name, beta)
+                else:
+                    logging.warning(f"Bias {bias_name} has already been marked for quantization")
         else:
             logging.warning(f"Expected {bias_name} to be a weight")
 
     def remove_node(self, node):
         self.nodes_to_remove.append(node)
 
     def remove_nodes(self):
@@ -250,19 +363,18 @@
 
     def quantize_model(self):
         for node in self.model.nodes():
             if self.should_quantize_node(node):
                 op_quantizer = CreateQDQQuantizer(self, node)
                 op_quantizer.quantize()
 
-                if self.dedicated_qdq_pair:
-                    for tensor_name in node.input:
-                        if tensor_name not in self.tensor_to_its_receiving_nodes:
-                            self.tensor_to_its_receiving_nodes[tensor_name] = []
-                        self.tensor_to_its_receiving_nodes[tensor_name].append(node)
+                for tensor_name in node.input:
+                    if tensor_name not in self.tensor_to_its_receiving_nodes:
+                        self.tensor_to_its_receiving_nodes[tensor_name] = []
+                    self.tensor_to_its_receiving_nodes[tensor_name].append(node)
 
         self._quantize_normal_tensors()
         self._quantize_sharing_param_tensors()
         if self.quantize_bias:
             self._quantize_bias_tensors()
         self.remove_nodes()
         if not self.add_qdq_pair_to_weight:
@@ -274,24 +386,70 @@
             self.model.set_opset_import(ms_domain, 1)
 
         return self.model.model
 
     def try_replacing_upstream_output(self, upstream_output_name, output_name):
         if (
             output_name in self.quantization_params
+            and self.quantization_params[output_name].converted is None
+            and self.quantization_params[upstream_output_name].converted is None
             and len(self.model.input_name_to_nodes()[upstream_output_name]) == 1
             and not self.model.is_graph_output(upstream_output_name)
             and not self.model.is_graph_input(upstream_output_name)
         ):
             self.model.replace_output_of_all_nodes(upstream_output_name, output_name)
             if upstream_output_name in self.tensors_to_quantize:
                 del self.tensors_to_quantize[upstream_output_name]
             return True
         return False
 
+    def _create_q_node(
+        self,
+        q_input: str,
+        q_output: str,
+        quant_node_name: str,
+        scale_name: str,
+        zp_name: str,
+        axis: int | None = None,
+    ):
+        """
+        Creates a QuantizeLinear node and adds it to the model.
+        """
+        qlinear_node = onnx.helper.make_node(
+            QUANT_OP_NAME,
+            [q_input, scale_name, zp_name],
+            [q_output],
+            quant_node_name,
+            axis=axis,
+            domain=self.qdq_op_domain,
+        )
+        self.model.add_nodes([qlinear_node])
+
+    def _create_dq_node(
+        self,
+        dq_input: str,
+        dq_output: str,
+        dequant_node_name: str,
+        scale_name: str,
+        zp_name: str,
+        axis: int | None = None,
+    ):
+        """
+        Creates a DequantizeLinear node and adds it to the model.
+        """
+        dequant_node = onnx.helper.make_node(
+            DEQUANT_OP_NAME,
+            [dq_input, scale_name, zp_name],
+            [dq_output],
+            dequant_node_name,
+            axis=axis,
+            domain=self.qdq_op_domain,
+        )
+        self.model.add_nodes([dequant_node])
+
     def _create_qdq_nodes(
         self, q_input, q_output, quant_node_name, dq_input, dq_output, dequant_node_name, scale_name, zp_name, axis=None
     ):
         qlinear_node = onnx.helper.make_node(
             QUANT_OP_NAME,
             [q_input, scale_name, zp_name],
             [q_output],
@@ -313,14 +471,15 @@
         weight_name = weight_proto.name
         if axis is not None:
             if self.opset_version < 13:
                 raise ValueError("Per-Channel support with QDQ format requires onnx opset version 13 or above.")
             qtype = self.activation_qType
             if self.activation_qType == onnx.onnx_pb.TensorProto.UINT8:
                 qtype = onnx_proto.TensorProto.INT8
+
             q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
                 weight_name,
                 # Quantization type is forced to be TensorProto.INT8.
                 # when the expected value would be (see below)
                 # self.weight_qType if tensor_type is QDQQuantTensorType.WEIGHT else self.activation_qType.
                 # QLinearConv expects to have a unique value for all channels.
                 # This code does not enforce that but it is necessarily the case when the
@@ -394,15 +553,15 @@
                         tensor_name,
                         tensor_name_dequant_output_postfix,
                         scale_name,
                         zp_name,
                         QuantizedValueType.Input,
                         scale_type=data_type,
                     )
-                    self.quantized_value_map[tensor_name] = quantized_value
+                    self.quantized_value_map[tensor_name] = QDQTensorQuantizedValue(quantized_value, None, None)
         else:
             q_input = tensor_name
             dq_output = add_dequant_output_suffix(tensor_name)
             if self.model.is_graph_output(tensor_name):
                 q_input = add_quant_input_suffix(tensor_name)
                 dq_output = tensor_name
                 self.model.replace_output_of_all_nodes(tensor_name, q_input)
@@ -424,74 +583,282 @@
                 tensor_name,
                 dq_output,
                 scale_name,
                 zp_name,
                 QuantizedValueType.Input,
                 scale_type=data_type,
             )
-            self.quantized_value_map[tensor_name] = quantized_value
+            self.quantized_value_map[tensor_name] = QDQTensorQuantizedValue(quantized_value, None, None)
+
+    def _add_qdq_ops_for_converted_activation(
+        self,
+        tensor_name,
+        first_scale_name,
+        first_zp_name,
+        scale_data_type,
+        convert_scale_name,
+        convert_zp_name,
+        convert_recv_nodes,
+    ):
+        """
+        Adds Q and DQ ops to a tensor whose quantized data type is converted. That is, some consumers may use the
+        original data type from the producer, while other consumers use the converted data type.
+        This is generally done by adding a sequence of ops that convert from one data type (e.g., uint8) to another (e.g., uint16).
+
+        T_float ---> Quant(to u8) ---> Convert(to u16) ---> Dequant(to float) ---> T_float'
+        where Convert(to u16) is equivalent to: ---> Dequant(to float) ---> Quant(to u16) --->
+
+        This function handles the following scenarios:
+
+        1) Tensor T is not a graph output; all consumers use the converted type
+
+            <Producer> ---> Q1 ---> DQ1 ---> Q2 ---> DQ2 ---> <Consumers>
+
+        2) Tensor T is not a graph output; some consumers use the original type, others use the converted type
+
+            <Producer> ---> Q1 -+-> DQ1 ---> <Consumers of original type>
+                                |
+                                +-> DQ1' ---> Q2 ---> DQ2 ---> <Consumers of converted type>
+
+        3) Tensor T is a graph output; all consumers use the converted type
+
+            <Producer> ---> Q1 ---> DQ1 ---> Q2 ---> DQ2 -+-> <Consumers>
+                                                          |
+                                                          +-> <Graph output>
+
+        4) Tensor T is a graph output; some consumers use the original type, others use the converted type
+
+            <Producer> ---> Q1 -+-> DQ1 -+-> <Consumers of original type>
+                                |        |
+                                |        +-> <Graph output>
+                                |
+                                +-> DQ1' ---> Q2 ---> DQ2 ---> <Consumers of converted type>
+        """
+        tensor_recv_nodes = set([node.name for node in self.tensor_to_its_receiving_nodes[tensor_name]])
+
+        if (
+            self.dedicated_qdq_pair
+            and tensor_name in self.tensor_to_its_receiving_nodes
+            and len(self.tensor_to_its_receiving_nodes[tensor_name]) > 1
+        ):
+            # TODO: Add support for dedicated_qdq_pair if/when needed.
+            raise ValueError(
+                "Do not currently support converted quant_types in TensorQuantOverrides when the `dedicated_qdq_pair` extra_option is enabled"
+            )
+
+        # Determine which nodes consume the original quantized type and which nodes
+        # consume the converted quantized type.
+        original_recv_nodes = tensor_recv_nodes
+        if convert_recv_nodes is None:  # In this case, all consumers receive the converted type.
+            convert_recv_nodes = tensor_recv_nodes
+            original_recv_nodes = set()
+        else:
+            original_recv_nodes = original_recv_nodes - convert_recv_nodes
+
+        all_use_converted = len(convert_recv_nodes) == len(tensor_recv_nodes)
+        is_graph_output = self.model.is_graph_output(tensor_name)
+
+        # Create first Q op.
+        first_q_input = tensor_name
+        if is_graph_output:
+            first_q_input = add_quant_input_suffix(tensor_name)
+            self.model.replace_output_of_all_nodes(tensor_name, first_q_input)
+
+        first_q_output = add_quant_output_suffix(tensor_name)
+        self._create_q_node(
+            first_q_input, first_q_output, add_quant_suffix(tensor_name), first_scale_name, first_zp_name
+        )
+
+        # Create first DQ op.
+        first_dq_output = add_dequant_output_suffix(tensor_name)
+        if is_graph_output and not all_use_converted:
+            first_dq_output = tensor_name
+        if original_recv_nodes and first_dq_output != tensor_name:
+            self.model.replace_input_of_nodes(tensor_name, first_dq_output, original_recv_nodes)
+
+        self._create_dq_node(
+            first_q_output, first_dq_output, add_dequant_suffix(tensor_name), first_scale_name, first_zp_name
+        )
+
+        # Create parallel clone of first DQ op if _not all_ consumers use the converted type.
+        # --> DQ1' --> Q2 --> DQ2 --> <Consumers of converted type>
+        #
+        # This DQ clone would only have one consumer Q node (Q2) and could be potentially fused with
+        # it by some EPs (e.g., QNN) without breaking other "node units".
+        # Ex QNN fusion:
+        # --> Convert (fused) --> DQ2 --> <Consumers of converted type>
+        second_q_input = first_dq_output
+        if not all_use_converted:
+            second_q_input = add_quant_input_suffix(f"{tensor_name}_convert")
+            self._create_dq_node(
+                first_q_output,
+                second_q_input,
+                add_dequant_suffix(f"{tensor_name}_convert_clone"),
+                first_scale_name,
+                first_zp_name,
+            )
+
+        # Create second Q op.
+        second_q_output = add_quant_output_suffix(f"{tensor_name}_convert")
+        self._create_q_node(
+            second_q_input,
+            second_q_output,
+            add_quant_suffix(f"{tensor_name}_convert"),
+            convert_scale_name,
+            convert_zp_name,
+        )
+
+        # Create second DQ op.
+        second_dq_output = add_dequant_output_suffix(f"{tensor_name}_convert")
+        if is_graph_output and all_use_converted:
+            second_dq_output = tensor_name
+        if convert_recv_nodes and second_dq_output != tensor_name:
+            self.model.replace_input_of_nodes(tensor_name, second_dq_output, convert_recv_nodes)
+        self._create_dq_node(
+            second_q_output,
+            second_dq_output,
+            add_dequant_suffix(f"{tensor_name}_convert"),
+            convert_scale_name,
+            convert_zp_name,
+        )
+
+        # Store in quantized_value_map
+        original_quantized_value = QuantizedValue(
+            tensor_name,
+            first_dq_output,
+            first_scale_name,
+            first_zp_name,
+            QuantizedValueType.Input,
+            scale_type=scale_data_type,
+        )
+        converted_quantized_value = QuantizedValue(
+            tensor_name,
+            second_dq_output,
+            convert_scale_name,
+            convert_zp_name,
+            QuantizedValueType.Input,
+            scale_type=scale_data_type,
+        )
+        self.quantized_value_map[tensor_name] = QDQTensorQuantizedValue(
+            original_quantized_value, converted_quantized_value, convert_recv_nodes
+        )
 
     def _quantize_normal_tensors(self):
+        """
+        Adds Q/DQ ops to tensors (activations and weights) that have been marked for quantization by op quantizers.
+        """
         for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
             if tensor_name in self.quantized_value_map:
                 continue
 
             if not tensor_info.is_shared:
                 # Quantize the input
                 initializer = find_by_name(tensor_name, self.model.initializer())
                 if initializer:
                     self._add_qdq_pair_for_initializer(initializer, tensor_info.tensor_type, tensor_info.axis)
                 else:
-                    used_scale, used_zp = self.find_quant_scale_zp(tensor_name)
-                    if used_scale is not None and not hasattr(used_scale, "dtype"):
-                        raise TypeError(
-                            f"Unexpected type {type(used_scale)} for used_scale and tensor_name={tensor_name!r}"
-                        )
-                    data_found, scale_name, zp_name, _, _ = self._get_quantization_params(
-                        tensor_name, used_scale, used_zp
-                    )
-
-                    if not data_found:
+                    tensor_qparam_initializers = self._make_tensor_scale_zp_initializers(tensor_name)
+                    if not tensor_qparam_initializers:
                         raise ValueError(
                             f"Quantization parameters are not specified for param {tensor_name}. "
                             "In static mode quantization params for inputs and outputs of nodes to be quantized are required."
                         )
 
-                    self._add_qdq_pair_for_activation(tensor_name, scale_name, zp_name, data_type=tensor_info.data_type)
+                    if tensor_qparam_initializers.converted is None:
+                        # Normal case: <producer> --> Q --> DQ --> <consumers>
+                        self._add_qdq_pair_for_activation(
+                            tensor_name,
+                            tensor_qparam_initializers.original.scale.name,
+                            tensor_qparam_initializers.original.zero_point.name,
+                            data_type=tensor_info.data_type,
+                        )
+                    else:
+                        # Conversion case: <producer> ---> Q1 -+-> DQ1 --> <consumers of original type>
+                        #                                      |
+                        #                                      +-> DQ1' --> Q2 --> DQ2 --> <consumers of converted type>
+                        assert tensor_info.data_type == tensor_qparam_initializers.original.scale.data_type
+                        self._add_qdq_ops_for_converted_activation(
+                            tensor_name,
+                            tensor_qparam_initializers.original.scale.name,
+                            tensor_qparam_initializers.original.zero_point.name,
+                            tensor_info.data_type,
+                            tensor_qparam_initializers.converted.scale.name,
+                            tensor_qparam_initializers.converted.zero_point.name,
+                            tensor_qparam_initializers.converted_recv_nodes,
+                        )
 
                 del self.tensors_to_quantize[tensor_name]
 
     def _quantize_sharing_param_tensors(self):
+        """
+        Adds Q/DQ ops to tensors that have been marked for quantization by op quantizers.
+        Only operates on tensors that want to use the quantization parameter initializers from an upstream tensor.
+        For example, a Transpose node's output tensor will typically want to use the same quantization parameter
+        initializers as the Transpose node's input.
+        """
         while self.tensors_to_quantize:
             for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
-                tensor_provider_name = tensor_info.quant_para_provider
-                if tensor_provider_name in self.quantized_value_map:
+                quant_provider = tensor_info.quant_para_provider
+                if quant_provider and quant_provider.input_name in self.quantized_value_map:
                     del self.tensors_to_quantize[tensor_name]
 
-                    quantized_value = self.quantized_value_map[tensor_provider_name]
-                    # Quantize the input
-                    initializer = find_by_name(tensor_name, self.model.initializer())
-                    if initializer is not None:
+                    quantized_value = self.quantized_value_map[quant_provider.input_name].get_for_consumer(
+                        quant_provider.node_name
+                    )
+                    if self.is_input_a_initializer(tensor_name):
                         raise ValueError("Quantization parameter shared mode is not supported for weight yet")
-                    self._add_qdq_pair_for_activation(tensor_name, quantized_value.scale_name, quantized_value.zp_name)
+
+                    # Need to check if this tensor's quant_type is converted for some consumers.
+                    # If so, create new scale/zp initializers for these consumers.
+                    converted_qparam_inits = None
+                    converted_recv_nodes = None
+                    if tensor_name in self.quantization_params:
+                        tensor_params = self.quantization_params[tensor_name]
+                        if tensor_params.converted:
+                            converted_qparam_inits = self._make_scale_zp_initializers(
+                                tensor_name, tensor_params.converted, "_convert"
+                            )
+                            converted_recv_nodes = tensor_params.converted_recv_nodes
+
+                    if converted_qparam_inits is None:
+                        # Normal case: <producer> --> Q_shared --> DQ_shared --> <consumers>
+                        self._add_qdq_pair_for_activation(
+                            tensor_name, quantized_value.scale_name, quantized_value.zp_name
+                        )
+                    else:
+                        # Conversion case: <producer> ---> Q_shared -+-> DQ_shared --> <consumers of original type>
+                        #                                            |
+                        #                                            +-> DQ_shared' --> Q2 --> DQ2 --> <consumers of converted type>
+                        self._add_qdq_ops_for_converted_activation(
+                            tensor_name,
+                            quantized_value.scale_name,
+                            quantized_value.zp_name,
+                            converted_qparam_inits.scale.data_type,
+                            converted_qparam_inits.scale.name,
+                            converted_qparam_inits.zero_point.name,
+                            converted_recv_nodes,
+                        )
 
     def _quantize_bias_tensors(self):
-        for bias_name, input_name, weight_name, beta in self.bias_to_quantize:
+        """
+        Adds DQ ops (or Cast) for bias tensors that have been marked for quantization by op quantizers.
+        """
+        for bias_name, bias_info in self.bias_to_quantize.items():
             if bias_name in self.quantized_value_map:
                 continue
             # Quantize the input
-            self.quantize_bias_static(bias_name, input_name, weight_name, beta)
+            self.quantize_bias_static(bias_name, bias_info)
             init = find_by_name(bias_name, self.model.initializer())
             self.model.remove_initializer(init)
-            quant_value = self.quantized_value_map[bias_name]
+            quant_value = self.quantized_value_map[bias_name].original
             if quant_value.node_type == "Cast":
                 # simple cast to float 16 and not DequantizeLinear
                 # cublasLtMatmul only supports (b)float16, float bias.
                 if not isinstance(init.data_type, int):
-                    raise TypeError(f"Unexpected type {type(init.data_type)} for input={input_name!r}")
+                    raise TypeError(f"Unexpected type {type(init.data_type)} for input={bias_info.input_name!r}")
                 node_name = add_dequant_suffix(bias_name)
                 dequant_node = onnx.helper.make_node(
                     "Cast",
                     [quant_value.q_name],
                     [bias_name],
                     name=node_name,
                     to=init.data_type,
@@ -522,9 +889,287 @@
                         node_name,
                         domain=self.qdq_op_domain,
                     )
             else:
                 raise RuntimeError(f"Unexpected operator type {quant_value.node_type!r}.")
             self.model.add_node(dequant_node)
 
-    def is_tensor_quantized(self, tensor_name):
+    def is_tensor_quantized(self, tensor_name: str):
         return tensor_name in self.tensors_to_quantize or tensor_name in self.bias_to_quantize
+
+    def quantize_initializer(
+        self,
+        weight: onnx.TensorProto,
+        qType: onnx.TensorProto.DataType,
+        reduce_range: bool = False,
+        keep_float_weight: bool = False,
+    ) -> tuple[str, str, str]:
+        """
+        :param weight: TensorProto initializer
+        :param qType: type to quantize to
+        :param keep_float_weight: Whether to quantize the weight. In some cases, we only want to qunatize scale and zero point.
+                                  If keep_float_weight is False, quantize the weight, or don't quantize the weight.
+        :return: quantized weight name, zero point name, scale name
+        """
+        # Find if this input is already quantized
+        if weight.name in self.quantized_value_map:
+            quantized_value = self.quantized_value_map[weight.name].original
+            return (
+                quantized_value.q_name,
+                quantized_value.zp_name,
+                quantized_value.scale_name,
+            )
+
+        q_weight_name, zp_name, scale_name = self.quantize_initializer_impl(
+            weight, qType, reduce_range, keep_float_weight
+        )
+
+        # Log entry for this quantized weight
+        quantized_value = QuantizedValue(
+            weight.name,
+            q_weight_name,
+            scale_name,
+            zp_name,
+            QuantizedValueType.Initializer,
+            None,
+        )
+        self.quantized_value_map[weight.name] = QDQTensorQuantizedValue(quantized_value, None, None)
+        return q_weight_name, zp_name, scale_name
+
+    def is_tensor_per_channel(
+        self,
+        tensor_name: str,
+        default_axis: int,
+        op_type: str | None = None,
+    ) -> tuple[bool, int | None]:
+        """
+        Checks if a given tensor is configured to be quantized per-channel. If so, also returns the channel axis.
+
+        ORT only supports per-channel quantization on static weights (i.e., ONNX initializers). If the user did not provide
+        tensor quantization overrides for this tensor, then the value of self.per_channel determines if the weight
+        is to be quantized per-channel.
+
+        Params:
+            tensor_name: The name of the tensor to check.
+            default_axis: The default channel axis. This method checks if the normalized axis is within bounds.
+                          Can be overridden via the extra_options 'QDQOpTypePerChannelSupportToAxis'
+                          and 'TensorQuantOverrides'.
+            op_type: Optional, defaults to None. The operator type that is the only consumer of this weight.
+                     Used to access the extra option 'QDQOpTypePerChannelSupportToAxis'.
+        Returns:
+            A tuple (is_per_channel, axis) in which the first element indicates whether the tensor is
+            quantized per-channel and the second element is the channel axis.
+            The returned axis is only None if the tensor is not per-channel or the axis is out of bounds.
+        """
+        weight_initializer = self.initializers.get(tensor_name)
+        if weight_initializer is None:
+            return False, None  # Only support per-channel weights
+
+        if self.tensor_quant_overrides.has_per_tensor_overrides(tensor_name):
+            return False, None  # User provided per-tensor overrides for this initializer
+
+        has_per_chan_overrides = self.tensor_quant_overrides.has_per_channel_overrides(tensor_name)
+        if not self.per_channel and not has_per_chan_overrides:
+            return False, None  # global self.per_channel is off and user did not provide per-channel overrides.
+
+        axis = self.qdq_op_type_per_channel_support_to_axis.get(op_type, default_axis) if op_type else default_axis
+        if has_per_chan_overrides:
+            per_chan_overrides = self.tensor_quant_overrides.get_per_channel_overrides(tensor_name)
+            axis = per_chan_overrides[0]["axis"]  # Prefer axis from user-specified tensor-level overrides if available
+
+        weight_nparray = tensor_proto_to_array(weight_initializer)
+        weight_rank = len(weight_nparray.shape)
+        axis_valid, axis = normalize_axis(axis, weight_rank)
+        if not axis_valid:
+            logging.warning(f"Axis {axis} is out-of-range for weight '{tensor_name}' with rank {weight_rank}")
+            return False, None
+
+        return True, axis
+
+    def quantize_weight_per_channel(
+        self,
+        weight_name: str,
+        weight_qType: onnx.TensorProto.DataType,
+        channel_axis: int,
+        reduce_range: bool = True,
+        keep_float_weight: bool = False,
+    ) -> tuple[str, str, str]:
+        # Find if this input is already quantized
+        if weight_name in self.quantized_value_map:
+            quantized_value = self.quantized_value_map[weight_name].original
+            return (
+                quantized_value.q_name,
+                quantized_value.zp_name,
+                quantized_value.scale_name,
+            )
+
+        q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel_impl(
+            weight_name, weight_qType, channel_axis, reduce_range, keep_float_weight
+        )
+        quantized_value = QuantizedValue(
+            weight_name,
+            q_weight_name,
+            scale_name,
+            zp_name,
+            QuantizedValueType.Initializer,
+            None,
+        )
+        self.quantized_value_map[weight_name] = QDQTensorQuantizedValue(quantized_value, None, None)
+
+        return q_weight_name, zp_name, scale_name
+
+    def quantize_bias_static(self, bias_name: str, bias_info: QDQBiasQuantInfo) -> str:
+        """
+        Quantized the bias. Zero Point == 0 and Scale == Input_Scale * Weight_Scale
+        """
+
+        # Handle case where bias already in quantization map
+        if bias_name in self.quantized_value_map:
+            return self.quantized_value_map[bias_name].original.q_name
+
+        # get scale for weight
+        weight_scale_name = self.quantized_value_map[bias_info.weight_name].original.scale_name
+        weight_initializer = find_by_name(weight_scale_name, self.model.initializer())
+        weight_scale = tensor_proto_to_array(weight_initializer)
+
+        # get scale for input
+        input_scale_name = (
+            self.quantized_value_map[bias_info.input_name].get_for_consumer(bias_info.node_name).scale_name
+        )
+        inputscale_initializer = find_by_name(input_scale_name, self.model.initializer())
+        input_scale = tensor_proto_to_array(inputscale_initializer)
+
+        (
+            quantized_bias_name,
+            quantized_bias_scale_name,
+            quantized_bias_zp_name,
+            bias_scale_data,
+            node_type,
+            node_qtype,
+        ) = self.quantize_bias_static_impl(bias_name, input_scale, weight_scale, bias_info.beta)
+
+        quantized_value = QuantizedValue(
+            bias_name,
+            quantized_bias_name,
+            quantized_bias_scale_name,
+            quantized_bias_zp_name,
+            QuantizedValueType.Initializer,
+            0 if bias_scale_data.size > 1 else None,
+            node_type=node_type,
+            node_qtype=node_qtype,
+        )
+        self.quantized_value_map[bias_name] = QDQTensorQuantizedValue(quantized_value, None, None)
+
+        return quantized_bias_name
+
+    def _make_scale_zp_initializers(
+        self, param_name: str, params: QuantizationParams, init_name_suffix: str = ""
+    ) -> QDQScaleZpInitializers:
+        """
+        Creates and returns scale and zero-point initializers for the given quantization params. The initializers are
+        named:
+            - {param_name}_zero_point{init_name_suffix}
+            - {param_name}_scale{init_name_suffix}
+        """
+        zero_point_values = np.array([params["zero_point"]])
+        if not hasattr(params["scale"], "dtype") or params["scale"].dtype not in (np.float32, np.float16):
+            raise ValueError(f"Unexpected type {type(params['scale'])} and param_name={param_name!r}")
+        scale_values = np.array([params["scale"]])
+        assert scale_values.dtype != np.float64
+        zero_point_type = params.data.get("quant_type", self.activation_qType)
+
+        zero_point_shape = []
+        zero_point_name = param_name + "_zero_point" + init_name_suffix
+        scale_shape = []
+        scale_name = param_name + "_scale" + init_name_suffix
+
+        # Add initializers to model
+        init_zp = onnx.helper.make_tensor(
+            zero_point_name, zero_point_type, zero_point_shape, zero_point_values.ravel().tolist()
+        )
+        self.model.add_initializer(init_zp)
+
+        if scale_values.dtype == np.float32:
+            scale_type = onnx_proto.TensorProto.FLOAT
+        elif scale_values.dtype == np.float16:
+            scale_type = onnx_proto.TensorProto.FLOAT16
+        else:
+            raise ValueError(f"Unexpected dtype={scale_values.dtype} for param_name={param_name!r}")
+        init_scale = onnx.helper.make_tensor(scale_name, scale_type, scale_shape, scale_values.reshape((-1,)).tolist())
+        self.model.add_initializer(init_scale)
+
+        return QDQScaleZpInitializers(init_scale, init_zp)
+
+    def _make_tensor_scale_zp_initializers(self, tensor_name: str) -> QDQTensorScaleZpInitializers | None:
+        """
+        Create and returns all scale/zero_point initializers for a given tensor. If the tensor is converted
+        to a different quantization type, this function creates two pairs of zp/scale initializers. Otherwise,
+        only one pair of zp/scale initializers is created.
+        """
+        if self.quantization_params is None or tensor_name not in self.quantization_params:
+            logging.info(f'Quantization parameters for tensor:"{tensor_name}" not specified')
+            return None
+
+        tensor_params = self.quantization_params[tensor_name]
+        if not isinstance(tensor_params, QDQTensorQuantParams):
+            raise TypeError(f"Unexpected type {type(tensor_params)} for {tensor_name!r}.")
+
+        original_inits = self._make_scale_zp_initializers(tensor_name, tensor_params.original)
+        converted_inits = (
+            self._make_scale_zp_initializers(tensor_name, tensor_params.converted, "_convert")
+            if tensor_params.converted
+            else None
+        )
+
+        return QDQTensorScaleZpInitializers(original_inits, converted_inits, tensor_params.converted_recv_nodes)
+
+    def calc_quant_params(self, tensor_data: TensorData, quant_overrides: dict[str, Any]) -> QuantizationParams:
+        """
+        Calculates quantization parameters (scale/zero-point) given a tensor's min/max range and optional
+        user-provided overrides.
+        """
+        quant_type = self.activation_qType
+        if "quant_type" in quant_overrides:
+            quant_type = quant_overrides["quant_type"].tensor_type
+
+        if "scale" in quant_overrides and "zero_point" in quant_overrides:
+            zero, scale = quant_overrides["zero_point"], quant_overrides["scale"]
+        elif quant_type == onnx.TensorProto.FLOAT8E4M3FN:
+            zero, scale = compute_scale_zp_float8(quant_type, tensor_data.avg_std[1])
+        else:
+            rmin = quant_overrides.get("rmin", tensor_data.range_value[0])
+            rmax = quant_overrides.get("rmax", tensor_data.range_value[1])
+            symmetric = quant_overrides.get("symmetric", self.is_activation_symmetric)
+            reduce_range = quant_overrides.get("reduce_range", False)
+            qmin, qmax = get_qmin_qmax_for_qType(quant_type, reduce_range=reduce_range, symmetric=symmetric)
+            zero, scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric, self.min_real_range)
+
+        return QuantizationParams(zero_point=zero, scale=scale, quant_type=quant_type)
+
+    def calc_graph_quant_params(self) -> dict[str, QDQTensorQuantParams]:
+        """
+        Calculates quantization parameters (scale/zero-point) for all tensors in the graph using each tensor's min/max range
+        and optional user-provided overrides.
+        """
+        if self.tensors_range is None:
+            return {}
+
+        self.adjust_tensor_ranges()
+
+        quantization_params = {}
+        for tensor_name in self.tensors_range:
+            td = self.tensors_range[tensor_name]
+            if not isinstance(td, TensorData):
+                raise TypeError(f"Unexpected type {type(td)} for {tensor_name!r}.")
+
+            quant_overrides = self.tensor_quant_overrides.get_per_tensor_overrides(tensor_name, default_val={})
+            original = self.calc_quant_params(td, quant_overrides)
+            converted = None
+            converted_recv_nodes = None
+
+            if "convert" in quant_overrides:
+                converted = self.calc_quant_params(td, quant_overrides["convert"])
+                converted_recv_nodes = quant_overrides["convert"].get("recv_nodes")
+
+            quantization_params[tensor_name] = QDQTensorQuantParams(original, converted, converted_recv_nodes)
+
+        return quantization_params
```

## onnxruntime/quantization/quant_utils.py

```diff
@@ -1,7 +1,14 @@
+# -------------------------------------------------------------------------
+# Copyright (c) Microsoft Corporation. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+# --------------------------------------------------------------------------
+from __future__ import annotations
+
 import logging
 import os
 import tempfile
 from enum import Enum
 from pathlib import Path
 
 import numpy
@@ -249,15 +256,25 @@
     dq = numpy.array(qmax, dtype=numpy.float64) - numpy.array(qmin, dtype=numpy.float64)
     scale = numpy.array(dr / dq)
     assert scale >= 0, "scale isse"
     if scale < numpy.finfo(rmax.dtype).tiny:
         scale = numpy.array(1.0, dtype=rmax.dtype)
         zero_point = numpy.array(0, dtype=qmin.dtype)
     else:
-        zero_point = numpy.array(numpy.round(qmin - rmin / scale), dtype=qmin.dtype)
+        if symmetric:
+            # When symmetric (i.e., rmax == -rmin), the zero_point formula reduces to round((qmax + qmin) / 2.0).
+            # This simpler formula doesn't depend on scale and guarantees that the zero point values
+            # for int8, uint8, int16, and uint16 are always 0, 128, 0, and 32768, respectively.
+            # This is important for per-channel/symmetric QLinearConv on CPU EP, which requires all channels to have
+            # the exact same zero_point values.
+            zero_point = numpy.array(
+                numpy.round((qmin + qmax) / numpy.array(2.0, dtype=numpy.float64)), dtype=qmin.dtype
+            )
+        else:
+            zero_point = numpy.array(numpy.round(qmin - rmin / scale), dtype=qmin.dtype)
         scale = scale.astype(rmax.dtype)
 
     return [zero_point, scale]
 
 
 def compute_scale_zp_float8(element_type, std):
     """Calculate the scale s for a float8 type (E4M3FN).
@@ -272,15 +289,15 @@
     zp_dtype = None
     if element_type not in FLOAT8_DISTRIBUTIONS:
         if element_type == TensorProto.FLOAT8E4M3FN:
             from onnx.numpy_helper import float8e4m3_to_float32
             from onnx.reference.custom_element_types import float8e4m3fn
 
             zp_dtype = float8e4m3fn
-            all_values = [float8e4m3_to_float32(i) for i in range(0, 256)]
+            all_values = [float8e4m3_to_float32(i) for i in range(256)]
             values = numpy.array(
                 [f for f in all_values if not numpy.isnan(f) and not numpy.isinf(f)], dtype=numpy.float32
             )
         else:
             raise ValueError(f"Quantization to element_type={element_type} not implemented.")
         FLOAT8_DISTRIBUTIONS[element_type] = values
     elif element_type == TensorProto.FLOAT8E4M3FN:
@@ -403,14 +420,26 @@
         parameter qType: quantization type.
         return: quantization range.
     """
     qmin, qmax = get_qmin_qmax_for_qType(qType, reduce_range, symmetric=symmetric)
     return qmax - qmin
 
 
+def normalize_axis(axis: int, rank: int) -> tuple[bool, int]:
+    """
+    Helper function that tries to return a normalized axis in the range [0, rank - 1].
+    :parameter axis: The axis to normalize.
+    :parameter rank: The tensor rank (number of dimensions).
+    :return (is_valid, axis_norm)
+    """
+    axis_norm = axis + rank if axis < 0 else axis
+    is_valid = axis_norm >= 0 and axis_norm < rank
+    return is_valid, axis_norm
+
+
 class QuantizedInitializer:
     """
     Represents a linearly quantized weight input from ONNX operators
     """
 
     def __init__(
         self,
@@ -526,15 +555,15 @@
 
 
 def get_elem_index(elem_name, elem_list):
     """
     Helper function to return index of an item in a node list
     """
     elem_idx = -1
-    for i in range(0, len(elem_list)):
+    for i in range(len(elem_list)):
         if elem_list[i] == elem_name:
             elem_idx = i
     return elem_idx
 
 
 def get_mul_node(inputs, output, name):
     """
```

## onnxruntime/quantization/quantize.py

```diff
@@ -2,24 +2,28 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
 import logging
 import tempfile
 from pathlib import Path
+from typing import Union
+
+import onnx
 
 from .calibrate import CalibrationDataReader, CalibrationMethod, TensorsData, create_calibrator
 from .onnx_quantizer import ONNXQuantizer
 from .qdq_quantizer import QDQQuantizer
 from .quant_utils import (
     QuantFormat,
     QuantizationMode,
     QuantType,
     load_model_with_shape_infer,
     model_has_pre_process_metadata,
+    save_and_reload_model_with_shape_infer,
 )
 from .registry import IntegerOpsRegistry, QDQRegistry, QLinearOpsRegistry
 
 
 class QuantConfig:
     def __init__(
         self,
@@ -276,16 +280,16 @@
         logging.warning(
             "Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. "
             "Or it will lead to bad performance on x64."
         )
 
 
 def quantize_static(
-    model_input,
-    model_output,
+    model_input: Union[str, Path, onnx.ModelProto],
+    model_output: Union[str, Path],
     calibration_data_reader: CalibrationDataReader,
     quant_format=QuantFormat.QDQ,
     op_types_to_quantize=None,
     per_channel=False,
     reduce_range=False,
     activation_type=QuantType.QInt8,
     weight_type=QuantType.QInt8,
@@ -300,15 +304,15 @@
     It is recommended to use QuantFormat.QDQ format from 1.11 with activation_type = QuantType.QInt8 and weight_type
     = QuantType.QInt8. If model is targeted to GPU/TRT, symmetric activation and weight are required. If model is
     targeted to CPU, asymmetric activation and symmetric weight are recommended for balance of performance and
     accuracy.
 
     Args:
 
-        model_input: file path of model to quantize
+        model_input: file path of model or ModelProto to quantize
         model_output: file path of quantized model
         calibration_data_reader: a calibration data reader. It
             enumerates calibration data and generates inputs for the
             original model.
         quant_format: QuantFormat{QOperator, QDQ}.
             QOperator format quantizes the model with quantized operators directly.
             QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
@@ -431,15 +435,19 @@
     mode = QuantizationMode.QLinearOps
 
     if not op_types_to_quantize or len(op_types_to_quantize) == 0:
         q_linear_ops = list(QLinearOpsRegistry.keys())
         qdq_ops = list(QDQRegistry.keys())
         op_types_to_quantize = list(set(q_linear_ops + qdq_ops))
 
-    model = load_model_with_shape_infer(Path(model_input))
+    model = (
+        save_and_reload_model_with_shape_infer(model_input)
+        if isinstance(model_input, onnx.ModelProto)
+        else load_model_with_shape_infer(Path(model_input))
+    )
 
     pre_processed: bool = model_has_pre_process_metadata(model)
     if not pre_processed:
         logging.warning(
             "Please consider to run pre-processing before quantization. Refer to example: "
             "https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification"
             "/cpu/ReadMe.md "
@@ -475,20 +483,29 @@
 
         orig_nodes = [i.name for i in model.graph.node]
         dataloader = inc_dataloader()
         sq = ORTSmoothQuant(model_input, dataloader, reduce_range)
         del dataloader
         model = sq.transform(extra_options.get("SmoothQuantAlpha", 0.5), extra_options.get("SmoothQuantFolding", True))
         sq_path = tempfile.TemporaryDirectory(prefix="ort.quant.")
-        model_input = Path(sq_path).joinpath("sq_model.onnx").as_posix()
+        model_input = Path(sq_path.name).joinpath("sq_model.onnx").as_posix()
         model.save(model_input)
         nodes_to_exclude.extend([i.name for i in model.model.graph.node if i.name not in orig_nodes])
         model = load_model_with_shape_infer(Path(model_input))  # use smooth quant model for calibration
 
     with tempfile.TemporaryDirectory(prefix="ort.quant.") as quant_tmp_dir:
+        if isinstance(model_input, onnx.ModelProto):
+            output_path = str(Path(quant_tmp_dir) / "model_input.onnx")
+            onnx.save_model(
+                model_input,
+                output_path,
+                save_as_external_data=True,
+            )
+            model_input = output_path
+
         calibrator = create_calibrator(
             Path(model_input),
             op_types_to_quantize,
             augmented_model_path=Path(quant_tmp_dir).joinpath("augmented_model.onnx").as_posix(),
             calibrate_method=calibrate_method,
             use_external_data_format=use_external_data_format,
             extra_options=calib_extra_options,
@@ -519,16 +536,14 @@
             extra_options,
         )
     else:
         quantizer = QDQQuantizer(
             model,
             per_channel,
             reduce_range,
-            mode,
-            True,  # static
             weight_type,
             activation_type,
             tensors_range,
             nodes_to_quantize,
             nodes_to_exclude,
             op_types_to_quantize,
             extra_options,
@@ -544,29 +559,29 @@
         )
 
     if extra_options.get("SmoothQuant", False):
         sq_path.cleanup()
 
 
 def quantize_dynamic(
-    model_input: Path,
-    model_output: Path,
+    model_input: Union[str, Path, onnx.ModelProto],
+    model_output: Union[str, Path],
     op_types_to_quantize=None,
     per_channel=False,
     reduce_range=False,
     weight_type=QuantType.QInt8,
     nodes_to_quantize=None,
     nodes_to_exclude=None,
     use_external_data_format=False,
     extra_options=None,
 ):
     """Given an onnx model, create a quantized onnx model and save it into a file
 
     Args:
-        model_input: file path of model to quantize
+        model_input: file path of model or ModelProto to quantize
         model_output: file path of quantized model
         op_types_to_quantize:
             specify the types of operators to quantize, like ['Conv'] to quantize Conv only.
             It quantizes all supported operators by default.
         per_channel: quantize weights per channel
         reduce_range:
             quantize weights with 7-bits. It may improve the accuracy for some models running on non-VNNI machine,
@@ -607,15 +622,19 @@
     op_types_to_quantize = op_types_to_quantize or []
 
     mode = QuantizationMode.IntegerOps
 
     if not op_types_to_quantize or len(op_types_to_quantize) == 0:
         op_types_to_quantize = list(IntegerOpsRegistry.keys())
 
-    model = load_model_with_shape_infer(Path(model_input))
+    model = (
+        save_and_reload_model_with_shape_infer(model_input)
+        if isinstance(model_input, onnx.ModelProto)
+        else load_model_with_shape_infer(Path(model_input))
+    )
 
     pre_processed: bool = model_has_pre_process_metadata(model)
     if not pre_processed:
         logging.warning(
             "Please consider to run pre-processing before quantization. Refer to example: "
             "https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification"
             "/cpu/ReadMe.md "
@@ -640,23 +659,23 @@
     )
 
     quantizer.quantize_model()
     quantizer.model.save_model_to_file(model_output, use_external_data_format)
 
 
 def quantize(
-    model_input: Path,
-    model_output: Path,
+    model_input: Union[str, Path, onnx.ModelProto],
+    model_output: Union[str, Path],
     quant_config: QuantConfig,
 ):
     """Quantize a model with QuantConfig.
 
     Args:
-        model_input (Path): Path to the model to quantize.
-        model_output (Path): Path to save the quantized model.
+        model_input (str | Path | ModelProto): Path to the model or ModelProto to quantize.
+        model_output (str | Path): Path to save the quantized model.
         quant_config (QuantConfig): Quantization Configuration.
     """
 
     if isinstance(quant_config, StaticQuantConfig):
         quantize_static(
             model_input,
             model_output,
```

## onnxruntime/quantization/registry.py

```diff
@@ -14,15 +14,15 @@
 from .operators.matmul import MatMulInteger, QDQMatMul, QLinearMatMul
 from .operators.maxpool import QDQMaxPool, QMaxPool
 from .operators.norm import QDQNormalization
 from .operators.pad import QPad
 from .operators.pooling import QLinearPool
 from .operators.qdq_base_operator import QDQOperatorBase
 from .operators.resize import QDQResize, QResize
-from .operators.softmax import QDQSoftmax, QLinearSoftmax
+from .operators.softmax import QLinearSoftmax
 from .operators.split import QDQSplit, QSplit
 from .operators.where import QDQWhere, QLinearWhere
 from .quant_utils import QuantizationMode
 
 CommonOpsRegistry = {
     "Gather": GatherQuant,
     "Transpose": Direct8BitOp,
@@ -75,15 +75,14 @@
     "Unsqueeze": QDQDirect8BitOp,
     "Resize": QDQResize,
     "MaxPool": QDQMaxPool,
     "AveragePool": QDQDirect8BitOp,
     "MatMul": QDQMatMul,
     "Split": QDQSplit,
     "Gather": QDQGather,
-    "Softmax": QDQSoftmax,
     "Where": QDQWhere,
     "InstanceNormalization": QDQNormalization,
     "LayerNormalization": QDQNormalization,
 }
 
 
 def CreateDefaultOpQuantizer(onnx_quantizer, node):  # noqa: N802
```

## onnxruntime/quantization/shape_inference.py

```diff
@@ -5,45 +5,47 @@
 # --------------------------------------------------------------------------
 
 
 import logging
 import tempfile
 import traceback
 from pathlib import Path
-from typing import Optional
+from typing import Optional, Union
 
 import onnx
 
 import onnxruntime
 from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference
+from onnxruntime.transformers.onnx_utils import extract_raw_data_from_model, has_external_data
 
 from .quant_utils import add_pre_process_metadata
 
 logger = logging.getLogger(__name__)
 
 
 def quant_pre_process(
-    input_model_path: str,
-    output_model_path: str,
+    input_model: Optional[Union[str, Path, onnx.ModelProto]] = None,
+    output_model_path: Optional[Union[str, Path]] = None,
     skip_optimization: bool = False,
     skip_onnx_shape: bool = False,
     skip_symbolic_shape: bool = False,
     auto_merge: bool = False,
     int_max: int = 2**31 - 1,
     guess_output_rank: bool = False,
     verbose: int = 0,
     save_as_external_data: bool = False,
     all_tensors_to_one_file: bool = False,
     external_data_location: Optional[str] = None,
     external_data_size_threshold: int = 1024,
+    **deprecated_kwargs,
 ) -> None:
     """Shape inference and model optimization, in preparation for quantization.
 
     Args:
-        input_model_path: Path to the input model file")
+        input_model: Path to the input model file or ModelProto
         output_model_path: Path to the output model file
         skip_optimization: Skip model optimization step if true. This may result in ONNX shape
             inference failure for some models.
         skip_onnx_shape: Skip ONNX shape inference. Symbolic shape inference is most effective
             with transformer based models. Skipping all shape inferences may
             reduce the effectiveness of quantization, as a tensor with unknown
             shape can not be quantized.
@@ -58,89 +60,120 @@
         guess_output_rank: Guess output rank to be the same as input 0 for unknown ops
         verbose: Logs detailed info of inference, 0: turn off, 1: warnings, 3: detailed
         save_as_external_data: Saving an ONNX model to external data
         all_tensors_to_one_file: Saving all the external data to one file
         external_data_location: The file location to save the external file
         external_data_size_threshold: The size threshold for external data
     """
+
+    if input_model is None:
+        input_model = deprecated_kwargs.pop("input_model_path", None)
+    assert input_model is not None
+
+    assert output_model_path is not None, "output_model_path is required."
+
     with tempfile.TemporaryDirectory(prefix="pre.quant.") as quant_tmp_dir:
         temp_path = Path(quant_tmp_dir)
         model = None
 
         if not skip_symbolic_shape:
             logger.info("Performing symbolic shape inference...")
+            loaded_model = input_model if isinstance(input_model, onnx.ModelProto) else onnx.load(input_model)
             model = SymbolicShapeInference.infer_shapes(
-                onnx.load(input_model_path),
+                loaded_model,
                 int_max,
                 auto_merge,
                 guess_output_rank,
                 verbose,
             )
 
         if not skip_optimization:
             # Use ORT optimizers (native code) to optimize model
             if not skip_symbolic_shape:
                 # Need to save the inferenced model to file so as to run the optimizer
-                input_model_path = str(temp_path / "symbolic_shape_inferred.onnx")
+                input_model = str(temp_path / "symbolic_shape_inferred.onnx")
                 if save_as_external_data:
                     onnx.save_model(
                         model,
-                        input_model_path,
+                        input_model,
                         save_as_external_data=True,
                         all_tensors_to_one_file=all_tensors_to_one_file,
                         size_threshold=external_data_size_threshold,
                         convert_attribute=False,
                     )
                 else:
-                    onnx.save(model, input_model_path)
+                    onnx.save(model, input_model)
                 model = None
 
             opt_model_path = str(temp_path / "optimized.onnx")
             try:
                 sess_option = onnxruntime.SessionOptions()
                 sess_option.optimized_model_filepath = opt_model_path
                 sess_option.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
-                sess = onnxruntime.InferenceSession(input_model_path, sess_option, providers=["CPUExecutionProvider"])
+                # For large model, extract external data from model and add to session options
+                if isinstance(input_model, onnx.ModelProto):
+                    if has_external_data(input_model):
+                        raise ValueError(
+                            "ModelProto has external data not loaded into memory, ORT cannot create session. "
+                            "Please load external data before calling this function. "
+                            "See https://onnx.ai/onnx/repo-docs/ExternalData.html for more information."
+                        )
+                    external_names, external_values = extract_raw_data_from_model(input_model)
+                    sess_option.add_external_initializers(list(external_names), list(external_values))
+                    input_model = input_model.SerializeToString()
+
+                sess = onnxruntime.InferenceSession(input_model, sess_option, providers=["CPUExecutionProvider"])
                 # Close the session to avoid the cleanup error on Windows for temp folders
                 # https://github.com/microsoft/onnxruntime/issues/17627
                 del sess
             except Exception:
                 logger.error(
                     "ONNX Runtime Model Optimization Failed! Consider rerun with option `--skip_optimization'."
                 )
                 logger.error(traceback.format_exc())
 
-            input_model_path = opt_model_path
+            input_model = opt_model_path
 
         if not skip_onnx_shape:
             # ONNX shape inference.
             # According to docs, infer_shapes_path should be used for 2G+ models.
             # If the skip optimization is specified, we could be dealing with a
             # large model. So be on the safe side, save the model
             if model is not None:
-                input_model_path = str(temp_path / "symbolic_shape_inferred.onnx")
+                input_model = str(temp_path / "symbolic_shape_inferred.onnx")
                 if save_as_external_data:
                     onnx.save_model(
                         model,
-                        input_model_path,
+                        input_model,
                         save_as_external_data=True,
                         all_tensors_to_one_file=all_tensors_to_one_file,
                         size_threshold=external_data_size_threshold,
                         convert_attribute=False,
                     )
                 else:
-                    onnx.save(model, input_model_path)
+                    onnx.save(model, input_model)
                 model = None
 
+            if isinstance(input_model, onnx.ModelProto):
+                input_model = str(Path(quant_tmp_dir) / "model_input.onnx")
+                onnx.save_model(
+                    model,
+                    input_model,
+                    save_as_external_data=True,
+                    all_tensors_to_one_file=all_tensors_to_one_file,
+                    size_threshold=external_data_size_threshold,
+                    convert_attribute=False,
+                )
+
             inferred_model_path = str(temp_path / "onnx_shape_inferred.onnx")
-            onnx.shape_inference.infer_shapes_path(input_model_path, inferred_model_path)
+            onnx.shape_inference.infer_shapes_path(input_model, inferred_model_path)
             model = onnx.load(inferred_model_path)
 
     if model is None:
-        model = onnx.load(input_model_path)
+        model = input_model if isinstance(input_model, onnx.ModelProto) else onnx.load(input_model)
 
     add_pre_process_metadata(model)
 
     if save_as_external_data:
         onnx.save_model(
             model,
             output_model_path,
```

## onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py

```diff
@@ -118,10 +118,15 @@
 
         subgraph_nodes = [reduce_node, clip_node, expand_node, div_node]
         if not self.is_safe_to_fuse_nodes(subgraph_nodes, [subgraph_output], input_name_to_nodes, output_name_to_node):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
         fused_node = onnx.helper.make_node(
-            self.fused_op_type, inputs=[subgraph_input], outputs=[subgraph_output], p=2, axis=-1
+            self.fused_op_type,
+            name=self.create_unique_node_name(),
+            inputs=[subgraph_input],
+            outputs=[subgraph_output],
+            p=2,
+            axis=-1,
         )
         self.nodes_to_add.append(fused_node)
```

## onnxruntime/quantization/execution_providers/qnn/preprocess.py

```diff
@@ -1,25 +1,92 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
+from __future__ import annotations
+
 import logging
 from pathlib import Path
 
 import onnx
 
 from ...fusions import FusionGelu, FusionLayerNormalization
 from ...onnx_model import ONNXModel
 from .fusion_lpnorm import FusionLpNormalization
 
 
-def qnn_preprocess_model(model_input: Path, model_output: Path, fuse_layernorm: bool = False) -> bool:
+def qnn_preprocess_model(
+    model_input: str | Path | onnx.ModelProto,
+    model_output: str | Path,
+    fuse_layernorm: bool = False,
+    save_as_external_data: bool = False,
+    all_tensors_to_one_file: bool = False,
+    external_data_location: str | None = None,
+    external_data_size_threshold: int = 1024,
+    external_data_convert_attribute: bool = False,
+    inputs_to_make_channel_last: list[str] | None = None,
+    outputs_to_make_channel_last: list[str] | None = None,
+) -> bool:
+    """
+    If necessary, this method creates a new "pre-processed" model in preparation for
+    quantization of a model to be used in QNN EP. Returns true if a new model was created.
+
+    This method perfoms the following operations:
+    - Fuse Erf sequence into a single Gelu node.
+    - Fuse ReduceL2 sequence into a single LpNormalization node (p == 2).
+    - (Optional) Fuse ReduceMean sequence into a single LayerNormalization node.
+
+    Args:
+        model_input: Path to the input model file or ModelProto.
+        model_output: Path the output model file, which is only created if this method returns True.
+        fuse_layernorm: True if ReduceMean sequences should be fused into LayerNormalization nodes.
+            Defaults to False.
+        save_as_external_data: True if output model should be saved with external data. Defaults to false.
+        all_tensors_to_one_file: Effective only if save_as_external_data is true. Defaults to false.
+            If true, save all tensors to one external file specified by external_data_location.
+            If false, save each tensor to a file named with the tensor name.
+        external_data_location: Effective only if save_as_external_data is true. Defaults to None.
+            Specify the external file to which all tensors are saved. Path is relative
+            to the model path. If not specified, the model's name is used.
+        external_data_size_threshold: Effective only if save_as_external_data is true. Defaults to 1024.
+            Tensors with a data size >= external_data_size_threshold are converted to external data.
+            To convert every tensor with raw data to external data, set to 0.
+        external_data_convert_attribute: Effective only if save_as_external_data is true. Defaults to false.
+            If true, convert all tensors to external data.
+            If false, convert only non-attribute tensors to external data.
+        inputs_to_make_channel_last: List of graph input names to transpose to be "channel-last". For example,
+            if "input0" originally has the shape (N, C, D1, D2, ..., Dn), the resulting model will change input0's
+            shape to (N, D1, D2, ..., Dn, C) and add a transpose node after it.
+
+            Original:
+                input0 (N, C, D1, D2, ..., Dn) --> <Nodes>
+
+            Updated:
+                input0 (N, D1, D2, ..., Dn, C) --> Transpose --> input0_chanfirst (N, C, D1, D2, ..., Dn) --> <Nodes>
+
+            This can potentially improve inference latency for QDQ models running on QNN EP because the
+            additional transpose node may allow other transpose nodes inserted during ORT layout transformation
+            to cancel out.
+        outputs_to_make_channel_last: List of graph output names to transpose to be "channel-last". For example,
+            if "output0" originally has the shape (N, C, D1, D2, ..., Dn), the resulting model will change output0's
+            shape to (N, D1, D2, ..., Dn, C) and add a transpose node before it.
+
+            Original:
+                <Nodes> --> output0 (N, C, D1, D2, ..., Dn)
+
+            Updated:
+                <Nodes> --> output0_chanfirst (N, C, D1, D2, ..., Dn) --> Transpose --> output0 (N, D1, D2, ..., Dn, C)
+
+            This can potentially improve inference latency for QDQ models running on QNN EP because the
+            additional transpose node may allow other transpose nodes inserted during ORT layout transformation
+            to cancel out.
+    """
     modified = False
-    model = onnx.load_model(model_input)
+    model = model_input if isinstance(model_input, onnx.ModelProto) else onnx.load_model(model_input)
     onnx_model = ONNXModel(model)
 
     # Fuse Erf sequence into a single Gelu
     fusion_gelu = FusionGelu(onnx_model)
     if fusion_gelu.apply():
         modified = True
 
@@ -40,12 +107,201 @@
                 f"but found version {onnx_opset.version}. Please use onnx.version_converter to update your model."
             )
         else:
             fusion_layernorm = FusionLayerNormalization(onnx_model)
             if fusion_layernorm.apply():
                 modified = True
 
+    # Optionally, transpose inputs and/or outputs to make them "channel-last".
+    if inputs_to_make_channel_last or outputs_to_make_channel_last:
+        transpose_node_prefix = "Transpose_channel_"
+        transpose_node_suffix: int = onnx_model.get_largest_node_name_suffix(transpose_node_prefix) + 1
+        update_io_to_channel_last(
+            onnx_model.model,
+            inputs_to_make_channel_last,
+            outputs_to_make_channel_last,
+            transpose_node_name_prefix=transpose_node_prefix,
+            transpose_node_name_start_suffix=transpose_node_suffix,
+        )
+        modified = True
+
+    # Make sure all nodes have a name.
+    unnamed_node_prefix = "qnn_preproc_node_"
+    available_suffix = onnx_model.get_largest_node_name_suffix(unnamed_node_prefix) + 1
+    for node in onnx_model.model.graph.node:
+        if node.op_type != "Constant" and not node.name:
+            new_node_name = f"{unnamed_node_prefix}{available_suffix!s}"
+            available_suffix += 1
+            node.name = new_node_name
+            modified = True
+            logging.warning(f"Node of type {node.op_type} does not have a name. Renamed to {new_node_name}.")
+
     if modified:
         onnx_model.topological_sort()
-        onnx.save_model(model, model_output)
+        onnx.save_model(
+            model,
+            model_output,
+            save_as_external_data=save_as_external_data,
+            all_tensors_to_one_file=all_tensors_to_one_file,
+            location=external_data_location,
+            size_threshold=external_data_size_threshold,
+            convert_attribute=external_data_convert_attribute,
+        )
 
     return modified
+
+
+class InputOutputNameMap:
+    def __init__(
+        self,
+        orig_tensor_names: set[str],
+        orig_graph_inputs: dict[str, onnx.ValueInfoProto],
+        orig_graph_outputs: dict[str, onnx.ValueInfoProto],
+    ):
+        self.orig_tensor_names = orig_tensor_names
+        self.orig_graph_inputs = orig_graph_inputs
+        self.orig_graph_outputs = orig_graph_outputs
+        self.updated_io_names = {}
+        self.new_value_infos = []
+
+    def get_new_name(self, orig_name: str):
+        if orig_name in self.updated_io_names:
+            return self.updated_io_names[orig_name]
+
+        # Make a new tensor name that is unique among all tensors in the graph.
+        prefix: str = f"{orig_name}_channel_first_"
+        suffix: int = -1
+        for tensor_name in self.orig_tensor_names:
+            if tensor_name.startswith(prefix) and tensor_name[len(prefix) :].isdigit():
+                index = int(tensor_name[len(prefix) :])
+                suffix = max(suffix, index)
+
+        suffix += 1  # This is the first available suffix.
+        new_name = f"{prefix}{suffix!s}"
+
+        # Add new value_info objects for these new tensors.
+        orig_value_info = self.orig_graph_inputs.get(orig_name) or self.orig_graph_outputs[orig_name]
+        value_info_proto = onnx.ValueInfoProto()
+        value_info_proto.CopyFrom(orig_value_info)
+        value_info_proto.name = new_name
+        self.new_value_infos.append(value_info_proto)
+
+        self.updated_io_names[orig_name] = new_name
+        return self.updated_io_names[orig_name]
+
+
+def update_io_to_channel_last(
+    model: onnx.ModelProto,
+    inputs_to_update: list[str] | None,
+    outputs_to_update: list[str] | None,
+    transpose_node_name_prefix: str = "Transpose_channel_",
+    transpose_node_name_start_suffix: int = 0,
+):
+    inputs_to_update = set(inputs_to_update or [])
+    outputs_to_update = set(outputs_to_update or [])
+
+    if not inputs_to_update and not outputs_to_update:
+        return
+
+    graph = model.graph
+    orig_graph_inputs = {ginput.name: ginput for ginput in graph.input}
+    orig_graph_outputs = {goutput.name: goutput for goutput in graph.output}
+
+    # Check that the user passed in actual input and output names.
+    for input_name in inputs_to_update:
+        if input_name not in orig_graph_inputs:
+            raise ValueError(f"{input_name} is not a graph input")
+
+    for output_name in outputs_to_update:
+        if output_name not in orig_graph_outputs:
+            raise ValueError(f"{output_name} is not a graph output")
+
+    orig_tensor_names = set()
+    orig_tensor_names.update(set(orig_graph_inputs))
+    orig_tensor_names.update(set(orig_graph_outputs))
+    orig_tensor_names.update(input_name for node in graph.node for input_name in node.input if input_name)
+
+    # Maps original input (or output) name to its updated name used within the graph.
+    io_map = InputOutputNameMap(orig_tensor_names, orig_graph_inputs, orig_graph_outputs)
+
+    # Update each node's inputs/outputs to use the transposed versions.
+    for node in graph.node:
+        for i in range(len(node.input)):
+            if node.input[i] and node.input[i] in inputs_to_update:
+                node.input[i] = io_map.get_new_name(node.input[i])
+            elif node.input[i] and node.input[i] in outputs_to_update:
+                node.input[i] = io_map.get_new_name(node.input[i])
+
+        for i in range(len(node.output)):
+            if node.output[i] in outputs_to_update:
+                node.output[i] = io_map.get_new_name(node.output[i])
+
+    # Update graph inputs to channel-last and a Transpose (to channel-first) after each.
+    for g_input_name in inputs_to_update:
+        g_input = orig_graph_inputs[g_input_name]
+
+        if not g_input.type.HasField("tensor_type") or not g_input.type.tensor_type.HasField("shape"):
+            raise ValueError(f"Expected input {g_input.name} to have a tensor_type with a shape")
+
+        input_shape = g_input.type.tensor_type.shape
+        input_rank = len(input_shape.dim)
+
+        if input_rank < 3:
+            raise ValueError(f"Expected input {g_input.name} to be of rank >= 3")
+
+        channel_dim = onnx.TensorShapeProto.Dimension()
+        channel_dim.CopyFrom(input_shape.dim[1])
+        for i in range(1, input_rank - 1):
+            input_shape.dim[i].CopyFrom(input_shape.dim[i + 1])
+        input_shape.dim[input_rank - 1].CopyFrom(channel_dim)
+
+        transpose_perm = list(range(input_rank))
+        for i in range(input_rank):
+            transpose_perm[i] = i if i < 1 else i - 1
+        transpose_perm[1] = input_rank - 1
+
+        transpose_node = onnx.helper.make_node(
+            "Transpose",
+            name=f"{transpose_node_name_prefix}{transpose_node_name_start_suffix!s}",
+            inputs=[g_input.name],
+            outputs=[io_map.get_new_name(g_input.name)],
+            perm=transpose_perm,
+        )
+        transpose_node_name_start_suffix += 1
+
+        graph.node.extend([transpose_node])
+
+    # Update graph outputs to channel-last and a Transpose (from channel-first) before each.
+    for g_output_name in outputs_to_update:
+        g_output = orig_graph_outputs[g_output_name]
+        if not g_output.type.HasField("tensor_type") or not g_output.type.tensor_type.HasField("shape"):
+            raise ValueError(f"Expected output {g_output.name} to have a tensor_type with a shape")
+
+        output_shape = g_output.type.tensor_type.shape
+        output_rank = len(output_shape.dim)
+
+        if output_rank < 3:
+            raise ValueError(f"Expected output {g_output.name} to be of rank >= 3")
+
+        channel_dim = onnx.TensorShapeProto.Dimension()
+        channel_dim.CopyFrom(output_shape.dim[1])
+        for i in range(1, output_rank - 1):
+            output_shape.dim[i].CopyFrom(output_shape.dim[i + 1])
+        output_shape.dim[output_rank - 1].CopyFrom(channel_dim)
+
+        transpose_perm = list(range(output_rank))
+        for i in range(output_rank):
+            transpose_perm[i] = i if i == 0 else i + 1
+        transpose_perm[output_rank - 1] = 1
+
+        transpose_node = onnx.helper.make_node(
+            "Transpose",
+            name=f"{transpose_node_name_prefix}{transpose_node_name_start_suffix!s}",
+            inputs=[io_map.get_new_name(g_output.name)],
+            outputs=[g_output.name],
+            perm=transpose_perm,
+        )
+        transpose_node_name_start_suffix += 1
+
+        graph.node.extend([transpose_node])
+
+    graph.value_info.extend(io_map.new_value_infos)
```

## onnxruntime/quantization/execution_providers/qnn/quant_config.py

```diff
@@ -1,93 +1,376 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 # --------------------------------------------------------------------------
+from __future__ import annotations
+
+import copy
+import logging
 from pathlib import Path
+from typing import Any
 
 import numpy as np
 import onnx
 
 from ...calibrate import CalibrationDataReader, CalibrationMethod
 from ...quant_utils import QuantType
 from ...quantize import StaticQuantConfig
+from ...tensor_quant_overrides import TensorQuantOverridesHelper
+from .mixed_precision_overrides_utils import MixedPrecisionTensorQuantOverridesFixer
 
 Q16_TYPES = {QuantType.QInt16, QuantType.QUInt16}
 Q8_TYPES = {QuantType.QInt8, QuantType.QUInt8}
 OP_TYPES_TO_EXCLUDE = {"Cast"}
+MODEL_SIZE_THRESHOLD = 2147483648  # Quant model should use external data if >= 2GB
 
 
-def get_qnn_qdq_config(
-    model_input: Path,
-    calibration_data_reader: CalibrationDataReader,
-    calibrate_method=CalibrationMethod.MinMax,
-    activation_type=QuantType.QUInt8,
-    weight_type=QuantType.QUInt8,
-    per_channel=False,
+def warn_unable_to_override(
+    node: onnx.NodeProto,
+    what_str: str,
+    tensor_name: str,
+    io_kind: str,
 ):
-    if per_channel:
-        raise ValueError("QNN EP does not yet support per-channel quantization.")
+    logging.warning(
+        f"Unable to override {what_str} for {node.op_type} node's {io_kind} "
+        "because it has already been overridden! Check the initial quantization overrides provided "
+        "to get_qnn_qdq_config() if the generated QDQ model does not run on QNN EP. "
+        f"Node name: {node.name}, {io_kind} name: {tensor_name}"
+    )
 
-    # Process model nodes to setup overrides.
-    model = onnx.load_model(model_input)
+
+def get_qnn_qdq_config(
+    model_input: str | Path | onnx.ModelProto,
+    calibration_data_reader: CalibrationDataReader,
+    calibrate_method: CalibrationMethod = CalibrationMethod.MinMax,
+    activation_type: QuantType = QuantType.QUInt8,
+    weight_type: QuantType = QuantType.QUInt8,
+    per_channel: bool = False,
+    init_overrides: dict[str, list[dict[str, Any]]] | None = None,
+    add_qtype_converts: bool = True,
+    activation_symmetric: bool = False,
+    weight_symmetric: bool | None = None,
+) -> StaticQuantConfig:
+    """
+    Returns a static quantization configuration suitable for running QDQ models on QNN EP.
+    This is done primarily by setting tensor-level quantization overrides.
+
+    Params:
+        model_input: Path to the input model file or ModelProto.
+        calibration_data_reader: Calibration data reader.
+        calibrate_methode: The calibration method. Defaults to MinMax.
+        activation_type: The default activation quantization type. Defaults to QUInt8.
+        weight_type: The default weight quantization type. Defaults to QUInt8.
+        per_channel: Global option that determines if a fixed set of operator types should be quantized per-channel.
+            Defaults to false. Alternatively, use the tensor-level `init_overrides` to select individual operators
+            and their quantization axes.
+
+            If set, the quantization tool uses per-channel quantization for the following operator types and inputs:
+                - Conv:
+                    - input[1] on axis 0
+                    - input[2] (bias) on axis 0
+                - ConvTranspose:
+                    - input[1] on axis 1
+                    - input[2] (bias) on axis 0
+        init_overrides: Initial tensor-level quantization overrides. Defaults to None. This function updates of a copy
+            of these overrides with any necessary adjustments and includes them in the returned
+            configuration object (i.e., config.extra_options['TensorQuantOverrides']).
+
+            The key is a tensor name and the value is a list of dictionaries. For per-tensor quantization, the list
+            contains a single dictionary. For per-channel quantization, the list contains either a dictionary for
+            each channel in the tensor or a single dictionary that is assumed to apply to all channels. An 'axis'
+            key must be present in the first dictionary for per-channel quantization.
+
+            Each dictionary contains optional overrides with the following keys and values.
+                'quant_type' = QuantType : The tensor's quantization data type.
+                'axis' = Int             : The per-channel axis. Must be present for per-channel weights.
+                'scale' =  Float         : The scale value to use. Must also specify `zero_point` if set.
+                'zero_point' = Int       : The zero-point value to use. Must also specify `scale` is set.
+                'symmetric' = Bool       : If the tensor should use symmetric quantization. Invalid if also
+                                            set `scale` or `zero_point`.
+                'reduce_range' = Bool    : If the quantization range should be reduced. Invalid if also
+                                            set `scale` or `zero_point`. Only valid for initializers.
+                'rmax' = Float           : Override the maximum real tensor value in calibration data.
+                                            Invalid if also set `scale` or `zero_point`.
+                'rmin' = Float           : Override the minimum real tensor value in calibration data.
+                                            Invalid if also set `scale` or `zero_point`.
+                'convert' = Dict         : A nested dictionary with the same keys for an activation
+                                           tensor that should be converted to another quantization type.
+                'convert["recv_nodes"] = Set : Set of node names that consume the converted activation,
+                                               other nodes get the original type. If not specified,
+                                               assume all consumer nodes get the converted type.
+        add_qtype_converts: True if this function should automatically add "convert" entries to the provided
+            `init_overrides` to ensure that operators use valid input/output types (activations only).
+            Ex: if you override the output of an Add to 16-bit, this option ensures that the activation inputs
+            of the Add are also up-converted to 16-bit and that data types for surrounding ops are converted
+            appropriately. Refer to the documentation in mixed_precision_overrides_utils.py for additional details.
+        activation_symmetric: True if activations should be quantized symmetrically (i.e, rmax == -rmin) by default.
+            Defaults to false. For int8 and int16, this results in zero-point values of 0. For uint8 and uin16,
+            the zero-point values are 128 and 32,768, respectively.
+        weight_symmetric: True if weights should be quantized symmetrically (i.e., rmax == -rmin) by default.
+            Defaults to None. If set to None, weight_symmetric is assumed true if the weight_type is a signed int.
+
+    Returns:
+        A StaticQuantConfig object
+    """
+    if weight_symmetric is None:
+        weight_symmetric = weight_type in {QuantType.QInt8, QuantType.QInt16}
+
+    model = (
+        model_input
+        if isinstance(model_input, onnx.ModelProto)
+        else onnx.load_model(model_input, load_external_data=False)
+    )
 
     op_types = set()
-    tensor_quant_overrides = {}
+    model_has_external_data = False
+    name_to_initializer = {}
 
-    name_to_initializer = {initializer.name: initializer for initializer in model.graph.initializer}
+    # Build map of initializers (name -> initializer) and
+    # check if the model has external data.
+    for initializer in model.graph.initializer:
+        name_to_initializer[initializer.name] = initializer
+        if onnx.external_data_helper.uses_external_data(initializer):
+            model_has_external_data = True
+
+    overrides_helper = TensorQuantOverridesHelper(copy.deepcopy(init_overrides) if init_overrides else {})
+
+    if not overrides_helper.empty() and add_qtype_converts:
+        # Fix mixed-precision overrides.
+        overrides_fixer = MixedPrecisionTensorQuantOverridesFixer.create_from_model(
+            overrides_helper, model, activation_type
+        )
+        overrides_fixer.apply(activation_type, activation_symmetric)
+
+    # Setup quantization overrides for specific operator types to ensure compatibility with QNN EP.
+    qnn_compat = QnnCompatibilityOverrides(
+        activation_type,
+        weight_type,
+        activation_symmetric,
+        weight_symmetric,
+        per_channel,
+        overrides_helper,
+        name_to_initializer,
+    )
 
     for node in model.graph.node:
         op_types.add(node.op_type)
-
-        if node.op_type == "MatMul" and activation_type in Q16_TYPES and weight_type in Q8_TYPES:
-            weight_symmetric = weight_type == QuantType.QInt8
-
-            # Override initializers to use the weight_type
-            for input_name in node.input:
-                if input_name in name_to_initializer:
-                    tensor_quant_overrides[input_name] = [{"quant_type": weight_type, "symmetric": weight_symmetric}]
-        elif node.op_type == "LayerNormalization" and activation_type in Q16_TYPES and weight_type in Q8_TYPES:
-            weight_symmetric = weight_type == QuantType.QInt8
-
-            # Override initializers to use the weight_type. Don't override the bias input.
-            for i in range(2):
-                input_name = node.input[i]
-                if input_name in name_to_initializer:
-                    tensor_quant_overrides[input_name] = [{"quant_type": weight_type, "symmetric": weight_symmetric}]
-        elif node.op_type == "Sigmoid":
-            if activation_type == QuantType.QUInt16:
-                tensor_quant_overrides[node.output[0]] = [
-                    {"scale": np.array(1.0 / 65536.0, dtype=np.float32), "zero_point": np.array(0, dtype=np.uint16)}
-                ]
-            elif activation_type == QuantType.QInt16:
-                tensor_quant_overrides[node.output[0]] = [
-                    {"scale": np.array(1.0 / 32768.0, dtype=np.float32), "zero_point": np.array(0, dtype=np.int16)}
-                ]
-        elif node.op_type == "Tanh":
-            if activation_type == QuantType.QUInt16:
-                tensor_quant_overrides[node.output[0]] = [
-                    {"scale": np.array(1.0 / 32768.0, dtype=np.float32), "zero_point": np.array(32768, dtype=np.uint16)}
-                ]
-            elif activation_type == QuantType.QInt16:
-                tensor_quant_overrides[node.output[0]] = [
-                    {"scale": np.array(1.0 / 32768.0, dtype=np.float32), "zero_point": np.array(0, dtype=np.int16)}
-                ]
+        qnn_compat.process_node(node)
 
     extra_options = {
         "MinimumRealRange": 0.0001,
         "DedicatedQDQPair": False,  # Let ORT optimizer duplicate DQ nodes
-        "TensorQuantOverrides": tensor_quant_overrides,
+        "TensorQuantOverrides": overrides_helper.get_dict(),
+        "ActivationSymmetric": activation_symmetric,
+        "WeightSymmetric": weight_symmetric,
     }
 
-    # TODO: Remove this extra option once ORT uses an ONNX version that supports 16-bit Q/DQ ops.
-    if activation_type in Q16_TYPES or weight_type in Q16_TYPES:
-        extra_options["UseQDQContribOps"] = True
+    # ONNX opset < 21 does not support 16-bit quantization, so must use 'com.microsoft' domain
+    # on Q/DQ operators if using 16-bit quantization.
+    onnx_opset = next(x for x in model.opset_import if x.domain == "" or x.domain == "ai.onnx")
+    if onnx_opset.version < 21:
+        overrides_have_int16 = any(t in Q16_TYPES for t in overrides_helper.get_quant_types())
+        if activation_type in Q16_TYPES or weight_type in Q16_TYPES or overrides_have_int16:
+            extra_options["UseQDQContribOps"] = True
 
     return StaticQuantConfig(
         calibration_data_reader,
         calibrate_method=calibrate_method,
         activation_type=activation_type,
         weight_type=weight_type,
         op_types_to_quantize=list(op_types.difference(OP_TYPES_TO_EXCLUDE)),
+        per_channel=per_channel,
+        use_external_data_format=(model_has_external_data or model.ByteSize() >= MODEL_SIZE_THRESHOLD),
         extra_options=extra_options,
     )
+
+
+class QnnCompatibilityOverrides:
+    """
+    Helper that processes nodes to generate quantization overrides that make the resulting QDQ model
+    compatible with QNN EP.
+    """
+
+    def __init__(
+        self,
+        default_activation_qtype: QuantType,
+        default_weight_qtype: QuantType,
+        activation_symmetric: bool,
+        weight_symmetric: bool,
+        per_channel: bool,
+        overrides: TensorQuantOverridesHelper,
+        initializers: dict[str, onnx.TensorProto],
+    ):
+        self.default_activation_qtype = default_activation_qtype
+        self.default_weight_qtype = default_weight_qtype
+        self.activation_symmetric = activation_symmetric
+        self.weight_symmetric = weight_symmetric
+        self.per_channel = per_channel
+        self.overrides = overrides
+        self.initializers = initializers
+
+        self.process_fns = {
+            "MatMul": self._process_matmul,
+            "LayerNormalization": self._process_layernorm,
+            "Sigmoid": self._process_sigmoid,
+            "Tanh": self._process_tanh,
+        }
+
+    def process_node(self, node: onnx.NodeProto):
+        process_fn = self.process_fns.get(node.op_type)
+
+        if process_fn is not None:
+            process_fn(node)
+
+    def _make_static_inputs_use_default_weight_type(self, node: onnx.NodeProto):
+        """
+        Overrides initializer input(s) to use the default weight type if:
+        - The default weight type is 8-bit
+        - One of the inputs is a 16-bit activation
+        - The other input is an initializer (per-tensor quantized)
+
+        This is necessary because the quantization tool does not assign MatMul or LayerNorm initializer
+        inputs the default weight type. Instead, it assigns the default activation type.
+        """
+        if self.default_weight_qtype not in Q8_TYPES:
+            return
+
+        input_16bit_act_name = None
+        input_weight_name = None
+
+        # Loop through first 2 inputs to find a 16-bit activation and a (per-tensor) weight.
+        for i in range(2):
+            input_name = node.input[i]
+            if not input_name:
+                continue
+
+            is_weight = input_name in self.initializers
+            qtype_info = self.overrides.get_node_input_qtype_info(
+                input_name,
+                node.name,
+                default_qtype=None if is_weight else self.default_activation_qtype,
+            )
+
+            if qtype_info.axis is not None:
+                return  # Don't process MatMul with a per-channel quantized input.
+
+            if (
+                is_weight
+                and qtype_info.quant_type == self.default_weight_qtype
+                and qtype_info.symmetric == self.weight_symmetric
+            ):
+                return  # Return. Weight is already overridden to use the desired weight type.
+
+            if is_weight:
+                input_weight_name = input_name
+            elif qtype_info.quant_type in Q16_TYPES:
+                input_16bit_act_name = input_name
+
+        # Override initializer input to use the default weight type.
+        if input_16bit_act_name and input_weight_name:
+            did_update = self.overrides.update_tensor_overrides(
+                input_weight_name,
+                {"quant_type": self.default_weight_qtype, "symmetric": self.weight_symmetric},
+                overwrite=False,
+            )
+
+            if not did_update:
+                warn_unable_to_override(node, "quant_type/symmetric", input_weight_name, "input weight")
+
+    def _process_matmul(self, node: onnx.NodeProto):
+        assert node.op_type == "MatMul", f"Expected MatMul, but got {node.op_type}"
+
+        if not self.per_channel:
+            self._make_static_inputs_use_default_weight_type(node)
+            return
+
+        # QNN does not support per-channel MatMul. However, the ORT quantization tool attempts to use per-channel
+        # quantization for MatMul by default *if* the global per_channel setting is enabled. So, we need to
+        # provide explicit per-tensor quantization overrides for MatMul if per_channel is enabled and
+        # the user did not provide any other overrides.
+        for input_name in node.input:
+            is_weight_no_overrides = input_name in self.initializers and input_name not in self.overrides
+            if is_weight_no_overrides:
+                self.overrides.update_tensor_overrides(
+                    input_name,
+                    {"quant_type": self.default_weight_qtype, "symmetric": self.weight_symmetric},
+                )
+
+    def _process_layernorm(self, node: onnx.NodeProto):
+        assert node.op_type == "LayerNormalization", f"Expected LayerNormalization, but got {node.op_type}"
+
+        if not self.per_channel:
+            self._make_static_inputs_use_default_weight_type(node)
+            return
+
+        has_weight_no_overrides = node.input[1] in self.initializers and node.input[1] not in self.overrides
+        has_bias_no_overrides = (
+            len(node.input) > 2
+            and node.input[2]
+            and node.input[2] in self.initializers
+            and node.input[2] not in self.overrides
+        )
+
+        if has_weight_no_overrides or has_bias_no_overrides:
+            # TODO: Make bias input not per-channel. QNN needs it to be per-tensor, but quantizer
+            # tries to makes it per-channel if the weight is also per-channel.
+            raise ValueError(
+                "get_qnn_qdq_config() does not currently support the global per_channel option with LayerNormalization."
+                " Please try using custom overrides that make bias per-tensor quantized."
+            )
+
+    def _process_sigmoid(self, node: onnx.NodeProto):
+        """
+        Overrides 16-bit Sigmoid's output scale and zero-point as per QNN requirements.
+        """
+        assert node.op_type == "Sigmoid", f"Expected Sigmoid, but got {node.op_type}"
+        output_type = self.overrides.get_node_output_qtype_info(
+            node.output[0], self.default_activation_qtype
+        ).quant_type
+
+        if output_type == QuantType.QUInt16:
+            self.overrides.update_tensor_overrides(
+                node.output[0],
+                {
+                    "quant_type": output_type,
+                    "scale": np.array(1.0 / 65536.0, dtype=np.float32),
+                    "zero_point": np.array(0, dtype=np.uint16),
+                },
+            )
+        elif output_type == QuantType.QInt16:
+            self.overrides.update_tensor_overrides(
+                node.output[0],
+                {
+                    "quant_type": output_type,
+                    "scale": np.array(1.0 / 32768.0, dtype=np.float32),
+                    "zero_point": np.array(0, dtype=np.int16),
+                },
+            )
+
+    def _process_tanh(self, node: onnx.NodeProto):
+        """
+        Overrides 16-bit Tanh's output scale and zero-point as per QNN requirements.
+        """
+        assert node.op_type == "Tanh", f"Expected Tanh, but got {node.op_type}"
+        output_type = self.overrides.get_node_output_qtype_info(
+            node.output[0], self.default_activation_qtype
+        ).quant_type
+
+        if output_type == QuantType.QUInt16:
+            self.overrides.update_tensor_overrides(
+                node.output[0],
+                {
+                    "quant_type": output_type,
+                    "scale": np.array(1.0 / 32768.0, dtype=np.float32),
+                    "zero_point": np.array(32768, dtype=np.uint16),
+                },
+            )
+        elif output_type == QuantType.QInt16:
+            self.overrides.update_tensor_overrides(
+                node.output[0],
+                {
+                    "quant_type": output_type,
+                    "scale": np.array(1.0 / 32768.0, dtype=np.float32),
+                    "zero_point": np.array(0, dtype=np.int16),
+                },
+            )
```

## onnxruntime/quantization/fusions/fusion.py

```diff
@@ -20,14 +20,17 @@
     def __init__(self, model: ONNXModel, fused_op_type: str, search_op_type: str):
         self.search_op_type: str = search_op_type
         self.fused_op_type: str = fused_op_type
         self.model: ONNXModel = model
         self.nodes_to_remove: list = []
         self.nodes_to_add: list = []
 
+        self._new_node_name_prefix = self.fused_op_type + "_fused_" + self.search_op_type + "_"
+        self._new_node_name_suffix = None  # int|None used to create unique node names for the fused ops.
+
     def fuse(
         self,
         node: onnx.NodeProto,
         input_name_to_nodes: dict[str, list[onnx.NodeProto]],
         output_name_to_node: dict[str, onnx.NodeProto],
     ):
         """
@@ -53,14 +56,26 @@
         graph_updated = bool(self.nodes_to_remove or self.nodes_to_add)
 
         if graph_updated:
             self.model.remove_unused_constant()
 
         return graph_updated
 
+    def create_unique_node_name(self):
+        prefix = self._new_node_name_prefix
+
+        if self._new_node_name_suffix is None:
+            largest_suffix: int = self.model.get_largest_node_name_suffix(prefix)
+            self._new_node_name_suffix = largest_suffix + 1
+
+        new_name = f"{prefix}{self._new_node_name_suffix!s}"
+        self._new_node_name_suffix += 1
+
+        return new_name
+
     @staticmethod
     def is_safe_to_fuse_nodes(
         nodes_to_remove: list[onnx.NodeProto],
         keep_outputs: list[str],
         input_name_to_nodes: dict[str, list[onnx.NodeProto]],
         output_name_to_node: dict[str, onnx.NodeProto],
     ) -> bool:
@@ -82,19 +97,17 @@
             if attr.name == attribute_name:
                 value = onnx.helper.get_attribute_value(attr)
                 return value
         return None
 
     @staticmethod
     def input_index(node_output: str, child_node: onnx.NodeProto) -> int:
-        index = 0
-        for input_name in child_node.input:
+        for index, input_name in enumerate(child_node.input):
             if input_name == node_output:
                 return index
-            index += 1
         return -1
 
     @staticmethod
     def tensor_shape_to_list(tensor_type) -> list[int]:
         shape_list = []
         for d in tensor_type.shape.dim:
             if d.HasField("dim_value"):
```

## onnxruntime/quantization/fusions/fusion_gelu.py

```diff
@@ -108,15 +108,17 @@
             subgraph_output = mul_after_erf.output[0]
 
         subgraph_nodes = [div, erf_node, add_after_erf, mul_after_erf, mul_half]
         if not self.is_safe_to_fuse_nodes(subgraph_nodes, [subgraph_output], input_name_to_nodes, output_name_to_node):
             return False
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = onnx.helper.make_node("Gelu", inputs=[subgraph_input], outputs=[subgraph_output])
+        fused_node = onnx.helper.make_node(
+            "Gelu", name=self.create_unique_node_name(), inputs=[subgraph_input], outputs=[subgraph_output]
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         return True
 
     def fuse_2(
         self,
         erf_node: onnx.NodeProto,
@@ -169,30 +171,30 @@
         if self.find_constant_input(div, 1.4142, delta=0.001) != 1:
             sqrt_node = self.match_parent(div, "Sqrt", 1, output_name_to_node)
             if sqrt_node is None:
                 return False
             if not self.has_constant_input(sqrt_node, 2.0):
                 return False
 
-        root_node = self.model.get_parent(div, 0, output_name_to_node)
-        if root_node is None:
-            return False
+        subgraph_input = div.input[0]
 
-        if root_node.output[0] not in mul.input:
+        if subgraph_input not in mul.input:
             return False
 
         subgraph_nodes = [div, erf_node, add_after_erf, mul_after_erf, mul]
         if sqrt_node:
             subgraph_nodes.append(sqrt_node)
 
         if not self.is_safe_to_fuse_nodes(subgraph_nodes, [mul.output[0]], input_name_to_nodes, output_name_to_node):
             return False
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = onnx.helper.make_node("Gelu", inputs=[root_node.output[0]], outputs=[mul.output[0]])
+        fused_node = onnx.helper.make_node(
+            "Gelu", name=self.create_unique_node_name(), inputs=[subgraph_input], outputs=[mul.output[0]]
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         return True
 
     def fuse_3(
         self,
         erf_node: onnx.NodeProto,
@@ -235,35 +237,36 @@
         if first_mul is None:
             return False
 
         i = self.find_constant_input(first_mul, 0.7071067690849304, delta=0.001)
         if i < 0:
             return False
 
-        root_node = self.model.get_parent(first_mul, 0 if i == 1 else 1, output_name_to_node)
-        if root_node is None:
-            return False
+        root_input_index = 1 - i
+        subgraph_input = first_mul.input[root_input_index]
 
         if mul_half.output[0] not in input_name_to_nodes:
             return False
         children = input_name_to_nodes[mul_half.output[0]]
         if len(children) != 1 or children[0].op_type != "Mul":
             return False
         last_mul = children[0]
 
-        if not (last_mul.input[0] == root_node.output[0] or last_mul.input[1] == root_node.output[0]):
+        if not (last_mul.input[0] == subgraph_input or last_mul.input[1] == subgraph_input):
             return False
 
         subgraph_nodes = [first_mul, erf_node, add_after_erf, mul_half, last_mul]
         if not self.is_safe_to_fuse_nodes(
             subgraph_nodes,
             [last_mul.output[0]],
             input_name_to_nodes,
             output_name_to_node,
         ):
             return False
 
         self.nodes_to_remove.extend(subgraph_nodes)
-        fused_node = onnx.helper.make_node("Gelu", inputs=[root_node.output[0]], outputs=[last_mul.output[0]])
+        fused_node = onnx.helper.make_node(
+            "Gelu", name=self.create_unique_node_name(), inputs=[subgraph_input], outputs=[last_mul.output[0]]
+        )
         fused_node.domain = "com.microsoft"
         self.nodes_to_add.append(fused_node)
         return True
```

## onnxruntime/quantization/fusions/fusion_layernorm.py

```diff
@@ -123,12 +123,13 @@
         if not self.is_constant_with_specified_rank(bias_input, 1):
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
 
         normalize_node = onnx.helper.make_node(
             "LayerNormalization",
+            name=self.create_unique_node_name(),
             inputs=[reduce_mean_node.input[0], weight_input, bias_input],
             outputs=[last_add_node.output[0]],
         )
         normalize_node.attribute.extend([onnx.helper.make_attribute("epsilon", float(add_weight))])
         self.nodes_to_add.append(normalize_node)
```

## onnxruntime/quantization/operators/concat.py

```diff
@@ -26,15 +26,15 @@
             _,
         ) = self.quantizer._get_quantization_params(node.output[0])
         (
             q_input_names,
             zero_point_names,
             scale_names,
             nodes,
-        ) = self.quantizer.quantize_activation(node, [*range(0, len(node.input))])
+        ) = self.quantizer.quantize_activation(node, [*range(len(node.input))])
         if not data_found or q_input_names is None:
             return super().quantize()
 
         # Create an entry for output quantized value
         quantized_input_value = self.quantizer.quantized_value_map[node.input[0]]
         quantized_output_value = QuantizedValue(
             node.output[0],
@@ -48,15 +48,15 @@
         kwargs = {}
         for attribute in node.attribute:
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
         qnode_name = node.name + "_quant" if node.name else ""
 
         qlconcat_inputs = [output_scale_name, output_zp_name]
-        for i in range(0, len(q_input_names)):
+        for i in range(len(q_input_names)):
             qlconcat_inputs.extend([q_input_names[i], scale_names[i], zero_point_names[i]])
         qlconcat_node = onnx.helper.make_node(
             "QLinearConcat", qlconcat_inputs, [quantized_output_value.q_name], qnode_name, **kwargs
         )
 
         self.quantizer.new_nodes += nodes
         self.quantizer.new_nodes += [qlconcat_node]
```

## onnxruntime/quantization/operators/conv.py

```diff
@@ -242,14 +242,17 @@
         node = self.node
         assert node.op_type == "Conv" or node.op_type == "ConvTranspose"
 
         self.quantizer.quantize_activation_tensor(node.input[0])
         if not self.disable_qdq_for_node_output:
             self.quantizer.quantize_activation_tensor(node.output[0])
 
-        if self.quantizer.is_per_channel():
-            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], 0)
+        is_weight_per_channel, weight_axis = self.quantizer.is_tensor_per_channel(
+            node.input[1], default_axis=0 if node.op_type == "Conv" else 1
+        )
+        if is_weight_per_channel:
+            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], weight_axis)
         else:
             self.quantizer.quantize_weight_tensor(node.input[1])
 
         if len(node.input) == 3:
-            self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1])
+            self.quantizer.quantize_bias_tensor(node.name, node.input[2], node.input[0], node.input[1])
```

## onnxruntime/quantization/operators/direct_q8.py

```diff
@@ -69,10 +69,10 @@
     def __init__(self, onnx_quantizer, onnx_node):
         super().__init__(onnx_quantizer, onnx_node)
 
     def quantize(self):
         if self.quantizer.force_quantize_no_input_check:
             self.quantizer.quantize_activation_tensor(self.node.input[0])
             if not self.disable_qdq_for_node_output:
-                self.quantizer.quantize_activation_tensor(self.node.output[0], self.node.input[0])
+                self.quantizer.quantize_output_same_as_input(self.node.output[0], self.node.input[0], self.node.name)
         elif self.quantizer.is_tensor_quantized(self.node.input[0]) and not self.disable_qdq_for_node_output:
-            self.quantizer.quantize_activation_tensor(self.node.output[0], self.node.input[0])
+            self.quantizer.quantize_output_same_as_input(self.node.output[0], self.node.input[0], self.node.name)
```

## onnxruntime/quantization/operators/gather.py

```diff
@@ -55,10 +55,10 @@
 
     def quantize(self):
         node = self.node
         assert node.op_type == "Gather"
 
         if self.quantizer.is_valid_quantize_weight(node.input[0]) or self.quantizer.force_quantize_no_input_check:
             self.quantizer.quantize_activation_tensor(node.input[0])
-            self.quantizer.quantize_activation_tensor(node.output[0], node.input[0])
+            self.quantizer.quantize_output_same_as_input(node.output[0], node.input[0], node.name)
         elif self.quantizer.is_tensor_quantized(node.input[0]):
-            self.quantizer.quantize_activation_tensor(node.output[0], node.input[0])
+            self.quantizer.quantize_output_same_as_input(node.output[0], node.input[0], node.name)
```

## onnxruntime/quantization/operators/gemm.py

```diff
@@ -142,22 +142,25 @@
         node = self.node
         assert node.op_type == "Gemm"
 
         self.quantizer.quantize_activation_tensor(node.input[0])
         if not self.disable_qdq_for_node_output:
             self.quantizer.quantize_activation_tensor(node.output[0])
 
-        if self.quantizer.is_per_channel():
-            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], 0 if is_B_transposed(node) else 1)
+        is_weight_per_channel, weight_axis = self.quantizer.is_tensor_per_channel(
+            node.input[1], default_axis=0 if is_B_transposed(node) else 1
+        )
+        if is_weight_per_channel:
+            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], weight_axis)
         else:
             self.quantizer.quantize_weight_tensor(node.input[1])
 
         if len(node.input) == 3:
             if self.quantizer.is_input_a_initializer(node.input[2]):
-                self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1], get_beta(self.node))
+                self.quantizer.quantize_bias_tensor(
+                    node.name, node.input[2], node.input[0], node.input[1], get_beta(self.node)
+                )
                 set_default_beta(self.node)
             else:
                 logging.warning(
-                    "Bias of Gemm node '{}' is not constant. Please exclude this node for better performance.".format(
-                        self.node.name
-                    )
+                    f"Bias of Gemm node '{self.node.name}' is not constant. Please exclude this node for better performance."
                 )
```

## onnxruntime/quantization/operators/lstm.py

```diff
@@ -99,14 +99,16 @@
                 quant_recurrent_weight_tuple[2],
                 quant_recurrent_weight_tuple[1],
             ]
         )
 
         kwargs = {}
         for attribute in node.attribute:
+            if attribute.name == "layout":
+                continue
             kwargs.update(attribute_to_kwarg(attribute))
         kwargs["domain"] = ms_domain
 
         quant_lstm_name = "" if not node.name else node.name + "_quant"
         quant_lstm_node = onnx.helper.make_node("DynamicQuantizeLSTM", inputs, node.output, quant_lstm_name, **kwargs)
         self.quantizer.new_nodes.append(quant_lstm_node)
```

## onnxruntime/quantization/operators/matmul.py

```diff
@@ -215,13 +215,14 @@
 
         if self.disable_qdq_for_node_output:
             nodes_to_iterate = node.input
         else:
             nodes_to_iterate = itertools.chain(node.input, node.output)
 
         for tensor_name in nodes_to_iterate:
-            # only support per-channel quantization on weight
-            if self.quantizer.is_per_channel() and find_by_name(tensor_name, self.quantizer.model.initializer()):
-                channel_axis = self.quantizer.qdq_op_type_per_channel_support_to_axis.get(node.op_type, 1)
+            is_per_channel, channel_axis = self.quantizer.is_tensor_per_channel(
+                tensor_name, default_axis=1, op_type=node.op_type
+            )
+            if is_per_channel:
                 self.quantizer.quantize_weight_tensor_per_channel(tensor_name, channel_axis)
             else:
                 self.quantizer.quantize_activation_tensor(tensor_name)
```

## onnxruntime/quantization/operators/norm.py

```diff
@@ -15,23 +15,26 @@
         assert node.op_type == "InstanceNormalization" or node.op_type == "LayerNormalization"
 
         # Input
         self.quantizer.quantize_activation_tensor(node.input[0])
 
         # Scale
         scale_is_initializer = self.quantizer.is_input_a_initializer(node.input[1])
+        scale_is_per_channel, scale_channel_axis = self.quantizer.is_tensor_per_channel(
+            node.input[1], default_axis=1, op_type=node.op_type
+        )
 
-        if self.quantizer.is_per_channel() and scale_is_initializer:
-            channel_axis = self.quantizer.qdq_op_type_per_channel_support_to_axis.get(node.op_type, 1)
-            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], axis=channel_axis)
+        if scale_is_per_channel:
+            self.quantizer.quantize_weight_tensor_per_channel(node.input[1], axis=scale_channel_axis)
         elif scale_is_initializer:
             self.quantizer.quantize_weight_tensor(node.input[1])
         else:
             self.quantizer.quantize_activation_tensor(node.input[1])
 
         # Bias
-        self.quantizer.quantize_bias_tensor(node.input[2], node.input[0], node.input[1])
+        if len(node.input) > 2 and node.input[2]:
+            self.quantizer.quantize_bias_tensor(node.name, node.input[2], node.input[0], node.input[1])
 
         # Output
         if not self.disable_qdq_for_node_output:
             for output_name in node.output:
                 self.quantizer.quantize_activation_tensor(output_name)
```

## onnxruntime/quantization/operators/pad.py

```diff
@@ -64,14 +64,15 @@
                     # TODO: check quantize_inputs after sub graph is supported
                     pad_value_qnodes = self.quantizer._get_quantize_input_nodes(
                         node,
                         2,
                         self.quantizer.activation_qType,
                         quantized_input_value.scale_name,
                         quantized_input_value.zp_name,
+                        initial_type=scale_tensor.data_type,
                     )
                     self.quantizer.new_nodes.extend(pad_value_qnodes)
                     node.input[2] = pad_value_qnodes[0].output[0]
             else:
                 # In quantized format, the `zero` before quantization is mapped
                 # to quantized_input_value.zp_name. Thus, padding 0 to
                 # original tensor should become padding zero point to quantized
```

## onnxruntime/quantization/operators/softmax.py

```diff
@@ -1,22 +1,12 @@
-import numpy as np
 import onnx
 import onnx.helper
 
-from ..quant_utils import (
-    TENSOR_NAME_QUANT_SUFFIX,
-    QuantizedValue,
-    QuantizedValueType,
-    attribute_to_kwarg,
-    compute_scale_zp,
-    get_qmin_qmax_for_qType,
-    ms_domain,
-)
+from ..quant_utils import TENSOR_NAME_QUANT_SUFFIX, QuantizedValue, QuantizedValueType, attribute_to_kwarg, ms_domain
 from .base_operator import QuantOperatorBase
-from .qdq_base_operator import QDQOperatorBase
 
 
 class QLinearSoftmax(QuantOperatorBase):
     def quantize(self):
         node = self.node
         # set limitations for softmax output scale and zp, because the output of softmax is always 0-1
         if self.quantizer.activation_qType == onnx.onnx_pb.TensorProto.UINT8:
@@ -78,33 +68,7 @@
             **kwargs,
         )
 
         # add all newly created nodes
         nodes.append(qnode)
         self.quantizer.new_nodes += nodes
         return None
-
-
-class QDQSoftmax(QDQOperatorBase):
-    def quantize(self):
-        super().quantize()
-        output_name = self.node.output[0]
-        quant_overrides = self.quantizer.get_per_tensor_quant_overrides(output_name)
-
-        quant_type = self.quantizer.activation_qType
-        if "quant_type" in quant_overrides:
-            quant_type = quant_overrides["quant_type"].tensor_type
-
-        if "scale" in quant_overrides and "zero_point" in quant_overrides:
-            out_zero_point, out_scale = quant_overrides["zero_point"], quant_overrides["scale"]
-        else:
-            # Unless overridden by the user, force Softmax to range from 0.0 to 1.0
-            qparams = self.quantizer.quantization_params[output_name]
-            dtype = qparams.data["scale"].dtype
-            rmin = quant_overrides.get("rmin", np.array(0, dtype=dtype))
-            rmax = quant_overrides.get("rmax", np.array(1, dtype=dtype))
-            symmetric = quant_overrides.get("symmetric", self.quantizer.is_activation_symmetric)
-            reduce_range = quant_overrides.get("reduce_range", False)
-            qmin, qmax = get_qmin_qmax_for_qType(quant_type, reduce_range=reduce_range, symmetric=symmetric)
-            out_zero_point, out_scale = compute_scale_zp(rmin, rmax, qmin, qmax, symmetric=symmetric)
-
-        self.quantizer.set_quant_scale_zp(output_name, (out_scale, out_zero_point))
```

## onnxruntime/quantization/operators/split.py

```diff
@@ -56,8 +56,8 @@
         node = self.node
         assert node.op_type == "Split"
 
         if not self.quantizer.is_tensor_quantized(node.input[0]):
             self.quantizer.quantize_activation_tensor(node.input[0])
         if not self.disable_qdq_for_node_output:
             for output in node.output:
-                self.quantizer.quantize_activation_tensor(output, node.input[0])
+                self.quantizer.quantize_output_same_as_input(output, node.input[0], node.name)
```

## onnxruntime/tools/convert_onnx_models_to_ort.py

```diff
@@ -298,17 +298,15 @@
     if target_platform is not None and target_platform == "arm":
         session_options_config_entries["session.qdqisint8allowed"] = "1"
     else:
         session_options_config_entries["session.qdqisint8allowed"] = "0"
 
     for optimization_style in optimization_styles:
         print(
-            "Converting models with optimization style '{}' and level '{}'".format(
-                optimization_style.name, optimization_level_str
-            )
+            f"Converting models with optimization style '{optimization_style.name}' and level '{optimization_level_str}'"
         )
 
         converted_models = _convert(
             model_path_or_dir=model_path_or_dir,
             output_dir=output_dir,
             optimization_level_str=optimization_level_str,
             optimization_style=optimization_style,
@@ -326,17 +324,17 @@
                 # without runtime optimizations to get a complete set of ops that may be needed for the config file.
                 model_dir = model_path_or_dir if model_path_or_dir.is_dir() else model_path_or_dir.parent
                 temp_output_dir = context_stack.enter_context(
                     tempfile.TemporaryDirectory(dir=model_dir, suffix=".without_runtime_opt")
                 )
                 session_options_config_entries_for_second_conversion = session_options_config_entries.copy()
                 # Limit the optimizations to those that can run in a model with runtime optimizations.
-                session_options_config_entries_for_second_conversion[
-                    "optimization.minimal_build_optimizations"
-                ] = "apply"
+                session_options_config_entries_for_second_conversion["optimization.minimal_build_optimizations"] = (
+                    "apply"
+                )
 
                 print(
                     "Converting models again without runtime optimizations to generate a complete config file. "
                     "These converted models are temporary and will be deleted."
                 )
                 converted_models += _convert(
                     model_path_or_dir=model_path_or_dir,
@@ -347,17 +345,15 @@
                     create_optimized_onnx_model=False,  # not useful as they would be created in a temp directory
                     allow_conversion_failures=allow_conversion_failures,
                     target_platform=target_platform,
                     session_options_config_entries=session_options_config_entries_for_second_conversion,
                 )
 
             print(
-                "Generating config file from ORT format models with optimization style '{}' and level '{}'".format(
-                    optimization_style.name, optimization_level_str
-                )
+                f"Generating config file from ORT format models with optimization style '{optimization_style.name}' and level '{optimization_level_str}'"
             )
 
             config_file = _create_config_file_path(
                 model_path_or_dir,
                 output_dir,
                 optimization_level_str,
                 optimization_style,
```

## onnxruntime/tools/symbolic_shape_infer.py

```diff
@@ -201,21 +201,24 @@
             "EmbedLayerNormalization": self._infer_EmbedLayerNormalization,
             "FastGelu": self._infer_FastGelu,
             "GatedRelativePositionBias": self._infer_GatedRelativePositionBias,
             "Gelu": self._infer_Gelu,
             "GemmFastGelu": self._infer_GemmFastGelu,
             "GemmFloat8": self._infer_GemmFloat8,
             "GroupNorm": self._infer_GroupNorm,
+            "GroupQueryAttention": self._infer_GroupQueryAttention,
+            "SparseAttention": self._infer_SparseAttention,
             "SkipGroupNorm": self._infer_SkipGroupNorm,
             "LayerNormalization": self._infer_LayerNormalization,
             "LongformerAttention": self._infer_LongformerAttention,
             "MultiHeadAttention": self._infer_MultiHeadAttention,
             "NhwcConv": self._infer_NhwcConv,
             "PackedAttention": self._infer_PackedAttention,
             "PackedMultiHeadAttention": self._infer_PackedMultiHeadAttention,
+            "PagedAttention": self._infer_PagedAttention,
             "PythonOp": self._infer_PythonOp,
             "QuantizeLinear": self._infer_QuantizeLinear,
             "QuickGelu": self._infer_FastGelu,
             "RelativePositionBias": self._infer_RelativePositionBias,
             "RemovePadding": self._infer_RemovePadding,
             "RestorePadding": self._infer_RestorePadding,
             "RotaryEmbedding": self._infer_RotaryEmbedding,
@@ -276,15 +279,15 @@
             for s in symbols:
                 if type(self.symbolic_dims_[s]) == sympy.Symbol:
                     map_to = s
                     break
         # when nothing to map to, use the shorter one
         if map_to is None:
             if self.verbose_ > 0:
-                logger.warning("Potential unsafe merge between symbolic expressions: ({})".format(",".join(symbols)))
+                logger.warning("Potential unsafe merge between symbolic expressions: (%s)", ",".join(symbols))
             symbols_list = list(symbols)
             lens = [len(s) for s in symbols_list]
             map_to = symbols_list[lens.index(min(lens))]
             symbols.remove(map_to)
 
         for s in symbols:
             if s == map_to:
@@ -329,30 +332,27 @@
                 unique_dims = list(set(dims))
                 is_int = [is_literal(d) for d in unique_dims]
                 assert sum(is_int) <= 1  # if there are more than 1 unique ints, something is wrong
                 if sum(is_int) == 1:
                     int_dim = is_int.index(1)
                     if self.verbose_ > 0:
                         logger.debug(
-                            "dim {} has been merged with value {}".format(
-                                unique_dims[:int_dim] + unique_dims[int_dim + 1 :],
-                                unique_dims[int_dim],
-                            )
+                            f"dim {unique_dims[:int_dim] + unique_dims[int_dim + 1 :]} has been merged with value {unique_dims[int_dim]}"
                         )
                     self._check_merged_dims(unique_dims, allow_broadcast=False)
                     return unique_dims[int_dim]
                 else:
                     if self.verbose_ > 0:
                         logger.debug(f"dim {unique_dims[1:]} has been merged with dim {unique_dims[0]}")
                     return dims[0]
             else:
                 return None
         if all([d == dims[0] for d in dims]):
             return dims[0]
-        merged = [self.suggested_merge_[d] if d in self.suggested_merge_ else d for d in dims]
+        merged = [self.suggested_merge_.get(d, d) for d in dims]
         if all([d == merged[0] for d in merged]):
             assert merged[0] in self.symbolic_dims_
             return merged[0]
         else:
             return None
 
     # broadcast from right to left, and merge symbolic dims if needed
@@ -373,15 +373,15 @@
                 if not new_dim:
                     # warning about unsupported broadcast when not auto merge
                     # note that auto merge has the risk of incorrectly merge symbols while one of them being 1
                     # for example, 'a' = 1, 'b' = 5 at runtime is valid broadcasting, but with auto merge 'a' == 'b'
                     if self.auto_merge_:
                         self._add_suggested_merge([dim1, dim2], apply=True)
                     else:
-                        logger.warning("unsupported broadcast between " + str(dim1) + " " + str(dim2))
+                        logger.warning("unsupported broadcast between " + str(dim1) + " " + str(dim2))  # noqa: G003
             new_shape = [new_dim, *new_shape]
         return new_shape
 
     def _get_shape(self, node, idx):
         name = node.input[idx]
         if name in self.known_vi_:
             vi = self.known_vi_[name]
@@ -465,17 +465,20 @@
             "RelativePositionBias",
             "RemovePadding",
             "RestorePadding",
             "SimplifiedLayerNormalization",
             "SkipLayerNormalization",
             "SkipSimplifiedLayerNormalization",
             "PackedAttention",
+            "PagedAttention",
             "PythonOp",
             "MultiHeadAttention",
             "GroupNorm",
+            "GroupQueryAttention",
+            "SparseAttention",
             "SkipGroupNorm",
             "BiasSplitGelu",
             "BiasAdd",
             "NhwcConv",
             "QuickGelu",
             "RotaryEmbedding",
         ]
@@ -490,14 +493,36 @@
             if (get_opset(self.out_mp_) >= 9) and node.op_type in ["Unsqueeze"]:
                 initializers = [
                     self.initializers_[name]
                     for name in node.input
                     if (name in self.initializers_ and name not in self.graph_inputs_)
                 ]
 
+            if node.op_type in [
+                "Add",
+                "Sub",
+                "Mul",
+                "Div",
+                "MatMul",
+                "MatMulInteger",
+                "MatMulInteger16",
+                "Where",
+                "Sum",
+            ]:
+                if node.output[0] in self.known_vi_:
+                    vi = self.known_vi_[node.output[0]]
+                    out_rank = len(get_shape_from_type_proto(vi.type))
+                    in_shapes = [self._get_shape(node, i) for i in range(len(node.input))]
+                    for d in range(
+                        out_rank - (2 if node.op_type in ["MatMul", "MatMulInteger", "MatMulInteger16"] else 0)
+                    ):
+                        in_dims = [s[len(s) - out_rank + d] for s in in_shapes if len(s) + d >= out_rank]
+                        if len(in_dims) > 1:
+                            self._check_merged_dims(in_dims, allow_broadcast=True)
+
             # run single node inference with self.known_vi_ shapes
             tmp_graph = helper.make_graph(
                 [node],
                 "tmp",
                 [self.known_vi_[i] for i in node.input if i],
                 [make_named_value_info(i) for i in node.output],
                 initializers,
@@ -655,20 +680,15 @@
         else:
             new_symbolic_dim = sympy.Symbol(new_dim, integer=True, nonnegative=True)
             self.symbolic_dims_[new_dim] = new_symbolic_dim
         return new_symbolic_dim
 
     def _new_symbolic_dim_from_output(self, node, out_idx=0, dim=0):
         return self._new_symbolic_dim(
-            "{}{}_{}_o{}_".format(
-                node.op_type,
-                self.prefix_,
-                list(self.out_mp_.graph.node).index(node),
-                out_idx,
-            ),
+            f"{node.op_type}{self.prefix_}_{list(self.out_mp_.graph.node).index(node)}_o{out_idx}_",
             dim,
         )
 
     def _new_symbolic_shape(self, rank, node, out_idx=0):
         return [self._new_symbolic_dim_from_output(node, out_idx, i) for i in range(rank)]
 
     def _compute_conv_pool_shape(self, node, channels_last=False):
@@ -816,25 +836,29 @@
                 data_shape[:-1] + indices_shape,
             )
         )
 
     def _infer_symbolic_compute_ops(self, node):
         funcs = {
             "Add": lambda l: l[0] + l[1],  # noqa: E741
-            "Div": lambda l: int(l[0] // l[1])  # noqa: E741
-            if isinstance(l[0] // l[1], float)
-            else l[0] // l[1],  # integer div in sympy
+            "Div": lambda l: (  # noqa: E741
+                int(l[0] // l[1]) if isinstance(l[0] // l[1], float) else l[0] // l[1]
+            ),  # integer div in sympy
             "Equal": lambda l: l[0] == l[1],  # noqa: E741
             "Floor": lambda l: sympy.floor(l[0]),  # noqa: E741
-            "Max": lambda l: l[1]  # noqa: E741
-            if is_literal(l[0]) and int(l[0]) < -self.int_max_
-            else (l[0] if is_literal(l[1]) and int(l[1]) < -self.int_max_ else sympy.Max(l[0], l[1])),
-            "Min": lambda l: l[1]  # noqa: E741
-            if is_literal(l[0]) and int(l[0]) > self.int_max_
-            else (l[0] if is_literal(l[1]) and int(l[1]) > self.int_max_ else sympy.Min(l[0], l[1])),
+            "Max": lambda l: (  # noqa: E741
+                l[1]
+                if is_literal(l[0]) and int(l[0]) < -self.int_max_
+                else (l[0] if is_literal(l[1]) and int(l[1]) < -self.int_max_ else sympy.Max(l[0], l[1]))
+            ),
+            "Min": lambda l: (  # noqa: E741
+                l[1]
+                if is_literal(l[0]) and int(l[0]) > self.int_max_
+                else (l[0] if is_literal(l[1]) and int(l[1]) > self.int_max_ else sympy.Min(l[0], l[1]))
+            ),
             "Mul": lambda l: int(l[0] * l[1]) if isinstance(l[0] * l[1], float) else l[0] * l[1],  # noqa: E741
             "Sub": lambda l: l[0] - l[1],  # noqa: E741
             "Where": lambda l: l[1] if l[0] else l[2],  # noqa: E741
             "Neg": lambda l: -l[0],  # noqa: E741
         }
         assert node.op_type in funcs
         self._compute_on_sympy_data(node, funcs[node.op_type])
@@ -1204,17 +1228,15 @@
                         si.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
                         so.type.tensor_type.shape.dim[di].CopyFrom(new_dim)
                         need_second_infer = True
 
         if need_second_infer:
             if self.verbose_ > 2:
                 logger.debug(
-                    "Rerun Loop: {}({}...), because of sequence in loop carried variables".format(
-                        node.name, node.output[0]
-                    )
+                    f"Rerun Loop: {node.name}({node.output[0]}...), because of sequence in loop carried variables"
                 )
             self._onnx_infer_subgraph(node, subgraph, inc_subgraph_id=False)
 
         # create a new symbolic dimension for iteration dependent dimension
         loop_iter_dim = str(self._new_symbolic_dim_from_output(node))
         for i in range(len(node.output)):
             vi = self.known_vi_[node.output[i]]
@@ -1468,17 +1490,19 @@
                 vi = self.known_vi_[node.output[i]]
                 vi.CopyFrom(
                     helper.make_tensor_value_info(
                         node.output[i],
                         output_dtype,
                         [
                             N if N is not None else str(self._new_symbolic_dim_from_output(node, i, 0)),
-                            as_scalar(group)
-                            if group is not None
-                            else str(self._new_symbolic_dim_from_output(node, i, 1)),
+                            (
+                                as_scalar(group)
+                                if group is not None
+                                else str(self._new_symbolic_dim_from_output(node, i, 1))
+                            ),
                         ],
                     )
                 )
 
     def _infer_aten_upsample(self, node):
         new_shape = None
         input_shape = self._get_shape(node, 0)
@@ -1829,15 +1853,15 @@
             steps = [1] * len(axes)
         else:
             starts = as_list(self._try_get_value(node, 1), keep_none=True)
             ends = as_list(self._try_get_value(node, 2), keep_none=True)
             axes = self._try_get_value(node, 3)
             steps = self._try_get_value(node, 4)
             if axes is None and not (starts is None and ends is None):
-                axes = list(range(0, len(starts if starts is not None else ends)))
+                axes = list(range(len(starts if starts is not None else ends)))
             if steps is None and not (starts is None and ends is None):
                 steps = [1] * len(starts if starts is not None else ends)
             axes = as_list(axes, keep_none=True)
             steps = as_list(steps, keep_none=True)
 
         new_sympy_shape = self._get_sympy_shape(node, 0)
         if starts is None or ends is None:
@@ -1926,16 +1950,25 @@
             data_shape = self._get_shape(node, 0)
             vi = self.known_vi_[node.output[1]]
             vi.CopyFrom(helper.make_tensor_value_info(vi.name, elem_type, data_shape))
 
     def _infer_Split_Common(self, node, make_value_info_func):  # noqa: N802
         input_sympy_shape = self._get_sympy_shape(node, 0)
         axis = handle_negative_axis(get_attribute(node, "axis", 0), len(input_sympy_shape))
-        split = get_attribute(node, "split")
-        if not split:
+        op_set = get_opset(self.out_mp_)
+
+        # Depending on op-version 'split' are provided as attribute or via 2nd input
+        if op_set < 13:
+            split = get_attribute(node, "split")
+            assert self._try_get_value(node, 1) is None
+        else:
+            split = self._try_get_value(node, 1)
+            assert get_attribute(node, "split") is None
+
+        if split is None:
             num_outputs = len(node.output)
             split = [input_sympy_shape[axis] / sympy.Integer(num_outputs)] * num_outputs
             self._update_computed_dims(split)
         else:
             split = [sympy.Integer(s) for s in split]
 
         for i_o in range(len(split)):
@@ -2406,14 +2439,48 @@
         # output for inference, infer the shape and type for it too
         if len(node.output) > 3:
             self._propagate_shape_and_type(node, 0, 3)
 
     def _infer_GroupNorm(self, node):  # noqa: N802
         self._propagate_shape_and_type(node)
 
+    def _infer_PagedAttention(self, node):  # noqa: N802
+        self._propagate_shape_and_type(node)
+
+    def _infer_GroupQueryAttention(self, node):  # noqa: N802
+        output_dtype = self.known_vi_[node.input[0]].type.tensor_type.elem_type
+
+        past_shape = self._try_get_shape(node, 3)
+        if past_shape is not None:
+            # When past and present has the maximum sequence length, we can propagate the shape from past to present.
+            # Note that GQA also supports different sequence lengths for past and present, but it is rarely used.
+            vi = self.known_vi_[node.output[1]]
+            vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, past_shape))
+            vi = self.known_vi_[node.output[2]]
+            vi.CopyFrom(helper.make_tensor_value_info(vi.name, output_dtype, past_shape))
+
+        if node.input[1] != "" and node.input[2] != "":
+            self._propagate_shape_and_type(node, 0, 0)
+        else:
+            # combined qkv: (batch_size, sequence_length, num_heads * head_size + 2 * kv_num_heads * head_size)
+            assert node.input[1] == "" and node.input[2] == ""
+            num_heads = get_attribute(node, "num_heads")
+            kv_num_heads = get_attribute(node, "kv_num_heads")
+            query_shape = self._get_shape(node, 0)
+            if query_shape is not None:
+                hidden_size = query_shape[2]
+                if isinstance(hidden_size, int):
+                    head_size = int(hidden_size / (num_heads + 2 * kv_num_heads))
+                    query_shape[2] = num_heads * head_size
+                    vi = self.known_vi_[node.output[0]]
+                    vi.CopyFrom(helper.make_tensor_value_info(node.output[0], output_dtype, query_shape))
+
+    def _infer_SparseAttention(self, node):  # noqa: N802
+        self._infer_GroupQueryAttention(node)
+
     def _infer_SkipGroupNorm(self, node):  # noqa: N802
         self._propagate_shape_and_type(node, 0, 0)
         if len(node.output) > 1:
             self._propagate_shape_and_type(node, 0, 1)
 
     def _infer_BiasSplitGelu(self, node):  # noqa: N802
         input_shape = self._get_shape(node, 0)
@@ -2617,19 +2684,17 @@
                         aten_op_name = attr.s.decode("utf-8") if isinstance(attr.s, bytes) else attr.s
                         if aten_op_name in self.aten_op_dispatcher_:
                             known_aten_op = True
                             self.aten_op_dispatcher_[aten_op_name](node)
                         break
 
             if self.verbose_ > 2:
-                logger.debug(node.op_type + ": " + node.name)
+                logger.debug(node.op_type + ": " + node.name)  # noqa: G003
                 for i, name in enumerate(node.input):
-                    logger.debug(
-                        "  Input {}: {} {}".format(i, name, "initializer" if name in self.initializers_ else "")
-                    )
+                    logger.debug("  Input %s: %s %s", i, name, "initializer" if name in self.initializers_ else "")
 
             # onnx automatically merge dims with value, i.e. Mul(['aaa', 'bbb'], [1000, 1]) -> [1000, 'bbb']
             # symbolic shape inference needs to apply merge of 'aaa' -> 1000 in this case
             if node.op_type in [
                 "Add",
                 "Sub",
                 "Mul",
@@ -2670,15 +2735,15 @@
                 # do not process shape for non-tensors
                 if out_type_kind not in ["tensor_type", "sparse_tensor_type", None]:
                     if self.verbose_ > 2:
                         if out_type_kind == "sequence_type":
                             seq_cls_type = out_type.sequence_type.elem_type.WhichOneof("value")
                             if seq_cls_type == "tensor_type":
                                 logger.debug(
-                                    "  {}: sequence of {} {}".format(
+                                    "  {}: sequence of {} {}".format(  # noqa: G001
                                         node.output[i_o],
                                         str(get_shape_from_value_info(vi)),
                                         onnx.TensorProto.DataType.Name(
                                             vi.type.sequence_type.elem_type.tensor_type.elem_type
                                         ),
                                     )
                                 )
@@ -2688,22 +2753,18 @@
                             logger.debug(f"  {node.output[i_o]}: {out_type_kind}")
                     continue
 
                 out_shape = get_shape_from_value_info(vi)
                 out_type_undefined = out_type.tensor_type.elem_type == onnx.TensorProto.UNDEFINED
                 if self.verbose_ > 2:
                     logger.debug(
-                        "  {}: {} {}".format(
-                            node.output[i_o],
-                            str(out_shape),
-                            onnx.TensorProto.DataType.Name(vi.type.tensor_type.elem_type),
-                        )
+                        f"  {node.output[i_o]}: {out_shape!s} {onnx.TensorProto.DataType.Name(vi.type.tensor_type.elem_type)}"
                     )
                     if node.output[i_o] in self.sympy_data_:
-                        logger.debug("  Sympy Data: " + str(self.sympy_data_[node.output[i_o]]))
+                        logger.debug("  Sympy Data: " + str(self.sympy_data_[node.output[i_o]]))  # noqa: G003
 
                 # onnx >= 1.11.0, use unk__#index instead of None when the shape dim is uncertain
                 if (
                     out_shape is not None and (None in out_shape or self._is_shape_contains_none_dim(out_shape))
                 ) or out_type_undefined:
                     if self.auto_merge_:
                         if node.op_type in [
@@ -2796,46 +2857,38 @@
                                     get_shape_from_sympy_shape(new_shape),
                                 )
                             )
 
                             if self.verbose_ > 0:
                                 if is_unknown_op:
                                     logger.debug(
-                                        "Possible unknown op: {} node: {}, guessing {} shape".format(
-                                            node.op_type, node.name, vi.name
-                                        )
+                                        f"Possible unknown op: {node.op_type} node: {node.name}, guessing {vi.name} shape"
                                     )
                                 if self.verbose_ > 2:
-                                    logger.debug(
-                                        "  {}: {} {}".format(
-                                            node.output[i_o],
-                                            str(new_shape),
-                                            vi.type.tensor_type.elem_type,
-                                        )
-                                    )
+                                    logger.debug(f"  {node.output[i_o]}: {new_shape!s} {vi.type.tensor_type.elem_type}")
 
                             self.run_ = True
                             continue  # continue the inference after guess, no need to stop as no merge is needed
 
                     if self.verbose_ > 0 or not self.auto_merge_ or out_type_undefined:
-                        logger.debug("Stopping at incomplete shape inference at " + node.op_type + ": " + node.name)
+                        logger.debug("Stopping at incomplete shape inference at %s: %s", node.op_type, node.name)
                         logger.debug("node inputs:")
                         for i in node.input:
                             if i in self.known_vi_:
                                 logger.debug(self.known_vi_[i])
                             else:
                                 logger.debug(f"not in known_vi_ for {i}")
                         logger.debug("node outputs:")
                         for o in node.output:
                             if o in self.known_vi_:
                                 logger.debug(self.known_vi_[o])
                             else:
                                 logger.debug(f"not in known_vi_ for {o}")
                         if self.auto_merge_ and not out_type_undefined:
-                            logger.debug("Merging: " + str(self.suggested_merge_))
+                            logger.debug("Merging: " + str(self.suggested_merge_))  # noqa: G003
                     return False
 
         self.run_ = False
         return True
 
     def _update_output_from_vi(self):
         for output in self.out_mp_.graph.output:
@@ -2912,17 +2965,17 @@
         default=1024,
     )
     return parser.parse_args()
 
 
 if __name__ == "__main__":
     args = parse_arguments()
-    logger.info("input model: " + args.input)
+    logger.info("input model: " + args.input)  # noqa: G003
     if args.output:
-        logger.info("output model " + args.output)
+        logger.info("output model " + args.output)  # noqa: G003
     logger.info("Doing symbolic shape inference...")
     out_mp = SymbolicShapeInference.infer_shapes(
         onnx.load(args.input),
         args.int_max,
         args.auto_merge,
         args.guess_output_rank,
         args.verbose,
```

## onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py

```diff
@@ -101,15 +101,15 @@
 
         # special case Constant as we will convert to an initializer during model load
         if domain == "ai.onnx" and node.op_type == "Constant":
             continue
 
         # some models don't have complete imports. use 1 as a default as that's valid for custom domains and should
         # result in an error for any others. not sure why ONNX or ORT validation allows this though.
-        opset = opsets[domain] if domain in opsets else 1
+        opset = opsets.get(domain, 1)
         if (
             domain not in required_ops
             or opset not in required_ops[domain]
             or node.op_type not in required_ops[domain][opset]
         ):
             unsupported_ops.add(f"{domain}:{opset}:{node.op_type}")
 
@@ -226,15 +226,15 @@
         unsupported_ops,
         logger,
     )
 
     if unsupported_ops:
         logger.info("Unsupported operators:")
         for entry in sorted(unsupported_ops):
-            logger.info("  " + entry)
+            logger.info("  " + entry)  # noqa: G003
 
     if unsupported:
         logger.info("\nModel is not supported by the pre-built package due to unsupported types and/or operators.")
         logger.info(
             "Please see https://onnxruntime.ai/docs/install/#install-on-web-and-mobile for information "
             "on what is supported in the pre-built package."
         )
```

## onnxruntime/tools/ort_format_model/operator_type_usage_processors.py

```diff
@@ -88,24 +88,22 @@
 
     @abstractmethod
     def to_config_entry(self):
         """
         Generate a configuration file entry in JSON format with the required types for the operator.
         :return: JSON string with required type information.
         """
-        pass
 
     @abstractmethod
     def from_config_entry(self, entry: str):
         """
         Re-create the types required from a configuration file entry created with to_config_entry.
         NOTE: Any existing type information should be cleared prior to re-creating from a config file entry.
         :param entry: Configuration file entry
         """
-        pass
 
 
 class DefaultTypeUsageProcessor(TypeUsageProcessor):
     """
     Operator processor which tracks the types used for selected input/s and/or output/s.
     """
 
@@ -178,17 +176,15 @@
                 type_str = value_name_to_typestr(node.Inputs(i), value_name_to_typeinfo)
                 self._input_types[i].add(type_str)
 
         for o in self._output_types:
             # Don't know of any ops where the number of outputs changed across versions, so require a valid length
             if o >= node.OutputsLength():
                 raise RuntimeError(
-                    "Node has {} outputs. Tracker for {} incorrectly configured as it requires {}.".format(
-                        node.OutputsLength(), self.name, o
-                    )
+                    f"Node has {node.OutputsLength()} outputs. Tracker for {self.name} incorrectly configured as it requires {o}."
                 )
 
             type_str = value_name_to_typestr(node.Outputs(o), value_name_to_typeinfo)
             self._output_types[o].add(type_str)
 
     def is_typed_registration_needed(
         self, type_in_registration: str, globally_allowed_types: typing.Optional[typing.Set[str]]
@@ -510,23 +506,21 @@
         """
         Given the string from a kernel registration, determine if the registration is required or not.
         :param domain: Operator domain.
         :param optype: Operator type.
         :param type_registration_str: Type string from kernel registration
         :return: True is required. False if not.
         """
-        pass
 
     @abstractmethod
     def get_cpp_entries(self):
         """
         Get the C++ code that specifies the operator types to enable.
         :return: List of strings. One line of C++ code per entry.
         """
-        pass
 
 
 class OperatorTypeUsageManager:
     """
     Class to manage the operator type usage processors.
     TODO: Currently the type tracking is not specific to a version of the operator.
     It's unclear how/where version specific logic could/should be added, and it would add significant complexity
@@ -640,17 +634,15 @@
     _valid_allowed_types = set(FbsTypeInfo.tensordatatype_to_string.values())  # noqa: RUF012
 
     def __init__(self, globally_allowed_types: typing.Set[str]):
         self._operator_processors = _create_operator_type_usage_processors()
 
         if not globally_allowed_types.issubset(self._valid_allowed_types):
             raise ValueError(
-                "Globally allowed types must all be valid. Invalid types: {}".format(
-                    sorted(globally_allowed_types - self._valid_allowed_types)
-                )
+                f"Globally allowed types must all be valid. Invalid types: {sorted(globally_allowed_types - self._valid_allowed_types)}"
             )
 
         self._globally_allowed_types = globally_allowed_types
 
     def is_typed_registration_needed(self, domain: str, optype: str, type_registration_str: str):
         key = _create_op_key(domain, optype)
         if key in self._operator_processors:
```

## onnxruntime/tools/ort_format_model/ort_model_processor.py

```diff
@@ -31,15 +31,15 @@
         We copy the current list which represents the outer scope values, and add the local node args to that
         to create the valid list of values for the current Graph.
         :param graph: Graph to create NodeArg list for
         :param outer_scope_value_typeinfo: TypeInfo for outer scope values. Empty for the top-level graph in a model.
         :return: Dictionary of NodeArg name to TypeInfo
         """
         value_name_to_typeinfo = outer_scope_value_typeinfo.copy()
-        for j in range(0, graph.NodeArgsLength()):
+        for j in range(graph.NodeArgsLength()):
             n = graph.NodeArgs(j)
             value_name_to_typeinfo[n.Name()] = n.Type()  # TypeInfo for this NodeArg's name
 
         return value_name_to_typeinfo
 
     def _add_required_op(self, domain: str, opset: int, op_type: str):
         if domain not in self._required_ops:
@@ -53,34 +53,34 @@
         """
         Process one level of the Graph, descending into any subgraphs when they are found
         :param outer_scope_value_typeinfo: Outer scope NodeArg dictionary from ancestor graphs
         """
         # Merge the TypeInfo for all values in this level of the graph with the outer scope value TypeInfo.
         value_name_to_typeinfo = OrtFormatModelProcessor._setup_type_info(graph, outer_scope_value_typeinfo)
 
-        for i in range(0, graph.NodesLength()):
+        for i in range(graph.NodesLength()):
             node = graph.Nodes(i)
 
             optype = node.OpType().decode()
             domain = node.Domain().decode() or "ai.onnx"  # empty domain defaults to ai.onnx
 
             self._add_required_op(domain, node.SinceVersion(), optype)
 
             if self._op_type_processors:
                 self._op_type_processors.process_node(node, value_name_to_typeinfo)
 
             # Read all the attributes
-            for j in range(0, node.AttributesLength()):
+            for j in range(node.AttributesLength()):
                 attr = node.Attributes(j)
                 attr_type = attr.Type()
                 if attr_type == fbs.AttributeType.AttributeType.GRAPH:
                     self._process_graph(attr.G(), value_name_to_typeinfo)
                 elif attr_type == fbs.AttributeType.AttributeType.GRAPHS:
                     # the ONNX spec doesn't currently define any operators that have multiple graphs in an attribute
                     # so entering this 'elif' isn't currently possible
-                    for k in range(0, attr.GraphsLength()):
+                    for k in range(attr.GraphsLength()):
                         self._process_graph(attr.Graphs(k), value_name_to_typeinfo)
 
     def process(self):
         graph = self._model.Graph()
         outer_scope_value_typeinfo = {}  # no outer scope values for the main graph
         self._process_graph(graph, outer_scope_value_typeinfo)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py

```diff
@@ -1,8 +1,7 @@
 # automatically generated by the FlatBuffers compiler, do not modify
 
 # namespace: fbs
 
 class ArgType(object):
     INPUT = 0
     OUTPUT = 1
-
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class ArgTypeAndIndex(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsArgTypeAndIndex(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = ArgTypeAndIndex()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsArgTypeAndIndex(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def ArgTypeAndIndexBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # ArgTypeAndIndex
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -34,11 +38,30 @@
     # ArgTypeAndIndex
     def Index(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Uint32Flags, o + self._tab.Pos)
         return 0
 
-def ArgTypeAndIndexStart(builder): builder.StartObject(2)
-def ArgTypeAndIndexAddArgType(builder, argType): builder.PrependInt8Slot(0, argType, 0)
-def ArgTypeAndIndexAddIndex(builder, index): builder.PrependUint32Slot(1, index, 0)
-def ArgTypeAndIndexEnd(builder): return builder.EndObject()
+def ArgTypeAndIndexStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    ArgTypeAndIndexStart(builder)
+
+def ArgTypeAndIndexAddArgType(builder, argType):
+    builder.PrependInt8Slot(0, argType, 0)
+
+def AddArgType(builder, argType):
+    ArgTypeAndIndexAddArgType(builder, argType)
+
+def ArgTypeAndIndexAddIndex(builder, index):
+    builder.PrependUint32Slot(1, index, 0)
+
+def AddIndex(builder, index):
+    ArgTypeAndIndexAddIndex(builder, index)
+
+def ArgTypeAndIndexEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return ArgTypeAndIndexEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Attribute(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsAttribute(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Attribute()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsAttribute(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def AttributeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Attribute
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -208,27 +212,126 @@
         return 0
 
     # Attribute
     def GraphsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(28))
         return o == 0
 
-def AttributeStart(builder): builder.StartObject(13)
-def AttributeAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def AttributeAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
-def AttributeAddType(builder, type): builder.PrependInt32Slot(2, type, 0)
-def AttributeAddF(builder, f): builder.PrependFloat32Slot(3, f, 0.0)
-def AttributeAddI(builder, i): builder.PrependInt64Slot(4, i, 0)
-def AttributeAddS(builder, s): builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(s), 0)
-def AttributeAddT(builder, t): builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(t), 0)
-def AttributeAddG(builder, g): builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(g), 0)
-def AttributeAddFloats(builder, floats): builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(floats), 0)
-def AttributeStartFloatsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def AttributeAddInts(builder, ints): builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(ints), 0)
-def AttributeStartIntsVector(builder, numElems): return builder.StartVector(8, numElems, 8)
-def AttributeAddStrings(builder, strings): builder.PrependUOffsetTRelativeSlot(10, flatbuffers.number_types.UOffsetTFlags.py_type(strings), 0)
-def AttributeStartStringsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def AttributeAddTensors(builder, tensors): builder.PrependUOffsetTRelativeSlot(11, flatbuffers.number_types.UOffsetTFlags.py_type(tensors), 0)
-def AttributeStartTensorsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def AttributeAddGraphs(builder, graphs): builder.PrependUOffsetTRelativeSlot(12, flatbuffers.number_types.UOffsetTFlags.py_type(graphs), 0)
-def AttributeStartGraphsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def AttributeEnd(builder): return builder.EndObject()
+def AttributeStart(builder):
+    builder.StartObject(13)
+
+def Start(builder):
+    AttributeStart(builder)
+
+def AttributeAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    AttributeAddName(builder, name)
+
+def AttributeAddDocString(builder, docString):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
+
+def AddDocString(builder, docString):
+    AttributeAddDocString(builder, docString)
+
+def AttributeAddType(builder, type):
+    builder.PrependInt32Slot(2, type, 0)
+
+def AddType(builder, type):
+    AttributeAddType(builder, type)
+
+def AttributeAddF(builder, f):
+    builder.PrependFloat32Slot(3, f, 0.0)
+
+def AddF(builder, f):
+    AttributeAddF(builder, f)
+
+def AttributeAddI(builder, i):
+    builder.PrependInt64Slot(4, i, 0)
+
+def AddI(builder, i):
+    AttributeAddI(builder, i)
+
+def AttributeAddS(builder, s):
+    builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(s), 0)
+
+def AddS(builder, s):
+    AttributeAddS(builder, s)
+
+def AttributeAddT(builder, t):
+    builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(t), 0)
+
+def AddT(builder, t):
+    AttributeAddT(builder, t)
+
+def AttributeAddG(builder, g):
+    builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(g), 0)
+
+def AddG(builder, g):
+    AttributeAddG(builder, g)
+
+def AttributeAddFloats(builder, floats):
+    builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(floats), 0)
+
+def AddFloats(builder, floats):
+    AttributeAddFloats(builder, floats)
+
+def AttributeStartFloatsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartFloatsVector(builder, numElems: int) -> int:
+    return AttributeStartFloatsVector(builder, numElems)
+
+def AttributeAddInts(builder, ints):
+    builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(ints), 0)
+
+def AddInts(builder, ints):
+    AttributeAddInts(builder, ints)
+
+def AttributeStartIntsVector(builder, numElems):
+    return builder.StartVector(8, numElems, 8)
+
+def StartIntsVector(builder, numElems: int) -> int:
+    return AttributeStartIntsVector(builder, numElems)
+
+def AttributeAddStrings(builder, strings):
+    builder.PrependUOffsetTRelativeSlot(10, flatbuffers.number_types.UOffsetTFlags.py_type(strings), 0)
+
+def AddStrings(builder, strings):
+    AttributeAddStrings(builder, strings)
+
+def AttributeStartStringsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartStringsVector(builder, numElems: int) -> int:
+    return AttributeStartStringsVector(builder, numElems)
+
+def AttributeAddTensors(builder, tensors):
+    builder.PrependUOffsetTRelativeSlot(11, flatbuffers.number_types.UOffsetTFlags.py_type(tensors), 0)
+
+def AddTensors(builder, tensors):
+    AttributeAddTensors(builder, tensors)
+
+def AttributeStartTensorsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartTensorsVector(builder, numElems: int) -> int:
+    return AttributeStartTensorsVector(builder, numElems)
+
+def AttributeAddGraphs(builder, graphs):
+    builder.PrependUOffsetTRelativeSlot(12, flatbuffers.number_types.UOffsetTFlags.py_type(graphs), 0)
+
+def AddGraphs(builder, graphs):
+    AttributeAddGraphs(builder, graphs)
+
+def AttributeStartGraphsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartGraphsVector(builder, numElems: int) -> int:
+    return AttributeStartGraphsVector(builder, numElems)
+
+def AttributeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return AttributeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py

```diff
@@ -12,8 +12,7 @@
     FLOATS = 6
     INTS = 7
     STRINGS = 8
     TENSORS = 9
     GRAPHS = 10
     SPARSE_TENSOR = 11
     SPARSE_TENSORS = 12
-
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Checkpoint(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsCheckpoint(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Checkpoint()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsCheckpoint(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def CheckpointBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # Checkpoint
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -74,14 +78,48 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.PropertyBag import PropertyBag
             obj = PropertyBag()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def CheckpointStart(builder): builder.StartObject(4)
-def CheckpointAddVersion(builder, version): builder.PrependInt32Slot(0, version, 0)
-def CheckpointAddModuleState(builder, moduleState): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(moduleState), 0)
-def CheckpointAddOptimizerGroups(builder, optimizerGroups): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(optimizerGroups), 0)
-def CheckpointStartOptimizerGroupsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def CheckpointAddPropertyBag(builder, propertyBag): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(propertyBag), 0)
-def CheckpointEnd(builder): return builder.EndObject()
+def CheckpointStart(builder):
+    builder.StartObject(4)
+
+def Start(builder):
+    CheckpointStart(builder)
+
+def CheckpointAddVersion(builder, version):
+    builder.PrependInt32Slot(0, version, 0)
+
+def AddVersion(builder, version):
+    CheckpointAddVersion(builder, version)
+
+def CheckpointAddModuleState(builder, moduleState):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(moduleState), 0)
+
+def AddModuleState(builder, moduleState):
+    CheckpointAddModuleState(builder, moduleState)
+
+def CheckpointAddOptimizerGroups(builder, optimizerGroups):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(optimizerGroups), 0)
+
+def AddOptimizerGroups(builder, optimizerGroups):
+    CheckpointAddOptimizerGroups(builder, optimizerGroups)
+
+def CheckpointStartOptimizerGroupsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartOptimizerGroupsVector(builder, numElems: int) -> int:
+    return CheckpointStartOptimizerGroupsVector(builder, numElems)
+
+def CheckpointAddPropertyBag(builder, propertyBag):
+    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(propertyBag), 0)
+
+def AddPropertyBag(builder, propertyBag):
+    CheckpointAddPropertyBag(builder, propertyBag)
+
+def CheckpointEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return CheckpointEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py

```diff
@@ -7,21 +7,25 @@
 np = import_numpy()
 
 # deprecated: no longer using kernel def hashes
 class DeprecatedKernelCreateInfos(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsDeprecatedKernelCreateInfos(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = DeprecatedKernelCreateInfos()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsDeprecatedKernelCreateInfos(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def DeprecatedKernelCreateInfosBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # DeprecatedKernelCreateInfos
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -75,13 +79,42 @@
         return 0
 
     # DeprecatedKernelCreateInfos
     def KernelDefHashesIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def DeprecatedKernelCreateInfosStart(builder): builder.StartObject(2)
-def DeprecatedKernelCreateInfosAddNodeIndices(builder, nodeIndices): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(nodeIndices), 0)
-def DeprecatedKernelCreateInfosStartNodeIndicesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def DeprecatedKernelCreateInfosAddKernelDefHashes(builder, kernelDefHashes): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(kernelDefHashes), 0)
-def DeprecatedKernelCreateInfosStartKernelDefHashesVector(builder, numElems): return builder.StartVector(8, numElems, 8)
-def DeprecatedKernelCreateInfosEnd(builder): return builder.EndObject()
+def DeprecatedKernelCreateInfosStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    DeprecatedKernelCreateInfosStart(builder)
+
+def DeprecatedKernelCreateInfosAddNodeIndices(builder, nodeIndices):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(nodeIndices), 0)
+
+def AddNodeIndices(builder, nodeIndices):
+    DeprecatedKernelCreateInfosAddNodeIndices(builder, nodeIndices)
+
+def DeprecatedKernelCreateInfosStartNodeIndicesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartNodeIndicesVector(builder, numElems: int) -> int:
+    return DeprecatedKernelCreateInfosStartNodeIndicesVector(builder, numElems)
+
+def DeprecatedKernelCreateInfosAddKernelDefHashes(builder, kernelDefHashes):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(kernelDefHashes), 0)
+
+def AddKernelDefHashes(builder, kernelDefHashes):
+    DeprecatedKernelCreateInfosAddKernelDefHashes(builder, kernelDefHashes)
+
+def DeprecatedKernelCreateInfosStartKernelDefHashesVector(builder, numElems):
+    return builder.StartVector(8, numElems, 8)
+
+def StartKernelDefHashesVector(builder, numElems: int) -> int:
+    return DeprecatedKernelCreateInfosStartKernelDefHashesVector(builder, numElems)
+
+def DeprecatedKernelCreateInfosEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return DeprecatedKernelCreateInfosEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py

```diff
@@ -7,21 +7,25 @@
 np = import_numpy()
 
 # deprecated: no longer using kernel def hashes
 class DeprecatedNodeIndexAndKernelDefHash(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsDeprecatedNodeIndexAndKernelDefHash(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = DeprecatedNodeIndexAndKernelDefHash()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsDeprecatedNodeIndexAndKernelDefHash(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def DeprecatedNodeIndexAndKernelDefHashBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # DeprecatedNodeIndexAndKernelDefHash
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -35,11 +39,30 @@
     # DeprecatedNodeIndexAndKernelDefHash
     def KernelDefHash(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Uint64Flags, o + self._tab.Pos)
         return 0
 
-def DeprecatedNodeIndexAndKernelDefHashStart(builder): builder.StartObject(2)
-def DeprecatedNodeIndexAndKernelDefHashAddNodeIndex(builder, nodeIndex): builder.PrependUint32Slot(0, nodeIndex, 0)
-def DeprecatedNodeIndexAndKernelDefHashAddKernelDefHash(builder, kernelDefHash): builder.PrependUint64Slot(1, kernelDefHash, 0)
-def DeprecatedNodeIndexAndKernelDefHashEnd(builder): return builder.EndObject()
+def DeprecatedNodeIndexAndKernelDefHashStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    DeprecatedNodeIndexAndKernelDefHashStart(builder)
+
+def DeprecatedNodeIndexAndKernelDefHashAddNodeIndex(builder, nodeIndex):
+    builder.PrependUint32Slot(0, nodeIndex, 0)
+
+def AddNodeIndex(builder, nodeIndex):
+    DeprecatedNodeIndexAndKernelDefHashAddNodeIndex(builder, nodeIndex)
+
+def DeprecatedNodeIndexAndKernelDefHashAddKernelDefHash(builder, kernelDefHash):
+    builder.PrependUint64Slot(1, kernelDefHash, 0)
+
+def AddKernelDefHash(builder, kernelDefHash):
+    DeprecatedNodeIndexAndKernelDefHashAddKernelDefHash(builder, kernelDefHash)
+
+def DeprecatedNodeIndexAndKernelDefHashEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return DeprecatedNodeIndexAndKernelDefHashEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py

```diff
@@ -7,21 +7,25 @@
 np = import_numpy()
 
 # deprecated: no longer using kernel def hashes
 class DeprecatedSessionState(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsDeprecatedSessionState(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = DeprecatedSessionState()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsDeprecatedSessionState(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def DeprecatedSessionStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # DeprecatedSessionState
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -57,12 +61,36 @@
         return 0
 
     # DeprecatedSessionState
     def SubGraphSessionStatesIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def DeprecatedSessionStateStart(builder): builder.StartObject(2)
-def DeprecatedSessionStateAddKernels(builder, kernels): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernels), 0)
-def DeprecatedSessionStateAddSubGraphSessionStates(builder, subGraphSessionStates): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(subGraphSessionStates), 0)
-def DeprecatedSessionStateStartSubGraphSessionStatesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def DeprecatedSessionStateEnd(builder): return builder.EndObject()
+def DeprecatedSessionStateStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    DeprecatedSessionStateStart(builder)
+
+def DeprecatedSessionStateAddKernels(builder, kernels):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernels), 0)
+
+def AddKernels(builder, kernels):
+    DeprecatedSessionStateAddKernels(builder, kernels)
+
+def DeprecatedSessionStateAddSubGraphSessionStates(builder, subGraphSessionStates):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(subGraphSessionStates), 0)
+
+def AddSubGraphSessionStates(builder, subGraphSessionStates):
+    DeprecatedSessionStateAddSubGraphSessionStates(builder, subGraphSessionStates)
+
+def DeprecatedSessionStateStartSubGraphSessionStatesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartSubGraphSessionStatesVector(builder, numElems: int) -> int:
+    return DeprecatedSessionStateStartSubGraphSessionStatesVector(builder, numElems)
+
+def DeprecatedSessionStateEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return DeprecatedSessionStateEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py

```diff
@@ -7,21 +7,25 @@
 np = import_numpy()
 
 # deprecated: no longer using kernel def hashes
 class DeprecatedSubGraphSessionState(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsDeprecatedSubGraphSessionState(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = DeprecatedSubGraphSessionState()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsDeprecatedSubGraphSessionState(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def DeprecatedSubGraphSessionStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # DeprecatedSubGraphSessionState
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -39,11 +43,30 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.DeprecatedSessionState import DeprecatedSessionState
             obj = DeprecatedSessionState()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def DeprecatedSubGraphSessionStateStart(builder): builder.StartObject(2)
-def DeprecatedSubGraphSessionStateAddGraphId(builder, graphId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(graphId), 0)
-def DeprecatedSubGraphSessionStateAddSessionState(builder, sessionState): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(sessionState), 0)
-def DeprecatedSubGraphSessionStateEnd(builder): return builder.EndObject()
+def DeprecatedSubGraphSessionStateStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    DeprecatedSubGraphSessionStateStart(builder)
+
+def DeprecatedSubGraphSessionStateAddGraphId(builder, graphId):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(graphId), 0)
+
+def AddGraphId(builder, graphId):
+    DeprecatedSubGraphSessionStateAddGraphId(builder, graphId)
+
+def DeprecatedSubGraphSessionStateAddSessionState(builder, sessionState):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(sessionState), 0)
+
+def AddSessionState(builder, sessionState):
+    DeprecatedSubGraphSessionStateAddSessionState(builder, sessionState)
+
+def DeprecatedSubGraphSessionStateEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return DeprecatedSubGraphSessionStateEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Dimension(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsDimension(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Dimension()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsDimension(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def DimensionBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Dimension
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -38,11 +42,30 @@
     # Dimension
     def Denotation(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-def DimensionStart(builder): builder.StartObject(2)
-def DimensionAddValue(builder, value): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
-def DimensionAddDenotation(builder, denotation): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(denotation), 0)
-def DimensionEnd(builder): return builder.EndObject()
+def DimensionStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    DimensionStart(builder)
+
+def DimensionAddValue(builder, value):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
+
+def AddValue(builder, value):
+    DimensionAddValue(builder, value)
+
+def DimensionAddDenotation(builder, denotation):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(denotation), 0)
+
+def AddDenotation(builder, denotation):
+    DimensionAddDenotation(builder, denotation)
+
+def DimensionEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return DimensionEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class DimensionValue(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsDimensionValue(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = DimensionValue()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsDimensionValue(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def DimensionValueBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # DimensionValue
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -41,12 +45,36 @@
     # DimensionValue
     def DimParam(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-def DimensionValueStart(builder): builder.StartObject(3)
-def DimensionValueAddDimType(builder, dimType): builder.PrependInt8Slot(0, dimType, 0)
-def DimensionValueAddDimValue(builder, dimValue): builder.PrependInt64Slot(1, dimValue, 0)
-def DimensionValueAddDimParam(builder, dimParam): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dimParam), 0)
-def DimensionValueEnd(builder): return builder.EndObject()
+def DimensionValueStart(builder):
+    builder.StartObject(3)
+
+def Start(builder):
+    DimensionValueStart(builder)
+
+def DimensionValueAddDimType(builder, dimType):
+    builder.PrependInt8Slot(0, dimType, 0)
+
+def AddDimType(builder, dimType):
+    DimensionValueAddDimType(builder, dimType)
+
+def DimensionValueAddDimValue(builder, dimValue):
+    builder.PrependInt64Slot(1, dimValue, 0)
+
+def AddDimValue(builder, dimValue):
+    DimensionValueAddDimValue(builder, dimValue)
+
+def DimensionValueAddDimParam(builder, dimParam):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dimParam), 0)
+
+def AddDimParam(builder, dimParam):
+    DimensionValueAddDimParam(builder, dimParam)
+
+def DimensionValueEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return DimensionValueEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py

```diff
@@ -2,8 +2,7 @@
 
 # namespace: fbs
 
 class DimensionValueType(object):
     UNKNOWN = 0
     VALUE = 1
     PARAM = 2
-
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py

```diff
@@ -5,14 +5,18 @@
 import flatbuffers
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class EdgeEnd(object):
     __slots__ = ['_tab']
 
+    @classmethod
+    def SizeOf(cls):
+        return 12
+
     # EdgeEnd
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
     # EdgeEnd
     def NodeIndex(self): return self._tab.Get(flatbuffers.number_types.Uint32Flags, self._tab.Pos + flatbuffers.number_types.UOffsetTFlags.py_type(0))
     # EdgeEnd
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class FloatProperty(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsFloatProperty(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = FloatProperty()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsFloatProperty(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def FloatPropertyBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # FloatProperty
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -34,11 +38,30 @@
     # FloatProperty
     def Value(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Float32Flags, o + self._tab.Pos)
         return 0.0
 
-def FloatPropertyStart(builder): builder.StartObject(2)
-def FloatPropertyAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def FloatPropertyAddValue(builder, value): builder.PrependFloat32Slot(1, value, 0.0)
-def FloatPropertyEnd(builder): return builder.EndObject()
+def FloatPropertyStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    FloatPropertyStart(builder)
+
+def FloatPropertyAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    FloatPropertyAddName(builder, name)
+
+def FloatPropertyAddValue(builder, value):
+    builder.PrependFloat32Slot(1, value, 0.0)
+
+def AddValue(builder, value):
+    FloatPropertyAddValue(builder, value)
+
+def FloatPropertyEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return FloatPropertyEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Graph(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsGraph(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Graph()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsGraph(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def GraphBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Graph
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -203,25 +207,114 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.RuntimeOptimizations import RuntimeOptimizations
             obj = RuntimeOptimizations()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def GraphStart(builder): builder.StartObject(9)
-def GraphAddInitializers(builder, initializers): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(initializers), 0)
-def GraphStartInitializersVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddNodeArgs(builder, nodeArgs): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(nodeArgs), 0)
-def GraphStartNodeArgsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddNodes(builder, nodes): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(nodes), 0)
-def GraphStartNodesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddMaxNodeIndex(builder, maxNodeIndex): builder.PrependUint32Slot(3, maxNodeIndex, 0)
-def GraphAddNodeEdges(builder, nodeEdges): builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(nodeEdges), 0)
-def GraphStartNodeEdgesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddInputs(builder, inputs): builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(inputs), 0)
-def GraphStartInputsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddOutputs(builder, outputs): builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(outputs), 0)
-def GraphStartOutputsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddSparseInitializers(builder, sparseInitializers): builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(sparseInitializers), 0)
-def GraphStartSparseInitializersVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def GraphAddRuntimeOptimizations(builder, runtimeOptimizations): builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(runtimeOptimizations), 0)
-def GraphEnd(builder): return builder.EndObject()
+def GraphStart(builder):
+    builder.StartObject(9)
+
+def Start(builder):
+    GraphStart(builder)
+
+def GraphAddInitializers(builder, initializers):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(initializers), 0)
+
+def AddInitializers(builder, initializers):
+    GraphAddInitializers(builder, initializers)
+
+def GraphStartInitializersVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartInitializersVector(builder, numElems: int) -> int:
+    return GraphStartInitializersVector(builder, numElems)
+
+def GraphAddNodeArgs(builder, nodeArgs):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(nodeArgs), 0)
+
+def AddNodeArgs(builder, nodeArgs):
+    GraphAddNodeArgs(builder, nodeArgs)
+
+def GraphStartNodeArgsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartNodeArgsVector(builder, numElems: int) -> int:
+    return GraphStartNodeArgsVector(builder, numElems)
+
+def GraphAddNodes(builder, nodes):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(nodes), 0)
+
+def AddNodes(builder, nodes):
+    GraphAddNodes(builder, nodes)
+
+def GraphStartNodesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartNodesVector(builder, numElems: int) -> int:
+    return GraphStartNodesVector(builder, numElems)
+
+def GraphAddMaxNodeIndex(builder, maxNodeIndex):
+    builder.PrependUint32Slot(3, maxNodeIndex, 0)
+
+def AddMaxNodeIndex(builder, maxNodeIndex):
+    GraphAddMaxNodeIndex(builder, maxNodeIndex)
+
+def GraphAddNodeEdges(builder, nodeEdges):
+    builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(nodeEdges), 0)
+
+def AddNodeEdges(builder, nodeEdges):
+    GraphAddNodeEdges(builder, nodeEdges)
+
+def GraphStartNodeEdgesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartNodeEdgesVector(builder, numElems: int) -> int:
+    return GraphStartNodeEdgesVector(builder, numElems)
+
+def GraphAddInputs(builder, inputs):
+    builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(inputs), 0)
+
+def AddInputs(builder, inputs):
+    GraphAddInputs(builder, inputs)
+
+def GraphStartInputsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartInputsVector(builder, numElems: int) -> int:
+    return GraphStartInputsVector(builder, numElems)
+
+def GraphAddOutputs(builder, outputs):
+    builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(outputs), 0)
+
+def AddOutputs(builder, outputs):
+    GraphAddOutputs(builder, outputs)
+
+def GraphStartOutputsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartOutputsVector(builder, numElems: int) -> int:
+    return GraphStartOutputsVector(builder, numElems)
+
+def GraphAddSparseInitializers(builder, sparseInitializers):
+    builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(sparseInitializers), 0)
+
+def AddSparseInitializers(builder, sparseInitializers):
+    GraphAddSparseInitializers(builder, sparseInitializers)
+
+def GraphStartSparseInitializersVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartSparseInitializersVector(builder, numElems: int) -> int:
+    return GraphStartSparseInitializersVector(builder, numElems)
+
+def GraphAddRuntimeOptimizations(builder, runtimeOptimizations):
+    builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(runtimeOptimizations), 0)
+
+def AddRuntimeOptimizations(builder, runtimeOptimizations):
+    GraphAddRuntimeOptimizations(builder, runtimeOptimizations)
+
+def GraphEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return GraphEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class InferenceSession(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsInferenceSession(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = InferenceSession()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsInferenceSession(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def InferenceSessionBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # InferenceSession
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -49,12 +53,36 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.KernelTypeStrResolver import KernelTypeStrResolver
             obj = KernelTypeStrResolver()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def InferenceSessionStart(builder): builder.StartObject(4)
-def InferenceSessionAddOrtVersion(builder, ortVersion): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(ortVersion), 0)
-def InferenceSessionAddModel(builder, model): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(model), 0)
-def InferenceSessionAddKernelTypeStrResolver(builder, kernelTypeStrResolver): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStrResolver), 0)
-def InferenceSessionEnd(builder): return builder.EndObject()
+def InferenceSessionStart(builder):
+    builder.StartObject(4)
+
+def Start(builder):
+    InferenceSessionStart(builder)
+
+def InferenceSessionAddOrtVersion(builder, ortVersion):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(ortVersion), 0)
+
+def AddOrtVersion(builder, ortVersion):
+    InferenceSessionAddOrtVersion(builder, ortVersion)
+
+def InferenceSessionAddModel(builder, model):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(model), 0)
+
+def AddModel(builder, model):
+    InferenceSessionAddModel(builder, model)
+
+def InferenceSessionAddKernelTypeStrResolver(builder, kernelTypeStrResolver):
+    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStrResolver), 0)
+
+def AddKernelTypeStrResolver(builder, kernelTypeStrResolver):
+    InferenceSessionAddKernelTypeStrResolver(builder, kernelTypeStrResolver)
+
+def InferenceSessionEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return InferenceSessionEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class IntProperty(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsIntProperty(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = IntProperty()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsIntProperty(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def IntPropertyBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # IntProperty
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -34,11 +38,30 @@
     # IntProperty
     def Value(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Int64Flags, o + self._tab.Pos)
         return 0
 
-def IntPropertyStart(builder): builder.StartObject(2)
-def IntPropertyAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def IntPropertyAddValue(builder, value): builder.PrependInt64Slot(1, value, 0)
-def IntPropertyEnd(builder): return builder.EndObject()
+def IntPropertyStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    IntPropertyStart(builder)
+
+def IntPropertyAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    IntPropertyAddName(builder, name)
+
+def IntPropertyAddValue(builder, value):
+    builder.PrependInt64Slot(1, value, 0)
+
+def AddValue(builder, value):
+    IntPropertyAddValue(builder, value)
+
+def IntPropertyEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return IntPropertyEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class KernelTypeStrArgsEntry(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsKernelTypeStrArgsEntry(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = KernelTypeStrArgsEntry()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsKernelTypeStrArgsEntry(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def KernelTypeStrArgsEntryBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # KernelTypeStrArgsEntry
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -52,12 +56,36 @@
         return 0
 
     # KernelTypeStrArgsEntry
     def ArgsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def KernelTypeStrArgsEntryStart(builder): builder.StartObject(2)
-def KernelTypeStrArgsEntryAddKernelTypeStr(builder, kernelTypeStr): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStr), 0)
-def KernelTypeStrArgsEntryAddArgs(builder, args): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(args), 0)
-def KernelTypeStrArgsEntryStartArgsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def KernelTypeStrArgsEntryEnd(builder): return builder.EndObject()
+def KernelTypeStrArgsEntryStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    KernelTypeStrArgsEntryStart(builder)
+
+def KernelTypeStrArgsEntryAddKernelTypeStr(builder, kernelTypeStr):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStr), 0)
+
+def AddKernelTypeStr(builder, kernelTypeStr):
+    KernelTypeStrArgsEntryAddKernelTypeStr(builder, kernelTypeStr)
+
+def KernelTypeStrArgsEntryAddArgs(builder, args):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(args), 0)
+
+def AddArgs(builder, args):
+    KernelTypeStrArgsEntryAddArgs(builder, args)
+
+def KernelTypeStrArgsEntryStartArgsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartArgsVector(builder, numElems: int) -> int:
+    return KernelTypeStrArgsEntryStartArgsVector(builder, numElems)
+
+def KernelTypeStrArgsEntryEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return KernelTypeStrArgsEntryEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class KernelTypeStrResolver(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsKernelTypeStrResolver(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = KernelTypeStrResolver()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsKernelTypeStrResolver(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def KernelTypeStrResolverBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # KernelTypeStrResolver
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -45,11 +49,30 @@
         return 0
 
     # KernelTypeStrResolver
     def OpKernelTypeStrArgsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         return o == 0
 
-def KernelTypeStrResolverStart(builder): builder.StartObject(1)
-def KernelTypeStrResolverAddOpKernelTypeStrArgs(builder, opKernelTypeStrArgs): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(opKernelTypeStrArgs), 0)
-def KernelTypeStrResolverStartOpKernelTypeStrArgsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def KernelTypeStrResolverEnd(builder): return builder.EndObject()
+def KernelTypeStrResolverStart(builder):
+    builder.StartObject(1)
+
+def Start(builder):
+    KernelTypeStrResolverStart(builder)
+
+def KernelTypeStrResolverAddOpKernelTypeStrArgs(builder, opKernelTypeStrArgs):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(opKernelTypeStrArgs), 0)
+
+def AddOpKernelTypeStrArgs(builder, opKernelTypeStrArgs):
+    KernelTypeStrResolverAddOpKernelTypeStrArgs(builder, opKernelTypeStrArgs)
+
+def KernelTypeStrResolverStartOpKernelTypeStrArgsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartOpKernelTypeStrArgsVector(builder, numElems: int) -> int:
+    return KernelTypeStrResolverStartOpKernelTypeStrArgsVector(builder, numElems)
+
+def KernelTypeStrResolverEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return KernelTypeStrResolverEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class MapType(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsMapType(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = MapType()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsMapType(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def MapTypeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # MapType
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -38,11 +42,30 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
             obj = TypeInfo()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def MapTypeStart(builder): builder.StartObject(2)
-def MapTypeAddKeyType(builder, keyType): builder.PrependInt32Slot(0, keyType, 0)
-def MapTypeAddValueType(builder, valueType): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(valueType), 0)
-def MapTypeEnd(builder): return builder.EndObject()
+def MapTypeStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    MapTypeStart(builder)
+
+def MapTypeAddKeyType(builder, keyType):
+    builder.PrependInt32Slot(0, keyType, 0)
+
+def AddKeyType(builder, keyType):
+    MapTypeAddKeyType(builder, keyType)
+
+def MapTypeAddValueType(builder, valueType):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(valueType), 0)
+
+def AddValueType(builder, valueType):
+    MapTypeAddValueType(builder, valueType)
+
+def MapTypeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return MapTypeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Model(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsModel(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Model()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsModel(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def ModelBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Model
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -130,21 +134,90 @@
         return 0
 
     # Model
     def MetadataPropsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(22))
         return o == 0
 
-def ModelStart(builder): builder.StartObject(10)
-def ModelAddIrVersion(builder, irVersion): builder.PrependInt64Slot(0, irVersion, 0)
-def ModelAddOpsetImport(builder, opsetImport): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(opsetImport), 0)
-def ModelStartOpsetImportVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def ModelAddProducerName(builder, producerName): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(producerName), 0)
-def ModelAddProducerVersion(builder, producerVersion): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(producerVersion), 0)
-def ModelAddDomain(builder, domain): builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
-def ModelAddModelVersion(builder, modelVersion): builder.PrependInt64Slot(5, modelVersion, 0)
-def ModelAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
-def ModelAddGraph(builder, graph): builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(graph), 0)
-def ModelAddGraphDocString(builder, graphDocString): builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(graphDocString), 0)
-def ModelAddMetadataProps(builder, metadataProps): builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(metadataProps), 0)
-def ModelStartMetadataPropsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def ModelEnd(builder): return builder.EndObject()
+def ModelStart(builder):
+    builder.StartObject(10)
+
+def Start(builder):
+    ModelStart(builder)
+
+def ModelAddIrVersion(builder, irVersion):
+    builder.PrependInt64Slot(0, irVersion, 0)
+
+def AddIrVersion(builder, irVersion):
+    ModelAddIrVersion(builder, irVersion)
+
+def ModelAddOpsetImport(builder, opsetImport):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(opsetImport), 0)
+
+def AddOpsetImport(builder, opsetImport):
+    ModelAddOpsetImport(builder, opsetImport)
+
+def ModelStartOpsetImportVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartOpsetImportVector(builder, numElems: int) -> int:
+    return ModelStartOpsetImportVector(builder, numElems)
+
+def ModelAddProducerName(builder, producerName):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(producerName), 0)
+
+def AddProducerName(builder, producerName):
+    ModelAddProducerName(builder, producerName)
+
+def ModelAddProducerVersion(builder, producerVersion):
+    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(producerVersion), 0)
+
+def AddProducerVersion(builder, producerVersion):
+    ModelAddProducerVersion(builder, producerVersion)
+
+def ModelAddDomain(builder, domain):
+    builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
+
+def AddDomain(builder, domain):
+    ModelAddDomain(builder, domain)
+
+def ModelAddModelVersion(builder, modelVersion):
+    builder.PrependInt64Slot(5, modelVersion, 0)
+
+def AddModelVersion(builder, modelVersion):
+    ModelAddModelVersion(builder, modelVersion)
+
+def ModelAddDocString(builder, docString):
+    builder.PrependUOffsetTRelativeSlot(6, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
+
+def AddDocString(builder, docString):
+    ModelAddDocString(builder, docString)
+
+def ModelAddGraph(builder, graph):
+    builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(graph), 0)
+
+def AddGraph(builder, graph):
+    ModelAddGraph(builder, graph)
+
+def ModelAddGraphDocString(builder, graphDocString):
+    builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(graphDocString), 0)
+
+def AddGraphDocString(builder, graphDocString):
+    ModelAddGraphDocString(builder, graphDocString)
+
+def ModelAddMetadataProps(builder, metadataProps):
+    builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(metadataProps), 0)
+
+def AddMetadataProps(builder, metadataProps):
+    ModelAddMetadataProps(builder, metadataProps)
+
+def ModelStartMetadataPropsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartMetadataPropsVector(builder, numElems: int) -> int:
+    return ModelStartMetadataPropsVector(builder, numElems)
+
+def ModelEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return ModelEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class ModuleState(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsModuleState(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = ModuleState()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsModuleState(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def ModuleStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # ModuleState
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -70,13 +74,68 @@
         return 0
 
     # ModuleState
     def FrozenParamsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def ModuleStateStart(builder): builder.StartObject(2)
-def ModuleStateAddRequiresGradParams(builder, requiresGradParams): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(requiresGradParams), 0)
-def ModuleStateStartRequiresGradParamsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def ModuleStateAddFrozenParams(builder, frozenParams): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(frozenParams), 0)
-def ModuleStateStartFrozenParamsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def ModuleStateEnd(builder): return builder.EndObject()
+    # ModuleState
+    def IsNominalState(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
+        if o != 0:
+            return bool(self._tab.Get(flatbuffers.number_types.BoolFlags, o + self._tab.Pos))
+        return False
+
+    # ModuleState
+    def HasExternalData(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
+        if o != 0:
+            return bool(self._tab.Get(flatbuffers.number_types.BoolFlags, o + self._tab.Pos))
+        return False
+
+def ModuleStateStart(builder):
+    builder.StartObject(4)
+
+def Start(builder):
+    ModuleStateStart(builder)
+
+def ModuleStateAddRequiresGradParams(builder, requiresGradParams):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(requiresGradParams), 0)
+
+def AddRequiresGradParams(builder, requiresGradParams):
+    ModuleStateAddRequiresGradParams(builder, requiresGradParams)
+
+def ModuleStateStartRequiresGradParamsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartRequiresGradParamsVector(builder, numElems: int) -> int:
+    return ModuleStateStartRequiresGradParamsVector(builder, numElems)
+
+def ModuleStateAddFrozenParams(builder, frozenParams):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(frozenParams), 0)
+
+def AddFrozenParams(builder, frozenParams):
+    ModuleStateAddFrozenParams(builder, frozenParams)
+
+def ModuleStateStartFrozenParamsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartFrozenParamsVector(builder, numElems: int) -> int:
+    return ModuleStateStartFrozenParamsVector(builder, numElems)
+
+def ModuleStateAddIsNominalState(builder, isNominalState):
+    builder.PrependBoolSlot(2, isNominalState, 0)
+
+def AddIsNominalState(builder, isNominalState):
+    ModuleStateAddIsNominalState(builder, isNominalState)
+
+def ModuleStateAddHasExternalData(builder, hasExternalData):
+    builder.PrependBoolSlot(3, hasExternalData, 0)
+
+def AddHasExternalData(builder, hasExternalData):
+    ModuleStateAddHasExternalData(builder, hasExternalData)
+
+def ModuleStateEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return ModuleStateEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Node(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsNode(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Node()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsNode(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def NodeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Node
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -188,27 +192,126 @@
         return 0
 
     # Node
     def ImplicitInputsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(28))
         return o == 0
 
-def NodeStart(builder): builder.StartObject(13)
-def NodeAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def NodeAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
-def NodeAddDomain(builder, domain): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
-def NodeAddSinceVersion(builder, sinceVersion): builder.PrependInt32Slot(3, sinceVersion, 0)
-def NodeAddIndex(builder, index): builder.PrependUint32Slot(4, index, 0)
-def NodeAddOpType(builder, opType): builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(opType), 0)
-def NodeAddType(builder, type): builder.PrependInt32Slot(6, type, 0)
-def NodeAddExecutionProviderType(builder, executionProviderType): builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(executionProviderType), 0)
-def NodeAddInputs(builder, inputs): builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(inputs), 0)
-def NodeStartInputsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def NodeAddOutputs(builder, outputs): builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(outputs), 0)
-def NodeStartOutputsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def NodeAddAttributes(builder, attributes): builder.PrependUOffsetTRelativeSlot(10, flatbuffers.number_types.UOffsetTFlags.py_type(attributes), 0)
-def NodeStartAttributesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def NodeAddInputArgCounts(builder, inputArgCounts): builder.PrependUOffsetTRelativeSlot(11, flatbuffers.number_types.UOffsetTFlags.py_type(inputArgCounts), 0)
-def NodeStartInputArgCountsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def NodeAddImplicitInputs(builder, implicitInputs): builder.PrependUOffsetTRelativeSlot(12, flatbuffers.number_types.UOffsetTFlags.py_type(implicitInputs), 0)
-def NodeStartImplicitInputsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def NodeEnd(builder): return builder.EndObject()
+def NodeStart(builder):
+    builder.StartObject(13)
+
+def Start(builder):
+    NodeStart(builder)
+
+def NodeAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    NodeAddName(builder, name)
+
+def NodeAddDocString(builder, docString):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
+
+def AddDocString(builder, docString):
+    NodeAddDocString(builder, docString)
+
+def NodeAddDomain(builder, domain):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
+
+def AddDomain(builder, domain):
+    NodeAddDomain(builder, domain)
+
+def NodeAddSinceVersion(builder, sinceVersion):
+    builder.PrependInt32Slot(3, sinceVersion, 0)
+
+def AddSinceVersion(builder, sinceVersion):
+    NodeAddSinceVersion(builder, sinceVersion)
+
+def NodeAddIndex(builder, index):
+    builder.PrependUint32Slot(4, index, 0)
+
+def AddIndex(builder, index):
+    NodeAddIndex(builder, index)
+
+def NodeAddOpType(builder, opType):
+    builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(opType), 0)
+
+def AddOpType(builder, opType):
+    NodeAddOpType(builder, opType)
+
+def NodeAddType(builder, type):
+    builder.PrependInt32Slot(6, type, 0)
+
+def AddType(builder, type):
+    NodeAddType(builder, type)
+
+def NodeAddExecutionProviderType(builder, executionProviderType):
+    builder.PrependUOffsetTRelativeSlot(7, flatbuffers.number_types.UOffsetTFlags.py_type(executionProviderType), 0)
+
+def AddExecutionProviderType(builder, executionProviderType):
+    NodeAddExecutionProviderType(builder, executionProviderType)
+
+def NodeAddInputs(builder, inputs):
+    builder.PrependUOffsetTRelativeSlot(8, flatbuffers.number_types.UOffsetTFlags.py_type(inputs), 0)
+
+def AddInputs(builder, inputs):
+    NodeAddInputs(builder, inputs)
+
+def NodeStartInputsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartInputsVector(builder, numElems: int) -> int:
+    return NodeStartInputsVector(builder, numElems)
+
+def NodeAddOutputs(builder, outputs):
+    builder.PrependUOffsetTRelativeSlot(9, flatbuffers.number_types.UOffsetTFlags.py_type(outputs), 0)
+
+def AddOutputs(builder, outputs):
+    NodeAddOutputs(builder, outputs)
+
+def NodeStartOutputsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartOutputsVector(builder, numElems: int) -> int:
+    return NodeStartOutputsVector(builder, numElems)
+
+def NodeAddAttributes(builder, attributes):
+    builder.PrependUOffsetTRelativeSlot(10, flatbuffers.number_types.UOffsetTFlags.py_type(attributes), 0)
+
+def AddAttributes(builder, attributes):
+    NodeAddAttributes(builder, attributes)
+
+def NodeStartAttributesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartAttributesVector(builder, numElems: int) -> int:
+    return NodeStartAttributesVector(builder, numElems)
+
+def NodeAddInputArgCounts(builder, inputArgCounts):
+    builder.PrependUOffsetTRelativeSlot(11, flatbuffers.number_types.UOffsetTFlags.py_type(inputArgCounts), 0)
+
+def AddInputArgCounts(builder, inputArgCounts):
+    NodeAddInputArgCounts(builder, inputArgCounts)
+
+def NodeStartInputArgCountsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartInputArgCountsVector(builder, numElems: int) -> int:
+    return NodeStartInputArgCountsVector(builder, numElems)
+
+def NodeAddImplicitInputs(builder, implicitInputs):
+    builder.PrependUOffsetTRelativeSlot(12, flatbuffers.number_types.UOffsetTFlags.py_type(implicitInputs), 0)
+
+def AddImplicitInputs(builder, implicitInputs):
+    NodeAddImplicitInputs(builder, implicitInputs)
+
+def NodeStartImplicitInputsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartImplicitInputsVector(builder, numElems: int) -> int:
+    return NodeStartImplicitInputsVector(builder, numElems)
+
+def NodeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return NodeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class NodeEdge(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsNodeEdge(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = NodeEdge()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsNodeEdge(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def NodeEdgeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # NodeEdge
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -75,14 +79,48 @@
         return 0
 
     # NodeEdge
     def OutputEdgesIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         return o == 0
 
-def NodeEdgeStart(builder): builder.StartObject(3)
-def NodeEdgeAddNodeIndex(builder, nodeIndex): builder.PrependUint32Slot(0, nodeIndex, 0)
-def NodeEdgeAddInputEdges(builder, inputEdges): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(inputEdges), 0)
-def NodeEdgeStartInputEdgesVector(builder, numElems): return builder.StartVector(12, numElems, 4)
-def NodeEdgeAddOutputEdges(builder, outputEdges): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(outputEdges), 0)
-def NodeEdgeStartOutputEdgesVector(builder, numElems): return builder.StartVector(12, numElems, 4)
-def NodeEdgeEnd(builder): return builder.EndObject()
+def NodeEdgeStart(builder):
+    builder.StartObject(3)
+
+def Start(builder):
+    NodeEdgeStart(builder)
+
+def NodeEdgeAddNodeIndex(builder, nodeIndex):
+    builder.PrependUint32Slot(0, nodeIndex, 0)
+
+def AddNodeIndex(builder, nodeIndex):
+    NodeEdgeAddNodeIndex(builder, nodeIndex)
+
+def NodeEdgeAddInputEdges(builder, inputEdges):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(inputEdges), 0)
+
+def AddInputEdges(builder, inputEdges):
+    NodeEdgeAddInputEdges(builder, inputEdges)
+
+def NodeEdgeStartInputEdgesVector(builder, numElems):
+    return builder.StartVector(12, numElems, 4)
+
+def StartInputEdgesVector(builder, numElems: int) -> int:
+    return NodeEdgeStartInputEdgesVector(builder, numElems)
+
+def NodeEdgeAddOutputEdges(builder, outputEdges):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(outputEdges), 0)
+
+def AddOutputEdges(builder, outputEdges):
+    NodeEdgeAddOutputEdges(builder, outputEdges)
+
+def NodeEdgeStartOutputEdgesVector(builder, numElems):
+    return builder.StartVector(12, numElems, 4)
+
+def StartOutputEdgesVector(builder, numElems: int) -> int:
+    return NodeEdgeStartOutputEdgesVector(builder, numElems)
+
+def NodeEdgeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return NodeEdgeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py

```diff
@@ -1,8 +1,7 @@
 # automatically generated by the FlatBuffers compiler, do not modify
 
 # namespace: fbs
 
 class NodeType(object):
     Primitive = 0
     Fused = 1
-
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py

```diff
@@ -8,21 +8,25 @@
 
 # nodes to consider for a runtime optimization
 # see corresponding type in onnxruntime/core/graph/runtime_optimization_record.h
 class NodesToOptimizeIndices(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsNodesToOptimizeIndices(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = NodesToOptimizeIndices()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsNodesToOptimizeIndices(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def NodesToOptimizeIndicesBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # NodesToOptimizeIndices
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -91,17 +95,66 @@
     # NodesToOptimizeIndices
     def NumVariadicOutputs(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(16))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Uint32Flags, o + self._tab.Pos)
         return 0
 
-def NodesToOptimizeIndicesStart(builder): builder.StartObject(7)
-def NodesToOptimizeIndicesAddNodeIndices(builder, nodeIndices): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(nodeIndices), 0)
-def NodesToOptimizeIndicesStartNodeIndicesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def NodesToOptimizeIndicesAddNumInputs(builder, numInputs): builder.PrependUint32Slot(1, numInputs, 0)
-def NodesToOptimizeIndicesAddNumOutputs(builder, numOutputs): builder.PrependUint32Slot(2, numOutputs, 0)
-def NodesToOptimizeIndicesAddHasVariadicInput(builder, hasVariadicInput): builder.PrependBoolSlot(3, hasVariadicInput, 0)
-def NodesToOptimizeIndicesAddHasVariadicOutput(builder, hasVariadicOutput): builder.PrependBoolSlot(4, hasVariadicOutput, 0)
-def NodesToOptimizeIndicesAddNumVariadicInputs(builder, numVariadicInputs): builder.PrependUint32Slot(5, numVariadicInputs, 0)
-def NodesToOptimizeIndicesAddNumVariadicOutputs(builder, numVariadicOutputs): builder.PrependUint32Slot(6, numVariadicOutputs, 0)
-def NodesToOptimizeIndicesEnd(builder): return builder.EndObject()
+def NodesToOptimizeIndicesStart(builder):
+    builder.StartObject(7)
+
+def Start(builder):
+    NodesToOptimizeIndicesStart(builder)
+
+def NodesToOptimizeIndicesAddNodeIndices(builder, nodeIndices):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(nodeIndices), 0)
+
+def AddNodeIndices(builder, nodeIndices):
+    NodesToOptimizeIndicesAddNodeIndices(builder, nodeIndices)
+
+def NodesToOptimizeIndicesStartNodeIndicesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartNodeIndicesVector(builder, numElems: int) -> int:
+    return NodesToOptimizeIndicesStartNodeIndicesVector(builder, numElems)
+
+def NodesToOptimizeIndicesAddNumInputs(builder, numInputs):
+    builder.PrependUint32Slot(1, numInputs, 0)
+
+def AddNumInputs(builder, numInputs):
+    NodesToOptimizeIndicesAddNumInputs(builder, numInputs)
+
+def NodesToOptimizeIndicesAddNumOutputs(builder, numOutputs):
+    builder.PrependUint32Slot(2, numOutputs, 0)
+
+def AddNumOutputs(builder, numOutputs):
+    NodesToOptimizeIndicesAddNumOutputs(builder, numOutputs)
+
+def NodesToOptimizeIndicesAddHasVariadicInput(builder, hasVariadicInput):
+    builder.PrependBoolSlot(3, hasVariadicInput, 0)
+
+def AddHasVariadicInput(builder, hasVariadicInput):
+    NodesToOptimizeIndicesAddHasVariadicInput(builder, hasVariadicInput)
+
+def NodesToOptimizeIndicesAddHasVariadicOutput(builder, hasVariadicOutput):
+    builder.PrependBoolSlot(4, hasVariadicOutput, 0)
+
+def AddHasVariadicOutput(builder, hasVariadicOutput):
+    NodesToOptimizeIndicesAddHasVariadicOutput(builder, hasVariadicOutput)
+
+def NodesToOptimizeIndicesAddNumVariadicInputs(builder, numVariadicInputs):
+    builder.PrependUint32Slot(5, numVariadicInputs, 0)
+
+def AddNumVariadicInputs(builder, numVariadicInputs):
+    NodesToOptimizeIndicesAddNumVariadicInputs(builder, numVariadicInputs)
+
+def NodesToOptimizeIndicesAddNumVariadicOutputs(builder, numVariadicOutputs):
+    builder.PrependUint32Slot(6, numVariadicOutputs, 0)
+
+def AddNumVariadicOutputs(builder, numVariadicOutputs):
+    NodesToOptimizeIndicesAddNumVariadicOutputs(builder, numVariadicOutputs)
+
+def NodesToOptimizeIndicesEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return NodesToOptimizeIndicesEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class OpIdKernelTypeStrArgsEntry(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsOpIdKernelTypeStrArgsEntry(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = OpIdKernelTypeStrArgsEntry()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsOpIdKernelTypeStrArgsEntry(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def OpIdKernelTypeStrArgsEntryBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # OpIdKernelTypeStrArgsEntry
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -52,12 +56,36 @@
         return 0
 
     # OpIdKernelTypeStrArgsEntry
     def KernelTypeStrArgsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def OpIdKernelTypeStrArgsEntryStart(builder): builder.StartObject(2)
-def OpIdKernelTypeStrArgsEntryAddOpId(builder, opId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(opId), 0)
-def OpIdKernelTypeStrArgsEntryAddKernelTypeStrArgs(builder, kernelTypeStrArgs): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStrArgs), 0)
-def OpIdKernelTypeStrArgsEntryStartKernelTypeStrArgsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def OpIdKernelTypeStrArgsEntryEnd(builder): return builder.EndObject()
+def OpIdKernelTypeStrArgsEntryStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    OpIdKernelTypeStrArgsEntryStart(builder)
+
+def OpIdKernelTypeStrArgsEntryAddOpId(builder, opId):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(opId), 0)
+
+def AddOpId(builder, opId):
+    OpIdKernelTypeStrArgsEntryAddOpId(builder, opId)
+
+def OpIdKernelTypeStrArgsEntryAddKernelTypeStrArgs(builder, kernelTypeStrArgs):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(kernelTypeStrArgs), 0)
+
+def AddKernelTypeStrArgs(builder, kernelTypeStrArgs):
+    OpIdKernelTypeStrArgsEntryAddKernelTypeStrArgs(builder, kernelTypeStrArgs)
+
+def OpIdKernelTypeStrArgsEntryStartKernelTypeStrArgsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartKernelTypeStrArgsVector(builder, numElems: int) -> int:
+    return OpIdKernelTypeStrArgsEntryStartKernelTypeStrArgsVector(builder, numElems)
+
+def OpIdKernelTypeStrArgsEntryEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return OpIdKernelTypeStrArgsEntryEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class OperatorSetId(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsOperatorSetId(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = OperatorSetId()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsOperatorSetId(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def OperatorSetIdBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # OperatorSetId
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -34,11 +38,30 @@
     # OperatorSetId
     def Version(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.Get(flatbuffers.number_types.Int64Flags, o + self._tab.Pos)
         return 0
 
-def OperatorSetIdStart(builder): builder.StartObject(2)
-def OperatorSetIdAddDomain(builder, domain): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
-def OperatorSetIdAddVersion(builder, version): builder.PrependInt64Slot(1, version, 0)
-def OperatorSetIdEnd(builder): return builder.EndObject()
+def OperatorSetIdStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    OperatorSetIdStart(builder)
+
+def OperatorSetIdAddDomain(builder, domain):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(domain), 0)
+
+def AddDomain(builder, domain):
+    OperatorSetIdAddDomain(builder, domain)
+
+def OperatorSetIdAddVersion(builder, version):
+    builder.PrependInt64Slot(1, version, 0)
+
+def AddVersion(builder, version):
+    OperatorSetIdAddVersion(builder, version)
+
+def OperatorSetIdEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return OperatorSetIdEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class OptimizerGroup(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsOptimizerGroup(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = OptimizerGroup()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsOptimizerGroup(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def OptimizerGroupBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # OptimizerGroup
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -66,14 +70,48 @@
         return 0
 
     # OptimizerGroup
     def OptimizerStatesIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
         return o == 0
 
-def OptimizerGroupStart(builder): builder.StartObject(4)
-def OptimizerGroupAddGroupName(builder, groupName): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(groupName), 0)
-def OptimizerGroupAddStep(builder, step): builder.PrependInt64Slot(1, step, 0)
-def OptimizerGroupAddInitialLearningRate(builder, initialLearningRate): builder.PrependFloat32Slot(2, initialLearningRate, 0.0)
-def OptimizerGroupAddOptimizerStates(builder, optimizerStates): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(optimizerStates), 0)
-def OptimizerGroupStartOptimizerStatesVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def OptimizerGroupEnd(builder): return builder.EndObject()
+def OptimizerGroupStart(builder):
+    builder.StartObject(4)
+
+def Start(builder):
+    OptimizerGroupStart(builder)
+
+def OptimizerGroupAddGroupName(builder, groupName):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(groupName), 0)
+
+def AddGroupName(builder, groupName):
+    OptimizerGroupAddGroupName(builder, groupName)
+
+def OptimizerGroupAddStep(builder, step):
+    builder.PrependInt64Slot(1, step, 0)
+
+def AddStep(builder, step):
+    OptimizerGroupAddStep(builder, step)
+
+def OptimizerGroupAddInitialLearningRate(builder, initialLearningRate):
+    builder.PrependFloat32Slot(2, initialLearningRate, 0.0)
+
+def AddInitialLearningRate(builder, initialLearningRate):
+    OptimizerGroupAddInitialLearningRate(builder, initialLearningRate)
+
+def OptimizerGroupAddOptimizerStates(builder, optimizerStates):
+    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(optimizerStates), 0)
+
+def AddOptimizerStates(builder, optimizerStates):
+    OptimizerGroupAddOptimizerStates(builder, optimizerStates)
+
+def OptimizerGroupStartOptimizerStatesVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartOptimizerStatesVector(builder, numElems: int) -> int:
+    return OptimizerGroupStartOptimizerStatesVector(builder, numElems)
+
+def OptimizerGroupEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return OptimizerGroupEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class ParameterOptimizerState(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsParameterOptimizerState(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = ParameterOptimizerState()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsParameterOptimizerState(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def ParameterOptimizerStateBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # ParameterOptimizerState
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -52,12 +56,36 @@
         return 0
 
     # ParameterOptimizerState
     def MomentumsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def ParameterOptimizerStateStart(builder): builder.StartObject(2)
-def ParameterOptimizerStateAddParamName(builder, paramName): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(paramName), 0)
-def ParameterOptimizerStateAddMomentums(builder, momentums): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(momentums), 0)
-def ParameterOptimizerStateStartMomentumsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def ParameterOptimizerStateEnd(builder): return builder.EndObject()
+def ParameterOptimizerStateStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    ParameterOptimizerStateStart(builder)
+
+def ParameterOptimizerStateAddParamName(builder, paramName):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(paramName), 0)
+
+def AddParamName(builder, paramName):
+    ParameterOptimizerStateAddParamName(builder, paramName)
+
+def ParameterOptimizerStateAddMomentums(builder, momentums):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(momentums), 0)
+
+def AddMomentums(builder, momentums):
+    ParameterOptimizerStateAddMomentums(builder, momentums)
+
+def ParameterOptimizerStateStartMomentumsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartMomentumsVector(builder, numElems: int) -> int:
+    return ParameterOptimizerStateStartMomentumsVector(builder, numElems)
+
+def ParameterOptimizerStateEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return ParameterOptimizerStateEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class PropertyBag(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsPropertyBag(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = PropertyBag()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsPropertyBag(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def PropertyBagBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # PropertyBag
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -95,15 +99,54 @@
         return 0
 
     # PropertyBag
     def StringsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         return o == 0
 
-def PropertyBagStart(builder): builder.StartObject(3)
-def PropertyBagAddInts(builder, ints): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(ints), 0)
-def PropertyBagStartIntsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def PropertyBagAddFloats(builder, floats): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(floats), 0)
-def PropertyBagStartFloatsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def PropertyBagAddStrings(builder, strings): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(strings), 0)
-def PropertyBagStartStringsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def PropertyBagEnd(builder): return builder.EndObject()
+def PropertyBagStart(builder):
+    builder.StartObject(3)
+
+def Start(builder):
+    PropertyBagStart(builder)
+
+def PropertyBagAddInts(builder, ints):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(ints), 0)
+
+def AddInts(builder, ints):
+    PropertyBagAddInts(builder, ints)
+
+def PropertyBagStartIntsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartIntsVector(builder, numElems: int) -> int:
+    return PropertyBagStartIntsVector(builder, numElems)
+
+def PropertyBagAddFloats(builder, floats):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(floats), 0)
+
+def AddFloats(builder, floats):
+    PropertyBagAddFloats(builder, floats)
+
+def PropertyBagStartFloatsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartFloatsVector(builder, numElems: int) -> int:
+    return PropertyBagStartFloatsVector(builder, numElems)
+
+def PropertyBagAddStrings(builder, strings):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(strings), 0)
+
+def AddStrings(builder, strings):
+    PropertyBagAddStrings(builder, strings)
+
+def PropertyBagStartStringsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartStringsVector(builder, numElems: int) -> int:
+    return PropertyBagStartStringsVector(builder, numElems)
+
+def PropertyBagEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return PropertyBagEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py

```diff
@@ -8,21 +8,25 @@
 
 # a single runtime optimization
 # see corresponding type in onnxruntime/core/graph/runtime_optimization_record.h
 class RuntimeOptimizationRecord(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsRuntimeOptimizationRecord(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = RuntimeOptimizationRecord()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsRuntimeOptimizationRecord(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def RuntimeOptimizationRecordBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # RuntimeOptimizationRecord
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -60,13 +64,42 @@
         return 0
 
     # RuntimeOptimizationRecord
     def ProducedOpIdsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
         return o == 0
 
-def RuntimeOptimizationRecordStart(builder): builder.StartObject(4)
-def RuntimeOptimizationRecordAddActionId(builder, actionId): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(actionId), 0)
-def RuntimeOptimizationRecordAddNodesToOptimizeIndices(builder, nodesToOptimizeIndices): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(nodesToOptimizeIndices), 0)
-def RuntimeOptimizationRecordAddProducedOpIds(builder, producedOpIds): builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(producedOpIds), 0)
-def RuntimeOptimizationRecordStartProducedOpIdsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def RuntimeOptimizationRecordEnd(builder): return builder.EndObject()
+def RuntimeOptimizationRecordStart(builder):
+    builder.StartObject(4)
+
+def Start(builder):
+    RuntimeOptimizationRecordStart(builder)
+
+def RuntimeOptimizationRecordAddActionId(builder, actionId):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(actionId), 0)
+
+def AddActionId(builder, actionId):
+    RuntimeOptimizationRecordAddActionId(builder, actionId)
+
+def RuntimeOptimizationRecordAddNodesToOptimizeIndices(builder, nodesToOptimizeIndices):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(nodesToOptimizeIndices), 0)
+
+def AddNodesToOptimizeIndices(builder, nodesToOptimizeIndices):
+    RuntimeOptimizationRecordAddNodesToOptimizeIndices(builder, nodesToOptimizeIndices)
+
+def RuntimeOptimizationRecordAddProducedOpIds(builder, producedOpIds):
+    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(producedOpIds), 0)
+
+def AddProducedOpIds(builder, producedOpIds):
+    RuntimeOptimizationRecordAddProducedOpIds(builder, producedOpIds)
+
+def RuntimeOptimizationRecordStartProducedOpIdsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartProducedOpIdsVector(builder, numElems: int) -> int:
+    return RuntimeOptimizationRecordStartProducedOpIdsVector(builder, numElems)
+
+def RuntimeOptimizationRecordEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return RuntimeOptimizationRecordEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class RuntimeOptimizationRecordContainerEntry(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsRuntimeOptimizationRecordContainerEntry(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = RuntimeOptimizationRecordContainerEntry()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsRuntimeOptimizationRecordContainerEntry(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def RuntimeOptimizationRecordContainerEntryBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # RuntimeOptimizationRecordContainerEntry
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -52,12 +56,36 @@
         return 0
 
     # RuntimeOptimizationRecordContainerEntry
     def RuntimeOptimizationRecordsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         return o == 0
 
-def RuntimeOptimizationRecordContainerEntryStart(builder): builder.StartObject(2)
-def RuntimeOptimizationRecordContainerEntryAddOptimizerName(builder, optimizerName): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(optimizerName), 0)
-def RuntimeOptimizationRecordContainerEntryAddRuntimeOptimizationRecords(builder, runtimeOptimizationRecords): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(runtimeOptimizationRecords), 0)
-def RuntimeOptimizationRecordContainerEntryStartRuntimeOptimizationRecordsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def RuntimeOptimizationRecordContainerEntryEnd(builder): return builder.EndObject()
+def RuntimeOptimizationRecordContainerEntryStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    RuntimeOptimizationRecordContainerEntryStart(builder)
+
+def RuntimeOptimizationRecordContainerEntryAddOptimizerName(builder, optimizerName):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(optimizerName), 0)
+
+def AddOptimizerName(builder, optimizerName):
+    RuntimeOptimizationRecordContainerEntryAddOptimizerName(builder, optimizerName)
+
+def RuntimeOptimizationRecordContainerEntryAddRuntimeOptimizationRecords(builder, runtimeOptimizationRecords):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(runtimeOptimizationRecords), 0)
+
+def AddRuntimeOptimizationRecords(builder, runtimeOptimizationRecords):
+    RuntimeOptimizationRecordContainerEntryAddRuntimeOptimizationRecords(builder, runtimeOptimizationRecords)
+
+def RuntimeOptimizationRecordContainerEntryStartRuntimeOptimizationRecordsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartRuntimeOptimizationRecordsVector(builder, numElems: int) -> int:
+    return RuntimeOptimizationRecordContainerEntryStartRuntimeOptimizationRecordsVector(builder, numElems)
+
+def RuntimeOptimizationRecordContainerEntryEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return RuntimeOptimizationRecordContainerEntryEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class RuntimeOptimizations(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsRuntimeOptimizations(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = RuntimeOptimizations()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsRuntimeOptimizations(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def RuntimeOptimizationsBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # RuntimeOptimizations
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -46,11 +50,30 @@
         return 0
 
     # RuntimeOptimizations
     def RecordsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         return o == 0
 
-def RuntimeOptimizationsStart(builder): builder.StartObject(1)
-def RuntimeOptimizationsAddRecords(builder, records): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(records), 0)
-def RuntimeOptimizationsStartRecordsVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def RuntimeOptimizationsEnd(builder): return builder.EndObject()
+def RuntimeOptimizationsStart(builder):
+    builder.StartObject(1)
+
+def Start(builder):
+    RuntimeOptimizationsStart(builder)
+
+def RuntimeOptimizationsAddRecords(builder, records):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(records), 0)
+
+def AddRecords(builder, records):
+    RuntimeOptimizationsAddRecords(builder, records)
+
+def RuntimeOptimizationsStartRecordsVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartRecordsVector(builder, numElems: int) -> int:
+    return RuntimeOptimizationsStartRecordsVector(builder, numElems)
+
+def RuntimeOptimizationsEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return RuntimeOptimizationsEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class SequenceType(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsSequenceType(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = SequenceType()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsSequenceType(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def SequenceTypeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # SequenceType
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -31,10 +35,24 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
             obj = TypeInfo()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def SequenceTypeStart(builder): builder.StartObject(1)
-def SequenceTypeAddElemType(builder, elemType): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(elemType), 0)
-def SequenceTypeEnd(builder): return builder.EndObject()
+def SequenceTypeStart(builder):
+    builder.StartObject(1)
+
+def Start(builder):
+    SequenceTypeStart(builder)
+
+def SequenceTypeAddElemType(builder, elemType):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(elemType), 0)
+
+def AddElemType(builder, elemType):
+    SequenceTypeAddElemType(builder, elemType)
+
+def SequenceTypeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return SequenceTypeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Shape(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsShape(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Shape()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsShape(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def ShapeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Shape
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -45,11 +49,30 @@
         return 0
 
     # Shape
     def DimIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
         return o == 0
 
-def ShapeStart(builder): builder.StartObject(1)
-def ShapeAddDim(builder, dim): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(dim), 0)
-def ShapeStartDimVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def ShapeEnd(builder): return builder.EndObject()
+def ShapeStart(builder):
+    builder.StartObject(1)
+
+def Start(builder):
+    ShapeStart(builder)
+
+def ShapeAddDim(builder, dim):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(dim), 0)
+
+def AddDim(builder, dim):
+    ShapeAddDim(builder, dim)
+
+def ShapeStartDimVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartDimVector(builder, numElems: int) -> int:
+    return ShapeStartDimVector(builder, numElems)
+
+def ShapeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return ShapeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class SparseTensor(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsSparseTensor(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = SparseTensor()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsSparseTensor(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def SparseTensorBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # SparseTensor
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -69,13 +73,42 @@
         return 0
 
     # SparseTensor
     def DimsIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
         return o == 0
 
-def SparseTensorStart(builder): builder.StartObject(3)
-def SparseTensorAddValues(builder, values): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(values), 0)
-def SparseTensorAddIndices(builder, indices): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(indices), 0)
-def SparseTensorAddDims(builder, dims): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dims), 0)
-def SparseTensorStartDimsVector(builder, numElems): return builder.StartVector(8, numElems, 8)
-def SparseTensorEnd(builder): return builder.EndObject()
+def SparseTensorStart(builder):
+    builder.StartObject(3)
+
+def Start(builder):
+    SparseTensorStart(builder)
+
+def SparseTensorAddValues(builder, values):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(values), 0)
+
+def AddValues(builder, values):
+    SparseTensorAddValues(builder, values)
+
+def SparseTensorAddIndices(builder, indices):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(indices), 0)
+
+def AddIndices(builder, indices):
+    SparseTensorAddIndices(builder, indices)
+
+def SparseTensorAddDims(builder, dims):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dims), 0)
+
+def AddDims(builder, dims):
+    SparseTensorAddDims(builder, dims)
+
+def SparseTensorStartDimsVector(builder, numElems):
+    return builder.StartVector(8, numElems, 8)
+
+def StartDimsVector(builder, numElems: int) -> int:
+    return SparseTensorStartDimsVector(builder, numElems)
+
+def SparseTensorEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return SparseTensorEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class StringProperty(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsStringProperty(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = StringProperty()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsStringProperty(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def StringPropertyBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x44\x54\x43", size_prefixed=size_prefixed)
 
     # StringProperty
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -34,11 +38,30 @@
     # StringProperty
     def Value(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-def StringPropertyStart(builder): builder.StartObject(2)
-def StringPropertyAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def StringPropertyAddValue(builder, value): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
-def StringPropertyEnd(builder): return builder.EndObject()
+def StringPropertyStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    StringPropertyStart(builder)
+
+def StringPropertyAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    StringPropertyAddName(builder, name)
+
+def StringPropertyAddValue(builder, value):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
+
+def AddValue(builder, value):
+    StringPropertyAddValue(builder, value)
+
+def StringPropertyEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return StringPropertyEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class StringStringEntry(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsStringStringEntry(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = StringStringEntry()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsStringStringEntry(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def StringStringEntryBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # StringStringEntry
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -34,11 +38,30 @@
     # StringStringEntry
     def Value(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
         if o != 0:
             return self._tab.String(o + self._tab.Pos)
         return None
 
-def StringStringEntryStart(builder): builder.StartObject(2)
-def StringStringEntryAddKey(builder, key): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(key), 0)
-def StringStringEntryAddValue(builder, value): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
-def StringStringEntryEnd(builder): return builder.EndObject()
+def StringStringEntryStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    StringStringEntryStart(builder)
+
+def StringStringEntryAddKey(builder, key):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(key), 0)
+
+def AddKey(builder, key):
+    StringStringEntryAddKey(builder, key)
+
+def StringStringEntryAddValue(builder, value):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
+
+def AddValue(builder, value):
+    StringStringEntryAddValue(builder, value)
+
+def StringStringEntryEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return StringStringEntryEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class Tensor(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsTensor(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = Tensor()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsTensor(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def TensorBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # Tensor
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -115,18 +119,85 @@
         return 0
 
     # Tensor
     def StringDataIsNone(self):
         o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(14))
         return o == 0
 
-def TensorStart(builder): builder.StartObject(6)
-def TensorAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def TensorAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
-def TensorAddDims(builder, dims): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dims), 0)
-def TensorStartDimsVector(builder, numElems): return builder.StartVector(8, numElems, 8)
-def TensorAddDataType(builder, dataType): builder.PrependInt32Slot(3, dataType, 0)
-def TensorAddRawData(builder, rawData): builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(rawData), 0)
-def TensorStartRawDataVector(builder, numElems): return builder.StartVector(1, numElems, 1)
-def TensorAddStringData(builder, stringData): builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(stringData), 0)
-def TensorStartStringDataVector(builder, numElems): return builder.StartVector(4, numElems, 4)
-def TensorEnd(builder): return builder.EndObject()
+    # Tensor
+    def ExternalDataOffset(self):
+        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(16))
+        if o != 0:
+            return self._tab.Get(flatbuffers.number_types.Int64Flags, o + self._tab.Pos)
+        return -1
+
+def TensorStart(builder):
+    builder.StartObject(7)
+
+def Start(builder):
+    TensorStart(builder)
+
+def TensorAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    TensorAddName(builder, name)
+
+def TensorAddDocString(builder, docString):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
+
+def AddDocString(builder, docString):
+    TensorAddDocString(builder, docString)
+
+def TensorAddDims(builder, dims):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(dims), 0)
+
+def AddDims(builder, dims):
+    TensorAddDims(builder, dims)
+
+def TensorStartDimsVector(builder, numElems):
+    return builder.StartVector(8, numElems, 8)
+
+def StartDimsVector(builder, numElems: int) -> int:
+    return TensorStartDimsVector(builder, numElems)
+
+def TensorAddDataType(builder, dataType):
+    builder.PrependInt32Slot(3, dataType, 0)
+
+def AddDataType(builder, dataType):
+    TensorAddDataType(builder, dataType)
+
+def TensorAddRawData(builder, rawData):
+    builder.PrependUOffsetTRelativeSlot(4, flatbuffers.number_types.UOffsetTFlags.py_type(rawData), 0)
+
+def AddRawData(builder, rawData):
+    TensorAddRawData(builder, rawData)
+
+def TensorStartRawDataVector(builder, numElems):
+    return builder.StartVector(1, numElems, 1)
+
+def StartRawDataVector(builder, numElems: int) -> int:
+    return TensorStartRawDataVector(builder, numElems)
+
+def TensorAddStringData(builder, stringData):
+    builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(stringData), 0)
+
+def AddStringData(builder, stringData):
+    TensorAddStringData(builder, stringData)
+
+def TensorStartStringDataVector(builder, numElems):
+    return builder.StartVector(4, numElems, 4)
+
+def StartStringDataVector(builder, numElems: int) -> int:
+    return TensorStartStringDataVector(builder, numElems)
+
+def TensorAddExternalDataOffset(builder, externalDataOffset):
+    builder.PrependInt64Slot(6, externalDataOffset, -1)
+
+def AddExternalDataOffset(builder, externalDataOffset):
+    TensorAddExternalDataOffset(builder, externalDataOffset)
+
+def TensorEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return TensorEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py

```diff
@@ -20,8 +20,7 @@
     COMPLEX64 = 14
     COMPLEX128 = 15
     BFLOAT16 = 16
     FLOAT8E4M3FN = 17
     FLOAT8E4M3FNUZ = 18
     FLOAT8E5M2 = 19
     FLOAT8E5M2FNUZ = 20
-
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class TensorTypeAndShape(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsTensorTypeAndShape(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = TensorTypeAndShape()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsTensorTypeAndShape(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def TensorTypeAndShapeBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # TensorTypeAndShape
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -38,11 +42,30 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.Shape import Shape
             obj = Shape()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def TensorTypeAndShapeStart(builder): builder.StartObject(2)
-def TensorTypeAndShapeAddElemType(builder, elemType): builder.PrependInt32Slot(0, elemType, 0)
-def TensorTypeAndShapeAddShape(builder, shape): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(shape), 0)
-def TensorTypeAndShapeEnd(builder): return builder.EndObject()
+def TensorTypeAndShapeStart(builder):
+    builder.StartObject(2)
+
+def Start(builder):
+    TensorTypeAndShapeStart(builder)
+
+def TensorTypeAndShapeAddElemType(builder, elemType):
+    builder.PrependInt32Slot(0, elemType, 0)
+
+def AddElemType(builder, elemType):
+    TensorTypeAndShapeAddElemType(builder, elemType)
+
+def TensorTypeAndShapeAddShape(builder, shape):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(shape), 0)
+
+def AddShape(builder, shape):
+    TensorTypeAndShapeAddShape(builder, shape)
+
+def TensorTypeAndShapeEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return TensorTypeAndShapeEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class TypeInfo(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsTypeInfo(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = TypeInfo()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsTypeInfo(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def TypeInfoBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # TypeInfo
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -44,12 +48,36 @@
         if o != 0:
             from flatbuffers.table import Table
             obj = Table(bytearray(), 0)
             self._tab.Union(obj, o)
             return obj
         return None
 
-def TypeInfoStart(builder): builder.StartObject(3)
-def TypeInfoAddDenotation(builder, denotation): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(denotation), 0)
-def TypeInfoAddValueType(builder, valueType): builder.PrependUint8Slot(1, valueType, 0)
-def TypeInfoAddValue(builder, value): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
-def TypeInfoEnd(builder): return builder.EndObject()
+def TypeInfoStart(builder):
+    builder.StartObject(3)
+
+def Start(builder):
+    TypeInfoStart(builder)
+
+def TypeInfoAddDenotation(builder, denotation):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(denotation), 0)
+
+def AddDenotation(builder, denotation):
+    TypeInfoAddDenotation(builder, denotation)
+
+def TypeInfoAddValueType(builder, valueType):
+    builder.PrependUint8Slot(1, valueType, 0)
+
+def AddValueType(builder, valueType):
+    TypeInfoAddValueType(builder, valueType)
+
+def TypeInfoAddValue(builder, value):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(value), 0)
+
+def AddValue(builder, value):
+    TypeInfoAddValue(builder, value)
+
+def TypeInfoEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return TypeInfoEnd(builder)
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py

```diff
@@ -3,8 +3,7 @@
 # namespace: fbs
 
 class TypeInfoValue(object):
     NONE = 0
     tensor_type = 1
     sequence_type = 2
     map_type = 3
-
```

## onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py

```diff
@@ -6,21 +6,25 @@
 from flatbuffers.compat import import_numpy
 np = import_numpy()
 
 class ValueInfo(object):
     __slots__ = ['_tab']
 
     @classmethod
-    def GetRootAsValueInfo(cls, buf, offset):
+    def GetRootAs(cls, buf, offset=0):
         n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
         x = ValueInfo()
         x.Init(buf, n + offset)
         return x
 
     @classmethod
+    def GetRootAsValueInfo(cls, buf, offset=0):
+        """This method is deprecated. Please switch to GetRootAs."""
+        return cls.GetRootAs(buf, offset)
+    @classmethod
     def ValueInfoBufferHasIdentifier(cls, buf, offset, size_prefixed=False):
         return flatbuffers.util.BufferHasIdentifier(buf, offset, b"\x4F\x52\x54\x4D", size_prefixed=size_prefixed)
 
     # ValueInfo
     def Init(self, buf, pos):
         self._tab = flatbuffers.table.Table(buf, pos)
 
@@ -45,12 +49,36 @@
             x = self._tab.Indirect(o + self._tab.Pos)
             from ort_flatbuffers_py.fbs.TypeInfo import TypeInfo
             obj = TypeInfo()
             obj.Init(self._tab.Bytes, x)
             return obj
         return None
 
-def ValueInfoStart(builder): builder.StartObject(3)
-def ValueInfoAddName(builder, name): builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
-def ValueInfoAddDocString(builder, docString): builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
-def ValueInfoAddType(builder, type): builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(type), 0)
-def ValueInfoEnd(builder): return builder.EndObject()
+def ValueInfoStart(builder):
+    builder.StartObject(3)
+
+def Start(builder):
+    ValueInfoStart(builder)
+
+def ValueInfoAddName(builder, name):
+    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(name), 0)
+
+def AddName(builder, name):
+    ValueInfoAddName(builder, name)
+
+def ValueInfoAddDocString(builder, docString):
+    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(docString), 0)
+
+def AddDocString(builder, docString):
+    ValueInfoAddDocString(builder, docString)
+
+def ValueInfoAddType(builder, type):
+    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(type), 0)
+
+def AddType(builder, type):
+    ValueInfoAddType(builder, type)
+
+def ValueInfoEnd(builder):
+    return builder.EndObject()
+
+def End(builder):
+    return ValueInfoEnd(builder)
```

## onnxruntime/training/__init__.py

```diff
@@ -19,13 +19,13 @@
     "amp",
     "artifacts",
     "optim",
 ]
 
 try:
     if is_ortmodule_available():
-        from .ortmodule import ORTModule  # noqa: F401
+        from .ortmodule import ORTModule
 
-        __all__.append("ORTModule")
+        __all__ += ["ORTModule"]
 except ImportError:
     # That is OK iff this is not a ORTModule training package
     pass
```

## onnxruntime/training/artifacts.py

```diff
@@ -37,17 +37,22 @@
 
 
 def generate_artifacts(
     model: onnx.ModelProto,
     requires_grad: Optional[List[str]] = None,
     frozen_params: Optional[List[str]] = None,
     loss: Optional[Union[LossType, onnxblock.Block]] = None,
-    optimizer: Optional[OptimType] = None,
+    optimizer: Optional[Union[OptimType, onnxblock.Block]] = None,
     artifact_directory: Optional[Union[str, bytes, os.PathLike]] = None,
-    **extra_options,
+    prefix: str = "",
+    ort_format: bool = False,
+    custom_op_library: Optional[Union[str, bytes, os.PathLike]] = None,
+    additional_output_names: Optional[List[str]] = None,
+    nominal_checkpoint: bool = False,
+    loss_input_names: Optional[List[str]] = None,
 ) -> None:
     """Generates artifacts required for training with ORT training api.
 
     This function generates the following artifacts:
         1. Training model (onnx.ModelProto): Contains the base model graph, loss sub graph and the gradient graph.
         2. Eval model (onnx.ModelProto):  Contains the base model graph and the loss sub graph
         3. Checkpoint (directory): Contains the model parameters.
@@ -55,24 +60,31 @@
 
     All generated ModelProtos will use the same opsets defined by *model*.
 
     Args:
         model: The base model to be used for gradient graph generation.
         requires_grad: List of names of model parameters that require gradient computation
         frozen_params: List of names of model parameters that should be frozen.
-        loss: The loss function enum to be used for training. If None, no loss node is added to the graph.
-        optimizer: The optimizer enum to be used for training. If None, no optimizer model is generated.
+        loss: The loss function enum or onnxblock to be used for training. If None, no loss node is added to the graph.
+        optimizer: The optimizer enum or onnxblock to be used for training. If None, no optimizer model is generated.
         artifact_directory: The directory to save the generated artifacts.
             If None, the current working directory is used.
-        prefix (str): The prefix to be used for the generated artifacts. If not specified, no prefix is used.
-        ort_format (bool): Whether to save the generated artifacts in ORT format or not. Default is False.
-        custom_op_library (str | os.PathLike): The path to the custom op library.
-                                               If not specified, no custom op library is used.
-        additional_output_names (List[str]): List of additional output names to be added to the training/eval model.
-
+        prefix: The prefix to be used for the generated artifacts. If not specified, no prefix is used.
+        ort_format: Whether to save the generated artifacts in ORT format or not. Default is False.
+        custom_op_library: The path to the custom op library.
+            If not specified, no custom op library is used.
+        additional_output_names: List of additional output names to be added to the training/eval model in addition
+            to the loss output. Default is None.
+        nominal_checkpoint: Whether to generate the nominal checkpoint in addition to the complete checkpoint.
+            Default is False. Nominal checkpoint is a checkpoint that contains nominal information about the model
+            parameters. It can be used on the device to reduce overhead while constructing the training model
+            as well as to reduce the size of the checkpoint packaged with the on-device application.
+        loss_input_names: Specifies a list of input names to be used specifically for the loss computation. When provided,
+            only these inputs will be passed to the loss function. If `None`, all graph outputs are passed to
+            the loss function.
     Raises:
         RuntimeError: If the loss provided is neither one of the supported losses nor an instance of `onnxblock.Block`
         RuntimeError: If the optimizer provided is not one of the supported optimizers.
     """
 
     loss_blocks = {
         LossType.MSELoss: onnxblock.loss.MSELoss,
@@ -98,36 +110,41 @@
                 f"Unknown loss provided {type(loss)}. Expected loss to be either one of"
                 "onnxruntime.training.artifacts.LossType or onnxruntime.training.onnxblock.Block."
             )
         loss_block = loss
         logging.info("Custom loss block provided: %s", loss.__class__.__name__)
 
     class _TrainingBlock(onnxblock.TrainingBlock):
-        def __init__(self, _loss):
+        def __init__(self, _loss, _loss_input_names=None):
             super().__init__()
             self._loss = _loss
+            self._loss_input_names = _loss_input_names
 
         def build(self, *inputs_to_loss):
-            if "additional_output_names" in extra_options:
+            # If loss_input_names is passed, only pass the specified input names to the loss function.
+            if self._loss_input_names:
+                inputs_to_loss = self._loss_input_names
+
+            if additional_output_names:
                 # If additional output names is not a list, raise an error
-                if not isinstance(extra_options["additional_output_names"], list):
+                if not isinstance(additional_output_names, list):
                     raise RuntimeError(
-                        f"Unknown type provided for additional output names {type(extra_options['additional_output_names'])}. "
+                        f"Unknown type provided for additional output names {type(additional_output_names)}. "
                         "Expected additional output names to be a list of strings."
                     )
 
                 loss_output = self._loss(*inputs_to_loss)
                 if isinstance(loss_output, tuple):
-                    return (*loss_output, *tuple(extra_options["additional_output_names"]))
+                    return (*loss_output, *tuple(additional_output_names))
                 else:
-                    return (loss_output, *tuple(extra_options["additional_output_names"]))
+                    return (loss_output, *tuple(additional_output_names))
 
             return self._loss(*inputs_to_loss)
 
-    training_block = _TrainingBlock(loss_block)
+    training_block = _TrainingBlock(loss_block, loss_input_names)
 
     if requires_grad is not None and frozen_params is not None and set(requires_grad).intersection(set(frozen_params)):
         raise RuntimeError(
             "A parameter cannot be frozen and require gradient computation at the same "
             f"time {set(requires_grad).intersection(set(frozen_params))}"
         )
 
@@ -139,91 +156,95 @@
         for arg in frozen_params:
             training_block.requires_grad(arg, False)
 
     training_model = None
     eval_model = None
     model_params = None
 
-    custom_op_library = extra_options.get("custom_op_library", None)
+    custom_op_library_path = None
     if custom_op_library is not None:
         logging.info("Custom op library provided: %s", custom_op_library)
-        custom_op_library = pathlib.Path(custom_op_library)
+        custom_op_library_path = pathlib.Path(custom_op_library)
 
-    with onnxblock.base(model), onnxblock.custom_op_library(
-        custom_op_library
-    ) if custom_op_library is not None else contextlib.nullcontext():
+    with onnxblock.base(model), (
+        onnxblock.custom_op_library(custom_op_library_path)
+        if custom_op_library is not None
+        else contextlib.nullcontext()
+    ):
         _ = training_block(*[output.name for output in model.graph.output])
         training_model, eval_model = training_block.to_model_proto()
         model_params = training_block.parameters()
 
-    def _export_to_ort_format(model_path, output_dir, extra_options):
-        if extra_options.get("ort_format", False):
-            custom_op_library = extra_options.get("custom_op_library", None)
-            if custom_op_library is not None:
-                custom_op_library = pathlib.Path(custom_op_library)
+    def _export_to_ort_format(model_path, output_dir, ort_format, custom_op_library_path):
+        if ort_format:
             convert_onnx_models_to_ort(
                 model_path,
                 output_dir=output_dir,
-                custom_op_library_path=custom_op_library,
+                custom_op_library_path=custom_op_library_path,
                 optimization_styles=[OptimizationStyle.Fixed],
             )
 
     if artifact_directory is None:
         artifact_directory = pathlib.Path.cwd()
-    prefix = ""
-    if "prefix" in extra_options:
-        prefix = extra_options["prefix"]
-        logging.info("Using prefix %s for generated artifacts.", prefix)
-
     artifact_directory = pathlib.Path(artifact_directory)
 
+    if prefix:
+        logging.info("Using prefix %s for generated artifacts.", prefix)
+
     training_model_path = artifact_directory / f"{prefix}training_model.onnx"
     if os.path.exists(training_model_path):
         logging.info("Training model path %s already exists. Overwriting.", training_model_path)
     onnx.save(training_model, training_model_path)
-    _export_to_ort_format(training_model_path, artifact_directory, extra_options)
+    _export_to_ort_format(training_model_path, artifact_directory, ort_format, custom_op_library_path)
     logging.info("Saved training model to %s", training_model_path)
 
     eval_model_path = artifact_directory / f"{prefix}eval_model.onnx"
     if os.path.exists(eval_model_path):
         logging.info("Eval model path %s already exists. Overwriting.", eval_model_path)
     onnx.save(eval_model, eval_model_path)
-    _export_to_ort_format(eval_model_path, artifact_directory, extra_options)
+    _export_to_ort_format(eval_model_path, artifact_directory, ort_format, custom_op_library_path)
     logging.info("Saved eval model to %s", eval_model_path)
 
     checkpoint_path = artifact_directory / f"{prefix}checkpoint"
     if os.path.exists(checkpoint_path):
         logging.info("Checkpoint path %s already exists. Overwriting.", checkpoint_path)
-    onnxblock.save_checkpoint(training_block.parameters(), checkpoint_path)
+    onnxblock.save_checkpoint(training_block.parameters(), checkpoint_path, nominal_checkpoint=False)
     logging.info("Saved checkpoint to %s", checkpoint_path)
+    if nominal_checkpoint:
+        nominal_checkpoint_path = artifact_directory / f"{prefix}nominal_checkpoint"
+        onnxblock.save_checkpoint(training_block.parameters(), nominal_checkpoint_path, nominal_checkpoint=True)
+        logging.info("Saved nominal checkpoint to %s", nominal_checkpoint_path)
 
     # If optimizer is not specified, skip creating the optimizer model
     if optimizer is None:
         logging.info("No optimizer enum provided. Skipping optimizer model generation.")
         return
 
-    if not isinstance(optimizer, OptimType):
-        raise RuntimeError(
-            f"Unknown optimizer provided {type(optimizer)}. Expected optimizer to be of type "
-            "onnxruntime.training.artifacts.OptimType."
-        )
-
-    logging.info("Optimizer enum provided: %s", optimizer.name)
-
     opset_version = None
     for domain in model.opset_import:
         if domain.domain == "" or domain.domain == "ai.onnx":
             opset_version = domain.version
             break
 
     optim_model = None
     optim_blocks = {OptimType.AdamW: onnxblock.optim.AdamW, OptimType.SGD: onnxblock.optim.SGD}
+    optim_block = None
+    if isinstance(optimizer, OptimType):
+        logging.info("Optimizer enum provided: %s", optimizer.name)
+        optim_block = optim_blocks[optimizer]()
+    elif isinstance(optimizer, onnxblock.Block):
+        logging.info("Optimizer block provided: %s", optimizer.__class__.__name__)
+        optim_block = optimizer
+    else:
+        raise TypeError(
+            f"Unknown optimizer provided {type(optimizer)}. Expected optimizer to be either one of"
+            "onnxruntime.training.artifacts.OptimType or onnxruntime.training.onnxblock.Block."
+        )
 
-    optim_block = optim_blocks[optimizer]()
     with onnxblock.empty_base(opset_version=opset_version):
         _ = optim_block(model_params)
         optim_model = optim_block.to_model_proto()
 
     optimizer_model_path = artifact_directory / f"{prefix}optimizer_model.onnx"
     onnx.save(optim_model, optimizer_model_path)
-    _export_to_ort_format(optimizer_model_path, artifact_directory, extra_options)
+    _export_to_ort_format(optimizer_model_path, artifact_directory, ort_format, custom_op_library_path)
     logging.info("Saved optimizer model to %s", optimizer_model_path)
```

## onnxruntime/training/api/checkpoint_state.py

```diff
@@ -218,14 +218,16 @@
         self._parameters = Parameters(self._state)
         self._properties = Properties(self._state)
 
     @classmethod
     def load_checkpoint(cls, checkpoint_uri: str | os.PathLike) -> CheckpointState:
         """Loads the checkpoint state from the checkpoint file
 
+        The checkpoint file can either be the complete checkpoint or the nominal checkpoint.
+
         Args:
             checkpoint_uri: The path to the checkpoint file.
 
         Returns:
             CheckpointState: The checkpoint state object.
         """
         return cls(C.load_checkpoint(os.fspath(checkpoint_uri)))
```

## onnxruntime/training/api/module.py

```diff
@@ -174,14 +174,17 @@
             The number of primitive (example floating point) elements in the parameters.
         """
         return self._model.get_parameters_size(trainable_only)
 
     def copy_buffer_to_parameters(self, buffer: OrtValue, trainable_only: bool = True) -> None:
         """Copies the OrtValue buffer to the training session parameters.
 
+        In case the module was loaded from a nominal checkpoint, invoking this function is required
+        to load the updated parameters onto the checkpoint to complete it.
+
         Args:
             buffer: The OrtValue buffer to copy to the training session parameters.
         """
         self._model.copy_buffer_to_parameters(buffer, trainable_only)
 
     def export_model_for_inferencing(
         self, inference_model_uri: str | os.PathLike, graph_output_names: list[str]
```

## onnxruntime/training/onnxblock/checkpoint_utils.py

```diff
@@ -2,39 +2,42 @@
 # Licensed under the MIT License.
 
 import os
 from typing import List, Tuple, Union
 
 import onnx
 
-from onnxruntime.capi._pybind_state import get_model_after_loading_checkpoint as _internal_load_checkpoint_to_model
-from onnxruntime.capi._pybind_state import save_checkpoint as _internal_save_checkpoint
+from onnxruntime.capi._pybind_state import get_model_after_loading_checkpoint as _load_checkpoint_to_model
+from onnxruntime.capi._pybind_state import save_checkpoint as _save_checkpoint
 
 
 def save_checkpoint(
-    parameters: Tuple[List[onnx.TensorProto], List[onnx.TensorProto]], path_to_checkpoint: Union[str, os.PathLike]
+    parameters: Tuple[List[onnx.TensorProto], List[onnx.TensorProto]],
+    path_to_checkpoint: Union[str, os.PathLike],
+    nominal_checkpoint: bool = False,
 ) -> None:
     """Saves the parameters to the checkpoint directory path_to_checkpoint.
 
     Args:
         parameters tuple(trainable_params, non_trainable_params): The parameters to save to the checkpoint file.
-        path_to_checkpoint (str): The path to the checkpoint directory.
+        path_to_checkpoint: The path to the checkpoint directory.
+        nominal_checkpoint: If True, the checkpoint is saved as a nominal checkpoint. Default is False.
     """
 
     if parameters is None:
         raise RuntimeError("No checkpoint parameters provided.")
 
     trainable_params, non_trainable_params = parameters
     trainable_params = [param.SerializeToString() for param in trainable_params]
     non_trainable_params = [param.SerializeToString() for param in non_trainable_params]
-    _internal_save_checkpoint(trainable_params, non_trainable_params, os.fspath(path_to_checkpoint))
+    _save_checkpoint(trainable_params, non_trainable_params, os.fspath(path_to_checkpoint), nominal_checkpoint)
 
 
 def load_checkpoint_to_model(path_to_checkpoint: Union[str, os.PathLike], model: onnx.ModelProto) -> None:
     """Loads the checkpoint to an onnx inference model.
 
     Args:
         path_to_checkpoint (str): The path to the checkpoint directory.
         model (onnx.ModelProto): The model to load the checkpoint to.
     """
 
-    model.ParseFromString(_internal_load_checkpoint_to_model(os.fspath(path_to_checkpoint), model.SerializeToString()))
+    model.ParseFromString(_load_checkpoint_to_model(os.fspath(path_to_checkpoint), model.SerializeToString()))
```

## onnxruntime/training/optim/_apex_amp_modifier.py

```diff
@@ -11,15 +11,14 @@
 
 from ._modifier import FP16OptimizerModifier
 
 
 class ApexAMPModifier(FP16OptimizerModifier):
     def __init__(self, optimizer, **kwargs) -> None:
         super().__init__(optimizer)
-        pass
 
     def can_be_modified(self):
         return self.check_requirements(
             ["_post_amp_backward", "zero_grad"], require_apex=True, require_torch_non_finite_check=False
         )
 
     def override_function(m_self):  # noqa: N805
```

## onnxruntime/training/ort_triton/__init__.py

```diff
@@ -5,14 +5,15 @@
 
 import threading
 from functools import wraps
 
 from onnxruntime.capi import _pybind_state as _C
 
 from .kernel import *  # noqa: F403
+from .triton_op_executor import register_triton_kernel  # noqa: F401
 from .triton_op_executor import call_triton_by_name, call_triton_by_onnx, get_config
 
 
 def run_once_register_triton_op_executor(f):
     """
     Decorator to run a function only once.
     :param f: function to be run only once during execution time despite the number of calls
```

## onnxruntime/training/ort_triton/_cache.py

```diff
@@ -5,14 +5,15 @@
 
 # from torch/_inductor/codecache.py
 import base64
 import functools
 import getpass
 import hashlib
 import os
+import sys
 import tempfile
 from types import ModuleType
 from typing import Tuple
 
 
 @functools.lru_cache(None)
 def _cache_dir():
@@ -57,14 +58,15 @@
         if key not in cls.cache:
             with open(path) as f:
                 code = compile(f.read(), path, "exec")
                 mod = ModuleType(f"{__name__}.{key}")
                 mod.__file__ = path
                 mod.key = key
                 exec(code, mod.__dict__, mod.__dict__)
+                sys.modules[mod.__name__] = mod
                 # another thread might set this first
                 cls.cache.setdefault(key, mod)
         return cls.cache[key]
 
 
 class ModuleCache:
     cache = dict()  # noqa: RUF012
```

## onnxruntime/training/ort_triton/_codegen.py

```diff
@@ -10,15 +10,14 @@
     2. ReduceKernelNode: perform a reduction computation on a tensor, e.g. reduce_sum/max/min
         one or more axes are supported
 
 """
 
 from typing import Tuple
 
-import numpy as np
 import sympy
 import torch
 from sympy.codegen.rewriting import create_expand_pow_optimization
 
 from ._common import CodeBuffer, CodegenContext, NodeVisitor
 from ._ir import (
     ComputeNode,
@@ -33,15 +32,15 @@
     ReduceForLoopStart,
     ReduceKernelNode,
     ReduceNode,
 )
 from ._lowering import lower
 from ._sorted_graph import SortedGraph
 from ._sympy_utils import parse_shape, sympy_dot
-from ._utils import may_add_brackets
+from ._utils import is_number, may_add_brackets
 
 
 class TritonCodegen(NodeVisitor):
     """
     Specialized codegen for Triton backend.
     """
 
@@ -312,22 +311,22 @@
                 op_type = "Pow2"
             elif kwargs["i1"] == 3:
                 op_type = "Pow3"
             elif kwargs["i1"] == 0.5:
                 op_type = "Sqrt"
 
         if op_type == "Cast":
-            from_dtype = node.inputs[0].dtype.type
-            to_dtype = node.outputs[0].dtype.type
-            if from_dtype == to_dtype:
+            from_dtype = node.inputs[0].dtype
+            to_dtype = node.outputs[0].dtype
+            if from_dtype == to_dtype or is_number(kwargs["i0"]):
                 op_type = "Identity"
-            elif to_dtype == np.bool_:
+            elif to_dtype == torch.bool:
                 op_type = "CastBool"
             else:
-                kwargs["dtype"] = to_dtype.__name__
+                kwargs["dtype"] = str(to_dtype)[6:]  # Remove "torch." prefix.
 
         if op_type == "QuickGelu" or op_type == "QuickGeluGrad":
             kwargs["alpha"] = str(node.attributes.get("alpha", 1.702))
 
         if op_type == "Sum":
             output_var = kwargs["o0"]
             formula = " + ".join([kwargs[f"i{idx}"] for idx in range(len(node.inputs))])
@@ -469,15 +468,15 @@
             )
 
         for idx, kernel_node in enumerate(node.kernels):
             if idx != 0:
                 code_buffer += "\n"
             # Allocate output tensor.
             for output in kernel_node.outputs:
-                torch_dtype = torch.from_numpy(np.zeros(1, dtype=output.dtype)).dtype
+                torch_dtype = output.dtype
                 # Workaround for DLPack which doesn't support bool.
                 if torch_dtype == torch.bool:
                     torch_dtype = torch.uint8
                 code_buffer += (
                     f"{space_indent}{context.get_variable_name(output.name)} = "
                     f'torch.empty({tuple(output.shape)}, dtype={torch_dtype}, device="cuda")\n'
                 )
```

## onnxruntime/training/ort_triton/_common.py

```diff
@@ -26,15 +26,15 @@
     # Get variable name by the node arg name in ONNX graph.
     def get_variable_name(self, name: str) -> str:
         return self._var_map[name]
 
     # For some operators such as data load/store, we need an internal variable name inside the kernel function.
     def get_internal_variable_name(self, name: str) -> str:
         var_name = self._var_map[name]
-        var_name = self._var_map[var_name] if var_name in self._var_map else var_name
+        var_name = self._var_map.get(var_name, var_name)
         return f'float("{var_name}")' if var_name in _SPECIAL_FLOATS else var_name
 
 
 class CodeBuffer:
     def __init__(self):
         self.buffer: List[str] = []
```

## onnxruntime/training/ort_triton/_decompose.py

```diff
@@ -6,19 +6,18 @@
 """
 Decompose a complicated op into a series of simple ops.
 "simple ops" can be executed in one pass
 """
 
 from typing import List
 
-import numpy as np
 import sympy
 from onnx import GraphProto, NodeProto, TensorProto, helper
 
-from ._utils import get_attribute, get_reduce_info, to_numpy_type
+from ._utils import get_attribute, get_reduce_info
 
 
 def _is_half_dtype(dtype: int):
     return dtype in [TensorProto.FLOAT16, TensorProto.BFLOAT16]
 
 
 class DecomposeDispatch:
@@ -128,15 +127,15 @@
                 cast_node3, _ = self._new_node(node_name, "Cast", [layer_norm_node_outputs[1]], outputs=[y], to=wdtype)
                 decomposed_nodes.append(cast_node3)
             return decomposed_nodes
         rank = len(shape)
         if axis < 0:
             axis += rank
         axes = list(range(axis, rank))
-        epsilon_tensor = helper.make_tensor(name="epsilon_const", data_type=xdtype, dims=(1,), vals=np.array([epsilon]))
+        epsilon_tensor = helper.make_tensor(name="epsilon_const", data_type=xdtype, dims=(1,), vals=[epsilon])
         const_node, const_out = self._new_node(node_name, "Constant", [], value=epsilon_tensor)
         reducemean_node, reducemean_out = self._new_node(node_name, "ReduceMean", [x], outputs=[mean], axes=axes)
         sub_node, sub_out = self._new_node(node_name, "Sub", [x, reducemean_out])
         mul_node, mul_out = self._new_node(node_name, "Mul", [sub_out, sub_out])
         reducemean_node1, reducemean_out1 = self._new_node(node_name, "ReduceMean", [mul_out], axes=axes)
         add_node, add_out = self._new_node(node_name, "Add", [reducemean_out1, const_out])
         rsqrt_node, rsqrt_out = self._new_node(node_name, "Rsqrt", [add_out], outputs=[inv_std_dev])
@@ -367,12 +366,12 @@
         # which is not supported yet.
         assert all(shape[axis].is_number for axis in axes)
         denominator = int(sympy.prod([shape[axis] for axis in axes]))
         denominator_tensor = helper.make_tensor(
             name=f"{node_name}_denominator",
             dims=(),
             data_type=dtype,
-            vals=np.array([denominator], dtype=to_numpy_type(dtype)),
+            vals=[denominator],
         )
         denominator_node, denominator_out = self._new_node(node_name, "Constant", [], value=denominator_tensor)
         div_node, _ = self._new_node(node_name, "Div", [sum_out, denominator_out], outputs=[y])
         return [sum_node, denominator_node, div_node]
```

## onnxruntime/training/ort_triton/_ir.py

```diff
@@ -3,55 +3,55 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 from abc import abstractmethod
 from collections import defaultdict
 from typing import Any, Dict, List, Optional, Set, Tuple
 
-import numpy as np
 import sympy
+import torch
 
 from ._common import AutotuneConfigs, CodeBuffer, CodegenContext, NodeVisitor, TensorInfo
 from ._sympy_utils import parse_shape
-from ._utils import gen_unique_name, gen_variable_name, sort_reduce_axes, to_numpy_type
+from ._utils import gen_unique_name, gen_variable_name, sort_reduce_axes, to_torch_dtype
 
 
 class TensorArg:
     """
     A TensorArg represents a tensor argument in the kernel function.
     It contains a name (from ONNX graph), the data type, the shape.
     If it's constant (initializer or constant node), it also contains the data in numpy array.
     """
 
-    def __init__(self, name: str, tensor_info: Optional[TensorInfo] = None, data: Optional[np.ndarray] = None):
+    def __init__(self, name: str, tensor_info: Optional[TensorInfo] = None, data: Optional[torch.Tensor] = None):
         self._name: str = name
-        self._data: Optional[np.ndarray] = data
+        self._data: Optional[torch.Tensor] = data
         if data is not None:
-            self._dtype: np.dtype = data.dtype
+            self._dtype: torch.dtype = data.dtype
             self._shape: List[sympy.Expr] = parse_shape(list(data.shape))
         else:
             assert tensor_info is not None
-            self._dtype: np.dtype = to_numpy_type(tensor_info.dtype)
+            self._dtype: torch.dtype = to_torch_dtype(tensor_info.dtype)
             self._shape: List[sympy.Expr] = tensor_info.shape
         self.cross_kernels: bool = False
 
     @property
     def name(self) -> str:
         return self._name
 
     @property
-    def dtype(self) -> np.dtype:
+    def dtype(self) -> torch.dtype:
         return self._dtype
 
     @property
     def shape(self) -> List[sympy.Expr]:
         return self._shape
 
     @property
-    def data(self) -> Optional[np.ndarray]:
+    def data(self) -> Optional[torch.Tensor]:
         return self._data
 
 
 class OffsetCalculator:
     """
     OffsetCalculator maps tensor arguments to the target shape of a kernel.
     It' used to generate the offset code for data load/store for a tensor argument in a kernel with
@@ -324,19 +324,18 @@
             self.var_map[name] = "t_" + name
         for name in self.internal_args:
             self.var_map[name] = gen_variable_name(name, "t", existing_names)
         for name, tensor_arg in self.constants.items():
             self.var_map[name] = gen_variable_name(name, "c", existing_names)
             if tensor_arg.data is not None:
                 value = tensor_arg.data
-                if value is not None:
-                    assert value.size == 1, f"unsupported constant array {value}"
-                    variable_name = self.var_map[name]
-                    assert variable_name not in self.var_map
-                    self.var_map[variable_name] = str(np.array(value.item(), value.dtype))
+                assert value.numel() == 1, f"unsupported constant {value} which has more than one element."
+                variable_name = self.var_map[name]
+                assert variable_name not in self.var_map
+                self.var_map[variable_name] = str(value.item())
 
 
 class ElementwiseKernelNode(KernelNode):
     def __init__(self, inputs: List[TensorArg], outputs: List[TensorArg], target_shape: List[sympy.Expr]):
         super().__init__(inputs, outputs, target_shape, [])
 
 
@@ -388,9 +387,12 @@
             self.var_map[pair[0].name] = name
         running_offset = sympy.Integer(0)
         self.has_dropout: bool = False
         for kernel in self.kernels:
             for ir_node in kernel.sub_nodes:
                 if isinstance(ir_node, DropoutNode):
                     ir_node.global_offset = running_offset
+                    kernel.offset_calc.symbolic_shape_variables.update(
+                        [symbol.name for symbol in running_offset.free_symbols]
+                    )
                     running_offset = running_offset + sympy.prod(ir_node.outputs[0].shape)
                     self.has_dropout = True
```

## onnxruntime/training/ort_triton/_lowering.py

```diff
@@ -24,15 +24,15 @@
     ReduceForLoopStart,
     ReduceKernelNode,
     ReduceNode,
     TensorArg,
 )
 from ._op_config import is_reduction_node
 from ._sorted_graph import SortedGraph
-from ._utils import get_reduce_info, sort_reduce_axes, to_numpy_array
+from ._utils import get_reduce_info, sort_reduce_axes, to_torch_tensor
 
 
 class NodeGroup:
     """
     A NodeGroup contains nodes that can be lowered to a single Triton kernel node.
 
     """
@@ -241,18 +241,18 @@
     def _extract_module_io(self):
         graph = self._sorted_graph.original_graph
         self._module_inputs = [TensorArg(input.name, self._node_arg_infos[input.name]) for input in graph.input]
         self._module_input_names = set(arg.name for arg in self._module_inputs)
         self._module_outputs = [TensorArg(output.name, self._node_arg_infos[output.name]) for output in graph.output]
         self._module_output_names = set(arg.name for arg in self._module_outputs)
         for initializer in graph.initializer:
-            data = to_numpy_array(initializer)
+            data = to_torch_tensor(initializer)
             self._module_constants.append(TensorArg(initializer.name, data=data))
         for const_node in self._sorted_graph.const_nodes:
-            data = to_numpy_array(const_node)
+            data = to_torch_tensor(const_node)
             self._module_constants.append(TensorArg(const_node.output[0], data=data))
         self._module_constant_names = set(arg.name for arg in self._module_constants)
         self._tensor_args = dict(
             (arg.name, arg)
             for arg in itertools.chain(self._module_inputs, self._module_outputs, self._module_constants)
         )
 
@@ -308,15 +308,15 @@
         for i in range(len(groups) - 1):
             group_inputs = set()
             for node in groups[i].dependent_nodes(True)[0].values():
                 group_inputs.update(node.input)
             for j in range(i + 1, len(groups)):
                 if any(output in group_inputs for output in groups[j].nodes_groups[0].output):
                     group_dependencies[i].add(j)
-                    for k in range(0, i):
+                    for k in range(i):
                         if i in group_dependencies[k]:
                             group_dependencies[k].add(j)
 
         flag = set()
         for i, group_i in enumerate(groups):
             if i in flag:
                 continue
@@ -411,15 +411,15 @@
                 nxt += 1
             load_cache = set()
             load_nodes = []
             store_nodes = []
             for idx in range(cur, nxt):
                 for input in sub_nodes[idx].inputs:
                     if input.name in kernel_node.constants or input.name in input_names:
-                        if (input.data is not None and input.data.size == 1) or input.name in load_cache:
+                        if (input.data is not None and input.data.numel() == 1) or input.name in load_cache:
                             continue
                         load_nodes.append(IONode(input, kernel_node.offset_calc, True))
                         load_cache.add(input.name)
                 for output in sub_nodes[idx].outputs:
                     if output.name in output_name_map:
                         output_name_map[output.name] -= 1
                         if output_name_map[output.name] == 0:
@@ -428,15 +428,15 @@
                 new_sub_nodes.append(sub_nodes[cur])
                 cur += 1
             if nxt < len(sub_nodes):
                 assert isinstance(sub_nodes[nxt], ReduceForLoopEnd)
                 for reduce_node in sub_nodes[nxt].reduce_nodes:
                     input = reduce_node.inputs[0]
                     if input.name in kernel_node.constants or input.name in input_names:
-                        if (input.data is not None and input.data.size == 1) or input.name in load_cache:
+                        if (input.data is not None and input.data.numel() == 1) or input.name in load_cache:
                             continue
                         load_nodes.append(IONode(input, kernel_node.offset_calc, True))
                         load_cache.add(input.name)
             new_sub_nodes.extend(load_nodes)
             new_sub_nodes.extend(sub_nodes[cur:nxt])
             new_sub_nodes.extend(store_nodes)
             if nxt < len(sub_nodes):
```

## onnxruntime/training/ort_triton/_sorted_graph.py

```diff
@@ -3,24 +3,23 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import copy
 import itertools
 from typing import Dict, List, Set
 
-import numpy as np
 import onnx
 import sympy
 from onnx import GraphProto, ModelProto, NodeProto, TensorProto, helper
 
 from ._common import SymbolicDSU, TensorInfo, TypeAndShapeInfer
 from ._decompose import DecomposeDispatch
 from ._op_config import is_elementwise_node
 from ._sympy_utils import parse_shape
-from ._utils import get_attribute, to_numpy_array, topological_sort
+from ._utils import get_attribute, to_torch_tensor, topological_sort
 
 
 class SortedGraph:
     """
     This class is used to
         1. decompose complex operators into preliminary operators,
         2. sort the operators in topological order,
@@ -54,15 +53,15 @@
 
         self._node_arg_infos: Dict[str, TensorInfo] = {}
         for idx, input in enumerate(self._graph.input):
             self._node_arg_infos[input.name] = TensorInfo(input.type.tensor_type.elem_type, self._input_shapes[idx])
         for initializer in self._graph.initializer:
             self._node_arg_infos[initializer.name] = TensorInfo(
                 initializer.data_type,
-                parse_shape(list(to_numpy_array(initializer).shape)),
+                parse_shape(list(initializer.dims)),
             )
 
         # Decompose complex operators.
         self._decompose()
 
         # Sort the initializers in reference order.
         # We try to reuse Triton module for different ONNX models with same graph structure,
@@ -93,21 +92,21 @@
             shape_str = str(self._input_shapes[idx]).replace(" ", "")
             graph_inputs.append(f"({input.type.tensor_type.elem_type!s},{shape_str})")
             name_map[input.name] = f"i{idx}"
         graph_inputs_str = ",".join(graph_inputs)
 
         constants = []
         for idx, initializer in enumerate(self._sorted_initializers):
-            data_str = np.array2string(to_numpy_array(initializer), separator=",").replace("\n", "").replace(" ", "")
+            data_str = str(to_torch_tensor(initializer).tolist()).replace("\n", "").replace(" ", "")
             constants.append(f"({initializer.data_type},{data_str})")
             name_map[initializer.name] = f"c{idx}"
 
         for idx, node in enumerate(self._const_nodes):
             value_attr = get_attribute(node, "value")
-            data_str = np.array2string(to_numpy_array(value_attr), separator=",").replace("\n", "").replace(" ", "")
+            data_str = str(to_torch_tensor(value_attr).tolist()).replace("\n", "").replace(" ", "")
             constants.append(f"({value_attr.data_type},{data_str})")
             name_map[node.output[0]] = f"c{idx + len(self._sorted_initializers)}"
         constants_str = ",".join(constants)
 
         for idx, output in enumerate(self._graph.output):
             name_map[output.name] = f"o{idx}"
 
@@ -177,15 +176,15 @@
                     new_nodes = topological_sort(node.input, new_nodes)
                     self._sorted_nodes[pos : pos + 1] = new_nodes
                     continue
             if node.op_type == "Constant":
                 value_attr = get_attribute(node, "value")
                 self._node_arg_infos[node.output[0]] = TensorInfo(
                     value_attr.data_type,
-                    parse_shape(list(to_numpy_array(value_attr).shape)),
+                    parse_shape(list(value_attr.dims)),
                 )
             else:
                 input_infos = []
                 for input in node.input:
                     input_infos.append(self._node_arg_infos[input])
                 output_infos = TypeAndShapeInfer.infer(node, input_infos, self._graph, symbolics)
                 for idx, output in enumerate(node.output):
```

## onnxruntime/training/ort_triton/_utils.py

```diff
@@ -5,16 +5,16 @@
 
 import re
 import uuid
 from collections import defaultdict
 from typing import Any, List, Tuple
 
 import numpy as np
+import torch
 from onnx import GraphProto, NodeProto, TensorProto, helper, numpy_helper
-from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE
 
 
 def gen_unique_name(prefix: str) -> str:
     return prefix + "_" + uuid.uuid4().hex[:8]
 
 
 def _topological_sort_internal(node, visited, output_consumers, sorted_nodes):
@@ -69,25 +69,32 @@
 def get_attribute(node: NodeProto, attr_name: str, default_value: Any = None) -> Any:
     found = [attr for attr in node.attribute if attr.name == attr_name]
     if found:
         return helper.get_attribute_value(found[0])
     return default_value
 
 
-# Convert Constant node or TensorProto to numpy array.
-def to_numpy_array(node: Any) -> np.ndarray:
+# Convert Constant node or TensorProto to torch.Tensor.
+def to_torch_tensor(node: Any) -> torch.Tensor:
     tensor = node
     if isinstance(node, NodeProto):
         tensor = get_attribute(node, "value")
     assert isinstance(tensor, TensorProto)
-    return numpy_helper.to_array(tensor)
+    torch_tensor = torch.from_numpy(numpy_helper.to_array(tensor))
+    # numpy does not support bfloat16 and create a float32 tensor instead.
+    if tensor.data_type == TensorProto.BFLOAT16:
+        torch_tensor = torch_tensor.to(torch.bfloat16)
+    return torch_tensor
 
 
-def to_numpy_type(tensor_type: TensorProto.DataType) -> np.dtype:
-    return TENSOR_TYPE_TO_NP_TYPE[tensor_type] if not isinstance(tensor_type, np.dtype) else tensor_type
+def to_torch_dtype(tensor_type: TensorProto.DataType) -> torch.dtype:
+    # Native numpy does not support bfloat16.
+    if tensor_type == TensorProto.BFLOAT16:
+        return torch.bfloat16
+    return torch.from_numpy(np.zeros(1, dtype=helper.tensor_dtype_to_np_dtype(tensor_type))).dtype
 
 
 # Generate a unique variable name based on the node arg name.
 def gen_variable_name(name: str, prefix: str, existing_names: set) -> str:
     pos = name.rfind("/")
     if pos != -1:
         name = name[pos + 1 :]
@@ -129,24 +136,33 @@
     if axes is None and len(node.input) > 1:
         axes_initializer = None
         for initializer in graph.initializer:
             if initializer.name == node.input[1]:
                 axes_initializer = initializer
                 break
         assert axes_initializer is not None
-        axes = to_numpy_array(axes_initializer).tolist()
+        axes = to_torch_tensor(axes_initializer).tolist()
     if axes is None:
         axes = list(range(input_rank)) if noop_with_empty_axes == 0 else []
     axes = sort_reduce_axes(axes, input_rank, check_contiguous=False)
     return keep_dims, axes
 
 
 def next_power_of_2(n: int) -> int:
-    assert n <= 2**32, "32-bit only"
+    """Return the smallest power of 2 greater than or equal to n"""
     n -= 1
     n |= n >> 1
     n |= n >> 2
     n |= n >> 4
     n |= n >> 8
     n |= n >> 16
+    n |= n >> 32
     n += 1
     return n
+
+
+def is_number(name: str) -> bool:
+    try:
+        float(name)
+        return True
+    except ValueError:
+        return name.startswith("float(") and name.endswith(")")
```

## onnxruntime/training/ort_triton/triton_op_executor.py

```diff
@@ -2,31 +2,35 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import functools
 import json
 import os
+import re
 import sys
 from types import ModuleType
 from typing import List, Tuple, Union
 
 import onnx
+from onnx import ModelProto
 from torch._C import _from_dlpack
 from torch.utils.dlpack import to_dlpack
 
 from ._cache import ModuleCache, PyCodeCache
 from ._codegen import codegen
 from ._op_config import get_supported_ops
 from ._sorted_graph import SortedGraph
 from ._sympy_utils import extract_shape_from_symbol, parse_shape
 from ._utils import gen_unique_name, next_power_of_2
 
 _DEBUG_MODE = "ORTMODULE_TRITON_DEBUG" in os.environ and int(os.getenv("ORTMODULE_TRITON_DEBUG")) == 1
 
+_CUSTOM_KERNELS = dict()
+
 
 @functools.lru_cache(None)
 def _gen_module_internal(sorted_graph: SortedGraph) -> Tuple[str, str, ModuleType]:
     func_name = gen_unique_name("func")
     src_code = codegen(func_name, sorted_graph)
     return func_name, src_code, PyCodeCache().load(src_code)
 
@@ -35,47 +39,67 @@
     """
     Cache the shapes of the inputs. The inputs are the concrete shapes of inputs from each step for a given ONNX model.
     For those dimensions that the concrete shape is not changed, we use the same concrete shape.
     For those dimensions that the concrete shape is changed between different steps, we use a symbolic shape.
     """
 
     cache = dict()  # noqa: RUF012
+    symbolic_shape_hint = None
+    min_symbolic_shape = 0
     clear = staticmethod(cache.clear)
 
     @classmethod
-    def get_shape(cls, onnx_key: int, shapes: List[List[int]]) -> List[List[Union[int, str]]]:
+    def set_symbolic_shape_hint(cls, symbolic_shape_hint_config):
+        for k, v in symbolic_shape_hint_config.items():
+            if k == "*":
+                cls.min_symbolic_shape = v
+            else:
+                if cls.symbolic_shape_hint is None:
+                    cls.symbolic_shape_hint = dict()
+                cls.symbolic_shape_hint[k] = v
+
+    @classmethod
+    def get_shape(cls, onnx_key: int, model: ModelProto, shapes: List[List[int]]) -> List[List[Union[int, str]]]:
         if onnx_key not in cls.cache:
+            if cls.symbolic_shape_hint is not None:
+                for i, input in enumerate(model.graph.input):
+                    if input.type.tensor_type.HasField("shape"):
+                        for j, dim in enumerate(input.type.tensor_type.shape.dim):
+                            if dim.dim_param:
+                                for k, v in cls.symbolic_shape_hint.items():
+                                    if re.fullmatch(k, dim.dim_param):
+                                        shapes[i][j] = f"i{i}_dim{j}_{v}"
+                                        break
             cls.cache[onnx_key] = shapes
         else:
             changed = False
             for i, shape in enumerate(shapes):
                 for j, dim in enumerate(shape):
-                    if dim != cls.cache[onnx_key][i][j] and isinstance(cls.cache[onnx_key][i][j], int):
-                        max_dim = max(dim, cls.cache[onnx_key][i][j])
+                    if isinstance(cls.cache[onnx_key][i][j], int) and dim != cls.cache[onnx_key][i][j]:
+                        max_dim = max(dim, cls.cache[onnx_key][i][j], cls.min_symbolic_shape)
                         shape[j] = f"i{i}_dim{j}_{next_power_of_2(max_dim)}"
                         changed = True
                     elif isinstance(cls.cache[onnx_key][i][j], str):
                         pre = extract_shape_from_symbol(cls.cache[onnx_key][i][j])
                         if pre >= dim:
                             shape[j] = cls.cache[onnx_key][i][j]
                         else:
                             shape[j] = f"i{i}_dim{j}_{next_power_of_2(dim)}"
                             changed = True
             if changed:
                 cls.cache[onnx_key] = shapes
         return cls.cache[onnx_key]
 
 
-def _gen_key(onnx_key: int, onnx_str: bytes, shapes: List[List[Union[int, str]]]) -> int:
+def _gen_key(onnx_key: int, model: ModelProto, shapes: List[List[Union[int, str]]]) -> int:
     # pylint: disable=unused-argument
-    return hash(f"{onnx_key}|{str(shapes).replace(' ', '')}") % (10**8)
+    return hash(f"{onnx_key}|{str(shapes).replace(' ', '')}")
 
 
-def _gen_module(onnx_key: int, onnx_str: bytes, shapes: List[List[Union[int, str]]]) -> Tuple[str, ModuleType]:
-    model = onnx.load_model_from_string(onnx_str)
+def _gen_module(onnx_key: int, model: ModelProto, shapes: List[List[Union[int, str]]]) -> Tuple[str, ModuleType]:
     sorted_graph = SortedGraph(model, [parse_shape(shape) for shape in shapes])
     if _DEBUG_MODE:
         os.makedirs(os.path.dirname("triton_debug/"), exist_ok=True)
         sorted_graph.save_onnx(f"triton_debug/{onnx_key}")
     func_name, src_code, mod = _gen_module_internal(sorted_graph)
     if _DEBUG_MODE:
         py_file_path = f"triton_debug/{func_name}_{onnx_key}.py"
@@ -90,34 +114,51 @@
     All supported ops are from user config specified by env ORTMODULE_TRITON_CONFIG_FILE or from _op_config.py.
     The Triton fusion will try to fuse subgraphs with connected supported ops.
     The initializer value can be "none", "scalar", and "all".
         "none": no initializer will be added to subgraphs.
         "scalar": only related scalar initializers will be added to subgraphs.
         "all": all related initializers will be added to subgraphs.
     The min_nodes is used to control the minimum number of non-no-op nodes in a subgraph.
+    User can also specify symbolic_shape_hint in the config, which is a dict to control the symbolic shape hint.
+    Each entry is a regex pattern to match the dim_param in ONNX model and the value is the power of 2 for the symbolic
+    shape. Each dim_param will be replaced by i{input_index}_dim{dim_index}_{power_of_2} in the symbolic shape.
     """
 
+    config = dict()
     config_file = os.getenv("ORTMODULE_TRITON_CONFIG_FILE", "")
     if config_file and os.path.exists(config_file):
         with open(config_file, encoding="UTF-8") as f:
-            return f.read()
+            config = json.load(f)
+
+    if "ops" not in config:
+        config["ops"] = get_supported_ops()
+    if "initializer" not in config:
+        config["initializer"] = "scalar"
+    if "min_nodes" not in config:
+        config["min_nodes"] = 2
+
+    if "symbolic_shape_hint" in config and len(config["symbolic_shape_hint"]) > 0:
+        _ShapeCache.set_symbolic_shape_hint(config["symbolic_shape_hint"])
+        del config["symbolic_shape_hint"]
 
-    config = {"ops": get_supported_ops(), "initializer": "scalar", "min_nodes": 2}
     return json.dumps(config)
 
 
 def call_triton_by_name(func_name: str, *tensors, **kwargs):
     """
     Call triton kernel by function name. It's expected that there are functions and kernels registered manually
     with that func_name (normally in .kernel sub-module), this function try to get the Python function by name
     and execute it with the given tensors.
     """
 
     torch_tensors = [_from_dlpack(tensor) if tensor is not None else None for tensor in tensors]
-    func = getattr(sys.modules[".".join(__name__.split(".")[:-1])], func_name)
+    func = getattr(sys.modules[".".join(__name__.split(".")[:-1])], func_name, None)
+    if func is None:
+        func = _CUSTOM_KERNELS.get(func_name)
+    assert func is not None, f"Function {func_name} is not found in the registered kernels."
     output = func(*torch_tensors, **kwargs)
     if output is not None:
         if isinstance(output, tuple):
             return tuple([to_dlpack(tensor) for tensor in output])
         return to_dlpack(output)
     return None
 
@@ -127,14 +168,20 @@
     Call triton kernel by ONNX model. Load the ONNX model from onnx_str, generate the Triton function and kernels,
     and execute the function with the given tensors.
     """
 
     assert all(tensor is not None for tensor in tensors)
     torch_tensors = [_from_dlpack(tensor) for tensor in tensors]
     concrete_shapes = [list(tensor.size()) for tensor in torch_tensors]
-    shapes = _ShapeCache.get_shape(onnx_key, concrete_shapes)
-    func_name, mod = ModuleCache.load(_gen_key, _gen_module, onnx_key, onnx_str, shapes)
+    model = onnx.load_model_from_string(onnx_str)
+    shapes = _ShapeCache.get_shape(onnx_key, model, concrete_shapes)
+    func_name, mod = ModuleCache.load(_gen_key, _gen_module, onnx_key, model, shapes)
     func = getattr(mod, func_name)
     output = func(*torch_tensors)
     if isinstance(output, tuple):
         return tuple([to_dlpack(tensor) for tensor in output])
     return to_dlpack(output)
+
+
+def register_triton_kernel(fn):
+    _CUSTOM_KERNELS[fn.__name__] = fn
+    return fn
```

## onnxruntime/training/ort_triton/kernel/_flash_attn.py

```diff
@@ -690,15 +690,15 @@
     if BIAS_TYPE != "none":
         Bias += off_b * stride_bb + off_h * stride_bh
     # pointer to row-wise quantities in value-like data
     D += off_hb * seqlen_q_rounded
     LSE += off_hb * seqlen_q_rounded
     if not SEQUENCE_PARALLEL:
         num_block_n = tl.cdiv(seqlen_k, BLOCK_N)
-        for start_n in range(0, num_block_n):
+        for start_n in range(num_block_n):
             _bwd_kernel_one_col_block(
                 start_n,
                 Q,
                 K,
                 V,
                 Bias,
                 DO,
```

## onnxruntime/training/ort_triton/kernel/_mm.py

```diff
@@ -7,15 +7,15 @@
 import os
 from types import ModuleType
 from typing import Tuple
 
 import torch
 
 from .._cache import ModuleCache, PyCodeCache
-from .._utils import next_power_of_2
+from .._utils import gen_unique_name, next_power_of_2
 
 _DEBUG_MODE = "ORTMODULE_TRITON_DEBUG" in os.environ and int(os.getenv("ORTMODULE_TRITON_DEBUG")) == 1
 
 
 _MM_TEMPLATE = """
 import torch
 import triton
@@ -301,26 +301,26 @@
         allow_tf32=torch.backends.cuda.matmul.allow_tf32,
         post_process=post_process,
         func_name=func_name,
     )
 
 
 def _gen_mm_key(dtype: torch.dtype, m: int, n: int, k: int, trans_a: bool, trans_b: bool, alpha: float) -> int:
-    return hash(f"mm|{dtype}|{m}|{n}|{k}|{trans_a}|{trans_b}|{alpha}") % (10**8)
+    return hash(f"mm|{dtype}|{m}|{n}|{k}|{trans_a}|{trans_b}|{alpha}")
 
 
 def _gen_mm_module(
     dtype: torch.dtype, m: int, n: int, k: int, trans_a: bool, trans_b: bool, alpha: float
 ) -> Tuple[str, ModuleType]:
-    func_name = f"mm_{_gen_mm_key(dtype, m, n, k, trans_a, trans_b, alpha)}"
+    func_name = gen_unique_name("mm")
     kwargs = _mm_configs(dtype, m, n, k, trans_a, trans_b, alpha, func_name)
     src_code = _MM_TEMPLATE.format(**kwargs)
     if _DEBUG_MODE:
         os.makedirs(os.path.dirname("triton_debug/"), exist_ok=True)
-        with open(f"triton_debug/{func_name}.py", "w") as f:
+        with open(f"triton_debug/{func_name}.py", "w", encoding="utf-8") as f:
             f.write(src_code)
     return func_name, PyCodeCache().load(src_code)
 
 
 def _gen_gemm_key(
     dtype: torch.dtype,
     m: int,
@@ -329,61 +329,61 @@
     stride_cm: int,
     stride_cn: int,
     trans_a: bool,
     trans_b: bool,
     alpha: float,
     beta: float,
 ) -> int:
-    return hash(f"gemm|{dtype}|{m}|{n}|{k}|{stride_cm}|{stride_cn}|{trans_a}|{trans_b}|{alpha}|{beta}") % (10**8)
+    return hash(f"gemm|{dtype}|{m}|{n}|{k}|{stride_cm}|{stride_cn}|{trans_a}|{trans_b}|{alpha}|{beta}")
 
 
 def _gen_gemm_module(
     dtype: torch.dtype,
     m: int,
     n: int,
     k: int,
     stride_cm: int,
     stride_cn: int,
     trans_a: bool,
     trans_b: bool,
     alpha: float,
     beta: float,
 ) -> Tuple[str, ModuleType]:
-    func_name = f"gemm_{_gen_gemm_key(dtype, m, n, k, stride_cm, stride_cn, trans_a, trans_b, alpha, beta)}"
+    func_name = gen_unique_name("gemm")
     kwargs = _mm_configs(dtype, m, n, k, trans_a, trans_b, alpha, func_name)
     kwargs["stride_cm"] = stride_cm
     kwargs["stride_cn"] = stride_cn
     kwargs["beta"] = beta
     src_code = _GEMM_TEMPLATE.format(**kwargs)
     if _DEBUG_MODE:
         os.makedirs(os.path.dirname("triton_debug/"), exist_ok=True)
-        with open(f"triton_debug/{func_name}.py", "w") as f:
+        with open(f"triton_debug/{func_name}.py", "w", encoding="utf-8") as f:
             f.write(src_code)
     return func_name, PyCodeCache().load(src_code)
 
 
 def _gen_bmm_key(
     dtype: torch.dtype, m: int, n: int, k: int, batch_a: int, batch_b: int, trans_a: bool, trans_b: bool, alpha: float
 ) -> int:
-    return hash(f"bmm|{dtype}|{m}|{n}|{k}|{batch_a}|{batch_b}|{trans_a}|{trans_b}|{alpha}") % (10**8)
+    return hash(f"bmm|{dtype}|{m}|{n}|{k}|{batch_a}|{batch_b}|{trans_a}|{trans_b}|{alpha}")
 
 
 def _gen_bmm_module(
     dtype: torch.dtype, m: int, n: int, k: int, batch_a: int, batch_b: int, trans_a: bool, trans_b: bool, alpha: float
 ) -> Tuple[str, ModuleType]:
-    func_name = f"bmm_{_gen_bmm_key(dtype, m, n, k, batch_a, batch_b, trans_a, trans_b, alpha)}"
+    func_name = gen_unique_name("bmm")
     kwargs = _mm_configs(dtype, m, n, k, trans_a, trans_b, alpha, func_name)
     batch = batch_a if batch_a >= batch_b else batch_b
     kwargs["stride_aq"] = m * k if batch_a == batch else 0
     kwargs["stride_bq"] = k * n if batch_b == batch else 0
     kwargs["batch"] = batch
     src_code = _BMM_TEMPLATE.format(**kwargs)
     if _DEBUG_MODE:
         os.makedirs(os.path.dirname("triton_debug/"), exist_ok=True)
-        with open(f"triton_debug/{func_name}.py", "w") as f:
+        with open(f"triton_debug/{func_name}.py", "w", encoding="utf-8") as f:
             f.write(src_code)
     return func_name, PyCodeCache().load(src_code)
 
 
 def _matmul_internal(a, b, out, **kwargs):
     rank_a = len(a.shape)
     rank_b = len(b.shape)
```

## onnxruntime/training/ort_triton/kernel/_slice_scel.py

```diff
@@ -9,15 +9,15 @@
 import torch
 import triton
 import triton.language as tl
 from onnx import TensorProto, helper
 
 from onnxruntime.training.ortmodule import register_graph_optimizer
 
-from .._utils import get_attribute, to_numpy_array
+from .._utils import get_attribute, to_torch_tensor
 
 
 @triton.jit
 def _triton_slice_log_softmax(log_prob, logit, d: tl.constexpr, c: tl.constexpr, RBLOCK: tl.constexpr):
     xoffset = tl.program_id(0)
     logit_xoffset = (xoffset // d * (d + 1) + xoffset % d) * c
     rbase = tl.arange(0, RBLOCK)
@@ -208,29 +208,29 @@
 def _get_constant(graph, arg):
     initializer = None
     for init in graph.initializer:
         if init.name == arg:
             initializer = init
     if initializer is None:
         return None
-    return to_numpy_array(initializer)
+    return to_torch_tensor(initializer).tolist()
 
 
 def _check_slice(graph, node, start, end, axis, step):
     _, shape = _get_arg_info(graph, node.input[0])
     if shape is None:
         return False
     rank = len(shape.dim)
     if axis < 0:
         axis += rank
     for idx, value in enumerate([start, end, axis, step]):
         constant = _get_constant(graph, node.input[idx + 1])
-        if constant is None or constant.size != 1:
+        if constant is None or len(constant) != 1:
             return False
-        constant_value = constant.item()
+        constant_value = constant[0]
         if idx == 2 and constant_value < 0:
             constant_value += rank
         if constant_value != value:
             return False
     return True
```

## onnxruntime/training/ortmodule/__init__.py

```diff
@@ -1,12 +1,14 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+import contextlib
+import inspect
 import os
 import sys
 import warnings
 
 import torch
 from packaging import version
 
@@ -17,33 +19,116 @@
 from ._fallback import ORTModuleFallbackException, ORTModuleInitException, _FallbackPolicy, wrap_exception
 from .torch_cpp_extensions import is_installed as is_torch_cpp_extensions_installed
 
 if not is_ortmodule_available():
     raise ImportError("ORTModule is not supported on this platform.")
 
 
-def _defined_from_envvar(name, default_value, warn=True):
+def _defined_from_envvar(name: str, default_value: any, warn: bool = True):
+    """Check given name exists in the environment variable and return the value using the default_value's
+    type if it exists.
+    """
     new_value = os.getenv(name, None)
     if new_value is None:
         return default_value
     try:
         new_value = type(default_value)(new_value)
     except (TypeError, ValueError) as e:
         if warn:
             warnings.warn(f"Unable to overwrite constant {name!r} due to {e!r}.")
         return default_value
     return new_value
 
 
+def _override_gradient_checkpoint(original_checkpoint):
+    """
+    Best effort to override `torch.utils.checkpoint` and `deepspeed.checkpointing.checkpoint` during ONNX export.
+
+    Despite importing `torch.utils.checkpoint` or `deepspeed.checkpointing.checkpoint` in `__init__.py`,
+    users might import it first, causing our override to not take effect. We still attempt to override
+    it to work in most cases.
+
+    We replace the checkpoint function with our implementation, without condition checks.
+    The actual check is in the overridden function, verifying if:
+    1) `checkpoint` is called during ORTModule model export,
+    2) Gradient checkpoint autograd function is disallowed (ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT),
+    3) Memory optimization level is not specified by the user (ORTMODULE_MEMORY_OPT_LEVEL).
+    If true, we reset memory optimization to layer-wise recompute.
+
+    """
+
+    # Note: The `torch.utils.checkpoint` checkpoint function signature looks like below:
+    #   `checkpoint(function, *args,
+    #               use_reentrant = None,
+    #               context_fn = noop_context_fn,
+    #               determinism_check = _DEFAULT_DETERMINISM_MODE,
+    #               debug = False,
+    #               **kwargs).`
+    # The few keyword arguments are not used in the recompute module forward function, but by the
+    # checkpoint function itself, so we need to filter them out otherwise module forward function
+    # would complain about unexpected keyword arguments.
+    all_input_parameters = inspect.signature(original_checkpoint).parameters.values()
+    outside_kwarg_params = []
+    for input_parameter in all_input_parameters:
+        if (
+            input_parameter.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
+            or input_parameter.kind == inspect.Parameter.KEYWORD_ONLY
+            or input_parameter.kind == inspect.Parameter.VAR_KEYWORD
+        ):
+            outside_kwarg_params.append(input_parameter.name)
+
+    def _checkpoint(
+        function,
+        *args,
+        **kwargs,
+    ):
+        # Conditions to activate layer-wise memory optimization automatically:
+        # 1. Checkpoint is called during ORTModule model export context.
+        # 2. Gradient checkpoint autograd function export is disallowed.
+        # 3. Memory optimization level is layer-wise recompute.
+        if (
+            ORTMODULE_ONNX_EXPORT_CONTEXT[0] is True
+            and _defined_from_envvar("ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT", 0) != 1
+            and _defined_from_envvar("ORTMODULE_MEMORY_OPT_LEVEL", 0) == 1
+        ):
+            for name in outside_kwarg_params:
+                if name in kwargs:
+                    # Pop out the keyword argument to avoid passing it to the module run function
+                    kwargs.pop(name)
+            print(
+                "Layer-wise memory optimization is enabled upon detecting "
+                "gradient checkpointing autograd function usage during model execution."
+            )
+            return function(*args, **kwargs)
+        return original_checkpoint(
+            function,
+            *args,
+            **kwargs,
+        )
+
+    return _checkpoint
+
+
+with contextlib.suppress(Exception):
+    from torch.utils.checkpoint import checkpoint as original_torch_checkpoint
+
+    torch.utils.checkpoint.checkpoint = _override_gradient_checkpoint(original_torch_checkpoint)
+
+    import deepspeed
+
+    original_deepspeed_checkpoint = deepspeed.checkpointing.checkpoint
+    deepspeed.checkpointing.checkpoint = _override_gradient_checkpoint(original_deepspeed_checkpoint)
+
+
 ################################################################################
 # All global constant goes here, before ORTModule is imported ##################
 # NOTE: To *change* values in runtime, import onnxruntime.training.ortmodule and
 # assign them new values. Importing them directly do not propagate changes.
 ################################################################################
-ONNX_OPSET_VERSION = 15
+ONNX_OPSET_VERSION = 17
 MINIMUM_RUNTIME_PYTORCH_VERSION_STR = "1.8.1"
 ORTMODULE_TORCH_CPP_DIR = os.path.join(os.path.dirname(__file__), "torch_cpp_extensions")
 _FALLBACK_INIT_EXCEPTION = None
 ORTMODULE_FALLBACK_POLICY = (
     _FallbackPolicy.FALLBACK_UNSUPPORTED_DEVICE
     | _FallbackPolicy.FALLBACK_UNSUPPORTED_DATA
     | _FallbackPolicy.FALLBACK_UNSUPPORTED_TORCH_MODEL
@@ -51,29 +136,48 @@
 )
 ORTMODULE_FALLBACK_RETRY = False
 ORTMODULE_IS_DETERMINISTIC = torch.are_deterministic_algorithms_enabled()
 
 ONNXRUNTIME_CUDA_VERSION = ort_info.cuda_version if hasattr(ort_info, "cuda_version") else None
 ONNXRUNTIME_ROCM_VERSION = ort_info.rocm_version if hasattr(ort_info, "rocm_version") else None
 
-# Verify minimum PyTorch version is installed before proceding to ONNX Runtime initialization
+# The first value indicates whether the code is in ONNX export context.
+# The export context here include the full export process, including prepare export input/output information,
+# and export model.
+ORTMODULE_ONNX_EXPORT_CONTEXT = [False]
+
+
+@contextlib.contextmanager
+def export_context():
+    """Context manager for model export."""
+    try:
+        ORTMODULE_ONNX_EXPORT_CONTEXT[0] = True
+
+        yield
+    finally:
+        ORTMODULE_ONNX_EXPORT_CONTEXT[0] = False
+
+
+# Verify minimum PyTorch version is installed before proceeding to ONNX Runtime initialization
 try:
     import torch
 
     runtime_pytorch_version = version.parse(torch.__version__.split("+")[0])
     minimum_runtime_pytorch_version = version.parse(MINIMUM_RUNTIME_PYTORCH_VERSION_STR)
     if runtime_pytorch_version < minimum_runtime_pytorch_version:
         raise wrap_exception(
             ORTModuleInitException,
             RuntimeError(
                 "ONNX Runtime ORTModule frontend requires PyTorch version greater"
                 f" or equal to {MINIMUM_RUNTIME_PYTORCH_VERSION_STR},"
                 f" but version {torch.__version__} was found instead."
             ),
         )
+
+
 except ORTModuleFallbackException as e:
     # Initialization fallback is handled at ORTModule.__init__
     _FALLBACK_INIT_EXCEPTION = e
 except ImportError as e:
     raise RuntimeError(
         f"PyTorch {MINIMUM_RUNTIME_PYTORCH_VERSION_STR} must be "
         "installed in order to run ONNX Runtime ORTModule frontend!"
```

## onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py

```diff
@@ -21,14 +21,15 @@
     register_torch_autograd_function,
 )
 from onnxruntime.training import ortmodule
 from onnxruntime.training.utils import pytorch_scalar_type_to_pytorch_dtype, pytorch_type_to_onnx_dtype
 
 from ._custom_op_symbolic_registry import wrap_custom_export_function
 from ._fallback import ORTModuleONNXModelException, wrap_exception
+from ._logger import LogColor
 from ._utils import get_fully_qualified_class_name, get_runtime_pytorch_version
 
 
 class _SpecialCustomFunctionHandler:
     """A class to handle high priority export of torch.autograd.Function.
     `register_high_priority_handler` can be used as function decorator to register a handler for a torch.autograd.Function.
     """
@@ -108,43 +109,14 @@
     if hasattr(kclass, "infer_shape"):
         register_shape_inference_function(kclass_name, kclass.infer_shape)
 
     if hasattr(kclass, "alias_input"):
         register_input_alias_function(kclass_name, kclass.alias_input)
 
 
-"""
-Defines a list of names of torch.autograd.Function, for checkpoint activation purposes.
-
-Note:
-    If CheckpointFunction is exported as PythonOp, the checkpoint-ed computation
-    (applied on every N transformer layer) may be computed by PyTorch, not ORT.
-    This situation should be especially noted for large language models such as GPT-2.
-
-As alternatives to using checkpoint activation:
-1. Users could leverage HierarchalORTModule to wrap the model, which only wrap exportable
-sub-nn.Module's as ORTModule. In this way, ideally, ORT could cover most of the model computation,
-other than dispatching them to PyTorch.
-2. Users could disable the check by setting the environment variable ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT=1.
-This may imply that the exported model is not fully running on ORT, users should be aware of the potential
-performance impact.
-3. Users could also leverage ORT's memory optimization feature to achieve a similar effect as checkpointing
-activations. Turn off PyTorch's checkpointing activation, then refer to env var ORTMODULE_MEMORY_OPT_CONFIG
-to enable ORT's recomputation feature.
-
-"""
-_UNSUPPORTED_CKPT_FUNC_NAMES = frozenset(
-    [
-        # Full qualified name.
-        "torch.utils.checkpoint.CheckpointFunction",
-        "deepspeed.checkpointing.CheckpointFunction",
-    ]
-)
-
-
 def _get_training_mode() -> bool:
     # TODO move to public API once the exporter team exposes that
     training_mode = None
     if get_runtime_pytorch_version() >= version.parse("1.12"):
         # FIXME: using private modules
         from torch.onnx import _globals
 
@@ -188,23 +160,14 @@
         if hi_pri_handler:
             try_export = hi_pri_handler(g, n, *args, **kwargs)
             if try_export is not None:
                 return try_export
 
         # Fall back to common exporter if not handled by high priority exporter.
 
-        # Check if the checkpointing activation is allowed.
-        is_ckpt_activation_allowed = ortmodule._defined_from_envvar("ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT", 0) == 1
-        if is_ckpt_activation_allowed is False and func_full_qual_name in _UNSUPPORTED_CKPT_FUNC_NAMES:
-            raise Exception(
-                f"The torch.autograd.Function {func_full_qual_name} should not be exported to ONNX. "
-                "Please replace ORTModule with HierarchalORTModule to only"
-                "wrap exportable sub-nn.Module's as ORTModule."
-            )
-
         cconv = n.cconv()
 
         input_tensor_types = []
         input_tensor_ranks = []
 
         input_bool_scalars = []
         input_bool_scalar_positions = []
@@ -372,34 +335,96 @@
 
 
 _export = wrap_custom_export_function(_export_pt_1_10)
 
 
 def post_process_enabling_autograd_function(exported_model: ModelProto) -> ModelProto:
     # Loop all PythonOp, append "_ctx" as the first output.
-    index = 0
-    for node in exported_model.graph.node:
+    for index, node in enumerate(exported_model.graph.node):
         op_name_prefix = node.op_type
         if node.domain == "com.microsoft" and node.op_type == "PythonOp":
             output_names = list(node.output)
             del node.output[:]
             node.output.append(output_names[0] + "_ctx")
             node.output.extend(output_names)
             for attr in node.attribute:
                 if attr.name == "func_name":
                     kclass_name = attr.s.decode("utf-8") if isinstance(attr.s, bytes) else attr.s
                     op_name_prefix = kclass_name
                     break
 
             node.name = f"{op_name_prefix}_id_{index}"
-        index += 1
 
     return exported_model
 
 
+@register_high_priority_handler("torch.utils.checkpoint.CheckpointFunction")
+@register_high_priority_handler("deepspeed.checkpointing.CheckpointFunction")
+def _gradient_checkpointing_export(g, n, *args, **kwargs):
+    """
+    Register specialized exporter for torch.autograd.Function(s) used for checkpoint activation purposes.
+
+    Note:
+        If CheckpointFunction is exported as PythonOp, the checkpoint-ed computation
+        (applied on every N transformer layer) may be computed by PyTorch, not ORT.
+        This situation should be especially noted for large language models such as GPT-2.
+
+    As alternatives to using checkpoint activation:
+    1. Users could leverage HierarchalORTModule to wrap the model, which only wrap exportable
+    sub-nn.Module's as ORTModule. In this way, ideally, ORT could cover most of the model computation,
+    other than dispatching them to PyTorch.
+    2. Users could disable the check by setting the environment variable ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT=1.
+    This may imply that the exported model is not fully running on ORT, users should be aware of the potential
+    performance impact.
+    3. Users could also leverage ORT's memory optimization feature to achieve a similar effect as checkpointing
+    activations. Turn off PyTorch's checkpointing activation, then refer to env var ORTMODULE_MEMORY_OPT_LEVEL
+    to enable ORT's recomputation feature.
+
+    """
+    # Check if the checkpointing activation is allowed.
+    is_ckpt_activation_allowed = ortmodule._defined_from_envvar("ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT", 0) == 1
+    if is_ckpt_activation_allowed is False:
+        is_layerwise_recompute_enabled = ortmodule._defined_from_envvar("ORTMODULE_MEMORY_OPT_LEVEL", 0) == 1
+        if not is_layerwise_recompute_enabled:
+            raise Exception(
+                f"{LogColor.RED}"
+                "Model uses gradient checkpointing (via {func_full_qual_name}), "
+                "which is not supported for export. \n"
+                "Consider these alternatives:\n"
+                "1) Enable ORTModule's gradient checkpointing for similar or better "
+                "memory efficiency with `export ORTMODULE_MEMORY_OPT_LEVEL=1`.\n"
+                "2) Allow gradient checkpointing export by setting the environment "
+                "variable `ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT=1`, though subsequent "
+                "execution may fail."
+                "3) Replace ORTModule with HierarchalORTModule to wrap exportable "
+                "sub-nn.Module's as ORTModule.\n"
+                f"{LogColor.ENDC}"
+            )
+
+        # Hitting this branch means the user has enabled layerwise recompute, but _override_gradient_checkpoint didn't
+        # catch the checkpointing function. This is usually because model code is importing torch.utils.checkpoint
+        # earlier than ORTModule. We should tolerantly allow this case to happen.
+        raise Exception(
+            f"{LogColor.RED}"
+            "Model uses gradient checkpointing (via {func_full_qual_name}), which is not "
+            "supported for export. \n"
+            "Consider these alternatives:\n"
+            "1) `ORTMODULE_MEMORY_OPT_LEVEL=1` is set but checkpoint functions in the model "
+            "are not overridden during onnxruntime.training.ortmodule import, consider importing "
+            "onnxruntime.training.ortmodule earlier before any model code loaded.\n"
+            "2) To allow gradient checkpointing export, set `ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT=1`. "
+            "Subsequent execution may fail.\n"
+            "3) Replace ORTModule with HierarchalORTModule to wrap exportable sub-nn.Module's as "
+            "ORTModule.\n"
+            f"{LogColor.ENDC}"
+        )
+    else:
+        return None  # Let the common exporter handle the checkpointing function
+
+
 @register_high_priority_handler("bitsandbytes.autograd._functions.MatMul4Bit")
 def _matmul4bit_export(g, n, *args, **kwargs):
     cconv = n.cconv()
     can_converted = (
         len(cconv) >= 5
         and cconv[0] == "d"
         and cconv[1] == "d"
```

## onnxruntime/training/ortmodule/_custom_gradient_registry.py

```diff
@@ -44,15 +44,15 @@
         attributes = []
         if len(node) >= 4:
             for key, value in node[3].items():
                 attr_def = C.GradientNodeAttributeDefinition()
                 attr_def.name = key
                 attr_def.value_json = json.dumps(value["value"])
                 attr_def.dtype = value["dtype"]
-                attr_def.is_tensor = value["is_tensor"] if "is_tensor" in value else False
+                attr_def.is_tensor = value.get("is_tensor", False)
                 attributes.append(attr_def)
         node_def.attributes = attributes
         node_defs.append(node_def)
     return node_defs
 
 
 class CustomGradientRegistry:
```

## onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py

```diff
@@ -6,15 +6,15 @@
 from typing import Callable
 
 import torch
 import torch.onnx.symbolic_helper as sym_help
 from packaging import version
 from packaging.version import Version
 from torch.onnx import register_custom_op_symbolic
-from torch.onnx.symbolic_helper import _get_tensor_dim_size, _get_tensor_sizes, parse_args
+from torch.onnx.symbolic_helper import parse_args
 
 from onnxruntime.training.utils import pytorch_type_to_onnx_dtype
 
 from ._utils import get_runtime_pytorch_version
 
 
 def wrap_custom_export_function(original_func: Callable) -> Callable:
@@ -172,17 +172,17 @@
     output = g.op(
         "org.pytorch.aten::ATen", weight, indices, padding_idx, scale_grad_by_freq, sparse, operator_s="embedding"
     )
 
     try:
         # Tolerant to the case when sizes of indices are not available or not usable (for example
         # when DeepSpeed stage3 enabled, all weights size is (0), this will fail.)
-        indices_shape = _get_tensor_sizes(indices)
+        indices_shape = sym_help._get_tensor_sizes(indices)
         if indices_shape is not None and hasattr(weight.type(), "with_sizes"):
-            output_type = weight.type().with_sizes([*indices_shape, _get_tensor_dim_size(weight, 1)])
+            output_type = weight.type().with_sizes([*indices_shape, sym_help._get_tensor_dim_size(weight, 1)])
             output.setType(output_type)
     except IndexError:
         output.setType(weight.type())
     return output
 
 
 @register_symbolic("bitwise_or")
@@ -817,7 +817,155 @@
         input,
         output_size,
         align_corners,
         scale_factors,
         operator_s="upsample_bicubic2d",
         overload_name_s="vec",
     )
+
+
+@register_symbolic("layer_norm")
+@parse_args("v", "is", "v", "v", "f", "none")
+def layer_norm(g, input, normalized_shape, weight, bias, eps, cudnn_enable):
+    # normalized_shape: input shape from an expected input of size
+    # axis: The first normalization dimension.
+    # layer_norm normalizes on the last D dimensions,
+    # where D is the size of normalized_shape
+    axis = -len(normalized_shape)
+
+    res, new_running_mean, new_running_var = g.op(
+        "LayerNormalization",
+        input,
+        weight,
+        bias,
+        epsilon_f=eps,
+        axis_i=axis,
+        outputs=3,  # force all 3 outputs to be exported in training mode
+        operator_s="layer_norm",
+        overload_name_s="vec",
+    )
+
+    return res
+
+
+# Adapted from torch.onnx.symbolic_opset9._convolution -
+# https://github.com/pytorch/pytorch/blob/cf06189a2d2785ac493bcd0d55e520af5a0e3b97/torch/onnx/symbolic_opset9.py#L2334
+# We override aten::_convolution here to support bf16 for phimm model from GenAI team.
+# For bf16 inputs, we will convert input to float32, do convolution then convert output back to bf16.
+# TODO: This might have negative impact on performance.
+@register_symbolic("_convolution")
+@parse_args("v", "v", "v", "is", "is", "is", "i", "is", "i", "i", "i", "i", "i")
+def convolution(
+    g,
+    input,
+    weight,
+    bias,
+    stride,
+    padding,
+    dilation,
+    transposed,
+    output_padding,
+    groups,
+    benchmark,
+    deterministic,
+    cudnn_enabled,
+    allow_tf32=None,
+):
+    from torch.onnx.symbolic_opset9 import _convolution
+
+    input_casted = (
+        g.op("Cast", input, to_i=torch.onnx.TensorProtoDataType.FLOAT)
+        if input.type().scalarType() == "BFloat16"
+        else input
+    )
+    weight_casted = (
+        g.op("Cast", weight, to_i=torch.onnx.TensorProtoDataType.FLOAT)
+        if weight.type().scalarType() == "BFloat16"
+        else weight
+    )
+
+    n = _convolution(
+        g,
+        input_casted,
+        weight_casted,
+        bias,
+        stride,
+        padding,
+        dilation,
+        transposed,
+        output_padding,
+        groups,
+        benchmark,
+        deterministic,
+        cudnn_enabled,
+        allow_tf32,
+    )
+
+    n_casted = (
+        g.op("Cast", n, to_i=torch.onnx.TensorProtoDataType.BFLOAT16) if input.type().scalarType() == "BFloat16" else n
+    )
+    return n_casted
+
+
+# Adapted from torch.onnx.symbolic_opset9._convolution_mode -
+# https://github.com/pytorch/pytorch/blob/cf06189a2d2785ac493bcd0d55e520af5a0e3b97/torch/onnx/symbolic_opset9.py#L2406
+# We override aten::_convolution_mode here to support bf16 for phimm model from GenAI team.
+# For bf16 inputs, we will convert input to float32, do convolution then convert output back to bf16.
+# TODO: This might have negative impact on performance.
+@register_symbolic("_convolution_mode")
+@parse_args("v", "v", "v", "is", "s", "is", "i")
+def convolution_mode(
+    g,
+    input,
+    weight,
+    bias,
+    stride,
+    padding,
+    dilation,
+    groups,
+):
+    from torch.onnx.symbolic_opset9 import _convolution_mode
+
+    input_casted = (
+        g.op("Cast", input, to_i=torch.onnx.TensorProtoDataType.FLOAT)
+        if input.type().scalarType() == "BFloat16"
+        else input
+    )
+    weight_casted = (
+        g.op("Cast", weight, to_i=torch.onnx.TensorProtoDataType.FLOAT)
+        if weight.type().scalarType() == "BFloat16"
+        else weight
+    )
+
+    n = _convolution_mode(g, input_casted, weight_casted, bias, stride, padding, dilation, groups)
+
+    n_casted = (
+        g.op("Cast", n, to_i=torch.onnx.TensorProtoDataType.BFLOAT16) if input.type().scalarType() == "BFloat16" else n
+    )
+    return n_casted
+
+
+# Adapted from torch.onnx.symbolic_opset13.softmax -
+# https://github.com/pytorch/pytorch/blob/cf06189a2d2785ac493bcd0d55e520af5a0e3b97/torch/onnx/symbolic_opset13.py#L27
+# We don't need overloads symbolic_opset9 because training support opsets >= 13.
+#
+# Why we need to define softmax export logic here?
+# For the usage `nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32)` in the model,
+# https://github.com/huggingface/transformers/blob/76a33a10923ccc1074917f6b6a1e719e626b7dc9/src/transformers/models/mistral/modeling_mistral.py#L302
+# If dtype is specified, the input tensor is casted to dtype before the operation is performed.
+# This is useful for preventing data type overflows. While existing ONNX exporter do the cast after the operation.
+# This override can be a workaround before PyTorch fix the issues in coming releases.
+# (TODO: pengwa - add PyTorch versions when the issue is fixed).
+@register_symbolic("softmax")
+@parse_args("v", "i", "none")
+def softmax(g, input, dim, dtype=None):
+    from torch.onnx import _type_utils
+
+    casted_input = input
+    need_cast_for_compute = dtype and dtype.node().kind() != "prim::Constant"
+    if need_cast_for_compute:
+        parsed_dtype = sym_help._get_const(dtype, "i", "dtype")
+        casted_input = g.op("Cast", input, to_i=_type_utils.JitScalarType(parsed_dtype).onnx_type())
+
+    softmax = g.op("Softmax", casted_input, axis_i=dim)
+
+    return softmax
```

## onnxruntime/training/ortmodule/_fallback_exceptions.py

```diff
@@ -6,67 +6,55 @@
 class ORTModuleFallbackException(Exception):  # noqa: N818
     """Base exception class for fallback
 
     Although it must be specialized for specific scenarios,
     it can also be used for generic exception that require fallback
     """
 
-    pass
-
 
 class ORTModuleInitException(ORTModuleFallbackException):
     """Trigger fallback for ORTModule initialization related exceptions
 
     This exception is triggered when an incompatible or missing requirements for ORTModule are detected,
     including PyTorch version, missing ORTModule's PyTorch C++ extension binaries, etc.
     """
 
-    pass
-
 
 class ORTModuleDeviceException(ORTModuleFallbackException):
     """Trigger fallback for device related exceptions
 
     NOTE: This exception is raised during device validation within ORTModule frontend.
     Some device related exceptions can only be detected during PyTorch ONNX exporter execution.
     This exception does not capture these scenarios.
     """
 
-    pass
-
 
 class ORTModuleIOError(ORTModuleFallbackException):
     """Trigger fallback for I/O related exceptions
 
     NOTE: This exception is raised during I/O validation within ORTModule Frontend.
     Some I/O related exceptions can only be detected during PyTorch ONNX exporter execution.
     This exception does not capture these scenarios.
     """
 
-    pass
-
 
 class ORTModuleTorchModelException(ORTModuleFallbackException):
     """Trigger fallback for PyTorch modules related exceptions
 
     This exception is raised during model validation within ORTModule frontend and is based on
     checking type(model) over a hardcoded list of incompatible models.
     """
 
-    pass
-
 
 class ORTModuleONNXModelException(ORTModuleFallbackException):
     """Trigger fallback for ONNX model related exceptions
 
     This exception is raised during model conversion to ONNX and post-processing validation within ORTModule frontend.
     """
 
-    pass
-
 
 def wrap_exception(
     new_exception: ORTModuleFallbackException, raised_exception: Exception
 ) -> ORTModuleFallbackException:
     """Wraps `raised_exception` exception as cause for the returned `new_exception` exception"""
 
     exception = None
```

## onnxruntime/training/ortmodule/_graph_execution_manager.py

```diff
@@ -16,29 +16,29 @@
 import torch
 from torch.utils.cpp_extension import ROCM_HOME
 
 import onnxruntime
 from onnxruntime.capi import _pybind_state as C
 from onnxruntime.tools.symbolic_shape_infer import SymbolicShapeInference
 from onnxruntime.training.utils import ORTModelInputOutputSchemaType, PTable, onnx_dtype_to_pytorch_dtype
-from onnxruntime.training.utils.hooks import configure_ort_compatible_zero_stage3
 
-from . import _are_deterministic_algorithms_enabled, _io, _logger, _onnx_models, _utils
+from . import _are_deterministic_algorithms_enabled, _io, _logger, _onnx_models, _utils, export_context
 from ._fallback import (
     ORTModuleDeviceException,
     ORTModuleONNXModelException,
     ORTModuleTorchModelException,
     _FallbackManager,
     _FallbackPolicy,
     wrap_exception,
 )
 from ._gradient_accumulation_manager import GradientAccumulationManager
 from ._graph_execution_interface import GraphExecutionInterface
 from ._io import _FlattenedModule, _InputInfo
-from ._runtime_inspector import RuntimeInspector
+from ._logger import LogColor
+from ._runtime_inspector import FlagPaddingElimination, RuntimeInspector
 from ._utils import check_function_has_param, get_rank
 from .options import DebugOptions, LogLevel, _MemoryOptimizationLevel, _RuntimeOptions
 from .torch_cpp_extensions.cpu.aten_op_executor import load_aten_op_executor_cpp_extension
 
 
 class _RunStateInfo:
     def __init__(self, state, output_info: List[Tuple[torch.Size, torch.device, torch.dtype]]):
@@ -51,18 +51,28 @@
 
 
 class GraphExecutionManager(GraphExecutionInterface):
     def __init__(
         self,
         module: _FlattenedModule,
         debug_options: DebugOptions,
+        export_mode: int,
         fallback_manager: _FallbackManager,
         logger: logging.Logger,
     ):
-        """Manages construction and execution of ONNX graphs"""
+        """Manages construction and execution of ONNX graphs.
+
+        Args:
+            module: The flatten PyTorch module to be executed.
+            debug_options: Debug options for ORTModule.
+            export_mode: export mode, should be torch.onnx.TrainingMode.TRAINING or torch.onnx.TrainingMode.EVAL.
+            fallback_manager: Fallback manager to handle exceptions.
+            logger: Logger for ORTModule.
+
+        """
 
         super().__init__(module._original_module)
 
         # IMPORTANT: Debug and Fallback must the configured first
         self._debug_options = debug_options
         self._fallback_manager = fallback_manager
 
@@ -85,24 +95,20 @@
         self._graph_initializers: List[torch.nn.parameter.Parameter] = []
 
         # TrainingAgent or InferenceAgent
         self._execution_agent = None
 
         self._first_skip_check_warning = True
 
-        # Inspector for runtime information, for example input data, memory usage, etc.
-        self._runtime_inspector = RuntimeInspector(self._logger, self._original_module)
-        self._runtime_inspector.memory_ob.enable_memory_stats_by_step(self._runtime_options.print_memory_stat_by_step)
-
         # Tracker for ORTModule model export, session creation overhead.
         self.time_tracker = _logger.TimeTracker()
 
         # Value can be either torch.onnx.TrainingMode.TRAINING or torch.onnx.TrainingMode.EVAL
         # To be instantiated in the concrete implementation of GraphExecutionManager
-        self._export_mode = None
+        self._export_mode = export_mode
 
         # Exporter can take extra arguments for ORTModule extensions
         # It cannot overlap with required/immutable arguments (validated in runtime)
         self._export_extra_kwargs = {}
 
         # Input and output infos (including schema) for exported model.
         self._input_info: Optional[_InputInfo] = None
@@ -126,27 +132,36 @@
         # WIP feature to enable caching in Gradient accumulation scenario.
         self._gradient_accumulation_manager = GradientAccumulationManager()
 
         # Flag to re-export the model due to attribute change on the original module.
         # Re-export will be avoided if _skip_check is enabled.
         self._original_model_has_changed = False
 
+        # Inspector for runtime information, for example input data, memory usage, etc.
+        self._runtime_inspector = RuntimeInspector(
+            self._logger, self._original_module, self._export_mode == torch.onnx.TrainingMode.TRAINING
+        )
+        self._runtime_inspector.memory_ob.enable_memory_stats_by_step(self._runtime_options.print_memory_stat_by_step)
+
         # Load ATen operator executor extension.
         load_aten_op_executor_cpp_extension()
 
         # Assign self._torch_alloc and self._torch_free if self._use_external_gpu_allocator is True
         self._get_torch_gpu_allocator_function_addresses()
 
         if self._runtime_options.enable_triton:
             from onnxruntime.training.ort_triton import register_triton_op_executor
 
             register_triton_op_executor()
 
         self._zero_stage3_param_map = {}
         if self._runtime_options.enable_zero_stage3_support:
+            # Move import to here to avoid circular dependency error
+            from onnxruntime.training.utils.hooks import configure_ort_compatible_zero_stage3  # type: ignore[import]
+
             # Cannot toggle feature enabling/disabling after the first time enabled.
 
             configure_ort_compatible_zero_stage3(debug=False, stats_output_dir="ort_output", stats_overwrite=True)
 
         # Will be reset everytime we re-initialize the graph builder.
         # Be noted, we will never enable this feature for inference mode.
         self._mem_efficient_grad_management_is_enabled = False
@@ -182,15 +197,14 @@
     @abstractmethod
     def forward(self):
         """Executes the forward method for ORTModule
 
         This is an abstract method and must be overridden by a concrete implementation.
         This is the only method that the user should call on a concrete instance of the ExecutionManager
         All other methods are internal"""
-        pass
 
     def _build_graph(self, config):
         if self._runtime_options.use_static_shape:
             self._graph_builder.build(config, self._input_info.shape)
         else:
             self._graph_builder.build(config)
 
@@ -237,22 +251,25 @@
             providers = list(provider_info.keys())
             provider_options = [provider_info[providers[0]]]
 
         session_options = onnxruntime.SessionOptions()
         session_options.enable_mem_pattern = False
         session_options.enable_mem_reuse = False
         session_options.use_deterministic_compute = _are_deterministic_algorithms_enabled()
-        # DEFAULT order is reversed DFS order, while PRIORITY_BASED order is forward BFS order.
-        # DEFAULT order is likely to be better than PRIORITY_BASED order on memory. However, our recompute feature
-        # requires PRIORITY_BASED order to work properly. So we use PRIORITY_BASED order when recompute is enabled.
+        # Enable  memory efficient execution order for training if 1). memory efficient grad management is enabled
+        # or 2). memory optimizer is enabled.
+        use_memory_efficient_topo_sort = (self._export_mode == torch.onnx.TrainingMode.TRAINING) and (
+            self._mem_efficient_grad_management_is_enabled or self._runtime_options.memory_optimizer_is_enabled()
+        )
         session_options.execution_order = (
-            onnxruntime.ExecutionOrder.PRIORITY_BASED
-            if self._runtime_options.memory_optimizer_is_enabled()
+            onnxruntime.ExecutionOrder.MEMORY_EFFICIENT
+            if use_memory_efficient_topo_sort
             else onnxruntime.ExecutionOrder.DEFAULT
         )
+
         # 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. Default is 2.
         session_options.log_severity_level = int(self._debug_options.logging.log_level)
 
         # Disable weight prepacking
         session_options.add_session_config_entry("session.disable_prepacking", "1")
 
         if self._debug_options.save_onnx_models.save:
@@ -288,19 +305,24 @@
             self._onnx_models.exported_model
             and schema == self._input_info.schema
             and not self._original_model_has_changed
         ):
             # All required models have already been exported previously
             return False
         self._set_device_from_module(inputs, kwargs)
+        # TODO: move it into runtime_inspector
+        embedding_hook_handles = self._add_check_embedding_sparsity_hook()
 
         from onnxruntime.training.utils.hooks._subscriber_manager import no_increase_global_step
 
-        with no_increase_global_step():
+        with export_context(), no_increase_global_step():
             self._onnx_models.exported_model = self._get_exported_model(schema, *inputs, **kwargs)
+
+        for hook in embedding_hook_handles:
+            hook.remove()
         if self._debug_options.save_onnx_models.save:
             self._onnx_models.save_exported_model(
                 self._debug_options.save_onnx_models.path,
                 self._debug_options.save_onnx_models.name_prefix,
                 self._export_mode,
             )
 
@@ -406,17 +428,17 @@
                     "keep_initializers_as_inputs": True,
                 }
 
                 if check_function_has_param(torch.onnx.export, "autograd_inlining"):
                     # From some PyTorch version, autograd_inlining is a valid argument.
                     # We allow it to be True if custom autograd function is disabled (where autograd.Function
                     # anyway is not supported in ONNX until it can be inlined).
-                    required_export_kwargs[
-                        "autograd_inlining"
-                    ] = not self._runtime_options.enable_custom_autograd_function
+                    required_export_kwargs["autograd_inlining"] = (
+                        not self._runtime_options.enable_custom_autograd_function
+                    )
 
                 invalid_args = self._export_extra_kwargs.keys() & required_export_kwargs.keys()
 
                 if len(invalid_args) != 0:
                     error_msg = f"The following PyTorch exporter arguments cannot be specified: '{invalid_args}'."
                     raise RuntimeError(error_msg)
 
@@ -424,20 +446,57 @@
                     self._flattened_module,
                     sample_inputs_as_tuple,
                     f,
                     **required_export_kwargs,
                     **self._export_extra_kwargs,
                 )
         except Exception as e:
+            message = _utils.get_exception_as_string(e)
+
+            # Special handling when Huggingface transformers gradient checkpoint usage pattern found.
+            # For new versions of PyTorch 2, tracing torch.utils.checkpoint.checkpoint will be failed like this:
+            #   File "microsoft/phi-2/b10c3eba545ad279e7208ee3a5d644566f001670/modeling_phi.py", line 919, in forward
+            #     layer_outputs = self._gradient_checkpointing_func(
+            #   File "/site-packages/torch/_compile.py", line 24, in inner
+            #     return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
+            #   File "/site-packages/torch/_dynamo/eval_frame.py", line 470, in _fn
+            #     raise RuntimeError(
+            #   RuntimeError: Detected that you are using FX to torch.jit.trace a dynamo-optimized function. This is not supported at the moment.
+            if (
+                "_gradient_checkpointing_func" in message
+                and "Detected that you are using FX to torch.jit.trace a dynamo-optimized function" in message
+            ):
+                is_ckpt_activation_allowed = int(os.getenv("ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT", "0")) == 1
+                notes = (
+                    " Your model is running with gradient checkpointing, yet the PyTorch exporter\n"
+                    " failed during tracing the graph. Try to enable ORTModule's\n"
+                    " gradient checkpointing (a.k.a. Transformer layerwise subgraph recompute)\n"
+                    " using `export ORTMODULE_MEMORY_OPT_LEVEL=1` for similar or even better memory efficiency.\n"
+                )
+                if is_ckpt_activation_allowed:
+                    # If the user allows the gradient checkpointing export, we should inform the user to disable it,
+                    # to make layerwise recompute work.
+                    notes += (
+                        " We also notice your setting `export ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT=1`,\n"
+                        " which enables gradient checkpointing torch.autograd.Functions(s) to export.\n"
+                        " To enable ORTModule's layerwise recompute, it needs to be turned OFF by\n"
+                        " `export ORTMODULE_ALLOW_AUTOGRAD_CHECKPOINT=0`.\n"
+                    )
+
+                self._logger.error(
+                    f"{LogColor.RED}\n"
+                    "******************************** IMPORTANT NOTE *******************************\n"
+                    f"{notes}"
+                    "*******************************************************************************\n"
+                    f"{LogColor.ENDC}\n"
+                )
+
             raise wrap_exception(  # noqa: B904
                 ORTModuleONNXModelException,
-                RuntimeError(
-                    f"There was an error while exporting the PyTorch model to ONNX: "
-                    f"\n\n{_utils.get_exception_as_string(e)}"
-                ),
+                RuntimeError(f"There was an error while exporting the PyTorch model to ONNX: \n\n{message}"),
             )
         exported_model = onnx.load_model_from_string(f.getvalue())
 
         if self._runtime_options.enable_custom_autograd_function:
             from ._custom_autograd_function_exporter import post_process_enabling_autograd_function
 
             exported_model = post_process_enabling_autograd_function(exported_model)
@@ -616,14 +675,67 @@
         return state
 
     def __setstate__(self, state):
         self.__dict__.update(state)
 
         _utils.reinitialize_graph_execution_manager(self)
 
+    def _add_check_embedding_sparsity_hook(self):
+        """
+        Add hook to check embedding sparsity and enable padding elimination if applicable.
+        1. Iterate through all modules to find Embedding modules with padding_idx >= 0.
+        2. Register forward hook to the Embedding module and the hook will check sparsity of the embedding input.
+        3. If the sparsity is below a threshold, enable padding elimination by adding FlagPaddingElimination after the
+           output. GraphTransformer of PaddingElimination will check the FlagPaddingElimination and do the actual
+           padding elimination graph modification.
+        4. Return the hook handles for later removal.
+
+        """
+        if (
+            not self._runtime_options.enable_sparse_optimizer
+            or not self._runtime_options.enable_embedding_sparse_optimizer
+            or self._device.type != "cuda"
+        ):
+            return []
+
+        def _embedding_hook(module, args, output):
+            ebd_input = args[0]
+            if ebd_input is None or not isinstance(ebd_input, torch.Tensor):
+                self._logger.warning("Embedding input is not a tensor.")
+                return None
+
+            valid_token = torch.count_nonzero(ebd_input - module.padding_idx)
+            total_token = ebd_input.numel()
+            embed_density = float(valid_token) / float(total_token) * 100
+            if module not in self._runtime_inspector._embedding_module_to_padding_density_map:
+                self._logger.warning("Found Embedding module not in the map. %s", module)
+                return None
+
+            if embed_density < 90:
+                self._logger.info("Embedding sparsity-based optimization is ON for density: %.0f%%", embed_density)
+                if self._runtime_inspector._embedding_module_to_padding_density_map[module][1] != -1:
+                    self._logger.warning(
+                        "Found duplicate Embedding module. %s",
+                        self._runtime_inspector._embedding_module_to_padding_density_map[module][0],
+                    )
+                self._runtime_inspector._embedding_module_to_padding_density_map[module][1] = embed_density
+                return FlagPaddingElimination.apply(output)
+            else:
+                self._logger.info("Embedding sparsity-based optimization is OFF for density: %.0f%%", embed_density)
+                return None
+
+        embedding_hook_handles = []
+        for name, sub_module in self._flattened_module.named_modules():
+            if isinstance(sub_module, torch.nn.modules.sparse.Embedding):
+                if sub_module.padding_idx is not None and sub_module.padding_idx >= 0:
+                    self._runtime_inspector._embedding_module_to_padding_density_map[sub_module] = [name, -1]
+                    embedding_hook_handles.append(sub_module.register_forward_hook(_embedding_hook))
+
+        return embedding_hook_handles
+
     @_logger.TrackTime(_logger.ORTModuleInitPhase.DETECTION)
     def _enable_conditional_optimizations(
         self, graph_transformer_config: C.TrainingGraphTransformerConfiguration, inputs: Tuple, kwargs: Dict
     ):
         """
         Based on runtime inspection, enable conditional optimizations if applicable.
 
@@ -654,15 +766,15 @@
 
                     param_to_append_as_onnx_graph_inputs = get_params_not_connected_to_pull_param_trigger(
                         self._flattened_module.named_parameters(), self._onnx_models.exported_model
                     )
                 else:
                     param_to_append_as_onnx_graph_inputs = self._graph_initializers
 
-                _, embed_sparsity_results, label_sparsity_results = _io._combine_input_buffers_initializers(
+                _, _, label_sparsity_results = _io._combine_input_buffers_initializers(
                     param_to_append_as_onnx_graph_inputs,
                     self._graph_builder.get_graph_info().user_input_names,
                     self._input_info,
                     self._flattened_module.named_buffers(),
                     inputs,
                     kwargs,
                     detected_device,
@@ -674,19 +786,21 @@
                 if len(label_sparsity_results) > 0:
                     graph_transformer_config.sparse_label_input_names = list(label_sparsity_results.keys())
                     self._logger.info("Label sparsity-based optimization is ON for %s", label_sparsity_results)
                     self._runtime_options.label_sparsity_ratio = ",".join(
                         [f"{k}:{v:.0f}%" for k, v in label_sparsity_results.items()]
                     )
 
-                if self._runtime_options.enable_embedding_sparse_optimizer and len(embed_sparsity_results) > 0:
-                    graph_transformer_config.sparse_embedding_input_names = list(embed_sparsity_results.keys())
-                    self._logger.info("Embedding sparsity-based optimization is ON for %s", embed_sparsity_results)
+                if self._runtime_inspector._embedding_module_to_padding_density_map:
                     self._runtime_options.embed_sparsity_ratio = ",".join(
-                        [f"{k}:{v:.0f}%" for k, v in embed_sparsity_results.items()]
+                        [
+                            f"{v[0]}:{v[1]:.0f}%"
+                            for v in self._runtime_inspector._embedding_module_to_padding_density_map.values()
+                            if v[1] != -1
+                        ]
                     )
 
             # If users don't want to print input density, disable the input density observer to avoid overhead
             # when looping through inputs during training.
             if not self._runtime_options.print_input_density:
                 self._runtime_inspector.disable_input_inspector()
 
@@ -744,39 +858,48 @@
                 self._runtime_options.enable_custom_autograd_function,
                 "Support custom torch.autograd.Function export and execution",
             ],
         )
 
         if self._runtime_options.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
             opt_config_to_display = "ALL_RECOMPUTE_FOR_EACH_LAYER"
+        elif (
+            self._runtime_options.memory_optimization_level
+            == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE
+        ):
+            opt_config_to_display = "ALL_RECOMPUTE_FOR_EACH_LAYER_WITH_COMPROMISE"
         else:
             opt_config_to_display = self._runtime_options.memory_optimizer_config
 
+        mem_infos = ""
+        if self._runtime_options.memory_optimizer_is_enabled():
+            mem_infos += (
+                f"Memory Optimization Level: [{_MemoryOptimizationLevel.to_string(self._runtime_options.memory_optimization_level)}], "
+                f"Optimization Config: [{opt_config_to_display}]"
+            )
+        else:
+            mem_infos = "Enable with env ORTMODULE_MEMORY_OPT_LEVEL=1/2 or ORTMODULE_MEMORY_OPT_CONFIG=<plan1 config>,<plan2 config>,..."
+
         mem_row = _add_record(
             tbl,
             [
                 "Memory Optimizer",
-                len(self._runtime_options.memory_optimizer_config) > 0,
-                (
-                    f"Memory Optimization Level: [{_MemoryOptimizationLevel.to_string(self._runtime_options.memory_optimization_level)}], "
-                    f"Optimization Config: [{opt_config_to_display}]"
-                    if len(self._runtime_options.memory_optimizer_config) > 0
-                    else "Enable with env ORTMODULE_MEMORY_OPT_LEVEL=1 or ORTMODULE_MEMORY_OPT_CONFIG=<plan1 config>,<plan2 config>,..."
-                ),
+                self._runtime_options.memory_optimizer_is_enabled(),
+                mem_infos,
             ],
         )
 
         if self._runtime_inspector.memory_ob.is_enabled() and self._debug_options.logging.log_level < LogLevel.WARNING:
             mem_notes, mem_tbl = self._runtime_inspector.memory_ob.display_memory_optimization_plans(
                 self._runtime_options.memory_optimizer_config,
                 details=True,
             )
             if mem_tbl is not None:
                 mem_row.append_annotation_table(mem_tbl)
-                notes.extend(mem_notes)
+                notes.extend([f"[{mem_row._columns[0]}] {n}" for n in mem_notes])
 
         compute_opt_row = _add_record(
             tbl,
             [
                 "Compute Optimizer",
                 self._runtime_options.enable_compute_optimizer,
                 "Enable/Disable with env ORTMODULE_ENABLE_COMPUTE_OPTIMIZER=1/0",
@@ -793,21 +916,29 @@
             ],
         )
 
         if self._runtime_options.enable_compute_optimizer:
             if len(self._runtime_options.label_sparsity_ratio) > 0:
                 _add_record(
                     compute_opt_annotation_tbl,
-                    [" - Label Sparsity Opt", True, f"Input density: {self._runtime_options.label_sparsity_ratio}"],
+                    [
+                        " - Label Sparsity",
+                        True,
+                        f"[AUTO ENABLED] Input density: {self._runtime_options.label_sparsity_ratio}",
+                    ],
                 )
 
             if len(self._runtime_options.embed_sparsity_ratio) > 0:
                 _add_record(
                     compute_opt_annotation_tbl,
-                    [" - Embed Sparsity Opt", True, f"Input density: {self._runtime_options.embed_sparsity_ratio}"],
+                    [
+                        " - Embed Sparsity",
+                        True,
+                        f"[AUTO ENABLED] Input density: {self._runtime_options.embed_sparsity_ratio}",
+                    ],
                 )
 
         compute_opt_row.append_annotation_table(compute_opt_annotation_tbl)
 
         # Add fallback
         _add_record(
             tbl,
```

## onnxruntime/training/ortmodule/_inference_manager.py

```diff
@@ -24,16 +24,15 @@
 class InferenceManager(GraphExecutionManager):
     """Concrete instance of GraphExecutionManager that is able to manage the inference model
 
     InferenceManager is responsible for building and running the forward graph of the inference model
     """
 
     def __init__(self, model, debug_options: DebugOptions, fallback_manager: _FallbackManager, logger: Logger):
-        super().__init__(model, debug_options, fallback_manager, logger)
-        self._export_mode = torch.onnx.TrainingMode.EVAL
+        super().__init__(model, debug_options, torch.onnx.TrainingMode.EVAL, fallback_manager, logger)
 
     @staticmethod
     def execution_session_run_forward(
         execution_session,
         onnx_model: onnx.ModelProto,
         device: torch.device,
         *inputs,
```

## onnxruntime/training/ortmodule/_logger.py

```diff
@@ -263,17 +263,19 @@
                 raise RuntimeError("The class of the function to be tracked must have a '_debug_options' attribute.")
 
             with _suppress_os_stream_output(
                 enable=graph_execution_manager._debug_options.log_level >= LogLevel.DEVINFO,
                 on_exit=partial(
                     _log_with_filter,
                     graph_execution_manager._logger,
-                    graph_execution_manager._debug_options.onnxruntime_log_filter
-                    if self.is_ort_filter
-                    else graph_execution_manager._debug_options.torch_exporter_filter,
+                    (
+                        graph_execution_manager._debug_options.onnxruntime_log_filter
+                        if self.is_ort_filter
+                        else graph_execution_manager._debug_options.torch_exporter_filter
+                    ),
                     self.phase.to_string(),
                 ),
             ):
                 result = func(graph_execution_manager, *args, **kwargs)
             return result
 
         return wrapper
```

## onnxruntime/training/ortmodule/_runtime_inspector.py

```diff
@@ -10,15 +10,15 @@
 import onnx
 import torch
 from onnx import ModelProto, helper
 from onnx import onnx_pb as onnx_proto
 from sympy import Symbol, simplify
 from sympy.parsing.sympy_parser import parse_expr
 
-from onnxruntime.training.utils import PTable
+from onnxruntime.training.utils import PTable, log_memory_usage
 
 from ._execution_agent import TrainingAgent
 from .options import _MemoryOptimizationLevel, _RuntimeOptions
 
 
 class Phase(IntEnum):
     INVALID = -1
@@ -42,19 +42,27 @@
 
 
 class RuntimeInspector:
     """
     Runtime inspector for ORTModule.
     """
 
-    def __init__(self, logger: Logger, module: torch.nn.Module):
+    def __init__(self, logger: Logger, module: torch.nn.Module, training: bool):
+        """Initialize runtime inspector.
+
+        Args:
+            logger: Logger.
+            module: Torch module.
+            training: a boolean indicating whether the module is in training mode.
+        """
         self._logger = logger
 
         self.input_density_ob: Union[InputDensityObserver, None] = None
-        self.memory_ob = MemoryObserver(module, self._logger)
+        self.memory_ob = MemoryObserver(module, self._logger, training)
+        self._embedding_module_to_padding_density_map = {}
 
     def enable_input_inspector(self, model: ModelProto, user_input_names: List[str]) -> None:
         """Initialize input inspector from the given ONNX model and user input names.
 
         Args:
             model: ONNX model.
             user_input_names: User input names in the ONNX model.
@@ -429,17 +437,15 @@
                 input_name,
                 padding_idx,
                 density,
                 valid_token,
                 total_token,
                 valid_token_per_batch,
             ) in self._stats:
-                stat += "\t| {:<10} | {:<10} | {:<15} | {:<10} | {:<9.2f}% | {:<15} | {:<15} | {:<15} |\n".format(
-                    step, input_type, input_name, padding_idx, density, valid_token, total_token, valid_token_per_batch
-                )
+                stat += f"\t| {step:<10} | {input_type:<10} | {input_name:<15} | {padding_idx:<10} | {density:<9.2f}% | {valid_token:<15} | {total_token:<15} | {valid_token_per_batch:<15} |\n"
             stat += "<<<\n"
             self._logger.info(stat)
             self._stats.clear()
 
     def _try_get_node_from_its_output(self, name):
         if name == "" or name not in self._tensor_to_node_map:
             return None
@@ -477,15 +483,22 @@
     On different training/inference phases, `inspect_memory` is called to print out the memory usage, including
     current/peak memory usage, current/peak inactive and non-releasable memory.
     """
 
     NORMALIZER_FACTOR = float(1024 * 1024)
     NORMALIZER_UNIT = "MiB"
 
-    def __init__(self, m: torch.nn.Module, logger: Logger):
+    def __init__(self, m: torch.nn.Module, logger: Logger, training: bool):
+        """Initialize memory observer.
+
+        Args:
+            m: Torch module.
+            logger: Logger.
+            training: a boolean indicating whether the module is in training mode.
+        """
         self._logger = logger
         self._is_enabled = True
 
         # Memory optimization related.
         self.memory_optimization_opportunity_table_str = None
         self.cluster_id_combination_to_saving_symbolics_map: Dict[str, MemoryOptimizationSummary] = {}
         ## The value is a list of symbolic dim values parsed from the first batch.
@@ -501,18 +514,23 @@
         self._world_size = 1
         if torch.distributed.is_initialized():
             self._rank = torch.distributed.get_rank()
             self._world_size = torch.distributed.get_world_size()
 
         self._rank_info = f"[{self._rank}/{self._world_size}]"
         self._pre_phase = Phase.INVALID
-        self._last_phase = Phase.POST_BACKWARD if m.training else Phase.POST_FORWARD
+
+        # Cannot infer it is for training or inferencing purpose from module.training,
+        # because it probabbly is not set correctly when this happens.
+        self._last_phase = Phase.POST_BACKWARD if training else Phase.POST_FORWARD
 
         self._is_first_inspect = True
 
+        self._m = m
+
     def is_enabled(self) -> bool:
         """Check if memory inspector is enabled."""
         return self._is_enabled
 
     def enable_memory_stats_by_step(self, print_memory_stats_by_step: bool):
         # For per-step memory inspection.
         self._print_memory_stats_by_step = print_memory_stats_by_step
@@ -539,15 +557,18 @@
         """
 
         recompute_probe_config = runtime_options.recompute_probe_config
         memory_optimizer_config = runtime_options.memory_optimizer_config
 
         # If the memory optimization level is aggressive, we will first collect all
         # recompute subgraph by passing empty memory_optimizer_config to get_serialized_ortmodule_memory_stat.
-        if runtime_options.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+        if runtime_options.memory_optimization_level in [
+            _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE,
+            _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE,
+        ]:
             memory_optimizer_config = ""
 
         (
             self.memory_optimization_opportunity_table_str,
             memory_optimization_saving_symbolics,
         ) = execution_agent.get_serialized_ortmodule_memory_stat(memory_optimizer_config, recompute_probe_config)
 
@@ -575,24 +596,35 @@
             reverse=True,
         )
 
         for cluster_id, values in sorted_list:
             self.cluster_id_combination_to_saving_symbolics_map[cluster_id] = values
 
         # For aggressive memory optimization, we update the memory_optimizer_config using all.
-        if runtime_options.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+        if runtime_options.memory_optimization_level > 0:
             recompute_configs = []
             for cluster_id in self.cluster_id_combination_to_saving_symbolics_map:
                 config_values = cluster_id.split(":")
                 opt_type = int(config_values[1])
-                # TODO(pengwa): use enum instead of 1 here.
-                if opt_type != 1:
-                    continue
-
-                recompute_configs.append(cluster_id)
+                if (
+                    runtime_options.memory_optimization_level
+                    == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE
+                    and opt_type == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE
+                ):
+                    recompute_configs.append(cluster_id)
+                elif (
+                    runtime_options.memory_optimization_level
+                    == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE
+                    and opt_type
+                    in [
+                        _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE,
+                        _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE,
+                    ]
+                ):
+                    recompute_configs.append(cluster_id)
 
             runtime_options.memory_optimizer_config = ",".join(recompute_configs)
 
     def inspect_memory(self, cur_phase: Phase):
         """Inspect memory usage and print statistics.
 
         Args:
@@ -617,51 +649,32 @@
         if (cur_phase - self._pre_phase) != 1:
             raise RuntimeError(f"Invalid phase transition detected: {self._pre_phase} -> {cur_phase}")
 
         # For the 10+ steps, only print when it is power of 2.
         need_print = self._current_step < 10 or (self._current_step & (self._current_step - 1) == 0)
 
         if need_print:
-            cur_mem_allocated = self._normalize(torch.cuda.memory_allocated())
-            max_mem_allocated = self._normalize(torch.cuda.max_memory_allocated())
-            cur_mem_cached = self._normalize(torch.cuda.memory_reserved())
-            max_mem_cached = self._normalize(torch.cuda.max_memory_reserved())
-            torch_mem_stat = torch.cuda.memory_stats()
-            cur_mem_inactive = self._normalize(torch_mem_stat.get("inactive_split_bytes.all.current", 0))
-            max_mem_inactive = self._normalize(torch_mem_stat.get("inactive_split_bytes.all.peak", 0))
-
-            mem_stats = [
-                ["phase", _convert_phase_to_string(cur_phase)],
-                ["allocated", cur_mem_allocated],  # current memory allocated for tensors
-                ["max allocated", max_mem_allocated],  # peak memory allocated for tensors
-                ["cached", cur_mem_cached],  # current memory cached for the caching allocator
-                ["max cached", max_mem_cached],  # peak memory cached for caching allocator.
-                ["inactive", cur_mem_inactive],  # amount of inactive, non-releasable memory
-                ["max inactive", max_mem_inactive],  # peak of inactive, non-releasable memory
-            ]
-
-            summ = f"{self._rank_info} step {self._current_step} memory ({MemoryObserver.NORMALIZER_UNIT})"
-            for stat in mem_stats:
-                summ += f" | {stat[0]}: {stat[1]}"
-
-            self._logger.info(summ)
+            log_memory_usage(
+                _convert_phase_to_string(cur_phase),
+                rank_0_only=True,
+                step_info=f"step {self._current_step}",
+                logger=self._logger,
+                module=self._m,
+            )
 
         if cur_phase == self._last_phase:
             self._increase_step()
             self._pre_phase = Phase.INVALID
             return
 
         self._pre_phase = cur_phase
 
     def _increase_step(self):
         self._current_step += 1
 
-    def _normalize(self, mem_size_in_bytes: Union[float, int]) -> str:
-        return f"{float(mem_size_in_bytes) / MemoryObserver.NORMALIZER_FACTOR:.0f}"
-
     def display_memory_optimization_plans(self, memory_optimizer_config, details=False) -> Tuple[List[str], PTable]:
         mem_plan_count = len(self.cluster_id_combination_to_saving_symbolics_map)
 
         if mem_plan_count > 0:
             mem_tbl = PTable()
             if details:
                 mem_tbl.add_row(["", "", "", "", "Configs", "Freq", "Max Saving(Bytes)", "Saving Symbolic(Bytes)"])
@@ -696,38 +709,72 @@
 
                 cluster_ids_without_freq = _get_user_config_without_freq(cluster_id)
 
                 mem_tbl.add_row(
                     [
                         f" - Plan {index}",
                         ":",
-                        "ON"
-                        if all(cluster_id in user_configs_with_out_freq for cluster_id in cluster_ids_without_freq)
-                        else "OFF",
+                        (
+                            "ON"
+                            if all(cluster_id in user_configs_with_out_freq for cluster_id in cluster_ids_without_freq)
+                            else "OFF"
+                        ),
                         ":",
                         cluster_id,
                         saving_symbolic.freq if details else "",
                         saving_bytes if details else "",
                         saving_symbolic.simplified_symbolic_saving_expr if details else "",
                     ]
                 )
 
                 index += 1
 
             notes = []
             if details:
                 notes.append(
-                    "[Memory Optimizer] Use ORTMODULE_MEMORY_OPT_LEVEL=1 to enable all recomputable subgraphs per transformer layer."
+                    "Use ORTMODULE_MEMORY_OPT_LEVEL=1 or 2 to enable all recomputable subgraphs per transformer layer."
+                )
+                saving_recommendation = (
+                    "Or use comma as a delimiter to selectively enable multiple memory optimization plans:\n"
                 )
-                saving_recommendation = "[Memory Optimizer] Or use comma as a delimiter to selectively enable multiple memory optimization plans:\n"
                 saving_recommendation += "  export ORTMODULE_MEMORY_OPT_CONFIG=<plan1 config>,<plan2 config>,..."
 
                 notes.append(saving_recommendation)
 
-                saving_recommendation = "memory saving is calculated based on the 1st batch symbolic dim values:\n"
+                saving_recommendation = "Memory saving is calculated based on the 1st batch symbolic dim values:\n"
                 for dim_param, dim_value in self.symbolic_dim_name_to_value_map.items():
                     saving_recommendation += f"  {dim_param}={dim_value},"
                 notes.append(saving_recommendation)
 
             return notes, mem_tbl
 
         return [], None
+
+
+class FlagPaddingElimination(torch.autograd.Function):
+    """
+    FlagPaddingElimination is a PyTorch autograd function that does nothing in forward pass and backward pass.
+    It is used as a flag to tell the GraphTransformer of PaddingElimination to modify the graph to eliminate
+    the embedding padding.
+    """
+
+    @staticmethod
+    def forward(ctx, input):
+        return input
+
+    @staticmethod
+    def backward(ctx, grad_output: torch.Tensor):
+        return grad_output
+
+    @staticmethod
+    def infer_shape(
+        node: onnx.NodeProto,
+        tensor_input_shapes: List[Optional[List[Union[int, str]]]],
+        tensor_input_dtypes: List[torch.onnx.TensorProtoDataType],
+    ) -> Tuple[List[Optional[List[Union[int, str]]]], List[torch.onnx.TensorProtoDataType]]:
+        return tensor_input_shapes, tensor_input_dtypes
+
+    @staticmethod
+    def alias_input(node_proto_str: str):
+        fw_alias_map = [0]
+        bw_alias_map = [0]
+        return fw_alias_map, bw_alias_map
```

## onnxruntime/training/ortmodule/_training_manager.py

```diff
@@ -34,17 +34,15 @@
     def __init__(
         self,
         model: _FlattenedModule,
         debug_options: DebugOptions,
         fallback_manager: _FallbackManager,
         logger: Logger,
     ):
-        super().__init__(model, debug_options, fallback_manager, logger)
-
-        self._export_mode = torch.onnx.TrainingMode.TRAINING
+        super().__init__(model, debug_options, torch.onnx.TrainingMode.TRAINING, fallback_manager, logger)
         self._forward_class = self._create_autofunction_class()
 
     @staticmethod
     def execution_session_run_forward(
         execution_session,
         onnx_model: onnx.ModelProto,
         device: torch.device,
@@ -167,18 +165,18 @@
                 # Push user output grads to ONNX backend.
                 backward_inputs = C.OrtValueVector()
                 # Preallocate length of the vector. And then delete as required towards the end.
                 backward_inputs.reserve(len(grad_outputs))
                 for idx, grad_output in enumerate(grad_outputs):
                     if idx in self._graph_info.output_grad_indices_non_differentiable:
                         assert grad_output is None, (
-                            "ORT found the {}-th module output '{}' is "
+                            f"ORT found the {idx}-th module output '{self._graph_info.user_output_names[idx]}' is "
                             "non-differentiable according to the onnx graph. "
                             "However, the gradient value is still provided by "
-                            "PyTorch's autograd engine.".format(idx, self._graph_info.user_output_names[idx])
+                            "PyTorch's autograd engine."
                         )
                         continue
 
                     if grad_output is None:
                         shape, device, dtype = ctx.run_info.output_info[idx]
                         if idx in self._graph_info.output_grad_indices_require_full_shape:
                             grad_output = torch.zeros(shape, device=device, dtype=dtype)  # noqa: PLW2901
@@ -192,26 +190,28 @@
                         backward_inputs.push_back(
                             _utils._torch_tensor_to_dlpack(grad_output), grad_output.dtype is torch.bool
                         )
                 backward_inputs.shrink_to_fit()
 
                 # Run and get results
                 backward_outputs = C.OrtValueVector()
-                self._execution_agent.run_backward(backward_inputs, backward_outputs, ctx.run_info.state)
-                # Destroy the state immediately (as opposed to be at the mercy of garbage collector) so it does not
-                # affect peak memory usage in a subsequent graph run.
-                del ctx.run_info.state
-
-                # Fast version: all backward_outputs are converted first.
-                # This version only works if backward_outputs is an OrtValueVector.
-                transferred_backward_outputs = _utils._ortvalues_to_torch_tensor(backward_outputs, self._device)
-
-                self._runtime_inspector.memory_ob.inspect_memory(Phase.POST_BACKWARD)
-
-                return tuple(transferred_backward_outputs[idx] if idx != -1 else None for idx in self._gradient_map)
+                try:
+                    self._execution_agent.run_backward(backward_inputs, backward_outputs, ctx.run_info.state)
+                    # Destroy the state immediately (as opposed to be at the mercy of garbage collector) so it does not
+                    # affect peak memory usage in a subsequent graph run.
+
+                    # Fast version: all backward_outputs are converted first.
+                    # This version only works if backward_outputs is an OrtValueVector.
+                    transferred_backward_outputs = _utils._ortvalues_to_torch_tensor(backward_outputs, self._device)
+
+                    self._runtime_inspector.memory_ob.inspect_memory(Phase.POST_BACKWARD)
+                    res = tuple(transferred_backward_outputs[idx] if idx != -1 else None for idx in self._gradient_map)
+                    return res
+                finally:
+                    del ctx.run_info.state
 
         return _ORTModuleFunction
 
     def forward(self, *inputs, **kwargs):
         """Forward pass starts here and continues at `_ORTModuleFunction.forward`
 
         ONNX model is exported the first time this method is executed.
```

## onnxruntime/training/ortmodule/_utils.py

```diff
@@ -87,15 +87,15 @@
         # Therefore, the function `fct` does not know if the dlpack structure
         # is a boolean tensor or a uint8 tensor.
         # We could either consider another function as an input in
         # `to_dlpacks` or add an argument to `fct(dlp, ortvalue)`.
         # Second option makes it impossible to directly use `_from_dlpack` or
         # or `from_dlpack` from torch.
         # The best option would be to add boolean type in DLDataTypeCode.
-        for i in range(0, len(bool_indices)):
+        for i in range(len(bool_indices)):
             j = bool_indices[i]
             res[j] = res[j].to(torch.bool)
 
     return tuple(res)
 
 
 def _torch_tensor_to_dlpack(tensor: torch.Tensor):
```

## onnxruntime/training/ortmodule/options.py

```diff
@@ -192,24 +192,30 @@
         return _SkipCheck.SKIP_CHECK_DISABLED in self
 
 
 class _MemoryOptimizationLevel(IntFlag):
     """Enumeration to specify memory optimization level"""
 
     USER_SPECIFIED = 0  # Fully respect user-specified config
-    TRANSFORMER_LAYERWISE_RECOMPUTE = 1  # Enable all recomputable subgraphs per layer
+    TRANSFORMER_LAYERWISE_RECOMPUTE = (
+        1  # Enable all recomputable subgraphs (excluding compromised recomptable graphs) per layer
+    )
+    TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE = 2  # Enable all recomputable subgraphs per layer
 
     @staticmethod
     def to_string(memory_optimization_level):
         if memory_optimization_level == _MemoryOptimizationLevel.USER_SPECIFIED:
             return "USER_SPECIFIED"
 
         if memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
             return "TRANSFORMER_LAYERWISE_RECOMPUTE"
 
+        if memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE:
+            return "TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE"
+
         return ""
 
 
 class _RuntimeOptions:
     """Configurable runtime options for ORTModule."""
 
     def __init__(self, logger: Logger):
@@ -267,15 +273,15 @@
         self.use_memory_efficient_gradient = False
 
         # Configuration for compute optimization.
         self.enable_compute_optimizer = True
         self.enable_sparse_optimizer = True
         self.label_sparsity_ratio = ""
         self.embed_sparsity_ratio = ""
-        self.enable_embedding_sparse_optimizer = False  # TODO(pengwa): remove once validation on more models are done.
+        self.enable_embedding_sparse_optimizer = True
 
         # Configuration for memory optimization.
         self.memory_optimization_level = (
             _MemoryOptimizationLevel.USER_SPECIFIED
         )  # 0: use `memory_optimizer_config`; 1: aggressive optimization, enable all recomputable subgraphs.
         self.memory_optimizer_config = ""  # This is an advanced config, please refer to onnxruntime docs for details.
         # 1 is the op set level; 0 indicates whether consider the Transformer-based model's layer boundary when
@@ -340,15 +346,18 @@
                 self.enable_sparse_optimizer and int(os.getenv("ORTMODULE_ENABLE_EMBEDDING_SPARSE_OPTIMIZER")) == 1
             )
 
         # Configuration for memory optimization.
         self.memory_optimization_level = int(os.getenv("ORTMODULE_MEMORY_OPT_LEVEL", self.memory_optimization_level))
         user_given_memory_optimizer_config = os.getenv("ORTMODULE_MEMORY_OPT_CONFIG", self.memory_optimizer_config)
         self.memory_optimizer_config = ",".join([c for c in user_given_memory_optimizer_config.split(",") if c])
-        if self.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+        if self.memory_optimization_level in [
+            _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE,
+            _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE,
+        ]:
             # For transformer layer-wise recompute, we enable layer boundary when detecting subgraphs.
             # Then all detected subgraphs will not cross different layers.
             self.recompute_probe_config = "1:1"
 
         # Configuration for dev tools.
         if "ORTMODULE_PRINT_INPUT_DENSITY" in os.environ:
             self.print_input_density = int(os.getenv("ORTMODULE_PRINT_INPUT_DENSITY")) == 1
@@ -374,15 +383,17 @@
             "ORTMODULE_USE_TRITON" in os.environ
             and int(os.getenv("ORTMODULE_USE_TRITON")) == 1
             and C.is_triton_enabled()
         ):
             try:
                 import triton  # noqa: F401
             except ImportError:
-                pass
+                self._logger.warning(
+                    "triton library missing. Please install triton with `pip install triton`. Triton feature will be off."
+                )
             else:
                 self.enable_triton = True
 
         if "ORTMODULE_ENABLE_TUNING" in os.environ and int(os.getenv("ORTMODULE_ENABLE_TUNING")) == 1:
             self.enable_tuning = True
         if "ORTMODULE_MAX_TUNING_DURATION_MS" in os.environ:
             max_tuning_duration_ms = int(os.getenv("ORTMODULE_MAX_TUNING_DURATION_MS"))
@@ -412,11 +423,14 @@
         if "ORTMODULE_DEEPCOPY_BEFORE_MODEL_EXPORT" in os.environ:
             self.deepcopy_before_model_export = int(os.getenv("ORTMODULE_DEEPCOPY_BEFORE_MODEL_EXPORT")) == 1
 
     def memory_optimizer_is_enabled(self) -> bool:
         """Check whether memory optimizer is enabled."""
         if self.memory_optimization_level == _MemoryOptimizationLevel.USER_SPECIFIED:
             return len(self.memory_optimizer_config) > 0
-        elif self.memory_optimization_level == _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE:
+        elif self.memory_optimization_level in [
+            _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE,
+            _MemoryOptimizationLevel.TRANSFORMER_LAYERWISE_RECOMPUTE_WITH_COMPROMISE,
+        ]:
             return True
 
         return False
```

## onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py

```diff
@@ -210,16 +210,15 @@
         def recursive_wrap(module, save_onnx=False, onnx_prefix=""):
             sub_module_dict = module._modules
             for name, sub_module in sub_module_dict.items():
                 new_prefix = onnx_prefix + "_" + name
                 if isinstance(sub_module, torch.nn.ModuleList):
                     # We encounter a list of sub-modules.
                     # Let's wrap them one-by-one.
-                    idx = 0
-                    for item_name, sub_module_item in sub_module._modules.items():
+                    for idx, (item_name, sub_module_item) in enumerate(sub_module._modules.items()):
                         # Avoid saving too many graphs.
                         new_save_onnx = save_onnx and idx == 0
                         sub_new_prefix = new_prefix + "_" + item_name
                         if is_supported(sub_module_item):
                             if sub_module_item in module_arg_pool and len(module_arg_pool[sub_module_item]) > 1:
                                 sub_module._modules[item_name] = _IteratedORTModule(
                                     sub_module_item,
@@ -233,15 +232,14 @@
                                     sub_module_item,
                                     debug_options=DebugOptions(
                                         log_level=self._log_level, save_onnx=new_save_onnx, onnx_prefix=sub_new_prefix
                                     ),
                                 )
                         else:
                             recursive_wrap(sub_module_item, new_save_onnx, sub_new_prefix)
-                        idx += 1
                 else:
                     if is_supported(sub_module):
                         # Just wrap it as ORTModule when possible.
                         if sub_module in module_arg_pool and len(module_arg_pool[sub_module]) > 1:
                             sub_module_dict[name] = _IteratedORTModule(
                                 sub_module, len(module_arg_pool[sub_module]), self._log_level, save_onnx, new_prefix
                             )
```

## onnxruntime/training/ortmodule/graph_optimizers/__init__.py

```diff
@@ -9,14 +9,14 @@
 from packaging.version import Version
 
 _all_optimizers = []
 
 if (
     "ORTMODULE_USE_EFFICIENT_ATTENTION" in os.environ
     and int(os.getenv("ORTMODULE_USE_EFFICIENT_ATTENTION")) == 1
-    and Version(torch.__version__) >= Version("2.1.1")
+    and Version(torch.__version__) >= Version("2.3.0")
 ):
     from ._aten_attn import optimize_graph_for_aten_efficient_attention  # noqa: F401
 
     _all_optimizers.append("optimize_graph_for_aten_efficient_attention")
 
 __all__ = _all_optimizers  # noqa: PLE0605
```

## onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py

```diff
@@ -1,17 +1,20 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 """
 PyTorch's _efficient_attention_forward/_efficient_attention_backward APIs is keep changing. Current implementation
-is tested well on version 2.2.0.dev20231010+cu121, and should be run well since official version 2.2.0. If may fail to
+is tested well on version 2.3.0.dev20240221+cu118, and should be run well since official version 2.3.0. If may fail to
 run is you are using PyTorch with older versions.
 
+This file is more like an example of how to add a new graph optimizer. Ideally user can add graph optimizer according
+to the specific model they are using on their own instead of putting every possible graph optimizer here.
+
 PyTorch also has API for flash attention (currently doesn't support random attention mask or Dropout), we can add
 support if we want to try in the future.
 """
 
 from typing import List, Tuple
 
 from onnx import GraphProto, NodeProto, TensorProto, helper
@@ -36,89 +39,92 @@
     dropout_ratio: float,
     causal: bool,
 ):
     nodes_to_add = []
     scale_node = make_constant_node("scale_" + str(idx), TensorProto.FLOAT, [], [scale])
     dropout_ratio_node = make_constant_node("dropout_ratio_" + str(idx), TensorProto.FLOAT, [], [dropout_ratio])
     causal_node = make_constant_node("causal_" + str(idx), TensorProto.INT64, [], [1 if causal else 0])
-    int_zero_node = make_constant_node("int_zero_" + str(idx), TensorProto.INT64, [], [0])
-    true_node = make_constant_node("true_" + str(idx), TensorProto.BOOL, [], [True])
-    false_node = make_constant_node("false_" + str(idx), TensorProto.BOOL, [], [False])
+    one_node = make_constant_node("one_" + str(idx), TensorProto.INT64, [], [1])
+    zero_node = make_constant_node("zero_" + str(idx), TensorProto.INT64, [], [0])
     logsumexp = helper.make_tensor_value_info("logsumexp" + str(idx), TensorProto.FLOAT, [])
     seed = helper.make_tensor_value_info("seed" + str(idx), TensorProto.INT64, [])
     offset = helper.make_tensor_value_info("offset" + str(idx), TensorProto.INT64, [])
-    new_value_infos = [logsumexp, seed, offset]
+    msb_q = helper.make_tensor_value_info("msb_q_" + str(idx), TensorProto.INT64, [])
+    msb_k = helper.make_tensor_value_info("msb_k_" + str(idx), TensorProto.INT64, [])
+    new_value_infos = [logsumexp, seed, offset, msb_q, msb_k]
     if expand_bias:
         shape_0 = helper.make_node("Shape", [q], ["shape_0_" + str(idx)], start=0, end=1)
         shape_1 = helper.make_node("Shape", [q], ["shape_1_" + str(idx)], start=2, end=3)
         shape_2 = helper.make_node("Shape", [q], ["shape_2_" + str(idx)], start=1, end=2)
         shape_3 = helper.make_node("Shape", [k], ["shape_3_" + str(idx)], start=1, end=2)
         concat = helper.make_node(
             "Concat",
-            ["shape_0_" + str(idx), "shape_1_" + str(idx), "shape_2_" + str(idx), "shape_3_" + str(idx)],
+            [shape_0.output[0], shape_1.output[0], shape_2.output[0], shape_3.output[0]],
             ["concated_shape_" + str(idx)],
             axis=0,
         )
-        expand = helper.make_node("Expand", [bias, "concated_shape_" + str(idx)], ["expanded_bias_" + str(idx)])
+        expand = helper.make_node("Expand", [bias, concat.output[0]], ["expanded_bias_" + str(idx)])
         nodes_to_add.extend([shape_0, shape_1, shape_2, shape_3, concat, expand])
-        bias = "expanded_bias_" + str(idx)
+        bias = expand.output[0]
     fwd_node = helper.make_node(
         "ATen",
         [
             q,
             k,
             v,
             bias,
             "",
             "",
             "",
+            "",
             dropout_ratio_node.output[0],
             causal_node.output[0],
-            true_node.output[0],
+            one_node.output[0],
             scale_node.output[0],
             "",
             "",
         ],
-        [y, logsumexp.name, seed.name, offset.name],
+        [y, logsumexp.name, seed.name, offset.name, msb_q.name, msb_k.name],
         "efficient_attention_forward_" + str(idx),
         None,
         "org.pytorch.aten",
         operator="_efficient_attention_forward",
+        cpu_input_args=[4, 5, 12, 13],
+        cpu_output_args=[2, 3, 4, 5],
     )
     bwd_node = helper.make_node(
         "ATen",
         [
             dy,
             q,
             k,
             v,
             bias,
             y,
             "",
             "",
-            int_zero_node.output[0],
-            int_zero_node.output[0],
+            msb_q.name,
+            msb_k.name,
             logsumexp.name,
             dropout_ratio_node.output[0],
             seed.name,
             offset.name,
             causal_node.output[0],
-            false_node.output[0],
+            zero_node.output[0],
             scale_node.output[0],
             "",
         ],
         [dq, dk, dv, ""],
         "efficient_attention_backward_" + str(idx),
         None,
         "org.pytorch.aten",
         operator="_efficient_attention_backward",
+        cpu_input_args=[6, 7, 12, 13],
     )
-    nodes_to_add.extend(
-        [scale_node, dropout_ratio_node, causal_node, int_zero_node, true_node, false_node, fwd_node, bwd_node]
-    )
+    nodes_to_add.extend([scale_node, dropout_ratio_node, causal_node, one_node, zero_node, fwd_node, bwd_node])
     return nodes_to_add, new_value_infos
 
 
 # Without causal mask, with Dropout. For example, BERT model in HuggingFace.
 _PATTERN_0: List[Tuple[str, bool, List[Tuple[int, int, int]]]] = [
     ("MatMul", False, []),  # 0
     ("Transpose", True, [(0, 0, 0)]),  # 1
@@ -236,148 +242,17 @@
         1 / float(scale_value[0] if isinstance(scale_value, list) else scale_value),
         0.0,
         False,
     )
     return nodes, nodes_to_add, new_value_infos
 
 
-# No causal mask, no attention mask, without Dropout.
-_PATTERN_2: List[Tuple[str, bool, List[Tuple[int, int, int]]]] = [
-    ("MatMul", False, []),  # 0
-    ("Mul", True, [(0, 0, 0)]),  # 1
-    ("Mul", True, [(0, 0, 1)]),  # 2
-    ("Transpose", True, [(1, 0, 0)]),  # 3
-    ("Transpose", True, [(2, 0, 0)]),  # 4
-    ("Softmax", False, [(0, 0, 0)]),  # 5
-    ("MatMul", False, [(5, 0, 0)]),  # 6
-    ("Transpose", True, [(6, 0, 1)]),  # 7
-    ("Transpose", False, [(6, 0, 0)]),  # 8
-    ("FusedMatMul", False, [(7, 0, 1)]),  # 9
-    ("SoftmaxGrad_13", False, [(9, 0, 0), (5, 0, 1)]),  # 10
-    ("FusedMatMul", False, [(2, 0, 1), (10, 0, 0)]),  # 11
-    ("FusedMatMul", False, [(1, 0, 0), (10, 0, 1)]),  # 12
-    ("Mul", False, [(11, 0, 0)]),  # 13
-    ("Mul", False, [(12, 0, 0)]),  # 14
-    ("Identity", False, [(13, 0, 0)]),  # 15
-    ("Identity", False, [(14, 0, 0)]),  # 16
-    ("Transpose", False, [(15, 0, 0)]),  # 17
-    ("Transpose", False, [(16, 0, 0)]),  # 18
-    ("FusedMatMul", False, [(5, 0, 0)]),  # 19
-    ("Transpose", True, [(19, 0, 1)]),  # 20
-    ("Transpose", False, [(19, 0, 0)]),  # 21
-]
-
-
-def _optimize_for_pattern_2(matcher: GraphMatcher, idx: int, nodes: List[NodeProto]):
-    # Check forward only as the backward is expected to be consistent if it's built correctly.
-    scale_value_1 = matcher.get_constant_value(nodes[1].input[1])
-    scale_value_1 = scale_value_1[0] if isinstance(scale_value_1, list) else scale_value_1
-    scale_value_2 = matcher.get_constant_value(nodes[2].input[1])
-    scale_value_2 = scale_value_2[0] if isinstance(scale_value_2, list) else scale_value_2
-    if not (
-        check_attribute_value(nodes[3], "perm", [0, 2, 1, 3])
-        and check_attribute_value(nodes[4], "perm", [0, 2, 3, 1])
-        and check_attribute_value(nodes[7], "perm", [0, 2, 1, 3])
-        and check_attribute_value(nodes[8], "perm", [0, 2, 1, 3])
-        and scale_value_1 == scale_value_2
-    ):
-        return [], [], []
-
-    nodes_to_add, new_value_infos = _make_efficient_attention_nodes(
-        idx,
-        nodes[3].input[0],
-        nodes[4].input[0],
-        nodes[7].input[0],
-        nodes[8].output[0],
-        nodes[20].input[0],
-        nodes[17].output[0],
-        nodes[18].output[0],
-        nodes[21].output[0],
-        "",
-        False,
-        scale_value_1,
-        0.0,
-        False,
-    )
-    return nodes, nodes_to_add, new_value_infos
-
-
-# Has causal mask, no attention mask, without Dropout.
-_PATTERN_3: List[Tuple[str, bool, List[Tuple[int, int, int]]]] = [
-    ("MatMul", False, []),  # 0
-    ("Mul", True, [(0, 0, 0)]),  # 1
-    ("Mul", True, [(0, 0, 1)]),  # 2
-    ("Transpose", True, [(1, 0, 0)]),  # 3
-    ("Transpose", True, [(2, 0, 0)]),  # 4
-    ("Add", False, [(0, 0, 0)]),  # 5
-    ("Slice", True, [(5, 0, 1)]),  # 6
-    ("Slice", True, [(6, 0, 0)]),  # 7
-    ("Unsqueeze", True, [(6, 0, 2)]),  # 8
-    ("Gather", True, [(8, 0, 0)]),  # 9
-    ("Shape", True, [(9, 0, 0)]),  # 10
-    ("Softmax", False, [(5, 0, 0)]),  # 11
-    ("MatMul", False, [(11, 0, 0)]),  # 12
-    ("Transpose", True, [(12, 0, 1)]),  # 13
-    ("Transpose", False, [(12, 0, 0)]),  # 14
-    ("FusedMatMul", False, [(13, 0, 1)]),  # 15
-    ("SoftmaxGrad_13", False, [(15, 0, 0), (11, 0, 1)]),  # 16
-    ("Identity", False, [(16, 0, 0)]),  # 17
-    ("FusedMatMul", False, [(2, 0, 1), (17, 0, 0)]),  # 18
-    ("FusedMatMul", False, [(1, 0, 0), (17, 0, 1)]),  # 19
-    ("Mul", False, [(18, 0, 0)]),  # 20
-    ("Mul", False, [(19, 0, 0)]),  # 21
-    ("Identity", False, [(20, 0, 0)]),  # 22
-    ("Identity", False, [(21, 0, 0)]),  # 23
-    ("Transpose", False, [(22, 0, 0)]),  # 24
-    ("Transpose", False, [(23, 0, 0)]),  # 25
-    ("FusedMatMul", False, [(11, 0, 0)]),  # 26
-    ("Transpose", True, [(26, 0, 1)]),  # 27
-    ("Transpose", False, [(26, 0, 0)]),  # 28
-]
-
-
-def _optimize_for_pattern_3(matcher: GraphMatcher, idx: int, nodes: List[NodeProto]):
-    # Check forward only as the backward is expected to be consistent if it's built correctly.
-    scale_value_1 = matcher.get_constant_value(nodes[1].input[1])
-    scale_value_1 = scale_value_1[0] if isinstance(scale_value_1, list) else scale_value_1
-    scale_value_2 = matcher.get_constant_value(nodes[2].input[1])
-    scale_value_2 = scale_value_2[0] if isinstance(scale_value_2, list) else scale_value_2
-    if not (
-        check_attribute_value(nodes[3], "perm", [0, 2, 1, 3])
-        and check_attribute_value(nodes[4], "perm", [0, 2, 3, 1])
-        and check_attribute_value(nodes[13], "perm", [0, 2, 1, 3])
-        and check_attribute_value(nodes[14], "perm", [0, 2, 1, 3])
-        and scale_value_1 == scale_value_2
-    ):
-        return [], [], []
-
-    nodes_to_add, new_value_infos = _make_efficient_attention_nodes(
-        idx,
-        nodes[3].input[0],
-        nodes[4].input[0],
-        nodes[13].input[0],
-        nodes[14].output[0],
-        nodes[27].input[0],
-        nodes[24].output[0],
-        nodes[25].output[0],
-        nodes[28].output[0],
-        "",
-        False,
-        scale_value_1,
-        0.0,
-        True,
-    )
-    return nodes, nodes_to_add, new_value_infos
-
-
 _PATTERNS = [
     (_PATTERN_0, _optimize_for_pattern_0),
     (_PATTERN_1, _optimize_for_pattern_1),
-    (_PATTERN_2, _optimize_for_pattern_2),
-    (_PATTERN_3, _optimize_for_pattern_3),
 ]
 
 
 @register_graph_optimizer(devices="cuda")
 def optimize_graph_for_aten_efficient_attention(graph: GraphProto):
     nodes_to_remove = []
     nodes_to_add = []
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py

```diff
@@ -25,9 +25,9 @@
 
 
 @run_once_aten_op_executor
 def load_aten_op_executor_cpp_extension():
     from onnxruntime.training.ortmodule.torch_cpp_extensions import aten_op_executor
 
     _C.register_aten_op_executor(
-        str(aten_op_executor.is_cpu_argument_address()), str(aten_op_executor.execute_aten_operator_address())
+        str(aten_op_executor.is_tensor_argument_address()), str(aten_op_executor.execute_aten_operator_address())
     )
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc

```diff
@@ -30,26 +30,31 @@
   std::shared_ptr<torch::jit::Operator> op;
   size_t argument_size;
   std::vector<c10::TypeKind> elem_kinds;
   std::vector<bool> is_list_arguments;
   std::vector<bool> is_optional_arguments;
   std::vector<c10::optional<c10::IValue>> default_values;
   size_t return_size;
+  std::vector<c10::TypeKind> ret_kinds;
 
-  c10::IValue ToIValueArgument(const DLManagedTensor* dlpack, size_t index) const {
+  c10::IValue ToIValueArgument(DLManagedTensor* dlpack, size_t index) const {
     TORCH_INTERNAL_ASSERT(index < argument_size);
     bool is_optional = is_optional_arguments[index];
-    TORCH_INTERNAL_ASSERT(dlpack || is_optional || default_values[index]);
+    TORCH_INTERNAL_ASSERT(dlpack || is_optional || default_values[index] ||
+                          elem_kinds[index] == c10::TypeKind::TensorType);
     if (!dlpack) {
       if (is_optional) {
         // Optional argument always has no default value.
         return c10::IValue(c10::nullopt);
       }
-
-      return *default_values[index];
+      if (default_values[index]) {
+        return *default_values[index];
+      }
+      // Fow bw func, it's possible that input is an undefined tensor from fw outputs, dlpack is nullptr for such case.
+      return c10::IValue(at::Tensor());
     }
 
     bool is_list = is_list_arguments[index];
     c10::IValue i_value;
     // Create the torch tensor from this DLPack no matter we need it or not below,
     // so that the dlpack's deleter will be triggered when torch tensor is out of scope.
     at::Tensor tensor = at::fromDLPack(dlpack);
@@ -138,52 +143,38 @@
         aten_op.elem_kinds.emplace_back(elem_type);
         aten_op.is_list_arguments.emplace_back(is_list);
         aten_op.is_optional_arguments.emplace_back(is_optional);
         aten_op.default_values.emplace_back(argument.default_value());
       }
       aten_op.return_size = schema.returns().size();
       for (const auto& ret : schema.returns()) {
-        TORCH_INTERNAL_ASSERT(ret.type()->kind() == c10::TypeKind::TensorType);
+        c10::TypeKind ret_type = ret.type()->kind();
+        // Support tensor or int only for now.
+        TORCH_INTERNAL_ASSERT(ret_type == c10::TypeKind::TensorType || ret_type == c10::TypeKind::IntType);
+        aten_op.ret_kinds.emplace_back(ret_type);
       }
       ops_.emplace(key, aten_op);
     }
     return ops_.at(key);
   }
 
  private:
   ATenOperatorCache() = default;
   std::unordered_map<std::pair<std::string, std::string>, ATenOperator, PairHash> ops_;
 };
 
-const std::unordered_map<std::string, std::unordered_set<size_t>> kCpuTensorInputsMap = {
-    {"_efficient_attention_forward", {4, 5, 11, 12}}, {"_efficient_attention_backward", {6, 7, 12, 13}}};
-
-const std::unordered_map<std::string, std::unordered_set<size_t>> kCpuTensorOutputsMap = {
-    {"_efficient_attention_forward", {2, 3}}};
-
-// Backend uses this function to check if an argument is CPU input or not.
-bool IsCpuArgument(const char* op_name, const char* overload_name, size_t index, bool is_input) {
+// Backend uses this function to check if an argument is tensor type or not.
+bool IsTensorArgument(const char* op_name, const char* overload_name, size_t index, bool is_input) {
+  const auto& aten_op = ATenOperatorCache::Instance().GetOperator(op_name, overload_name);
   if (is_input) {
-    // If the argument is non-tensor type, it's CPU argument.
-    const auto& aten_op = ATenOperatorCache::Instance().GetOperator(op_name, overload_name);
     TORCH_INTERNAL_ASSERT(index < aten_op.argument_size);
-    if (aten_op.elem_kinds[index] != c10::TypeKind::TensorType) {
-      return true;
-    }
-  }
-
-  std::string full_name = std::string(op_name);
-  std::string overload_name_str = std::string(overload_name);
-  if (overload_name_str != "") {
-    full_name += ("." + overload_name_str);
+    return aten_op.elem_kinds[index] == c10::TypeKind::TensorType;
   }
-
-  const auto& cpu_tensors_map = is_input ? kCpuTensorInputsMap : kCpuTensorOutputsMap;
-  return cpu_tensors_map.find(full_name) != cpu_tensors_map.end() &&
-         cpu_tensors_map.at(full_name).find(index) != cpu_tensors_map.at(full_name).end();
+  TORCH_INTERNAL_ASSERT(index < aten_op.return_size);
+  return aten_op.ret_kinds[index] == c10::TypeKind::TensorType;
 }
 
 void ExecuteATenOperator(const char* op_name, const char* overload_name, size_t input_size,
                          DLManagedTensor** dlpack_inputs, size_t output_size, DLManagedTensor** dlpack_outputs) {
   const auto& aten_op = ATenOperatorCache::Instance().GetOperator(op_name, overload_name);
   TORCH_INTERNAL_ASSERT(input_size == aten_op.argument_size);
   std::vector<c10::IValue> arguments;
@@ -212,20 +203,27 @@
   // torch version is < 1.10
   aten_op.op->getOperation()(&stack);
 #endif
 
   TORCH_INTERNAL_ASSERT(output_size == aten_op.return_size);
   size_t output_index = 0;
   for (const auto& ret : torch::jit::pop(stack, output_size)) {
-    const auto& tensor = ret.toTensor();
-    dlpack_outputs[output_index++] =
-        tensor.defined() ? at::toDLPack(tensor.is_contiguous() ? tensor : tensor.contiguous()) : nullptr;
+    if (ret.isTensor()) {
+      const auto& tensor = ret.toTensor();
+      dlpack_outputs[output_index++] =
+          tensor.defined() ? at::toDLPack(tensor.is_contiguous() ? tensor : tensor.contiguous()) : nullptr;
+    } else if (ret.isInt()) {
+      at::Tensor scalar = at::scalar_to_tensor(at::Scalar(ret.toInt()));
+      dlpack_outputs[output_index++] = at::toDLPack(scalar);
+    } else {
+      TORCH_INTERNAL_ASSERT(false);
+    }
   }
 }
 
-size_t is_cpu_argument_address() { return reinterpret_cast<size_t>(&IsCpuArgument); }
+size_t is_tensor_argument_address() { return reinterpret_cast<size_t>(&IsTensorArgument); }
 size_t execute_aten_operator_address() { return reinterpret_cast<size_t>(&ExecuteATenOperator); }
 
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-  m.def("is_cpu_argument_address", &is_cpu_argument_address, "Address of tensor argument check.");
+  m.def("is_tensor_argument_address", &is_tensor_argument_address, "Address of tensor argument check.");
   m.def("execute_aten_operator_address", &execute_aten_operator_address, "Address of Aten operator executor");
 }
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc

```diff
@@ -56,17 +56,18 @@
 
       at::Tensor tensor;
       bool is_dlpack = PyCapsule_IsValid(args[arg_index], "dltensor") != 0;
       if (is_dlpack) {
         tensor = torch::utils::tensor_fromDLPack(args[arg_index]);
       } else {
         TORCH_CHECK(args[arg_index] == Py_None, "Only None is supported for non-tensor input.");
-        PyObject* fw_kernel_invoke_id = PyObject_GetAttrString(ctx.ptr(), "fw_kernel_invoke_id");
+        py::object fw_kernel_invoke_id = PyObject_FastGetAttrString(ctx.ptr(), "fw_kernel_invoke_id");
+        TORCH_CHECK(fw_kernel_invoke_id.ptr() != nullptr, "fw_kernel_invoke_id is not found in the context.");
         std::string fw_kernel_invoke_id_str =
-            py::cast<std::string>(py::reinterpret_borrow<py::object>(fw_kernel_invoke_id));
+            py::cast<std::string>(fw_kernel_invoke_id);
         CustomFuncOpKernelInfo& fw_kernel_info =
             KernelInfoStore::GetInstance().GetKernelInfoMap().at(fw_kernel_invoke_id_str);
         if (fw_kernel_info.materialize_grads) {
           auto& config = fw_kernel_info.materialize_grads_config.at(arg_index - 1);
           tensor = at::zeros(std::get<0>(config), std::get<1>(config));  // shift by 1 to skip context input.
         }
       }
```

## onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc

```diff
@@ -251,15 +251,15 @@
         py::reinterpret_steal<py::object>(
             PyImport_ImportModule("onnxruntime.training.ortmodule.torch_cpp_extensions.cpu.torch_interop_utils.fake_ctx"));
     if (!module.ptr()) {
       PyErr_Print();
       throw std::runtime_error("Fails to import the module.");
     }
 
-    auto python_class = py::reinterpret_steal<py::object>(PyObject_GetAttrString(module.ptr(), "FakeContext"));
+    auto python_class = PyObject_FastGetAttrString(module.ptr(), "FakeContext");
     if (!PyCallable_Check(python_class.ptr())) {
       throw std::runtime_error("Cannot instantiate the Python class");
     }
 
     kclass_obj = py::reinterpret_borrow<py::object>(python_class.ptr());
   }
```

## onnxruntime/training/utils/__init__.py

```diff
@@ -8,14 +8,15 @@
     ORTModelInputOutputSchemaType,
     ORTModelInputOutputType,
     PrimitiveType,
     extract_data_and_schema,
     unflatten_data_using_schema,
 )
 from onnxruntime.training.utils.torch_profile_utils import (
+    log_memory_usage,
     nvtx_function_decorator,
     torch_nvtx_range_pop,
     torch_nvtx_range_push,
 )
 from onnxruntime.training.utils.torch_type_map import (
     onnx_dtype_to_pytorch_dtype,
     pytorch_scalar_type_to_pytorch_dtype,
@@ -27,12 +28,13 @@
     "ORTModelInputOutputType",
     "ORTModelInputOutputSchemaType",
     "extract_data_and_schema",
     "unflatten_data_using_schema",
     "torch_nvtx_range_push",
     "torch_nvtx_range_pop",
     "nvtx_function_decorator",
+    "log_memory_usage",
     "pytorch_type_to_onnx_dtype",
     "onnx_dtype_to_pytorch_dtype",
     "pytorch_scalar_type_to_pytorch_dtype",
     "PTable",
 ]
```

## onnxruntime/training/utils/torch_profile_utils.py

```diff
@@ -1,12 +1,14 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+from __future__ import annotations
+
 import torch
 
 
 def torch_nvtx_range_push(msg):
     if hasattr(torch.cuda.nvtx, "range_push"):
         torch.cuda.nvtx.range_push(msg)
 
@@ -22,7 +24,81 @@
     def wrapped_fn(*args, **kwargs):
         torch_nvtx_range_push(func.__qualname__)
         ret_val = func(*args, **kwargs)
         torch_nvtx_range_pop()
         return ret_val
 
     return wrapped_fn
+
+
+def log_memory_usage(cur_phase: str, rank_0_only=True, step_info="", logger=None, module=None):
+    """Log memory usage for the current phase.
+    Args:
+        cur_phase (str): The current phase.
+        rank_0_only (bool, optional): Only log the memory usage for rank 0. Defaults to True.
+        step_info (str, optional): The step information. Defaults to "".
+        logger (logging.Logger, optional): The logger to log the memory usage. Defaults to None, which means print to stdout.
+        module (torch.nn.Module, optional): The module to get parameter, buffer and grad sizes. Defaults to None.
+    """
+    rank = 0
+    if rank_0_only is True:
+        if torch.distributed.is_initialized():
+            rank = torch.distributed.get_rank()
+        if rank != 0:
+            return
+
+    _normalizer_factor = float(1024 * 1024)
+    _normalizer_unit = "MiB"
+
+    def _normalize(mem_size_in_bytes: float | int) -> str:
+        return f"{float(mem_size_in_bytes) / _normalizer_factor:.0f}"
+
+    cur_mem_allocated = _normalize(torch.cuda.memory_allocated())
+    max_mem_allocated = _normalize(torch.cuda.max_memory_allocated())
+    cur_mem_cached = _normalize(torch.cuda.memory_reserved())
+    max_mem_cached = _normalize(torch.cuda.max_memory_reserved())
+    torch_mem_stat = torch.cuda.memory_stats()
+    cur_mem_inactive = _normalize(torch_mem_stat.get("inactive_split_bytes.all.current", 0))
+    max_mem_inactive = _normalize(torch_mem_stat.get("inactive_split_bytes.all.peak", 0))
+
+    mem_stats = [
+        ["phase", cur_phase],
+        ["allocated", cur_mem_allocated],  # current memory allocated for tensors
+        ["max allocated", max_mem_allocated],  # peak memory allocated for tensors
+        ["cached", cur_mem_cached],  # current memory cached for the caching allocator
+        ["max cached", max_mem_cached],  # peak memory cached for caching allocator.
+        ["inactive", cur_mem_inactive],  # amount of inactive, non-releasable memory
+        ["max inactive", max_mem_inactive],  # peak of inactive, non-releasable memory
+    ]
+
+    # Calculate the total size of parameters and gradients in the model
+    if module:
+        param_total_size = 0
+        grad_total_size = 0
+        for p in module.parameters():
+            if p.is_cuda:
+                param_total_size += p.numel() * p.element_size()
+            if p.grad is not None and p.grad.is_cuda:
+                grad_total_size += p.grad.numel() * p.grad.element_size()
+
+        # Calculate the total size of buffers in the model
+        buffer_total_size = 0
+        for b in module.buffers():
+            if b.is_cuda:
+                buffer_total_size += b.numel() * b.element_size()
+
+        mem_stats.extend(
+            [
+                ["param", _normalize(param_total_size)],
+                ["grad", _normalize(grad_total_size)],
+                ["buffer", _normalize(buffer_total_size)],
+            ]
+        )
+
+    summ = f"rank-{rank} {step_info} memory ({_normalizer_unit})"
+    for stat in mem_stats:
+        summ += f" | {stat[0]}: {stat[1]}"
+
+    if logger is None:
+        print(summ)
+    else:
+        logger.info(summ)
```

## onnxruntime/training/utils/hooks/_statistics_subscriber.py

```diff
@@ -10,14 +10,15 @@
 from pathlib import Path
 from typing import List, Optional, Tuple, Union
 
 import onnx
 import torch
 
 from ._subscriber_base import RuntimeStates, SubscriberBase
+from ._subscriber_manager import ORT_NO_INCREASE_GLOBAL_STEP
 
 
 class _InspectActivation(torch.autograd.Function):
     """
     This class is used to run the subscriber's forward and backward functions.
     The function will be called by two kinds of callers:
         1. SubscriberManager calls it for each registered nn.Module.
@@ -172,29 +173,31 @@
         output_file_path = os.path.join(f"{self._output_dir}", f"step_{step}")
         return self._summarize_activations(activation, depth, name, output_file_path, False)
 
     def _summarize_activations(self, tensor: torch.Tensor, depth: int, name: str, step_folder: str, is_forward: bool):
         display_name = name + " forward run" if is_forward is True else name + " backward run"
         output_file_name = name + "_forward" if is_forward is True else name + "_backward"
 
-        if tensor is None or not isinstance(tensor, torch.Tensor):
-            print(f"{display_name} not a torch tensor, value: {tensor}")
-            return
-
-        step_path = Path(step_folder)
-        if not step_path.exists():
-            step_path.mkdir(parents=True, exist_ok=False)
-        order_file_path = step_path / "order.txt"
-        tensor_file_path = step_path / output_file_name
+        # Skip dump during model pre-export output schema preparison run and export run.
+        if ORT_NO_INCREASE_GLOBAL_STEP[0] is False:
+            if tensor is None or not isinstance(tensor, torch.Tensor):
+                print(f"{display_name} not a torch tensor, value: {tensor}")
+                return
+
+            step_path = Path(step_folder)
+            if not step_path.exists():
+                step_path.mkdir(parents=True, exist_ok=False)
+            order_file_path = step_path / "order.txt"
+            tensor_file_path = step_path / output_file_name
 
-        with order_file_path.open(mode="a", encoding="utf-8") as f:
-            f.write(f"{output_file_name}\n")
+            with order_file_path.open(mode="a", encoding="utf-8") as f:
+                f.write(f"{output_file_name}\n")
 
-        with tensor_file_path.open(mode="w", encoding="utf-8") as f:
-            _summarize_tensor(display_name, tensor, f, depth, self._run_on_cpu, self._bucket_size)
+            with tensor_file_path.open(mode="w", encoding="utf-8") as f:
+                _summarize_tensor(display_name, tensor, f, depth, self._run_on_cpu, self._bucket_size)
 
 
 def _summarize_tensor(
     display_name: str,
     tensor: torch.Tensor,
     f: TextIOWrapper,
     depth: int = 0,
```

## onnxruntime/transformers/benchmark.py

```diff
@@ -32,14 +32,16 @@
             python benchmark.py -e torchscript -g
         Run TorchScript on GPU for all models with fp16:
             python benchmark.py -e torchscript -g -p "fp16"
         Run ONNXRuntime and TorchScript on CPU for all models with quantization:
             python benchmark.py -e torchscript onnxruntime -p "int8" -o
         Run OnnxRuntime with the ROCM provider and graph optimization script:
             python benchmark.py -g -m bert-base-cased --provider rocm --optimizer_info by_script --disable_embed_layer_norm
+        Run OnnxRuntime with bfloat16 fastmath mode kernels on aarch64 platforms with bfloat16 support:
+            python benchmark.py --enable_arm64_bfloat16_fastmath_mlas_gemm
 
     It is recommended to use run_benchmark.sh to launch benchmark.
 """
 
 import argparse
 import logging
 import os
@@ -102,14 +104,15 @@
     onnx_dir,
     verbose,
     overwrite,
     disable_ort_io_binding,
     use_raw_attention_mask,
     model_fusion_statistics,
     model_source,
+    enable_arm64_bfloat16_fastmath_mlas_gemm,
     args,
 ):
     import onnxruntime
 
     results = []
     if (
         use_gpu
@@ -205,14 +208,15 @@
             ort_session = create_onnxruntime_session(
                 onnx_model_file,
                 use_gpu,
                 provider,
                 enable_all_optimization=True,
                 num_threads=num_threads,
                 verbose=verbose,
+                enable_mlas_gemm_fastmath_arm64_bfloat16=enable_arm64_bfloat16_fastmath_mlas_gemm,
             )
             if ort_session is None:
                 continue
 
             ort_output_names = [node_arg.name for node_arg in ort_session.get_outputs()]
             output_buffers = []
             device = "cuda" if use_gpu else "cpu"
@@ -340,17 +344,15 @@
 
         if config.model_type in ["vit", "swin"]:
             # These models don't use sequence lengths, so just pick the first sequence length so that the summary still works
             sequence_lengths = [sequence_lengths[0]]
         else:
             tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
 
-            max_input_size = (
-                tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
-            )
+            max_input_size = tokenizer.max_model_input_sizes.get(model_name, 1024)
 
         logger.debug(f"Model {model}")
         logger.debug(f"Number of parameters {model.num_parameters()}")
 
         if precision == Precision.FLOAT16:
             model.half()
 
@@ -494,17 +496,15 @@
             cache_dir=cache_dir,
             custom_model_class=model_class,
             is_tf_model=True,
         )
 
         tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
 
-        max_input_size = (
-            tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
-        )
+        max_input_size = tokenizer.max_model_input_sizes.get(model_name, 1024)
 
         for batch_size in batch_sizes:
             if batch_size <= 0:
                 continue
 
             for sequence_length in sequence_lengths:
                 if max_input_size is not None and sequence_length > max_input_size:
@@ -760,14 +760,22 @@
         "--force_num_layers",
         required=False,
         type=int,
         default=None,
         help="Manually set the model's layer number",
     )
 
+    parser.add_argument(
+        "--enable_arm64_bfloat16_fastmath_mlas_gemm",
+        required=False,
+        action="store_true",
+        help="Enable bfloat16 mlas gemm kernels on aarch64. Supported only for CPU EP ",
+    )
+    parser.set_defaults(enable_arm64_bfloat16_fastmath_mlas_gemm=False)
+
     FusionOptions.add_arguments(parser)
 
     args = parser.parse_args()
     return args
 
 
 def main():
@@ -790,15 +798,15 @@
 
     logger.info(f"Arguments: {args}")
 
     if not os.path.exists(args.cache_dir):
         try:
             os.mkdir(args.cache_dir)
         except OSError:
-            logger.error("Creation of the directory %s failed" % args.cache_dir)
+            logger.error("Creation of the directory %s failed" % args.cache_dir)  # noqa: G002
 
     enable_torch = "torch" in args.engines
     enable_torch2 = "torch2" in args.engines
     enable_torchscript = "torchscript" in args.engines
     enable_onnxruntime = "onnxruntime" in args.engines
     enable_tensorflow = "tensorflow" in args.engines
 
@@ -905,18 +913,19 @@
                     args.onnx_dir,
                     args.verbose,
                     args.overwrite,
                     args.disable_ort_io_binding,
                     use_raw_attention_mask,
                     model_fusion_statistics,
                     args.model_source,
+                    args.enable_arm64_bfloat16_fastmath_mlas_gemm,
                     args,
                 )
             except Exception:
-                logger.error("Exception", exc_info=True)
+                logger.exception("Exception")
 
     time_stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
     if model_fusion_statistics:
         csv_filename = args.fusion_csv or f"benchmark_fusion_{time_stamp}.csv"
         output_fusion_statistics(model_fusion_statistics, csv_filename)
 
     if len(results) == 0:
```

## onnxruntime/transformers/benchmark_helper.py

```diff
@@ -81,14 +81,15 @@
     onnx_model_path,
     use_gpu,
     provider=None,
     enable_all_optimization=True,
     num_threads=-1,
     enable_profiling=False,
     verbose=False,
+    enable_mlas_gemm_fastmath_arm64_bfloat16=False,
     provider_options={},  # map execution provider name to its option  # noqa: B006
 ):
     session = None
     try:
         sess_options = onnxruntime.SessionOptions()
 
         if enable_all_optimization:
@@ -132,17 +133,20 @@
                 providers = ["CUDAExecutionProvider", "CPUExecutionProvider"]
         else:
             providers = ["CPUExecutionProvider"]
 
         if provider_options:
             providers = [(name, provider_options[name]) if name in provider_options else name for name in providers]
 
+        if enable_mlas_gemm_fastmath_arm64_bfloat16:
+            sess_options.add_session_config_entry("mlas.enable_gemm_fastmath_arm64_bfloat16", "1")
+
         session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=providers)
     except Exception:
-        logger.error("Exception", exc_info=True)
+        logger.error("Exception", exc_info=True)  # noqa: G201
 
     return session
 
 
 def setup_logger(verbose=True):
     if verbose:
         coloredlogs.install(
@@ -337,19 +341,15 @@
     result = {}
 
     # Bind inputs and outputs to onnxruntime session
     io_binding = ort_session.io_binding()
     # Bind inputs to device
     for name in ort_inputs:
         np_input = torch.from_numpy(ort_inputs[name]).to(device)
-        input_type = (
-            IO_BINDING_DATA_TYPE_MAP[str(ort_inputs[name].dtype)]
-            if str(ort_inputs[name].dtype) in IO_BINDING_DATA_TYPE_MAP
-            else data_type
-        )
+        input_type = IO_BINDING_DATA_TYPE_MAP.get(str(ort_inputs[name].dtype), data_type)
         io_binding.bind_input(
             name,
             np_input.device.type,
             0,
             input_type,
             np_input.shape,
             np_input.data_ptr(),
@@ -585,15 +585,15 @@
             finally:
                 monitor.keep_measuring = False
                 max_usage = mem_thread.result()
 
             if max_usage is None:
                 return None
 
-            print(f"GPU memory usage: before={memory_before_test}  peak={max_usage}")
+            logger.info(f"GPU memory usage: before={memory_before_test}  peak={max_usage}")
             if len(memory_before_test) >= 1 and len(max_usage) >= 1 and len(memory_before_test) == len(max_usage):
                 # When there are multiple GPUs, we will check the one with maximum usage.
                 max_used = 0
                 for i, memory_before in enumerate(memory_before_test):
                     before = memory_before["max_used_MB"]
                     after = max_usage[i]["max_used_MB"]
                     used = after - before
@@ -616,15 +616,15 @@
         try:
             fn_thread = executor.submit(func)
             _ = fn_thread.result()
         finally:
             monitor.keep_measuring = False
             max_usage = mem_thread.result()
 
-        print(f"CPU memory usage: before={memory_before_test:.1f} MB, peak={max_usage:.1f} MB")
+        logger.info(f"CPU memory usage: before={memory_before_test:.1f} MB, peak={max_usage:.1f} MB")
         return max_usage - memory_before_test
 
 
 def get_ort_environment_variables():
     # Environment variables might impact ORT performance on transformer models. Note that they are for testing only.
     env_names = [
         "ORT_DISABLE_FUSED_ATTENTION",
```

## onnxruntime/transformers/bert_perf_test.py

```diff
@@ -228,17 +228,17 @@
         latency_list.append(latency)
     return results, latency_list
 
 
 def to_string(model_path, session, test_setting):
     sess_options = session.get_session_options()
     option = f"model={os.path.basename(model_path)},"
-    option += "graph_optimization_level={},intra_op_num_threads={},".format(
-        sess_options.graph_optimization_level, sess_options.intra_op_num_threads
-    ).replace("GraphOptimizationLevel.ORT_", "")
+    option += f"graph_optimization_level={sess_options.graph_optimization_level},intra_op_num_threads={sess_options.intra_op_num_threads},".replace(
+        "GraphOptimizationLevel.ORT_", ""
+    )
 
     option += f"batch_size={test_setting.batch_size},sequence_length={test_setting.sequence_length},"
     option += f"test_cases={test_setting.test_cases},test_times={test_setting.test_times},"
     option += f"use_gpu={test_setting.use_gpu},use_io_binding={test_setting.use_io_binding},"
     option += f"average_sequence_length={test_setting.average_sequence_length},"
     option += f"random_sequence_length={test_setting.random_sequence_length}"
     return option
```

## onnxruntime/transformers/bert_test_data.py

```diff
@@ -170,20 +170,18 @@
         except OSError:
             print("Creation of the directory %s failed" % directory)
         else:
             print("Successfully created the directory %s " % directory)
     else:
         print("Warning: directory %s existed. Files will be overwritten." % directory)
 
-    index = 0
-    for name, data in inputs.items():
+    for index, (name, data) in enumerate(inputs.items()):
         tensor = numpy_helper.from_array(data, name)
         with open(os.path.join(directory, f"input_{index}.pb"), "wb") as file:
             file.write(tensor.SerializeToString())
-        index += 1
 
 
 def fake_test_data(
     batch_size: int,
     sequence_length: int,
     test_cases: int,
     dictionary_size: int,
```

## onnxruntime/transformers/compare_bert_results.py

```diff
@@ -55,24 +55,18 @@
 
                     if verbose:
                         print(f"case {test_case_id} output {i}")
                         print(f"baseline={results[i].tolist()}\ntreatment={treatment_output}")
                         print(f"abs_diff={abs_diff}")
 
     if diff_count == 0:
-        print(
-            "100% passed for {} random inputs given thresholds (rtol={}, atol={}).".format(
-                len(baseline_results), rtol, atol
-            )
-        )
+        print(f"100% passed for {len(baseline_results)} random inputs given thresholds (rtol={rtol}, atol={atol}).")
     else:
         print(
-            "WARNING: {} out of {} results NOT passed for thresholds (rtol={}, atol={}).".format(
-                diff_count, len(baseline_results), rtol, atol
-            )
+            f"WARNING: {diff_count} out of {len(baseline_results)} results NOT passed for thresholds (rtol={rtol}, atol={atol})."
         )
 
     print(f"maximum absolute difference={max_abs_diff}")
     return max_abs_diff, case_passed
 
 
 def run_test(
@@ -113,19 +107,15 @@
         mask_type,
     )
 
     baseline_results, baseline_latency, output_names = run_model(
         baseline_model, all_inputs, use_gpu, disable_optimization=True
     )
     if verbose:
-        print(
-            "baseline average latency (all optimizations disabled): {} ms".format(
-                statistics.mean(baseline_latency) * 1000
-            )
-        )
+        print(f"baseline average latency (all optimizations disabled): {statistics.mean(baseline_latency) * 1000} ms")
 
     if output_dir is not None:
         for i, inputs in enumerate(all_inputs):
             output_test_data(output_dir, i, inputs)
 
     treatment_results, treatment_latency, treatment_output_names = run_model(
         optimized_model, all_inputs, use_gpu, disable_optimization=False
```

## onnxruntime/transformers/convert_generation.py

```diff
@@ -51,32 +51,32 @@
 from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
 import onnx
 import torch
 from benchmark_helper import Precision, setup_logger
 from fusion_utils import NumpyHelper
-from models.gpt2.convert_to_onnx import main as convert_gpt2_to_onnx
-from models.gpt2.gpt2_helper import PRETRAINED_GPT2_MODELS
-from models.t5.convert_to_onnx import export_onnx_models as export_t5_onnx_models
-from models.t5.t5_helper import PRETRAINED_MT5_MODELS, PRETRAINED_T5_MODELS
 from onnx import GraphProto, ModelProto, TensorProto
 from onnx_model import OnnxModel
 from transformers import (
     GPT2Config,
     GPT2LMHeadModel,
     GPT2Tokenizer,
     MT5Config,
     MT5ForConditionalGeneration,
     T5Config,
     T5ForConditionalGeneration,
     T5Tokenizer,
 )
 
 from onnxruntime import GraphOptimizationLevel, InferenceSession, SessionOptions, get_available_providers
+from onnxruntime.transformers.models.gpt2.convert_to_onnx import main as convert_gpt2_to_onnx
+from onnxruntime.transformers.models.gpt2.gpt2_helper import PRETRAINED_GPT2_MODELS
+from onnxruntime.transformers.models.t5.convert_to_onnx import export_onnx_models as export_t5_onnx_models
+from onnxruntime.transformers.models.t5.t5_helper import PRETRAINED_MT5_MODELS, PRETRAINED_T5_MODELS
 
 logger = logging.getLogger("")
 
 
 class GenerationType(Enum):
     BEAMSEARCH = "beam_search"
     GREEDYSEARCH = "greedy_search"
@@ -368,15 +368,15 @@
     )
 
     beam_parameters_group.add_argument(
         "--min_tokens_to_keep",
         type=int,
         required=False,
         default=1,
-        help="Minimumber of tokens we keep per batch example in the output.",
+        help="Minimum number of tokens we keep per batch example in the output.",
     )
 
     beam_parameters_group.add_argument(
         "--presence_penalty",
         type=float,
         required=False,
         default=0.0,
@@ -462,15 +462,15 @@
         help="Number of times of inference for latency measurement",
     )
 
     test_group.add_argument(
         "--save_test_data",
         required=False,
         action="store_true",
-        help="save test data for onnxruntimer_perf_test tool",
+        help="save test data for onnxruntime_perf_test tool",
     )
     test_group.set_defaults(save_test_data=False)
 
     args = parser.parse_args(argv)
 
     return args
 
@@ -1221,15 +1221,15 @@
     return:
         tensor_names_to_rename : set of tensor names which is equal to past_sequence_length
         nodes_to_remove : list of node to remove
     """
     tensor_names_to_rename = set()
     nodes_to_remove = []
 
-    graph_intput_names = {inp.name: index for index, inp in enumerate(subg.input)}
+    graph_input_names = {inp.name: index for index, inp in enumerate(subg.input)}
 
     input_name_to_nodes = {}
     output_name_to_node = {}
     for node in subg.node:
         for input_name in node.input:
             if input_name:
                 if input_name not in input_name_to_nodes:
@@ -1255,15 +1255,15 @@
                 continue
             gather_indices_arr = onnx.numpy_helper.to_array(ini_gather_indices)
             if gather_indices_arr.size == 1 and gather_indices_arr.item() == 2 and node.input[0] in output_name_to_node:
                 shape_node = output_name_to_node[shape_tensor_name]
                 if (
                     shape_node.op_type == "Shape"
                     and shape_node.input[0]
-                    and shape_node.input[0] in graph_intput_names
+                    and shape_node.input[0] in graph_input_names
                     and (
                         shape_node.input[0].startswith("past_key_self_")
                         or shape_node.input[0].startswith("past_value_self_")
                     )
                 ):
                     tensor_names_to_rename.add(node.output[0])
                     nodes_to_remove.append(node)
@@ -1551,15 +1551,15 @@
 
     new_nodes = []
     old_nodes = []
     for node in subg.node:
         if node.op_type == "MultiHeadAttention":
             old_nodes.extend([node])
 
-    # If not all the MultiheadAttention nodes are fused, this optimization is not applicable
+    # If not all the MultiHeadAttention nodes are fused, this optimization is not applicable
     if len(old_nodes) < num_layers:
         return False
 
     # Redirect the RelativePositionBias node's input from past_key_self_0.shape[2] to past_sequence_length.
     # There is only one RelativePositionBias node in T5 decoder subgraph.
     rel_pos_bias_node = None
     for node in subg.node:
```

## onnxruntime/transformers/float16.py

```diff
@@ -170,14 +170,15 @@
     max_finite_val=65504.0,
     keep_io_types=False,
     disable_shape_infer=False,
     op_block_list=None,
     node_block_list=None,
     force_fp16_initializers=False,
     force_fp16_inputs=None,
+    use_bfloat16_as_blocked_nodes_dtype=False,
 ):
     """Convert tensor float type in the input ONNX model to tensor float16.
 
     Args:
         model (ModelProto or str): The ONNX model or path of the model to convert.
         min_positive_val (float, optional): minimal positive value. Defaults to 5.96e-08.
         max_finite_val (float, optional): maximal finite value of float16. Defaults to 65504.
@@ -406,17 +407,15 @@
     for value in fp32_initializers.values():
         # By default, to avoid precision loss, do not convert an initializer to fp16 when it is used only by fp32 nodes.
         if force_fp16_initializers or value.fp16_nodes:
             value.initializer = convert_tensor_float_to_float16(value.initializer, min_positive_val, max_finite_val)
             value_info_list.append(make_value_info_from_tensor(value.initializer))
             if value.fp32_nodes and not force_fp16_initializers:
                 logger.info(
-                    "initializer is used by both fp32 and fp16 nodes. Consider add these nodes to block list:{}".format(
-                        value.fp16_nodes
-                    )
+                    f"initializer is used by both fp32 and fp16 nodes. Consider add these nodes to block list:{value.fp16_nodes}"
                 )
 
     # Some operators have data type fixed as float for some input. Add a float16 to float cast for those inputs.
     for node in mixed_float_type_node_list:
         for i, input_name in enumerate(node.input):
             if i not in ALWAYS_FLOAT_INPUTS[node.op_type] or i in force_fp16_inputs_dict.get(node.op_type, []):
                 continue
@@ -432,32 +431,33 @@
                     node_name = node.name + "_input_cast" + str(i)
                     new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.input[i] = output_name
                     break
 
+    accuracy_type = TensorProto.BFLOAT16 if use_bfloat16_as_blocked_nodes_dtype else TensorProto.FLOAT
     # process the nodes in block list that doesn't support tensor(float16)
     for node in node_list:
         # if input's name is in the value_info_list meaning input is tensor(float16) type,
         # insert a float16 to float Cast node before the node,
         # change current node's input name and create new value_info for the new name
         for i in range(len(node.input)):
             input_name = node.input[i]
             for value_info in value_info_list:
                 if input_name == value_info.name:
                     # create new value_info for current node's new input name
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
                     output_name = node.name + "_input_cast_" + str(i)
                     new_value_info.name = output_name
-                    new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT
+                    new_value_info.type.tensor_type.elem_type = accuracy_type
                     # add Cast node (from tensor(float16) to tensor(float) before current node
                     node_name = node.name + "_input_cast" + str(i)
-                    new_node = [helper.make_node("Cast", [input_name], [output_name], to=1, name=node_name)]
+                    new_node = [helper.make_node("Cast", [input_name], [output_name], to=accuracy_type, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.input[i] = output_name
                     break
         # if output's name is in the value_info_list meaning output is tensor(float16) type, insert a float to
         # float16 Cast node after the node, change current node's output name and create new value_info for the new name
         for i in range(len(node.output)):
@@ -465,15 +465,15 @@
             for value_info in value_info_list:
                 if output == value_info.name:
                     # create new value_info for current node's new output
                     new_value_info = model.graph.value_info.add()
                     new_value_info.CopyFrom(value_info)
                     input_name = node.name + "_output_cast_" + str(i)
                     new_value_info.name = input_name
-                    new_value_info.type.tensor_type.elem_type = TensorProto.FLOAT
+                    new_value_info.type.tensor_type.elem_type = accuracy_type
                     # add Cast node (from tensor(float) to tensor(float16) after current node
                     node_name = node.name + "_output_cast" + str(i)
                     new_node = [helper.make_node("Cast", [input_name], [output], to=10, name=node_name)]
                     model.graph.node.extend(new_node)
                     # change current node's input name
                     node.output[i] = input_name
                     break
```

## onnxruntime/transformers/fusion_attention_unet.py

```diff
@@ -369,17 +369,15 @@
 
         counter_name = (
             "Attention (self attention)"
             if is_self_attention and not self.enable_packed_qkv
             else "MultiHeadAttention ({})".format(
                 "self attention with packed qkv"
                 if self.enable_packed_qkv
-                else "cross attention with packed kv"
-                if self.enable_packed_kv
-                else "cross attention"
+                else "cross attention with packed kv" if self.enable_packed_kv else "cross attention"
             )
         )
         self.increase_counter(counter_name)
         return attention_node
 
     def create_attention_node_lora(
         self,
@@ -839,17 +837,15 @@
 
         counter_name = (
             "Attention (self attention)"
             if is_self_attention and not self.enable_packed_qkv
             else "MultiHeadAttention ({})".format(
                 "self attention with packed qkv"
                 if self.enable_packed_qkv
-                else "cross attention with packed kv"
-                if self.enable_packed_kv
-                else "cross attention"
+                else "cross attention with packed kv" if self.enable_packed_kv else "cross attention"
             )
         )
         self.increase_counter(counter_name)
         return attention_node
 
     def fuse(self, normalize_node, input_name_to_nodes, output_name_to_node):
         if self.fuse_a1111_fp16(normalize_node, input_name_to_nodes, output_name_to_node):
```

## onnxruntime/transformers/fusion_embedlayer.py

```diff
@@ -341,26 +341,21 @@
             assert input_ids_shape and position_ids_shape
             if not (
                 len(input_ids_shape) == 2
                 and len(position_ids_shape) == 2
                 and input_ids_shape[1] == position_ids_shape[1]
             ):
                 logger.info(
-                    "Cannot fuse EmbedLayerNormalization: input_ids and position_ids not matched in 2nd dimension: {} vs {}".format(
-                        input_ids_shape, position_ids_shape
-                    )
+                    f"Cannot fuse EmbedLayerNormalization: input_ids and position_ids not matched in 2nd dimension: {input_ids_shape} vs {position_ids_shape}"
                 )
                 return False
 
             if segment_ids and not self.shape_infer.compare_shape(input_ids, segment_ids):
                 logger.info(
-                    "Cannot fuse EmbedLayerNormalization: input_ids and segment_ids does not have same shape: {} != {}".format(
-                        input_ids_shape,
-                        self.shape_infer.get_edge_shape(segment_ids),
-                    )
+                    f"Cannot fuse EmbedLayerNormalization: input_ids and segment_ids does not have same shape: {input_ids_shape} != {self.shape_infer.get_edge_shape(segment_ids)}"
                 )
                 return False
 
         word_embedding_table = self.model.get_constant_value(word_embedding_gather.input[0])
         if word_embedding_table is None or len(word_embedding_table.shape) != 2:
             logger.info("Cannot fuse EmbedLayerNormalization: word embedding table is not expected")
             return False
```

## onnxruntime/transformers/fusion_options.py

```diff
@@ -1,12 +1,13 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from argparse import ArgumentParser
+from enum import Enum
 
 
 class AttentionMaskFormat:
     # Build 1D mask indice (sequence length). It requires right side padding! Recommended for BERT model to get best performance.
     MaskIndexEnd = 0
 
     # For experiment only. Do not use it in production.
@@ -15,14 +16,31 @@
     # Raw attention mask with 0 means padding (or no attention) and 1 otherwise.
     AttentionMask = 2
 
     # No attention mask
     NoMask = 3
 
 
+class AttentionOpType(Enum):
+    Attention = "Attention"
+    MultiHeadAttention = "MultiHeadAttention"
+    GroupQueryAttention = "GroupQueryAttention"
+    PagedAttention = "PagedAttention"
+
+    def __str__(self):
+        return self.value
+
+    # Override __eq__ to return string comparison
+    def __hash__(self):
+        return hash(self.value)
+
+    def __eq__(self, other):
+        return other.value == self.value
+
+
 class FusionOptions:
     """Options of fusion in graph optimization"""
 
     def __init__(self, model_type):
         self.enable_gelu = True
         self.enable_layer_norm = True
         self.enable_attention = True
@@ -53,14 +71,16 @@
         # Note that embed layer normalization will convert 2D mask to 1D when mask type is MaskIndexEnd.
         self.attention_mask_format = AttentionMaskFormat.AttentionMask
         if model_type == "bert":
             self.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
         elif model_type == "vit":
             self.attention_mask_format = AttentionMaskFormat.NoMask
 
+        self.attention_op_type = None
+
         # options for stable diffusion
         if model_type in ["unet", "vae", "clip"]:
             self.enable_nhwc_conv = True
             self.enable_group_norm = True
             self.enable_skip_group_norm = True
             self.enable_bias_splitgelu = True
             self.enable_packed_qkv = True
@@ -72,14 +92,17 @@
             self.attention_mask_format = AttentionMaskFormat.AttentionMask
         else:
             self.attention_mask_format = AttentionMaskFormat.MaskIndexEnd
 
     def disable_attention_mask(self):
         self.attention_mask_format = AttentionMaskFormat.NoMask
 
+    def set_attention_op_type(self, attn_op_type: AttentionOpType):
+        self.attention_op_type = attn_op_type
+
     @staticmethod
     def parse(args):
         options = FusionOptions(args.model_type)
         if args.disable_gelu:
             options.enable_gelu = False
         if args.disable_layer_norm:
             options.enable_layer_norm = False
```

## onnxruntime/transformers/fusion_qordered_gelu.py

```diff
@@ -71,17 +71,19 @@
 
         # Fusion logic
         subgraph_nodes = [node]  # Gelu/FastGelu
         subgraph_nodes.extend([downstream_quantize_node, upstream_dequantize_node])  # Relevant Q, DQ nodes
 
         if not self.model.is_safe_to_fuse_nodes(
             subgraph_nodes,
-            [node.output[0], downstream_quantize_node.output[0]]
-            if downstream_shape_node is not None
-            else downstream_quantize_node.output,
+            (
+                [node.output[0], downstream_quantize_node.output[0]]
+                if downstream_shape_node is not None
+                else downstream_quantize_node.output
+            ),
             input_name_to_nodes,
             output_name_to_node,
         ):
             logger.debug("It is not safe to fuse QOrderedGelu node. Skip")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
```

## onnxruntime/transformers/fusion_qordered_layernorm.py

```diff
@@ -73,17 +73,19 @@
         # In GPT2, the DQ node will be feeding a residual downstream Add and hence,
         # we do not want to remove it
         if len(upstream_dequantize_node_children) == 1:
             subgraph_nodes.extend([upstream_dequantize_node])  # DQ node before LayerNormalization
 
         if not self.model.is_safe_to_fuse_nodes(
             subgraph_nodes,
-            [node.output[0], downstream_quantize_node.output[0]]
-            if downstream_shape_node is not None
-            else downstream_quantize_node.output,
+            (
+                [node.output[0], downstream_quantize_node.output[0]]
+                if downstream_shape_node is not None
+                else downstream_quantize_node.output
+            ),
             input_name_to_nodes,
             output_name_to_node,
         ):
             logger.debug("It is not safe to fuse QOrderedLayerNormalization node. Skip")
             return
 
         self.nodes_to_remove.extend(subgraph_nodes)
```

## onnxruntime/transformers/fusion_rotary_attention.py

```diff
@@ -358,16 +358,18 @@
         else:
             logger.debug("fuse_rotary_attention: failed to match qkv nodes")
             return
 
         # v_nodes_1 is for LLaMA-2 Microsoft
         # v_nodes_3 is for LLaMA-2 Hugging Face
         # v_nodes_4 is for LLaMA-2 70B model
+        # v_nodes_5 is for Phi-2 DirectML
         past_v, present_v, past_seq_len = "", "", ""
         v_nodes = None
+        add_v = None
         v_nodes_1 = self.model.match_parent_path(
             matmul_qkv,
             ["Reshape", "Transpose", "Concat", "Transpose", "Reshape", "MatMul"],
             [1, 0, 0, 1, 0, 0],
         )
         v_nodes_2 = self.model.match_parent_path(
             matmul_qkv,
@@ -487,14 +489,19 @@
                 (
                     ["Reshape", "Concat", "Unsqueeze", "Gather", "Shape", "Concat", "Transpose", "Reshape", "MatMul"],
                     [1, 1, 3, 0, 0, 0, 1, 0, 0],
                 ),
             ],
             output_name_to_node=None,
         )
+        v_nodes_5 = self.model.match_parent_path(
+            matmul_qkv,
+            ["Concat", "Transpose", "Reshape", "Add", "MatMul"],
+            [1, 1, 0, 0, 1],
+        )
         if v_nodes_1 is not None:
             reshape_v_2, _, concat_v, _, reshape_v_1, matmul_v = v_nodes_1
             v_nodes = v_nodes_1
 
             concat_v_path = self.model.match_parent_path(
                 concat_v,
                 ["Slice", "Unsqueeze"],
@@ -517,14 +524,20 @@
             v_nodes = v_nodes_3
             present_v = transpose_v.output[0]
         elif v_nodes_4 is not None and len(v_nodes_4) == 9:
             concat_v, transpose_v, reshape_v, matmul_v = v_nodes_4[0][-4:]
             v_nodes = v_nodes_4
             past_v = concat_v.input[0]
             present_v = concat_v.output[0]
+        elif v_nodes_5 is not None:
+            concat_v, transpose_v, reshape_v, add_v, matmul_v = v_nodes_5
+            matmul_v = add_v
+            v_nodes = v_nodes_5
+            past_v = concat_v.input[0]
+            present_v = concat_v.output[0]
         else:
             logger.debug("fuse_rotary_attention: failed to match v path")
             return
 
         qk_nodes = self.model.match_parent_path(
             matmul_qkv,
             ["Softmax", "Add", "Div", "MatMul"],
@@ -603,14 +616,16 @@
             return
 
         # k_nodes_1 is for LLaMA-2 Microsoft
         # k_nodes_2 is for LLaMA-2 Hugging Face
         # k_nodes_4 is for LLaMA-2 70B Hugging Face
         past_k, present_k = "", ""
         k_nodes = None
+        slice_k = None
+        concat_k_half = None
         k_nodes_1 = self.model.match_parent_path(
             matmul_qk,
             ["Reshape", "Transpose", "Concat", "Transpose", "RotaryEmbedding", "MatMul"],
             [1, 0, 0, 1, 0, 0],
         )
         k_nodes_2 = self.model.match_parent_path(
             matmul_qk,
@@ -786,14 +801,19 @@
                         "MatMul",
                     ],
                     [1, 0, 1, 3, 0, 0, 0, 1, 0, 0, 0],
                 ),
             ],
             output_name_to_node=None,
         )
+        k_nodes_5 = self.model.match_parent_path(
+            matmul_qk,
+            ["Transpose", "Concat", "Concat", "RotaryEmbedding", "Slice", "Transpose", "Reshape", "Add", "MatMul"],
+            [1, 0, 1, 0, 0, 0, 0, 0, 1],
+        )
         if k_nodes_1 is not None:
             reshape_k_2, _, concat_k, _, rotary_k, matmul_k = k_nodes_1
             k_nodes = k_nodes_1
 
             concat_k_path = self.model.match_parent_path(
                 concat_k,
                 ["Slice", "Unsqueeze"],
@@ -819,37 +839,53 @@
             present_k = concat_k.output[0]
         elif k_nodes_4 is not None and len(k_nodes_4) == 9:
             reshape_k, matmul_k = k_nodes_4[0][-2:]
             concat_k, rotary_k = k_nodes_4[0][-5:-3]
             k_nodes = k_nodes_4
             past_k = concat_k.input[0]
             present_k = concat_k.output[0]
+        elif k_nodes_5 is not None:
+            _, concat_k, concat_k_half, rotary_k, slice_k, _, reshape_k, _, matmul_k = k_nodes_5
+            k_nodes = k_nodes_5
+            past_k = concat_k.input[0]
+            present_k = concat_k.output[0]
         else:
             logger.debug("fuse_rotary_attention: failed to match k nodes")
             return
 
         # q_nodes_1 is for LLaMA-2 Microsoft
         # q_nodes_2 is for LLaMA-2 Hugging Face
+        # q_nodes_3 is for Phi-2 DirectML
         q_nodes = None
+        slice_q = None
+        concat_q_half = None
         q_nodes_1 = self.model.match_parent_path(
             matmul_qk,
             ["Reshape", "Transpose", "RotaryEmbedding", "MatMul"],
             [0, 0, 0, 0],
         )
         q_nodes_2 = self.model.match_parent_path(
             matmul_qk,
             ["RotaryEmbedding", "Transpose", "Reshape", "MatMul"],
             [0, 0, 0, 0],
         )
+        q_nodes_3 = self.model.match_parent_path(
+            matmul_qk,
+            ["Concat", "RotaryEmbedding", "Slice", "Transpose", "Reshape", "Add", "MatMul"],
+            [0, 0, 0, 0, 0, 0, 1],
+        )
         if q_nodes_1 is not None:
             reshape_q_2, _, rotary_q, matmul_q = q_nodes_1
             q_nodes = q_nodes_1
         elif q_nodes_2 is not None:
             rotary_q, _, reshape_q, matmul_q = q_nodes_2
             q_nodes = q_nodes_2
+        elif q_nodes_3 is not None:
+            concat_q_half, rotary_q, slice_q, _, reshape_q, _, matmul_q = q_nodes_3
+            q_nodes = q_nodes_3
         else:
             logger.debug("fuse_rotary_attention: failed to match q nodes")
             return
 
         if matmul_q.input[0] != matmul_k.input[0] and matmul_k.input[0] != matmul_v.input[0]:
             logger.debug("fuse_rotary_attention: failed to find the same root_input for q, k, v paths")
             return
@@ -881,23 +917,140 @@
                 logger.debug("fuse_rotary_attention: failed to verify runtime shape paths")
                 return
             root_output = reshape_qkv.output[0]
 
             # Rename inputs of rotary_q/k so it connects with output of matmul_q/k
             # Before: MatMul --> Reshape --> Transpose --> RotaryEmbedding
             # After: MatMul --> RotaryEmbedding
-            rotary_q.input[0] = matmul_q.output[0]
-            rotary_k.input[0] = matmul_k.output[0]
+            rotary_q.input[0] = slice_q.output[0] if slice_q else matmul_q.output[0]
+            rotary_k.input[0] = slice_k.output[0] if slice_k else matmul_k.output[0]
 
             # Rename current output of rotary_k (present_key) so it doesn't match output of MHA (present_key)
-            rotary_k.output[0] = rotary_k.name + "_output_0"
+            if concat_q_half is None:
+                rotary_k.output[0] = rotary_k.name + "_output_0"
 
             if qkv_nodes == qkv_nodes_3:
                 qkv_nodes = qkv_nodes[1:]
 
+        def create_hidden_size_concat_node(reshape_q):
+            """Detect num_heads and hidden_size for ONNX model from phi-2
+            Args:
+                reshape_q (NodeProto): reshape node for q
+            Returns:
+                hidden_size_concat_node(NodeProto): Concat node to be used by reshape
+            """
+            concat = self.model.match_parent(reshape_q, "Concat", 1)
+
+            if concat is None:
+                logger.debug("fuse_rotary_attention: failed to trace the concat node from reshape_q")
+                return None
+
+            # The shape is a tensor like [?, ?, num_heads, head_size]
+            num_head_constant_node = self.model.get_constant_value(concat.input[2])
+            head_size_constant_node = self.model.get_constant_value(concat.input[3])
+
+            if num_head_constant_node is None or head_size_constant_node is None:
+                logger.debug("fuse_rotary_attention: failed to get constant nodes of num_heads or head_size")
+                return None
+
+            num_head_value = num_head_constant_node[0]
+            head_size_value = head_size_constant_node[0]
+
+            hidden_size = num_head_value * head_size_value
+
+            hidden_size_initilizer = self.model.create_node_name("Initializer", name_prefix="hidden_size")
+            if self.model.get_initializer(hidden_size_initilizer) is None:
+                self.add_initializer(
+                    name=hidden_size_initilizer,
+                    data_type=TensorProto.INT64,
+                    dims=[1],
+                    vals=[hidden_size],
+                    raw=False,
+                )
+
+            hidden_size_reshape_node_name = self.model.create_node_name("Concat", name_prefix="hidden_size_concat")
+
+            hidden_size_concat_node = helper.make_node(
+                "Concat",
+                inputs=[
+                    concat.input[0],
+                    concat.input[1],
+                    hidden_size_initilizer,
+                ],
+                outputs=[hidden_size_reshape_node_name + "output_0"],
+                name=hidden_size_reshape_node_name,
+            )
+            hidden_size_concat_node.attribute.extend([helper.make_attribute("axis", 0)])
+
+            return hidden_size_concat_node
+
+        # Add Tranpose and Reshape nodes for patial rotary embedding applied in phi-2 before passing into MHA
+        if concat_q_half and concat_k_half:
+            # Transpose the key output of rotary Embedding
+            k_transpose_node_name = self.model.create_node_name("Transpose")
+            k_tranpose_output_name = k_transpose_node_name + "_output_0"
+            k_transpose_node = helper.make_node(
+                "Transpose",
+                inputs=[concat_k_half.output[0]],
+                outputs=[k_tranpose_output_name],
+                name=k_transpose_node_name,
+            )
+
+            k_transpose_node.attribute.extend([helper.make_attribute("perm", [0, 2, 1, 3])])
+
+            # Transpose the query output of rotary Embedding
+            q_transpose_node_name = self.model.create_node_name("Transpose")
+            q_tranpose_output_name = q_transpose_node_name + "_output_0"
+            q_transpose_node = helper.make_node(
+                "Transpose",
+                inputs=[concat_q_half.output[0]],
+                outputs=[q_tranpose_output_name],
+                name=q_transpose_node_name,
+            )
+
+            q_transpose_node.attribute.extend([helper.make_attribute("perm", [0, 2, 1, 3])])
+
+            hidden_size_concat_node = create_hidden_size_concat_node(reshape_k)
+            if hidden_size_concat_node is None:
+                logger.debug("fuse_rotary_attention: failed to create hidden_size_concat_node")
+                return
+
+            # Reshape the Rotary Embedding output for key for 4D to 3D
+            concat_k_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="concat_k_half")
+            concat_k_reshape_node = helper.make_node(
+                "Reshape",
+                inputs=[k_transpose_node.output[0], hidden_size_concat_node.output[0]],
+                outputs=[concat_k_reshape_node_name + "_output_0"],
+                name=concat_k_reshape_node_name,
+            )
+
+            # Reshape the Rotary Embedding output for query from 4D to 3D
+            concat_q_reshape_node_name = self.model.create_node_name("Reshape", name_prefix="concat_q_half")
+            concat_q_reshape_node = helper.make_node(
+                "Reshape",
+                inputs=[q_transpose_node.output[0], hidden_size_concat_node.output[0]],
+                outputs=[concat_q_reshape_node_name + "_output_0"],
+                name=concat_q_reshape_node_name,
+            )
+
+            rotary_k = concat_k_reshape_node
+            rotary_q = concat_q_reshape_node
+
+            self.nodes_to_add.append(hidden_size_concat_node)
+            self.nodes_to_add.append(k_transpose_node)
+            self.nodes_to_add.append(q_transpose_node)
+            self.nodes_to_add.append(concat_k_reshape_node)
+            self.nodes_to_add.append(concat_q_reshape_node)
+
+            self.node_name_to_graph_name[hidden_size_concat_node.name] = self.this_graph_name
+            self.node_name_to_graph_name[k_transpose_node.name] = self.this_graph_name
+            self.node_name_to_graph_name[q_transpose_node.name] = self.this_graph_name
+            self.node_name_to_graph_name[concat_k_reshape_node.name] = self.this_graph_name
+            self.node_name_to_graph_name[concat_q_reshape_node.name] = self.this_graph_name
+
         new_node = self.create_mha_node(
             matmul_q.input[0],
             root_output,
             rotary_q,
             rotary_k,
             matmul_v,
             attn_mask,
@@ -913,15 +1066,15 @@
 
         self.nodes_to_add.append(new_node)
         self.node_name_to_graph_name[new_node.name] = self.this_graph_name
 
         self.nodes_to_remove.extend(qkv_nodes[1:])
 
         if v_nodes != v_nodes_4:
-            self.nodes_to_remove.extend(v_nodes[:-1])
+            self.nodes_to_remove.extend(v_nodes[:-1] if add_v is None else v_nodes[:-2])
         else:
             nodes_to_keep = [v_nodes[0][-1]]
             for temp_path in v_nodes:
                 self.add_nodes_to_remove_with_nodes_to_keep(temp_path, nodes_to_keep)
 
         self.nodes_to_remove.extend(qk_nodes)
 
@@ -932,25 +1085,27 @@
             self.nodes_to_remove.append(k_nodes[2])
             self.nodes_to_remove.append(k_nodes[3])
         elif k_nodes == k_nodes_3:
             self.nodes_to_remove.append(k_nodes[0])
             self.nodes_to_remove.append(k_nodes[1])
             self.nodes_to_remove.append(k_nodes[3])
             self.nodes_to_remove.append(k_nodes[4])
+        elif k_nodes == k_nodes_5:
+            self.nodes_to_remove.append(k_nodes[0])
+            self.nodes_to_remove.append(k_nodes[1])
         elif k_nodes == k_nodes_4:
             nodes_to_keep = [k_nodes[0][-1], k_nodes[0][-4]]
             for temp_path in k_nodes:
                 self.add_nodes_to_remove_with_nodes_to_keep(temp_path, nodes_to_keep)
 
         if q_nodes == q_nodes_1:
             self.nodes_to_remove.extend(q_nodes[:-2])
         elif q_nodes == q_nodes_2:
             self.nodes_to_remove.append(q_nodes[1])
             self.nodes_to_remove.append(q_nodes[2])
-
         self.prune_graph = True
 
 
 class FusionRotaryEmbeddings(Fusion):
     def __init__(self, model: OnnxModel):
         self.base_name = "RotaryEmbedding"
         super().__init__(model, self.base_name, [self.base_name, self.base_name + ".1", "Add"])
@@ -1163,57 +1318,102 @@
             #     sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
             #     cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
             #     sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
             #     x_embed = (x * cos) + (rotate_half(x) * sin)
             #     return x_embed
 
             # Check paths for rotate_half(x)
-            rotate_half_x2_path_1 = self.model.match_parent_path(
+            rotate_half_x2_path_1_1 = self.model.match_parent_path(
                 node,
                 ["Mul", "Concat", "Neg", "Slice", "Transpose"],
                 [1, 0, 0, 0, 0],
             )
-            rotate_half_x2_path_2 = self.model.match_parent_path(
+
+            rotate_half_x2_path_1_2 = self.model.match_parent_path(
+                node,
+                ["Mul", "Concat", "Neg", "Slice", "Slice"],
+                [1, 0, 0, 0, 0],
+            )
+
+            rotate_half_x2_path_1 = rotate_half_x2_path_1_1 or rotate_half_x2_path_1_2
+
+            rotate_half_x2_path_2_1 = self.model.match_parent_path(
                 node,
                 ["Mul", "Concat", "Neg", "Slice", "Unsqueeze", "Div", "Gather", "Shape", "Transpose"],
                 [1, 0, 0, 0, 1, 0, 0, 0, 0],
             )
+
+            rotate_half_x2_path_2_2 = self.model.match_parent_path(
+                node,
+                ["Mul", "Concat", "Neg", "Slice", "Unsqueeze", "Div", "Gather", "Shape", "Slice"],
+                [1, 0, 0, 0, 1, 0, 0, 0, 0],
+            )
+
+            rotate_half_x2_path_2 = rotate_half_x2_path_2_1 or rotate_half_x2_path_2_2
+
             if rotate_half_x2_path_1 is None or rotate_half_x2_path_2 is None:
                 logger.debug("fuse_rotary_embeddings: failed to match x2 in rotate_half")
                 return
 
-            rotate_half_x1_path_1 = self.model.match_parent_path(
+            rotate_half_x1_path_1_1 = self.model.match_parent_path(
                 node,
                 ["Mul", "Concat", "Slice", "Transpose"],
                 [1, 0, 1, 0],
             )
-            rotate_half_x1_path_2 = self.model.match_parent_path(
+
+            rotate_half_x1_path_1_2 = self.model.match_parent_path(
+                node,
+                ["Mul", "Concat", "Slice", "Slice"],
+                [1, 0, 1, 0],
+            )
+
+            rotate_half_x1_path_1 = rotate_half_x1_path_1_1 or rotate_half_x1_path_1_2
+
+            rotate_half_x1_path_2_1 = self.model.match_parent_path(
                 node,
                 ["Mul", "Concat", "Slice", "Unsqueeze", "Div", "Gather", "Shape", "Transpose"],
                 [1, 0, 1, 2, 0, 0, 0, 0],
             )
+
+            rotate_half_x1_path_2_2 = self.model.match_parent_path(
+                node,
+                ["Mul", "Concat", "Slice", "Unsqueeze", "Div", "Gather", "Shape", "Slice"],
+                [1, 0, 1, 2, 0, 0, 0, 0],
+            )
+
+            rotate_half_x1_path_2 = rotate_half_x1_path_2_1 or rotate_half_x1_path_2_2
+
             if rotate_half_x1_path_1 is None or rotate_half_x1_path_2 is None:
                 logger.debug("fuse_rotary_embeddings: failed to match x1 in rotate_half")
                 return
 
             if (
                 rotate_half_x1_path_1[-1].name != rotate_half_x1_path_2[-1].name
                 or rotate_half_x2_path_1[-1].name != rotate_half_x2_path_2[-1].name
                 or rotate_half_x1_path_1[-1].name != rotate_half_x2_path_1[-1].name
                 or rotate_half_x1_path_2[-1].name != rotate_half_x2_path_2[-1].name
             ):
                 logger.debug("fuse_rotary_embeddings: failed to match common input in rotate_half")
                 return
 
             # Check path for x
-            x_path = self.model.match_parent_path(
+            x_path_1 = self.model.match_parent_path(
                 node,
                 ["Mul", "Transpose"],
                 [0, 0],
             )
+
+            x_path_2 = self.model.match_parent_path(
+                node,
+                ["Mul", "Slice"],
+                [0, 0],
+            )
+
+            x_path = x_path_1 or x_path_2
+
             if x_path is None:
                 logger.debug("fuse_rotary_embeddings: failed to match x in rotate_half")
                 return
 
             # Check path for sin
             sin_path, sin_cache, position_ids = None, "", ""
             sin_path_1 = self.model.match_parent_path(
```

## onnxruntime/transformers/fusion_skip_group_norm.py

```diff
@@ -143,15 +143,15 @@
         reshape.input[0] = matmul.output[0]
         self.remove_if_safe(add_bias, input_name_to_nodes)
 
         return bias
 
     def match_transpose_from_nhwc(self, output_name, input_name_to_nodes, output_name_to_node):
         """Match whether an output is from a Transpose(perm=[0,3,1,2]) node."""
-        parent = output_name_to_node[output_name] if output_name in output_name_to_node else None
+        parent = output_name_to_node.get(output_name, None)
         if parent is not None and parent.op_type == "Transpose":
             permutation = OnnxModel.get_node_attribute(parent, "perm")
             if permutation == [0, 3, 1, 2]:
                 self.remove_if_safe(parent, input_name_to_nodes)
                 return parent
         return None
```

## onnxruntime/transformers/io_binding_helper.py

```diff
@@ -1,15 +1,16 @@
+import copy
 import logging
 from collections import OrderedDict
-from typing import Any, Dict, List, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple, Union
 
 import numpy
 import torch
 
-from onnxruntime import InferenceSession
+from onnxruntime import InferenceSession, RunOptions
 
 logger = logging.getLogger(__name__)
 
 
 class TypeHelper:
     @staticmethod
     def get_input_type(ort_session: InferenceSession, name: str) -> str:
@@ -219,19 +220,51 @@
         self.io_binding = self.ort_session.io_binding()
         self.enable_cuda_graph = enable_cuda_graph
 
         self.input_tensors = OrderedDict()
         self.output_tensors = OrderedDict()
         self.device = device
 
+        # Pairs of input and output names that share the same buffer.
+        self.buffer_sharing: Dict[str, str] = {}
+
+    def set_buffer_sharing(self, input_name: str, output_name: str):
+        assert input_name in self.input_names
+        assert output_name in self.output_names
+        self.buffer_sharing[input_name] = output_name
+        self.buffer_sharing[output_name] = input_name
+
     def __del__(self):
         del self.input_tensors
         del self.output_tensors
         del self.io_binding
-        del self.ort_session
+
+    def bind_input_and_buffer_sharing(self, name: str, tensor: torch.Tensor):
+        device_id = tensor.device.index if tensor.device.index is not None else 0
+        tensor_shape = [1] if len(tensor.shape) == 0 else list(tensor.shape)
+
+        self.io_binding.bind_input(
+            name,
+            tensor.device.type,
+            device_id,
+            self.io_name_to_numpy_type[name],
+            tensor_shape,
+            tensor.data_ptr(),
+        )
+
+        if name in self.buffer_sharing:
+            self.io_binding.bind_output(
+                self.buffer_sharing[name],
+                tensor.device.type,
+                device_id,
+                self.io_name_to_numpy_type[name],
+                tensor_shape,
+                tensor.data_ptr(),
+            )
+            self.output_tensors[self.buffer_sharing[name]] = tensor
 
     def allocate_buffers(self, shape_dict: Dict[str, Union[Tuple[int], List[int]]]):
         """Allocate tensors for I/O Binding"""
         if self.enable_cuda_graph:
             for name, shape in shape_dict.items():
                 if name in self.input_names:
                     # Reuse allocated buffer when the shape is same
@@ -241,78 +274,167 @@
                         raise RuntimeError("Expect static input shape for cuda graph")
 
                     numpy_dtype = self.io_name_to_numpy_type[name]
                     tensor = torch.empty(tuple(shape), dtype=TypeHelper.numpy_type_to_torch_type(numpy_dtype)).to(
                         device=self.device
                     )
                     self.input_tensors[name] = tensor
-
-                    self.io_binding.bind_input(
-                        name,
-                        tensor.device.type,
-                        tensor.device.index,
-                        numpy_dtype,
-                        list(tensor.size()),
-                        tensor.data_ptr(),
-                    )
+                    self.bind_input_and_buffer_sharing(name, tensor)
 
         for name, shape in shape_dict.items():
             if name in self.output_names:
                 # Reuse allocated buffer when the shape is same
                 if name in self.output_tensors and tuple(self.output_tensors[name].shape) == tuple(shape):
                     continue
 
+                if name in self.buffer_sharing:
+                    continue
+
                 numpy_dtype = self.io_name_to_numpy_type[name]
                 tensor = torch.empty(tuple(shape), dtype=TypeHelper.numpy_type_to_torch_type(numpy_dtype)).to(
                     device=self.device
                 )
                 self.output_tensors[name] = tensor
 
                 self.io_binding.bind_output(
                     name,
                     tensor.device.type,
-                    tensor.device.index,
+                    tensor.device.index if tensor.device.index is not None else 0,
                     numpy_dtype,
                     list(tensor.size()),
                     tensor.data_ptr(),
                 )
 
-    def infer(self, feed_dict: Dict[str, torch.Tensor]):
+    def infer(self, feed_dict: Dict[str, torch.Tensor], run_options: RunOptions = None, synchronize: bool = False):
         """Bind input tensors and run inference"""
         for name, tensor in feed_dict.items():
             assert isinstance(tensor, torch.Tensor) and tensor.is_contiguous()
             if name in self.input_names:
                 if self.enable_cuda_graph:
                     assert self.input_tensors[name].nelement() == tensor.nelement()
                     assert self.input_tensors[name].dtype == tensor.dtype
                     assert tensor.device.type == "cuda"
-                    # Please install cuda-python package with a version corresponding to CUDA in your machine.
-                    from cuda import cudart
-
-                    # Update input tensor inplace since cuda graph requires input and output has fixed memory address.
-                    cudart.cudaMemcpy(
-                        self.input_tensors[name].data_ptr(),
-                        tensor.data_ptr(),
-                        tensor.element_size() * tensor.nelement(),
-                        cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice,
-                    )
+                    self.input_tensors[name].copy_(tensor)
                 else:
-                    self.io_binding.bind_input(
-                        name,
-                        tensor.device.type,
-                        tensor.device.index,
-                        TypeHelper.torch_type_to_numpy_type(tensor.dtype),
-                        [1] if len(tensor.shape) == 0 else list(tensor.shape),
-                        tensor.data_ptr(),
-                    )
+                    self.bind_input_and_buffer_sharing(name, tensor)
 
-        self.ort_session.run_with_iobinding(self.io_binding)
+        # Synchronization are not needed in most cases unless different streams are used or inputs/outputs are in CPU.
+        if synchronize:
+            self.io_binding.synchronize_inputs()
+            self.ort_session.run_with_iobinding(self.io_binding, run_options)
+            self.io_binding.synchronize_outputs()
+        else:
+            self.ort_session.run_with_iobinding(self.io_binding, run_options)
 
         return self.output_tensors
 
     @staticmethod
-    def get_cuda_provider_options(device_id: int, enable_cuda_graph: bool) -> Dict[str, Any]:
-        return {
+    def get_cuda_provider_options(device_id: int, enable_cuda_graph: bool, stream: int = 0) -> Dict[str, Any]:
+        options = {
             "device_id": device_id,
             "arena_extend_strategy": "kSameAsRequested",
             "enable_cuda_graph": enable_cuda_graph,
         }
+
+        # Stream is address of a CUDA stream. 0 means the default stream.
+        if stream != 0:
+            options["user_compute_stream"] = str(stream)
+
+        return options
+
+
+class GpuBinding(CudaSession):
+    def __init__(
+        self,
+        ort_session: InferenceSession,
+        device: torch.device,
+        shape_dict: Dict[str, Union[Tuple[int], List[int]]],
+        enable_gpu_graph: bool = False,
+        gpu_graph_id: int = -1,
+        stream: int = 0,
+        buffer_sharing: Optional[Dict[str, str]] = None,
+    ):
+        super().__init__(ort_session, device, enable_gpu_graph)
+        if buffer_sharing:
+            for input_name, output_name in buffer_sharing.items():
+                self.set_buffer_sharing(input_name, output_name)
+
+        self.allocate_buffers(shape_dict)
+        self.gpu_graph_id = gpu_graph_id
+        # For cuda graph, we need to keep a copy of shape_dict to check if the shape is same in inference later.
+        self.shape_dict = copy.deepcopy(shape_dict) if enable_gpu_graph else None
+        self.stream = stream
+        # The gpu graph id of last run. It will be saved to image metadata.
+        self.last_run_gpu_graph_id = None
+
+    def get_run_options(self, disable_cuda_graph_in_run: bool = False) -> RunOptions:
+        options = RunOptions()
+
+        gpu_graph_id = -1 if disable_cuda_graph_in_run else self.gpu_graph_id
+
+        options.add_run_config_entry("gpu_graph_id", str(gpu_graph_id))
+
+        self.last_run_gpu_graph_id = gpu_graph_id
+
+        return options
+
+    def infer(self, feed_dict: Dict[str, torch.Tensor], disable_cuda_graph_in_run: bool = False):
+        run_options = self.get_run_options(disable_cuda_graph_in_run)
+
+        if self.stream:
+            run_options.add_run_config_entry("disable_synchronize_execution_providers", "1")
+
+        return super().infer(feed_dict, run_options)
+
+
+class GpuBindingManager:
+    """A manager for I/O bindings that support multiple CUDA Graphs.
+    One cuda graph is reused for same input shape. Automatically add a new cuda graph for new input shape.
+    """
+
+    def __init__(self, ort_session: InferenceSession, device: torch.device, stream: int = 0, max_cuda_graphs: int = 1):
+        self.ort_session = ort_session
+        self.device = device
+
+        # Binding supports cuda graphs. For a binding, it is able to disable cuda graph for a specific run.
+        self.graph_bindings = []
+
+        # Binding for not using cuda graph.
+        self.no_graph_binding = None
+
+        self.stream = stream
+
+        self.max_cuda_graphs = max_cuda_graphs
+
+    def get_binding(
+        self,
+        shape_dict: Dict[str, Union[Tuple[int], List[int]]],
+        use_cuda_graph: bool = False,
+        buffer_sharing: Optional[Dict[str, str]] = None,
+    ) -> GpuBinding:
+        for gpu_graph_binding in self.graph_bindings:
+            # Found a cuda graph that captured with the same shape
+            if gpu_graph_binding.shape_dict == shape_dict:
+                return gpu_graph_binding
+
+        # Reached the maximum number of cuda graphs. Return a binding without cuda graph.
+        if len(self.graph_bindings) >= self.max_cuda_graphs or (not use_cuda_graph):
+            if self.no_graph_binding is None:
+                self.no_graph_binding = GpuBinding(
+                    self.ort_session, self.device, shape_dict, stream=self.stream, buffer_sharing=buffer_sharing
+                )
+            else:
+                self.no_graph_binding.allocate_buffers(shape_dict)
+            return self.no_graph_binding
+
+        # This is a new input shape, create a new cuda graph
+        gpu_graph_binding = GpuBinding(
+            self.ort_session,
+            self.device,
+            shape_dict,
+            enable_gpu_graph=True,
+            gpu_graph_id=len(self.graph_bindings),
+            stream=self.stream,
+            buffer_sharing=buffer_sharing,
+        )
+        self.graph_bindings.append(gpu_graph_binding)
+        return gpu_graph_binding
```

## onnxruntime/transformers/large_model_exporter.py

```diff
@@ -249,16 +249,14 @@
             onnx_dynamic_axes[onnx_inp_names[-1]] = kv_cache_axis
             onnx_dynamic_axes[onnx_inp_names[-2]] = kv_cache_axis
 
     if with_past or input_with_past:
         for i in range(num_of_past_key):
             onnx_out_names += (f"present.{i}.key",)
             onnx_out_names += (f"present.{i}.value",)
-            onnx_dynamic_axes[onnx_out_names[-1]] = kv_cache_axis
-            onnx_dynamic_axes[onnx_out_names[-2]] = kv_cache_axis
 
     for idx, name in enumerate(torch_input_names):
         if input_with_past:
             if name == "past_key_values":
                 onnx_inputs[idx] = past_key_values
             elif name == "attention_mask":
                 attn_mask = onnx_inputs[idx]
```

## onnxruntime/transformers/onnx_exporter.py

```diff
@@ -488,18 +488,15 @@
         data = numpy.random.randint(
             low=0, high=256, size=config.image_size * config.image_size * 3, dtype=numpy.uint8
         ).reshape(config.image_size, config.image_size, 3)
 
         example_inputs = image_processor(data, return_tensors="pt")
     else:
         tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
-        max_input_size = (
-            tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
-        )
-
+        max_input_size = tokenizer.max_model_input_sizes.get(model_name, 1024)
         example_inputs = tokenizer.encode_plus("This is a sample input", return_tensors="pt")
 
     example_inputs = filter_inputs(example_inputs, input_names)
 
     example_outputs = model(**example_inputs)
 
     assert isinstance(example_outputs, (list, tuple)), f"type of output is not list or tuple: {type(example_outputs)}"
@@ -595,17 +592,15 @@
 
     tf.config.set_visible_devices([], "GPU")
 
     tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
     # Fix "Using pad_token, but it is not set yet" error.
     if tokenizer.pad_token is None:
         tokenizer.add_special_tokens({"pad_token": "[PAD]"})
-    max_input_size = (
-        tokenizer.max_model_input_sizes[model_name] if model_name in tokenizer.max_model_input_sizes else 1024
-    )
+    max_input_size = tokenizer.max_model_input_sizes.get(model_name, 1024)
 
     config, model = load_tf_model(model_name, model_class, cache_dir, config_modifier)
     model.resize_token_embeddings(len(tokenizer))
 
     example_inputs = tokenizer.encode_plus(
         "This is a sample input",
         return_tensors="tf",
```

## onnxruntime/transformers/onnx_model.py

```diff
@@ -78,14 +78,18 @@
         output_name_to_node = {}
         for node in self.nodes():
             for output_name in node.output:
                 if output_name:  # could be empty when it is optional
                     output_name_to_node[output_name] = node
         return output_name_to_node
 
+    def functions(self):
+        all_functions = [list(self.model.functions)]
+        return all_functions
+
     def nodes(self):
         all_nodes = []
         for graph in self.graphs():
             for node in graph.node:
                 all_nodes.append(node)  # noqa: PERF402
         return all_nodes
 
@@ -777,14 +781,15 @@
                     "keep_io_types",
                     "min_positive_val",
                     "max_finite_val",
                     "op_block_list",
                     "node_block_list",
                     "force_fp16_initializers",
                     "force_fp16_inputs",
+                    "use_bfloat16_as_blocked_nodes_dtype",
                 ]
                 if key in kwargs
             }
         )
 
         fp16_model = convert_float_to_float16(model, **parameters)
         self.initialize(fp16_model)
@@ -877,19 +882,17 @@
                 for input in node.input:
                     if self.find_graph_input(input) and input not in graph_inputs:
                         graph_inputs.append(input)
         return graph_inputs
 
     @staticmethod
     def input_index(node_output, child_node):
-        index = 0
-        for input in child_node.input:
+        for index, input in enumerate(child_node.input):
             if input == node_output:
                 return index
-            index += 1
         return -1
 
     def remove_unused_constant(self):
         input_name_to_nodes = self.input_name_to_nodes()
 
         # remove unused constant
         unused_nodes = []
@@ -947,15 +950,15 @@
                         dq.appendleft(output_name_to_node[name])
 
         # Keep only those nodes in the output_to_node dictionary.
         nodes_to_keep = []
         num_nodes_removed = 0
         for node in self.model.graph.node:
             first_output = get_first_output(node)
-            kept_node = output_to_node[first_output] if first_output in output_to_node else None
+            kept_node = output_to_node.get(first_output)
 
             # Need double check the node since fused node might reuse output name of some nodes to be removed.
             # It is slow to compare whole node, so we compare op_type first to avoid comparing node in most cases.
             if kept_node and kept_node.op_type == node.op_type and kept_node == node:
                 nodes_to_keep.append(node)
             else:
                 num_nodes_removed += 1
```

## onnxruntime/transformers/onnx_model_unet.py

```diff
@@ -123,15 +123,15 @@
     def optimize(self, options: Optional[FusionOptions] = None):
         if is_installed("tqdm"):
             import tqdm
             from tqdm.contrib.logging import logging_redirect_tqdm
 
             with logging_redirect_tqdm():
                 steps = 18
-                progress_bar = tqdm.tqdm(range(0, steps), initial=0, desc="fusion")
+                progress_bar = tqdm.tqdm(range(steps), initial=0, desc="fusion")
                 self._optimize(options, progress_bar)
         else:
             logger.info("tqdm is not installed. Run optimization without progress bar")
             self._optimize(options, None)
 
     def _optimize(self, options: Optional[FusionOptions] = None, progress_bar=None):
         if (options is not None) and not options.enable_shape_inference:
```

## onnxruntime/transformers/optimizer.py

```diff
@@ -17,31 +17,36 @@
 #  (2) Change input data type from int64 to int32.
 #  (3) Some model cannot be handled by OnnxRuntime, and you can modify this script to get optimized model.
 
 import argparse
 import logging
 import os
 import tempfile
-from typing import Dict, List, Optional
+from pathlib import Path
+from typing import Dict, List, Optional, Union
 
 import coloredlogs
 from fusion_options import FusionOptions
-from onnx import ModelProto, TensorProto, load_model
+from onnx import ModelProto, load_model
 from onnx_model import OnnxModel
 from onnx_model_bart import BartOnnxModel
 from onnx_model_bert import BertOnnxModel
 from onnx_model_bert_keras import BertOnnxModelKeras
 from onnx_model_bert_tf import BertOnnxModelTF
 from onnx_model_clip import ClipOnnxModel
 from onnx_model_conformer import ConformerOnnxModel
 from onnx_model_gpt2 import Gpt2OnnxModel
+from onnx_model_phi import PhiOnnxModel
 from onnx_model_t5 import T5OnnxModel
 from onnx_model_tnlr import TnlrOnnxModel
 from onnx_model_unet import UnetOnnxModel
 from onnx_model_vae import VaeOnnxModel
+from onnx_utils import extract_raw_data_from_model, has_external_data
+
+import onnxruntime
 
 logger = logging.getLogger(__name__)
 
 # Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx), and default opt_level
 MODEL_TYPES = {
     "bart": (BartOnnxModel, "pytorch", 1),
     "bert": (BertOnnxModel, "pytorch", 1),
@@ -54,62 +59,70 @@
     "swin": (BertOnnxModel, "pytorch", 1),
     "tnlr": (TnlrOnnxModel, "pytorch", 1),
     "t5": (T5OnnxModel, "pytorch", 2),
     "unet": (UnetOnnxModel, "pytorch", 1),  # UNet in Stable Diffusion
     "vae": (VaeOnnxModel, "pytorch", 1),  # UAE in Stable Diffusion
     "vit": (BertOnnxModel, "pytorch", 1),
     "conformer": (ConformerOnnxModel, "pytorch", 1),
+    "phi": (PhiOnnxModel, "pytorch", 0),
 }
 
 
 def optimize_by_onnxruntime(
-    onnx_model_path: str,
+    onnx_model: Optional[Union[str, ModelProto]] = None,
     use_gpu: bool = False,
     optimized_model_path: Optional[str] = None,
     opt_level: Optional[int] = 99,
     disabled_optimizers: List[str] = [],  # noqa: B006
     verbose: bool = False,
     save_as_external_data: bool = False,
     external_data_filename: str = "",
     external_data_file_threshold: int = 1024,
     *,
     provider: Optional[str] = None,
+    **deprecated_kwargs,
 ) -> str:
     """
     Use onnxruntime to optimize model.
 
     Args:
-        onnx_model_path (str): the path of input onnx model.
+        onnx_model (str | ModelProto): the path of input onnx model or ModelProto.
         use_gpu (bool): whether the optimized model is targeted to run in GPU.
         optimized_model_path (str or None): the path of optimized model.
         opt_level (int): graph optimization level.
         disabled_optimizers (List[str]): a list of names of disabled optimizers
         save_as_external_data (bool): whether to save external data outside of ONNX model
         external_data_filename (str): name of external data file. If not provided, name is automatically created from ONNX model.
         external_data_file_threshold (int): threshold to decide whether to save tensor in ONNX model or in external data file
         provider (str or None): execution provider to use if use_gpu
     Returns:
         optimized_model_path (str): the path of optimized model
     """
     assert opt_level in [1, 2, 99]
     from torch import version as torch_version
 
-    import onnxruntime
+    if onnx_model is None:
+        onnx_model = deprecated_kwargs.pop("onnx_model_path", None)
+    assert onnx_model is not None
 
     if (
         use_gpu
         and provider is None
         and set(onnxruntime.get_available_providers()).isdisjoint(
             ["CUDAExecutionProvider", "ROCMExecutionProvider", "MIGraphXExecutionProvider"]
         )
     ):
         logger.error("There is no gpu for onnxruntime to do optimization.")
-        return onnx_model_path
+        return onnx_model
 
-    model = OnnxModel(load_model(onnx_model_path, load_external_data=False))
+    model = (
+        OnnxModel(load_model(onnx_model, load_external_data=False))
+        if isinstance(onnx_model, str)
+        else OnnxModel(onnx_model)
+    )
     if model.use_float16() and not use_gpu:
         logger.warning(
             "This model uses float16 in the graph, use_gpu=False might cause extra Cast nodes. "
             "Most operators have no float16 implementation in CPU, so Cast nodes are added to compute them in float32. "
             "If the model is intended to use in GPU, please set use_gpu=True. "
             "Otherwise, consider exporting onnx in float32 and optional int8 quantization for better performance. "
         )
@@ -119,15 +132,18 @@
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC
     elif opt_level == 2:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
     else:
         sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
 
     if optimized_model_path is None:
-        path_prefix = onnx_model_path[:-5]  # remove .onnx suffix
+        if isinstance(onnx_model, str):
+            path_prefix = str(Path(onnx_model).with_suffix(""))  # remove .onnx suffix
+        else:
+            path_prefix = "optimized_model"
         optimized_model_path = "{}_o{}_{}.onnx".format(path_prefix, opt_level, "gpu" if use_gpu else "cpu")
 
     sess_options.optimized_model_filepath = optimized_model_path
     if save_as_external_data:
         if len(external_data_filename) == 0:
             # Set external data filename to model_name.onnx.data
             external_data_filename = os.path.basename(optimized_model_path) + ".data"
@@ -168,28 +184,41 @@
 
         if torch_version.hip:
             providers.append("MIGraphXExecutionProvider")
             providers.append("ROCMExecutionProvider")
         else:
             providers.append("CUDAExecutionProvider")
 
-    onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=providers, **kwargs)
+    # For large model, extract external data from model and add to session options
+    if isinstance(onnx_model, ModelProto):
+        if has_external_data(onnx_model):
+            raise ValueError(
+                "ModelProto has external data not loaded into memory, ORT cannot create session. "
+                "Please load external data before calling this function. "
+                "See https://onnx.ai/onnx/repo-docs/ExternalData.html for more information."
+            )
+        external_names, external_values = extract_raw_data_from_model(onnx_model)
+        sess_options.add_external_initializers(list(external_names), list(external_values))
+
+    # Inference session is only used to optimize the model.
+    onnx_model = onnx_model.SerializeToString() if isinstance(onnx_model, ModelProto) else onnx_model
+    onnxruntime.InferenceSession(onnx_model, sess_options, providers=providers, **kwargs)
 
     assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)
     logger.debug("Save optimized model by onnxruntime to %s", optimized_model_path)
     return optimized_model_path
 
 
 def optimize_by_fusion(
     model: ModelProto,
     model_type: str = "bert",
     num_heads: int = 0,
     hidden_size: int = 0,
     optimization_options: Optional[FusionOptions] = None,
-):
+) -> OnnxModel:
     """Optimize Model by graph fusion logic.
 
     Note that ONNXRuntime graph optimizations (like constant folding) will not be applied. So it is better to enable
     constant folding during exporting ONNX model, or run optimize_by_onnxruntime on the model first like optimize_model.
 
     For BERT model, num_heads and hidden_size are optional. For other model types, you need to specify these parameters.
 
@@ -235,26 +264,26 @@
 
     optimizer.model.producer_version = onnxruntime_version
 
     return optimizer
 
 
 def optimize_model(
-    input: str,
+    input: Union[str, ModelProto],
     model_type: str = "bert",
     num_heads: int = 0,
     hidden_size: int = 0,
     optimization_options: Optional[FusionOptions] = None,
     opt_level: Optional[int] = None,
     use_gpu: bool = False,
     only_onnxruntime: bool = False,
     verbose: bool = False,
     *,
     provider: Optional[str] = None,
-):
+) -> OnnxModel:
     """Optimize Model by OnnxRuntime and/or python fusion logic.
 
     ONNX Runtime has graph optimizations (https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html).
     However, the coverage is limited. We also have graph fusions that implemented in Python to improve the coverage.
     They can combined: ONNX Runtime will run first when opt_level > 0, then graph fusions in Python will be applied.
 
     To use ONNX Runtime only and no Python fusion logic, use only_onnxruntime flag and a positive opt_level like
@@ -269,15 +298,15 @@
 
     If your model is intended for GPU inference only (especially float16 or mixed precision model), it is recommended to
     set use_gpu to be True, otherwise the model is not optimized for GPU inference.
 
     For BERT model, num_heads and hidden_size are optional. For other model types, you need specify these parameters.
 
     Args:
-        input (str): input model path.
+        input (str | ModelProto): input model path or ModelProto.
         model_type (str, optional): model type - like bert, bert_tf, bert_keras or gpt2. Defaults to 'bert'.
         num_heads (int, optional): number of attention heads. Defaults to 0.
             0 allows detect the parameter from graph automatically.
         hidden_size (int, optional): hidden size. Defaults to 0.
             0 allows detect the parameter from graph automatically.
         optimization_options (FusionOptions, optional): optimization options that turn on/off some fusions.
             Defaults to None.
@@ -292,17 +321,17 @@
      Returns:
         object of an optimizer class.
     """
     assert opt_level is None or opt_level in [0, 1, 2, 99]
 
     if model_type not in MODEL_TYPES:
         logger.warning(f"Unsupported model type: {model_type} for optimization, directly return model.")
-        return OnnxModel(load_model(input))
+        return OnnxModel(load_model(input)) if isinstance(input, str) else OnnxModel(input)
 
-    (optimizer_class, _producer, default_opt_level) = MODEL_TYPES[model_type]
+    (optimizer_class, _, default_opt_level) = MODEL_TYPES[model_type]
 
     if opt_level is None:
         opt_level = default_opt_level
 
     # Disable constant sharing to avoid model proto str mismatch in test. Ideally the optimizer should not
     # affect other fusions. We can update the expected model proto once the ConstantSharing optimizer logic becomes
     # stable.
@@ -310,19 +339,17 @@
     temp_model_path = None
     temp_dir = tempfile.TemporaryDirectory()
     optimized_model_name = "model_o{}_{}.onnx".format(opt_level, "gpu" if use_gpu else "cpu")
     optimized_model_path = os.path.join(temp_dir.name, optimized_model_name)
 
     # Auto detect if input model has external data
     has_external_data_file = False
-    original_model = load_model(input, load_external_data=False)
-    for initializer in original_model.graph.initializer:
-        if initializer.HasField("data_location") and initializer.data_location == TensorProto.EXTERNAL:
-            has_external_data_file = True
-            break
+    original_model = load_model(input, load_external_data=False) if isinstance(input, str) else input
+    if has_external_data(original_model):
+        has_external_data_file = True
     del original_model
 
     if opt_level > 1:
         # Disable some optimizers that might cause failure in symbolic shape inference or attention fusion.
         disabled_optimizers += (
             []
             if only_onnxruntime
@@ -359,15 +386,20 @@
             verbose=verbose,
             save_as_external_data=has_external_data_file,
         )
 
     if only_onnxruntime and not temp_model_path:
         logger.warning("Please specify a positive value for opt_level when only_onnxruntime is True")
 
-    model = load_model(temp_model_path or input)
+    if temp_model_path is not None:
+        model = load_model(temp_model_path)
+    elif isinstance(input, str):
+        model = load_model(input)
+    else:
+        model = input
 
     if only_onnxruntime:
         optimizer = optimizer_class(model, num_heads, hidden_size)
     else:
         optimizer = optimize_by_fusion(model, model_type, num_heads, hidden_size, optimization_options)
 
     # remove the temporary directory
```

## onnxruntime/transformers/profiler.py

```diff
@@ -325,15 +325,15 @@
     ]
     before_percentage = 0.0
     for node_name in node_name_list:
         duration = node_time[node_name]
         calls = node_freq[node_name]
         avg_time = duration / float(calls)
         percentage = (duration / total) * 100.0
-        provider = node_provider[node_name] if node_name in node_provider else ""
+        provider = node_provider.get(node_name, "")
         before_percentage += percentage
         lines.append(
             f"{duration:10d}\t{percentage:5.2f}\t{before_percentage:5.2f}\t{avg_time:8.1f}\t{calls:5d}\t{provider:8s}\t{node_name}"
         )
 
     # Output items with run time ratio > thresholds, and sorted by duration in the descending order.
     lines.append(f"\nTop expensive nodes with Time% >= {threshold*100:.2f}:")
@@ -343,15 +343,15 @@
         ratio = duration / total
         if ratio < threshold:
             continue
 
         calls = node_freq[node_name]
         avg_time = duration / float(calls)
         percentage = (duration / total) * 100.0
-        provider = node_provider[node_name] if node_name in node_provider else ""
+        provider = node_provider.get(node_name, "")
         lines.append(f"{duration:10d}\t{percentage:5.2f}\t{avg_time:8.1f}\t{calls:5d}\t{provider:8s}\t{node_name}")
 
     return lines
 
 
 def group_node_results(sess_time, kernel_time_only, use_gpu):
     """Group results by operator name.
@@ -389,15 +389,15 @@
                     if op_name in op_fence_time:
                         op_fence_time[op_name] += item["dur"]
                     else:
                         op_fence_time[op_name] = item["dur"]
                     total_fence_time += item["dur"]
                 continue
 
-            provider = item["args"]["provider"] if "provider" in item["args"] else ""
+            provider = item["args"].get("provider", "")
             if provider in provider_counter:
                 provider_counter[provider] += 1
             else:
                 provider_counter[provider] = 1
 
             key = f"{provider}:{op_name}"
             if key in provider_op_kernel_time:
@@ -421,15 +421,15 @@
 
             total_kernel_time += item["dur"]
 
     lines = ["", "Grouped by operator"]
     lines.append("-" * 64)
     lines.append("Total(μs)\tTime%\tKernel(μs)\tKernel%\tCalls\tAvgKernel(μs)\tFence(μs)\tOperator")
     for op_name, kernel_time in sorted(op_kernel_time.items(), key=lambda x: x[1], reverse=True):
-        fence_time = op_fence_time[op_name] if op_name in op_fence_time else 0
+        fence_time = op_fence_time.get(op_name, 0)
         kernel_time_ratio = kernel_time / total_kernel_time
         total_time = kernel_time + fence_time
         time_ratio = total_time / (total_kernel_time + total_fence_time)
         kernel_calls = op_kernel_records[op_name]
         avg_kernel_time = kernel_time / kernel_calls
         lines.append(
             f"{total_time:10d}\t{time_ratio * 100.0:5.2f}\t{kernel_time:11d}\t{kernel_time_ratio * 100.0:5.2f}\t{kernel_calls:5d}\t{avg_kernel_time:14.1f}\t{fence_time:10d}\t{op_name}"
```

## onnxruntime/transformers/shape_optimizer.py

```diff
@@ -129,17 +129,15 @@
                 dim_proto = input.type.tensor_type.shape.dim[0]
                 dim_proto.dim_value = batch_size
                 dim_proto = input.type.tensor_type.shape.dim[1]
                 if dim_proto.HasField("dim_param"):
                     dim_proto.dim_value = max_seq_len
                 elif dim_proto.HasField("dim_value") and dim_proto.dim_value != max_seq_len:
                     raise ValueError(
-                        "Unable to set dimension value to {} for axis {} of {}. Contradicts existing dimension value {}.".format(
-                            max_seq_len, 1, input.name, dim_proto.dim_value
-                        )
+                        f"Unable to set dimension value to {max_seq_len} for axis {1} of {input.name}. Contradicts existing dimension value {dim_proto.dim_value}."
                     )
 
     def create_dummy_inputs(
         self,
         input_ids,
         segment_ids,
         input_mask,
```

## onnxruntime/transformers/models/bert/eval_squad.py

```diff
@@ -189,15 +189,15 @@
                     key = f"b{batch_size}_s{sequence_length}"
 
                     if key in key_names:
                         values[key] = result[metric_name]
 
             if row:
                 for key in key_names:
-                    row[key] = values[key] if key in values else ""
+                    row[key] = values.get(key, "")
                 csv_writer.writerow(row)
 
         csv_file.flush()
 
     print(f"Summary results for {metric_name} are saved to csv file: {csv_filename}")
```

## onnxruntime/transformers/models/gpt2/benchmark_gpt2.py

```diff
@@ -396,15 +396,15 @@
                             "past_sequence_length": past_sequence_length,
                             "disable_io_binding": args.disable_io_binding,
                             "torch_latency": f"{torch_latency:.2f}" if torch_latency else "None",
                             "onnxruntime_latency": f"{ort_latency:.2f}",
                         }
                         csv_writer.writerow(row)
                     except Exception:
-                        logger.error("Exception", exc_info=True)
+                        logger.error("Exception", exc_info=True)  # noqa: G201
                         return None
 
     logger.info(f"Results are saved to file {csv_filename}")
     return csv_filename
 
 
 if __name__ == "__main__":
```

## onnxruntime/transformers/models/gpt2/gpt2_helper.py

```diff
@@ -626,15 +626,15 @@
         with torch.no_grad():
             for _ in range(total_runs):
                 start = time.time()
                 outputs = model(*input_list)
                 latency.append(time.time() - start)
 
         average_latency = sum(latency) * 1000 / len(latency)
-        logger.debug("PyTorch inference time = {} ms".format(format(average_latency, ".2f")))
+        logger.debug("PyTorch inference time = {} ms".format(format(average_latency, ".2f")))  # noqa: G001
 
         return outputs, average_latency
 
     @staticmethod
     def onnxruntime_inference(ort_session, inputs: Gpt2Inputs, total_runs: int = 0):
         """Run inference of ONNX model, and returns average latency in ms when total_runs > 0 besides outputs."""
         logger.debug("start onnxruntime_inference")
@@ -658,15 +658,15 @@
         latency = []
         for _ in range(total_runs):
             start = time.time()
             ort_outputs = ort_session.run(None, ort_inputs)
             latency.append(time.time() - start)
 
         average_latency = sum(latency) * 1000 / len(latency)
-        logger.debug("OnnxRuntime Inference time = {} ms".format(format(average_latency, ".2f")))
+        logger.debug("OnnxRuntime Inference time = {} ms".format(format(average_latency, ".2f")))  # noqa: G001
 
         return ort_outputs, average_latency
 
     @staticmethod
     def prepare_io_binding(
         ort_session,
         input_ids,
@@ -737,15 +737,15 @@
             if include_copy_output_latency:
                 _ = Gpt2Helper.get_outputs_from_io_binding_buffer(
                     ort_session, output_buffers, output_shapes, return_numpy
                 )
             latency.append(time.time() - start)
 
         average_latency = sum(latency) * 1000 / len(latency)
-        logger.debug("OnnxRuntime with IO binding inference time = {} ms".format(format(average_latency, ".2f")))
+        logger.debug("OnnxRuntime with IO binding inference time = %.2f ms", average_latency)
 
         return ort_outputs, average_latency
 
     @staticmethod
     def save_outputs(i, ort_outputs, torch_outputs):
         with open(f"ort_outputs_{i}.pickle", "wb") as f:
             pickle.dump(ort_outputs, f)
```

## onnxruntime/transformers/models/gpt2/gpt2_parity.py

```diff
@@ -167,25 +167,23 @@
         )
     )
     logger.debug(f"{test_name} Wins:{sorted_wins}")
     logger.info(f"Based on {test_name} wins and a scoring function, the ranking:")
 
     rank = 0
     previous_value = -1
-    count = 0
-    for key, value in sorted_wins.items():
+    for count, (key, value) in enumerate(sorted_wins.items()):
         if value != previous_value:
             rank = count
         previous_value = value
-        count += 1
 
         for row in rows:
             if row["run_id"] == key:
                 logger.info(
-                    "{:02d}: WINs={:02d}, run_id={}, latency={:5.2f}, top1_match={:.4f}, size={}_MB, experiment={}, {}".format(
+                    "{:02d}: WINs={:02d}, run_id={}, latency={:5.2f}, top1_match={:.4f}, size={}_MB, experiment={}, {}".format(  # noqa: G001
                         rank,
                         value,
                         key,
                         get_latency(row),
                         float(row["top1_match_rate"]),
                         row["onnx_size_in_MB"],
                         row["experiment"],
```

## onnxruntime/transformers/models/gpt2/gpt2_tester.py

```diff
@@ -383,16 +383,16 @@
 
         for i, inputs in enumerate(test_inputs):
             if max_inputs > 0 and i == max_inputs:
                 break
             if i % 10 == 0:
                 print(f"{i}")
             input_ids = inputs["input_ids"]
-            position_ids = inputs["position_ids"] if "position_ids" in inputs else None
-            attention_mask = inputs["attention_mask"] if "attention_mask" in inputs else None
+            position_ids = inputs.get("position_ids", None)
+            attention_mask = inputs.get("attention_mask", None)
 
             onnx_runner = Gpt2Tester(
                 input_ids,
                 position_ids,
                 attention_mask,
                 n_head,
                 n_embd,
```

## onnxruntime/transformers/models/llama/benchmark.py

```diff
@@ -50,27 +50,17 @@
     return len(model.get_inputs())
 
 
 def get_inputs(args: argparse.Namespace, ort_model_inputs_len: int):
     init_inputs, iter_inputs = None, None
 
     # For past_present_share_buffer:
-    # Set max_seq_len to 16384 for CodeLLaMA (finetuned variant of LLaMA-2)
-    # Set max_seq_len to 4096 for Hugging Face LLaMA-2 model since that is the default value
     # Set max_seq_len to 2048 for Microsoft LLaMA-2 model since that is the max value currently supported
-    temp_name = args.model_name.lower().replace("-", "").replace("_", "")
-    max_seq_len = (
-        2048
-        if args.benchmark_type == "ort-msft"
-        else 16384
-        if "codellama" in temp_name
-        else 4096
-        if "llama2" in temp_name
-        else 2048
-    )
+    # Set max_seq_len to config value for other models
+    max_seq_len = 2048 if args.benchmark_type == "ort-msft" else args.config.max_position_embeddings
 
     if args.benchmark_type in {"hf-pt-eager", "hf-pt-compile"}:
         init_inputs = get_sample_inputs(
             args.config,
             args.target_device,
             args.batch_size,
             args.sequence_length,
@@ -109,55 +99,55 @@
                 args.config,
                 args.target_device,
                 args.batch_size,
                 seq_len=args.sequence_length,
                 past_seq_len=0,
                 max_seq_len=max_seq_len,
                 use_fp16=args.use_fp16,
-                use_gqa=args.use_gqa,
+                use_buffer_share=args.use_buffer_share,
                 engine="pt",
                 return_dict=True,
             )
             iter_inputs = get_merged_sample_with_past_kv_inputs(
                 args.config,
                 args.target_device,
                 args.batch_size,
                 seq_len=1,
                 past_seq_len=args.sequence_length,
                 max_seq_len=max_seq_len,
                 use_fp16=args.use_fp16,
-                use_gqa=args.use_gqa,
+                use_buffer_share=args.use_buffer_share,
                 engine="pt",
                 return_dict=True,
             )
 
     elif args.benchmark_type == "ort-convert-to-onnx":
         # Microsoft export from convert_to_onnx
         init_inputs = get_merged_sample_with_past_kv_inputs(
             args.config,
             args.target_device,
             args.batch_size,
             seq_len=args.sequence_length,
             past_seq_len=0,
             max_seq_len=max_seq_len,
             use_fp16=args.use_fp16,
-            use_gqa=args.use_gqa,
+            use_buffer_share=args.use_buffer_share,
             engine="ort",
             return_dict=True,
             world_size=args.world_size,
         )
         iter_inputs = get_merged_sample_with_past_kv_inputs(
             args.config,
             args.target_device,
             args.batch_size,
             seq_len=1,
             past_seq_len=args.sequence_length,
             max_seq_len=max_seq_len,
             use_fp16=args.use_fp16,
-            use_gqa=args.use_gqa,
+            use_buffer_share=args.use_buffer_share,
             engine="ort",
             return_dict=True,
             world_size=args.world_size,
         )
 
     elif args.benchmark_type == "ort-msft":
         # Microsoft export from https://github.com/microsoft/Llama-2-Onnx
@@ -166,25 +156,25 @@
         init_inputs = get_msft_sample_inputs(
             args.config,
             args.batch_size,
             past_seq_len=0,
             seq_len=args.sequence_length,
             max_seq_len=max_seq_len,
             use_fp16=args.use_fp16,
-            use_gqa=args.use_gqa,
+            use_buffer_share=args.use_buffer_share,
             split_kv=split_kv,
         )
         iter_inputs = get_msft_sample_inputs(
             args.config,
             args.batch_size,
             past_seq_len=args.sequence_length,
             seq_len=1,
             max_seq_len=max_seq_len,
             use_fp16=args.use_fp16,
-            use_gqa=args.use_gqa,
+            use_buffer_share=args.use_buffer_share,
             split_kv=split_kv,
         )
 
     else:
         raise Exception("Unable to auto-detect inputs for provided model")
 
     return init_inputs, iter_inputs
@@ -246,15 +236,15 @@
 
         start_time = time.time()
         model = ORTModelForCausalLM.from_pretrained(
             args.hf_ort_dir_path,
             decoder_file_name=decoder_file_name,
             decoder_with_past_file_name=decoder_with_past_file_name,
             use_auth_token=args.auth,
-            use_io_binding=(args.device != "cpu"),
+            use_io_binding=True,  # Large perf gain even for cpu due to avoiding output copy.
             use_merged=(True if decoder_file_name == "model.onnx" else None),
             provider=provider,
             provider_options=provider_options,
             session_options=sess_options,
         )
         end_time = time.time()
 
@@ -281,29 +271,33 @@
         else trange(args.warmup_runs, file=sys.stdout, desc="Warm up")
     )
 
     if args.verbose:
         outputs = fn(inputs)
         logger.info(outputs)
 
-    input_sync = (  # noqa: E731
-        lambda *kwargs: args.io_binding.synchronize_inputs()
+    input_sync = lambda *kwargs: (  # noqa: E731
+        args.io_binding.synchronize_inputs()
         if args.device != "cpu" and args.benchmark_type in {"ort-msft", "ort-convert-to-onnx"}  # ORT synchronize
-        else lambda *kwargs: torch.cuda.synchronize()
-        if args.device != "cpu" and torch.cuda.is_available()  # PyTorch synchronize
-        else lambda *kwargs: None  # no-op function
-    )
+        else lambda *kwargs: (
+            torch.cuda.synchronize()
+            if args.device != "cpu" and torch.cuda.is_available()  # PyTorch synchronize
+            else lambda *kwargs: None
+        )
+    )  # no-op function
 
-    output_sync = (  # noqa: E731
-        lambda *kwargs: args.io_binding.synchronize_outputs()
+    output_sync = lambda *kwargs: (  # noqa: E731
+        args.io_binding.synchronize_outputs()
         if args.device != "cpu" and args.benchmark_type in {"ort-msft", "ort-convert-to-onnx"}  # ORT synchronize
-        else lambda *kwargs: torch.cuda.synchronize()
-        if args.device != "cpu" and torch.cuda.is_available()  # PyTorch synchronize
-        else lambda *kwargs: None  # no-op function
-    )
+        else lambda *kwargs: (
+            torch.cuda.synchronize()
+            if args.device != "cpu" and torch.cuda.is_available()  # PyTorch synchronize
+            else lambda *kwargs: None
+        )
+    )  # no-op function
 
     for _ in warmup_range:
         input_sync()
         fn(inputs)
         output_sync()
 
     # Benchmark
@@ -453,15 +447,15 @@
     def prepare_ort_inputs(inputs, kv_cache_ortvalues):
         # Verify model inputs
         inputs = verify_ort_inputs(model, inputs)
 
         # Add IO bindings for non-CPU execution providers
         if args.device != "cpu":
             io_binding, kv_cache_ortvalues = add_io_bindings_as_ortvalues(
-                model, inputs, args.device, int(args.rank), args.use_gqa, kv_cache_ortvalues
+                model, inputs, args.device, int(args.rank), args.use_buffer_share, kv_cache_ortvalues
             )
             setattr(args, "io_binding", io_binding)  # noqa: B010
             return io_binding, kv_cache_ortvalues
 
         return inputs, kv_cache_ortvalues
 
     def with_io_binding(io_binding):
@@ -680,17 +674,17 @@
 
     # Check if past_present_share_buffer can be enabled (only for FP16 models with GQA)
     if args.benchmark_type in {"ort-convert-to-onnx", "ort-msft"}:
         onnx_model = onnx.load_model(args.ort_model_path.format(args.rank), load_external_data=False)
         gqa_nodes = list(filter(lambda node: node.op_type == "GroupQueryAttention", onnx_model.graph.node))
 
         use_buffer_share = use_fp16 and len(gqa_nodes) > 0 and args.device != "cpu"
-        setattr(args, "use_gqa", use_buffer_share)  # noqa: B010
+        setattr(args, "use_buffer_share", use_buffer_share)  # noqa: B010
     else:
-        setattr(args, "use_gqa", False)  # noqa: B010
+        setattr(args, "use_buffer_share", False)  # noqa: B010
 
     # Measure prompt cost (init_inputs) and generated token cost (iter_inputs)
     for batch_size, sequence_length in itertools.product(args.batch_sizes, args.sequence_lengths):
         if args.rank == 0:
             logger.info(f"\nBatch size = {batch_size} and sequence length = {sequence_length}...")
         setattr(args, "batch_size", int(batch_size))  # noqa: B010
         setattr(args, "sequence_length", int(sequence_length))  # noqa: B010
```

## onnxruntime/transformers/models/llama/benchmark_all.py

```diff
@@ -261,15 +261,15 @@
     if installed_packages_list:
         ort_pkg_name = installed_packages_list[0].split("==")[0]
         ort_pkg_version = installed_packages_list[0].split("==")[1]
 
     # Save results to csv with standard format
     records = []
     for _, row in df.iterrows():
-        if row["Engine"] == "optimum-ort":
+        if row["Engine"] in ["optimum-ort", "onnxruntime"]:
             record = BenchmarkRecord(
                 row["Model Name"], row["Precision"], "onnxruntime", row["Device"], ort_pkg_name, ort_pkg_version
             )
         elif row["Engine"] in ["pytorch-eager", "pytorch-compile"]:
             record = BenchmarkRecord(
                 row["Model Name"], row["Precision"], "pytorch", row["Device"], torch.__name__, torch.__version__
             )
```

## onnxruntime/transformers/models/llama/convert_to_onnx.py

```diff
@@ -782,24 +782,29 @@
     )
 
     parser.add_argument(
         "--optimize_optimum",
         action="store_true",
         help="Avoid exporting model, only apply quantizations and optimizations to existing model exported from optimum.",
     )
+
+    parser.add_argument(
+        "--small_gpu",
+        action="store_true",
+        help="Load the llama in GPU every time for parity_check if it's running in a machine which GPU memory < 36GB.",
+    )
+
     parser.set_defaults(optimize_optimum=False)
 
     args = parser.parse_args()
     return args
 
 
 def main():
-    if version.parse(torch.__version__) < version.parse("2.2.0") and "2.2.0.dev" not in torch.__version__:
-        # Second predicate is for comparing nightly (ex: 2.2.0.dev20230920 vs 2.2.0) since first predicate is false
-        # in that scenario. It can be removed when torch v2.2.0 is released in stable.
+    if version.parse(torch.__version__) < version.parse("2.2.0"):
         logger.error(f"Detected PyTorch version {torch.__version__}. Please upgrade and use v2.2.0 or newer.")
         return
 
     args = get_args()
     setup_logger(args.verbose)
     prepare_environment(args.input, args.output, args.execution_provider != "cpu")
     if args.reexport:
@@ -940,17 +945,19 @@
 
                     logger.info("Quantizing to int8...")
                     for fp32_path, int8_path in zip(old_paths, new_paths):
                         if os.path.exists(fp32_path):
                             ort_quantization.quantize_dynamic(
                                 fp32_path,
                                 int8_path,
-                                op_types_to_quantize=["MatMul", "Gemm", "Gather"]
-                                if args.quantize_embedding_layer
-                                else ["MatMul", "Gemm"],
+                                op_types_to_quantize=(
+                                    ["MatMul", "Gemm", "Gather"]
+                                    if args.quantize_embedding_layer
+                                    else ["MatMul", "Gemm"]
+                                ),
                                 per_channel=args.quantize_per_channel,
                                 reduce_range=args.quantize_reduce_range,
                                 use_external_data_format=True,
                                 extra_options={"MatMulConstBOnly": True},
                             )
                             logger.info(
                                 f"The ONNX model at {fp32_path} has been quantized to int8 and saved at {int8_path}!"
@@ -1018,28 +1025,30 @@
         parity_cmd = [
             "-m",
             original_model_name,
             "-o",
             os.path.join(args.output, filename),
             "-ep",
             args.execution_provider,
-            "-fp",
+            "--precision",
             args.precision,
             "--cache_dir",
             args.cache_dir,
+            "--torch_model_directory",
+            args.input,
         ]
+        if args.small_gpu:
+            parity_cmd.append("--small_gpu")
         if "with_past" in filename:
             parity_cmd.append("--use_past_kv")
         if "merged" in filename:
             parity_cmd.append("--merged")
-        if args.use_gqa:
-            parity_cmd.append("--use_gqa")
 
         try:
-            logger.debug(f"check parity with cmd: {parity_cmd}")
+            logger.info(f"check parity with cmd: {parity_cmd}")
             parity_check(parity_cmd)
         except Exception as e:
             logger.warning(f"An error occurred while verifying parity: {e}", exc_info=True)
 
 
 if __name__ == "__main__":
     main()
```

## onnxruntime/transformers/models/llama/llama_inputs.py

```diff
@@ -123,15 +123,15 @@
     config: AutoConfig,
     device: torch.device,
     batch_size: int,
     seq_len: int,
     past_seq_len: int,
     max_seq_len: int,
     use_fp16: bool = False,
-    use_gqa: bool = False,
+    use_buffer_share: bool = False,
     engine: str = "pt",
     return_dict: bool = False,
     world_size: int = 1,
 ):
     input_ids = torch.randint(low=0, high=config.vocab_size, size=(batch_size, seq_len), dtype=torch.int64)
     attention_mask = torch.ones(batch_size, past_seq_len + seq_len, dtype=torch.int64)
     # position_ids is of shape (batch_size, seq_len) for prompt generation, (batch_size, 1) for token generation
@@ -158,15 +158,15 @@
         "attention_mask": attention_mask,
         "position_ids": position_ids,
     }
     if engine == "ort":
         assert isinstance(past_kv, dict)
         inputs.update(past_kv)
 
-        if use_gqa:
+        if use_buffer_share:
             inputs = enable_past_present_share_buffer(inputs, past_seq_len, max_seq_len)
 
     else:
         assert isinstance(past_kv, list)
         inputs["past_key_values"] = past_kv
 
     return inputs
@@ -176,15 +176,15 @@
 def get_msft_sample_inputs(
     config: AutoConfig,
     batch_size: int,
     past_seq_len: int,
     seq_len: int,
     max_seq_len: int,
     use_fp16: bool,
-    use_gqa: bool,
+    use_buffer_share: bool,
     split_kv: bool,
 ):
     np_dtype = np.float16 if use_fp16 else np.float32
     head_size = config.hidden_size // config.num_attention_heads
 
     if not split_kv:
         ort_inputs = {
@@ -214,15 +214,15 @@
                     ).astype(np_dtype),
                     f"v_{i}_cache": np.random.rand(
                         batch_size, config.num_attention_heads, past_seq_len, head_size
                     ).astype(np_dtype),
                 }
             )
 
-        if use_gqa:
+        if use_buffer_share:
             ort_inputs = enable_past_present_share_buffer(ort_inputs, past_seq_len, max_seq_len)
 
     return ort_inputs
 
 
 # Create past_key_values
 # Each is of shape (batch_size, num_heads, past_sequence_length, head_size)
@@ -248,15 +248,15 @@
         past_kv[f"past_key_values.{i}.value"] = past_v.detach().cpu().numpy()
     return past_kv
 
 
 # Format PyTorch inputs to ONNX Runtime inputs
 def convert_inputs_for_ort(
     pt_inputs: dict,
-    use_gqa: bool = False,
+    use_buffer_share: bool = False,
     past_seq_len: int = 0,
     max_seq_len: int = 2048,
     device: str = "",
     device_id: int = -1,
 ):
     ort_inputs = {}
     for k, v in pt_inputs.items():
@@ -264,15 +264,15 @@
             ort_inputs[k] = v
         elif k == "past_key_values":
             ort_inputs.update(flatten_past_kv_inputs(v))
         else:
             ort_inputs[k] = v.detach().cpu().numpy()
 
     # Reshape KV caches if using past-present-share-buffer
-    if use_gqa and device != "" and device != "cpu" and device_id > -1:
+    if use_buffer_share and device != "" and device != "cpu" and device_id > -1:
         ort_inputs = enable_past_present_share_buffer(ort_inputs, past_seq_len, max_seq_len)
 
     return ort_inputs
 
 
 # Re-allocate KV caches from (batch_size, num_heads, past_sequence_length, head_size) to
 # (batch_size, num_heads, max_sequence_length, head_size) for past-present buffer sharing
@@ -307,42 +307,47 @@
 
     return ort_inputs
 
 
 # Add IO bindings for execution providers using OrtValue
 # Use when you need to run inference once or twice to save memory
 def add_io_bindings_as_ortvalues(
-    model: InferenceSession, ort_inputs: dict, device: str, device_id: int, use_gqa: bool, kv_cache_ortvalues: dict
+    model: InferenceSession,
+    ort_inputs: dict,
+    device: str,
+    device_id: int,
+    use_buffer_share: bool,
+    kv_cache_ortvalues: dict,
 ):
     io_binding = model.io_binding()
 
     model_inputs = set(map(lambda i: i.name, model.get_inputs()))
     for k, v in ort_inputs.items():
         # Use this check to handle scenarios such as INT4 CUDA and FP16 CUDA models with
         # GQA + RotaryEmbedding fusion where `position_ids` is removed as an ONNX model input
         # but `position_ids` is used as a PyTorch model input
         if k not in model_inputs:
             continue
 
         # Bind OrtValue inputs to device
-        if use_gqa and ("cache" in k or "past_key_values" in k):
+        if use_buffer_share and ("cache" in k or "past_key_values" in k):
             if k not in kv_cache_ortvalues:
                 v_device = OrtValue.ortvalue_from_numpy(v, device_type=device, device_id=device_id)
                 io_binding.bind_ortvalue_input(k, v_device)
                 kv_cache_ortvalues[k] = v_device
             else:
                 kv_cache_ortvalues[k].update_inplace(v)
                 io_binding.bind_ortvalue_input(k, kv_cache_ortvalues[k])
         else:
             v_device = OrtValue.ortvalue_from_numpy(v, device_type=device, device_id=device_id)
             io_binding.bind_ortvalue_input(k, v_device)
 
     for output in model.get_outputs():
         name = output.name
-        if use_gqa and ("out" in name or "present" in name):
+        if use_buffer_share and ("out" in name or "present" in name):
             # Bind present KV cache outputs to past KV cache inputs in order to buffer share
             input_name = name.replace("out", "cache").replace("present", "past_key_values")
             io_binding.bind_ortvalue_output(name, kv_cache_ortvalues[input_name])
         else:
             io_binding.bind_output(name, device_type=device, device_id=device_id)
 
     return io_binding, kv_cache_ortvalues
```

## onnxruntime/transformers/models/llama/llama_parity.py

```diff
@@ -18,44 +18,43 @@
     add_io_bindings_as_ortvalues,
     convert_inputs_for_ort,
     get_merged_sample_with_past_kv_inputs,
     get_sample_inputs,
     get_sample_with_past_kv_inputs,
 )
 from llama_torch import setup_torch_model
-from transformers import AutoConfig, AutoModelForCausalLM
+from transformers import AutoConfig
 
 import onnxruntime as ort
 
 logger = logging.getLogger("")
 
 
-def get_sequence_lengths(args: argparse.Namespace):
+def get_sequence_lengths(args: argparse.Namespace, config: AutoConfig):
     past_sequence_length, curr_sequence_length = (8, 1) if args.use_past_kv else (0, 8)
-    temp_name = args.model_name.lower().replace("-", "").replace("_", "")
-    max_sequence_length = 16384 if "codellama" in temp_name else 4096 if "llama2" in temp_name else 2048
+    max_sequence_length = config.max_position_embeddings
     return past_sequence_length, curr_sequence_length, max_sequence_length
 
 
 def get_inputs(args: argparse.Namespace, config: AutoConfig):
     # Dummy values for parity
     world_size = get_size()
     batch_size = 2
-    past_sequence_length, sequence_length, max_sequence_length = get_sequence_lengths(args)
+    past_sequence_length, sequence_length, max_sequence_length = get_sequence_lengths(args, config)
 
     if args.merged:
         inputs = get_merged_sample_with_past_kv_inputs(
             config,
             args.device,
             batch_size,
             seq_len=sequence_length,
             past_seq_len=past_sequence_length,
             max_seq_len=max_sequence_length,
             use_fp16=args.use_fp16,
-            use_gqa=args.use_gqa,
+            use_buffer_share=args.use_buffer_share,
             return_dict=True,
             world_size=world_size,
         )
     elif args.use_past_kv:
         inputs = get_sample_with_past_kv_inputs(
             config,
             args.device,
@@ -68,34 +67,53 @@
     else:
         inputs = get_sample_inputs(config, args.device, batch_size, sequence_length, return_dict=True)
 
     return inputs
 
 
 def verify_parity(
-    args: argparse.Namespace, config: AutoConfig, pt_model: AutoModelForCausalLM, kv_cache_ortvalues: dict
+    args: argparse.Namespace,
+    location: str,
+    use_auth_token: bool,
+    kv_cache_ortvalues: dict,
+    pytorch_model: None | torch.nn.Module = None,
+    config: None | AutoConfig = None,
 ):
+    # If it's running in a machine which GPU memory < 36GB, it should unload the llama in GPU in time and free the GPU memory for ORT.
+    py_model = pytorch_model
+    if py_model is None:
+        config, py_model = setup_torch_model(
+            args,
+            location,
+            use_auth_token,
+            torch_dtype=(torch.float16 if args.use_fp16 else torch.float32),
+            device=args.device,
+        )
+
     inputs = get_inputs(args, config)
 
     # Run inference with PyTorch
     if args.execution_provider != "cpu":
         torch.cuda.synchronize()
     start_time = time.time()
-    pt_outputs = pt_model(**inputs).logits.detach().cpu().numpy()
+    pt_outputs = py_model(**inputs).logits.detach().cpu().numpy()
     if args.execution_provider != "cpu":
         torch.cuda.synchronize()
     end_time = time.time()
     logger.info(f"PyTorch took {end_time - start_time} s")
-    del pt_model
+
+    if args.small_gpu and py_model is not None:
+        del py_model
+        torch.cuda.empty_cache()
 
     # Run inference with ORT
-    past_sequence_length, _, max_sequence_length = get_sequence_lengths(args)
+    past_sequence_length, _, max_sequence_length = get_sequence_lengths(args, config)
     inputs = convert_inputs_for_ort(
         inputs,
-        use_gqa=args.use_gqa,
+        use_buffer_share=args.use_buffer_share,
         past_seq_len=past_sequence_length,
         max_seq_len=max_sequence_length,
         device=args.execution_provider,
         device_id=int(args.rank),
     )
 
     ep = f"{args.execution_provider.upper()}ExecutionProvider"
@@ -107,19 +125,19 @@
         providers=[ep],
     )
 
     # Add IO bindings for non-CPU execution providers
     if args.execution_provider != "cpu":
         io_binding, kv_cache_ortvalues = add_io_bindings_as_ortvalues(
             ort_model,
-            inputs,
-            args.execution_provider,
-            int(args.rank),
-            args.use_gqa,
-            kv_cache_ortvalues,
+            ort_inputs=inputs,
+            device=args.execution_provider,
+            device_id=int(args.rank),
+            use_buffer_share=args.use_buffer_share,
+            kv_cache_ortvalues=kv_cache_ortvalues,
         )
 
         io_binding.synchronize_inputs()
         start_time = time.time()
         ort_model.run_with_iobinding(io_binding)
         io_binding.synchronize_outputs()
         end_time = time.time()
@@ -194,19 +212,19 @@
         action="store_true",
         help="Use past key and past value as inputs to the model. Necessary for decoder_with_past_model.onnx models.",
     )
     parser.set_defaults(use_past_kv=False)
 
     parser.add_argument(
         "-g",
-        "--use_gqa",
+        "--use_buffer_share",
         action="store_true",
-        help="Use if model has GroupQueryAttention",
+        help="Use if model has GroupQueryAttention and you want to enable past-present buffer sharing",
     )
-    parser.set_defaults(use_gqa=False)
+    parser.set_defaults(use_buffer_share=False)
 
     parser.add_argument(
         "--merged",
         action="store_true",
         help="Use merged model (i.e. decoder_merged_model.onnx).",
     )
     parser.set_defaults(merged=False)
@@ -223,14 +241,21 @@
         "--cache_dir",
         required=False,
         type=str,
         default="./model_cache",
         help="model cache dir to override default HF cache dir to avoid overflood the /home dir",
     )
 
+    # The argument is used for CI mainly, because the CI machine has 24G GPU memory at most.
+    parser.add_argument(
+        "--small_gpu",
+        action="store_true",
+        help="Load the llama in GPU every time for parity_check if it's running in a machine which GPU memory < 36GB. ",
+    )
+
     args = parser.parse_args() if argv == [] else parser.parse_args(argv)
 
     # Use FP32 precision for FP32, INT8, INT4 CPU models, use FP16 precision for FP16 and INT4 GPU models
     args.precision = (
         "fp32"
         if args.precision in {"int8", "fp32"} or (args.precision == "int4" and args.execution_provider == "cpu")
         else "fp16"
@@ -248,33 +273,37 @@
     setattr(args, "use_fp16", args.precision == "fp16")  # noqa: B010
     args.rank = rank
     setattr(args, "device_name", "cpu" if args.execution_provider == "cpu" else f"cuda:{rank}")  # noqa: B010
     setattr(args, "device", torch.device(args.device_name))  # noqa: B010
     use_auth_token = args.torch_model_directory == os.path.join(".")
     location = args.model_name if use_auth_token else args.torch_model_directory
 
-    config, llama = setup_torch_model(
-        args,
-        location,
-        use_auth_token,
-        torch_dtype=(torch.float16 if args.use_fp16 else torch.float32),
-        device=args.device,
-    )
-
     kv_cache_ortvalues = {}
     if not args.merged:
-        verify_parity(args, config, llama, kv_cache_ortvalues)
+        verify_parity(args, location, use_auth_token, kv_cache_ortvalues)
     else:
-        # Verify prompt generation in merged model (decoder_model.onnx)
+        config = llama = None
+        if not args.small_gpu:
+            config, llama = setup_torch_model(
+                args,
+                location,
+                use_auth_token,
+                torch_dtype=(torch.float16 if args.use_fp16 else torch.float32),
+                device=args.device,
+            )
+
+        # Verify prompt processing in merged model (decoder_model.onnx)
         args.use_past_kv = False
-        kv_cache_ortvalues = verify_parity(args, config, llama, kv_cache_ortvalues)
+        kv_cache_ortvalues = verify_parity(
+            args, location, use_auth_token, kv_cache_ortvalues, pytorch_model=llama, config=config
+        )
 
         # Verify token generation in merged model (decoder_with_past_model.onnx)
         args.use_past_kv = True
-        verify_parity(args, config, llama, kv_cache_ortvalues)
+        verify_parity(args, location, use_auth_token, kv_cache_ortvalues, pytorch_model=llama, config=config)
 
 
 if __name__ == "__main__":
     seed = 2
     np.random.seed(seed)
     torch.manual_seed(seed)
     main()
```

## onnxruntime/transformers/models/longformer/benchmark_longformer.py

```diff
@@ -285,17 +285,15 @@
         "test_times": test_times,
         "num_threads": num_threads,
         "memory": memory_used,
     }
 
 
 def load_torch_model(model_name, device):
-    torch_model_name_or_dir = (
-        PRETRAINED_LONGFORMER_MODELS[model_name] if model_name in PRETRAINED_LONGFORMER_MODELS else model_name
-    )
+    torch_model_name_or_dir = PRETRAINED_LONGFORMER_MODELS.get(model_name, model_name)
     model = LongformerModel.from_pretrained(torch_model_name_or_dir)
     model.to(device)
     return model
 
 
 def find_onnx_model(model_name, onnx_dir="."):
     # Search onnx model in the following order: optimized fp16 model, optimized fp32 model, raw model
@@ -333,15 +331,15 @@
 
 
 def test_ort(args, device) -> List[Dict[str, Any]]:
     model_name = args.model
 
     onnx_model_path = find_onnx_model(model_name) if not args.onnx else args.onnx
 
-    optimized = onnx_model_path.endswith("_fp16.onnx") or onnx_model_path.endswith("_fp32.onnx")
+    optimized = onnx_model_path.endswith("_fp16.onnx") or onnx_model_path.endswith("_fp32.onnx")  # noqa: PIE810
     precision = "fp32" if not onnx_model_path.endswith("_fp16.onnx") else "fp16"
 
     model = load_torch_model(model_name, device)
 
     num_threads = args.num_threads
 
     cuda_provider_options = {"arena_extend_strategy": "kSameAsRequested"}
@@ -588,15 +586,15 @@
     batch_size=1,
 ):
     compact_memory = "1" if use_compact_memory else "0"
     os.environ["ORT_LONGFORMER_COMPACT_MEMORY"] = compact_memory
     logger.info(f"ORT_LONGFORMER_COMPACT_MEMORY={compact_memory}")
 
     os.environ["ORT_LONGFORMER_USE_HALF4"] = "1" if use_half4 else "0"
-    logger.info("ORT_LONGFORMER_USE_HALF4={}".format("1" if use_half4 else "0"))
+    logger.info("ORT_LONGFORMER_USE_HALF4={}".format("1" if use_half4 else "0"))  # noqa: G001
 
     results = []
     test_times = 1000
     sequence_lengths = [4096, 2048, 1024, 512]
     batch_sizes = [batch_size]
     for model_name in ["longformer-base-4096"]:
         for batch_size in batch_sizes:
```

## onnxruntime/transformers/models/stable_diffusion/benchmark.py

```diff
@@ -311,37 +311,37 @@
 
     if directory is not None and os.path.exists(directory):
         if "xl" in model_name:
             pipeline = ORTStableDiffusionXLPipeline.from_pretrained(
                 directory,
                 provider=provider,
                 session_options=None,
-                use_io_binding=False,
+                use_io_binding=False,  # Not supported by Optimum version 1.17.1 at the time of verification.
             )
         else:
             pipeline = ORTStableDiffusionPipeline.from_pretrained(
                 directory,
                 provider=provider,
-                use_io_binding=False,
+                use_io_binding=False,  # Not supported by Optimum version 1.17.1 at the time of verification.
             )
     elif "xl" in model_name:
         pipeline = ORTStableDiffusionXLPipeline.from_pretrained(
             model_name,
             export=True,
             provider=provider,
             session_options=None,
-            use_io_binding=False,
+            use_io_binding=False,  # Not supported by Optimum version 1.17.1 at the time of verification.
         )
         pipeline.save_pretrained(directory)
     else:
         pipeline = ORTStableDiffusionPipeline.from_pretrained(
             model_name,
             export=True,
             provider=provider,
-            use_io_binding=False,
+            use_io_binding=False,  # Not supported by Optimum version 1.17.1 at the time of verification.
         )
         pipeline.save_pretrained(directory)
 
     if disable_safety_checker:
         pipeline.safety_checker = None
         pipeline.feature_extractor = None
```

## onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py

```diff
@@ -28,21 +28,16 @@
     get_metadata,
     load_pipelines,
     parse_arguments,
     process_controlnet_arguments,
     repeat_prompt,
 )
 
-if __name__ == "__main__":
-    coloredlogs.install(fmt="%(funcName)20s: %(message)s")
-
-    parser = arg_parser("Options for Stable Diffusion Demo")
-    add_controlnet_arguments(parser)
-    args = parse_arguments(is_xl=False, parser=parser)
 
+def main(args):
     controlnet_images, controlnet_scale = process_controlnet_arguments(args)
 
     pipeline, refiner = load_pipelines(args)
     assert refiner is None
 
     prompt, negative_prompt = repeat_prompt(args)
     batch_size = len(prompt)
@@ -57,14 +52,15 @@
             denoising_steps=args.denoising_steps,
             guidance=args.guidance,
             seed=args.seed,
             controlnet_images=controlnet_images,
             controlnet_scales=controlnet_scale,
             show_latency=not warmup,
             output_type="pil",
+            deterministic=args.deterministic,
         )
 
     if not args.disable_cuda_graph:
         # inference once to get cuda graph
         _, _ = run_inference(warmup=True)
 
     print("[I] Warming up ..")
@@ -83,7 +79,24 @@
     if perf_data:
         metadata.update(perf_data)
     metadata["images"] = len(images)
     print(metadata)
     pipeline.save_images(images, prompt, negative_prompt, metadata)
 
     pipeline.teardown()
+
+
+if __name__ == "__main__":
+    coloredlogs.install(fmt="%(funcName)20s: %(message)s")
+
+    parser = arg_parser("Options for Stable Diffusion Demo")
+    add_controlnet_arguments(parser)
+    args = parse_arguments(is_xl=False, parser=parser)
+
+    if args.user_compute_stream:
+        import torch
+
+        s = torch.cuda.Stream()
+        with torch.cuda.stream(s):
+            main(args)
+    else:
+        main(args)
```

## onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py

```diff
@@ -128,17 +128,19 @@
     run_pipelines(args, base, refiner, prompt, negative_prompt, controlnet_image, controlnet_scale)
     base.teardown()
     if refiner:
         refiner.teardown()
 
 
 def run_dynamic_shape_demo(args):
-    """Run demo of generating images with different settings with ORT CUDA provider."""
+    """
+    Run demo of generating images with different settings with ORT CUDA provider.
+    Try "python demo_txt2img_xl.py --max-cuda-graphs 3 --user-compute-stream" to see the effect of multiple CUDA graphs.
+    """
     args.engine = "ORT_CUDA"
-    args.disable_cuda_graph = True
     base, refiner = load_pipelines(args, 1)
 
     prompts = [
         "starry night over Golden Gate Bridge by van gogh",
         "beautiful photograph of Mt. Fuji during cherry blossom",
         "little cute gremlin sitting on a bed, cinematic",
         "cute grey cat with blue eyes, wearing a bowtie, acrylic painting",
@@ -212,15 +214,14 @@
     if refiner:
         refiner.teardown()
 
 
 def run_turbo_demo(args):
     """Run demo of generating images with test prompts with ORT CUDA provider."""
     args.engine = "ORT_CUDA"
-    args.disable_cuda_graph = True
     base, refiner = load_pipelines(args, 1)
 
     from datasets import load_dataset
 
     dataset = load_dataset("Gustavosta/Stable-Diffusion-Prompts")
     num_rows = dataset["test"].num_rows
     batch_size = args.batch_size
@@ -235,22 +236,33 @@
         run_pipelines(args, base, refiner, prompt, negative_prompt, is_warm_up=False)
 
     base.teardown()
     if refiner:
         refiner.teardown()
 
 
-if __name__ == "__main__":
-    coloredlogs.install(fmt="%(funcName)20s: %(message)s")
-
-    parser = arg_parser("Options for Stable Diffusion XL Demo")
-    add_controlnet_arguments(parser)
-    args = parse_arguments(is_xl=True, parser=parser)
-
+def main(args):
     no_prompt = isinstance(args.prompt, list) and len(args.prompt) == 1 and not args.prompt[0]
     if no_prompt:
         if args.version == "xl-turbo":
             run_turbo_demo(args)
         else:
             run_dynamic_shape_demo(args)
     else:
         run_demo(args)
+
+
+if __name__ == "__main__":
+    coloredlogs.install(fmt="%(funcName)20s: %(message)s")
+
+    parser = arg_parser("Options for Stable Diffusion XL Demo")
+    add_controlnet_arguments(parser)
+    args = parse_arguments(is_xl=True, parser=parser)
+
+    if args.user_compute_stream:
+        import torch
+
+        s = torch.cuda.Stream()
+        with torch.cuda.stream(s):
+            main(args)
+    else:
+        main(args)
```

## onnxruntime/transformers/models/stable_diffusion/demo_utils.py

```diff
@@ -19,15 +19,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # --------------------------------------------------------------------------
 import argparse
 import os
 import sys
 from importlib.metadata import PackageNotFoundError, version
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional
 
 import controlnet_aux
 import cv2
 import numpy as np
 import torch
 from cuda import cudart
 from diffusion_models import PipelineInfo
@@ -235,18 +235,23 @@
 
     # Inference related options
     parser.add_argument(
         "-nw", "--num-warmup-runs", type=int, default=5, help="Number of warmup runs before benchmarking performance."
     )
     parser.add_argument("--nvtx-profile", action="store_true", help="Enable NVTX markers for performance profiling.")
     parser.add_argument("--seed", type=int, default=None, help="Seed for random generator to get consistent results.")
+    parser.add_argument("--deterministic", action="store_true", help="use deterministic algorithms.")
     parser.add_argument("-dc", "--disable-cuda-graph", action="store_true", help="Disable cuda graph.")
 
+    parser.add_argument("--framework-model-dir", default=None, help="framework model directory")
+
     group = parser.add_argument_group("Options for ORT_CUDA engine only")
     group.add_argument("--enable-vae-slicing", action="store_true", help="True will feed only one image to VAE once.")
+    group.add_argument("--max-cuda-graphs", type=int, default=1, help="Max number of cuda graphs to use. Default 1.")
+    group.add_argument("--user-compute-stream", action="store_true", help="Use user compute stream.")
 
     # TensorRT only options
     group = parser.add_argument_group("Options for TensorRT (--engine=TRT) only")
     group.add_argument(
         "--build-all-tactics", action="store_true", help="Build TensorRT engines using all tactic sources."
     )
 
@@ -393,22 +398,24 @@
     use_cuda_graph=True,
     build_dynamic_batch=False,
     build_dynamic_shape=False,
     min_image_size: int = 512,
     max_image_size: int = 1024,
     max_batch_size: int = 16,
     opt_batch_size: int = 1,
-    build_all_tactics=False,
-    do_classifier_free_guidance=False,
-    lcm=False,
+    build_all_tactics: bool = False,
+    do_classifier_free_guidance: bool = False,
+    lcm: bool = False,
     controlnet=None,
     lora_weights=None,
-    lora_scale=1.0,
-    use_fp16_vae=True,
-    use_vae=True,
+    lora_scale: float = 1.0,
+    use_fp16_vae: bool = True,
+    use_vae: bool = True,
+    framework_model_dir: Optional[str] = None,
+    max_cuda_graphs: int = 1,
 ):
     pipeline_info = PipelineInfo(
         version,
         is_refiner=is_refiner,
         is_inpaint=is_inpaint,
         use_vae=use_vae,
         min_image_size=min_image_size,
@@ -420,15 +427,15 @@
         lora_weights=lora_weights,
         lora_scale=lora_scale,
     )
 
     input_engine_dir = engine_dir
 
     onnx_dir, engine_dir, output_dir, framework_model_dir, timing_cache = get_engine_paths(
-        work_dir=work_dir, pipeline_info=pipeline_info, engine_type=engine_type
+        work_dir=work_dir, pipeline_info=pipeline_info, engine_type=engine_type, framework_model_dir=framework_model_dir
     )
 
     pipeline = StableDiffusionPipeline(
         pipeline_info,
         scheduler=scheduler,
         output_dir=output_dir,
         verbose=False,
@@ -457,14 +464,15 @@
         pipeline.backend.build_engines(
             engine_dir=engine_dir,
             framework_model_dir=framework_model_dir,
             onnx_dir=onnx_dir,
             tmp_dir=os.path.join(work_dir or ".", engine_type.name, pipeline_info.short_name(), "tmp"),
             device_id=torch.cuda.current_device(),
             import_engine_dir=import_engine_dir,
+            max_cuda_graphs=max_cuda_graphs,
         )
     elif engine_type == EngineType.ORT_TRT:
         pipeline.backend.build_engines(
             engine_dir,
             framework_model_dir,
             onnx_dir,
             onnx_opset,
@@ -553,14 +561,16 @@
         "build_all_tactics": args.build_all_tactics,
         "do_classifier_free_guidance": args.guidance > 1.0,
         "controlnet": args.controlnet_type,
         "lora_weights": args.lora_weights,
         "lora_scale": args.lora_scale,
         "use_fp16_vae": "xl" in args.version,
         "use_vae": True,
+        "framework_model_dir": args.framework_model_dir,
+        "max_cuda_graphs": args.max_cuda_graphs,
     }
 
     if "xl" in args.version:
         params["lcm"] = args.lcm
         params["use_vae"] = not args.enable_refiner
     base = initialize_pipeline(**params)
```

## onnxruntime/transformers/models/stable_diffusion/diffusion_models.py

```diff
@@ -410,15 +410,14 @@
             else:
                 profile_id += f"_h_{min_image_height}_{max_image_height}_w_{min_image_width}_{max_image_width}"
 
         return profile_id
 
     def get_input_profile(self, batch_size, image_height, image_width, static_batch, static_image_shape):
         """For TensorRT"""
-        pass
 
     def get_shape_dict(self, batch_size, image_height, image_width):
         pass
 
     def fp32_input_output_names(self) -> List[str]:
         """For CUDA EP, we export ONNX model with FP32 first, then convert it to mixed precision model.
         This is a list of input or output names that are kept as float32 in optimized model.
```

## onnxruntime/transformers/models/stable_diffusion/engine_builder.py

```diff
@@ -1,14 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import hashlib
 import os
 from enum import Enum
+from typing import Optional
 
 import torch
 from diffusion_models import CLIP, VAE, CLIPWithProj, PipelineInfo, UNet, UNetXL
 
 
 class EngineType(Enum):
     ORT_CUDA = 0  # ONNX Runtime CUDA Execution Provider
@@ -87,15 +88,15 @@
         name_mapping = {
             "clip": "text_encoder",
             "clip2": "text_encoder_2",
             "unet": "unet",
             "unetxl": "unet",
             "vae": "vae_decoder",
         }
-        return name_mapping[model_name] if model_name in name_mapping else model_name
+        return name_mapping.get(model_name, model_name)
 
     def get_cached_model_name(self, model_name):
         model_name = self.get_diffusers_module_name(model_name)
         is_unet = model_name == "unet"
         hash_source = []
         if model_name in ["text_encoder", "text_encoder_2", "unet"] and self.pipeline_info.lora_weights:
             if self.pipeline_info.lora_weights in [
@@ -269,24 +270,27 @@
             # The output tensor points to same buffer. Need clone it to avoid overwritten.
             decoded_slices = [self._vae_decode(z_slice).clone() for z_slice in latents.split(1)]
             return torch.cat(decoded_slices)
 
         return self._vae_decode(latents)
 
 
-def get_engine_paths(work_dir: str, pipeline_info: PipelineInfo, engine_type: EngineType):
+def get_engine_paths(
+    work_dir: str, pipeline_info: PipelineInfo, engine_type: EngineType, framework_model_dir: Optional[str] = None
+):
     root_dir = work_dir or "."
     short_name = pipeline_info.short_name()
 
     # When both ORT_CUDA and ORT_TRT/TRT is used, we shall make sub directory for each engine since
     # ORT_CUDA need fp32 torch model, while ORT_TRT/TRT use fp16 torch model.
     onnx_dir = os.path.join(root_dir, engine_type.name, short_name, "onnx")
     engine_dir = os.path.join(root_dir, engine_type.name, short_name, "engine")
     output_dir = os.path.join(root_dir, engine_type.name, short_name, "output")
 
     timing_cache = os.path.join(root_dir, engine_type.name, "timing_cache")
 
     # Shared among ORT_CUDA, ORT_TRT and TRT engines, and need use load_model(..., always_download_fp16=True)
     # So that the shared model is always fp16.
-    framework_model_dir = os.path.join(root_dir, "torch_model")
+    if framework_model_dir is None:
+        framework_model_dir = os.path.join(root_dir, "torch_model")
 
     return onnx_dir, engine_dir, output_dir, framework_model_dir, timing_cache
```

## onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py

```diff
@@ -2,42 +2,42 @@
 # Copyright (c) Microsoft Corporation.  All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import gc
 import logging
 import os
-from typing import List, Optional
+from typing import Dict, List, Optional
 
 import onnx
 import torch
 from diffusion_models import PipelineInfo
 from engine_builder import EngineBuilder, EngineType
 from packaging import version
 
 import onnxruntime as ort
-from onnxruntime.transformers.io_binding_helper import CudaSession
+from onnxruntime.transformers.io_binding_helper import CudaSession, GpuBindingManager
 from onnxruntime.transformers.onnx_model import OnnxModel
 
 logger = logging.getLogger(__name__)
 
 
-class OrtCudaEngine(CudaSession):
+class OrtCudaEngine:
     def __init__(
         self,
         onnx_path,
         device_id: int = 0,
         enable_cuda_graph: bool = False,
         disable_optimization: bool = False,
+        max_cuda_graphs: int = 1,
     ):
         self.onnx_path = onnx_path
         self.provider = "CUDAExecutionProvider"
-        self.provider_options = CudaSession.get_cuda_provider_options(device_id, enable_cuda_graph)
-        # self.provider_options["enable_skip_layer_norm_strict_mode"] = True
-
+        self.stream = torch.cuda.current_stream().cuda_stream
+        self.provider_options = CudaSession.get_cuda_provider_options(device_id, enable_cuda_graph, self.stream)
         session_options = ort.SessionOptions()
 
         # When the model has been optimized by onnxruntime, we can disable optimization to save session creation time.
         if disable_optimization:
             session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
 
         logger.info("creating CUDA EP session for %s", onnx_path)
@@ -48,18 +48,41 @@
                 (self.provider, self.provider_options),
                 "CPUExecutionProvider",
             ],
         )
         logger.info("created CUDA EP session for %s", onnx_path)
 
         device = torch.device("cuda", device_id)
-        super().__init__(ort_session, device, enable_cuda_graph)
+        self.enable_cuda_graph = enable_cuda_graph
+
+        # Support multiple CUDA graphs for different input shapes.
+        # For clip2 model that disabled cuda graph, max_cuda_graphs is updated to 0 here.
+        self.gpu_binding_manager = GpuBindingManager(
+            ort_session=ort_session,
+            device=device,
+            stream=self.stream,
+            max_cuda_graphs=max_cuda_graphs if enable_cuda_graph else 0,
+        )
+
+        self.current_gpu_binding = None
+
+    def metadata(self, name: str):
+        data = {}
+        if self.current_gpu_binding is not None:
+            if self.current_gpu_binding.last_run_gpu_graph_id >= 0:
+                data[f"{name}.gpu_graph_id"] = self.current_gpu_binding.last_run_gpu_graph_id
+        return data
+
+    def infer(self, feed_dict: Dict[str, torch.Tensor]):
+        return self.current_gpu_binding.infer(feed_dict=feed_dict, disable_cuda_graph_in_run=not self.enable_cuda_graph)
 
     def allocate_buffers(self, shape_dict, device):
-        super().allocate_buffers(shape_dict)
+        self.current_gpu_binding = self.gpu_binding_manager.get_binding(
+            shape_dict=shape_dict, use_cuda_graph=self.enable_cuda_graph
+        )
 
 
 class _ModelConfig:
     """
     Configuration of one model (like Clip, UNet etc) on ONNX export and optimization for CUDA provider.
     For example, if you want to use fp32 in layer normalization, set the following:
         force_fp32_ops=["SkipLayerNormalization", "LayerNormalization"]
@@ -216,14 +239,15 @@
         framework_model_dir: str,
         onnx_dir: str,
         tmp_dir: Optional[str] = None,
         onnx_opset_version: int = 17,
         device_id: int = 0,
         save_fp32_intermediate_model: bool = False,
         import_engine_dir: Optional[str] = None,
+        max_cuda_graphs: int = 1,
     ):
         self.torch_device = torch.device("cuda", device_id)
         self.load_models(framework_model_dir)
 
         if not os.path.isdir(engine_dir):
             os.makedirs(engine_dir)
 
@@ -348,14 +372,15 @@
             use_cuda_graph = self.model_config[model_name].use_cuda_graph
 
             engine = OrtCudaEngine(
                 onnx_opt_path,
                 device_id=device_id,
                 enable_cuda_graph=use_cuda_graph,
                 disable_optimization=False,
+                max_cuda_graphs=max_cuda_graphs,
             )
 
             logger.info("%s options for %s: %s", engine.provider, model_name, engine.provider_options)
             built_engines[model_name] = engine
 
         self.engines = built_engines
```

## onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py

```diff
@@ -543,22 +543,28 @@
     def pt_to_numpy(images: torch.FloatTensor):
         """
         Convert a PyTorch tensor to a NumPy image.
         """
         return ((images + 1) / 2).clamp(0, 1).detach().permute(0, 2, 3, 1).float().cpu().numpy()
 
     def metadata(self) -> Dict[str, Any]:
-        return {
+        data = {
             "actual_steps": self.actual_steps,
             "seed": self.get_current_seed(),
             "name": self.pipeline_info.name(),
             "custom_vae": self.pipeline_info.custom_fp16_vae(),
             "custom_unet": self.pipeline_info.custom_unet(),
         }
 
+        if self.engine_type == EngineType.ORT_CUDA:
+            for engine_name, engine in self.backend.engines.items():
+                data.update(engine.metadata(engine_name))
+
+        return data
+
     def save_images(self, images: List, prompt: List[str], negative_prompt: List[str], metadata: Dict[str, Any]):
         session_id = str(random.randint(1000, 9999))
         for i, image in enumerate(images):
             seed = str(self.get_current_seed())
             prefix = "".join(x for x in prompt[i] if x.isalnum() or x in ", -").replace(" ", "_")[:20]
             parts = [prefix, session_id, str(i + 1), str(seed), self.current_scheduler, str(self.actual_steps)]
             image_path = os.path.join(self.output_dir, "-".join(parts) + ".png")
@@ -750,14 +756,15 @@
         seed: Optional[int] = None,
         image: Optional[torch.Tensor] = None,
         strength: float = 0.3,
         controlnet_images: Optional[torch.Tensor] = None,
         controlnet_scales: Optional[torch.Tensor] = None,
         show_latency: bool = False,
         output_type: str = "pil",
+        deterministic: bool = False,
     ):
         """
         Run the diffusion pipeline.
 
         Args:
             prompt (List[str]):
                 The text prompt to guide image generation.
@@ -779,14 +786,17 @@
                 Indicates extent to transform the reference image, which is used as a starting point,
                 and more noise is added the higher the strength.
             show_latency (bool):
                 Whether return latency data.
             output_type (str):
                 It can be "latent", "pt" or "pil".
         """
+        if deterministic:
+            torch.use_deterministic_algorithms(True)
+
         if self.is_backend_tensorrt():
             import tensorrt as trt
             from trt_utilities import TRT_LOGGER
 
             with trt.Runtime(TRT_LOGGER):
                 return self._infer(
                     prompt,
```

## onnxruntime/transformers/models/whisper/benchmark.py

```diff
@@ -141,18 +141,18 @@
         # Optimum export
         provider = args.execution_provider[0] if type(args.execution_provider) is tuple else args.execution_provider
         provider_options = args.execution_provider[1] if type(args.execution_provider) is tuple else None
 
         start_time = time.time()
         model = ORTModelForSpeechSeq2Seq.from_pretrained(
             args.hf_ort_dir_path,
-            use_io_binding=(args.device != "cpu"),
             provider=provider,
             provider_options=provider_options,
             session_options=sess_options,
+            use_io_binding=True,  # Avoid memory copy overhead
         )
         end_time = time.time()
 
     if args.benchmark_type == "ort":
         # convert_to_onnx.py export
         logger.info(f"Loading model from {args.ort_model_path}")
         start_time = time.time()
@@ -406,15 +406,16 @@
         # ONNX E2E model from Olive produces transcribed output
         logger.info(f"Transcription: {ort_outputs[0][0]}")
     else:
         # convert_to_onnx model produces generated ids
         actual_output = handle_output(ort_outputs[0][0])
         logger.info(f"Generated token length: {len(actual_output)} tokens")
         transcription = args.processor.batch_decode(ort_outputs[0], skip_special_tokens=True)[0]
-        logger.info(f"Transcription: {transcription}")
+        # print to stdout as the output for comparison
+        print(f"{transcription}")
 
     measure_fn(args, generate_fn, ort_inputs)
 
 
 def run_inference(args, inputs, model):
     if args.benchmark_type in {"hf-pt-eager", "hf-pt-compile", "hf-ort"}:
         run_hf_inference(args, inputs, model)
```

## onnxruntime/transformers/models/whisper/convert_to_onnx.py

```diff
@@ -410,17 +410,17 @@
                     )
                     onnx_path = output_path
 
                 if precision == Precision.INT8:
                     quantization.quantize_dynamic(
                         onnx_path,
                         output_path,
-                        op_types_to_quantize=["MatMul", "Gemm", "Gather"]
-                        if quantize_embedding_layer
-                        else ["MatMul", "Gemm"],
+                        op_types_to_quantize=(
+                            ["MatMul", "Gemm", "Gather"] if quantize_embedding_layer else ["MatMul", "Gemm"]
+                        ),
                         use_external_data_format=use_external_data_format,
                         per_channel=quantize_per_channel,
                         reduce_range=quantize_reduce_range,
                         extra_options={"MatMulConstBOnly": True},
                     )
             else:
                 logger.info(f"Skip optimizing: existing ONNX model {onnx_path}")
```

## onnxruntime/transformers/models/whisper/whisper_helper.py

```diff
@@ -321,20 +321,20 @@
         batch_size: int = 1,
         prompt_mode: bool = False,
     ):
         # Try to import `datasets` pip package
         try:
             from datasets import load_dataset
         except Exception as e:
-            logger.error(f"An error occurred while importing `datasets`: {e}", exc_info=True)
+            logger.error(f"An error occurred while importing `datasets`: {e}", exc_info=True)  # noqa: G201
             install_cmd = "pip install datasets"
             logger.warning(f"Could not import `datasets`. Attempting to install `datasets` via `{install_cmd}`.")
             os.system(install_cmd)
 
-        from datasets import load_dataset  # noqa: F811
+        from datasets import load_dataset
 
         ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
         input_features_ = []
         if batch_size == 1:
             input_features = processor([ds[0]["audio"]["array"]], return_tensors="pt").input_features
         else:
             input_features_ = [
```

## Comparing `onnxruntime_training_cpu-1.17.3.dist-info/METADATA` & `onnxruntime_training_cpu-1.18.0.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: onnxruntime-training-cpu
-Version: 1.17.3
+Version: 1.18.0
 Summary: ONNX Runtime is a runtime accelerator for Machine Learning models
 Home-page: https://onnxruntime.ai
 Download-URL: https://github.com/microsoft/onnxruntime/tags
 Author: Microsoft Corporation
 Author-email: onnxruntime@microsoft.com
 License: MIT License
 Keywords: onnx machine learning
@@ -44,28 +44,18 @@
 ONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.
 For more information on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_ or the `Github project <https://github.com/microsoft/onnxruntime/>`_.
 
 
 Changes
 -------
 
-1.17.3
+1.18.0
 ^^^^^^
 
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.3
-
-1.17.2
-^^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.2
-
-1.17.1
-^^^^^^
-
-Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.1
+Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.18.0
 
 1.17.0
 ^^^^^^
 
 Release Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.0
 
 1.16.0
```

## Comparing `onnxruntime_training_cpu-1.17.3.dist-info/RECORD` & `onnxruntime_training_cpu-1.18.0.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,254 +1,259 @@
 onnxruntime/LICENSE,sha256=wlDWJ48LR6ZDn7dZKwi1ilXrn1NapJodtjIRw_mCtnQ,1094
 onnxruntime/Privacy.md,sha256=v7dxKwdfPwfj6-5dwqKW0d4y2_ca0oZj9z0VOMtsOwg,2490
-onnxruntime/ThirdPartyNotices.txt,sha256=mGEBlz72r0WcNoRfYmeR2kya5Sva1KmAmJryxqXLWmE,345046
-onnxruntime/__init__.py,sha256=o-QUnRBl_af-iz8q8gy6CO_jEH6IGHEziteBkAAfTFA,4367
+onnxruntime/ThirdPartyNotices.txt,sha256=fhblebZUcLHgp2RXQTmBoZnXFhfuKYZvfUHCOmoA_us,345046
+onnxruntime/__init__.py,sha256=YQsrEUDiObQ6zlFjK-y2AOj7pU65gFk7hzxgu6Tfv0Y,4367
 onnxruntime/backend/__init__.py,sha256=5I1Ylsawf9w6MNmK4RiN1wA-EEQqlKKwYTNZB-m_k6M,334
 onnxruntime/backend/backend.py,sha256=SKFwZi8cQsR8HgCDpXeqMERIrqtgTHXxXrdZbIuwps0,8121
 onnxruntime/backend/backend_rep.py,sha256=8Hid8lLPmcBtXsEUfpXsamX0pN5XATIIun-U7A6oNmk,1821
 onnxruntime/capi/__init__.py,sha256=uRp4pMtfoayBhZgEsiFqFCD13Y6LUo82FdZsQX8X8LI,251
 onnxruntime/capi/_ld_preload.py,sha256=li6cbZ64hDfUndat4mprUWzowLa3RQdw0q2E56sXFwE,413
 onnxruntime/capi/_pybind_state.py,sha256=nbUpnUncwBv5pgJA8yugDYJRA4TTfC0gaYOED5jD-SA,1533
-onnxruntime/capi/build_and_package_info.py,sha256=BhRVAKw07p87mm9TlcXh4GfIT4glEDdvt2dusLNvUTc,67
+onnxruntime/capi/build_and_package_info.py,sha256=NvFkt_R7LLGd0jNyvWMEVHiP1K03x74mojdX0a86tVE,67
 onnxruntime/capi/onnxruntime_collect_build_info.py,sha256=N7ViCgTVKYLPiHXhf16ZkGK2FVNB3PzfWFLU4ykP28w,4068
-onnxruntime/capi/onnxruntime_inference_collection.py,sha256=3hTC6DKcpQbATIGkqnjcNNjNDOw2WBmnbax9L7y1vEk,42503
-onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=YMJ2oy7kBqy9qGq4agSTVB6OGXnfKLEYfq8Z_vnoirg,21936
-onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=0pjZnpZlYIVb3RsVp2jBypQOlFTDxg7TK_aJzqZtyAw,16793632
+onnxruntime/capi/onnxruntime_inference_collection.py,sha256=DvLaW-VGhNIWHT0IYCoCUPIR6o7Q8jNBidOuy3a2G1A,42452
+onnxruntime/capi/onnxruntime_providers_shared.dll,sha256=BEzr9tWIQLIChoBjmOl4OgeCbMhupagX-6Jr60sZZEs,22048
+onnxruntime/capi/onnxruntime_pybind11_state.pyd,sha256=cpppA0fybmjGi6teB92hwl8ECYrbG4MfxXm_ej24Y7s,16732192
 onnxruntime/capi/onnxruntime_validation.py,sha256=SP9G46H-OKpuy_p68r3r3qs_23yLhF2aU1mAj5Ny4pQ,6394
 onnxruntime/capi/pt_patch.py,sha256=hSKcZ9mAgLZMNVJXWlkJsucX1e8hYOwIuY7BEd_zWkE,2024
 onnxruntime/capi/version_info.py,sha256=8mm1VTXF8xgx6N8vFNe0Tiik9qdg9Vvi9f32bPE9ktw,34
 onnxruntime/datasets/__init__.py,sha256=0D1rdhXK940JccUq3Sj4BBMqjDpAPOcxlGcwJR4X3wc,471
 onnxruntime/datasets/logreg_iris.onnx,sha256=giR4TJjXNBLZ_ZmrzVejhWi9WQmA0PvlkWRkUxxS6Pw,670
 onnxruntime/datasets/mul_1.onnx,sha256=cfQxxOkyHsb76xWNAu0kBFmn3MmGc_p5pPQ5zkLvrxA,130
 onnxruntime/datasets/sigmoid.onnx,sha256=U0Crpnp-NHUWKteUN4r1XxcY9V-aXXS0r2Dsx_emJLY,103
 onnxruntime/quantization/__init__.py,sha256=eeIgS5jf18UjGelvD4Bf57Z6-Qxvg6J54V-PEtlcww0,686
-onnxruntime/quantization/calibrate.py,sha256=wEU4B5m8CZsFx2yrz5YkDgXQtiKxUe3_Uy3HaEoVFTQ,50946
-onnxruntime/quantization/matmul_4bits_quantizer.py,sha256=uqbXGXtvFFLzhDyligXS-nCayFlmJ_DBvg70d9xgWiU,16364
-onnxruntime/quantization/matmul_bnb4_quantizer.py,sha256=gvVRQFvIQPh8In4AdsHKVDzjo7_E23tqqRXhAMnkABU,9307
-onnxruntime/quantization/onnx_model.py,sha256=DhWf5-BkDZFZR26yjhvkMyYG1wIMREpC1_KSKh7yaRw,22628
-onnxruntime/quantization/onnx_quantizer.py,sha256=e7MPJpscq67-S5a_iqRn6cqdp3PIPLFnw2SRSBvKeis,66463
+onnxruntime/quantization/base_quantizer.py,sha256=ISPGK6UHxQaBlkgQvgUMRVVqVhlEHz3sxhjH1TcZPqg,24351
+onnxruntime/quantization/calibrate.py,sha256=2n8NFxCAFZyk5tjhyD_LtCIuZaaZf18GuIcrLYR4dko,50272
+onnxruntime/quantization/matmul_4bits_quantizer.py,sha256=BT4Mp6_nJeoUmo8AZ2_POHy6IXZgQPPDX8RR18DIdOI,28348
+onnxruntime/quantization/matmul_bnb4_quantizer.py,sha256=DVEK5seZJgdaN59Zwap_MyJZDQGxH_w8hhib_hz1mHQ,9300
+onnxruntime/quantization/onnx_model.py,sha256=5gqbni3PsYlTZKsQG8n7UDN8BTOy38KfdsOorHP1EK8,23689
+onnxruntime/quantization/onnx_quantizer.py,sha256=t7oFtAjRXE9ftrduVelVqTVXC_odhUPJ8QaUHwmcWjQ,43942
 onnxruntime/quantization/preprocess.py,sha256=VU4iX7g8gOgVH0zehOcOXsVWkZpx6kG_LFlwGM3Bs6c,5045
 onnxruntime/quantization/qdq_loss_debug.py,sha256=bQQvqzs24zQWRM3qmI97j3LKOKdExBDZ1fzb-xMtSdo,15887
-onnxruntime/quantization/qdq_quantizer.py,sha256=yMQQObgZHwkYYEJani4N6jvUI30N96Fh1QR3pyaePZ4,24144
-onnxruntime/quantization/quant_utils.py,sha256=xdd6cfCpigOS0qxJHU2EyYeaS-tLxjOhvqpuMGNNWzY,28779
-onnxruntime/quantization/quantize.py,sha256=kljbOCjZ2ePky5VOZVQajTsVMHpFphxYPl4llC_vtPM,38218
-onnxruntime/quantization/registry.py,sha256=X0GVwFZYaf4zN315L5zLSaPo5hHbSQVwEa7anE2XJ1s,3696
-onnxruntime/quantization/shape_inference.py,sha256=hsOJnsBFH5VKhCgibE9HZUnzLXdj2f8vrDsJvdqRjek,6835
+onnxruntime/quantization/qdq_quantizer.py,sha256=uagu3kHpzUhl24EXoYRrhHROKZ0BoShXJnJzLcmgVWM,54690
+onnxruntime/quantization/quant_utils.py,sha256=mD-onHdO9uwmMAbHq0LkZIJXOM3JJf59Ly3-70bZi2o,30255
+onnxruntime/quantization/quantize.py,sha256=yhkNNweBq7sCxIVb--JOQMRTE1_bSbZ2p_rBeS9MoW8,39055
+onnxruntime/quantization/registry.py,sha256=PY89m9Dlj-y-Vkjp5zh-9B49K7UEpMSv8iwsDJWSRcI,3656
+onnxruntime/quantization/shape_inference.py,sha256=9fJEciXXRizCyuVsys0GZOiwGr_d6UtqKmaqzD_p9lU,8711
+onnxruntime/quantization/tensor_quant_overrides.py,sha256=8lUZht34tml0x1AkOGuIOh1PLCub9TBNiOKBd7AG2oA,21101
 onnxruntime/quantization/CalTableFlatBuffers/KeyValue.py,sha256=e-jJFhw9fb775fDCLnWdbRSdoJ6vGD0c7qTnkIG-vNs,2250
 onnxruntime/quantization/CalTableFlatBuffers/TrtTable.py,sha256=QQ9_f60Wya8U-KQOMu0gXImfhiPN6jNkfjpoCdAFic4,2665
 onnxruntime/quantization/CalTableFlatBuffers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/quantization/execution_providers/qnn/__init__.py,sha256=nKKB7VEbO574HDL2xdJPD8VeXoK2a3jd8nLBxULiVvI,120
-onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py,sha256=1ArT-Bnfn0VhbxUUwk3P3XKesz3ruIhROQEH3FhqR6Y,5224
-onnxruntime/quantization/execution_providers/qnn/preprocess.py,sha256=MFIvUsdAH8MKqxor1FVY-XPKG7kAqZzTwuUERuNhDec,1992
-onnxruntime/quantization/execution_providers/qnn/quant_config.py,sha256=756a_KtU3kNXhh2S6cFcV73jswmhs5u5BSPkMFyr1FM,4123
+onnxruntime/quantization/execution_providers/qnn/fusion_lpnorm.py,sha256=vUrbMNorHH7_uKjeL1jlkPghnplPIDPz0kmN0Tt03mc,5327
+onnxruntime/quantization/execution_providers/qnn/mixed_precision_overrides_utils.py,sha256=vsZC2rwLA7Z9kYUF1loF_YZP_BO0glv70Dnx8VB4SZw,19000
+onnxruntime/quantization/execution_providers/qnn/preprocess.py,sha256=qLSdYvc18-u0h3CWaC8DKNWRmjdrb9Qup8nYk-DHiCM,14192
+onnxruntime/quantization/execution_providers/qnn/quant_config.py,sha256=ED4yUT_TBnAwTLWF7XfLATPOpcAXM0UGMrvneXiHVY0,17829
 onnxruntime/quantization/fusions/__init__.py,sha256=UMhvt6fL-eI4iadRoWpuFSktJRvNJjmGd5Rqw4nsFzY,163
-onnxruntime/quantization/fusions/fusion.py,sha256=Ygsft_guFZavP8Dc0b-JjjUlcMnKMLdu5UxTdBEiCb0,11494
-onnxruntime/quantization/fusions/fusion_gelu.py,sha256=IFTk8wLs5wbkTL8tzszUZVBTsTEzRecjCFNhRL_G_wE,10637
-onnxruntime/quantization/fusions/fusion_layernorm.py,sha256=8CAiD7kpzyUCmISCyRYjliXVdnIGoR0977nPoDyT4C4,5256
+onnxruntime/quantization/fusions/fusion.py,sha256=A6_77l5uw-hIwyoX7DPOFL6O-y3qXk-S16SMLv1Ncis,12088
+onnxruntime/quantization/fusions/fusion_gelu.py,sha256=3qOO4U95ATD6S14dyC-5-vGeaQBr5U-GCsjfvHqoL98,10647
+onnxruntime/quantization/fusions/fusion_layernorm.py,sha256=CKU--IH-xDUnm5qZtTK1ENYuBMnPsADUkzrOBjyW7kQ,5306
 onnxruntime/quantization/operators/__init__.py,sha256=IfKXrFWtRSye1mkgD9lpwxio0fw9cVr_1CdV1cvefig,85
 onnxruntime/quantization/operators/activation.py,sha256=JMkSthxHxIJe4wDnzhxi9nXmSdIG2Q98E7ahxXp3llM,4463
 onnxruntime/quantization/operators/argmax.py,sha256=pfE9_eSTZ2otTkUcWwlLi7HJKtN10kE5c2Lz0SeVADQ,589
 onnxruntime/quantization/operators/attention.py,sha256=eH7-Z3MfP6xRZCdhDAyNxWG2s2nZILxIEFVAHtqj7EQ,2637
 onnxruntime/quantization/operators/base_operator.py,sha256=vrAVfKJXZvF7ZherKw4JUGonNyNuoU2TWnwBy-EQ3QE,1118
 onnxruntime/quantization/operators/binary_op.py,sha256=pEQHRAS75EMp7LG6jzWV7gDQt_vzEPLJEI00eIOuoiA,2544
-onnxruntime/quantization/operators/concat.py,sha256=F8hZfd6dcnU-J2BxMHJj2FL1AIxabHIuOyFybSh20Xk,2149
-onnxruntime/quantization/operators/conv.py,sha256=5d0OxKVw9jwiGNeCi1wY0DyxHbDE8Eg7eFyYIascOxE,9986
-onnxruntime/quantization/operators/direct_q8.py,sha256=jNL6DZGKcc1GjvBTlO5m3uO5hsMKZzwE_9_KIpdp4EI,3350
+onnxruntime/quantization/operators/concat.py,sha256=fZFwnaqoOZ9b0ZvGpBK_MrJzVteeJguWRQ396kUh8QQ,2143
+onnxruntime/quantization/operators/conv.py,sha256=KhetFXKMcFjs6W6ZnOonyy_XrSFYNr54WeFoBMmIRK8,10168
+onnxruntime/quantization/operators/direct_q8.py,sha256=0-c-4O0eN2k1YJGRdG1UE1BpP0odgbmG0frHx4HV3Jk,3388
 onnxruntime/quantization/operators/embed_layernorm.py,sha256=2LsZk5Um0ELaRESWjScgYyQioJelRZK6oQbzAclSgXI,4058
-onnxruntime/quantization/operators/gather.py,sha256=oYPW3XdwWo7kqPYbEvCqPC6njAC2_zJN7c46z1xp6QE,2166
+onnxruntime/quantization/operators/gather.py,sha256=gv6aVXEqco5cY8g0ZxomMQdwgVh591vTjznld9xfOn4,2194
 onnxruntime/quantization/operators/gavgpool.py,sha256=wYyjEf3h-_QChWKnsZ2N-haBG1RSvqRitZ-Yvfwo9Dk,2445
-onnxruntime/quantization/operators/gemm.py,sha256=PWekjWTPKy25-dHig_wGdv71dwTwikX-IMRWVXqiw8s,6119
-onnxruntime/quantization/operators/lstm.py,sha256=sZg61vtRmpHvUpPSkiRJwLBRDD9OGM1mBZ2b3I1hhDY,5114
-onnxruntime/quantization/operators/matmul.py,sha256=1n3pcEWdG8_FzPUhA3exSc94dxVWadK5mawouw30lXI,8395
+onnxruntime/quantization/operators/gemm.py,sha256=vSMhyC_znzVl7uPzOnvHr87ReAWoGBNkZQwwse2TbN4,6252
+onnxruntime/quantization/operators/lstm.py,sha256=4diaxKg7OlCA3yq1_LDLCc2oFDqr_W0zA2XdOkznPp4,5184
+onnxruntime/quantization/operators/matmul.py,sha256=_NNIOMxwhIO5Cvj-izCVF7_BkBmohuiDf0hnk3sdh9M,8305
 onnxruntime/quantization/operators/maxpool.py,sha256=QyDmHyBo0QKf6kNFbp2a9v6ThrBO-OL3tW0PFdN6bkI,961
-onnxruntime/quantization/operators/norm.py,sha256=kK7MkJ-0Kw-ObnfhtvyNCsEr02c_DpYkwIk3AG6A71k,1545
-onnxruntime/quantization/operators/pad.py,sha256=CU6VGRZyNfwW79cblWx7pWUXhTURQxo_6arP4kW_xEU,4852
+onnxruntime/quantization/operators/norm.py,sha256=UivIysh8tCPpeD05PL2SLurf2ifR4nnrYneYQLG9_Vk,1643
+onnxruntime/quantization/operators/pad.py,sha256=voQm5gRA6Y_MP6Dntp6D0SAdIDsuqD5fqGHI6-y22rY,4914
 onnxruntime/quantization/operators/pooling.py,sha256=L0IT7G6-2XSx9-wUz5BX59Mc43FfJEg79NwW3yqEDhI,2285
 onnxruntime/quantization/operators/qdq_base_operator.py,sha256=Fco9JZxrXQoVgjKvmHFuzT0mogWo9-wHiDa51CjTioo,823
 onnxruntime/quantization/operators/resize.py,sha256=BMeym-7GHOSnGpZisa9BkdQkVmCXwKANA5NpnKRnaLI,962
-onnxruntime/quantization/operators/softmax.py,sha256=WP3JjKH4p3RyTWeYTGqEhh3XaZQZ0rctBLjHkzBs5Uc,4269
-onnxruntime/quantization/operators/split.py,sha256=ZY8aEpiF2xD0r5DTmm3wVlcpsepd-FOSYMZ86XCwUeI,2244
+onnxruntime/quantization/operators/softmax.py,sha256=e3ThVOh2TG1B8luG6xWkoT_hCdvsMRjvTlTje8CW-YQ,2714
+onnxruntime/quantization/operators/split.py,sha256=82R65-_Rw5g23f0uekUWpA3nhOzeUWdOQgr2JZXwrOc,2258
 onnxruntime/quantization/operators/where.py,sha256=wd6PQ7LlbrJTqamFMch_Fipnbt4IewMJSAPozMTrwKI,3127
 onnxruntime/tools/__init__.py,sha256=7up7iKcklVy6UcpIIIIlBaK690O32vaOxyaaTWvwyxU,528
 onnxruntime/tools/check_onnx_model_mobile_usability.py,sha256=h-xTaXu_uSVptpmx69FiYcwACzuvI-4sTqLKkKXMo08,2871
-onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=4B1892yphgRi_hUG62JizyO1SdnJrVLdPW6KnPdgZ0g,16900
+onnxruntime/tools/convert_onnx_models_to_ort.py,sha256=I_9nhCipOpgsX5dNovCOaY3bc6NyfQLE7KuPKMzucM8,16810
 onnxruntime/tools/file_utils.py,sha256=ONHY-VlxAJ7mlrTNZYkRD4I00RqsSHMZb1rUUxceQss,1569
 onnxruntime/tools/logger.py,sha256=s3M5-Akb69zubXNhCpsjIoJ052gYieHV5FsOfBZ6lrI,333
 onnxruntime/tools/make_dynamic_shape_fixed.py,sha256=GkbUE5kH1pOua5EJVH4trXs7mJIPIQ8T2YTeKQxr6ak,2608
 onnxruntime/tools/offline_tuning.py,sha256=Gd120-LGX04OJZ8nvErr_8h-5XGdDOEmuJPCWVQC76E,6380
 onnxruntime/tools/onnx_model_utils.py,sha256=RLeLn_0OsLRFq0rQmY4OwjXa9a5wHAxKOxA7NYxd67c,16692
 onnxruntime/tools/onnx_randomizer.py,sha256=9L96dzIf59cQ2oQsmR2EEsdrR4hHwEGrpZkajEgUPAY,3361
 onnxruntime/tools/onnxruntime_test.py,sha256=SvqgwrjiIpf_vsZfHmkE_FPXJkDA18mZpwYoyjMv5g0,5770
 onnxruntime/tools/optimize_onnx_model.py,sha256=J6rk1Ani3VWwe0JEy5fTJ2V_zVGrA1cjIKOX6zdHd5c,1969
 onnxruntime/tools/pytorch_export_contrib_ops.py,sha256=xxlw5jPDy72tWEPPYn8Qhof4H-edK7RwpT0ZXtWYfC4,4091
 onnxruntime/tools/pytorch_export_helpers.py,sha256=MRegHn3z3VhVbZQ4O-kTGedIE-pufyxhq1A1GVIdCjY,5971
 onnxruntime/tools/reduced_build_config_parser.py,sha256=O9XtpCRKoFiPcXuwfyGH1zcvpVU0cbOq9JxFh0Jm-Fs,10137
-onnxruntime/tools/symbolic_shape_infer.py,sha256=710xc-T0sCtqFiqxB0zQLbiv_oTtL2NuP_dibOLmaYM,138389
+onnxruntime/tools/symbolic_shape_infer.py,sha256=RYPlP39bsnWiptPi0Sgj6dEFuYjerxWcu7Ut4gJ_9qw,141288
 onnxruntime/tools/update_onnx_opset.py,sha256=fplb1ypV-pFhu8Xsi5u_bDfI7EsC4zamJkTziccgQ2c,1182
 onnxruntime/tools/mobile_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py,sha256=cYzx6XbiGdG_iJ-AlCg45FXQkUlfR0q0z7JaKRNUQ7A,12648
+onnxruntime/tools/mobile_helpers/check_model_can_use_ort_mobile_pkg.py,sha256=rqdyn3B7amoOPgpiGkPhFHaPCIZI4F58bISFq-xzBsg,12642
 onnxruntime/tools/mobile_helpers/coreml_supported_ops.md,sha256=OGQFtkoXn0qA17R0o22Bb69v3qPVpqDdJNrE9HnqObI,1796
 onnxruntime/tools/mobile_helpers/mobile_package.required_operators.config,sha256=nDi5sBRRAFxhelU7H6SJUEHuxiUfFRE8MIjw7sVJCXs,3069
 onnxruntime/tools/mobile_helpers/nnapi_supported_ops.md,sha256=uJznEyy7ZAdlrkKQeoWFFs55rPE-kOePIJiv741r98Q,2385
 onnxruntime/tools/mobile_helpers/usability_checker.py,sha256=7vxo604YSD45bUrhJRWV12P_EyAxcbz9oM4uQ7wg4b0,25977
 onnxruntime/tools/ort_format_model/__init__.py,sha256=gQqh9tWzGxeUllyIEF2FmIfee9ulji3mlJQNW7QrpJ0,1378
-onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=U4WbE7h3Tfrjix5vBasmY6e9Yat5qxplNed2D_BTEp4,27375
-onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=sT2if_kb7cwwfLp3m1zXPTNqy5pxn2NnitMrXjSftos,4484
+onnxruntime/tools/ort_format_model/operator_type_usage_processors.py,sha256=B5sRgMebZw3bsY2EkPQyJ57DCQA5UVwx4ncR98a_YPU,27211
+onnxruntime/tools/ort_format_model/ort_model_processor.py,sha256=Zh07RmcGYf0-qbXRiim2DWUGHbsrEeG6_rOcx3VLexw,4472
 onnxruntime/tools/ort_format_model/types.py,sha256=s32mQkFeWRdu3EzC1qd-lxhIvLQ1GOohyHgbslGcMes,4466
 onnxruntime/tools/ort_format_model/utils.py,sha256=Ix5mFXZCnMEHf8Tg7Mwg2GFdy0d1l-zocT2fsE8_8sU,2604
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py,sha256=KNRBlqUVQKtG2E7c2TvSUb29R3UXcR9cPUpdtmnR9QI,149
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py,sha256=Tr3VGnO2r4ZGT-OP_96qqUzTap1DS_0lysfzgODa1nQ,1611
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py,sha256=wfv752tTginm2d0tr2QecHSZAGoCTLwwTifXIXJC33A,9310
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py,sha256=1_69-oW8C6M_Wq2_1TXwXL_ryHXUpxIC42CU_QPYFss,348
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py,sha256=gd1ZmNjTqHhzuK42NGQpA9wzIPnsxc-ZyrAJrKDxGfY,3491
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py,sha256=cXdas8VWnTkcmOLfMcOVnMO3CPv6ETOkz4SizEiGAt4,3754
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py,sha256=ez5IgvSx0ds8ktg6u4_YtJ1KAjRbT7Wv203bHeEDZyk,1924
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py,sha256=R4u-ioNdtYqRD1ANa_VA1O2etj48ABTptyBnItp25WU,2939
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py,sha256=89oCxn7cIaQJXMeght_1y7CN0LY244sVCuTjrnNS8LM,2112
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py,sha256=i8aQjgIQLWaoiyso0qx7Zp7aJeWBPROqti6coq3xSWU,1792
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py,sha256=fqWUU7xUeEPZRCbjx1-T2XzSXCKhnSUWys1mngK5tyM,1988
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py,sha256=MvwDY8Du8E08-7-lpr2YrB5eRtSbnruGacgQu8zZwBo,176
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py,sha256=TH60nYg8iKcHZKO8-YpA2UMOLkNIFr79JdKoWXzbxTY,1076
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py,sha256=nN0XficPVRNEcNYDY2H7ghd-oqq-V85tOyX3L_EMyY4,1613
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py,sha256=TgNQezbzb1LgAKl6UQwBzeHOV8vDHESEfehzdvCgxEA,9000
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py,sha256=Oh1NVttahMXc8_PdFTjOGGrU1LM6KuPwckJBxkjdReY,2477
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py,sha256=MdHdHkAaJeIYH03U5vsLInNjkrwT0W9TDvHwE6jvU9k,1583
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py,sha256=eN88n-fwIl0MpkIUXyHhS_M1qwbaGArSc2EY15890mQ,2532
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py,sha256=yfzU2LLgnJsHhdgQDi1iLqdeaFlr9eiVmdnVDwfZx4g,2244
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py,sha256=5NHXISPEFJ3nWKEImkHs2izwbL8hQW3Ba5damX7Xrj8,1728
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py,sha256=GtXy--BasuQ_5pSrrMqF2K0oCQegL48wcZb9F1MclN4,6145
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py,sha256=2PUhTVSlaIcDVlV2MOD4DpnaSPuP9Ac6LUzXB2wGhQw,3177
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py,sha256=zwGG5ihskvWevt2MlhS8YOgNPTOFBdu7cFSPGPs5VU8,8635
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py,sha256=6BEVnqcv7huI0W7o1BWQJqaWm7tUf27rL7HnxW1iYDU,3339
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py,sha256=quSM-cnMeSAHZRPJdBXSCjC8Wnj30wc_1mKl9obfW4c,153
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py,sha256=9GVC9CpTSZ6UmPQ1tQOUEmJa1I65eAMQo93mw6wzsfA,4785
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py,sha256=czJ9B6XND3bwwBcBkQkVyvsUmKUVwrOZBk-07uTCmig,2664
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py,sha256=bZayVbeZdKnKi46Dkg1N4M70oHYaRdl2V87WHZPWIQE,1621
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py,sha256=eMNkxq4dvh1Na07fIb1pZANAVbdI-8cF9ZwSRaULRGI,3244
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py,sha256=8eRX_REy82p6uwVFaPGuo-TlpGBuBTwxErEouO1T5tc,2538
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py,sha256=lf9YxG91GfQkd7En0VbTI-fTKyFEon-hvxOLyf8GQko,4182
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py,sha256=vDhHlZq_DcrLo800B5hLXvEbXLEtcS5waJoLwDgwP9E,3194
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py,sha256=y4D8JefGWTYtwoDmh_LhCi7Qlo60OMTjGifzhnl4obI,2954
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py,sha256=I-HIh6O1l1Lw_zWtFr_JpBk8Ef_phGSoBpfkDp5wSZo,2253
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py,sha256=42QJ7DbSUilbbysiYHGmEtYbYoU77X8W0Lyy6qvyeiM,1437
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py,sha256=xwh9ZSvNNb0fOV_g9deY0JD6hEXPDgk1uAppsMLeGH8,1889
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py,sha256=zJ35it-hOQ_bT8QBYdncXbAS8ewQsXh3BJ_XEVdVt6M,3133
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py,sha256=IRkN1dsmOLvgu3ZGy7vxl0x7YWW44TBfBjcB7l_PlAk,1644
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py,sha256=AB9-zZAM9bJjrWNbe8QIOR37i92jPeBGlgd1uRp0vD0,1673
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py,sha256=KYzRQPUQ3INAPl7SoTZhALOVWDyQGdXg-Gm5qjfUvHE,5144
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py,sha256=NGQxs8ALIBwO23h0_h5KmrOsCYOb5x0DN4jztqCYrvI,502
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py,sha256=IGgnA7Kgz00idyxMVDAy-m-CAxvcl4NtGIai7L9TEVw,1828
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py,sha256=BozLwO5lX5f___cutnpJDD2oJWOnJqDDaeJPJaqD_WQ,2039
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py,sha256=vLFbH5F9iaEEF5d0OSnt1bvlAzQ9dddPc7pDu5uhvLA,200
-onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py,sha256=oT0gX2UAHgZqIn7oyetzlJIvQmFgeSeNuibt0TLr3dQ,2118
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgType.py,sha256=ErRXrmwza1xgVW4OAENw_B8yVc7WfWbAYYhiAO3Cc2g,147
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ArgTypeAndIndex.py,sha256=8Sx0mmUqjN4uN7MeTY6scIgeZO6gruARh8MvlEQdUdQ,2093
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Attribute.py,sha256=S5SA5y9FB2HWcPIPx99jz_ebTV85O8cTETJgGe_FKLo,11187
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/AttributeType.py,sha256=piF8u_U7zNE5eSrwcQhRLgvr2axK21-455McxIje2HQ,346
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Checkpoint.py,sha256=0cZ14luQy_w8GlbDF_xoGPWtee2UueQM4jt59YX5B9o,4342
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedKernelCreateInfos.py,sha256=K6Sl9lPxPE8huGE0tunG9MXDFYbzJs5Hc4nzKj8ft88,4648
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedNodeIndexAndKernelDefHash.py,sha256=Lr3cs1e80M1zLbZ1hBKwDsSl6d8zxjkbIfAguiTkJko,2526
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSessionState.py,sha256=N5CN72wJObCmYGcbA5z9o3u1khYxJ9bVSIaFIJCDR3E,3678
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DeprecatedSubGraphSessionState.py,sha256=ARjFKPJX_pB2KTrSUCHz_iN_UmrB3xRDKtUbIepTX-Q,2682
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Dimension.py,sha256=VHQcCV5ip0BIhm1EFtsAXA-uMp8tqlu1BfiJP2LZuQc,2262
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValue.py,sha256=14W1geQj5_V_5UiqgrG2oFGDgCkAIS5d71xemZwmO_Y,2574
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/DimensionValueType.py,sha256=2kmuPePZe_rWSMkI7ceIhewHL3wvvsDEhzH-LBusm0M,174
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/EdgeEnd.py,sha256=aNdY1SA8fDyvH1Vsqo5xklVuhGtw2sUmyy86HmtWEbk,1137
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/FloatProperty.py,sha256=qqpRpfy9QXs7WHhHUARUjv5q_zs8wiT2-iOjKD9v4NQ,2075
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Graph.py,sha256=Y_AnjssgrJ7o4oU1w-KG-rOl8EE8YuEnvDWu7rGN57E,11039
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/InferenceSession.py,sha256=0FgbOyXfWicKshG131GzHqPTFJvoVorBxTVP1xM6ZNU,3125
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/IntProperty.py,sha256=EjWio-yFwfkE_N764bYTaDRz0EHSZ1n0JePkLSNGitk,2037
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrArgsEntry.py,sha256=lKy6VMEjvkiv5BGOjugWEZtlobNMwvlfcSRSYzjCiAk,3193
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/KernelTypeStrResolver.py,sha256=cufEs5uTENEF6bvZQemzDztbErNtvDuzMpDoCnSBpik,2867
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/MapType.py,sha256=wlC76nL8dC0vdZVlEoUC9qwgbVDwnSt5RCRO5r_Wfmg,2194
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Model.py,sha256=CPbe2q2OcKU8LbShHGtnIG_jrW83f3nvMR5Trg1Nx90,7663
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ModuleState.py,sha256=rGMagBBqyCNbewI-xUygxN07m7fLDQmH2EWISmFd7ss,4994
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Node.py,sha256=IjM4YjGq_swHTiLw01xsSWmAYaZmkVZUR3cei3byY68,10718
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeEdge.py,sha256=LiB9vyQTsFkfRRTuiDcwrRfVaWrRzAALP5Rop0rtGKg,4183
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodeType.py,sha256=bBLbvDWziLLRxhjCAqlS44k_hdIWbWk3vsRxCDOLz7k,151
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/NodesToOptimizeIndices.py,sha256=KoWmACgRnK8qGNyXiKo6NywUvzpvYYxGQ0AyayjyaFQ,6144
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OpIdKernelTypeStrArgsEntry.py,sha256=6WV2Pj3vkqJ2NZ19VWrPAxKtoGCeiJ6NTVOvs89BIQ4,3387
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OperatorSetId.py,sha256=UOiU-CCvexY3OOWk2hiZyBI0OItcA6jiZTsERDZxu2U,2099
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/OptimizerGroup.py,sha256=Xri5OFTQ881fzaVdQZjVviLyE5vH2aFGGArQE5H1ZbU,4135
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ParameterOptimizerState.py,sha256=AIEaCRcEln5wC7X3C8vp7TmmpxuaH7t93UCphxbxRJA,3218
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/PropertyBag.py,sha256=QXs064QNug8pG9zjX9aWEPmO63CIWJcL-kMLgMPk7vI,5099
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecord.py,sha256=Lrl5lCzbUL323Y1nnij-7vn95d3qNHkiVPwkbg6SSds,4067
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizationRecordContainerEntry.py,sha256=PM7_R0kbnF3K32qoLOIPXmeudQfj0pvabMQLB-FjREw,3832
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/RuntimeOptimizations.py,sha256=qYkC65wMqu7FinW2X9pPTeRD4bY7xdIsOfXnIJoFDFU,2800
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SequenceType.py,sha256=iiMOzyjzXzjO3zoad_oG2-ct786Wv8u4LWoFOZdmc0A,1829
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Shape.py,sha256=4Y6yU35JHSXKzb2BDzObkss4yiopwS3IXkSBpFf88FI,2352
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/SparseTensor.py,sha256=3EZahzlyO_73WOzK9jZHwoVqsQkq3407ATBOG56KLEE,3806
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringProperty.py,sha256=Zd5QCcSgvf5ojuOBRV19gGyQPhqjJkoBaMdZ06kOc7c,2110
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/StringStringEntry.py,sha256=Iih7xRq4zvu4cVV4_TAC4sbjaH-dLqZGU68X0oLaWoI,2147
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/Tensor.py,sha256=CZItxbQ-B9cJnaS5HwBNWT1buat_JgX-W-zOn9m11a4,6802
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorDataType.py,sha256=WmblM8GoLES62L12Ea3dC655_ruO2T3R2Q085OLCFc8,500
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TensorTypeAndShape.py,sha256=noUmY1JdkO-Bjhghyuevf7cy0Gokhnh0r_AcmIt1fl4,2326
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfo.py,sha256=ToFYZzH0Vit5VVIztodfzP-fCOc97vgBHXz6Y_tHyWk,2599
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/TypeInfoValue.py,sha256=a2VKzgcR6v4qiffa5a5HYNUaHKDLdmsFbAdsNwrmT5Q,198
+onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/ValueInfo.py,sha256=ZC6peiqYRYKYw_si4BmtZUbRRLf9BkkD4za4xky3QHY,2655
 onnxruntime/tools/ort_format_model/ort_flatbuffers_py/fbs/__init__.py,sha256=EfkIrreUF6TrcYpBo1NJ8GOV_p_o_YXg3fSptBN5XUo,251
 onnxruntime/tools/qdq_helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/tools/qdq_helpers/optimize_qdq_model.py,sha256=9kpU0Dukc0ZEHnjwO7NqNFUFVRtOx8xgFPVWZpXkEcQ,1279
-onnxruntime/training/__init__.py,sha256=BS9WADiZ74bMkxv7Bxn5Fc8wrXPuZstwZTkClCiI6tw,880
+onnxruntime/training/__init__.py,sha256=5ItZQBxOFsEhUpxPOhx2laVsrl3ROKoqPbcXVKZ9Sl0,863
 onnxruntime/training/_utils.py,sha256=1ND6dG3Qxfy9C7trSmXRZoVIff4FrZFjUzHmvyDrWcI,6754
-onnxruntime/training/artifacts.py,sha256=IOip-rkKL7iRMFIqAyq7IjTJD01PsOhZgwES5bkv6xM,10222
+onnxruntime/training/artifacts.py,sha256=g7G1TJnvUPS5pqIp1NtwkNkQGLL2EFiRqaMOdCCwMZg,11789
 onnxruntime/training/amp/__init__.py,sha256=4NEFa-yWUb5UAdGszuVI2xERh-zX-uCu3mI0qyw9ql8,70
 onnxruntime/training/amp/loss_scaler.py,sha256=Fu_1kn1kDH_doUvmIKy_Td5umT-tFAhXEaGifvRmKac,4774
 onnxruntime/training/api/__init__.py,sha256=_r9GC3VSm6CTfRgP01GC7xe8fnAkv95xE0zL8IRuUKU,449
-onnxruntime/training/api/checkpoint_state.py,sha256=Q3G4rNqSxv2049g0wAH6uwXNabc0a7KnfDW7JyERieQ,8621
+onnxruntime/training/api/checkpoint_state.py,sha256=MQsCvwvc7X9prKf6ildcj7gZO9UbXmctOBndF_45oKg,8717
 onnxruntime/training/api/lr_scheduler.py,sha256=GPEw68Q_dpZdqBoeJXEfrxFkuesfLEqLxuSocl-3xck,1398
-onnxruntime/training/api/module.py,sha256=wyQKA3jCK04veDOmYen7rYVTfAGjltjaxh55xrxcaoE,7795
+onnxruntime/training/api/module.py,sha256=aBsfPv4-n2j2hOjx2w3lSjc32WJVmzvhpxaNh--s4Jk,7974
 onnxruntime/training/api/optimizer.py,sha256=p81TdN4QKKpZbDYfGerpW6hPnyiYQcsBnYi8ckyv4zM,1619
 onnxruntime/training/experimental/__init__.py,sha256=x6Zee-MkyZeHIoT_n6LivM5T0EQcdo9AgrAJ_tdXXK0,87
 onnxruntime/training/experimental/exporter.py,sha256=hmEdd9RsrWakWk-cdQmB7CAquCBxdY9WzDhcSLDuaNE,863
 onnxruntime/training/experimental/gradient_graph/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 onnxruntime/training/experimental/gradient_graph/_gradient_graph_tools.py,sha256=QRSUa1kYQZ8dqCHfq3Y4v-4JkdKpjvhHPhKkirnrmmM,3472
 onnxruntime/training/onnxblock/__init__.py,sha256=zYEP1kFspViy11T2XX7CTpcgLrE2NGrpXJzvum0P9LI,904
 onnxruntime/training/onnxblock/_graph_utils.py,sha256=ssYqA5p_Cu1hGpKIRqbJdVTaPHQRbZ5YQnxSxiiJ9y8,3010
 onnxruntime/training/onnxblock/_training_graph_utils.py,sha256=44woY6BnBF7GfYNjE3yEQrNQADXgINN1CR7wbAIqXUY,9940
 onnxruntime/training/onnxblock/blocks.py,sha256=3mlAr_J-LCtWp1sHd9fnJY6_dWmHvasQ2VCH2UDsDZ4,15827
-onnxruntime/training/onnxblock/checkpoint_utils.py,sha256=nfLocObyMkILpCsDMgbv_CKcUv1MQp8uMTRkNRsaJ3c,1698
+onnxruntime/training/onnxblock/checkpoint_utils.py,sha256=NQQzDJh1pH4BfsqWjDQW2M6iezhj8PLr_ZwIfmJuKbw,1826
 onnxruntime/training/onnxblock/model_accessor.py,sha256=yc1EmO3sJ-UYhAMwombcuv6Js5dxrMipXM1-MzproKg,5113
 onnxruntime/training/onnxblock/onnxblock.py,sha256=VGKS53Ma8kURHjXlT-PSCKWilmm6XJCMKQk4OOhxkts,9101
 onnxruntime/training/onnxblock/loss/__init__.py,sha256=bSuDfDkFF2AcbK3kCUkPS-0Bcg8uXyw2UnKVr8BZirA,281
 onnxruntime/training/onnxblock/loss/loss.py,sha256=ZNpHsJJTOUdLZO-IUsWEJjXd6OeGHKLa_ztf8R5Z42w,10050
 onnxruntime/training/onnxblock/optim/__init__.py,sha256=I0QQM2CMNMJjLICwMfN6iWf9d9WSJ89GuxLf_D__pfA,225
 onnxruntime/training/onnxblock/optim/optim.py,sha256=ItHJkHttcQHZvii9zNz2JLASYwB7tB09MMEAx5d2hHk,10580
 onnxruntime/training/optim/__init__.py,sha256=7E-Fd9SvbTDca2dziNJwi48As9OVsKTPwD7kJjaCiT0,519
-onnxruntime/training/optim/_apex_amp_modifier.py,sha256=tI0HbnhCxa2y6owSJOlIU0Kq6pNueAcfFxrOuoRt1vY,6555
+onnxruntime/training/optim/_apex_amp_modifier.py,sha256=2LTiXxtwtd96NIQQ1jIuJS5ktVL3RXszcnuSUerBicg,6541
 onnxruntime/training/optim/_ds_code_store.py,sha256=8NDb3nBHrzIliI3HevVKuQ80VKOtj1qGX27tua6WcBc,3775
 onnxruntime/training/optim/_ds_modifier.py,sha256=j3AlqFWYADl88a_RWPK_zyXJGJV0R-7tNcjUoXMljJE,12775
 onnxruntime/training/optim/_megatron_modifier.py,sha256=vCOwhkSwkup6rShUO5cEyhCGfUTcqm53u6AUm7_3EZ4,4182
 onnxruntime/training/optim/_modifier.py,sha256=eU_LYwA1IQ0BlrioBIs7Mvfe8OYweuOxCXX0RlGmrOE,6796
 onnxruntime/training/optim/_modifier_registry.py,sha256=8kWmg2uLANqzK2B4ErhiAmBin1bdCRbHTuIIUmv_w6Y,2574
 onnxruntime/training/optim/_multi_tensor_apply.py,sha256=hwB8NSiIFatJ0ZNXcQPM66gi16Fd8AOojcrlrEggLII,562
 onnxruntime/training/optim/config.py,sha256=j96KfRcXxYasjTCJWrS2B71Zt1UUCOPzsPOl94XE8YE,12622
 onnxruntime/training/optim/fp16_optimizer.py,sha256=fJNNKLypVW1JI4dH9EEjNJXC3XTXHFBBt5CCwBmAdy0,3961
 onnxruntime/training/optim/fused_adam.py,sha256=yarKk4DLh7WIA_o3SLR1x8Ok3WcuBVFmDsQ5xO8DYE8,8105
 onnxruntime/training/optim/lr_scheduler.py,sha256=2SlMfQv-m_gVenEMHh6aAd3WhBAFWlNuNjk5DDnwgxk,12986
-onnxruntime/training/ort_triton/__init__.py,sha256=-EezEfy8VQHdqPwD8cG4ZCztFs9_rogJ3zG6FYLcqcs,1508
-onnxruntime/training/ort_triton/_cache.py,sha256=n-sara0XIzKLxOcgn3za483Y05DrpN56miAaTOkCtzM,2525
-onnxruntime/training/ort_triton/_codegen.py,sha256=hcIFBiaVxK6BeT8j3I7sValS119gXqZ77GuljM2CrrQ,25508
-onnxruntime/training/ort_triton/_common.py,sha256=QCLqwYizQy6yGswbAyW6iRW7kqSpxiO8C_9BMFqTSeI,10032
-onnxruntime/training/ort_triton/_decompose.py,sha256=1C7CJvV8rKMhlVr63kibuUKMM0JDXDm5FUGCQ6emBw8,19133
-onnxruntime/training/ort_triton/_ir.py,sha256=fR9VfTa_IeX7Do1X1W0oUMvhgY5EMsw7fqlajc5YAKY,17634
-onnxruntime/training/ort_triton/_lowering.py,sha256=dpEC_Dnd9BYxVTCYpDT0luvt2edAguxwAPtKg8AIBHc,26876
+onnxruntime/training/ort_triton/__init__.py,sha256=MA_gW70lBj8GA7bt70SdyRWh0ijekfeulaPp9noGtHM,1578
+onnxruntime/training/ort_triton/_cache.py,sha256=rwOxMPDMgt6Pl7qIPUFlt0fxkPBKPwN6-IsSA5PliGk,2586
+onnxruntime/training/ort_triton/_codegen.py,sha256=AfWfvN-kfzGh6fZhblvxQAYyt31bbZnd44Uezo1nGDg,25502
+onnxruntime/training/ort_triton/_common.py,sha256=6ZC0DR-cdgUnN_jU5qls8T-5_OBh9iPej0lsRHEN1fA,10003
+onnxruntime/training/ort_triton/_decompose.py,sha256=OK6JKrjB_2MiabJOx3PpYYz7WGMIY8yV8JBKhkRO1Qk,19050
+onnxruntime/training/ort_triton/_ir.py,sha256=5FM6_mQdNZLgQ-D7a3cGUpMYAsXs046HHgAQbZDj95s,17774
+onnxruntime/training/ort_triton/_lowering.py,sha256=R-nLysgj3glcVvVrrim7vn1m28dVAOZAKRLLSG8B_9I,26882
 onnxruntime/training/ort_triton/_op_config.py,sha256=KNYHV4HAWSR9gbltguyt32Oota6HsgxaMY9SGuwBMcI,3367
-onnxruntime/training/ort_triton/_sorted_graph.py,sha256=WcqYhhsloUvdLIzulOnvoZVQq7JdOLZTNbuZGBv1M8U,10600
+onnxruntime/training/ort_triton/_sorted_graph.py,sha256=iIqd4b2h6Ust1HtqU81y-3Dh977tGsOKHAxvPib6fFI,10513
 onnxruntime/training/ort_triton/_sympy_utils.py,sha256=hk3gzM6GxTi9f2Afxq-G8-0tDyskzfXmFZdxZsuQbXI,1046
-onnxruntime/training/ort_triton/_utils.py,sha256=q-j1tahRDyqsnWaYXNrm_LwJkD1wgn0BxjfZ4OzY79k,5223
-onnxruntime/training/ort_triton/triton_op_executor.py,sha256=AiflpGpEV0VnAXUCcezFJYmx8whWXa0lvdY5z8furD4,6215
+onnxruntime/training/ort_triton/_utils.py,sha256=i5uaNG6u9hveXH37Bc90dGn9F86TeXe-E7xhc4rhvJk,5776
+onnxruntime/training/ort_triton/triton_op_executor.py,sha256=TiPeCmESpb4vldWFDfKEv3LNy738S0pIr9D_8V8XCDI,8336
 onnxruntime/training/ort_triton/kernel/__init__.py,sha256=6Cm_wp_Mk2Vma-rCttZgVKb1GKz1RtLSY8y9NOLnkQ8,1024
-onnxruntime/training/ort_triton/kernel/_flash_attn.py,sha256=PmXNyfMbH3UBcQrsCt61_8gQX2Y06IDoc0Xda_6KxoE,46891
-onnxruntime/training/ort_triton/kernel/_mm.py,sha256=vFBowMb8tmf7IZ_GjCoasxX87NtxEWYMQCcywuon4tk,16930
-onnxruntime/training/ort_triton/kernel/_slice_scel.py,sha256=vE_Sfdf5AL3G7252GrdU_aHcM-1iu_pW_UGJzPxtdfk,14688
-onnxruntime/training/ortmodule/__init__.py,sha256=H8J96IEP42HJbOJ7Aw4_Q_Xz8uEw8uRbw9BQlorpCtk,5283
+onnxruntime/training/ort_triton/kernel/_flash_attn.py,sha256=Y5r9FtTHXeccwMcMhrZGAMFHTuKVn6arCYjYlJ9w2FU,46888
+onnxruntime/training/ort_triton/kernel/_mm.py,sha256=-dh0sqPDtsAy6JiSoPAwgZyEkCOFkalKU852U7FztyI,16805
+onnxruntime/training/ort_triton/kernel/_slice_scel.py,sha256=1DLBUPMc9WzDQBCG6T4PB_gGs8RB1a5u4Y6waaosokg,14695
+onnxruntime/training/ortmodule/__init__.py,sha256=D-q8yOFAd7tdHI7GM9nA5xxy07QKOf3--5d5K5OuWnA,9635
 onnxruntime/training/ortmodule/_custom_autograd_function.py,sha256=2J3G98lliLL_53oE1wzFF3fzmp47HnCj1mHj3bCzUD4,3866
-onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py,sha256=XhNQf7prhCbq9orDcHEy1hEClK5CEYsFTZBo1I4Rqm4,19507
-onnxruntime/training/ortmodule/_custom_gradient_registry.py,sha256=IuE2RafwP3ytWjf-GA--GxyrG1AUoCeAbts3nV1blLk,11582
-onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py,sha256=VASCPzjaarA6ghUQEC3O1Nwl1yJQkRjMDom92nPPTVk,36602
+onnxruntime/training/ortmodule/_custom_autograd_function_exporter.py,sha256=jYgzZlnV16-lxBwrbyOoa9VaWzXyB-Mfv6kWo1mZJfU,21495
+onnxruntime/training/ortmodule/_custom_gradient_registry.py,sha256=mj1kAmLhVpl-AWQ_tScv1a_emFO_N9EUNHpyI8P024s,11558
+onnxruntime/training/ortmodule/_custom_op_symbolic_registry.py,sha256=YwIUFHywewybi6ARRFwCaxTh2iuCFmpi6r-SgimmBRY,41814
 onnxruntime/training/ortmodule/_execution_agent.py,sha256=kshfee-VA0oqwhCRP3SZrvg6wZlLC4VncHpMVzCDFRQ,7835
 onnxruntime/training/ortmodule/_fallback.py,sha256=yrQeG9XlAp-YG2XztuBV0hucaFtGHOz0CGwSRjEuVmQ,8208
-onnxruntime/training/ortmodule/_fallback_exceptions.py,sha256=lGd9Az7PEvZogi1aroRYJEUdtcieiSr6GI84evETdZk,2486
+onnxruntime/training/ortmodule/_fallback_exceptions.py,sha256=aVf8BlJDoJwLidEnOamH_d4bG68g08bVYtOi4bfGTgk,2414
 onnxruntime/training/ortmodule/_gradient_accumulation_manager.py,sha256=BiJio2BSpLmrZPh0HgYtM_ENEqVFCF0ygo06n02qSvc,4203
 onnxruntime/training/ortmodule/_graph_execution_interface.py,sha256=DodRDnz1J4CL5S_S4sDoIA4hkpRdRrTtTOBhOErBQMk,1121
-onnxruntime/training/ortmodule/_graph_execution_manager.py,sha256=08qDGg2nSsALGa7tHHCyoW49tFe4y5AgImMnyRUMZtA,43718
+onnxruntime/training/ortmodule/_graph_execution_manager.py,sha256=3MblFitHjA6jV3-f_blJJLQhkCihgBRrBcxPJriaaMU,50719
 onnxruntime/training/ortmodule/_graph_execution_manager_factory.py,sha256=X8MjxhL8m4JJS9Q7fvegj4RJUdxcC8zZW3JQDPRah90,1155
-onnxruntime/training/ortmodule/_inference_manager.py,sha256=2mq4i7IswosxIDcDPCtyS8GuZYS5rmk3e6iJnwfi4e4,11512
+onnxruntime/training/ortmodule/_inference_manager.py,sha256=ZiClm6eR_yLKMsZw2EHwD5vKEXw3zt0evdqDk4WXzbs,11484
 onnxruntime/training/ortmodule/_io.py,sha256=769rrNjiAjqe3qkWeElIqqs4m1fAFMqVlN3dIB6j3-0,27801
-onnxruntime/training/ortmodule/_logger.py,sha256=VtagS4O3uHpFj9wni08pMhEaONoZUtvcCsxutdu3hoE,11040
+onnxruntime/training/ortmodule/_logger.py,sha256=Lu9ivNXs0CW4CRAmjtN9V6JHgGup0f0f3KVA0IZPpd4,11098
 onnxruntime/training/ortmodule/_mem_efficient_grad_mgmt.py,sha256=kUJTXZYLRxb_MCwDFTh3jc8X2F-1BheXJ7h4_lS0RA4,9416
 onnxruntime/training/ortmodule/_onnx_models.py,sha256=_XYahGN2yLVAadLfyH8qtqjDnY926YmdKKG1ryPBK9s,1947
 onnxruntime/training/ortmodule/_pythonop_helper.py,sha256=atwtzZSyP3hQTiXmM6HubLovdkqLu9YO0mMpHU0ZiuA,9619
-onnxruntime/training/ortmodule/_runtime_inspector.py,sha256=Gaw6ueDl4Lx5nKuFJuFW-JmgZiLMibGOPTrTs2Ic1QU,32071
+onnxruntime/training/ortmodule/_runtime_inspector.py,sha256=Oido9K43glmgskHiQ_tpH9qdgKyf8WhKgqnyCI2RTyE,33194
 onnxruntime/training/ortmodule/_torch_module_factory.py,sha256=oLjFmo_jwHd28XVKUV_Ij9c9KKZxKT0FAI4uEiuXCKA,579
 onnxruntime/training/ortmodule/_torch_module_interface.py,sha256=Fr3-_lKhEQvWegwmvvj6ZIYGyB_D4LJ_9yEXXucOmNk,4607
 onnxruntime/training/ortmodule/_torch_module_ort.py,sha256=iLzAUu_3ApOR7NMsvcybTh37y0G974yEDhd9AzSaMk0,8465
 onnxruntime/training/ortmodule/_torch_module_pytorch.py,sha256=xRSONDmj2xJIZoDrk2hdYKmeO9sKCcr_XpntRYi0I9I,3832
-onnxruntime/training/ortmodule/_training_manager.py,sha256=-qBL0AlA8HuCVCuQjYWy3MC9_MGJqThvtjpzMJVLDG4,27572
-onnxruntime/training/ortmodule/_utils.py,sha256=3jz9x6CslqnFkcO2Szp11-UdsT45H0l4JB6GlRli6IM,20478
+onnxruntime/training/ortmodule/_training_manager.py,sha256=oNtCGF1pis5wem7WiEVKMgu8ZTpahFlneSG0R7JOqXs,27645
+onnxruntime/training/ortmodule/_utils.py,sha256=-HKZZfwU2wVpRzEZi60KmK7kN6fpT9VDzF77SK_V080,20475
 onnxruntime/training/ortmodule/_zero_stage3_compatibility.py,sha256=K4nyxUAYwP32MU-PWurg2dC-0P7OwvxD6IvkLjw51W0,19011
 onnxruntime/training/ortmodule/graph_optimizer_registry.py,sha256=jIMS5GEorpIqu2MtSZCLc9k8gg99Lk700Yx1ddudyrQ,2020
-onnxruntime/training/ortmodule/options.py,sha256=1ki-6OFEmA5DC3PwcmAPxL2zw_apPrEwIiq7yGHPS0k,20382
+onnxruntime/training/ortmodule/options.py,sha256=0n079KB2KKAysVF5egzYU9jWepjuN2zikL69hug_6xg,21056
 onnxruntime/training/ortmodule/ortmodule.py,sha256=lnAjrOn4YOkc8QWlSLdPYcWpBz-CKBdOH3alO0zRdpU,16392
 onnxruntime/training/ortmodule/experimental/__init__.py,sha256=Q7AHUREq3xOnDFTByds-xp4lwZQBBKnowVNREjZ-z0Y,111
 onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/__init__.py,sha256=5gY-b_zklvbAmDX04tCcRYopJnLiA6q9BgcVd4m3hoE,187
-onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py,sha256=wpXILz_qzAjrdHlBLFrHCWYGwBPwOPM--O7fzgmRsv8,13162
+onnxruntime/training/ortmodule/experimental/hierarchical_ortmodule/_hierarchical_ortmodule.py,sha256=8wWIau3aJrux_hlrOtPqgeVxT1jR-4UEYYw5K6-NiIk,13117
 onnxruntime/training/ortmodule/experimental/json_config/__init__.py,sha256=-r9zUTX8OxNAojnlrzxcDzPK0xzghaiMaY-KB_IR8Hc,280
 onnxruntime/training/ortmodule/experimental/json_config/_load_config_from_json.py,sha256=kNv3aYk1_XMjGWcXVJeQzP5CHtex4bP58vWwADhnBhk,13274
-onnxruntime/training/ortmodule/graph_optimizers/__init__.py,sha256=dqKEhWzzL6pUI6YOte2OXSwytIss8BLuGow8ose9Ifk,742
-onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py,sha256=5Z8Hw4n9BRDOX_lLP5Gz_vw1lf4Y0bd5GELhWxMjBh0,15778
+onnxruntime/training/ortmodule/experimental/pipe/__init__.py,sha256=Ll__AVyzq2f0oHNJ8MCjfnoXxJqKaHcQBAN5Mely7nQ,320
+onnxruntime/training/ortmodule/experimental/pipe/_ort_pipeline_module.py,sha256=_JrGWGte_4z0dp4CQFFsZIVArmvKZ_0vXLar2OvYf4k,8314
+onnxruntime/training/ortmodule/graph_optimizers/__init__.py,sha256=D6vNzS-pnz_L0iFkUPGrbtWl1KZkOga3lfWMynYB7QQ,742
+onnxruntime/training/ortmodule/graph_optimizers/_aten_attn.py,sha256=4jucy4l_laAm0rm4hJ_rbm7H83rGgwkFfdpaJGFFMKU,10842
 onnxruntime/training/ortmodule/graph_optimizers/utils.py,sha256=wqctQWHJUqg45hYWmQjIQBOUCAAj7sQUXCby2alyOMc,8012
 onnxruntime/training/ortmodule/torch_cpp_extensions/__init__.py,sha256=FOLMaBDHoWPdgLJw3qDzxm7WG3SuAaGbbHCKIMsdZAU,1875
 onnxruntime/training/ortmodule/torch_cpp_extensions/install.py,sha256=_wBnoYkvtClyIbrBRdPqljFlZcnbvw62eIBC1GqP44w,4493
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py,sha256=s0ZI8ZyxXZfwVlN9TY2TbsFejpwJJ7m-8oc-iet4LAs,1187
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc,sha256=-g67smoKDuUB9WV-nzG5UjbCuoqPlnVjUOSEGGUC8lQ,10153
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/__init__.py,sha256=B0UUAwjyz5EIVBJ967Go7GQsvwMTti1BLNfu5aatKUQ,1190
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/aten_op_executor.cc,sha256=kpLzJa2udUNQShKVFLJ_NtQJnlkOxcecrNqpqbrF4ik,10165
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/aten_op_executor/setup.py,sha256=UIn1O-bO5dd39F9zNJKWQpWW5yZM8WvYl8nvA49EA-c,604
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/__init__.py,sha256=66mJ6jKR-hnSl66droJ9x_jrBR5A8aQOO6GGlkDKPKc,353
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.cc,sha256=KCCdSCI6ZoMnyMvMPNcsEEkUmJE8Z52iekUC_-tUoCo,873
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/ctx_pool.h,sha256=zS9dEnL7CXInG-KUeX1v1ZXp0aipcvJ-r6ELrI-LrhY,5216
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc,sha256=iipi-964YoCzwwlU9pwCxzGpueGuttfUN6M3Ag1KrJQ,7452
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.cc,sha256=7am3FsNWdFsrFWDWSF9Ypoe7OjPxOYXLuwv7Og4WGy8,7533
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_bw.h,sha256=sXTYMb00759DSpdiTeQS-jXBjyjYwCsc8xqRUs-JtE4,968
-onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc,sha256=-43dZTOm9Vn9AoO-OrPJJL-n6svUybYveUxgDinI3y4,21395
+onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.cc,sha256=UIzqq6LBp6V0hTYS23TsHs8Y6o63yTfI_2yYdU7BQLY,21364
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_fw.h,sha256=Dud5uV6djrNVdcli7BoXmkEEk4a9OcwzEl0kkGU4pxQ,966
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.cc,sha256=Amvb7jNET5_wawAUu0qrxmx23jCbw0lAkCybjls_B2o,11685
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/custom_function_shared.h,sha256=aO_2warENwRQMZV1tfVbCuvuyevXjLOLXd6yxjF9kaU,4550
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/fake_ctx.py,sha256=IA-ba0b-AAAdlHQBJkQSdNH6haitkl8mgFMJL-Weebc,484
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/setup.py,sha256=D6Ax6E5VDnGYSQXY9X_A8HBCqgXPU4yxOL7n8wgnzhk,1166
 onnxruntime/training/ortmodule/torch_cpp_extensions/cpu/torch_interop_utils/torch_interop_utils.cc,sha256=xIeHzD6o2nhNpkXC-F4R2fm18Nvyu1cRR73Yum6xk6A,824
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/fused_ops_frontend.cpp,sha256=paXbSIGM9jC2ukmbHh3dN1CWGK0OKH0WLXCXFBIXnEI,10114
@@ -257,156 +262,162 @@
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_axpby_kernel.cu,sha256=OLEZ3vBNRYZeKIjlOQy7brcNiLU73AgZQ9l_oUVLIFw,5063
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_l2norm_kernel.cu,sha256=SxKaU8_1Z1YXLooiA-cvFLlIv5QYUOwX3P7tf5b5OSg,6377
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/multi_tensor_scale_kernel.cu,sha256=C8qHgb3m8IQFTmzRF-Ddvq2oKZL0V0sxt_01um2D7nU,4498
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/setup.py,sha256=kMDvMbiSwk1Cmvp0xogRDEcDWCQP5A-rsPT77a54nbE,1317
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/fused_ops/type_shim.h,sha256=eGkBVnDfGVIgHZw-w3CxmLdZ_vqqYlJ5a-vUg081uCs,2828
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/setup.py,sha256=1YO2THvLLIUgDBMQiY4Cz1rCsqXNvLYD6t4krz_5ho0,1624
 onnxruntime/training/ortmodule/torch_cpp_extensions/cuda/torch_gpu_allocator/torch_gpu_allocator.cc,sha256=tQ8ZtVCseMh7ve_6awNyFdKzWs8TWzLz4tzNkkVTt3U,1454
-onnxruntime/training/utils/__init__.py,sha256=uVx9Xuq7Jy2eHq3PN0SVWUfFuKUznKqSKNAKAXMzq6I,1100
+onnxruntime/training/utils/__init__.py,sha256=ikMrHr59PUzc2YEzRskf47UeJe7QsXiXwDf_wKffA0o,1148
 onnxruntime/training/utils/ptable.py,sha256=rmuLaFyjTwwbQBm6IZd1sz3w-2JNTJfM-4S-HUurDkE,2807
 onnxruntime/training/utils/torch_io_helper.py,sha256=iCucnxq8zYM1WxKmSe-6mDQ4lIFCU-gPexp8A43BrxY,13279
-onnxruntime/training/utils/torch_profile_utils.py,sha256=oDZ5ROLH8GrXPKvA4gU4OX_GbMA-WwQ8GpTxM5oHvIM,835
+onnxruntime/training/utils/torch_profile_utils.py,sha256=VeO4xm4I_SiWhI3e14ZoZyws2S4w6nyVuEb6mcdbNkg,4073
 onnxruntime/training/utils/torch_type_map.py,sha256=obU0vcSbUTeK178Vs56_LPa8QjXcIxquYnctzdJSqZU,3033
 onnxruntime/training/utils/data/__init__.py,sha256=aQIjB5ExoHnxHl3HOUZq1nOUI65WTTQD3JjCAyFB9lY,219
 onnxruntime/training/utils/data/sampler.py,sha256=wmk4ANxTJZy2ebiq1fHm4pSnnofbbyMfeQQE7s27CJk,17709
 onnxruntime/training/utils/hooks/__init__.py,sha256=GqJgqaWHmeC2ASe43QNvM0zXf1EcukmNtaIYLhfaans,1425
-onnxruntime/training/utils/hooks/_statistics_subscriber.py,sha256=TJKgR3xg3bonqbCNTjOaeJA4AowZ_UEE3c_d7URCjI4,12847
+onnxruntime/training/utils/hooks/_statistics_subscriber.py,sha256=LP6_ieqUAyxEUBlXB89pgR5QGpgwjW9Tbhk5vPg80Cg,13100
 onnxruntime/training/utils/hooks/_subscriber_base.py,sha256=L76focClhtADeB18GTPjat7B8-qUkry6asq4nVmZM5U,9347
 onnxruntime/training/utils/hooks/_subscriber_manager.py,sha256=_W-L5vq3MODH-e9HLfxTZdA4PURQv3rxmvCHmEiCVoc,14135
 onnxruntime/training/utils/hooks/_zero_offload_subscriber.py,sha256=Ljfqz8Iw8aXIHvDYBVQhx04IjfNSb3uTMoB-35lURI4,29148
 onnxruntime/training/utils/hooks/merge_activation_summary.py,sha256=wH1ES-u4LNmIMTzrmu3EUbdb49xCj9GTftita7ViNCU,5691
 onnxruntime/transformers/__init__.py,sha256=2c213CqzXrc0N6Cqf__Te5d_SH_stfLdNdeNrugB7SQ,321
 onnxruntime/transformers/affinity_helper.py,sha256=KOKBvzoBr-wOk0QHMGKzY1uy1iI7E6eHpYwBdTHM-Y4,1442
-onnxruntime/transformers/benchmark.py,sha256=EW6zzGuWlUhFuOut93OYHsXN-fq_oHwfIvqQvMP8CTI,33192
-onnxruntime/transformers/benchmark_helper.py,sha256=KU4fO1ZaKYnCsZT37fuzK2Ywnk7SL5dmuS9mdLYW9Qg,23121
-onnxruntime/transformers/bert_perf_test.py,sha256=v4pWz_Ucg-n9KTnxI9VyY8gY_1FjGhXvgpWA8y_hzbk,21005
-onnxruntime/transformers/bert_test_data.py,sha256=JQoTkxZYkP_iOGSUgpXiMXX5IQ6F2gdzqYhCo9KrIw0,23531
-onnxruntime/transformers/compare_bert_results.py,sha256=DTO9r4c7ARK8e2_vlswYQf5ekP3Xvf0wu9aWuA1O8pE,8086
+onnxruntime/transformers/benchmark.py,sha256=s21Aq5ialtyh_g9P2i4yv2YDTzYZ2ObIB5m8h4KFToM,33741
+onnxruntime/transformers/benchmark_helper.py,sha256=umWUsxNc2IkQbTn9bXtaObOvitY8fgW78KBsbzyqkE4,23245
+onnxruntime/transformers/bert_perf_test.py,sha256=P4N62-17JrwyIubnZS4vcne6AJ9k0FOCTIacAEqjfyU,20995
+onnxruntime/transformers/bert_test_data.py,sha256=x1my7LUl2OkiaRFxxXwvbYIisSyK7QUsM5sYiE6dcKA,23516
+onnxruntime/transformers/compare_bert_results.py,sha256=ZtnITLB0Wy10npJE4mAMGWvFSYdQb8BumJg3J8CPEwc,7908
 onnxruntime/transformers/constants.py,sha256=UfbiXD1CKrr9Rza6gBI7VbLT-FojgPuKLWs6GyBS_hE,1143
-onnxruntime/transformers/convert_generation.py,sha256=3I1fdW4SCNaBfJJxCEtCdXUpT1vqB7bE4OM8S9NxZRs,127465
+onnxruntime/transformers/convert_generation.py,sha256=A2E0RGGeTW60FCET7medSXng9vTBmteUXfEtpjvNpEE,127566
 onnxruntime/transformers/convert_tf_models_to_pytorch.py,sha256=JrMGzUi_4cMfYoIYrxa1n0jnMDG-WYj-xmUXZmH8aJ0,6705
 onnxruntime/transformers/convert_to_packing_mode.py,sha256=TGLK5ZwYvx79S8kJOsR78ydxY65YEe-OudhWvs-NbIU,16909
-onnxruntime/transformers/float16.py,sha256=cF-JEaj528t16ABt90GoGIvEoGuVwtSYpEjArq5Q698,24591
+onnxruntime/transformers/dynamo_onnx_helper.py,sha256=wrSGLiZfQBMQhborPj3kFzMwSwWIdtI0Z0AKiwyoMjM,3797
+onnxruntime/transformers/float16.py,sha256=GumnXmhSqmQsk1avBLok0fNHy1Q7mnO5XrNN31CbRJE,24691
 onnxruntime/transformers/fusion_attention.py,sha256=XVhct9HjfVWmyjUKZR9gbJZRUGjOoBOvp9-D9P_WDIE,52559
 onnxruntime/transformers/fusion_attention_clip.py,sha256=hZRHEPC1yQa43jiCL9nf8CRxwqs-wtQpNXe8NU_qKps,8722
-onnxruntime/transformers/fusion_attention_unet.py,sha256=HhjlAkP70F1Iu6SByhjMK3hQVXFVPC8S-Y42zTyf9fU,57000
+onnxruntime/transformers/fusion_attention_unet.py,sha256=b4FGGJGDDzIPtCO_WySiUNeiFHKODAwHScxIDDBh6Kc,56932
 onnxruntime/transformers/fusion_attention_vae.py,sha256=Ju-PG2LCnNM0KNmzbw4zXKwNkGxP4UOsawoJQ8WTbRw,12418
 onnxruntime/transformers/fusion_bart_attention.py,sha256=3AcCTwHuVdl6gE9QTV_55HgTSCjPk41KaKxap5hfU9s,29437
 onnxruntime/transformers/fusion_base.py,sha256=B8XFObxBIe6fv6lPKFHw8H-zIxOlNvNNRe67YJtFdmc,5870
 onnxruntime/transformers/fusion_bias_add.py,sha256=7JRHl-p1M8GxNfa9cgHsES7dwburpaTWqYh921_8QjQ,2066
 onnxruntime/transformers/fusion_biasgelu.py,sha256=vGamxthOu6jXsxCRVdTFaP25-_tnjz9TVq91pIRV_Is,2300
 onnxruntime/transformers/fusion_biassplitgelu.py,sha256=6G73bmAGM5y02Rm_Lupbn341O0Y5Sr-2Re_628Ez2Qo,4516
 onnxruntime/transformers/fusion_conformer_attention.py,sha256=-HRarRSTQEwq-XeDoRdzH9RY3konlVfhj-jtR9twOjU,5021
-onnxruntime/transformers/fusion_embedlayer.py,sha256=NtrmJKHAjlIVwsXa_ItBXvCxx-Nznj5VozRI2IiiJfQ,36892
+onnxruntime/transformers/fusion_embedlayer.py,sha256=Z7zXb7UYkgQO_Zikpnoj97KOoFYIHEm74yVvh-zf8kM,36750
 onnxruntime/transformers/fusion_fastgelu.py,sha256=pi2U93F4xWMThs6Yz9K1d6AZQ1kyWSZhjeGqs7WWAVI,13324
 onnxruntime/transformers/fusion_gelu.py,sha256=GrTB0LoVz_YRyTW-JoL4Fh_fz_IA01JweEP0Tj_Lwgs,10180
 onnxruntime/transformers/fusion_gelu_approximation.py,sha256=Xsa2v5mHjEuZrwnf1bm3UCCJ8I1is0dmuzzXgf5zDl4,1029
 onnxruntime/transformers/fusion_gemmfastgelu.py,sha256=jBQ1qx6rOJY-qY_35_HFlEjsp3aDuT7GSyXQqyXSQ4s,4258
 onnxruntime/transformers/fusion_gpt_attention.py,sha256=20ZhplkAVJ3rq1VWwcNRmRs6OZu7lTHKIop3SAyDSUw,22508
 onnxruntime/transformers/fusion_gpt_attention_megatron.py,sha256=HhoweTBxleb1niPOU_cfQzvUwM4LjxCVuZZWVEy3Imw,13639
 onnxruntime/transformers/fusion_gpt_attention_no_past.py,sha256=qQb8WekiDJeQUV8egoCTrLoZki018veZTVVE-w3p3ds,10794
 onnxruntime/transformers/fusion_group_norm.py,sha256=AUCHVK9FWmzSjIPFVIv194Qlure8xmFCyTKqcrQkkFA,7604
 onnxruntime/transformers/fusion_layernorm.py,sha256=CZknSEugEcncfVwdIJSUGPzhAHts7mBakBKkfYq3nVU,12217
 onnxruntime/transformers/fusion_nhwc_conv.py,sha256=xHP6QT4F-K7z3Cm_5zh1aPqE0UkDwB0votQbqsxJeZg,3973
-onnxruntime/transformers/fusion_options.py,sha256=gk_gl0oDlOOvHap2rSyfI1vJmBphXsSQePkA3wAnhWQ,12086
+onnxruntime/transformers/fusion_options.py,sha256=NSeQVc9Hz9S-4uGpxuYfsigzdLDOdsRqXyujbfRPGhw,12704
 onnxruntime/transformers/fusion_qordered_attention.py,sha256=VutuLlHX0oDnDhcbzWhVSq-VXlyKNaOXu2hW8gdn21c,17163
-onnxruntime/transformers/fusion_qordered_gelu.py,sha256=aRBTRACUuXMctEfyL1GICG5hFRqiuybJQ0B7Psgz5dQ,4393
-onnxruntime/transformers/fusion_qordered_layernorm.py,sha256=5GndWDsK2_3wURps3R4tVuGD0IuDFAcWzX3FXiysoGM,4915
+onnxruntime/transformers/fusion_qordered_gelu.py,sha256=V6BhjvA5rMpLOGw2bPQrierztLI_P_O99I9gLmy0nxM,4435
+onnxruntime/transformers/fusion_qordered_layernorm.py,sha256=dq5odT_lrs8zWZZuADDCPDOGBfE8JLo5n5jI2obtT3U,4957
 onnxruntime/transformers/fusion_qordered_matmul.py,sha256=j85chtrY9YrGD1ERNIHqCBAxZW51I2Sk_EFU4jg8qdM,8566
 onnxruntime/transformers/fusion_reshape.py,sha256=AfT88v22G6PgZPzVKM39_QduUnlIbe8dbxvPCh-5dkg,6403
-onnxruntime/transformers/fusion_rotary_attention.py,sha256=juXkUnc6xbpF961H_3qIU9mqCzaMSPIdr_MqIjNB2k0,59307
+onnxruntime/transformers/fusion_rotary_attention.py,sha256=tMuoiLV_Hv769vW7hlPdMpadleCix6GeLBoZGWSSxYg,68261
 onnxruntime/transformers/fusion_shape.py,sha256=EpmvtrdKOCD914c-tOqXzlZBHoN2mvc06R3MErCkfiw,3813
 onnxruntime/transformers/fusion_simplified_layernorm.py,sha256=KfU0Vs8XB7U5DzSAgDOilBGhEXhWc-vPbUDDz0MYVSI,6554
-onnxruntime/transformers/fusion_skip_group_norm.py,sha256=q2ChiZW6npM8jz5aDZBFhrCHbQJCiUmEIsFGU9qm8kg,10918
+onnxruntime/transformers/fusion_skip_group_norm.py,sha256=57T3O7C5pbama_5nQhiuOPbGnj2cAi9xHeoF8Fwqd_E,10880
 onnxruntime/transformers/fusion_skiplayernorm.py,sha256=NQuxs5_3p1SG12d6La8pNX-f1Ep8XfzdnGfC8vP3uhM,8639
 onnxruntime/transformers/fusion_transpose.py,sha256=TxoWRd7ItEt1CbgUeKVqgqQyAt8_S8thA6Et73_kDCs,7035
 onnxruntime/transformers/fusion_utils.py,sha256=yZMl3zVzkemljAwv8tPFWmJ4XUUF9Y1a2usJL6Ym1nU,12775
 onnxruntime/transformers/huggingface_models.py,sha256=C0B3Lh52edigeZd_JZEBBl2x_hruGF0u5VmsNOke_J4,9130
 onnxruntime/transformers/import_utils.py,sha256=_ILscQRcSyaJHt1l6jqcny5FWy7Qr6N_7hKs5aav8oM,651
-onnxruntime/transformers/io_binding_helper.py,sha256=bVFy6U4hsROpywbgGW3NC11ZZ7Ey1uevWy8j1IBSTrY,12734
-onnxruntime/transformers/large_model_exporter.py,sha256=iKQsdjzHjcGFgT3EQL8n1w3Ltx_DRKk2npiRF-EEqzM,15444
+onnxruntime/transformers/io_binding_helper.py,sha256=_rAfHoglqsMOH4WIcHaS1_mXtkwJPZQ9s-XnZE46juo,17543
+onnxruntime/transformers/large_model_exporter.py,sha256=dIQBjuu8YBh6DYAV-vgoAqiZe0Prb2nsZoVZEW0yf1Q,15310
 onnxruntime/transformers/machine_info.py,sha256=wAFDfkm-y_d5SfVQ7iaI_16QjpAvMrGPXHgNCoPBLiI,7282
 onnxruntime/transformers/metrics.py,sha256=I03M327XxrOgj4sLl-AzD5k9n2BlqGpF-cJfamEmHhA,5327
-onnxruntime/transformers/onnx_exporter.py,sha256=FOW0G0csj-pMdlSuEu1oujA8Ku1J_FrR4afo2Svs2cI,25320
-onnxruntime/transformers/onnx_model.py,sha256=GbVdqz6KWIXxp9StQpfSiF45D5P9FnQD3NAcja-6Ogk,64879
+onnxruntime/transformers/onnx_exporter.py,sha256=DvUj1-ozpVwNaY6bCF1u1byZTHWDtRJbd3ClA2opRI8,25176
+onnxruntime/transformers/onnx_model.py,sha256=HZeIHif1IqGWa2tkYa719ZoI7xMPgtWJ62JY_lrNbow,64986
 onnxruntime/transformers/onnx_model_bart.py,sha256=M_5C_iYSFhaJgtvtKEViM25Y_haeikC1XL-DekTFh1o,5579
 onnxruntime/transformers/onnx_model_bert.py,sha256=1pux4Dfi1WnPO7_fuNVPwM57vXCUaYILMvTQQUx8lhQ,19974
 onnxruntime/transformers/onnx_model_bert_keras.py,sha256=XuGewoX6nOch2caSomeCBM4NZRpNG-Pkd7ZOZ_WsKdI,18940
 onnxruntime/transformers/onnx_model_bert_tf.py,sha256=5dfNx09iB_9_IX2xf3ZHfFYalMOaYzpwFqOOWYrLYnE,25433
 onnxruntime/transformers/onnx_model_clip.py,sha256=F8tQrTwQH5691Lx6LdLJhYxfeHdrcBQg58Q8DTEyGks,1297
 onnxruntime/transformers/onnx_model_conformer.py,sha256=yEBXH4eRP7-3VqRN7EAMMTraasfpxXwdpEV99ek3oYw,1444
 onnxruntime/transformers/onnx_model_gpt2.py,sha256=3LmzgHuLvO5tyNHKWGidttyqrcpIE7aLBYbRqzjolUg,3913
+onnxruntime/transformers/onnx_model_phi.py,sha256=wgCktbyZLu6B_8A5ENPYpUpnYHZMQZWNULPOINj20Ms,36377
 onnxruntime/transformers/onnx_model_t5.py,sha256=d3jHUWdEhEr0ugo_N4F_egp_IMmLD7jYz6dfBT48fQw,28931
 onnxruntime/transformers/onnx_model_tnlr.py,sha256=2Y5l3DzuHHikd1taUZYhppES0D59UPciOvlyquNiJJE,8436
-onnxruntime/transformers/onnx_model_unet.py,sha256=QI8hSsC7_Nw34pm76HWz1BSSNPT5NB4gQpxwrNWoWpo,9520
+onnxruntime/transformers/onnx_model_unet.py,sha256=y5OJmrCEtQq2mvlYOHEPbIkVaZgUtkEOCUxK5wlgll4,9517
 onnxruntime/transformers/onnx_model_vae.py,sha256=W1Adx9YYwLhc5CSu3Ykgng2MtCn-OCMeFRVUmMeeY28,1545
-onnxruntime/transformers/optimizer.py,sha256=x3hZJekz5u6fywFqkyKewJzbQwU9Z1jrzHPNBlVCFNM,23667
-onnxruntime/transformers/profiler.py,sha256=5B5p3ANKBhJ7hyEVIfNOOsi5rMAyN0FGGtojY6HB3dw,25009
+onnxruntime/transformers/onnx_utils.py,sha256=MFOmBYBWeMNPiQhIaGPEiBvTRMofytW_wdUH6UGb7RI,2161
+onnxruntime/transformers/optimizer.py,sha256=llwRmYOXkHBwWgtJpwNjZVXA92LK0Q8Rl66ZKeILdZE,25221
+onnxruntime/transformers/profiler.py,sha256=Igx_QE-rVBL17UJlia0RP-QXYtM9AYuUt9gpN9GlsNc,24891
 onnxruntime/transformers/quantize_helper.py,sha256=wyVGd_PquMTf0oxA0iLZmfHEhdAEuPk5CTMKMyQcLrE,2885
 onnxruntime/transformers/shape_infer_helper.py,sha256=Y9RGSB75pGEmFlDS4X8LgivfqMupeE3NG503raVL45E,4591
-onnxruntime/transformers/shape_optimizer.py,sha256=YKiM58qDa8mrDaXQ3pnjGhgkhLItfE8LVt3lkczz9Y4,15575
+onnxruntime/transformers/shape_optimizer.py,sha256=50Loam-3xEYP-Y5h6FP173KHoPi_FtU1wnQBEOkfZeU,15505
 onnxruntime/transformers/torch_onnx_export_helper.py,sha256=DOTqWF9DEbxsxqKWtq3NCqcA7de-JSMgjS-MyczJimg,2575
 onnxruntime/transformers/models/bart/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
 onnxruntime/transformers/models/bart/export.py,sha256=PNlhkbvrxTxSSLXpzqoa02Lektzf8rdZpcVFBxw-qcI,4285
 onnxruntime/transformers/models/bert/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
-onnxruntime/transformers/models/bert/eval_squad.py,sha256=4pfPSaih5ZeYCgzrBouPojstLHqG6EZrYMAjAbD3-bs,12395
+onnxruntime/transformers/models/bert/eval_squad.py,sha256=hE5_03D6TBJJWcicVvJCSTWvvJTBGzFEqGw_6Y51Z0M,12378
 onnxruntime/transformers/models/gpt2/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
-onnxruntime/transformers/models/gpt2/benchmark_gpt2.py,sha256=qiHXuELRUirwGLLw8yB-T3LyqFO32X64SoX3yVf0C2A,15916
+onnxruntime/transformers/models/gpt2/benchmark_gpt2.py,sha256=MHPnn2HHYEJo4yECNDAfeMBGLeOX-OVYi1jxZLLgPsI,15930
 onnxruntime/transformers/models/gpt2/convert_to_onnx.py,sha256=7KR2nN6UJ86TsMNhhqT2fBsnb3LzfPvBrIUOFwUv_X4,20593
-onnxruntime/transformers/models/gpt2/gpt2_helper.py,sha256=jGH_RB-Tv-ho1QhqD087BwNeTZrASQbuOYJ1ByGClhs,41373
-onnxruntime/transformers/models/gpt2/gpt2_parity.py,sha256=jx-lonqQ-uQb8OCyUcUofE7LZbrXwHMZNt3xF4CCmeo,18239
-onnxruntime/transformers/models/gpt2/gpt2_tester.py,sha256=KdsIePL2b1JvNYRXmdqS7n3DUtqsJ2l0NVMcK4aX-10,20078
+onnxruntime/transformers/models/gpt2/gpt2_helper.py,sha256=PuV3Jbk1XPnk_jH8bs37n5GhRRpYqr_kG04rlV3i62M,41381
+onnxruntime/transformers/models/gpt2/gpt2_parity.py,sha256=iRMcaSLzmcCxeWfBaBf_GTWa-DXkcG8mCir2QMXyUzg,18238
+onnxruntime/transformers/models/gpt2/gpt2_tester.py,sha256=cZK0nIL4BToALBE86sPj3Rm9mNVC4BIe5GQPmmZj6zA,20020
 onnxruntime/transformers/models/gpt2/parity_check_helper.py,sha256=jU3bTPvyKgHqxrGIce12_LbqaXC688XnBnBp5AHz_ZM,5806
 onnxruntime/transformers/models/llama/__init__.py,sha256=yR2FucNw-jt_3CbNt-zuM7DmldPq1rJK3SV8gRISzN0,490
-onnxruntime/transformers/models/llama/benchmark.py,sha256=tf5EHU9boQQSRSJq-3DzPLcSLjGoweUIKKCKhdhr_O0,27321
-onnxruntime/transformers/models/llama/benchmark_all.py,sha256=xgom4v59gOYrlsKhzxMRQZadIcvLP1q9xjwM9QV_jw8,15834
+onnxruntime/transformers/models/llama/benchmark.py,sha256=gnjPEQvekmFuCLzOtNowA5WWtr5MK9YD5pwkYDQCYSw,27262
+onnxruntime/transformers/models/llama/benchmark_all.py,sha256=dBhvMrWfCYL768jv7C6r3kFZkIZoVknM-uodKsAgA-k,15851
 onnxruntime/transformers/models/llama/benchmark_e2e.py,sha256=wGOB-xEmI3aGy4bNn_qwz_ElHy9P0Jvy2DX6o_8r7-Y,24221
-onnxruntime/transformers/models/llama/convert_to_onnx.py,sha256=AayOwzzmOaoLelmST9BztaLyX-6H_-FExbM3kFxhtSw,43368
+onnxruntime/transformers/models/llama/convert_to_onnx.py,sha256=dwoZIRZSD_Pz2XdHmbzl0-AYBeSghlsUQzNFRuds0oE,43492
 onnxruntime/transformers/models/llama/dist_settings.py,sha256=4rLhv9WYMsAeSYTlLJfvGn3BlnUoXSGSEh3ORCmgpgc,1636
-onnxruntime/transformers/models/llama/llama_inputs.py,sha256=ua8ixJb_iOKmWMb9WEHjU4Xlz3IS8dR2l1zcsmvwcS8,20898
-onnxruntime/transformers/models/llama/llama_parity.py,sha256=v2ODsR38yqKx7hF4fKZ7Bi8-mbFlEeoYQ0iF9zwkbOU,9000
+onnxruntime/transformers/models/llama/llama_inputs.py,sha256=weNcvmNxOdy6zG1CZMmy5HnkOtpNFKEZtAwC1ZUJpIA,21005
+onnxruntime/transformers/models/llama/llama_parity.py,sha256=dWAqlbDds0zmgcnIqHUpSNQpqbwvUSscCzO9taflxcc,10236
 onnxruntime/transformers/models/llama/llama_torch.py,sha256=jhp3ladbXlo45w11ocUuU_QVIHdy77oNSFyu_tUnpbk,1665
 onnxruntime/transformers/models/llama/quant_kv_dataloader.py,sha256=piVldpGm9eBmF4wzgmKJprhujqTPddqORxZyLizcJdA,4959
 onnxruntime/transformers/models/longformer/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
-onnxruntime/transformers/models/longformer/benchmark_longformer.py,sha256=34LdDWpBgA7ysT6IYEoXLgTSFI2imgiYmnpbPMtTFD8,30284
+onnxruntime/transformers/models/longformer/benchmark_longformer.py,sha256=oaYEyUGnTBjXdayIMaKR6UGuCOsfn9FYeyNjin13oEY,30250
 onnxruntime/transformers/models/longformer/convert_to_onnx.py,sha256=cTmSpSZhytBrM40Ys1r4FCUctyovXS3_e40_iozD4Bk,15219
 onnxruntime/transformers/models/longformer/generate_test_data.py,sha256=wQxpgo_vZBhKRlquJwUB9FH3_xxvyDC3aCCZdkvADLM,9964
 onnxruntime/transformers/models/longformer/longformer_helper.py,sha256=FH7Uykc57rLNr1l0pr85OVgr9PZE_4x29xdE-t1riC4,3180
+onnxruntime/transformers/models/phi2/__init__.py,sha256=yR2FucNw-jt_3CbNt-zuM7DmldPq1rJK3SV8gRISzN0,490
+onnxruntime/transformers/models/phi2/convert_to_onnx.py,sha256=OZeFmtIYKrSYGkipD1-xcIYR8ryJfGUWqW2coqMZ2zo,20376
+onnxruntime/transformers/models/phi2/inference_example.py,sha256=xZBHx3iFJ8jQvvNxFZTdYAFLxdOPYcldmk53ktgcjew,17700
 onnxruntime/transformers/models/stable_diffusion/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
-onnxruntime/transformers/models/stable_diffusion/benchmark.py,sha256=FcJZIG1FCY8hUZbr77irhYgZHyDjKEP8VQpuq7uAjr4,48065
+onnxruntime/transformers/models/stable_diffusion/benchmark.py,sha256=hTMR3Jade2ce1V0NYM1imvImP3dVThIkU1y_2Zw_TJA,48353
 onnxruntime/transformers/models/stable_diffusion/benchmark_controlnet.py,sha256=0wLUcJMeSzrDOynjT30wC-8ncukGmgEnv5ngv9rFNk0,13253
-onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py,sha256=rfffYPmuhTluszytl_zKiP1WkJo8cx5nnELgxVcLos0,3142
-onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py,sha256=W8HAxbvYk3rTJsbZuJ0BvgLmsL3vFyRDO15YVHFdPYI,9914
-onnxruntime/transformers/models/stable_diffusion/demo_utils.py,sha256=wuv0dyHH9KDwXGizJjtRUrrfNiPUppBtpk0vgQd9asw,28609
-onnxruntime/transformers/models/stable_diffusion/diffusion_models.py,sha256=dd8SmVsRuX87gnaUY1TlLSqqhEvnn3hPerOfpijARXA,51724
+onnxruntime/transformers/models/stable_diffusion/demo_txt2img.py,sha256=k_H66rQBSryX5MvHzheAcKq5pXIMPeoktzxP-EKfrPw,3394
+onnxruntime/transformers/models/stable_diffusion/demo_txt2img_xl.py,sha256=F1DGjYE-kDuYdBn9ZMPT5Lc7jJNsCs_ZkTT5JgcGmD8,10179
+onnxruntime/transformers/models/stable_diffusion/demo_utils.py,sha256=yk5Bw1Pu3RAluY_UE5Z1FvmjU8vQt8aQp0Low2X3tI0,29367
+onnxruntime/transformers/models/stable_diffusion/diffusion_models.py,sha256=wbQLaKqh4q4_LVK5cwzbFI-r0m3zuw8AMB_o9CfRpK4,51710
 onnxruntime/transformers/models/stable_diffusion/diffusion_schedulers.py,sha256=liNQ-O8mXkVDd5BWACjV58Fg6kVx_gWf6hp0mmySKzw,49538
-onnxruntime/transformers/models/stable_diffusion/engine_builder.py,sha256=JzeHkXqqtaFzTn-yvkPApW0MFbws6MmeZrv3ygcQKmo,11889
-onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py,sha256=kHq7TjJ7QkcrKrNgSithVNKVco2aWTShGiI4E1syeso,15150
+onnxruntime/transformers/models/stable_diffusion/engine_builder.py,sha256=8VkzQCo_UwgU6GeMVXcm9QbZSz09gsv6o2TtFwSG7os,11980
+onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_cuda.py,sha256=hFXQIYGDYFzg8Vi4OVAcUknkHnwFe7B4gfEcifyZfZw,16294
 onnxruntime/transformers/models/stable_diffusion/engine_builder_ort_trt.py,sha256=0M-JLT3Z1zYPEVkJ0TPCZuhbIFCstbBi5Wh623VLcww,11451
 onnxruntime/transformers/models/stable_diffusion/engine_builder_tensorrt.py,sha256=LSoAcwy4nwFQ4aikwJimlAQl1iXAWX4nIbcWD2H7qPw,15999
 onnxruntime/transformers/models/stable_diffusion/engine_builder_torch.py,sha256=4SeIgxcd7Vv3WuSpKcklESud8-O6tZKDVpFssNCzUTg,4289
 onnxruntime/transformers/models/stable_diffusion/optimize_pipeline.py,sha256=LocY1eyRHdTGaTSjzBSLIA2-AnfSszfk7Lp4kr24vBE,12881
 onnxruntime/transformers/models/stable_diffusion/ort_optimizer.py,sha256=Xi35IWxLtzTcXBhffGeOx2ltV_xCsjAzz7BBwi78mDE,5836
-onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py,sha256=EeYRZpO3Hq8odN3wuwUiry1H48uMeXxVPIFiosz4PC0,33667
+onnxruntime/transformers/models/stable_diffusion/pipeline_stable_diffusion.py,sha256=TerkRHaxgs3HhSdT4YisYO_bijBBDZ3RmbUZojmosrw,33995
 onnxruntime/transformers/models/stable_diffusion/trt_utilities.py,sha256=XZCfqG_kZ72e-L9p7PlGqc4NLvFZF1h40A6Guyj6z8k,432
 onnxruntime/transformers/models/t5/__init__.py,sha256=F8Gml7gD7jmMyMIkKvHiXLfj7kWyz175X1-5_iaFx5k,495
 onnxruntime/transformers/models/t5/convert_to_onnx.py,sha256=6n8mojdaT-4PtVJyQeUYboIzJcIljjaL3xdVmigUDaM,9010
 onnxruntime/transformers/models/t5/past_helper.py,sha256=ounFkzTPTM0N9gjZ70jhh-grskckMQwCu2KsDupljpM,6987
 onnxruntime/transformers/models/t5/t5_decoder.py,sha256=jboQOt9bzfIYj5RiHr5c9Le8_xM2iCTy9a7ZoqDoQsY,17262
 onnxruntime/transformers/models/t5/t5_encoder.py,sha256=iTbEUNf9zE8wtrRYkvJ7mfSfXmGvwuDlcZTjKKhFRfE,6295
 onnxruntime/transformers/models/t5/t5_encoder_decoder_init.py,sha256=13eGZw0soWdbjzpldIfsBeEYyYCF3jFDSbaaeai25U0,12273
 onnxruntime/transformers/models/t5/t5_helper.py,sha256=GfXBXQ8PhJIvQxXBF3UajyQcKeAksYJwcvBVE1tjje8,11032
 onnxruntime/transformers/models/whisper/__init__.py,sha256=yR2FucNw-jt_3CbNt-zuM7DmldPq1rJK3SV8gRISzN0,490
-onnxruntime/transformers/models/whisper/benchmark.py,sha256=MPxpRQ7lsDt1Oq7OuojxZBTpfd6GW9ih6lI4dHKNmkg,23329
+onnxruntime/transformers/models/whisper/benchmark.py,sha256=Trl33hh_AudH13Ev3okjS3gEtL2Fd-BrKXj3vAnfZXc,23376
 onnxruntime/transformers/models/whisper/benchmark_all.py,sha256=I5o1O0-rKc1Z6RyE5mp51rHLyTdGFi-MoJIEZiB6m30,19461
-onnxruntime/transformers/models/whisper/convert_to_onnx.py,sha256=5T1CDxDkZB_Bwsq7H_92Rp2YgpctvDZVI0wpk_uPbDw,18396
+onnxruntime/transformers/models/whisper/convert_to_onnx.py,sha256=lD3GYfFviG03OJ_QRMNtgLsAjEldoBziUslkkHxhZow,18404
 onnxruntime/transformers/models/whisper/whisper_chain.py,sha256=r4csnoac1FiZh--2oHOwKqar4i-Uhe6TJ9ejkGE-PSQ,14910
 onnxruntime/transformers/models/whisper/whisper_decoder.py,sha256=8vqR-FefnQuFpgTOZHV7LUUG5sbZB0w_Cy_LzdjCn0c,16021
 onnxruntime/transformers/models/whisper/whisper_encoder.py,sha256=6LyQmT-JasSzfYRqG4Mr9Cg5vp60TqTt22q3__Ey700,5740
 onnxruntime/transformers/models/whisper/whisper_encoder_decoder_init.py,sha256=TA27lOA_Fd-j3yulAf3773Fv9UnyZYHWveqbJiAYCDg,12723
-onnxruntime/transformers/models/whisper/whisper_helper.py,sha256=ydEyBA3CuBrO_Jc0ly_uI2Zpce5rqQFE22RMNxCDkcQ,23487
+onnxruntime/transformers/models/whisper/whisper_helper.py,sha256=VatGmGiB0NJr1m1y5IqSLkWamQhj-ygmmL-9I-0lgxM,23487
 onnxruntime/transformers/models/whisper/whisper_openai_helper.py,sha256=uF1MpfwD8LWFmA6-tWLq10ocB-yVFx_7NA_dL3Rsy0g,3272
-onnxruntime_training_cpu-1.17.3.dist-info/METADATA,sha256=ZfhwxYwxrjvVPXfUy_PTyvavbSBY_xTSfGJ0LI3c1sM,4712
-onnxruntime_training_cpu-1.17.3.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-onnxruntime_training_cpu-1.17.3.dist-info/entry_points.txt,sha256=7qLS4FbGXwPZjfdpVAGpnmk9I6m6H5CxEnwcCx1Imjs,77
-onnxruntime_training_cpu-1.17.3.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
-onnxruntime_training_cpu-1.17.3.dist-info/RECORD,,
+onnxruntime_training_cpu-1.18.0.dist-info/METADATA,sha256=G8QNg8TeqW5xjGOo1AqdoMIYRsRHLOSywRtZLCXqqDc,4514
+onnxruntime_training_cpu-1.18.0.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+onnxruntime_training_cpu-1.18.0.dist-info/entry_points.txt,sha256=7qLS4FbGXwPZjfdpVAGpnmk9I6m6H5CxEnwcCx1Imjs,77
+onnxruntime_training_cpu-1.18.0.dist-info/top_level.txt,sha256=zk_fJEekrTm9DLxX2LwGegokVqP6blqPhFoMIuh0Nv8,12
+onnxruntime_training_cpu-1.18.0.dist-info/RECORD,,
```

