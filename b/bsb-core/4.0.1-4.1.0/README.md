# Comparing `tmp/bsb_core-4.0.1.tar.gz` & `tmp/bsb_core-4.1.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "bsb_core-4.0.1.tar", last modified: Fri Jan  1 00:00:00 2016, max compression
+gzip compressed data, was "bsb_core-4.1.0.tar", last modified: Fri Jan  1 00:00:00 2016, max compression
```

## Comparing `bsb_core-4.0.1.tar` & `bsb_core-4.1.0.tar`

### file list

```diff
@@ -1,84 +1,85 @@
--rwxr-xr-x   0        0        0    35823 2023-05-29 16:41:30.402591 bsb_core-4.0.1/LICENSE
--rwxr-xr-x   0        0        0     3414 2024-03-28 15:57:42.471064 bsb_core-4.0.1/README.md
--rwxr-xr-x   0        0        0    22920 2024-03-29 17:15:04.067335 bsb_core-4.0.1/bsb/__init__.py
--rwxr-xr-x   0        0        0       39 2022-12-26 16:57:07.079974 bsb_core-4.0.1/bsb/__main__.py
--rwxr-xr-x   0        0        0     1358 2024-03-28 14:35:37.800195 bsb_core-4.0.1/bsb/_contexts.py
--rwxr-xr-x   0        0        0     7065 2024-02-10 10:26:57.528814 bsb_core-4.0.1/bsb/_encoding.py
--rwxr-xr-x   0        0        0     3112 2024-03-27 13:51:06.269448 bsb_core-4.0.1/bsb/_options.py
--rwxr-xr-x   0        0        0     1769 2024-03-08 11:48:27.336674 bsb_core-4.0.1/bsb/_package_spec.py
--rwxr-xr-x   0        0        0     4313 2024-03-07 16:52:53.360895 bsb_core-4.0.1/bsb/_util.py
--rwxr-xr-x   0        0        0     4607 2024-03-28 15:57:42.047553 bsb_core-4.0.1/bsb/cell_types.py
--rwxr-xr-x   0        0        0     1256 2024-03-27 15:45:00.842970 bsb_core-4.0.1/bsb/cli/__init__.py
--rwxr-xr-x   0        0        0     5385 2024-03-28 15:57:42.047553 bsb_core-4.0.1/bsb/cli/commands/__init__.py
--rwxr-xr-x   0        0        0     7837 2024-03-28 14:35:37.800699 bsb_core-4.0.1/bsb/cli/commands/_commands.py
--rwxr-xr-x   0        0        0     2679 2024-03-08 11:48:27.339673 bsb_core-4.0.1/bsb/cli/commands/_projects.py
--rwxr-xr-x   0        0        0     6692 2024-03-28 15:57:42.048888 bsb_core-4.0.1/bsb/config/__init__.py
--rwxr-xr-x   0        0        0    39135 2024-03-28 14:35:37.801704 bsb_core-4.0.1/bsb/config/_attrs.py
--rwxr-xr-x   0        0        0     2292 2023-11-30 16:56:17.977899 bsb_core-4.0.1/bsb/config/_compile.py
--rwxr-xr-x   0        0        0     5625 2024-03-18 13:54:02.873201 bsb_core-4.0.1/bsb/config/_config.py
--rwxr-xr-x   0        0        0     1783 2023-11-30 16:56:17.979396 bsb_core-4.0.1/bsb/config/_distributions.py
--rwxr-xr-x   0        0        0     5127 2022-11-03 10:07:00.689267 bsb_core-4.0.1/bsb/config/_hooks.py
--rwxr-xr-x   0        0        0    26744 2024-03-08 11:48:27.340673 bsb_core-4.0.1/bsb/config/_make.py
--rwxr-xr-x   0        0        0     1021 2024-03-28 15:57:42.048888 bsb_core-4.0.1/bsb/config/parsers.py
--rwxr-xr-x   0        0        0     3586 2024-03-08 11:48:27.341674 bsb_core-4.0.1/bsb/config/refs.py
--rwxr-xr-x   0        0        0    24762 2024-03-27 13:36:54.614303 bsb_core-4.0.1/bsb/config/types.py
--rwxr-xr-x   0        0        0      234 2024-03-28 15:57:42.050352 bsb_core-4.0.1/bsb/connectivity/__init__.py
--rwxr-xr-x   0        0        0       51 2024-03-28 15:57:42.050352 bsb_core-4.0.1/bsb/connectivity/detailed/__init__.py
--rwxr-xr-x   0        0        0     3651 2024-03-28 14:35:37.802704 bsb_core-4.0.1/bsb/connectivity/detailed/shared.py
--rwxr-xr-x   0        0        0     7034 2024-03-08 11:48:27.346223 bsb_core-4.0.1/bsb/connectivity/detailed/voxel_intersection.py
--rwxr-xr-x   0        0        0     2572 2024-03-08 11:48:27.346223 bsb_core-4.0.1/bsb/connectivity/general.py
--rwxr-xr-x   0        0        0     6359 2024-03-28 14:35:37.802704 bsb_core-4.0.1/bsb/connectivity/import_.py
--rwxr-xr-x   0        0        0    10587 2024-03-27 13:36:54.615308 bsb_core-4.0.1/bsb/connectivity/strategy.py
--rwxr-xr-x   0        0        0    30394 2024-03-28 14:35:37.803704 bsb_core-4.0.1/bsb/core.py
--rwxr-xr-x   0        0        0     6402 2024-03-28 15:57:42.050352 bsb_core-4.0.1/bsb/exceptions.py
--rwxr-xr-x   0        0        0     4600 2024-03-28 14:35:37.804704 bsb_core-4.0.1/bsb/mixins.py
--rwxr-xr-x   0        0        0    61374 2024-03-27 15:31:01.390840 bsb_core-4.0.1/bsb/morphologies/__init__.py
--rwxr-xr-x   0        0        0      477 2024-03-08 11:48:27.350224 bsb_core-4.0.1/bsb/morphologies/parsers/__init__.py
--rwxr-xr-x   0        0        0    11479 2024-03-28 14:35:37.805704 bsb_core-4.0.1/bsb/morphologies/parsers/parser.py
--rwxr-xr-x   0        0        0     5982 2024-03-08 11:48:27.351223 bsb_core-4.0.1/bsb/morphologies/selector.py
--rwxr-xr-x   0        0        0    12956 2024-03-27 13:52:58.445336 bsb_core-4.0.1/bsb/option.py
--rwxr-xr-x   0        0        0    11657 2024-03-28 14:35:37.805704 bsb_core-4.0.1/bsb/options.py
--rwxr-xr-x   0        0        0      327 2024-03-28 15:57:42.051860 bsb_core-4.0.1/bsb/placement/__init__.py
--rwxr-xr-x   0        0        0     5225 2024-03-08 11:48:27.354224 bsb_core-4.0.1/bsb/placement/arrays.py
--rwxr-xr-x   0        0        0    13877 2024-03-28 14:35:37.806704 bsb_core-4.0.1/bsb/placement/distributor.py
--rwxr-xr-x   0        0        0     5365 2024-03-28 14:35:37.806704 bsb_core-4.0.1/bsb/placement/import_.py
--rwxr-xr-x   0        0        0     8446 2024-03-28 15:57:43.792010 bsb_core-4.0.1/bsb/placement/indicator.py
--rwxr-xr-x   0        0        0    11603 2024-03-28 15:57:42.053354 bsb_core-4.0.1/bsb/placement/random.py
--rwxr-xr-x   0        0        0     8458 2024-03-28 15:57:42.054360 bsb_core-4.0.1/bsb/placement/strategy.py
--rwxr-xr-x   0        0        0     2193 2024-03-08 11:48:27.358224 bsb_core-4.0.1/bsb/plugins.py
--rwxr-xr-x   0        0        0     6987 2024-03-18 13:54:02.875447 bsb_core-4.0.1/bsb/postprocessing.py
--rwxr-xr-x   0        0        0     4610 2024-03-18 15:27:07.950309 bsb_core-4.0.1/bsb/profiling.py
--rwxr-xr-x   0        0        0     1976 2024-03-28 14:35:37.807704 bsb_core-4.0.1/bsb/reporting.py
--rwxr-xr-x   0        0        0      781 2024-03-08 11:48:27.359769 bsb_core-4.0.1/bsb/services/__init__.py
--rwxr-xr-x   0        0        0     7382 2024-03-18 13:54:02.876934 bsb_core-4.0.1/bsb/services/_pool_listeners.py
--rwxr-xr-x   0        0        0      487 2024-03-28 14:35:37.808704 bsb_core-4.0.1/bsb/services/_util.py
--rwxr-xr-x   0        0        0     1811 2024-03-07 16:52:53.375263 bsb_core-4.0.1/bsb/services/mpi.py
--rwxr-xr-x   0        0        0     2857 2024-03-07 16:52:53.375263 bsb_core-4.0.1/bsb/services/mpilock.py
--rwxr-xr-x   0        0        0    28393 2024-03-28 17:32:25.418621 bsb_core-4.0.1/bsb/services/pool.py
--rwxr-xr-x   0        0        0      443 2024-03-08 11:48:27.362069 bsb_core-4.0.1/bsb/simulation/__init__.py
--rwxr-xr-x   0        0        0      895 2024-02-16 10:00:57.706008 bsb_core-4.0.1/bsb/simulation/_backends.py
--rwxr-xr-x   0        0        0     3730 2024-03-08 11:48:27.363083 bsb_core-4.0.1/bsb/simulation/adapter.py
--rwxr-xr-x   0        0        0      935 2024-03-08 11:48:27.364081 bsb_core-4.0.1/bsb/simulation/cell.py
--rwxr-xr-x   0        0        0      381 2024-03-08 11:48:27.364081 bsb_core-4.0.1/bsb/simulation/component.py
--rwxr-xr-x   0        0        0      313 2024-03-08 11:48:27.365081 bsb_core-4.0.1/bsb/simulation/connection.py
--rwxr-xr-x   0        0        0      389 2024-03-08 11:48:27.366081 bsb_core-4.0.1/bsb/simulation/device.py
--rwxr-xr-x   0        0        0      405 2024-03-08 11:48:27.366081 bsb_core-4.0.1/bsb/simulation/parameter.py
--rwxr-xr-x   0        0        0     1625 2024-03-08 11:48:27.367081 bsb_core-4.0.1/bsb/simulation/results.py
--rwxr-xr-x   0        0        0     2243 2024-03-08 11:48:27.367081 bsb_core-4.0.1/bsb/simulation/simulation.py
--rwxr-xr-x   0        0        0     8893 2024-03-08 11:48:27.368081 bsb_core-4.0.1/bsb/simulation/targetting.py
--rwxr-xr-x   0        0        0    13991 2024-03-28 14:35:37.809704 bsb_core-4.0.1/bsb/storage/__init__.py
--rwxr-xr-x   0        0        0     3404 2024-03-28 15:57:42.054360 bsb_core-4.0.1/bsb/storage/_chunks.py
--rwxr-xr-x   0        0        0    21179 2024-03-28 15:57:42.055362 bsb_core-4.0.1/bsb/storage/_files.py
--rwxr-xr-x   0        0        0      709 2024-03-08 11:48:27.370081 bsb_core-4.0.1/bsb/storage/_util.py
--rwxr-xr-x   0        0        0     1256 2024-03-08 11:48:27.371081 bsb_core-4.0.1/bsb/storage/decorators.py
--rwxr-xr-x   0        0        0     3073 2024-03-29 17:09:41.585448 bsb_core-4.0.1/bsb/storage/fs/__init__.py
--rwxr-xr-x   0        0        0     4849 2024-03-08 11:48:27.372081 bsb_core-4.0.1/bsb/storage/fs/file_store.py
--rwxr-xr-x   0        0        0    37120 2024-03-29 17:09:24.478260 bsb_core-4.0.1/bsb/storage/interfaces.py
--rwxr-xr-x   0        0        0     3493 2024-03-08 11:48:27.373081 bsb_core-4.0.1/bsb/topology/__init__.py
--rwxr-xr-x   0        0        0     3766 2023-11-30 16:56:18.016817 bsb_core-4.0.1/bsb/topology/_layout.py
--rwxr-xr-x   0        0        0    23875 2024-03-28 15:57:42.057361 bsb_core-4.0.1/bsb/topology/partition.py
--rwxr-xr-x   0        0        0     3610 2024-03-08 11:48:27.375568 bsb_core-4.0.1/bsb/topology/region.py
--rwxr-xr-x   0        0        0     2405 2023-11-30 16:56:18.018322 bsb_core-4.0.1/bsb/trees.py
--rwxr-xr-x   0        0        0    23088 2024-03-08 11:48:27.375568 bsb_core-4.0.1/bsb/voxels.py
--rwxr-xr-x   0        0        0     2912 2024-03-29 17:15:04.093294 bsb_core-4.0.1/pyproject.toml
--rw-r--r--   0        0        0     5671 1970-01-01 00:00:00.000000 bsb_core-4.0.1/PKG-INFO
+-rw-r--r--   0        0        0    35149 2024-05-12 11:26:48.206855 bsb_core-4.1.0/LICENSE
+-rw-r--r--   0        0        0     3334 2024-05-21 11:53:23.634194 bsb_core-4.1.0/README.md
+-rw-r--r--   0        0        0    22690 2024-05-21 12:09:20.534370 bsb_core-4.1.0/bsb/__init__.py
+-rw-r--r--   0        0        0       36 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/__main__.py
+-rw-r--r--   0        0        0     1313 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/_contexts.py
+-rw-r--r--   0        0        0     6869 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/_encoding.py
+-rw-r--r--   0        0        0     2952 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/_options.py
+-rw-r--r--   0        0        0     1709 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/_package_spec.py
+-rw-r--r--   0        0        0     4167 2024-05-20 08:10:33.397477 bsb_core-4.1.0/bsb/_util.py
+-rw-r--r--   0        0        0     4454 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/cell_types.py
+-rw-r--r--   0        0        0     1212 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/cli/__init__.py
+-rw-r--r--   0        0        0     5231 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/cli/commands/__init__.py
+-rw-r--r--   0        0        0     7572 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/cli/commands/_commands.py
+-rw-r--r--   0        0        0     2604 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/cli/commands/_projects.py
+-rw-r--r--   0        0        0     6450 2024-05-17 18:58:03.900657 bsb_core-4.1.0/bsb/config/__init__.py
+-rw-r--r--   0        0        0    38574 2024-05-20 11:05:25.786863 bsb_core-4.1.0/bsb/config/_attrs.py
+-rw-r--r--   0        0        0     2238 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/config/_compile.py
+-rw-r--r--   0        0        0     5434 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/config/_config.py
+-rw-r--r--   0        0        0     1725 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/config/_distributions.py
+-rw-r--r--   0        0        0     4960 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/config/_hooks.py
+-rw-r--r--   0        0        0    26006 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/config/_make.py
+-rw-r--r--   0        0        0     4667 2024-05-20 08:10:33.397477 bsb_core-4.1.0/bsb/config/_parse_types.py
+-rw-r--r--   0        0        0     7928 2024-05-21 11:49:12.901008 bsb_core-4.1.0/bsb/config/parsers.py
+-rw-r--r--   0        0        0     3451 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/config/refs.py
+-rw-r--r--   0        0        0    24042 2024-05-20 11:05:25.786863 bsb_core-4.1.0/bsb/config/types.py
+-rw-r--r--   0        0        0      226 2024-05-14 10:37:35.152261 bsb_core-4.1.0/bsb/connectivity/__init__.py
+-rw-r--r--   0        0        0       50 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/connectivity/detailed/__init__.py
+-rw-r--r--   0        0        0     3563 2024-05-14 10:37:35.152261 bsb_core-4.1.0/bsb/connectivity/detailed/shared.py
+-rw-r--r--   0        0        0     6873 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/connectivity/detailed/voxel_intersection.py
+-rw-r--r--   0        0        0     2495 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/connectivity/general.py
+-rw-r--r--   0        0        0     6193 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/connectivity/import_.py
+-rw-r--r--   0        0        0    10313 2024-05-14 10:37:35.152261 bsb_core-4.1.0/bsb/connectivity/strategy.py
+-rw-r--r--   0        0        0    29600 2024-05-21 11:49:12.901008 bsb_core-4.1.0/bsb/core.py
+-rw-r--r--   0        0        0     6293 2024-05-20 08:10:33.401477 bsb_core-4.1.0/bsb/exceptions.py
+-rw-r--r--   0        0        0     4464 2024-05-12 11:26:48.206855 bsb_core-4.1.0/bsb/mixins.py
+-rw-r--r--   0        0        0    60206 2024-05-20 11:05:25.786863 bsb_core-4.1.0/bsb/morphologies/__init__.py
+-rw-r--r--   0        0        0      462 2024-05-20 10:42:40.988597 bsb_core-4.1.0/bsb/morphologies/parsers/__init__.py
+-rw-r--r--   0        0        0    11210 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/morphologies/parsers/parser.py
+-rw-r--r--   0        0        0     5824 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/morphologies/selector.py
+-rw-r--r--   0        0        0    12546 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/option.py
+-rw-r--r--   0        0        0    11250 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/options.py
+-rw-r--r--   0        0        0      318 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/__init__.py
+-rw-r--r--   0        0        0     5122 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/arrays.py
+-rw-r--r--   0        0        0    13510 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/distributor.py
+-rw-r--r--   0        0        0     5217 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/import_.py
+-rw-r--r--   0        0        0     8246 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/indicator.py
+-rw-r--r--   0        0        0    11317 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/random.py
+-rw-r--r--   0        0        0     8240 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/placement/strategy.py
+-rw-r--r--   0        0        0     2115 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/plugins.py
+-rw-r--r--   0        0        0     6806 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/postprocessing.py
+-rw-r--r--   0        0        0     4425 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/profiling.py
+-rw-r--r--   0        0        0     1904 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/reporting.py
+-rw-r--r--   0        0        0      745 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/services/__init__.py
+-rw-r--r--   0        0        0     7167 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/services/_pool_listeners.py
+-rw-r--r--   0        0        0      466 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/services/_util.py
+-rw-r--r--   0        0        0     1741 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/services/mpi.py
+-rw-r--r--   0        0        0     2731 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/services/mpilock.py
+-rw-r--r--   0        0        0    27545 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/services/pool.py
+-rw-r--r--   0        0        0      424 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/__init__.py
+-rw-r--r--   0        0        0      867 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/_backends.py
+-rw-r--r--   0        0        0     3612 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/adapter.py
+-rw-r--r--   0        0        0      897 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/cell.py
+-rw-r--r--   0        0        0      361 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/component.py
+-rw-r--r--   0        0        0      300 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/connection.py
+-rw-r--r--   0        0        0      374 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/device.py
+-rw-r--r--   0        0        0      390 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/parameter.py
+-rw-r--r--   0        0        0     1563 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/results.py
+-rw-r--r--   0        0        0     2177 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/simulation.py
+-rwxr-xr-x   0        0        0     8589 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/simulation/targetting.py
+-rw-r--r--   0        0        0    13564 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/__init__.py
+-rw-r--r--   0        0        0     3286 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/_chunks.py
+-rw-r--r--   0        0        0    21326 2024-05-20 11:05:25.786863 bsb_core-4.1.0/bsb/storage/_files.py
+-rw-r--r--   0        0        0      676 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/_util.py
+-rw-r--r--   0        0        0     1212 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/decorators.py
+-rw-r--r--   0        0        0     2959 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/fs/__init__.py
+-rw-r--r--   0        0        0     4702 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/fs/file_store.py
+-rw-r--r--   0        0        0    35902 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/storage/interfaces.py
+-rw-r--r--   0        0        0     3379 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/topology/__init__.py
+-rw-r--r--   0        0        0     3617 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/topology/_layout.py
+-rw-r--r--   0        0        0    23197 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/topology/partition.py
+-rw-r--r--   0        0        0     3480 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/topology/region.py
+-rw-r--r--   0        0        0     2329 2024-05-12 11:26:48.210855 bsb_core-4.1.0/bsb/trees.py
+-rw-r--r--   0        0        0    22437 2024-05-20 11:05:25.786863 bsb_core-4.1.0/bsb/voxels.py
+-rw-r--r--   0        0        0     2912 2024-05-21 11:50:05.248371 bsb_core-4.1.0/pyproject.toml
+-rw-r--r--   0        0        0     5687 1970-01-01 00:00:00.000000 bsb_core-4.1.0/PKG-INFO
```

### Comparing `bsb_core-4.0.1/LICENSE` & `bsb_core-4.1.0/LICENSE`

 * *Ordering differences only*

 * *Files 7% similar despite different names*

```diff
@@ -1,674 +1,674 @@
-                    GNU GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU General Public License is a free, copyleft license for
-software and other kinds of works.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-the GNU General Public License is intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.  We, the Free Software Foundation, use the
-GNU General Public License for most of our software; it applies also to
-any other work released this way by its authors.  You can apply it to
-your programs, too.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  To protect your rights, we need to prevent others from denying you
-these rights or asking you to surrender the rights.  Therefore, you have
-certain responsibilities if you distribute copies of the software, or if
-you modify it: responsibilities to respect the freedom of others.
-
-  For example, if you distribute copies of such a program, whether
-gratis or for a fee, you must pass on to the recipients the same
-freedoms that you received.  You must make sure that they, too, receive
-or can get the source code.  And you must show them these terms so they
-know their rights.
-
-  Developers that use the GNU GPL protect your rights with two steps:
-(1) assert copyright on the software, and (2) offer you this License
-giving you legal permission to copy, distribute and/or modify it.
-
-  For the developers' and authors' protection, the GPL clearly explains
-that there is no warranty for this free software.  For both users' and
-authors' sake, the GPL requires that modified versions be marked as
-changed, so that their problems will not be attributed erroneously to
-authors of previous versions.
-
-  Some devices are designed to deny users access to install or run
-modified versions of the software inside them, although the manufacturer
-can do so.  This is fundamentally incompatible with the aim of
-protecting users' freedom to change the software.  The systematic
-pattern of such abuse occurs in the area of products for individuals to
-use, which is precisely where it is most unacceptable.  Therefore, we
-have designed this version of the GPL to prohibit the practice for those
-products.  If such problems arise substantially in other domains, we
-stand ready to extend this provision to those domains in future versions
-of the GPL, as needed to protect the freedom of users.
-
-  Finally, every program is threatened constantly by software patents.
-States should not allow patents to restrict development and use of
-software on general-purpose computers, but in those that do, we wish to
-avoid the special danger that patents applied to a free program could
-make it effectively proprietary.  To prevent this, the GPL assures that
-patents cannot be used to render the program non-free.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Use with the GNU Affero General Public License.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU Affero General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the special requirements of the GNU Affero General Public License,
-section 13, concerning interaction through a network will apply to the
-combination as such.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU General Public License from time to time.  Such new versions will
-be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
-
-                     END OF TERMS AND CONDITIONS
-
-            How to Apply These Terms to Your New Programs
-
-  If you develop a new program, and you want it to be of the greatest
-possible use to the public, the best way to achieve this is to make it
-free software which everyone can redistribute and change under these terms.
-
-  To do so, attach the following notices to the program.  It is safest
-to attach them to the start of each source file to most effectively
-state the exclusion of warranty; and each file should have at least
-the "copyright" line and a pointer to where the full notice is found.
-
-    <one line to give the program's name and a brief idea of what it does.>
-    Copyright (C) <year>  <name of author>
-
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU General Public License as published by
-    the Free Software Foundation, either version 3 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU General Public License for more details.
-
-    You should have received a copy of the GNU General Public License
-    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
-Also add information on how to contact you by electronic and paper mail.
-
-  If the program does terminal interaction, make it output a short
-notice like this when it starts in an interactive mode:
-
-    <program>  Copyright (C) <year>  <name of author>
-    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
-    This is free software, and you are welcome to redistribute it
-    under certain conditions; type `show c' for details.
-
-The hypothetical commands `show w' and `show c' should show the appropriate
-parts of the General Public License.  Of course, your program's commands
-might be different; for a GUI interface, you would use an "about box".
-
-  You should also get your employer (if you work as a programmer) or school,
-if any, to sign a "copyright disclaimer" for the program, if necessary.
-For more information on this, and how to apply and follow the GNU GPL, see
-<https://www.gnu.org/licenses/>.
-
-  The GNU General Public License does not permit incorporating your program
-into proprietary programs.  If your program is a subroutine library, you
-may consider it more useful to permit linking proprietary applications with
-the library.  If this is what you want to do, use the GNU Lesser General
-Public License instead of this License.  But first, please read
-<https://www.gnu.org/licenses/why-not-lgpl.html>.
+                    GNU GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU General Public License is a free, copyleft license for
+software and other kinds of works.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+the GNU General Public License is intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.  We, the Free Software Foundation, use the
+GNU General Public License for most of our software; it applies also to
+any other work released this way by its authors.  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  To protect your rights, we need to prevent others from denying you
+these rights or asking you to surrender the rights.  Therefore, you have
+certain responsibilities if you distribute copies of the software, or if
+you modify it: responsibilities to respect the freedom of others.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must pass on to the recipients the same
+freedoms that you received.  You must make sure that they, too, receive
+or can get the source code.  And you must show them these terms so they
+know their rights.
+
+  Developers that use the GNU GPL protect your rights with two steps:
+(1) assert copyright on the software, and (2) offer you this License
+giving you legal permission to copy, distribute and/or modify it.
+
+  For the developers' and authors' protection, the GPL clearly explains
+that there is no warranty for this free software.  For both users' and
+authors' sake, the GPL requires that modified versions be marked as
+changed, so that their problems will not be attributed erroneously to
+authors of previous versions.
+
+  Some devices are designed to deny users access to install or run
+modified versions of the software inside them, although the manufacturer
+can do so.  This is fundamentally incompatible with the aim of
+protecting users' freedom to change the software.  The systematic
+pattern of such abuse occurs in the area of products for individuals to
+use, which is precisely where it is most unacceptable.  Therefore, we
+have designed this version of the GPL to prohibit the practice for those
+products.  If such problems arise substantially in other domains, we
+stand ready to extend this provision to those domains in future versions
+of the GPL, as needed to protect the freedom of users.
+
+  Finally, every program is threatened constantly by software patents.
+States should not allow patents to restrict development and use of
+software on general-purpose computers, but in those that do, we wish to
+avoid the special danger that patents applied to a free program could
+make it effectively proprietary.  To prevent this, the GPL assures that
+patents cannot be used to render the program non-free.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Use with the GNU Affero General Public License.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU Affero General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the special requirements of the GNU Affero General Public License,
+section 13, concerning interaction through a network will apply to the
+combination as such.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If the program does terminal interaction, make it output a short
+notice like this when it starts in an interactive mode:
+
+    <program>  Copyright (C) <year>  <name of author>
+    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
+    This is free software, and you are welcome to redistribute it
+    under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate
+parts of the General Public License.  Of course, your program's commands
+might be different; for a GUI interface, you would use an "about box".
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a "copyright disclaimer" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU GPL, see
+<https://www.gnu.org/licenses/>.
+
+  The GNU General Public License does not permit incorporating your program
+into proprietary programs.  If your program is a subroutine library, you
+may consider it more useful to permit linking proprietary applications with
+the library.  If this is what you want to do, use the GNU Lesser General
+Public License instead of this License.  But first, please read
+<https://www.gnu.org/licenses/why-not-lgpl.html>.
```

### Comparing `bsb_core-4.0.1/README.md` & `bsb_core-4.1.0/README.md`

 * *Files 12% similar despite different names*

```diff
@@ -1,96 +1,96 @@
-[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
-[![Documentation Status](https://readthedocs.org/projects/bsb/badge/?version=latest)](https://bsb.readthedocs.io/en/latest/?badge=latest)
-[![Build Status](https://travis-ci.com/dbbs-lab/bsb.svg?branch=master)](https://travis-ci.com/dbbs-lab/bsb)
-[![codecov](https://codecov.io/gh/dbbs-lab/bsb/branch/master/graph/badge.svg)](https://codecov.io/gh/dbbs-lab/bsb)
-
-<h3>:closed_book: Read the documentation on https://bsb.readthedocs.io/en/latest</h3>
-
-# BSB: A component framework for neural modelling
-
-Developed by the Department of Brain and Behavioral Sciences at the University of Pavia,
-the BSB is a component framework for neural modelling, which focusses on component
-declarations to piece together a model. The component declarations can be made in any
-supported configuration language, or using the library functions in Python. It offers
-parallel reconstruction and simulation of any network topology, placement and/or
-connectivity strategy.
-
-
-## Installation
-
-The BSB requires Python 3.9+.
-
-### pip
-
-Any package in the BSB ecosystem can be installed from PyPI through `pip`. Most users
-will want to install the main [bsb](https://pypi.org/project/bsb/) framework:
-
-```
-pip install "bsb~=4.0"
-```
-
-Advanced users looking to control install an unconventional combination of plugins might
-be better of installing just this package, and the desired plugins:
-
-```
-pip install "bsb-core~=4.0"
-```
-
-Note that installing `bsb-core` does not come with any plugins installed and the usually
-available storage engines, or configuration parsers will be missing.
-
-### Developers
-
-Developers best use pip's *editable* install. This creates a live link between the
-installed package and the local git repository:
-
-```
- git clone git@github.com:dbbs-lab/bsb-core
- cd bsb
- pip install -e .[dev]
- pre-commit install
-```
-
-## Usage
-
-The scaffold framework is best used in a project context. Create a working directory for
-each of your modelling projects and use the command line to configure, reconstruct or
-simulate your models.
-
-### Creating a project
-
-You can create a quickstart project using:
-
-```
-bsb new my_model --quickstart
-cd my_model
-```
-
-### Reconstructing a network
-
-You can use your project to create reconstructions of your model, generating cell positions
-and connections:
-
-```
-bsb compile -p
-```
-
-This creates a [network file](bsb.readthedocs.io/getting-started/networks.html) and plots the network.
-
-### Simulating a network
-
-The default project currently contains no simulation config.
-
-# Contributing
-
-All contributions are very much welcome.
-Take a look at the [contribution guide](CONTRIBUTING.md)
-
-# Acknowledgements
-
-This research has received funding from the European Unions Horizon 2020 Framework
-Program for Research and Innovation under the Specific Grant Agreement No. 945539
-(Human Brain Project SGA3) and Specific Grant Agreement No. 785907 (Human Brain
-Project SGA2) and from Centro Fermi project Local Neuronal Microcircuits to ED. We
-acknowledge the use of EBRAINS platform and Fenix Infrastructure resources, which are
-partially funded from the European Unions Horizon 2020 research and innovation
-programme through the ICEI project under the grant agreement No. 800858
+[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
+[![Documentation Status](https://readthedocs.org/projects/bsb/badge/?version=latest)](https://bsb.readthedocs.io/en/latest/?badge=latest)
+[![Build Status](https://travis-ci.com/dbbs-lab/bsb-core.svg?branch=main)](https://travis-ci.com/dbbs-lab/bsb-core)
+[![codecov](https://codecov.io/gh/dbbs-lab/bsb-core/branch/main/graph/badge.svg)](https://codecov.io/gh/dbbs-lab/bsb-core)
+
+<h3>:closed_book: Read the documentation on https://bsb.readthedocs.io/en/latest</h3>
+
+# BSB: A component framework for neural modelling
+
+Developed by the Department of Brain and Behavioral Sciences at the University of Pavia,
+the BSB is a component framework for neural modelling, which focusses on component
+declarations to piece together a model. The component declarations can be made in any
+supported configuration language, or using the library functions in Python. It offers
+parallel reconstruction and simulation of any network topology, placement and/or
+connectivity strategy.
+
+
+## Installation
+
+The BSB requires Python 3.9+.
+
+### pip
+
+Any package in the BSB ecosystem can be installed from PyPI through `pip`. Most users
+will want to install the main [bsb](https://pypi.org/project/bsb/) framework:
+
+```
+pip install "bsb~=4.1"
+```
+
+Advanced users looking to control install an unconventional combination of plugins might
+be better of installing just this package, and the desired plugins:
+
+```
+pip install "bsb-core~=4.1"
+```
+
+Note that installing `bsb-core` does not come with any plugins installed and the usually
+available storage engines, or configuration parsers will be missing.
+
+### Developers
+
+Developers best use pip's *editable* install. This creates a live link between the
+installed package and the local git repository:
+
+```
+ git clone git@github.com:dbbs-lab/bsb-core
+ cd bsb
+ pip install -e .[dev]
+ pre-commit install
+```
+
+## Usage
+
+The scaffold framework is best used in a project context. Create a working directory for
+each of your modelling projects and use the command line to configure, reconstruct or
+simulate your models.
+
+### Creating a project
+
+You can create a quickstart project using:
+
+```
+bsb new my_model --quickstart
+cd my_model
+```
+
+### Reconstructing a network
+
+You can use your project to create reconstructions of your model, generating cell positions
+and connections:
+
+```
+bsb compile -p
+```
+
+This creates a [network file](bsb.readthedocs.io/getting-started/networks.html) and plots the network.
+
+### Simulating a network
+
+The default project currently contains no simulation config.
+
+# Contributing
+
+All contributions are very much welcome.
+Take a look at the [contribution guide](CONTRIBUTING.md)
+
+# Acknowledgements
+
+This research has received funding from the European Unions Horizon 2020 Framework
+Program for Research and Innovation under the Specific Grant Agreement No. 945539
+(Human Brain Project SGA3) and Specific Grant Agreement No. 785907 (Human Brain
+Project SGA2) and from Centro Fermi project Local Neuronal Microcircuits to ED. We
+acknowledge the use of EBRAINS platform and Fenix Infrastructure resources, which are
+partially funded from the European Unions Horizon 2020 research and innovation
+programme through the ICEI project under the grant agreement No. 800858
```

### Comparing `bsb_core-4.0.1/bsb/__init__.py` & `bsb_core-4.1.0/bsb/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,430 +1,433 @@
-"""
-`bsb-core` is the backbone package contain the essential code of the BSB: A component
-framework for multiscale bottom-up neural modelling.
-
-`bsb-core` needs to be installed alongside a bundle of desired bsb plugins, some of
-which are essential for `bsb-core` to function. First time users are recommended to
-install the `bsb` package instead.
-"""
-
-__version__ = "4.0.1"
-
-import functools
-import importlib
-import sys
-import typing
-import warnings
-
-import bsb.exceptions as _exc
-
-# Patch functools on 3.8
-try:
-    functools.cache
-except AttributeError:
-    functools.cache = functools.lru_cache
-
-    # Patch the 'register' method of `singledispatchmethod` pre python 3.10
-    def _register(self, cls, method=None):  # pragma: nocover
-        if hasattr(cls, "__func__"):
-            setattr(cls, "__annotations__", cls.__func__.__annotations__)
-        return self.dispatcher.register(cls, func=method)
-
-    functools.singledispatchmethod.register = _register
-
-
-# Always show all scaffold warnings
-for e in _exc.__dict__.values():
-    if isinstance(e, type) and issubclass(e, Warning):
-        warnings.simplefilter("always", e)
-
-try:
-    from .options import profiling as _pr
-except Exception:
-    pass
-else:
-    if _pr:
-        from .profiling import activate_session
-
-        activate_session()
-
-
-def _get_annotation_submodule(name: str):
-    annotation = __annotations__.get(name, None)
-    if annotation:
-        type_ = typing.get_args(annotation)
-        if type_:
-            # typing.Type["bsb.submodule.name"]
-            annotation = type_[0].__forward_arg__
-        return annotation[4 : -len(name) - 1]
-
-
-@functools.cache
-def __getattr__(name):
-    if name == "config":
-        return object.__getattribute__(sys.modules[__name__], name)
-    module = _get_annotation_submodule(name)
-    if module is None:
-        return object.__getattribute__(sys.modules[__name__], name)
-    else:
-        return getattr(importlib.import_module("." + module, package="bsb"), name)
-
-
-@functools.cache
-def __dir__():
-    return [*__annotations__.keys()]
-
-
-# Do not modify: autogenerated public API type annotations of the `bsb` module
-# fmt: off
-# isort: off
-if typing.TYPE_CHECKING:
-  import bsb.cell_types
-  import bsb.cli
-  import bsb.cli.commands
-  import bsb.config
-  import bsb.config.parsers
-  import bsb.config.refs
-  import bsb.config.types
-  import bsb.connectivity.detailed.shared
-  import bsb.connectivity.detailed.voxel_intersection
-  import bsb.connectivity.general
-  import bsb.connectivity.import_
-  import bsb.connectivity.strategy
-  import bsb.core
-  import bsb.exceptions
-  import bsb.mixins
-  import bsb.morphologies
-  import bsb.morphologies.parsers
-  import bsb.morphologies.parsers.parser
-  import bsb.morphologies.selector
-  import bsb.option
-  import bsb.options
-  import bsb.placement.arrays
-  import bsb.placement.distributor
-  import bsb.placement.import_
-  import bsb.placement.indicator
-  import bsb.placement.random
-  import bsb.placement.strategy
-  import bsb.plugins
-  import bsb.postprocessing
-  import bsb.profiling
-  import bsb.reporting
-  import bsb.services
-  import bsb.simulation
-  import bsb.simulation.adapter
-  import bsb.simulation.cell
-  import bsb.simulation.component
-  import bsb.simulation.connection
-  import bsb.simulation.device
-  import bsb.simulation.parameter
-  import bsb.simulation.results
-  import bsb.simulation.simulation
-  import bsb.simulation.targetting
-  import bsb.storage
-  import bsb.storage._chunks
-  import bsb.storage._files
-  import bsb.storage.decorators
-  import bsb.storage.interfaces
-  import bsb.topology
-  import bsb.topology.partition
-  import bsb.topology.region
-  import bsb.trees
-  import bsb.voxels
-
-AdapterError: typing.Type["bsb.exceptions.AdapterError"]
-AdapterProgress: typing.Type["bsb.simulation.adapter.AdapterProgress"]
-AfterConnectivityHook: typing.Type["bsb.postprocessing.AfterConnectivityHook"]
-AfterPlacementHook: typing.Type["bsb.postprocessing.AfterPlacementHook"]
-AllToAll: typing.Type["bsb.connectivity.general.AllToAll"]
-AllenApiError: typing.Type["bsb.exceptions.AllenApiError"]
-AllenStructure: typing.Type["bsb.topology.partition.AllenStructure"]
-AttributeMissingError: typing.Type["bsb.exceptions.AttributeMissingError"]
-BaseCommand: typing.Type["bsb.cli.commands.BaseCommand"]
-BidirectionalContact: typing.Type["bsb.postprocessing.BidirectionalContact"]
-BootError: typing.Type["bsb.exceptions.BootError"]
-BoxTree: typing.Type["bsb.voxels.BoxTree"]
-BoxTreeInterface: typing.Type["bsb.trees.BoxTreeInterface"]
-Branch: typing.Type["bsb.morphologies.Branch"]
-BranchLocTargetting: typing.Type["bsb.simulation.targetting.BranchLocTargetting"]
-BsbCommand: typing.Type["bsb.cli.commands.BsbCommand"]
-BsbOption: typing.Type["bsb.option.BsbOption"]
-BsbParser: typing.Type["bsb.morphologies.parsers.parser.BsbParser"]
-ByIdTargetting: typing.Type["bsb.simulation.targetting.ByIdTargetting"]
-ByLabelTargetting: typing.Type["bsb.simulation.targetting.ByLabelTargetting"]
-CLIError: typing.Type["bsb.exceptions.CLIError"]
-CLIOptionDescriptor: typing.Type["bsb.option.CLIOptionDescriptor"]
-CastConfigurationError: typing.Type["bsb.exceptions.CastConfigurationError"]
-CastError: typing.Type["bsb.exceptions.CastError"]
-CellModel: typing.Type["bsb.simulation.cell.CellModel"]
-CellModelFilter: typing.Type["bsb.simulation.targetting.CellModelFilter"]
-CellModelTargetting: typing.Type["bsb.simulation.targetting.CellModelTargetting"]
-CellTargetting: typing.Type["bsb.simulation.targetting.CellTargetting"]
-CellType: typing.Type["bsb.cell_types.CellType"]
-CfgReferenceError: typing.Type["bsb.exceptions.CfgReferenceError"]
-Chunk: typing.Type["bsb.storage._chunks.Chunk"]
-ChunkError: typing.Type["bsb.exceptions.ChunkError"]
-CircularMorphologyError: typing.Type["bsb.exceptions.CircularMorphologyError"]
-ClassError: typing.Type["bsb.exceptions.ClassError"]
-ClassMapMissingError: typing.Type["bsb.exceptions.ClassMapMissingError"]
-CodeDependencyNode: typing.Type["bsb.storage._files.CodeDependencyNode"]
-CodeImportError: typing.Type["bsb.exceptions.CodeImportError"]
-CommandError: typing.Type["bsb.exceptions.CommandError"]
-CompartmentError: typing.Type["bsb.exceptions.CompartmentError"]
-CompilationError: typing.Type["bsb.exceptions.CompilationError"]
-ConfigTemplateNotFoundError: typing.Type["bsb.exceptions.ConfigTemplateNotFoundError"]
-Configuration: typing.Type["bsb.config.Configuration"]
-ConfigurationAttribute: typing.Type["bsb.config.ConfigurationAttribute"]
-ConfigurationError: typing.Type["bsb.exceptions.ConfigurationError"]
-ConfigurationFormatError: typing.Type["bsb.exceptions.ConfigurationFormatError"]
-ConfigurationParser: typing.Type["bsb.config.parsers.ConfigurationParser"]
-ConfigurationWarning: typing.Type["bsb.exceptions.ConfigurationWarning"]
-ConnectionModel: typing.Type["bsb.simulation.connection.ConnectionModel"]
-ConnectionStrategy: typing.Type["bsb.connectivity.strategy.ConnectionStrategy"]
-ConnectionTargetting: typing.Type["bsb.simulation.targetting.ConnectionTargetting"]
-ConnectivityError: typing.Type["bsb.exceptions.ConnectivityError"]
-ConnectivityIterator: typing.Type["bsb.storage.interfaces.ConnectivityIterator"]
-ConnectivitySet: typing.Type["bsb.storage.interfaces.ConnectivitySet"]
-ConnectivityWarning: typing.Type["bsb.exceptions.ConnectivityWarning"]
-ContinuityError: typing.Type["bsb.exceptions.ContinuityError"]
-Convergence: typing.Type["bsb.connectivity.general.Convergence"]
-CsvImportConnectivity: typing.Type["bsb.connectivity.import_.CsvImportConnectivity"]
-CsvImportPlacement: typing.Type["bsb.placement.import_.CsvImportPlacement"]
-CylindricalTargetting: typing.Type["bsb.simulation.targetting.CylindricalTargetting"]
-DataNotFoundError: typing.Type["bsb.exceptions.DataNotFoundError"]
-DataNotProvidedError: typing.Type["bsb.exceptions.DataNotProvidedError"]
-DatasetExistsError: typing.Type["bsb.exceptions.DatasetExistsError"]
-DatasetNotFoundError: typing.Type["bsb.exceptions.DatasetNotFoundError"]
-DependencyError: typing.Type["bsb.exceptions.DependencyError"]
-DeviceModel: typing.Type["bsb.simulation.device.DeviceModel"]
-Distribution: typing.Type["bsb.config.Distribution"]
-DistributionCastError: typing.Type["bsb.exceptions.DistributionCastError"]
-DistributionContext: typing.Type["bsb.placement.distributor.DistributionContext"]
-Distributor: typing.Type["bsb.placement.distributor.Distributor"]
-DistributorError: typing.Type["bsb.exceptions.DistributorError"]
-DistributorsNode: typing.Type["bsb.placement.distributor.DistributorsNode"]
-DryrunError: typing.Type["bsb.exceptions.DryrunError"]
-DynamicClassError: typing.Type["bsb.exceptions.DynamicClassError"]
-DynamicClassInheritanceError: typing.Type["bsb.exceptions.DynamicClassInheritanceError"]
-DynamicObjectNotFoundError: typing.Type["bsb.exceptions.DynamicObjectNotFoundError"]
-EmptyBranchError: typing.Type["bsb.exceptions.EmptyBranchError"]
-EmptySelectionError: typing.Type["bsb.exceptions.EmptySelectionError"]
-EmptyVoxelSetError: typing.Type["bsb.exceptions.EmptyVoxelSetError"]
-Engine: typing.Type["bsb.storage.interfaces.Engine"]
-Entities: typing.Type["bsb.placement.strategy.Entities"]
-EnvOptionDescriptor: typing.Type["bsb.option.EnvOptionDescriptor"]
-ExplicitNoRotations: typing.Type["bsb.placement.distributor.ExplicitNoRotations"]
-ExternalSourceError: typing.Type["bsb.exceptions.ExternalSourceError"]
-FileDependency: typing.Type["bsb.storage._files.FileDependency"]
-FileDependencyNode: typing.Type["bsb.storage._files.FileDependencyNode"]
-FileScheme: typing.Type["bsb.storage._files.FileScheme"]
-FileStore: typing.Type["bsb.storage.interfaces.FileStore"]
-FixedIndegree: typing.Type["bsb.connectivity.general.FixedIndegree"]
-FixedPositions: typing.Type["bsb.placement.strategy.FixedPositions"]
-FractionFilter: typing.Type["bsb.simulation.targetting.FractionFilter"]
-GatewayError: typing.Type["bsb.exceptions.GatewayError"]
-GeneratedMorphology: typing.Type["bsb.storage.interfaces.GeneratedMorphology"]
-HasDependencies: typing.Type["bsb.mixins.HasDependencies"]
-Hemitype: typing.Type["bsb.connectivity.strategy.Hemitype"]
-HemitypeCollection: typing.Type["bsb.connectivity.strategy.HemitypeCollection"]
-Implicit: typing.Type["bsb.placement.distributor.Implicit"]
-ImplicitNoRotations: typing.Type["bsb.placement.distributor.ImplicitNoRotations"]
-ImportConnectivity: typing.Type["bsb.connectivity.import_.ImportConnectivity"]
-ImportPlacement: typing.Type["bsb.placement.import_.ImportPlacement"]
-IncompleteExternalMapError: typing.Type["bsb.exceptions.IncompleteExternalMapError"]
-IncompleteMorphologyError: typing.Type["bsb.exceptions.IncompleteMorphologyError"]
-IndicatorError: typing.Type["bsb.exceptions.IndicatorError"]
-InputError: typing.Type["bsb.exceptions.InputError"]
-Interface: typing.Type["bsb.storage.interfaces.Interface"]
-IntersectionDataNotFoundError: typing.Type["bsb.exceptions.IntersectionDataNotFoundError"]
-Intersectional: typing.Type["bsb.connectivity.detailed.shared.Intersectional"]
-InvalidReferenceError: typing.Type["bsb.exceptions.InvalidReferenceError"]
-InvertedRoI: typing.Type["bsb.mixins.InvertedRoI"]
-JobCancelledError: typing.Type["bsb.exceptions.JobCancelledError"]
-JobPool: typing.Type["bsb.services.JobPool"]
-JobPoolContextError: typing.Type["bsb.exceptions.JobPoolContextError"]
-JobPoolError: typing.Type["bsb.exceptions.JobPoolError"]
-JobSchedulingError: typing.Type["bsb.exceptions.JobSchedulingError"]
-LabelTargetting: typing.Type["bsb.simulation.targetting.LabelTargetting"]
-Layer: typing.Type["bsb.topology.partition.Layer"]
-LayoutError: typing.Type["bsb.exceptions.LayoutError"]
-LocationTargetting: typing.Type["bsb.simulation.targetting.LocationTargetting"]
-MPI: typing.Type["bsb.services.MPI"]
-MPILock: typing.Type["bsb.services.MPILock"]
-Meter: typing.Type["bsb.profiling.Meter"]
-MissingActiveConfigError: typing.Type["bsb.exceptions.MissingActiveConfigError"]
-MissingMorphologyError: typing.Type["bsb.exceptions.MissingMorphologyError"]
-MissingSourceError: typing.Type["bsb.exceptions.MissingSourceError"]
-MorphIOParser: typing.Type["bsb.morphologies.parsers.parser.MorphIOParser"]
-Morphology: typing.Type["bsb.morphologies.Morphology"]
-MorphologyDataError: typing.Type["bsb.exceptions.MorphologyDataError"]
-MorphologyDependencyNode: typing.Type["bsb.storage._files.MorphologyDependencyNode"]
-MorphologyDistributor: typing.Type["bsb.placement.distributor.MorphologyDistributor"]
-MorphologyError: typing.Type["bsb.exceptions.MorphologyError"]
-MorphologyGenerator: typing.Type["bsb.placement.distributor.MorphologyGenerator"]
-MorphologyOperation: typing.Type["bsb.storage._files.MorphologyOperation"]
-MorphologyParser: typing.Type["bsb.morphologies.parsers.parser.MorphologyParser"]
-MorphologyRepository: typing.Type["bsb.storage.interfaces.MorphologyRepository"]
-MorphologyRepositoryError: typing.Type["bsb.exceptions.MorphologyRepositoryError"]
-MorphologySelector: typing.Type["bsb.morphologies.selector.MorphologySelector"]
-MorphologySet: typing.Type["bsb.morphologies.MorphologySet"]
-MorphologyWarning: typing.Type["bsb.exceptions.MorphologyWarning"]
-NameSelector: typing.Type["bsb.morphologies.selector.NameSelector"]
-NetworkDescription: typing.Type["bsb.storage.interfaces.NetworkDescription"]
-NeuroMorphoScheme: typing.Type["bsb.storage._files.NeuroMorphoScheme"]
-NeuroMorphoSelector: typing.Type["bsb.morphologies.selector.NeuroMorphoSelector"]
-NoReferenceAttributeSignal: typing.Type["bsb.exceptions.NoReferenceAttributeSignal"]
-NodeNotFoundError: typing.Type["bsb.exceptions.NodeNotFoundError"]
-NoneReferenceError: typing.Type["bsb.exceptions.NoneReferenceError"]
-NoopLock: typing.Type["bsb.storage.interfaces.NoopLock"]
-NotParallel: typing.Type["bsb.mixins.NotParallel"]
-NotSupported: typing.Type["bsb.storage.NotSupported"]
-NrrdDependencyNode: typing.Type["bsb.storage._files.NrrdDependencyNode"]
-NrrdVoxels: typing.Type["bsb.topology.partition.NrrdVoxels"]
-Operation: typing.Type["bsb.storage._files.Operation"]
-OptionDescriptor: typing.Type["bsb.option.OptionDescriptor"]
-OptionError: typing.Type["bsb.exceptions.OptionError"]
-PackageRequirement: typing.Type["bsb.config.types.PackageRequirement"]
-PackageRequirementWarning: typing.Type["bsb.exceptions.PackageRequirementWarning"]
-PackingError: typing.Type["bsb.exceptions.PackingError"]
-PackingWarning: typing.Type["bsb.exceptions.PackingWarning"]
-ParallelArrayPlacement: typing.Type["bsb.placement.arrays.ParallelArrayPlacement"]
-Parameter: typing.Type["bsb.simulation.parameter.Parameter"]
-ParameterError: typing.Type["bsb.exceptions.ParameterError"]
-ParameterValue: typing.Type["bsb.simulation.parameter.ParameterValue"]
-ParserError: typing.Type["bsb.exceptions.ParserError"]
-Partition: typing.Type["bsb.topology.partition.Partition"]
-PlacementError: typing.Type["bsb.exceptions.PlacementError"]
-PlacementIndications: typing.Type["bsb.cell_types.PlacementIndications"]
-PlacementIndicator: typing.Type["bsb.placement.indicator.PlacementIndicator"]
-PlacementRelationError: typing.Type["bsb.exceptions.PlacementRelationError"]
-PlacementSet: typing.Type["bsb.storage.interfaces.PlacementSet"]
-PlacementStrategy: typing.Type["bsb.placement.strategy.PlacementStrategy"]
-PlacementWarning: typing.Type["bsb.exceptions.PlacementWarning"]
-Plotting: typing.Type["bsb.cell_types.Plotting"]
-PluginError: typing.Type["bsb.exceptions.PluginError"]
-ProfilingSession: typing.Type["bsb.profiling.ProfilingSession"]
-ProgressEvent: typing.Type["bsb.simulation.simulation.ProgressEvent"]
-ProjectOptionDescriptor: typing.Type["bsb.option.ProjectOptionDescriptor"]
-RandomMorphologies: typing.Type["bsb.placement.distributor.RandomMorphologies"]
-RandomPlacement: typing.Type["bsb.placement.random.RandomPlacement"]
-RandomRotations: typing.Type["bsb.placement.distributor.RandomRotations"]
-ReadOnlyManager: typing.Type["bsb.storage.interfaces.ReadOnlyManager"]
-ReadOnlyOptionError: typing.Type["bsb.exceptions.ReadOnlyOptionError"]
-RedoError: typing.Type["bsb.exceptions.RedoError"]
-Reference: typing.Type["bsb.config.refs.Reference"]
-Region: typing.Type["bsb.topology.region.Region"]
-RegionGroup: typing.Type["bsb.topology.region.RegionGroup"]
-ReificationError: typing.Type["bsb.exceptions.ReificationError"]
-Relay: typing.Type["bsb.postprocessing.Relay"]
-ReportListener: typing.Type["bsb.core.ReportListener"]
-RepresentativesTargetting: typing.Type["bsb.simulation.targetting.RepresentativesTargetting"]
-RequirementError: typing.Type["bsb.exceptions.RequirementError"]
-Rhomboid: typing.Type["bsb.topology.partition.Rhomboid"]
-RootCommand: typing.Type["bsb.cli.commands.RootCommand"]
-RotationDistributor: typing.Type["bsb.placement.distributor.RotationDistributor"]
-RotationSet: typing.Type["bsb.morphologies.RotationSet"]
-RoundRobinMorphologies: typing.Type["bsb.placement.distributor.RoundRobinMorphologies"]
-Scaffold: typing.Type["bsb.core.Scaffold"]
-ScaffoldError: typing.Type["bsb.exceptions.ScaffoldError"]
-ScaffoldWarning: typing.Type["bsb.exceptions.ScaffoldWarning"]
-ScriptOptionDescriptor: typing.Type["bsb.option.ScriptOptionDescriptor"]
-SelectorError: typing.Type["bsb.exceptions.SelectorError"]
-Simulation: typing.Type["bsb.simulation.simulation.Simulation"]
-SimulationBackendPlugin: typing.Type["bsb.simulation.SimulationBackendPlugin"]
-SimulationComponent: typing.Type["bsb.simulation.component.SimulationComponent"]
-SimulationData: typing.Type["bsb.simulation.adapter.SimulationData"]
-SimulationError: typing.Type["bsb.exceptions.SimulationError"]
-SimulationRecorder: typing.Type["bsb.simulation.results.SimulationRecorder"]
-SimulationResult: typing.Type["bsb.simulation.results.SimulationResult"]
-SimulatorAdapter: typing.Type["bsb.simulation.adapter.SimulatorAdapter"]
-SomaTargetting: typing.Type["bsb.simulation.targetting.SomaTargetting"]
-SourceQualityError: typing.Type["bsb.exceptions.SourceQualityError"]
-SphericalTargetting: typing.Type["bsb.simulation.targetting.SphericalTargetting"]
-SpoofDetails: typing.Type["bsb.postprocessing.SpoofDetails"]
-Stack: typing.Type["bsb.topology.region.Stack"]
-Storage: typing.Type["bsb.storage.Storage"]
-StorageError: typing.Type["bsb.exceptions.StorageError"]
-StorageNode: typing.Type["bsb.storage.interfaces.StorageNode"]
-StoredFile: typing.Type["bsb.storage.interfaces.StoredFile"]
-StoredMorphology: typing.Type["bsb.storage.interfaces.StoredMorphology"]
-SubTree: typing.Type["bsb.morphologies.SubTree"]
-Targetting: typing.Type["bsb.simulation.targetting.Targetting"]
-TopologyError: typing.Type["bsb.exceptions.TopologyError"]
-TreeError: typing.Type["bsb.exceptions.TreeError"]
-TypeHandler: typing.Type["bsb.config.types.TypeHandler"]
-TypeHandlingError: typing.Type["bsb.exceptions.TypeHandlingError"]
-UnfitClassCastError: typing.Type["bsb.exceptions.UnfitClassCastError"]
-UnknownConfigAttrError: typing.Type["bsb.exceptions.UnknownConfigAttrError"]
-UnknownGIDError: typing.Type["bsb.exceptions.UnknownGIDError"]
-UnknownStorageEngineError: typing.Type["bsb.exceptions.UnknownStorageEngineError"]
-UnmanagedPartitionError: typing.Type["bsb.exceptions.UnmanagedPartitionError"]
-UnresolvedClassCastError: typing.Type["bsb.exceptions.UnresolvedClassCastError"]
-UriScheme: typing.Type["bsb.storage._files.UriScheme"]
-UrlScheme: typing.Type["bsb.storage._files.UrlScheme"]
-VolumetricRotations: typing.Type["bsb.placement.distributor.VolumetricRotations"]
-VoxelData: typing.Type["bsb.voxels.VoxelData"]
-VoxelIntersection: typing.Type["bsb.connectivity.detailed.voxel_intersection.VoxelIntersection"]
-VoxelSet: typing.Type["bsb.voxels.VoxelSet"]
-VoxelSetError: typing.Type["bsb.exceptions.VoxelSetError"]
-Voxels: typing.Type["bsb.topology.partition.Voxels"]
-WeakInverter: typing.Type["bsb.config.types.WeakInverter"]
-WorkflowError: typing.Type["bsb.services.WorkflowError"]
-activate_session: "bsb.profiling.activate_session"
-box_layout: "bsb.topology.box_layout"
-branch_iter: "bsb.morphologies.branch_iter"
-chunklist: "bsb.storage._chunks.chunklist"
-compose_nodes: "bsb.config.compose_nodes"
-copy_configuration_template: "bsb.config.copy_configuration_template"
-create_engine: "bsb.storage.create_engine"
-create_topology: "bsb.topology.create_topology"
-discover: "bsb.plugins.discover"
-discover_engines: "bsb.storage.discover_engines"
-format_configuration_content: "bsb.config.format_configuration_content"
-from_storage: "bsb.core.from_storage"
-get_active_session: "bsb.profiling.get_active_session"
-get_config_attributes: "bsb.config.get_config_attributes"
-get_config_path: "bsb.config.get_config_path"
-get_configuration_parser: "bsb.config.parsers.get_configuration_parser"
-get_configuration_parser_classes: "bsb.config.parsers.get_configuration_parser_classes"
-get_engine_node: "bsb.storage.get_engine_node"
-get_engines: "bsb.storage.get_engines"
-get_module_option: "bsb.options.get_module_option"
-get_option: "bsb.options.get_option"
-get_option_classes: "bsb.options.get_option_classes"
-get_option_descriptor: "bsb.options.get_option_descriptor"
-get_option_descriptors: "bsb.options.get_option_descriptors"
-get_partitions: "bsb.topology.get_partitions"
-get_project_option: "bsb.options.get_project_option"
-get_root_regions: "bsb.topology.get_root_regions"
-get_simulation_adapter: "bsb.simulation.get_simulation_adapter"
-handle_cli: "bsb.cli.handle_cli"
-handle_command: "bsb.cli.handle_command"
-is_module_option_set: "bsb.options.is_module_option_set"
-is_partition: "bsb.topology.is_partition"
-is_region: "bsb.topology.is_region"
-load_root_command: "bsb.cli.commands.load_root_command"
-make_config_diagram: "bsb.config.make_config_diagram"
-meter: "bsb.profiling.meter"
-node_meter: "bsb.profiling.node_meter"
-on_main: "bsb.storage.decorators.on_main"
-on_main_until: "bsb.storage.decorators.on_main_until"
-open_storage: "bsb.storage.open_storage"
-parse_configuration_content: "bsb.config.parse_configuration_content"
-parse_configuration_file: "bsb.config.parse_configuration_file"
-parse_morphology_content: "bsb.morphologies.parsers.parse_morphology_content"
-parse_morphology_file: "bsb.morphologies.parsers.parse_morphology_file"
-read_option: "bsb.options.read_option"
-refs: "bsb.config.refs"
-register_option: "bsb.options.register_option"
-register_service: "bsb.services.register_service"
-report: "bsb.reporting.report"
-reset_module_option: "bsb.options.reset_module_option"
-set_module_option: "bsb.options.set_module_option"
-store_option: "bsb.options.store_option"
-types: "bsb.config.types"
-unregister_option: "bsb.options.unregister_option"
-view_profile: "bsb.profiling.view_profile"
-view_support: "bsb.storage.view_support"
-walk_node_attributes: "bsb.config.walk_node_attributes"
-walk_nodes: "bsb.config.walk_nodes"
-warn: "bsb.reporting.warn"
+"""
+`bsb-core` is the backbone package contain the essential code of the BSB: A component
+framework for multiscale bottom-up neural modelling.
+
+`bsb-core` needs to be installed alongside a bundle of desired bsb plugins, some of
+which are essential for `bsb-core` to function. First time users are recommended to
+install the `bsb` package instead.
+"""
+
+__version__ = "4.1.0"
+
+import functools
+import importlib
+import sys
+import typing
+import warnings
+
+import bsb.exceptions as _exc
+
+# Patch functools on 3.8
+try:
+    functools.cache
+except AttributeError:
+    functools.cache = functools.lru_cache
+
+    # Patch the 'register' method of `singledispatchmethod` pre python 3.10
+    def _register(self, cls, method=None):  # pragma: nocover
+        if hasattr(cls, "__func__"):
+            setattr(cls, "__annotations__", cls.__func__.__annotations__)
+        return self.dispatcher.register(cls, func=method)
+
+    functools.singledispatchmethod.register = _register
+
+
+# Always show all scaffold warnings
+for e in _exc.__dict__.values():
+    if isinstance(e, type) and issubclass(e, Warning):
+        warnings.simplefilter("always", e)
+
+try:
+    from .options import profiling as _pr
+except Exception:
+    pass
+else:
+    if _pr:
+        from .profiling import activate_session
+
+        activate_session()
+
+
+def _get_annotation_submodule(name: str):
+    annotation = __annotations__.get(name, None)
+    if annotation:
+        type_ = typing.get_args(annotation)
+        if type_:
+            # typing.Type["bsb.submodule.name"]
+            annotation = type_[0].__forward_arg__
+        return annotation[4 : -len(name) - 1]
+
+
+@functools.cache
+def __getattr__(name):
+    if name == "config":
+        return object.__getattribute__(sys.modules[__name__], name)
+    module = _get_annotation_submodule(name)
+    if module is None:
+        return object.__getattribute__(sys.modules[__name__], name)
+    else:
+        return getattr(importlib.import_module("." + module, package="bsb"), name)
+
+
+@functools.cache
+def __dir__():
+    return [*__annotations__.keys()]
+
+
+# Do not modify: autogenerated public API type annotations of the `bsb` module
+# fmt: off
+# isort: off
+if typing.TYPE_CHECKING:
+  import bsb.cell_types
+  import bsb.cli
+  import bsb.cli.commands
+  import bsb.config
+  import bsb.config.parsers
+  import bsb.config.refs
+  import bsb.config.types
+  import bsb.connectivity.detailed.shared
+  import bsb.connectivity.detailed.voxel_intersection
+  import bsb.connectivity.general
+  import bsb.connectivity.import_
+  import bsb.connectivity.strategy
+  import bsb.core
+  import bsb.exceptions
+  import bsb.mixins
+  import bsb.morphologies
+  import bsb.morphologies.parsers
+  import bsb.morphologies.parsers.parser
+  import bsb.morphologies.selector
+  import bsb.option
+  import bsb.options
+  import bsb.placement.arrays
+  import bsb.placement.distributor
+  import bsb.placement.import_
+  import bsb.placement.indicator
+  import bsb.placement.random
+  import bsb.placement.strategy
+  import bsb.plugins
+  import bsb.postprocessing
+  import bsb.profiling
+  import bsb.reporting
+  import bsb.services
+  import bsb.simulation
+  import bsb.simulation.adapter
+  import bsb.simulation.cell
+  import bsb.simulation.component
+  import bsb.simulation.connection
+  import bsb.simulation.device
+  import bsb.simulation.parameter
+  import bsb.simulation.results
+  import bsb.simulation.simulation
+  import bsb.simulation.targetting
+  import bsb.storage
+  import bsb.storage._chunks
+  import bsb.storage._files
+  import bsb.storage.decorators
+  import bsb.storage.interfaces
+  import bsb.topology
+  import bsb.topology.partition
+  import bsb.topology.region
+  import bsb.trees
+  import bsb.voxels
+
+AdapterError: typing.Type["bsb.exceptions.AdapterError"]
+AdapterProgress: typing.Type["bsb.simulation.adapter.AdapterProgress"]
+AfterConnectivityHook: typing.Type["bsb.postprocessing.AfterConnectivityHook"]
+AfterPlacementHook: typing.Type["bsb.postprocessing.AfterPlacementHook"]
+AllToAll: typing.Type["bsb.connectivity.general.AllToAll"]
+AllenApiError: typing.Type["bsb.exceptions.AllenApiError"]
+AllenStructure: typing.Type["bsb.topology.partition.AllenStructure"]
+AttributeMissingError: typing.Type["bsb.exceptions.AttributeMissingError"]
+BaseCommand: typing.Type["bsb.cli.commands.BaseCommand"]
+BidirectionalContact: typing.Type["bsb.postprocessing.BidirectionalContact"]
+BootError: typing.Type["bsb.exceptions.BootError"]
+BoxTree: typing.Type["bsb.trees.BoxTree"]
+BoxTreeInterface: typing.Type["bsb.trees.BoxTreeInterface"]
+Branch: typing.Type["bsb.morphologies.Branch"]
+BranchLocTargetting: typing.Type["bsb.simulation.targetting.BranchLocTargetting"]
+BsbCommand: typing.Type["bsb.cli.commands.BsbCommand"]
+BsbOption: typing.Type["bsb.option.BsbOption"]
+BsbParser: typing.Type["bsb.morphologies.parsers.parser.BsbParser"]
+ByIdTargetting: typing.Type["bsb.simulation.targetting.ByIdTargetting"]
+ByLabelTargetting: typing.Type["bsb.simulation.targetting.ByLabelTargetting"]
+CLIError: typing.Type["bsb.exceptions.CLIError"]
+CLIOptionDescriptor: typing.Type["bsb.option.CLIOptionDescriptor"]
+CastConfigurationError: typing.Type["bsb.exceptions.CastConfigurationError"]
+CastError: typing.Type["bsb.exceptions.CastError"]
+CellModel: typing.Type["bsb.simulation.cell.CellModel"]
+CellModelFilter: typing.Type["bsb.simulation.targetting.CellModelFilter"]
+CellModelTargetting: typing.Type["bsb.simulation.targetting.CellModelTargetting"]
+CellTargetting: typing.Type["bsb.simulation.targetting.CellTargetting"]
+CellType: typing.Type["bsb.cell_types.CellType"]
+CfgReferenceError: typing.Type["bsb.exceptions.CfgReferenceError"]
+Chunk: typing.Type["bsb.storage._chunks.Chunk"]
+ChunkError: typing.Type["bsb.exceptions.ChunkError"]
+CircularMorphologyError: typing.Type["bsb.exceptions.CircularMorphologyError"]
+ClassError: typing.Type["bsb.exceptions.ClassError"]
+ClassMapMissingError: typing.Type["bsb.exceptions.ClassMapMissingError"]
+CodeDependencyNode: typing.Type["bsb.storage._files.CodeDependencyNode"]
+CodeImportError: typing.Type["bsb.exceptions.CodeImportError"]
+CommandError: typing.Type["bsb.exceptions.CommandError"]
+CompartmentError: typing.Type["bsb.exceptions.CompartmentError"]
+CompilationError: typing.Type["bsb.exceptions.CompilationError"]
+ConfigTemplateNotFoundError: typing.Type["bsb.exceptions.ConfigTemplateNotFoundError"]
+Configuration: typing.Type["bsb.config.Configuration"]
+ConfigurationAttribute: typing.Type["bsb.config.ConfigurationAttribute"]
+ConfigurationError: typing.Type["bsb.exceptions.ConfigurationError"]
+ConfigurationFormatError: typing.Type["bsb.exceptions.ConfigurationFormatError"]
+ConfigurationParser: typing.Type["bsb.config.parsers.ConfigurationParser"]
+ConfigurationWarning: typing.Type["bsb.exceptions.ConfigurationWarning"]
+ConnectionModel: typing.Type["bsb.simulation.connection.ConnectionModel"]
+ConnectionStrategy: typing.Type["bsb.connectivity.strategy.ConnectionStrategy"]
+ConnectionTargetting: typing.Type["bsb.simulation.targetting.ConnectionTargetting"]
+ConnectivityError: typing.Type["bsb.exceptions.ConnectivityError"]
+ConnectivityIterator: typing.Type["bsb.storage.interfaces.ConnectivityIterator"]
+ConnectivitySet: typing.Type["bsb.storage.interfaces.ConnectivitySet"]
+ConnectivityWarning: typing.Type["bsb.exceptions.ConnectivityWarning"]
+ContinuityError: typing.Type["bsb.exceptions.ContinuityError"]
+Convergence: typing.Type["bsb.connectivity.general.Convergence"]
+CsvImportConnectivity: typing.Type["bsb.connectivity.import_.CsvImportConnectivity"]
+CsvImportPlacement: typing.Type["bsb.placement.import_.CsvImportPlacement"]
+CylindricalTargetting: typing.Type["bsb.simulation.targetting.CylindricalTargetting"]
+DataNotFoundError: typing.Type["bsb.exceptions.DataNotFoundError"]
+DataNotProvidedError: typing.Type["bsb.exceptions.DataNotProvidedError"]
+DatasetExistsError: typing.Type["bsb.exceptions.DatasetExistsError"]
+DatasetNotFoundError: typing.Type["bsb.exceptions.DatasetNotFoundError"]
+DependencyError: typing.Type["bsb.exceptions.DependencyError"]
+DeviceModel: typing.Type["bsb.simulation.device.DeviceModel"]
+Distribution: typing.Type["bsb.config.Distribution"]
+DistributionCastError: typing.Type["bsb.exceptions.DistributionCastError"]
+DistributionContext: typing.Type["bsb.placement.distributor.DistributionContext"]
+Distributor: typing.Type["bsb.placement.distributor.Distributor"]
+DistributorError: typing.Type["bsb.exceptions.DistributorError"]
+DistributorsNode: typing.Type["bsb.placement.distributor.DistributorsNode"]
+DryrunError: typing.Type["bsb.exceptions.DryrunError"]
+DynamicClassError: typing.Type["bsb.exceptions.DynamicClassError"]
+DynamicClassInheritanceError: typing.Type["bsb.exceptions.DynamicClassInheritanceError"]
+DynamicObjectNotFoundError: typing.Type["bsb.exceptions.DynamicObjectNotFoundError"]
+EmptyBranchError: typing.Type["bsb.exceptions.EmptyBranchError"]
+EmptySelectionError: typing.Type["bsb.exceptions.EmptySelectionError"]
+EmptyVoxelSetError: typing.Type["bsb.exceptions.EmptyVoxelSetError"]
+Engine: typing.Type["bsb.storage.interfaces.Engine"]
+Entities: typing.Type["bsb.placement.strategy.Entities"]
+EnvOptionDescriptor: typing.Type["bsb.option.EnvOptionDescriptor"]
+ExplicitNoRotations: typing.Type["bsb.placement.distributor.ExplicitNoRotations"]
+ExternalSourceError: typing.Type["bsb.exceptions.ExternalSourceError"]
+FileDependency: typing.Type["bsb.storage._files.FileDependency"]
+FileDependencyNode: typing.Type["bsb.storage._files.FileDependencyNode"]
+FileImportError: typing.Type["bsb.exceptions.FileImportError"]
+FileReferenceError: typing.Type["bsb.exceptions.FileReferenceError"]
+FileScheme: typing.Type["bsb.storage._files.FileScheme"]
+FileStore: typing.Type["bsb.storage.interfaces.FileStore"]
+FixedIndegree: typing.Type["bsb.connectivity.general.FixedIndegree"]
+FixedPositions: typing.Type["bsb.placement.strategy.FixedPositions"]
+FractionFilter: typing.Type["bsb.simulation.targetting.FractionFilter"]
+GatewayError: typing.Type["bsb.exceptions.GatewayError"]
+GeneratedMorphology: typing.Type["bsb.storage.interfaces.GeneratedMorphology"]
+HasDependencies: typing.Type["bsb.mixins.HasDependencies"]
+Hemitype: typing.Type["bsb.connectivity.strategy.Hemitype"]
+HemitypeCollection: typing.Type["bsb.connectivity.strategy.HemitypeCollection"]
+Implicit: typing.Type["bsb.placement.distributor.Implicit"]
+ImplicitNoRotations: typing.Type["bsb.placement.distributor.ImplicitNoRotations"]
+ImportConnectivity: typing.Type["bsb.connectivity.import_.ImportConnectivity"]
+ImportPlacement: typing.Type["bsb.placement.import_.ImportPlacement"]
+IncompleteExternalMapError: typing.Type["bsb.exceptions.IncompleteExternalMapError"]
+IncompleteMorphologyError: typing.Type["bsb.exceptions.IncompleteMorphologyError"]
+IndicatorError: typing.Type["bsb.exceptions.IndicatorError"]
+InputError: typing.Type["bsb.exceptions.InputError"]
+Interface: typing.Type["bsb.storage.interfaces.Interface"]
+IntersectionDataNotFoundError: typing.Type["bsb.exceptions.IntersectionDataNotFoundError"]
+Intersectional: typing.Type["bsb.connectivity.detailed.shared.Intersectional"]
+InvalidReferenceError: typing.Type["bsb.exceptions.InvalidReferenceError"]
+InvertedRoI: typing.Type["bsb.mixins.InvertedRoI"]
+JobCancelledError: typing.Type["bsb.exceptions.JobCancelledError"]
+JobPool: typing.Type["bsb.services.JobPool"]
+JobPoolContextError: typing.Type["bsb.exceptions.JobPoolContextError"]
+JobPoolError: typing.Type["bsb.exceptions.JobPoolError"]
+JobSchedulingError: typing.Type["bsb.exceptions.JobSchedulingError"]
+LabelTargetting: typing.Type["bsb.simulation.targetting.LabelTargetting"]
+Layer: typing.Type["bsb.topology.partition.Layer"]
+LayoutError: typing.Type["bsb.exceptions.LayoutError"]
+LocationTargetting: typing.Type["bsb.simulation.targetting.LocationTargetting"]
+MPI: typing.Type["bsb.services.MPI"]
+MPILock: typing.Type["bsb.services.MPILock"]
+Meter: typing.Type["bsb.profiling.Meter"]
+MissingActiveConfigError: typing.Type["bsb.exceptions.MissingActiveConfigError"]
+MissingMorphologyError: typing.Type["bsb.exceptions.MissingMorphologyError"]
+MissingSourceError: typing.Type["bsb.exceptions.MissingSourceError"]
+MorphIOParser: typing.Type["bsb.morphologies.parsers.parser.MorphIOParser"]
+Morphology: typing.Type["bsb.morphologies.Morphology"]
+MorphologyDataError: typing.Type["bsb.exceptions.MorphologyDataError"]
+MorphologyDependencyNode: typing.Type["bsb.storage._files.MorphologyDependencyNode"]
+MorphologyDistributor: typing.Type["bsb.placement.distributor.MorphologyDistributor"]
+MorphologyError: typing.Type["bsb.exceptions.MorphologyError"]
+MorphologyGenerator: typing.Type["bsb.placement.distributor.MorphologyGenerator"]
+MorphologyOperation: typing.Type["bsb.storage._files.MorphologyOperation"]
+MorphologyParser: typing.Type["bsb.morphologies.parsers.parser.MorphologyParser"]
+MorphologyRepository: typing.Type["bsb.storage.interfaces.MorphologyRepository"]
+MorphologyRepositoryError: typing.Type["bsb.exceptions.MorphologyRepositoryError"]
+MorphologySelector: typing.Type["bsb.morphologies.selector.MorphologySelector"]
+MorphologySet: typing.Type["bsb.morphologies.MorphologySet"]
+MorphologyWarning: typing.Type["bsb.exceptions.MorphologyWarning"]
+NameSelector: typing.Type["bsb.morphologies.selector.NameSelector"]
+NetworkDescription: typing.Type["bsb.storage.interfaces.NetworkDescription"]
+NeuroMorphoScheme: typing.Type["bsb.storage._files.NeuroMorphoScheme"]
+NeuroMorphoSelector: typing.Type["bsb.morphologies.selector.NeuroMorphoSelector"]
+NoReferenceAttributeSignal: typing.Type["bsb.exceptions.NoReferenceAttributeSignal"]
+NodeNotFoundError: typing.Type["bsb.exceptions.NodeNotFoundError"]
+NoneReferenceError: typing.Type["bsb.exceptions.NoneReferenceError"]
+NoopLock: typing.Type["bsb.storage.interfaces.NoopLock"]
+NotParallel: typing.Type["bsb.mixins.NotParallel"]
+NotSupported: typing.Type["bsb.storage.NotSupported"]
+NrrdDependencyNode: typing.Type["bsb.storage._files.NrrdDependencyNode"]
+NrrdVoxels: typing.Type["bsb.topology.partition.NrrdVoxels"]
+Operation: typing.Type["bsb.storage._files.Operation"]
+OptionDescriptor: typing.Type["bsb.option.OptionDescriptor"]
+OptionError: typing.Type["bsb.exceptions.OptionError"]
+PackageRequirement: typing.Type["bsb.config.types.PackageRequirement"]
+PackageRequirementWarning: typing.Type["bsb.exceptions.PackageRequirementWarning"]
+PackingError: typing.Type["bsb.exceptions.PackingError"]
+PackingWarning: typing.Type["bsb.exceptions.PackingWarning"]
+ParallelArrayPlacement: typing.Type["bsb.placement.arrays.ParallelArrayPlacement"]
+Parameter: typing.Type["bsb.simulation.parameter.Parameter"]
+ParameterError: typing.Type["bsb.exceptions.ParameterError"]
+ParameterValue: typing.Type["bsb.simulation.parameter.ParameterValue"]
+ParserError: typing.Type["bsb.exceptions.ParserError"]
+ParsesReferences: typing.Type["bsb.config.parsers.ParsesReferences"]
+Partition: typing.Type["bsb.topology.partition.Partition"]
+PlacementError: typing.Type["bsb.exceptions.PlacementError"]
+PlacementIndications: typing.Type["bsb.cell_types.PlacementIndications"]
+PlacementIndicator: typing.Type["bsb.placement.indicator.PlacementIndicator"]
+PlacementRelationError: typing.Type["bsb.exceptions.PlacementRelationError"]
+PlacementSet: typing.Type["bsb.storage.interfaces.PlacementSet"]
+PlacementStrategy: typing.Type["bsb.placement.strategy.PlacementStrategy"]
+PlacementWarning: typing.Type["bsb.exceptions.PlacementWarning"]
+Plotting: typing.Type["bsb.cell_types.Plotting"]
+PluginError: typing.Type["bsb.exceptions.PluginError"]
+ProfilingSession: typing.Type["bsb.profiling.ProfilingSession"]
+ProgressEvent: typing.Type["bsb.simulation.simulation.ProgressEvent"]
+ProjectOptionDescriptor: typing.Type["bsb.option.ProjectOptionDescriptor"]
+RandomMorphologies: typing.Type["bsb.placement.distributor.RandomMorphologies"]
+RandomPlacement: typing.Type["bsb.placement.random.RandomPlacement"]
+RandomRotations: typing.Type["bsb.placement.distributor.RandomRotations"]
+ReadOnlyManager: typing.Type["bsb.storage.interfaces.ReadOnlyManager"]
+ReadOnlyOptionError: typing.Type["bsb.exceptions.ReadOnlyOptionError"]
+RedoError: typing.Type["bsb.exceptions.RedoError"]
+Reference: typing.Type["bsb.config.refs.Reference"]
+Region: typing.Type["bsb.topology.region.Region"]
+RegionGroup: typing.Type["bsb.topology.region.RegionGroup"]
+ReificationError: typing.Type["bsb.exceptions.ReificationError"]
+Relay: typing.Type["bsb.postprocessing.Relay"]
+ReportListener: typing.Type["bsb.core.ReportListener"]
+RepresentativesTargetting: typing.Type["bsb.simulation.targetting.RepresentativesTargetting"]
+RequirementError: typing.Type["bsb.exceptions.RequirementError"]
+Rhomboid: typing.Type["bsb.topology.partition.Rhomboid"]
+RootCommand: typing.Type["bsb.cli.commands.RootCommand"]
+RotationDistributor: typing.Type["bsb.placement.distributor.RotationDistributor"]
+RotationSet: typing.Type["bsb.morphologies.RotationSet"]
+RoundRobinMorphologies: typing.Type["bsb.placement.distributor.RoundRobinMorphologies"]
+Scaffold: typing.Type["bsb.core.Scaffold"]
+ScaffoldError: typing.Type["bsb.exceptions.ScaffoldError"]
+ScaffoldWarning: typing.Type["bsb.exceptions.ScaffoldWarning"]
+ScriptOptionDescriptor: typing.Type["bsb.option.ScriptOptionDescriptor"]
+SelectorError: typing.Type["bsb.exceptions.SelectorError"]
+Simulation: typing.Type["bsb.simulation.simulation.Simulation"]
+SimulationBackendPlugin: typing.Type["bsb.simulation.SimulationBackendPlugin"]
+SimulationComponent: typing.Type["bsb.simulation.component.SimulationComponent"]
+SimulationData: typing.Type["bsb.simulation.adapter.SimulationData"]
+SimulationError: typing.Type["bsb.exceptions.SimulationError"]
+SimulationRecorder: typing.Type["bsb.simulation.results.SimulationRecorder"]
+SimulationResult: typing.Type["bsb.simulation.results.SimulationResult"]
+SimulatorAdapter: typing.Type["bsb.simulation.adapter.SimulatorAdapter"]
+SomaTargetting: typing.Type["bsb.simulation.targetting.SomaTargetting"]
+SourceQualityError: typing.Type["bsb.exceptions.SourceQualityError"]
+SphericalTargetting: typing.Type["bsb.simulation.targetting.SphericalTargetting"]
+SpoofDetails: typing.Type["bsb.postprocessing.SpoofDetails"]
+Stack: typing.Type["bsb.topology.region.Stack"]
+Storage: typing.Type["bsb.storage.Storage"]
+StorageError: typing.Type["bsb.exceptions.StorageError"]
+StorageNode: typing.Type["bsb.storage.interfaces.StorageNode"]
+StoredFile: typing.Type["bsb.storage.interfaces.StoredFile"]
+StoredMorphology: typing.Type["bsb.storage.interfaces.StoredMorphology"]
+SubTree: typing.Type["bsb.morphologies.SubTree"]
+Targetting: typing.Type["bsb.simulation.targetting.Targetting"]
+TopologyError: typing.Type["bsb.exceptions.TopologyError"]
+TreeError: typing.Type["bsb.exceptions.TreeError"]
+TypeHandler: typing.Type["bsb.config.types.TypeHandler"]
+TypeHandlingError: typing.Type["bsb.exceptions.TypeHandlingError"]
+UnfitClassCastError: typing.Type["bsb.exceptions.UnfitClassCastError"]
+UnknownConfigAttrError: typing.Type["bsb.exceptions.UnknownConfigAttrError"]
+UnknownGIDError: typing.Type["bsb.exceptions.UnknownGIDError"]
+UnknownStorageEngineError: typing.Type["bsb.exceptions.UnknownStorageEngineError"]
+UnmanagedPartitionError: typing.Type["bsb.exceptions.UnmanagedPartitionError"]
+UnresolvedClassCastError: typing.Type["bsb.exceptions.UnresolvedClassCastError"]
+UriScheme: typing.Type["bsb.storage._files.UriScheme"]
+UrlScheme: typing.Type["bsb.storage._files.UrlScheme"]
+VolumetricRotations: typing.Type["bsb.placement.distributor.VolumetricRotations"]
+VoxelData: typing.Type["bsb.voxels.VoxelData"]
+VoxelIntersection: typing.Type["bsb.connectivity.detailed.voxel_intersection.VoxelIntersection"]
+VoxelSet: typing.Type["bsb.voxels.VoxelSet"]
+VoxelSetError: typing.Type["bsb.exceptions.VoxelSetError"]
+Voxels: typing.Type["bsb.topology.partition.Voxels"]
+WeakInverter: typing.Type["bsb.config.types.WeakInverter"]
+WorkflowError: typing.Type["bsb.services.WorkflowError"]
+activate_session: "bsb.profiling.activate_session"
+box_layout: "bsb.topology.box_layout"
+branch_iter: "bsb.morphologies.branch_iter"
+chunklist: "bsb.storage._chunks.chunklist"
+compose_nodes: "bsb.config.compose_nodes"
+copy_configuration_template: "bsb.config.copy_configuration_template"
+create_engine: "bsb.storage.create_engine"
+create_topology: "bsb.topology.create_topology"
+discover: "bsb.plugins.discover"
+discover_engines: "bsb.storage.discover_engines"
+format_configuration_content: "bsb.config.format_configuration_content"
+from_storage: "bsb.core.from_storage"
+get_active_session: "bsb.profiling.get_active_session"
+get_config_attributes: "bsb.config.get_config_attributes"
+get_config_path: "bsb.config.get_config_path"
+get_configuration_parser: "bsb.config.parsers.get_configuration_parser"
+get_configuration_parser_classes: "bsb.config.parsers.get_configuration_parser_classes"
+get_engine_node: "bsb.storage.get_engine_node"
+get_engines: "bsb.storage.get_engines"
+get_module_option: "bsb.options.get_module_option"
+get_option: "bsb.options.get_option"
+get_option_classes: "bsb.options.get_option_classes"
+get_option_descriptor: "bsb.options.get_option_descriptor"
+get_option_descriptors: "bsb.options.get_option_descriptors"
+get_partitions: "bsb.topology.get_partitions"
+get_project_option: "bsb.options.get_project_option"
+get_root_regions: "bsb.topology.get_root_regions"
+get_simulation_adapter: "bsb.simulation.get_simulation_adapter"
+handle_cli: "bsb.cli.handle_cli"
+handle_command: "bsb.cli.handle_command"
+is_module_option_set: "bsb.options.is_module_option_set"
+is_partition: "bsb.topology.is_partition"
+is_region: "bsb.topology.is_region"
+load_root_command: "bsb.cli.commands.load_root_command"
+make_config_diagram: "bsb.config.make_config_diagram"
+meter: "bsb.profiling.meter"
+node_meter: "bsb.profiling.node_meter"
+on_main: "bsb.storage.decorators.on_main"
+on_main_until: "bsb.storage.decorators.on_main_until"
+open_storage: "bsb.storage.open_storage"
+parse_configuration_content: "bsb.config.parse_configuration_content"
+parse_configuration_file: "bsb.config.parse_configuration_file"
+parse_morphology_content: "bsb.morphologies.parsers.parse_morphology_content"
+parse_morphology_file: "bsb.morphologies.parsers.parse_morphology_file"
+read_option: "bsb.options.read_option"
+refs: "bsb.config.refs"
+register_option: "bsb.options.register_option"
+register_service: "bsb.services.register_service"
+report: "bsb.reporting.report"
+reset_module_option: "bsb.options.reset_module_option"
+set_module_option: "bsb.options.set_module_option"
+store_option: "bsb.options.store_option"
+types: "bsb.config.types"
+unregister_option: "bsb.options.unregister_option"
+view_profile: "bsb.profiling.view_profile"
+view_support: "bsb.storage.view_support"
+walk_node_attributes: "bsb.config.walk_node_attributes"
+walk_nodes: "bsb.config.walk_nodes"
+warn: "bsb.reporting.warn"
```

### Comparing `bsb_core-4.0.1/bsb/_contexts.py` & `bsb_core-4.1.0/bsb/_contexts.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-class Context:
-    def __init__(self, options):
-        self.options = options
-
-    def __str__(self):
-        opt = "; ".join(f"{opt.name}: {opt.get()}" for opt in self.options.values())
-        return f"<Context options({opt})>"
-
-
-class CLIContext(Context):
-    def set_cli_namespace(self, namespace):
-        for option in self.options.values():
-            for tag in _tags_to_namespace(option.__class__.cli.tags):
-                if hasattr(namespace, tag):
-                    option.cli = getattr(namespace, tag)
-        self.arguments = namespace
-
-    def __getattr__(self, attr):
-        for option in self.options.values():
-            if option.name == attr:
-                return option.get()
-        return super().__getattribute__(attr)
-
-    def __str__(self):
-        base = super().__str__()
-        if not hasattr(self, "arguments"):
-            return base[:-1] + " without CLI arguments>"
-        return base[:-1] + f" {self.arguments}>"
-
-
-def reset_cli_context():
-    from .options import get_option_descriptors
-
-    for opt in get_option_descriptors().values():
-        del opt.cli
-
-
-def get_cli_context():
-    from .options import get_option_descriptors
-
-    return CLIContext(get_option_descriptors())
-
-
-def _tags_to_namespace(tags):
-    yield from (tag.replace("-", "_") for tag in tags)
+class Context:
+    def __init__(self, options):
+        self.options = options
+
+    def __str__(self):
+        opt = "; ".join(f"{opt.name}: {opt.get()}" for opt in self.options.values())
+        return f"<Context options({opt})>"
+
+
+class CLIContext(Context):
+    def set_cli_namespace(self, namespace):
+        for option in self.options.values():
+            for tag in _tags_to_namespace(option.__class__.cli.tags):
+                if hasattr(namespace, tag):
+                    option.cli = getattr(namespace, tag)
+        self.arguments = namespace
+
+    def __getattr__(self, attr):
+        for option in self.options.values():
+            if option.name == attr:
+                return option.get()
+        return super().__getattribute__(attr)
+
+    def __str__(self):
+        base = super().__str__()
+        if not hasattr(self, "arguments"):
+            return base[:-1] + " without CLI arguments>"
+        return base[:-1] + f" {self.arguments}>"
+
+
+def reset_cli_context():
+    from .options import get_option_descriptors
+
+    for opt in get_option_descriptors().values():
+        del opt.cli
+
+
+def get_cli_context():
+    from .options import get_option_descriptors
+
+    return CLIContext(get_option_descriptors())
+
+
+def _tags_to_namespace(tags):
+    yield from (tag.replace("-", "_") for tag in tags)
```

### Comparing `bsb_core-4.0.1/bsb/_encoding.py` & `bsb_core-4.1.0/bsb/_encoding.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,196 +1,196 @@
-import itertools
-
-import numpy as np
-
-from ._util import obj_str_insert
-
-
-class _lset(set):
-    def __hash__(self):
-        return int.from_bytes(":|\\!#".join(sorted(self)).encode(), "little")
-
-    def __eq__(self, other):
-        return hash(self) == hash(_lset(other))
-
-    def copy(self):
-        return self.__class__(self)
-
-
-class EncodedLabels(np.ndarray):
-    def __new__(subtype, *args, labels=None, **kwargs):
-        kwargs["dtype"] = int
-        array = super().__new__(subtype, *args, **kwargs)
-        if labels is None:
-            labels = {0: _lset()}
-        array.labels = {int(k): _lset(v) for k, v in labels.items()}
-        return array
-
-    @obj_str_insert
-    def __repr__(self):
-        labellist = ", ".join(
-            (
-                f"{sum(self == k)} labelled {list(ls)}"
-                if len(ls)
-                else f"{sum(self == k)} unlabelled"
-            )
-            for k, ls in self.labels.items()
-        )
-        return f"with {len(self)} elements, of which {labellist}"
-
-    __str__ = __repr__
-
-    def __array_finalize__(self, array):
-        if array is not None:
-            self.labels = getattr(array, "labels", {0: _lset()})
-
-    def __eq__(self, other):
-        try:
-            return np.allclose(*EncodedLabels._merged_translate((self, other)))
-        except Exception:
-            return np.array(self, copy=False) == other
-
-    @property
-    def raw(self):
-        return np.array(self, copy=False)
-
-    def copy(self, *args, **kwargs):
-        cp = super().copy(*args, **kwargs)
-        cp.labels = {k: v.copy() for k, v in cp.labels.items()}
-        return cp
-
-    def label(self, labels, points):
-        if not len(points):
-            return
-        _transitions = {}
-        # A counter that skips existing values.
-        counter = (c for c in itertools.count() if c not in self.labels)
-
-        # This local function looks up the new id that a point should transition
-        # to when `labels` are added to the labels it already has.
-        def transition(point):
-            nonlocal _transitions
-            # Check if we already know the transition of this value.
-            if point in _transitions:
-                return _transitions[point]
-            else:
-                # First time making this transition. Join the existing and new labels
-                trans_labels = self.labels[point].copy()
-                trans_labels.update(labels)
-                # Check if this new combination of labels already is assigned an id.
-                for k, v in self.labels.items():
-                    if trans_labels == v:
-                        # Transition labels already exist, store and return it
-                        _transitions[point] = k
-                        return k
-                else:
-                    # Transition labels are a new combination, store them under a new id.
-                    transition = next(counter)
-                    self.labels[transition] = trans_labels
-                    # Cache the result
-                    _transitions[point] = transition
-                    return transition
-
-        # Replace the label values with the transition values
-        self[points] = np.vectorize(transition)(self[points])
-
-    def contains(self, labels):
-        return np.any(self.get_mask(labels))
-
-    def index_of(self, labels):
-        for i, lset in self.labels.items():
-            if lset == labels:
-                return i
-        else:
-            raise IndexError(f"Labelset {labels} does not exist")
-
-    def get_mask(self, labels):
-        has_any = [k for k, v in self.labels.items() if any(lbl in v for lbl in labels)]
-        return np.isin(self, has_any)
-
-    def walk(self):
-        """
-        Iterate over the branch, yielding the labels of each point.
-        """
-        for x in self:
-            yield self.labels[x].copy()
-
-    def expand(self, label):
-        """
-        Translate a label value into its corresponding labelset.
-        """
-        return self.labels[label].copy()
-
-    @classmethod
-    def none(cls, len):
-        """
-        Create EncodedLabels without any labelsets.
-        """
-        return cls(len, buffer=np.zeros(len, dtype=int))
-
-    @classmethod
-    def from_labelset(cls, len, labelset):
-        """
-        Create EncodedLabels with all points labelled to the given labelset.
-        """
-        return cls(len, buffer=np.ones(len), labels={0: _lset(), 1: _lset(labelset)})
-
-    @classmethod
-    def concatenate(cls, *label_arrs):
-        if not label_arrs:
-            return EncodedLabels.none(0)
-        lookups = EncodedLabels._get_merged_lookups(label_arrs)
-        total = sum(len(len_) for len_ in label_arrs)
-        concat = cls(total, labels=lookups[0])
-        ptr = 0
-        for block in EncodedLabels._merged_translate(label_arrs, lookups):
-            nptr = ptr + len(block)
-            # Concatenate the translated block
-            concat[ptr:nptr] = block
-            ptr = nptr
-        return concat
-
-    @staticmethod
-    def _get_merged_lookups(arrs):
-        if not arrs:
-            return {0: _lset()}
-        merged = {}
-        new_labelsets = set()
-        to_map_arrs = {}
-        for arr in arrs:
-            for k, l in arr.labels.items():
-                if k not in merged:
-                    # The label spot is available, so take it
-                    merged[k] = l
-                elif merged[k] != l:
-                    # The labelset doesn't match, so this array will have to be mapped,
-                    # and a new spot found for the conflicting labelset.
-                    new_labelsets.add(l)
-                    # np ndarray unhashable, for good reason, so use `id()` for quick hash
-                    to_map_arrs[id(arr)] = arr
-                # else: this labelset matches with the superset's nothing to do
-
-        # Collect new spots for new labelsets
-        counter = (c for c in itertools.count() if c not in merged)
-        lset_map = {}
-        for labelset in new_labelsets:
-            key = next(counter)
-            merged[key] = labelset
-            lset_map[labelset] = key
-
-        return merged, to_map_arrs, lset_map
-
-    def _merged_translate(arrs, lookups=None):
-        if lookups is None:
-            merged, to_map_arrs, lset_map = EncodedLabels._get_merged_lookups(arrs)
-        else:
-            merged, to_map_arrs, lset_map = lookups
-        for arr in arrs:
-            if id(arr) not in to_map_arrs:
-                # None of the label array's labelsets need to be mapped, good as is.
-                block = arr
-            else:
-                # Lookup each labelset, if found, map to new value, otherwise, map to
-                # original value.
-                arrmap = {og: lset_map.get(lset, og) for og, lset in arr.labels.items()}
-                block = np.vectorize(arrmap.get)(arr)
-            yield block
+import itertools
+
+import numpy as np
+
+from ._util import obj_str_insert
+
+
+class _lset(set):
+    def __hash__(self):
+        return int.from_bytes(":|\\!#".join(sorted(self)).encode(), "little")
+
+    def __eq__(self, other):
+        return hash(self) == hash(_lset(other))
+
+    def copy(self):
+        return self.__class__(self)
+
+
+class EncodedLabels(np.ndarray):
+    def __new__(subtype, *args, labels=None, **kwargs):
+        kwargs["dtype"] = int
+        array = super().__new__(subtype, *args, **kwargs)
+        if labels is None:
+            labels = {0: _lset()}
+        array.labels = {int(k): _lset(v) for k, v in labels.items()}
+        return array
+
+    @obj_str_insert
+    def __repr__(self):
+        labellist = ", ".join(
+            (
+                f"{sum(self == k)} labelled {list(ls)}"
+                if len(ls)
+                else f"{sum(self == k)} unlabelled"
+            )
+            for k, ls in self.labels.items()
+        )
+        return f"with {len(self)} elements, of which {labellist}"
+
+    __str__ = __repr__
+
+    def __array_finalize__(self, array):
+        if array is not None:
+            self.labels = getattr(array, "labels", {0: _lset()})
+
+    def __eq__(self, other):
+        try:
+            return np.allclose(*EncodedLabels._merged_translate((self, other)))
+        except Exception:
+            return np.array(self, copy=False) == other
+
+    @property
+    def raw(self):
+        return np.array(self, copy=False)
+
+    def copy(self, *args, **kwargs):
+        cp = super().copy(*args, **kwargs)
+        cp.labels = {k: v.copy() for k, v in cp.labels.items()}
+        return cp
+
+    def label(self, labels, points):
+        if not len(points):
+            return
+        _transitions = {}
+        # A counter that skips existing values.
+        counter = (c for c in itertools.count() if c not in self.labels)
+
+        # This local function looks up the new id that a point should transition
+        # to when `labels` are added to the labels it already has.
+        def transition(point):
+            nonlocal _transitions
+            # Check if we already know the transition of this value.
+            if point in _transitions:
+                return _transitions[point]
+            else:
+                # First time making this transition. Join the existing and new labels
+                trans_labels = self.labels[point].copy()
+                trans_labels.update(labels)
+                # Check if this new combination of labels already is assigned an id.
+                for k, v in self.labels.items():
+                    if trans_labels == v:
+                        # Transition labels already exist, store and return it
+                        _transitions[point] = k
+                        return k
+                else:
+                    # Transition labels are a new combination, store them under a new id.
+                    transition = next(counter)
+                    self.labels[transition] = trans_labels
+                    # Cache the result
+                    _transitions[point] = transition
+                    return transition
+
+        # Replace the label values with the transition values
+        self[points] = np.vectorize(transition)(self[points])
+
+    def contains(self, labels):
+        return np.any(self.get_mask(labels))
+
+    def index_of(self, labels):
+        for i, lset in self.labels.items():
+            if lset == labels:
+                return i
+        else:
+            raise IndexError(f"Labelset {labels} does not exist")
+
+    def get_mask(self, labels):
+        has_any = [k for k, v in self.labels.items() if any(lbl in v for lbl in labels)]
+        return np.isin(self, has_any)
+
+    def walk(self):
+        """
+        Iterate over the branch, yielding the labels of each point.
+        """
+        for x in self:
+            yield self.labels[x].copy()
+
+    def expand(self, label):
+        """
+        Translate a label value into its corresponding labelset.
+        """
+        return self.labels[label].copy()
+
+    @classmethod
+    def none(cls, len):
+        """
+        Create EncodedLabels without any labelsets.
+        """
+        return cls(len, buffer=np.zeros(len, dtype=int))
+
+    @classmethod
+    def from_labelset(cls, len, labelset):
+        """
+        Create EncodedLabels with all points labelled to the given labelset.
+        """
+        return cls(len, buffer=np.ones(len), labels={0: _lset(), 1: _lset(labelset)})
+
+    @classmethod
+    def concatenate(cls, *label_arrs):
+        if not label_arrs:
+            return EncodedLabels.none(0)
+        lookups = EncodedLabels._get_merged_lookups(label_arrs)
+        total = sum(len(len_) for len_ in label_arrs)
+        concat = cls(total, labels=lookups[0])
+        ptr = 0
+        for block in EncodedLabels._merged_translate(label_arrs, lookups):
+            nptr = ptr + len(block)
+            # Concatenate the translated block
+            concat[ptr:nptr] = block
+            ptr = nptr
+        return concat
+
+    @staticmethod
+    def _get_merged_lookups(arrs):
+        if not arrs:
+            return {0: _lset()}
+        merged = {}
+        new_labelsets = set()
+        to_map_arrs = {}
+        for arr in arrs:
+            for k, l in arr.labels.items():
+                if k not in merged:
+                    # The label spot is available, so take it
+                    merged[k] = l
+                elif merged[k] != l:
+                    # The labelset doesn't match, so this array will have to be mapped,
+                    # and a new spot found for the conflicting labelset.
+                    new_labelsets.add(l)
+                    # np ndarray unhashable, for good reason, so use `id()` for quick hash
+                    to_map_arrs[id(arr)] = arr
+                # else: this labelset matches with the superset's nothing to do
+
+        # Collect new spots for new labelsets
+        counter = (c for c in itertools.count() if c not in merged)
+        lset_map = {}
+        for labelset in new_labelsets:
+            key = next(counter)
+            merged[key] = labelset
+            lset_map[labelset] = key
+
+        return merged, to_map_arrs, lset_map
+
+    def _merged_translate(arrs, lookups=None):
+        if lookups is None:
+            merged, to_map_arrs, lset_map = EncodedLabels._get_merged_lookups(arrs)
+        else:
+            merged, to_map_arrs, lset_map = lookups
+        for arr in arrs:
+            if id(arr) not in to_map_arrs:
+                # None of the label array's labelsets need to be mapped, good as is.
+                block = arr
+            else:
+                # Lookup each labelset, if found, map to new value, otherwise, map to
+                # original value.
+                arrmap = {og: lset_map.get(lset, og) for og, lset in arr.labels.items()}
+                block = np.vectorize(arrmap.get)(arr)
+            yield block
```

### Comparing `bsb_core-4.0.1/bsb/_options.py` & `bsb_core-4.1.0/bsb/_options.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,160 +1,160 @@
-"""
-This module registers all the options that the BSB provides out of the box. They are
-registered using plugin registration. See `setup.py` for the setuptools metadata.
-"""
-
-from .option import BsbOption
-from .reporting import report
-
-
-class VerbosityOption(
-    BsbOption,
-    name="verbosity",
-    cli=("v", "verbosity"),
-    project=("verbosity",),
-    env=("BSB_VERBOSITY",),
-    script=("verbosity",),
-):
-    """
-    Set the verbosity of the package. Verbosity 0 is completely silent, 1 is default,
-    2 is verbose, 3 is progress and 4 is debug.
-    """
-
-    def setter(self, value):
-        return int(value)
-
-    def getter(self, value):
-        return int(value)
-
-    def get_default(self):
-        return 1
-
-
-class ForceFlag(
-    BsbOption,
-    name="force",
-    cli=("f", "force"),
-    env=("BSB_FOOTGUN_MODE",),
-    script=("sudo",),
-    flag=True,
-):
-    """
-    Enable sudo mode. Will execute destructive actions without confirmation, error or user
-    interaction.
-    """
-
-    def setter(self, value):
-        return bool(value)
-
-    def get_default(self):
-        return False
-
-
-class VersionFlag(
-    BsbOption,
-    name="version",
-    script=("version",),
-    cli=("version",),
-    readonly=True,
-    action=True,
-):
-    """
-    Return the version of the package.
-    """
-
-    def get_default(self):
-        from . import __version__
-
-        return __version__
-
-    def action(self, namespace):
-        report("bsb " + str(self.get()), level=1)
-
-
-class ConfigOption(
-    BsbOption,
-    name="config",
-    cli=("c", "config"),
-    script=("config",),
-    project=("config",),
-    env=("BSB_CONFIG_FILE",),
-):
-    """
-    Specify the config file to use when creating new networks through the CLI.
-    """
-
-    def get_default(self):
-        return "network_configuration.json"
-
-
-class ProfilingOption(
-    BsbOption,
-    name="profiling",
-    cli=("pr", "profiling"),
-    project=("profiling",),
-    script=("profiling",),
-    env=("BSB_PROFILING",),
-    flag=True,
-):
-    """
-    Enables profiling.
-    """
-
-    def setter(self, value):
-        from .profiling import activate_session, get_active_session
-
-        if value:
-            activate_session()
-        else:
-            get_active_session().stop()
-        return bool(value)
-
-    def getter(self, value):
-        return bool(value)
-
-    def get_default(self):
-        return False
-
-
-class DebugPoolFlag(
-    BsbOption,
-    name="debug_pool",
-    cli=("dp", "debug_pool"),
-    project=("debug_pool",),
-    env=("BSB_DEBUG_POOL",),
-    script=("debug_pool",),
-    flag=True,
-):
-    """
-    Debug job pools
-    """
-
-    def setter(self, value):
-        return bool(value)
-
-    def get_default(self):
-        return False
-
-
-def verbosity():
-    return VerbosityOption
-
-
-def version():
-    return VersionFlag
-
-
-def sudo():
-    return ForceFlag
-
-
-def config():
-    return ConfigOption
-
-
-def profiling():
-    return ProfilingOption
-
-
-def debug_pool():
-    return DebugPoolFlag
+"""
+This module registers all the options that the BSB provides out of the box. They are
+registered using plugin registration. See `setup.py` for the setuptools metadata.
+"""
+
+from .option import BsbOption
+from .reporting import report
+
+
+class VerbosityOption(
+    BsbOption,
+    name="verbosity",
+    cli=("v", "verbosity"),
+    project=("verbosity",),
+    env=("BSB_VERBOSITY",),
+    script=("verbosity",),
+):
+    """
+    Set the verbosity of the package. Verbosity 0 is completely silent, 1 is default,
+    2 is verbose, 3 is progress and 4 is debug.
+    """
+
+    def setter(self, value):
+        return int(value)
+
+    def getter(self, value):
+        return int(value)
+
+    def get_default(self):
+        return 1
+
+
+class ForceFlag(
+    BsbOption,
+    name="force",
+    cli=("f", "force"),
+    env=("BSB_FOOTGUN_MODE",),
+    script=("sudo",),
+    flag=True,
+):
+    """
+    Enable sudo mode. Will execute destructive actions without confirmation, error or user
+    interaction.
+    """
+
+    def setter(self, value):
+        return bool(value)
+
+    def get_default(self):
+        return False
+
+
+class VersionFlag(
+    BsbOption,
+    name="version",
+    script=("version",),
+    cli=("version",),
+    readonly=True,
+    action=True,
+):
+    """
+    Return the version of the package.
+    """
+
+    def get_default(self):
+        from . import __version__
+
+        return __version__
+
+    def action(self, namespace):
+        report("bsb " + str(self.get()), level=1)
+
+
+class ConfigOption(
+    BsbOption,
+    name="config",
+    cli=("c", "config"),
+    script=("config",),
+    project=("config",),
+    env=("BSB_CONFIG_FILE",),
+):
+    """
+    Specify the config file to use when creating new networks through the CLI.
+    """
+
+    def get_default(self):
+        return "network_configuration.json"
+
+
+class ProfilingOption(
+    BsbOption,
+    name="profiling",
+    cli=("pr", "profiling"),
+    project=("profiling",),
+    script=("profiling",),
+    env=("BSB_PROFILING",),
+    flag=True,
+):
+    """
+    Enables profiling.
+    """
+
+    def setter(self, value):
+        from .profiling import activate_session, get_active_session
+
+        if value:
+            activate_session()
+        else:
+            get_active_session().stop()
+        return bool(value)
+
+    def getter(self, value):
+        return bool(value)
+
+    def get_default(self):
+        return False
+
+
+class DebugPoolFlag(
+    BsbOption,
+    name="debug_pool",
+    cli=("dp", "debug_pool"),
+    project=("debug_pool",),
+    env=("BSB_DEBUG_POOL",),
+    script=("debug_pool",),
+    flag=True,
+):
+    """
+    Debug job pools
+    """
+
+    def setter(self, value):
+        return bool(value)
+
+    def get_default(self):
+        return False
+
+
+def verbosity():
+    return VerbosityOption
+
+
+def version():
+    return VersionFlag
+
+
+def sudo():
+    return ForceFlag
+
+
+def config():
+    return ConfigOption
+
+
+def profiling():
+    return ProfilingOption
+
+
+def debug_pool():
+    return DebugPoolFlag
```

### Comparing `bsb_core-4.0.1/bsb/_util.py` & `bsb_core-4.1.0/bsb/_util.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,148 +1,148 @@
-import contextlib as _ctxlib
-import functools as _ft
-import inspect as _inspect
-import itertools as _it
-import os as _os
-import sys as _sys
-import typing as _t
-
-import numpy as _np
-
-ichain = _it.chain.from_iterable
-
-
-def merge_dicts(a, b):
-    """
-    Merge 2 dictionaries and their subdictionaries
-    """
-    for key in b:
-        if key in a and isinstance(a[key], dict) and isinstance(b[key], dict):
-            merge_dicts(a[key], b[key])
-        else:
-            a[key] = b[key]
-    return a
-
-
-def obj_str_insert(__str__):
-    """
-    Decorator to insert the return value of __str__ into '<classname {returnvalue} at 0x...>'
-    """
-
-    @_ft.wraps(__str__)
-    def wrapper(self):
-        obj_str = object.__repr__(self)
-        return obj_str.replace("at 0x", f"{__str__(self)} at 0x")
-
-    return wrapper
-
-
-@_ctxlib.contextmanager
-def suppress_stdout():
-    """
-    Context manager that attempts to silence regular stdout and stderr. Some binary
-    components may yet circumvene this if they access the underlying OS's stdout directly,
-    like streaming to `/dev/stdout`.
-    """
-    with open(_os.devnull, "w") as devnull:
-        old_stdout = _sys.stdout
-        old_stderr = _sys.stderr
-        _sys.stdout = devnull
-        _sys.stderr = devnull
-        try:
-            yield
-        finally:
-            _sys.stdout = old_stdout
-            _sys.stderr = old_stderr
-
-
-def get_qualified_class_name(x):
-    """Return an object's module and class name"""
-    if _inspect.isclass(x):
-        return f"{x.__module__}.{str(x.__name__)}"
-    return f"{x.__class__.__module__}.{str(x.__class__.__name__)}"
-
-
-def listify_input(value):
-    """
-    Turn any non-list values into a list containing the value. Sequences will be
-    converted to a list using `list()`, `None` will  be replaced by an empty list.
-    """
-    if value is None:
-        return []
-    if isinstance(value, str):
-        return [str]
-    try:
-        return list(value)
-    except Exception:
-        return [value]
-
-
-def sanitize_ndarray(arr_input, shape, dtype=None):
-    """
-    Convert an object to an ndarray and shape, avoiding to copy it wherever possible.
-    """
-    kwargs = {"copy": False}
-    if dtype is not None:
-        kwargs["dtype"] = dtype
-    arr = _np.array(arr_input, **kwargs)
-    arr.shape = shape
-    return arr
-
-
-def assert_samelen(*args):
-    """
-    Assert that all input arguments have the same length.
-    """
-    len_ = None
-    assert all(
-        (len_ := len(arg) if len_ is None else len(arg)) == len_ for arg in args
-    ), "Input arguments should be of same length."
-
-
-def immutable():
-    """
-    Decorator to mark a method as immutable, so that any calls to it return, and are
-    performed on, a copy of the instance.
-    """
-
-    def immutable_decorator(f):
-        @_ft.wraps(f)
-        def immutable_action(self, *args, **kwargs):
-            new_instance = self.__copy__()
-            f(new_instance, *args, **kwargs)
-            return new_instance
-
-        return immutable_action
-
-    return immutable_decorator
-
-
-def unique(iter_: _t.Iterable[_t.Any]):
-    """Return a new list containing all the unique elements of an iterator"""
-    return [*set(iter_)]
-
-
-def rotation_matrix_from_vectors(vec1, vec2):
-    """Find the rotation matrix that aligns vec1 to vec2
-
-    :param vec1: A 3d "source" vector
-    :param vec2: A 3d "destination" vector
-    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.
-    """
-    if (
-        _np.isnan(vec1).any()
-        or _np.isnan(vec2).any()
-        or not _np.any(vec1)
-        or not _np.any(vec2)
-    ):
-        raise ValueError("Vectors should not contain nan and their norm should not be 0.")
-    a = (vec1 / _np.linalg.norm(vec1)).reshape(3)
-    b = (vec2 / _np.linalg.norm(vec2)).reshape(3)
-    v = _np.cross(a, b)
-    if any(v):  # if not all zeros then
-        c = _np.dot(a, b)
-        s = _np.linalg.norm(v)
-        kmat = _np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
-        return _np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s**2))
-    else:
-        return _np.eye(3)  # cross of all zeros only occurs on identical directions
+import contextlib as _ctxlib
+import functools as _ft
+import inspect as _inspect
+import itertools as _it
+import os as _os
+import sys as _sys
+import typing as _t
+
+import numpy as _np
+
+ichain = _it.chain.from_iterable
+
+
+def merge_dicts(a, b):
+    """
+    Merge 2 dictionaries and their subdictionaries
+    """
+    for key in b:
+        if key in a and isinstance(a[key], dict) and isinstance(b[key], dict):
+            merge_dicts(a[key], b[key])
+        else:
+            a[key] = b[key]
+    return a
+
+
+def obj_str_insert(__str__):
+    """
+    Decorator to insert the return value of __str__ into '<classname {returnvalue} at 0x...>'
+    """
+
+    @_ft.wraps(__str__)
+    def wrapper(self):
+        obj_str = object.__repr__(self)
+        return obj_str.replace("at 0x", f"{__str__(self)} at 0x")
+
+    return wrapper
+
+
+@_ctxlib.contextmanager
+def suppress_stdout():
+    """
+    Context manager that attempts to silence regular stdout and stderr. Some binary
+    components may yet circumvene this if they access the underlying OS's stdout directly,
+    like streaming to `/dev/stdout`.
+    """
+    with open(_os.devnull, "w") as devnull:
+        old_stdout = _sys.stdout
+        old_stderr = _sys.stderr
+        _sys.stdout = devnull
+        _sys.stderr = devnull
+        try:
+            yield
+        finally:
+            _sys.stdout = old_stdout
+            _sys.stderr = old_stderr
+
+
+def get_qualified_class_name(x):
+    """Return an object's module and class name"""
+    if _inspect.isclass(x):
+        return f"{x.__module__}.{str(x.__name__)}"
+    return f"{x.__class__.__module__}.{str(x.__class__.__name__)}"
+
+
+def listify_input(value):
+    """
+    Turn any non-list values into a list containing the value. Sequences will be
+    converted to a list using `list()`, `None` will  be replaced by an empty list.
+    """
+    if value is None:
+        return []
+    if isinstance(value, str):
+        return [str]
+    try:
+        return list(value)
+    except Exception:
+        return [value]
+
+
+def sanitize_ndarray(arr_input, shape, dtype=None):
+    """
+    Convert an object to an ndarray and shape, avoiding to copy it wherever possible.
+    """
+    kwargs = {"copy": False}
+    if dtype is not None:
+        kwargs["dtype"] = dtype
+    arr = _np.array(arr_input, **kwargs)
+    arr.shape = shape
+    return arr
+
+
+def assert_samelen(*args):
+    """
+    Assert that all input arguments have the same length.
+    """
+    len_ = None
+    assert all(
+        ((len_ := len(arg)) if len_ is None else len(arg)) == len_ for arg in args
+    ), "Input arguments should be of same length."
+
+
+def immutable():
+    """
+    Decorator to mark a method as immutable, so that any calls to it return, and are
+    performed on, a copy of the instance.
+    """
+
+    def immutable_decorator(f):
+        @_ft.wraps(f)
+        def immutable_action(self, *args, **kwargs):
+            new_instance = self.__copy__()
+            f(new_instance, *args, **kwargs)
+            return new_instance
+
+        return immutable_action
+
+    return immutable_decorator
+
+
+def unique(iter_: _t.Iterable[_t.Any]):
+    """Return a new list containing all the unique elements of an iterator"""
+    return [*set(iter_)]
+
+
+def rotation_matrix_from_vectors(vec1, vec2):
+    """Find the rotation matrix that aligns vec1 to vec2
+
+    :param vec1: A 3d "source" vector
+    :param vec2: A 3d "destination" vector
+    :return mat: A transform matrix (3x3) which when applied to vec1, aligns it with vec2.
+    """
+    if (
+        _np.isnan(vec1).any()
+        or _np.isnan(vec2).any()
+        or not _np.any(vec1)
+        or not _np.any(vec2)
+    ):
+        raise ValueError("Vectors should not contain nan and their norm should not be 0.")
+    a = (vec1 / _np.linalg.norm(vec1)).reshape(3)
+    b = (vec2 / _np.linalg.norm(vec2)).reshape(3)
+    v = _np.cross(a, b)
+    if any(v):  # if not all zeros then
+        c = _np.dot(a, b)
+        s = _np.linalg.norm(v)
+        kmat = _np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])
+        return _np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s**2))
+    else:
+        return _np.eye(3)  # cross of all zeros only occurs on identical directions
```

### Comparing `bsb_core-4.0.1/bsb/cli/commands/__init__.py` & `bsb_core-4.1.0/bsb/cli/commands/__init__.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,154 +1,154 @@
-"""
-Contains all of the logic required to create commands. It should always suffice to import
-just this module for a user to create their own commands.
-
-Inherit from :class:`BaseCommand` for regular CLI style commands, or from
-:class:`BsbCommand` if you want more freedom in what exactly constitutes a command to the
-BSB.
-"""
-
-import argparse
-
-from ...exceptions import CommandError
-from ...reporting import report
-
-
-class BaseParser(argparse.ArgumentParser):
-    """
-    Inherits from argparse.ArgumentParser and overloads the ``error``
-    method so that when an error occurs, instead of exiting and exception
-    is thrown.
-    """
-
-    def error(self, message):
-        """
-        Raise message, instead of exiting.
-
-        :param message: Error message
-        :type message: str
-        """
-        raise CommandError(message)
-
-
-_is_root = True
-
-
-class BsbCommand:
-    def add_to_parser(self):
-        raise NotImplementedError("Commands must implement a `add_to_parser` method.")
-
-    def handler(self, context):
-        raise NotImplementedError("Commands must implement a `handler` method.")
-
-    def __init_subclass__(cls, parent=None, abstract=False, name=None, **kwargs):
-        global _is_root
-        if abstract:
-            return
-        if cls.add_to_parser is BsbCommand.add_to_parser:
-            raise NotImplementedError("Commands must implement a `add_to_parser` method.")
-        if cls.handler is BsbCommand.handler:
-            raise NotImplementedError("Commands must implement a `handler` method.")
-        if name is None:
-            raise CommandError(f"{cls} must register a name.")
-        cls.name = name
-        cls._subcommands = []
-        # The very first registered command will be the RootCommand for `bsb`
-        if _is_root:
-            _is_root = False
-        else:
-            if parent is None:
-                parent = RootCommand
-            parent._subcommands.append(cls)
-
-
-class BaseCommand(BsbCommand, abstract=True):
-    def add_to_parser(self, parent, context, locals, level):
-        locals = locals.copy()
-        locals.update(self.get_options())
-        parser = parent.add_parser(self.name)
-        self.add_parser_arguments(parser)
-        self.add_parser_options(parser, context, locals, level)
-        parser.set_defaults(handler=self.execute_handler)
-        self.add_subparsers(parser, context, self._subcommands, locals, level)
-        return parser
-
-    def add_subparsers(self, parser, context, commands, locals, level):
-        if len(commands) > 0:
-            subparsers = parser.add_subparsers()
-            for command in commands:
-                c = command()
-                c._parent = self
-                c.add_to_parser(subparsers, context, locals, level + 1)
-
-    def execute_handler(self, namespace, dryrun=False):
-        reduced = {}
-        context = namespace._context
-        for k, v in namespace.__dict__.items():
-            if v is None or k in ["_context", "handler"]:
-                continue
-            stripped = k.lstrip("_")
-            level = len(k) - len(stripped)
-            if stripped not in reduced or level > reduced[stripped][0]:
-                reduced[stripped] = (level, v)
-
-        namespace.__dict__ = {k: v[1] for k, v in reduced.items()}
-        self.add_locals(context)
-        context.set_cli_namespace(namespace)
-        report(f"Context: {context}", level=4)
-        if not dryrun:
-            self.handler(context)
-
-    def add_locals(self, context):
-        # Merge our options into the context, preserving those in the context as we're
-        # going up the tree towards lower priority and less specific options.
-        options = self.get_options()
-        options.update(context.options)
-        context.options = options
-        if hasattr(self, "_parent"):
-            self._parent.add_locals(context)
-
-    def add_parser_options(self, parser, context, locals, level):
-        merged = {}
-        merged.update(context.options)
-        merged.update(locals)
-        for option in merged.values():
-            option.add_to_parser(parser, level)
-
-    def get_options(self):
-        raise NotImplementedError(
-            "BaseCommands must implement a `get_options(self)` method."
-        )
-
-    def add_parser_arguments(self, parser):
-        raise NotImplementedError(
-            "BaseCommands must implement an `add_parser_arguments(self, parser)` method."
-        )
-
-
-class RootCommand(BaseCommand, name="bsb"):
-    def handler(self, context):
-        pass
-
-    def get_parser(self, context):
-        parser = BaseParser()
-        parser.set_defaults(_context=context)
-        parser.set_defaults(handler=self.execute_handler)
-        locals = self.get_options()
-        self.add_parser_options(parser, context, locals, 0)
-        self.add_subparsers(parser, context, self._subcommands, locals, 0)
-        return parser
-
-    def get_options(self):
-        return {}
-
-
-def load_root_command():
-    from ...plugins import discover
-
-    # Simply discovering the plugin modules should append them to their parent command
-    # class using the `__init_subclass__` function.
-    discover("commands")
-    return RootCommand()
-
-
-__all__ = ["BaseCommand", "BsbCommand", "RootCommand", "load_root_command"]
+"""
+Contains all of the logic required to create commands. It should always suffice to import
+just this module for a user to create their own commands.
+
+Inherit from :class:`BaseCommand` for regular CLI style commands, or from
+:class:`BsbCommand` if you want more freedom in what exactly constitutes a command to the
+BSB.
+"""
+
+import argparse
+
+from ...exceptions import CommandError
+from ...reporting import report
+
+
+class BaseParser(argparse.ArgumentParser):
+    """
+    Inherits from argparse.ArgumentParser and overloads the ``error``
+    method so that when an error occurs, instead of exiting and exception
+    is thrown.
+    """
+
+    def error(self, message):
+        """
+        Raise message, instead of exiting.
+
+        :param message: Error message
+        :type message: str
+        """
+        raise CommandError(message)
+
+
+_is_root = True
+
+
+class BsbCommand:
+    def add_to_parser(self):
+        raise NotImplementedError("Commands must implement a `add_to_parser` method.")
+
+    def handler(self, context):
+        raise NotImplementedError("Commands must implement a `handler` method.")
+
+    def __init_subclass__(cls, parent=None, abstract=False, name=None, **kwargs):
+        global _is_root
+        if abstract:
+            return
+        if cls.add_to_parser is BsbCommand.add_to_parser:
+            raise NotImplementedError("Commands must implement a `add_to_parser` method.")
+        if cls.handler is BsbCommand.handler:
+            raise NotImplementedError("Commands must implement a `handler` method.")
+        if name is None:
+            raise CommandError(f"{cls} must register a name.")
+        cls.name = name
+        cls._subcommands = []
+        # The very first registered command will be the RootCommand for `bsb`
+        if _is_root:
+            _is_root = False
+        else:
+            if parent is None:
+                parent = RootCommand
+            parent._subcommands.append(cls)
+
+
+class BaseCommand(BsbCommand, abstract=True):
+    def add_to_parser(self, parent, context, locals, level):
+        locals = locals.copy()
+        locals.update(self.get_options())
+        parser = parent.add_parser(self.name)
+        self.add_parser_arguments(parser)
+        self.add_parser_options(parser, context, locals, level)
+        parser.set_defaults(handler=self.execute_handler)
+        self.add_subparsers(parser, context, self._subcommands, locals, level)
+        return parser
+
+    def add_subparsers(self, parser, context, commands, locals, level):
+        if len(commands) > 0:
+            subparsers = parser.add_subparsers()
+            for command in commands:
+                c = command()
+                c._parent = self
+                c.add_to_parser(subparsers, context, locals, level + 1)
+
+    def execute_handler(self, namespace, dryrun=False):
+        reduced = {}
+        context = namespace._context
+        for k, v in namespace.__dict__.items():
+            if v is None or k in ["_context", "handler"]:
+                continue
+            stripped = k.lstrip("_")
+            level = len(k) - len(stripped)
+            if stripped not in reduced or level > reduced[stripped][0]:
+                reduced[stripped] = (level, v)
+
+        namespace.__dict__ = {k: v[1] for k, v in reduced.items()}
+        self.add_locals(context)
+        context.set_cli_namespace(namespace)
+        report(f"Context: {context}", level=4)
+        if not dryrun:
+            self.handler(context)
+
+    def add_locals(self, context):
+        # Merge our options into the context, preserving those in the context as we're
+        # going up the tree towards lower priority and less specific options.
+        options = self.get_options()
+        options.update(context.options)
+        context.options = options
+        if hasattr(self, "_parent"):
+            self._parent.add_locals(context)
+
+    def add_parser_options(self, parser, context, locals, level):
+        merged = {}
+        merged.update(context.options)
+        merged.update(locals)
+        for option in merged.values():
+            option.add_to_parser(parser, level)
+
+    def get_options(self):
+        raise NotImplementedError(
+            "BaseCommands must implement a `get_options(self)` method."
+        )
+
+    def add_parser_arguments(self, parser):
+        raise NotImplementedError(
+            "BaseCommands must implement an `add_parser_arguments(self, parser)` method."
+        )
+
+
+class RootCommand(BaseCommand, name="bsb"):
+    def handler(self, context):
+        pass
+
+    def get_parser(self, context):
+        parser = BaseParser()
+        parser.set_defaults(_context=context)
+        parser.set_defaults(handler=self.execute_handler)
+        locals = self.get_options()
+        self.add_parser_options(parser, context, locals, 0)
+        self.add_subparsers(parser, context, self._subcommands, locals, 0)
+        return parser
+
+    def get_options(self):
+        return {}
+
+
+def load_root_command():
+    from ...plugins import discover
+
+    # Simply discovering the plugin modules should append them to their parent command
+    # class using the `__init_subclass__` function.
+    discover("commands")
+    return RootCommand()
+
+
+__all__ = ["BaseCommand", "BsbCommand", "RootCommand", "load_root_command"]
```

### Comparing `bsb_core-4.0.1/bsb/cli/commands/_commands.py` & `bsb_core-4.1.0/bsb/cli/commands/_commands.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,265 +1,265 @@
-"""
-Contains builtin commands.
-"""
-
-import itertools
-from uuid import uuid4
-
-import errr
-
-from ..._options import ConfigOption
-from ...config import parse_configuration_file
-from ...core import Scaffold, from_storage
-from ...exceptions import NodeNotFoundError
-from ...option import BsbOption
-from ...storage import open_storage
-from . import BaseCommand
-
-
-class XScale(BsbOption, name="x", cli=("x",), env=("BSB_CONFIG_NETWORK_X",)):
-    pass
-
-
-class YScale(BsbOption, name="y", cli=("y",), env=("BSB_CONFIG_NETWORK_Y",)):
-    pass
-
-
-class ZScale(BsbOption, name="z", cli=("z",), env=("BSB_CONFIG_NETWORK_Z",)):
-    pass
-
-
-class Skip(BsbOption, name="skip", cli=("skip",), env=("BSB_SELECTION_SKIP",), list=True):
-    pass
-
-
-class Only(BsbOption, name="only", cli=("only",), env=("BSB_SELECTION_ONLY",), list=True):
-    pass
-
-
-class Append(
-    BsbOption, name="append", cli=("append", "a"), env=("BSB_APPEND_MODE",), flag=True
-):
-    pass
-
-
-class Redo(BsbOption, name="redo", cli=("redo", "r"), env=("BSB_REDO_MODE",), flag=True):
-    pass
-
-
-class Clear(
-    BsbOption, name="clear", cli=("clear", "w"), env=("BSB_CLEAR_MODE",), flag=True
-):
-    pass
-
-
-class Output(BsbOption, name="output", cli=("output", "o"), env=("BSB_OUTPUT_FILE",)):
-    pass
-
-
-class SkipPlacement(
-    BsbOption,
-    name="skip_placement",
-    cli=("np", "skip-placement"),
-    env=("BSB_SKIP_PLACEMENT",),
-    flag=True,
-):
-    pass
-
-
-class SkipAfterPlacement(
-    BsbOption,
-    name="skip_after_placement",
-    cli=("nap", "skip-after-placement"),
-    env=("BSB_SKIP_AFTER_PLACEMENT",),
-    flag=True,
-):
-    pass
-
-
-class SkipConnectivity(
-    BsbOption,
-    name="skip_connectivity",
-    cli=("nc", "skip-connectivity"),
-    env=("BSB_SKIP_CONNECTIVITY",),
-    flag=True,
-):
-    pass
-
-
-class SkipAfterConnectivity(
-    BsbOption,
-    name="skip_after_connectivity",
-    cli=("nac", "skip-after-connectivity"),
-    env=("BSB_SKIP_AFTER_CONNECTIVITY",),
-    flag=True,
-):
-    pass
-
-
-class IgnoreErrors(
-    BsbOption,
-    name="ignore_errors",
-    cli=("ignore", "ignore-errors"),
-    env=("BSB_IGNORE_ERRORS",),
-    flag=True,
-):
-    pass
-
-
-def _flatten_arr_args(arr):
-    if arr is None:
-        return arr
-    else:
-        return list(itertools.chain.from_iterable(a.split(",") for a in arr))
-
-
-class MakeConfigCommand(BaseCommand, name="make-config"):
-    def handler(self, context):
-        from ...config import copy_template
-
-        args = context.arguments
-        copy_template(args.template, args.output, path=args.path or ())
-
-    def get_options(self):
-        return {}
-
-    def add_parser_arguments(self, parser):
-        parser.add_argument("template", nargs="?", default="skeleton.json")
-        parser.add_argument("output", nargs="?", default="network_configuration.json")
-        parser.add_argument(
-            "--path",
-            help="Additional paths to search for config templates",
-            action="extend",
-            nargs="+",
-            default=False,
-        )
-
-
-class BsbCompile(BaseCommand, name="compile"):
-    def handler(self, context):
-        cfg = parse_configuration_file(context.config)
-        network = Scaffold(cfg)
-        network.resize(context.x, context.y, context.z)
-        network.compile(
-            skip_placement=context.skip_placement,
-            skip_after_placement=context.skip_after_placement,
-            skip_connectivity=context.skip_connectivity,
-            skip_after_connectivity=context.skip_after_connectivity,
-            only=_flatten_arr_args(context.only),
-            skip=_flatten_arr_args(context.skip),
-            clear=context.clear,
-            force=context.force,
-            append=context.append,
-            redo=context.redo,
-            fail_fast=not context.ignore_errors,
-        )
-
-    def get_options(self):
-        return {
-            "x": XScale(),
-            "y": YScale(),
-            "z": ZScale(),
-            "skip": Skip(),
-            "only": Only(),
-            "config": ConfigOption(positional=True),
-            "no_placement": SkipPlacement(),
-            "no_after_placement": SkipAfterPlacement(),
-            "no_connectivity": SkipConnectivity(),
-            "no_after_connectivity": SkipAfterConnectivity(),
-            "append": Append(),
-            "redo": Redo(),
-            "clear": Clear(),
-            "output": Output(),
-            "ignore_errors": IgnoreErrors(),
-        }
-
-    def add_parser_arguments(self, parser):
-        pass
-
-
-class BsbReconfigure(BaseCommand, name="reconfigure"):
-    def handler(self, context):
-        cfg = parse_configuration_file(context.config)
-        # Bootstrap the scaffold and clear the storage if not in append mode
-        storage = open_storage(context.arguments.network)
-        storage.store_active_config(cfg)
-
-    def get_options(self):
-        return {
-            "config": ConfigOption(positional=True),
-        }
-
-    def add_parser_arguments(self, parser):
-        parser.add_argument("network")
-
-
-class BsbSimulate(BaseCommand, name="simulate"):
-    def handler(self, context):
-        network = from_storage(context.arguments.network)
-        config_option = context.options["config"]
-        sim_name = context.arguments.simulation
-        extra_simulations = {}
-        if config_option.is_set("cli"):
-            extra_simulations = parse_configuration_file(context.config).simulations
-            for name, sim in extra_simulations.items():
-                if name not in network.simulations and name == sim_name:
-                    network.simulations[sim_name] = sim
-        try:
-            result = network.run_simulation(sim_name)
-        except NodeNotFoundError as e:
-            append = ", " if len(network.simulations) else ""
-            append += ", ".join(f"'{name}'" for name in extra_simulations.keys())
-            errr.wrap(type(e), e, append=append)
-        else:
-            result.write(getattr(context.arguments, "output", f"{uuid4()}.nio"), "ow")
-
-    def get_options(self):
-        return {
-            "skip": Skip(),
-            "only": Only(),
-        }
-
-    def add_parser_arguments(self, parser):
-        parser.add_argument("network")
-        parser.add_argument("simulation")
-        parser.add_argument("-o", "--output")
-
-
-class CacheCommand(BaseCommand, name="cache"):  # pragma: nocover
-    def handler(self, context):
-        import shutil
-        from datetime import datetime
-
-        from ...storage._util import _cache_path
-
-        if context.clear:
-            shutil.rmtree(_cache_path)
-            _cache_path.mkdir(parents=True, exist_ok=True)
-            print("Cache cleared")
-        else:
-            _cache_path.mkdir(parents=True, exist_ok=True)
-            files = [*_cache_path.iterdir()]
-            maxlen = 5
-            try:
-                maxlen = max(maxlen, max(len(file.name) for file in files))
-            except ValueError:
-                print("Cache is empty")
-            else:
-                print(f"{'Files'.ljust(maxlen, ' ')}    Cached at\t\t\t    Size")
-                total_mb = 0
-                for f in files:
-                    name = f.name.ljust(maxlen, " ")
-                    stat = f.stat()
-                    stamp = datetime.fromtimestamp(stat.st_mtime)
-                    total_mb += (mb := stat.st_size / 1e6)
-                    line = f"{name}    {stamp}    {mb:.2f}MB"
-                    print(line)
-                print(f"Total: {total_mb:.2f}MB".rjust(len(line)))
-
-    def get_options(self):
-        return {
-            "clear": Clear(),
-        }
-
-    def add_parser_arguments(self, parser):
-        pass
+"""
+Contains builtin commands.
+"""
+
+import itertools
+from uuid import uuid4
+
+import errr
+
+from ..._options import ConfigOption
+from ...config import parse_configuration_file
+from ...core import Scaffold, from_storage
+from ...exceptions import NodeNotFoundError
+from ...option import BsbOption
+from ...storage import open_storage
+from . import BaseCommand
+
+
+class XScale(BsbOption, name="x", cli=("x",), env=("BSB_CONFIG_NETWORK_X",)):
+    pass
+
+
+class YScale(BsbOption, name="y", cli=("y",), env=("BSB_CONFIG_NETWORK_Y",)):
+    pass
+
+
+class ZScale(BsbOption, name="z", cli=("z",), env=("BSB_CONFIG_NETWORK_Z",)):
+    pass
+
+
+class Skip(BsbOption, name="skip", cli=("skip",), env=("BSB_SELECTION_SKIP",), list=True):
+    pass
+
+
+class Only(BsbOption, name="only", cli=("only",), env=("BSB_SELECTION_ONLY",), list=True):
+    pass
+
+
+class Append(
+    BsbOption, name="append", cli=("append", "a"), env=("BSB_APPEND_MODE",), flag=True
+):
+    pass
+
+
+class Redo(BsbOption, name="redo", cli=("redo", "r"), env=("BSB_REDO_MODE",), flag=True):
+    pass
+
+
+class Clear(
+    BsbOption, name="clear", cli=("clear", "w"), env=("BSB_CLEAR_MODE",), flag=True
+):
+    pass
+
+
+class Output(BsbOption, name="output", cli=("output", "o"), env=("BSB_OUTPUT_FILE",)):
+    pass
+
+
+class SkipPlacement(
+    BsbOption,
+    name="skip_placement",
+    cli=("np", "skip-placement"),
+    env=("BSB_SKIP_PLACEMENT",),
+    flag=True,
+):
+    pass
+
+
+class SkipAfterPlacement(
+    BsbOption,
+    name="skip_after_placement",
+    cli=("nap", "skip-after-placement"),
+    env=("BSB_SKIP_AFTER_PLACEMENT",),
+    flag=True,
+):
+    pass
+
+
+class SkipConnectivity(
+    BsbOption,
+    name="skip_connectivity",
+    cli=("nc", "skip-connectivity"),
+    env=("BSB_SKIP_CONNECTIVITY",),
+    flag=True,
+):
+    pass
+
+
+class SkipAfterConnectivity(
+    BsbOption,
+    name="skip_after_connectivity",
+    cli=("nac", "skip-after-connectivity"),
+    env=("BSB_SKIP_AFTER_CONNECTIVITY",),
+    flag=True,
+):
+    pass
+
+
+class IgnoreErrors(
+    BsbOption,
+    name="ignore_errors",
+    cli=("ignore", "ignore-errors"),
+    env=("BSB_IGNORE_ERRORS",),
+    flag=True,
+):
+    pass
+
+
+def _flatten_arr_args(arr):
+    if arr is None:
+        return arr
+    else:
+        return list(itertools.chain.from_iterable(a.split(",") for a in arr))
+
+
+class MakeConfigCommand(BaseCommand, name="make-config"):
+    def handler(self, context):
+        from ...config import copy_template
+
+        args = context.arguments
+        copy_template(args.template, args.output, path=args.path or ())
+
+    def get_options(self):
+        return {}
+
+    def add_parser_arguments(self, parser):
+        parser.add_argument("template", nargs="?", default="skeleton.json")
+        parser.add_argument("output", nargs="?", default="network_configuration.json")
+        parser.add_argument(
+            "--path",
+            help="Additional paths to search for config templates",
+            action="extend",
+            nargs="+",
+            default=False,
+        )
+
+
+class BsbCompile(BaseCommand, name="compile"):
+    def handler(self, context):
+        cfg = parse_configuration_file(context.config)
+        network = Scaffold(cfg)
+        network.resize(context.x, context.y, context.z)
+        network.compile(
+            skip_placement=context.skip_placement,
+            skip_after_placement=context.skip_after_placement,
+            skip_connectivity=context.skip_connectivity,
+            skip_after_connectivity=context.skip_after_connectivity,
+            only=_flatten_arr_args(context.only),
+            skip=_flatten_arr_args(context.skip),
+            clear=context.clear,
+            force=context.force,
+            append=context.append,
+            redo=context.redo,
+            fail_fast=not context.ignore_errors,
+        )
+
+    def get_options(self):
+        return {
+            "x": XScale(),
+            "y": YScale(),
+            "z": ZScale(),
+            "skip": Skip(),
+            "only": Only(),
+            "config": ConfigOption(positional=True),
+            "no_placement": SkipPlacement(),
+            "no_after_placement": SkipAfterPlacement(),
+            "no_connectivity": SkipConnectivity(),
+            "no_after_connectivity": SkipAfterConnectivity(),
+            "append": Append(),
+            "redo": Redo(),
+            "clear": Clear(),
+            "output": Output(),
+            "ignore_errors": IgnoreErrors(),
+        }
+
+    def add_parser_arguments(self, parser):
+        pass
+
+
+class BsbReconfigure(BaseCommand, name="reconfigure"):
+    def handler(self, context):
+        cfg = parse_configuration_file(context.config)
+        # Bootstrap the scaffold and clear the storage if not in append mode
+        storage = open_storage(context.arguments.network)
+        storage.store_active_config(cfg)
+
+    def get_options(self):
+        return {
+            "config": ConfigOption(positional=True),
+        }
+
+    def add_parser_arguments(self, parser):
+        parser.add_argument("network")
+
+
+class BsbSimulate(BaseCommand, name="simulate"):
+    def handler(self, context):
+        network = from_storage(context.arguments.network)
+        config_option = context.options["config"]
+        sim_name = context.arguments.simulation
+        extra_simulations = {}
+        if config_option.is_set("cli"):
+            extra_simulations = parse_configuration_file(context.config).simulations
+            for name, sim in extra_simulations.items():
+                if name not in network.simulations and name == sim_name:
+                    network.simulations[sim_name] = sim
+        try:
+            result = network.run_simulation(sim_name)
+        except NodeNotFoundError as e:
+            append = ", " if len(network.simulations) else ""
+            append += ", ".join(f"'{name}'" for name in extra_simulations.keys())
+            errr.wrap(type(e), e, append=append)
+        else:
+            result.write(getattr(context.arguments, "output", f"{uuid4()}.nio"), "ow")
+
+    def get_options(self):
+        return {
+            "skip": Skip(),
+            "only": Only(),
+        }
+
+    def add_parser_arguments(self, parser):
+        parser.add_argument("network")
+        parser.add_argument("simulation")
+        parser.add_argument("-o", "--output")
+
+
+class CacheCommand(BaseCommand, name="cache"):  # pragma: nocover
+    def handler(self, context):
+        import shutil
+        from datetime import datetime
+
+        from ...storage._util import _cache_path
+
+        if context.clear:
+            shutil.rmtree(_cache_path)
+            _cache_path.mkdir(parents=True, exist_ok=True)
+            print("Cache cleared")
+        else:
+            _cache_path.mkdir(parents=True, exist_ok=True)
+            files = [*_cache_path.iterdir()]
+            maxlen = 5
+            try:
+                maxlen = max(maxlen, max(len(file.name) for file in files))
+            except ValueError:
+                print("Cache is empty")
+            else:
+                print(f"{'Files'.ljust(maxlen, ' ')}    Cached at\t\t\t    Size")
+                total_mb = 0
+                for f in files:
+                    name = f.name.ljust(maxlen, " ")
+                    stat = f.stat()
+                    stamp = datetime.fromtimestamp(stat.st_mtime)
+                    total_mb += (mb := stat.st_size / 1e6)
+                    line = f"{name}    {stamp}    {mb:.2f}MB"
+                    print(line)
+                print(f"Total: {total_mb:.2f}MB".rjust(len(line)))
+
+    def get_options(self):
+        return {
+            "clear": Clear(),
+        }
+
+    def add_parser_arguments(self, parser):
+        pass
```

### Comparing `bsb_core-4.0.1/bsb/cli/commands/_projects.py` & `bsb_core-4.1.0/bsb/cli/commands/_projects.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-import pathlib
-
-import toml
-
-from ... import config
-from ...reporting import report
-from . import BaseCommand
-
-
-class ProjectNewCommand(BaseCommand, name="new"):
-    def get_options(self):
-        return {}
-
-    def add_parser_arguments(self, parser):
-        parser.add_argument("project_name", nargs="?", help="Project name", default="")
-        parser.add_argument(
-            "path", nargs="?", default=".", help="Location of the project"
-        )
-        parser.add_argument(
-            "--quickstart", action="store_true", help="Start an example project"
-        )
-        parser.add_argument(
-            "--json", action="store_true", help="Use JSON as configuration language"
-        )
-        parser.add_argument(
-            "--exists",
-            action="store_true",
-            help="Indicates whether the folder structure already exists.",
-        )
-
-    def handler(self, context):
-        name = (
-            context.arguments.project_name
-            or input("Project name [my_model]: ")
-            or "my_model"
-        )
-        root = pathlib.Path(context.arguments.path) / name
-        try:
-            root.mkdir(exist_ok=context.arguments.exists)
-        except FileExistsError:
-            return report(
-                f"Could not create '{root.absolute()}', directory exists.", level=0
-            )
-        ext = "json" if context.arguments.json else "yaml"
-        if context.arguments.quickstart:
-            template = f"starting_example.{ext}"
-            output = f"network_configuration.{ext}"
-        else:
-            template = input(f"Config template [skeleton.{ext}]: ") or f"skeleton.{ext}"
-            output = (
-                input(f"Config filename [network_configuration.{ext}]: ")
-                or f"network_configuration.{ext}"
-            )
-        config.copy_template(template, output=root / output)
-        with open(root / "pyproject.toml", "w") as f:
-            toml.dump(
-                {
-                    "tools": {
-                        "bsb": {
-                            "config": output,
-                        }
-                    }
-                },
-                f,
-            )
-        place_path = root / "placement.py"
-        conn_path = root / "connectome.py"
-        if not place_path.exists():
-            with open(place_path, "w") as f:
-                f.write("from bsb import PlacementStrategy\n")
-        if not conn_path.exists():
-            with open(conn_path, "w") as f:
-                f.write("from bsb import ConnectionStrategy\n")
-
-        report(f"Created '{name}' project structure.", level=1)
+import pathlib
+
+import toml
+
+from ... import config
+from ...reporting import report
+from . import BaseCommand
+
+
+class ProjectNewCommand(BaseCommand, name="new"):
+    def get_options(self):
+        return {}
+
+    def add_parser_arguments(self, parser):
+        parser.add_argument("project_name", nargs="?", help="Project name", default="")
+        parser.add_argument(
+            "path", nargs="?", default=".", help="Location of the project"
+        )
+        parser.add_argument(
+            "--quickstart", action="store_true", help="Start an example project"
+        )
+        parser.add_argument(
+            "--json", action="store_true", help="Use JSON as configuration language"
+        )
+        parser.add_argument(
+            "--exists",
+            action="store_true",
+            help="Indicates whether the folder structure already exists.",
+        )
+
+    def handler(self, context):
+        name = (
+            context.arguments.project_name
+            or input("Project name [my_model]: ")
+            or "my_model"
+        )
+        root = pathlib.Path(context.arguments.path) / name
+        try:
+            root.mkdir(exist_ok=context.arguments.exists)
+        except FileExistsError:
+            return report(
+                f"Could not create '{root.absolute()}', directory exists.", level=0
+            )
+        ext = "json" if context.arguments.json else "yaml"
+        if context.arguments.quickstart:
+            template = f"starting_example.{ext}"
+            output = f"network_configuration.{ext}"
+        else:
+            template = input(f"Config template [skeleton.{ext}]: ") or f"skeleton.{ext}"
+            output = (
+                input(f"Config filename [network_configuration.{ext}]: ")
+                or f"network_configuration.{ext}"
+            )
+        config.copy_template(template, output=root / output)
+        with open(root / "pyproject.toml", "w") as f:
+            toml.dump(
+                {
+                    "tools": {
+                        "bsb": {
+                            "config": output,
+                        }
+                    }
+                },
+                f,
+            )
+        place_path = root / "placement.py"
+        conn_path = root / "connectome.py"
+        if not place_path.exists():
+            with open(place_path, "w") as f:
+                f.write("from bsb import PlacementStrategy\n")
+        if not conn_path.exists():
+            with open(conn_path, "w") as f:
+                f.write("from bsb import ConnectionStrategy\n")
+
+        report(f"Created '{name}' project structure.", level=1)
```

### Comparing `bsb_core-4.0.1/bsb/config/__init__.py` & `bsb_core-4.1.0/bsb/config/__init__.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,242 +1,242 @@
-"""
-bsb.config module
-
-Contains the dynamic attribute system; Use ``@bsb.config.root/node/dynamic/pluggable`` to
-decorate your classes and add class attributes using ``x =
-config.attr/dict/list/ref/reflist`` to populate your classes with powerful attributes.
-"""
-
-import builtins
-import functools
-import glob
-import itertools
-import os
-import sys
-import traceback
-import typing
-from shutil import copy2 as copy_file
-
-from .. import plugins
-from .._util import ichain
-from ..exceptions import ConfigTemplateNotFoundError, ParserError
-from . import refs, types
-from ._attrs import (
-    ConfigurationAttribute,
-    attr,
-    catch_all,
-    dict,
-    dynamic,
-    file,
-    list,
-    node,
-    pluggable,
-    property,
-    provide,
-    ref,
-    reflist,
-    root,
-    slot,
-    unset,
-)
-from ._distributions import Distribution
-from ._hooks import after, before, has_hook, on, run_hook
-from ._make import (
-    compose_nodes,
-    get_config_attributes,
-    walk_node_attributes,
-    walk_nodes,
-)
-from .parsers import get_configuration_parser, get_configuration_parser_classes
-
-if typing.TYPE_CHECKING:
-    from ._config import Configuration
-
-
-@functools.cache
-def __getattr__(name):
-    if name == "Configuration":
-        # Load the Configuration class on demand, not on import, to avoid circular
-        # dependencies.
-        from ._config import Configuration
-
-        return Configuration
-    else:
-        raise object.__getattribute__(sys.modules[__name__], name)
-
-
-def get_config_path():
-    import os
-
-    env_paths = os.environ.get("BSB_CONFIG_PATH", None)
-    if env_paths is None:
-        env_paths = ()
-    else:
-        env_paths = env_paths.split(":")
-    plugin_paths = plugins.discover("config.templates")
-    return [*itertools.chain((os.getcwd(),), env_paths, *plugin_paths.values())]
-
-
-def copy_configuration_template(template, output="network_configuration.json", path=None):
-    path = [
-        *map(
-            os.path.abspath,
-            itertools.chain(get_config_path(), path or ()),
-        )
-    ]
-    for d in path:
-        if files := glob.glob(os.path.join(d, template)):
-            break
-    else:
-        raise ConfigTemplateNotFoundError(
-            "'%template%' not found in config path %path%", template, path
-        )
-    copy_file(files[0], output)
-
-
-def format_configuration_content(parser_name: str, config: "Configuration", **kwargs):
-    """
-    Convert a configuration object to a string using the given parser.
-    """
-    return get_configuration_parser(parser_name, **kwargs).generate(
-        config.__tree__(), pretty=True
-    )
-
-
-def make_configuration_diagram(config):
-    dot = f'digraph "{config.name or "network"}" {{'
-    for c in config.cell_types.values():
-        dot += f'\n  {c.name}[label="{c.name}"]'
-    for name, conn in config.connectivity.items():
-        for pre in conn.presynaptic.cell_types:
-            for post in conn.postsynaptic.cell_types:
-                dot += f'\n  {pre.name} -> {post.name}[label="{name}"];'
-    dot += "\n}\n"
-    return dot
-
-
-def _try_parsers(content, classes, ext=None, path=None):  # pragma: nocover
-    if ext is not None:
-
-        def file_has_parser_ext(kv):
-            return ext not in getattr(kv[1], "data_extensions", ())
-
-        classes = builtins.dict(sorted(classes.items(), key=file_has_parser_ext))
-    exc = {}
-    for name, cls in classes.items():
-        try:
-            tree, meta = cls().parse(content, path=path)
-        except Exception as e:
-            if getattr(e, "_bsbparser_show_user", False):
-                raise e from None
-            exc[name] = e
-        else:
-            return (name, tree, meta)
-    msges = [
-        (
-            f"- Can't parse contents with '{n}':\n",
-            "".join(traceback.format_exception(type(e), e, e.__traceback__)),
-        )
-        for n, e in exc.items()
-    ]
-    if path:
-        msg = f"Could not parse '{path}'"
-    else:
-        msg = f"Could not parse content string ({len(content)} characters long)"
-    raise ParserError("\n".join(ichain(msges)) + f"\n{msg}")
-
-
-def _from_parsed(parser_name, tree, meta, file=None):
-    from ._config import Configuration
-
-    conf = Configuration(tree)
-    conf._parser = parser_name
-    conf._meta = meta
-    conf._file = file
-    return conf
-
-
-def parse_configuration_file(file, parser=None, path=None, **kwargs):
-    if hasattr(file, "read"):
-        data = file.read()
-        try:
-            path = str(path) or os.fspath(file)
-        except TypeError:
-            pass
-    else:
-        file = os.path.abspath(file)
-        path = path or file
-        with open(file, "r") as f:
-            data = f.read()
-    return parse_configuration_content(data, parser, path, **kwargs)
-
-
-def parse_configuration_content(content, parser=None, path=None, **kwargs):
-    if parser is None:
-        parser_classes = get_configuration_parser_classes()
-        ext = path.split(".")[-1] if path is not None else None
-        parser_name, tree, meta = _try_parsers(content, parser_classes, ext, path=path)
-    elif isinstance(parser, str):
-        parser_name = parser
-        parser = get_configuration_parser(parser_name, **kwargs)
-        tree, meta = parser.parse(content, path=path)
-    else:
-        parser_name = parser.__name__
-        tree, meta = parser.parse(content, path=path)
-    return _from_parsed(parser_name, tree, meta, path)
-
-
-# Static public API
-__all__ = [
-    "Configuration",
-    "ConfigurationAttribute",
-    "Distribution",
-    "after",
-    "attr",
-    "before",
-    "catch_all",
-    "compose_nodes",
-    "copy_configuration_template",
-    "dict",
-    "dynamic",
-    "file",
-    "format_configuration_content",
-    "get_config_attributes",
-    "get_config_path",
-    "has_hook",
-    "list",
-    "make_configuration_diagram",
-    "node",
-    "on",
-    "parse_configuration_file",
-    "parse_configuration_content",
-    "pluggable",
-    "property",
-    "provide",
-    "ref",
-    "refs",
-    "reflist",
-    "root",
-    "run_hook",
-    "slot",
-    "types",
-    "unset",
-    "walk_node_attributes",
-    "walk_nodes",
-]
-__api__ = [
-    "Configuration",
-    "ConfigurationAttribute",
-    "Distribution",
-    "compose_nodes",
-    "copy_configuration_template",
-    "format_configuration_content",
-    "get_config_attributes",
-    "get_config_path",
-    "make_config_diagram",
-    "parse_configuration_file",
-    "parse_configuration_content",
-    "refs",
-    "types",
-    "walk_node_attributes",
-    "walk_nodes",
-]
+"""
+bsb.config module
+
+Contains the dynamic attribute system; Use ``@bsb.config.root/node/dynamic/pluggable`` to
+decorate your classes and add class attributes using ``x =
+config.attr/dict/list/ref/reflist`` to populate your classes with powerful attributes.
+"""
+
+import builtins
+import functools
+import glob
+import itertools
+import os
+import sys
+import traceback
+import typing
+from shutil import copy2 as copy_file
+
+from .. import plugins
+from .._util import ichain
+from ..exceptions import ConfigTemplateNotFoundError, ParserError
+from . import refs, types
+from ._attrs import (
+    ConfigurationAttribute,
+    attr,
+    catch_all,
+    dict,
+    dynamic,
+    file,
+    list,
+    node,
+    pluggable,
+    property,
+    provide,
+    ref,
+    reflist,
+    root,
+    slot,
+    unset,
+)
+from ._distributions import Distribution
+from ._hooks import after, before, has_hook, on, run_hook
+from ._make import (
+    compose_nodes,
+    get_config_attributes,
+    walk_node_attributes,
+    walk_nodes,
+)
+from .parsers import get_configuration_parser, get_configuration_parser_classes
+
+if typing.TYPE_CHECKING:
+    from ._config import Configuration
+
+
+@functools.cache
+def __getattr__(name):
+    if name == "Configuration":
+        # Load the Configuration class on demand, not on import, to avoid circular
+        # dependencies.
+        from ._config import Configuration
+
+        return Configuration
+    else:
+        raise object.__getattribute__(sys.modules[__name__], name)
+
+
+def get_config_path():
+    import os
+
+    env_paths = os.environ.get("BSB_CONFIG_PATH", None)
+    if env_paths is None:
+        env_paths = ()
+    else:
+        env_paths = env_paths.split(":")
+    plugin_paths = plugins.discover("config.templates")
+    return [*itertools.chain((os.getcwd(),), env_paths, *plugin_paths.values())]
+
+
+def copy_configuration_template(template, output="network_configuration.json", path=None):
+    path = [
+        *map(
+            os.path.abspath,
+            itertools.chain(get_config_path(), path or ()),
+        )
+    ]
+    for d in path:
+        if files := glob.glob(os.path.join(d, template)):
+            break
+    else:
+        raise ConfigTemplateNotFoundError(
+            "'%template%' not found in config path %path%", template, path
+        )
+    copy_file(files[0], output)
+
+
+def format_configuration_content(parser_name: str, config: "Configuration", **kwargs):
+    """
+    Convert a configuration object to a string using the given parser.
+    """
+    return get_configuration_parser(parser_name, **kwargs).generate(
+        config.__tree__(), pretty=True
+    )
+
+
+def make_configuration_diagram(config):
+    dot = f'digraph "{config.name or "network"}" {{'
+    for c in config.cell_types.values():
+        dot += f'\n  {c.name}[label="{c.name}"]'
+    for name, conn in config.connectivity.items():
+        for pre in conn.presynaptic.cell_types:
+            for post in conn.postsynaptic.cell_types:
+                dot += f'\n  {pre.name} -> {post.name}[label="{name}"];'
+    dot += "\n}\n"
+    return dot
+
+
+def _try_parsers(content, classes, ext=None, path=None):  # pragma: nocover
+    if ext is not None:
+
+        def file_has_parser_ext(kv):
+            return ext not in getattr(kv[1], "data_extensions", ())
+
+        classes = builtins.dict(sorted(classes.items(), key=file_has_parser_ext))
+    exc = {}
+    for name, cls in classes.items():
+        try:
+            tree, meta = cls().parse(content, path=path)
+        except Exception as e:
+            if getattr(e, "_bsbparser_show_user", False):
+                raise e from None
+            exc[name] = e
+        else:
+            return (name, tree, meta)
+    msges = [
+        (
+            f"- Can't parse contents with '{n}':\n",
+            "".join(traceback.format_exception(type(e), e, e.__traceback__)),
+        )
+        for n, e in exc.items()
+    ]
+    if path:
+        msg = f"Could not parse '{path}'"
+    else:
+        msg = f"Could not parse content string ({len(content)} characters long)"
+    raise ParserError("\n".join(ichain(msges)) + f"\n{msg}")
+
+
+def _from_parsed(parser_name, tree, meta, file=None):
+    from ._config import Configuration
+
+    conf = Configuration(tree)
+    conf._parser = parser_name
+    conf._meta = meta
+    conf._file = file
+    return conf
+
+
+def parse_configuration_file(file, parser=None, path=None, **kwargs):
+    if hasattr(file, "read"):
+        data = file.read()
+        try:
+            path = str(path) or os.fspath(file)
+        except TypeError:
+            pass
+    else:
+        file = os.path.abspath(file)
+        path = path or file
+        with open(file, "r") as f:
+            data = f.read()
+    return parse_configuration_content(data, parser, path, **kwargs)
+
+
+def parse_configuration_content(content, parser=None, path=None, **kwargs):
+    if parser is None:
+        parser_classes = get_configuration_parser_classes()
+        ext = path.split(".")[-1] if path is not None else None
+        parser_name, tree, meta = _try_parsers(content, parser_classes, ext, path=path)
+    elif isinstance(parser, str):
+        parser_name = parser
+        parser = get_configuration_parser(parser_name, **kwargs)
+        tree, meta = parser.parse(content, path=path)
+    else:
+        parser_name = parser.__name__
+        tree, meta = parser.parse(content, path=path)
+    return _from_parsed(parser_name, tree, meta, path)
+
+
+# Static public API
+__all__ = [
+    "Configuration",
+    "ConfigurationAttribute",
+    "Distribution",
+    "after",
+    "attr",
+    "before",
+    "catch_all",
+    "compose_nodes",
+    "copy_configuration_template",
+    "dict",
+    "dynamic",
+    "file",
+    "format_configuration_content",
+    "get_config_attributes",
+    "get_config_path",
+    "has_hook",
+    "list",
+    "make_configuration_diagram",
+    "node",
+    "on",
+    "parse_configuration_file",
+    "parse_configuration_content",
+    "pluggable",
+    "property",
+    "provide",
+    "ref",
+    "refs",
+    "reflist",
+    "root",
+    "run_hook",
+    "slot",
+    "types",
+    "unset",
+    "walk_node_attributes",
+    "walk_nodes",
+]
+__api__ = [
+    "Configuration",
+    "ConfigurationAttribute",
+    "Distribution",
+    "compose_nodes",
+    "copy_configuration_template",
+    "format_configuration_content",
+    "get_config_attributes",
+    "get_config_path",
+    "make_config_diagram",
+    "parse_configuration_file",
+    "parse_configuration_content",
+    "refs",
+    "types",
+    "walk_node_attributes",
+    "walk_nodes",
+]
```

### Comparing `bsb_core-4.0.1/bsb/config/_attrs.py` & `bsb_core-4.1.0/bsb/config/_attrs.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1147 +1,1164 @@
-"""
-    An attrs-inspired class annotation system, but my A stands for amateuristic.
-"""
-
-import builtins
-
-import errr
-
-from ..exceptions import (
-    BootError,
-    CastError,
-    CfgReferenceError,
-    NoReferenceAttributeSignal,
-    RequirementError,
-)
-from ..services import MPI
-from ._compile import _wrap_reserved
-from ._hooks import run_hook
-from ._make import (
-    MISSING,
-    _resolve_references,
-    compile_class,
-    compile_isc,
-    compile_new,
-    compile_postnew,
-    make_copyable,
-    make_dictable,
-    make_get_node_name,
-    make_tree,
-    walk_nodes,
-    wrap_root_postnew,
-)
-
-
-def root(root_cls):
-    """
-    Decorate a class as a configuration root node.
-    """
-    root_cls.attr_name = root_cls.node_name = r"{root}"
-    root_cls._config_isroot = True
-    return node(root_cls, root=True)
-
-
-def node(node_cls, root=False, dynamic=False, pluggable=False):
-    """
-    Decorate a class as a configuration node.
-    """
-    # Recreate the class to set its metaclass a posteriori
-    node_cls = compile_class(node_cls)
-    node_cls._config_unset = []
-    # Inherit the parent's attributes, if any exist on the class already
-    attrs = getattr(node_cls, "_config_attrs", {}).copy()
-    for k, v in builtins.dict(node_cls.__dict__).items():
-        # Add our attributes
-        if isinstance(v, ConfigurationAttribute):
-            if v.unset:
-                attrs.pop(k, None)
-                delattr(node_cls, k)
-                # Keep track of what this class wants to unset, in case of MRO traversal.
-                node_cls._config_unset.append(k)
-            else:
-                attrs[k] = v
-    node_cls._config_attrs = attrs
-    node_cls.__post_new__ = compile_postnew(node_cls)
-    node_cls._config_isroot = root
-    if root:
-        node_cls.__post_new__ = wrap_root_postnew(node_cls.__post_new__)
-        node_cls._config_isbooted = False
-    node_cls.__new__ = compile_new(
-        node_cls, dynamic=dynamic, pluggable=pluggable, root=root
-    )
-    if dynamic:
-        node_cls.__init_subclass__ = compile_isc(node_cls, dynamic)
-    make_get_node_name(node_cls, root=root)
-    make_tree(node_cls)
-    make_dictable(node_cls)
-    make_copyable(node_cls)
-
-    return node_cls
-
-
-def dynamic(
-    node_cls=None,
-    attr_name="cls",
-    classmap=None,
-    auto_classmap=False,
-    classmap_entry=None,
-    **kwargs,
-):
-    """
-    Decorate a class to be castable to a dynamically configurable class using
-    a class configuration attribute.
-
-    *Example*: Register a required string attribute ``class`` (this is the default):
-
-    .. code-block:: python
-
-        @dynamic
-        class Example:
-            pass
-
-    *Example*: Register a string attribute ``type`` with a default value
-    'pkg.DefaultClass' as dynamic attribute:
-
-    .. code-block:: python
-
-        @dynamic(attr_name='type', required=False, default='pkg.DefaultClass')
-        class Example:
-            pass
-
-    :param attr_name: Name under which to register the class attribute in the node.
-    :type attr_name: str
-    :param kwargs: All keyword arguments are passed to the constructor of the
-      :func:`attribute <.config.attr>`.
-    """
-    if "required" not in kwargs and "default" not in kwargs:
-        kwargs["required"] = True
-    if "type" not in kwargs:
-        kwargs["type"] = str
-    class_attr = ConfigurationAttribute(**kwargs)
-    dynamic_config = DynamicNodeConfiguration(classmap, auto_classmap, classmap_entry)
-    if node_cls is None:
-        # If node_cls is None, it means that no positional argument was given, which most
-        # likely means that the @dynamic(...) syntax was used instead of the @dynamic.
-        # This means we have to return an inner decorator instead of the decorated class
-        def decorator(node_cls):
-            return _dynamic(node_cls, class_attr, attr_name, dynamic_config)
-
-        return decorator
-    # Regular @dynamic syntax used, return decorated class
-    return _dynamic(node_cls, class_attr, attr_name, dynamic_config)
-
-
-class DynamicNodeConfiguration:
-    def __init__(self, classmap=None, auto_classmap=False, entry=None):
-        self.classmap = classmap
-        self.auto_classmap = auto_classmap
-        self.entry = entry
-
-
-def _dynamic(node_cls, class_attr, attr_name, config):
-    # Set the dynamic attribute
-    setattr(node_cls, attr_name, class_attr)
-    node_cls._config_dynamic_attr = attr_name
-    # Other than that compile the dynamic class like a regular node class
-    node_cls = node(node_cls, dynamic=config)
-
-    if config.auto_classmap or config.classmap:
-        node_cls._config_dynamic_classmap = config.classmap or {}
-    # This adds the parent class to its own classmap, which for subclasses happens in init
-    # subclass
-    if config.entry is not None:
-        if not hasattr(node_cls, "_config_dynamic_classmap"):
-            raise ValueError(
-                f"Calling `@config.dynamic` with `entry='{config.entry}'`"
-                + " requires `classmap` or `auto_classmap` to be set as well"
-                + f" on '{node_cls.__name__}'."
-            )
-        node_cls._config_dynamic_classmap[config.entry] = node_cls
-    # Mark the class as its own dynamic root, (grand)child classes will all need to
-    # inherit from this as an interface contract.
-    node_cls._config_dynamic_root = node_cls
-    return node_cls
-
-
-def pluggable(key, plugin_name=None):
-    """
-    Create a node whose configuration is defined by a plugin.
-
-    *Example*: If you want to use the :guilabel:`attr` to chose from all the installed
-    `dbbs_scaffold.my_plugin` plugins:
-
-    .. code-block:: python
-
-        @pluggable('attr', 'my_plugin')
-        class PluginNode:
-            pass
-
-    This will then read :guilabel:`attr`, load the plugin and configure the node from
-    the node class specified by the plugin.
-
-    :param plugin_name: The name of the category of the plugin endpoint
-    :type plugin_name: str
-    """
-
-    def inner_decorator(node_cls):
-        node_cls._config_plugin_name = plugin_name
-        node_cls._config_plugin_key = key
-        class_attr = ConfigurationAttribute(type=str, required=True)
-        setattr(node_cls, key, class_attr)
-        return node(node_cls, pluggable=True)
-
-    return inner_decorator
-
-
-def attr(**kwargs):
-    """
-    Create a configuration attribute.
-
-    Only works when used inside a class decorated with the :func:`node
-    <.config.node>`, :func:`dynamic <.config.dynamic>`,  :func:`root <.config.root>`
-    or  :func:`pluggable <.config.pluggable>` decorators.
-
-    :param type: Type of the attribute's value.
-    :type type: Callable
-    :param required: Should an error be thrown if the attribute is not present?
-    :type required: bool
-    :param default: Default value.
-    :type default: Any
-    :param call_default: Should the default value be used (False) or called (True).
-      Use this to prevent mutable default values.
-    :type call_default: bool
-    :param key: If set, the key of the parent is stored on this attribute.
-    """
-    return ConfigurationAttribute(**kwargs)
-
-
-def ref(reference, **kwargs):
-    """
-    Create a configuration reference.
-
-    Configuration references are attributes that transform their value into the value
-    of another node or value in the document::
-
-      {
-        "keys": {
-            "a": 3,
-            "b": 5
-        },
-        "simple_ref": "a"
-      }
-
-    With ``simple_ref = config.ref(lambda root, here: here["keys"])`` the value ``a``
-    will be looked up in the configuration object (after all values have been cast) at
-    the location specified by the callable first argument.
-    """
-    return ConfigurationReferenceAttribute(reference, **kwargs)
-
-
-def reflist(reference, **kwargs):
-    """
-    Create a configuration reference list.
-    """
-    if "default" not in kwargs:
-        kwargs["default"] = builtins.list
-        kwargs["call_default"] = True
-    return ConfigurationReferenceListAttribute(reference, **kwargs)
-
-
-def slot(**kwargs):
-    """
-    Create an attribute slot that is required to be overriden by child or plugin
-    classes.
-    """
-    return ConfigurationAttributeSlot(**kwargs)
-
-
-def property(val=None, /, type=None, **kwargs):
-    """
-    Create a configuration property attribute. You may provide a value or a callable. Call
-    `setter` on the return value as you would with a regular property.
-    """
-    if type is None:
-        type = lambda v: v
-
-    def decorator(val):
-        prop = val if callable(val) else lambda s: val
-        return ConfigurationProperty(prop, type=type, **kwargs)
-
-    if val is None:
-        return decorator
-    else:
-        return decorator(val)
-
-
-def provide(value):
-    """
-    Provide a value for a parent class' attribute. Can be a value or a callable, a
-    readonly configuration property will be created from it either way.
-    """
-    prop = property(value)
-
-    def provided(self, instance, value):
-        raise AttributeError(f"Can't set attribute, class provides the value '{value}'.")
-
-    # Create a callable object that invokes `provided` when called, and whose `bool()`
-    # returns `False`. Later in `_is_settable_attr`, we use this to trick the short
-    # circuiting logic, so that this setter doesn't make the internal logic set and error
-    # out on this attr.
-    prop.setter(
-        type("provision", (), {"__call__": provided, "__bool__": lambda s: False})()
-    )
-
-    return prop
-
-
-def list(**kwargs):
-    """
-    Create a configuration attribute that holds a list of configuration values.
-    Best used only for configuration nodes. Use an :func:`attr` in combination with a
-    :func:`types.list <.config.types.list>` type for simple values.
-    """
-    return ConfigurationListAttribute(**kwargs)
-
-
-def dict(**kwargs):
-    """
-    Create a configuration attribute that holds a key value pairs of configuration
-    values. Best used only for configuration nodes. Use an :func:`attr` in combination
-    with a :func:`types.dict <.config.types.dict>` type for simple values.
-    """
-    return ConfigurationDictAttribute(**kwargs)
-
-
-def catch_all(**kwargs):
-    """
-    Catches any unknown key with a value that can be cast to the given type and
-    collects them under the attribute name.
-    """
-    return ConfigurationAttributeCatcher(**kwargs)
-
-
-def unset():
-    """
-    Override and unset an inherited configuration attribute.
-    """
-    return ConfigurationAttribute(unset=True)
-
-
-def file(**kwargs):
-    """
-    Create a file dependency attribute.
-    """
-    from ..storage._files import FileDependencyNode
-
-    kwargs.setdefault("type", FileDependencyNode)
-    return attr(**kwargs)
-
-
-def _setattr(instance, name, value):
-    instance.__dict__["_" + name] = value
-
-
-def _getattr(instance, name):
-    try:
-        return instance.__dict__["_" + name]
-    except KeyError as e:
-        instance.__getattribute__(e.args[0])
-
-
-def _hasattr(instance, name):
-    return "_" + name in instance.__dict__
-
-
-def _get_root(obj):
-    parent = obj
-    while hasattr(parent, "_config_parent") and parent._config_parent is not None:
-        parent = parent._config_parent
-    return parent
-
-
-def _strict_root(obj):
-    root = _get_root(obj)
-    return root if getattr(root, "_config_isroot", False) else None
-
-
-def _booted_root(obj):
-    root = _strict_root(obj)
-    return root if root and root._config_isfinished and root._config_isbooted else None
-
-
-def _is_booted(obj):
-    return obj and obj._config_isbooted
-
-
-def _root_is_booted(obj):
-    root = _strict_root(obj)
-    return _is_booted(root)
-
-
-def _boot_nodes(top_node, scaffold):
-    for node in walk_nodes(top_node):
-        node.scaffold = scaffold
-        # Boot attributes
-        for attr in getattr(node, "_config_attrs", {}).values():
-            booted = {None}
-            for cls in type(node).__mro__:
-                cls_attr = getattr(cls, attr.attr_name, None)
-                if (boot := getattr(cls_attr, "__boot__", None)) and boot not in booted:
-                    boot(node, scaffold)
-                    booted.add(boot)
-        # Boot node hook
-        try:
-            run_hook(node, "boot")
-        except Exception as e:
-            errr.wrap(BootError, e, prepend=f"Failed to boot {node}:")
-    # fixme: why is this here? Will deadlock in case of BootError on specific node only.
-    MPI.barrier()
-
-
-def _unset_nodes(top_node):
-    for node in walk_nodes(top_node):
-        try:
-            del node.scaffold
-        except Exception:
-            pass
-        node._config_parent = None
-        node._config_key = None
-        if hasattr(node, "_config_index"):
-            node._config_index = None
-        run_hook(node, "unboot")
-
-
-class ConfigurationAttribute:
-    """
-    Base implementation of all the different configuration attributes. Call the factory
-    function :func:`.attr` instead.
-    """
-
-    def __init__(
-        self,
-        type=None,
-        default=None,
-        call_default=None,
-        required=False,
-        key=False,
-        unset=False,
-        hint=MISSING,
-    ):
-        if not callable(required):
-            self.required = lambda s: required
-        else:
-            self.required = required
-        self.key = key
-        self.default = default
-        self.call_default = call_default
-        self.type = self._set_type(type, key)
-        self.unset = unset
-        self.hint = hint
-
-    def __set_name__(self, owner, name):
-        self.attr_name = name
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        return _getattr(instance, self.attr_name)
-
-    def fset(self, instance, value):
-        return _setattr(instance, self.attr_name, value)
-
-    def __set__(self, instance, value):
-        if _hasattr(instance, self.attr_name):
-            ex_value = _getattr(instance, self.attr_name)
-            _unset_nodes(ex_value)
-        if value is None:
-            # Don't try to cast None to a value of the attribute type.
-            return self.fset(instance, None)
-        try:
-            value = self.type(value, _parent=instance, _key=self.attr_name)
-        except ValueError:
-            # This value error should only arise when users are manually setting
-            # attributes in an already bootstrapped config tree.
-            raise CastError(
-                f"'{value}' is not convertible to {self.type.__name__},"
-                f" for attribute '{self.attr_name}' of {instance}."
-            ) from None
-        except (RequirementError, CastError) as e:
-            if not hasattr(e, "node") or not e.node:
-                e.node, e.attr = instance, self.attr_name
-            raise
-        except Exception as e:
-            raise CastError(
-                f"Couldn't cast '{value}' into {self.type.__name__}: {e}",
-                instance,
-                self.attr_name,
-            ) from e
-        self.flag_dirty(instance)
-        # The value was cast to its intented type and the new value can be set.
-        self.fset(instance, value)
-        root = _strict_root(instance)
-        if _is_booted(root):
-            _boot_nodes(value, root.scaffold)
-
-    def _set_type(self, type, key):
-        self._config_type = type
-        # Determine type of the attribute
-        if not type and self.default is not None:
-            if self.should_call_default():
-                t = builtins.type(self.default())
-            else:
-                t = builtins.type(self.default)
-        else:
-            from . import types
-
-            t = type or (key and types.key()) or types.str()
-        # This call wraps the type handler so that it accepts all reserved keyword args
-        # like `_parent` and `_key`
-        t = _wrap_reserved(t)
-        return t
-
-    def get_type(self):
-        return self._config_type
-
-    def get_hint(self):
-        if self.hint is not MISSING:
-            return self.hint
-        if hasattr(self.type, "__hint__"):
-            return self.type.__hint__()
-        return MISSING
-
-    def get_node_name(self, instance):
-        return instance.get_node_name() + "." + self.attr_name
-
-    def is_node_type(self):
-        return hasattr(self._config_type, "_config_attrs")
-
-    def tree(self, instance):
-        val = _getattr(instance, self.attr_name)
-        return self.tree_of(val)
-
-    def tree_of(self, value):
-        # Allow subnodes and other class values to convert themselves to their tree
-        # representation
-        if hasattr(value, "__tree__"):
-            value = value.__tree__()
-        # Check if the type handler specifies any inversion function to convert tree
-        # values back to how they were found in the document.
-        if hasattr(self.type, "__inv__") and value is not None:
-            value = self.type.__inv__(value)
-        return value
-
-    def flag_dirty(self, instance):
-        instance._config_state[self.attr_name] = False
-        if self.attr_name not in instance._config_attr_order:
-            instance._config_attr_order.append(self.attr_name)
-
-    def is_dirty(self, instance):
-        return not instance._config_state.get(self.attr_name, True)
-
-    def flag_pristine(self, instance):
-        instance._config_state[self.attr_name] = True
-
-    def get_default(self):
-        return self.default() if self.should_call_default() else self.default
-
-    def should_call_default(self):
-        cdf = self.call_default
-        return cdf or (cdf is None and callable(self.default))
-
-
-class cfglist(builtins.list):
-    """
-    Extension of the builtin list to manipulate lists of configuration nodes.
-    """
-
-    def get_node_name(self):
-        return self._config_parent.get_node_name() + "." + self._config_attr_name
-
-    def append(self, item):
-        item = self._preset(len(self), item)
-        super().append(item)
-        self._postset((item,))
-        return self[-1]
-
-    def insert(self, index, item):
-        item = self._preset(index, item)
-        super().insert(index, item)
-        self._postset((item,))
-        self._reindex(index)
-
-    def pop(self, index=-1):
-        ex_item = super().pop(index)
-        _unset_nodes(ex_item)
-        self._reindex(index)
-        return ex_item
-
-    def clear(self):
-        for node in self:
-            _unset_nodes(node)
-        super().clear()
-
-    def sort(self, **kwargs):
-        super().sort(**kwargs)
-        self._reindex(0)
-
-    def reverse(self):
-        super().reverse()
-        self._reindex(0)
-
-    def extend(self, items):
-        items = self._fromiter(len(self), items)
-        super().extend(items)
-        self._postset(items)
-
-    def __setitem__(self, index, item):
-        if isinstance(index, int):
-            ex_items = [self[index]]
-            item = self._preset(index, item)
-            items = [item]
-            reindex_from = None
-        else:
-            ex_items = self[index]
-            reindex_from = index.indices(len(self))[0]
-            items = item = self._fromiter(reindex_from, item)
-        for ex_item in ex_items:
-            _unset_nodes(ex_item)
-        # Don't be fooled, item can be a single value or a list, depending on the index.
-        super().__setitem__(index, item)
-        if reindex_from is not None:
-            self._reindex(reindex_from)
-        self._postset(items)
-
-    def _reindex(self, start):
-        for i in range(start, len(self)):
-            self[i]._config_key = i
-            self[i]._config_index = i
-
-    def _fromiter(self, start, items):
-        return tuple(self._preset(start + i, item) for i, item in enumerate(items))
-
-    def _preset(self, index, item):
-        try:
-            item = self._elem_type(item, _parent=self, _key=index)
-            try:
-                item._config_index = index
-            except Exception as e:
-                pass
-            return item
-        except (RequirementError, CastError) as e:
-            e.args = (
-                f"Couldn't cast element {index} from '{item}'"
-                + f" into a {self._elem_type.__name__}. "
-                + e.msg,
-                *e.args,
-            )
-            if not e.node:
-                e.node, e.attr = self, index
-            raise
-        except Exception as e:
-            raise CastError(
-                f"Couldn't cast element {index} from '{item}'"
-                + f" into a {self._elem_type.__name__}: {e}"
-            )
-
-    def _postset(self, items):
-        root = _strict_root(self)
-        if root is not None:
-            for item in items:
-                _resolve_references(root, item)
-        if _is_booted(root):
-            for item in items:
-                _boot_nodes(item, root.scaffold)
-
-    @builtins.property
-    def _config_attr_name(self):
-        return self._config_attr.attr_name
-
-
-class ConfigurationListAttribute(ConfigurationAttribute):
-    def __init__(self, *args, size=None, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.size = size
-
-    def __set__(self, instance, value, _key=None):
-        _setattr(instance, self.attr_name, self.fill(value, _parent=instance))
-
-    def __populate__(self, instance, value, unique_list=False):
-        cfglist = _getattr(instance, self.attr_name)
-        if not unique_list or value not in cfglist:
-            builtins.list.append(cfglist, value)
-
-    def fill(self, value, _parent, _key=None):
-        _cfglist = cfglist()
-        _cfglist._config_parent = _parent
-        _cfglist._config_attr = self
-        _cfglist._elem_type = self.child_type
-        if isinstance(value, builtins.dict):
-            raise CastError(f"Dictionary `{value}` given where list is expected.")
-        _cfglist.extend(value or builtins.list())
-        if self.size is not None and len(_cfglist) != self.size:
-            raise CastError(
-                f"Couldn't cast {value} into a {self.size}-element list,"
-                + f" obtained {len(_cfglist)} elements"
-            )
-        return _cfglist
-
-    def _set_type(self, type, key=None):
-        self.child_type = super()._set_type(type, key=False)
-        return self.fill
-
-    def tree(self, instance):
-        val = _getattr(instance, self.attr_name)
-        return [self.tree_of(e) for e in val]
-
-    def get_hint(self):
-        if self.hint is not MISSING:
-            return self.hint
-        if hasattr(self.child_type, "__hint__"):
-            return [self.child_type.__hint__(), self.child_type.__hint__()]
-        return MISSING
-
-
-class cfgdict(builtins.dict):
-    """
-    Extension of the builtin dictionary to manipulate dicts of configuration nodes.
-    """
-
-    def __getattr__(self, name):
-        try:
-            return self[name]
-        except KeyError:
-            raise AttributeError(
-                self.get_node_name() + " object has no attribute '{}'".format(name)
-            )
-
-    def __setitem__(self, key, value):
-        if key in self:
-            _unset_nodes(self[key])
-        try:
-            value = self._elem_type(value, _parent=self, _key=key)
-        except (RequirementError, CastError) as e:
-            if not (hasattr(e, "node") and e.node):
-                e.node, e.attr = self, key
-            raise
-        except Exception:
-            import traceback
-
-            raise CastError(
-                "Couldn't cast {}.{} from '{}' into a {}".format(
-                    self.get_node_name(), key, value, self._elem_type.__name__
-                )
-                + "\n"
-                + traceback.format_exc()
-            )
-        else:
-            super().__setitem__(key, value)
-            root = _strict_root(value)
-            if root is not None:
-                _resolve_references(root, value)
-            if _is_booted(root):
-                _boot_nodes(value, root.scaffold)
-
-    def add(self, key, *args, **kwargs):
-        if key in self:
-            raise KeyError(
-                f"{self.get_node_name()} already contains '{key}'."
-                + " Use `node[key] = value` if you want to overwrite it."
-            )
-        self[key] = value = self._elem_type(*args, _parent=self, _key=key, **kwargs)
-        return value
-
-    def clear(self):
-        for node in self.values():
-            _unset_nodes(node)
-        super().clear()
-
-    def pop(self, key):
-        item = super().pop(key)
-        _unset_nodes(item)
-        return item
-
-    def popitem(self):
-        key, value = super().popitem()
-        _unset_nodes(value)
-        return key, value
-
-    def setdefault(self, key, value):
-        if key in self:
-            return self[key]
-        else:
-            self[key] = value
-            return self[key]
-
-    def update(self, other):
-        for ckey, value in other.items():
-            self[ckey] = value
-
-    def __ior__(self, other):
-        ex_values = tuple(self.values())
-        try:
-            merge_f = super().__ior__
-        except AttributeError:
-            # Patch for 3.8
-            merge_f = super().update
-        merge_f(other)
-        new_values = tuple(self.values())
-        for removed_node in (e for e in ex_values if e not in new_values):
-            _unset_nodes(removed_node)
-        for added_node in (a for a in new_values if a not in ex_values):
-            _boot_nodes(added_node, self.scaffold)
-        return self
-
-    def copy(self):
-        return cfgdictcopy(self)
-
-    @builtins.property
-    def _config_attr_name(self):
-        return self._config_attr.attr_name
-
-    def get_node_name(self):
-        return self._config_parent.get_node_name() + "." + self._config_attr_name
-
-
-class cfgdictcopy(builtins.dict):
-    def __init__(self, other):
-        super().__init__(other)
-        self._elem_type = other._elem_type
-        self._copied_from = other
-
-    @builtins.property
-    def _config_attr_name(self):
-        return self._copied_from._config_attr_name
-
-    def get_node_name(self):
-        return self._copied_from.get_node_name()
-
-
-class ConfigurationDictAttribute(ConfigurationAttribute):
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-
-    def __set__(self, instance, value, _key=None):
-        _setattr(
-            instance,
-            self.attr_name,
-            self.fill(value, _parent=instance, _key=_key or self.attr_name),
-        )
-
-    def fill(self, value, _parent, _key=None):
-        _cfgdict = cfgdict()
-        _cfgdict._config_parent = _parent
-        _cfgdict._config_key = _key
-        _cfgdict._config_attr = self
-        _cfgdict._elem_type = self.child_type
-        _cfgdict.update(value or builtins.dict())
-        return _cfgdict
-
-    def _set_type(self, type, key=None):
-        self.child_type = super()._set_type(type, key=False)
-        return self.fill
-
-    def tree(self, instance):
-        val = _getattr(instance, self.attr_name).items()
-        return {k: self.tree_of(v) for k, v in val}
-
-    def get_hint(self):
-        if self.hint is not MISSING:
-            return self.hint
-        if hasattr(self.child_type, "__hint__"):
-            return {
-                "key1": self.child_type.__hint__(),
-                "key2": self.child_type.__hint__(),
-            }
-        return MISSING
-
-
-class ConfigurationReferenceAttribute(ConfigurationAttribute):
-    def __init__(
-        self,
-        reference,
-        key=None,
-        ref_type=None,
-        populate=None,
-        backref=None,
-        pop_unique=True,
-        **kwargs,
-    ):
-        self.ref_lambda = reference
-        self.ref_key = key
-        self.ref_type = ref_type
-        self.populate = populate
-        self.backref = backref
-        self.pop_unique = pop_unique
-        # No need to cast to any types: the reference we fetch will already have been cast
-        if "type" in kwargs:  # pragma: nocover
-            del kwargs["type"]
-        super().__init__(**kwargs)
-
-    def get_ref_key(self):
-        return self.ref_key or (self.attr_name + "_reference")
-
-    def __set__(self, instance, value, key=None):
-        if self.is_reference_value(value):
-            _setattr(instance, self.attr_name, value)
-        else:
-            setattr(instance, self.get_ref_key(), value)
-            if self.should_resolve_on_set(instance):
-                if hasattr(instance, "_config_root"):  # pragma: nocover
-                    raise CfgReferenceError(
-                        "Can't autoresolve references without a config root."
-                    )
-                _setattr(
-                    instance,
-                    self.attr_name,
-                    self.__ref__(instance, instance._config_root),
-                )
-
-    def is_reference_value(self, value):
-        if value is None:
-            return True
-        if self.ref_type is not None:
-            return isinstance(value, self.ref_type)
-        elif hasattr(self.ref_lambda, "is_ref"):
-            return self.ref_lambda.is_ref(value)
-        else:
-            return not isinstance(value, str)
-
-    def should_resolve_on_set(self, instance):
-        return (
-            hasattr(instance, "_config_resolve_on_set")
-            and instance._config_resolve_on_set
-        )
-
-    def __ref__(self, instance, root):
-        try:
-            remote, remote_key = self._prepare_self(instance, root)
-        except NoReferenceAttributeSignal:
-            return None
-        return self.resolve_reference(instance, remote, remote_key)
-
-    def _prepare_self(self, instance, root):
-        instance._config_root = root
-        instance._config_resolve_on_set = True
-        remote = self.ref_lambda(root, instance)
-        local_attr = self.get_ref_key()
-        if not hasattr(instance, local_attr):
-            raise NoReferenceAttributeSignal()
-        return remote, getattr(instance, local_attr)
-
-    def resolve_reference(self, instance, remote, key):
-        if key not in remote:
-            raise CfgReferenceError(
-                "Reference '{}' of {} does not exist in {}".format(
-                    key,
-                    self.get_node_name(instance),
-                    remote.get_node_name(),
-                )
-            )
-        value = remote[key]
-        if self.populate:
-            self.populate_reference(instance, value)
-        if self.backref:
-            self.back_reference(instance, value)
-
-        return value
-
-    def populate_reference(self, instance, reference):
-        # Remote descriptors can ask to handle populating itself by implementing a
-        # __populate__ method. Here we check if the method exists and if so defer to it.
-        if (pop_attr := getattr(reference.__class__, self.populate, None)) and (
-            pop_func := getattr(pop_attr, "__populate__", None)
-        ):
-            pop_func(reference, instance, unique_list=self.pop_unique)
-        elif (population := getattr(reference, self.populate, None)) is not None:
-            if not self.pop_unique or instance not in population:
-                population.append(instance)
-        else:
-            setattr(reference, self.populate, [instance])
-
-    def back_reference(self, instance, reference):
-        if (ref_attr := getattr(reference.__class__, self.backref, None)) and (
-            ref_func := getattr(ref_attr, "__backref__", None)
-        ):
-            ref_func(reference, instance)
-        else:
-            setattr(reference, self.backref, instance)
-
-    def tree(self, instance):
-        val = getattr(instance, self.get_ref_key(), None)
-        if self.is_reference_value(val) and hasattr(val, "_config_key"):
-            val = val._config_key
-        return val
-
-
-class ConfigurationReferenceListAttribute(ConfigurationReferenceAttribute):
-    def __set__(self, instance, value, key=None):
-        if value is None:
-            setattr(instance, self.get_ref_key(), [])
-            _setattr(instance, self.attr_name, [])
-            return
-        try:
-            remote_keys = builtins.list(iter(value))
-        except TypeError:
-            raise CfgReferenceError(
-                "Reference list '{}' of {} is not iterable.".format(
-                    value, self.get_node_name(instance)
-                )
-            )
-        # Store the referring values to the references key.
-        setattr(instance, self.get_ref_key(), remote_keys)
-        if self.should_resolve_on_set(instance):
-            remote = self.ref_lambda(instance._config_root, instance)
-            refs = self.resolve_reference_list(instance, remote, remote_keys)
-            _setattr(instance, self.attr_name, refs)
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        if self.should_resolve_on_set(instance):
-            return super().__get__(instance, owner)
-        else:
-            return getattr(instance, self.get_ref_key())
-
-    def get_ref_key(self):
-        return self.ref_key or (self.attr_name + "_references")
-
-    def __ref__(self, instance, root):
-        try:
-            remote, remote_keys = self._prepare_self(instance, root)
-        except NoReferenceAttributeSignal:  # pragma: nocover
-            return None
-        return self.resolve_reference_list(instance, remote, remote_keys)
-
-    def resolve_reference_list(self, instance, remote, remote_keys):
-        refs = []
-        for remote_key in remote_keys:
-            if not self.is_reference_value(remote_key):
-                reference = self.resolve_reference(instance, remote, remote_key)
-            else:
-                reference = remote_key
-                # Usually resolve_reference also populates, but since we have our ref
-                # already we skip it and should call populate_reference ourselves.
-                if self.populate:
-                    self.populate_reference(instance, reference)
-            refs.append(reference)
-        return refs
-
-    def __populate__(self, instance, value, unique_list=False):
-        has_refs = hasattr(instance, self.get_ref_key())
-        has_pop = hasattr(instance, self.attr_name)
-        if has_pop:
-            population = getattr(instance, self.attr_name)
-        if has_refs:
-            references = getattr(instance, self.get_ref_key())
-        is_new = (not has_pop or value not in population) and (
-            not has_refs or value not in references
-        )
-        should_pop = has_pop and (not unique_list or is_new)
-        should_ref = should_pop or not has_pop
-        if should_pop:
-            population.append(value)
-        if should_ref:
-            if not has_refs:
-                setattr(instance, self.get_ref_key(), [value])
-            else:
-                references.append(value)
-
-    def tree(self, instance):
-        val = getattr(instance, self.get_ref_key(), [])
-        val = [v._config_key if self._tree_should_unreference(v) else v for v in val]
-        return val
-
-    def _tree_should_unreference(self, value):
-        return self.is_reference_value(value) and hasattr(value, "_config_key")
-
-
-class ConfigurationAttributeSlot(ConfigurationAttribute):
-    def __set__(self, instance, value):  # pragma: nocover
-        raise NotImplementedError(
-            f"Configuration slot '{self.attr_name}' of {instance.get_node_name()} is"
-            f" empty. The {instance.__class__._bsb_entry_point.module_name} plugin"
-            f" provided by '{instance.__class__._bsb_entry_point.dist}' should fill the"
-            " slot with a configuration attribute."
-        )
-
-
-class ConfigurationProperty(ConfigurationAttribute):
-    def __init__(self, fget, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.fget = fget
-        self.fset = None
-
-    def setter(self, f):
-        self.fset = f
-        return self
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        return self.fget(instance)
-
-    def __set__(self, instance, value):
-        if self.fset is None:
-            e = AttributeError(f"Can't set attribute '{self.attr_name}'")
-            e.node = self
-            raise e
-        else:
-            return super().__set__(instance, value)
-
-
-def _collect_kv(n, d, k, v):
-    d[k] = v
-
-
-class ConfigurationAttributeCatcher(ConfigurationAttribute):
-    def __init__(
-        self,
-        *args,
-        type=str,
-        initial=builtins.dict,
-        catch=_collect_kv,
-        contains=None,
-        tree_cb=None,
-        **kwargs,
-    ):
-        super().__init__(*args, type=type, default=initial, call_default=True, **kwargs)
-        self.catch_callback = catch
-        if contains is not None:
-            self.contains = contains
-        if tree_cb is not None:
-            self.tree_callback = tree_cb
-
-    def __set__(self, instance, value):
-        _setattr(instance, self.attr_name, value)
-
-    def get_caught(self, instance):
-        if not hasattr(instance, f"_{self.attr_name}_caught"):
-            setattr(instance, f"_{self.attr_name}_caught", {})
-        return getattr(instance, f"_{self.attr_name}_caught")
-
-    def __catch__(self, node, key, value):
-        # Try to cast to our type, if it fails it will be caught by whoever is asking us
-        # to catch this and we don't catch this value.
-        cast = self.type(value, _parent=node, _key=key)
-        # If succesfully cast, catch this value by executing our catch callback.
-        self.catch_callback(node, _getattr(node, self.attr_name), key, cast)
-        self.get_caught(node)[key] = cast
-
-    def tree(self, instance):
-        # The default attr catcher collects what it catches in a dict. When we want to
-        # build the config tree again these values should be placed back in their
-        # original keys. We don't want to store our caught values in the config file. To
-        # do so we use the `tree_callback` instead.
-        return None
-
-    def contains(self, instance, key):
-        return key in self.get_caught(instance)
-
-    def tree_callback(self, instance, key):
-        # When building the config tree the values that were caught can't be found in the
-        # attrs and the tree builder will check all catch-attr's `contains` methods and
-        # calls the right tree_callback to fetch the value.
-        value = _getattr(instance, self.attr_name)[key]
-        if hasattr(value, "__tree__"):
-            value = value.__tree__()
-        return value
+"""
+    An attrs-inspired class annotation system, but my A stands for amateuristic.
+"""
+
+import builtins
+from functools import wraps
+
+import errr
+
+from ..exceptions import (
+    BootError,
+    CastError,
+    CfgReferenceError,
+    NoReferenceAttributeSignal,
+    RequirementError,
+)
+from ..services import MPI
+from ._compile import _wrap_reserved
+from ._hooks import run_hook
+from ._make import (
+    MISSING,
+    _resolve_references,
+    compile_class,
+    compile_isc,
+    compile_new,
+    compile_postnew,
+    make_copyable,
+    make_dictable,
+    make_get_node_name,
+    make_tree,
+    walk_nodes,
+    wrap_root_postnew,
+)
+
+
+def root(root_cls):
+    """
+    Decorate a class as a configuration root node.
+    """
+    root_cls.attr_name = root_cls.node_name = r"{root}"
+    root_cls._config_isroot = True
+    return node(root_cls, root=True)
+
+
+def node(node_cls, root=False, dynamic=False, pluggable=False):
+    """
+    Decorate a class as a configuration node.
+    """
+    # Recreate the class to set its metaclass a posteriori
+    node_cls = compile_class(node_cls)
+    node_cls._config_unset = []
+    # Inherit the parent's attributes, if any exist on the class already
+    attrs = getattr(node_cls, "_config_attrs", {}).copy()
+    for k, v in builtins.dict(node_cls.__dict__).items():
+        # Add our attributes
+        if isinstance(v, ConfigurationAttribute):
+            if v.unset:
+                attrs.pop(k, None)
+                delattr(node_cls, k)
+                # Keep track of what this class wants to unset, in case of MRO traversal.
+                node_cls._config_unset.append(k)
+            else:
+                attrs[k] = v
+    node_cls._config_attrs = attrs
+    node_cls.__post_new__ = compile_postnew(node_cls)
+    node_cls._config_isroot = root
+    if root:
+        node_cls.__post_new__ = wrap_root_postnew(node_cls.__post_new__)
+        node_cls._config_isbooted = False
+    node_cls.__new__ = compile_new(
+        node_cls, dynamic=dynamic, pluggable=pluggable, root=root
+    )
+    if dynamic:
+        node_cls.__init_subclass__ = compile_isc(node_cls, dynamic)
+    make_get_node_name(node_cls, root=root)
+    make_tree(node_cls)
+    make_dictable(node_cls)
+    make_copyable(node_cls)
+
+    return node_cls
+
+
+def dynamic(
+    node_cls=None,
+    attr_name="cls",
+    classmap=None,
+    auto_classmap=False,
+    classmap_entry=None,
+    **kwargs,
+):
+    """
+    Decorate a class to be castable to a dynamically configurable class using
+    a class configuration attribute.
+
+    *Example*: Register a required string attribute ``class`` (this is the default):
+
+    .. code-block:: python
+
+        @dynamic
+        class Example:
+            pass
+
+    *Example*: Register a string attribute ``type`` with a default value
+    'pkg.DefaultClass' as dynamic attribute:
+
+    .. code-block:: python
+
+        @dynamic(attr_name='type', required=False, default='pkg.DefaultClass')
+        class Example:
+            pass
+
+    :param attr_name: Name under which to register the class attribute in the node.
+    :type attr_name: str
+    :param kwargs: All keyword arguments are passed to the constructor of the
+      :func:`attribute <.config.attr>`.
+    """
+    if "required" not in kwargs and "default" not in kwargs:
+        kwargs["required"] = True
+    if "type" not in kwargs:
+        kwargs["type"] = str
+    class_attr = ConfigurationAttribute(**kwargs)
+    dynamic_config = DynamicNodeConfiguration(classmap, auto_classmap, classmap_entry)
+    if node_cls is None:
+        # If node_cls is None, it means that no positional argument was given, which most
+        # likely means that the @dynamic(...) syntax was used instead of the @dynamic.
+        # This means we have to return an inner decorator instead of the decorated class
+        def decorator(node_cls):
+            return _dynamic(node_cls, class_attr, attr_name, dynamic_config)
+
+        return decorator
+    # Regular @dynamic syntax used, return decorated class
+    return _dynamic(node_cls, class_attr, attr_name, dynamic_config)
+
+
+class DynamicNodeConfiguration:
+    def __init__(self, classmap=None, auto_classmap=False, entry=None):
+        self.classmap = classmap
+        self.auto_classmap = auto_classmap
+        self.entry = entry
+
+
+def _dynamic(node_cls, class_attr, attr_name, config):
+    # Set the dynamic attribute
+    setattr(node_cls, attr_name, class_attr)
+    node_cls._config_dynamic_attr = attr_name
+    # Other than that compile the dynamic class like a regular node class
+    node_cls = node(node_cls, dynamic=config)
+
+    if config.auto_classmap or config.classmap:
+        node_cls._config_dynamic_classmap = config.classmap or {}
+    # This adds the parent class to its own classmap, which for subclasses happens in init
+    # subclass
+    if config.entry is not None:
+        if not hasattr(node_cls, "_config_dynamic_classmap"):
+            raise ValueError(
+                f"Calling `@config.dynamic` with `entry='{config.entry}'`"
+                + " requires `classmap` or `auto_classmap` to be set as well"
+                + f" on '{node_cls.__name__}'."
+            )
+        node_cls._config_dynamic_classmap[config.entry] = node_cls
+    # Mark the class as its own dynamic root, (grand)child classes will all need to
+    # inherit from this as an interface contract.
+    node_cls._config_dynamic_root = node_cls
+    return node_cls
+
+
+def pluggable(key, plugin_name=None):
+    """
+    Create a node whose configuration is defined by a plugin.
+
+    *Example*: If you want to use the :guilabel:`attr` to chose from all the installed
+    `dbbs_scaffold.my_plugin` plugins:
+
+    .. code-block:: python
+
+        @pluggable('attr', 'my_plugin')
+        class PluginNode:
+            pass
+
+    This will then read :guilabel:`attr`, load the plugin and configure the node from
+    the node class specified by the plugin.
+
+    :param plugin_name: The name of the category of the plugin endpoint
+    :type plugin_name: str
+    """
+
+    def inner_decorator(node_cls):
+        node_cls._config_plugin_name = plugin_name
+        node_cls._config_plugin_key = key
+        class_attr = ConfigurationAttribute(type=str, required=True)
+        setattr(node_cls, key, class_attr)
+        return node(node_cls, pluggable=True)
+
+    return inner_decorator
+
+
+def attr(**kwargs):
+    """
+    Create a configuration attribute.
+
+    Only works when used inside a class decorated with the :func:`node
+    <.config.node>`, :func:`dynamic <.config.dynamic>`,  :func:`root <.config.root>`
+    or  :func:`pluggable <.config.pluggable>` decorators.
+
+    :param type: Type of the attribute's value.
+    :type type: Callable
+    :param required: Should an error be thrown if the attribute is not present?
+    :type required: bool
+    :param default: Default value.
+    :type default: Any
+    :param call_default: Should the default value be used (False) or called (True).
+      Use this to prevent mutable default values.
+    :type call_default: bool
+    :param key: If set, the key of the parent is stored on this attribute.
+    """
+    return ConfigurationAttribute(**kwargs)
+
+
+def ref(reference, **kwargs):
+    """
+    Create a configuration reference.
+
+    Configuration references are attributes that transform their value into the value
+    of another node or value in the document::
+
+      {
+        "keys": {
+            "a": 3,
+            "b": 5
+        },
+        "simple_ref": "a"
+      }
+
+    With ``simple_ref = config.ref(lambda root, here: here["keys"])`` the value ``a``
+    will be looked up in the configuration object (after all values have been cast) at
+    the location specified by the callable first argument.
+    """
+    return ConfigurationReferenceAttribute(reference, **kwargs)
+
+
+def reflist(reference, **kwargs):
+    """
+    Create a configuration reference list.
+    """
+    if "default" not in kwargs:
+        kwargs["default"] = builtins.list
+        kwargs["call_default"] = True
+    return ConfigurationReferenceListAttribute(reference, **kwargs)
+
+
+def slot(**kwargs):
+    """
+    Create an attribute slot that is required to be overriden by child or plugin
+    classes.
+    """
+    return ConfigurationAttributeSlot(**kwargs)
+
+
+def property(val=None, /, type=None, **kwargs):
+    """
+    Create a configuration property attribute. You may provide a value or a callable. Call
+    `setter` on the return value as you would with a regular property.
+    """
+    if type is None:
+        type = lambda v: v
+
+    def decorator(val):
+        prop = val if callable(val) else lambda s: val
+        return ConfigurationProperty(prop, type=type, **kwargs)
+
+    if val is None:
+        return decorator
+    else:
+        return decorator(val)
+
+
+def provide(value):
+    """
+    Provide a value for a parent class' attribute. Can be a value or a callable, a
+    readonly configuration property will be created from it either way.
+    """
+    prop = property(value)
+
+    def provided(self, instance, value):
+        raise AttributeError(f"Can't set attribute, class provides the value '{value}'.")
+
+    # Create a callable object that invokes `provided` when called, and whose `bool()`
+    # returns `False`. Later in `_is_settable_attr`, we use this to trick the short
+    # circuiting logic, so that this setter doesn't make the internal logic set and error
+    # out on this attr.
+    prop.setter(
+        type("provision", (), {"__call__": provided, "__bool__": lambda s: False})()
+    )
+
+    return prop
+
+
+def list(**kwargs):
+    """
+    Create a configuration attribute that holds a list of configuration values.
+    Best used only for configuration nodes. Use an :func:`attr` in combination with a
+    :func:`types.list <.config.types.list>` type for simple values.
+    """
+    return ConfigurationListAttribute(**kwargs)
+
+
+def dict(**kwargs):
+    """
+    Create a configuration attribute that holds a key value pairs of configuration
+    values. Best used only for configuration nodes. Use an :func:`attr` in combination
+    with a :func:`types.dict <.config.types.dict>` type for simple values.
+    """
+    return ConfigurationDictAttribute(**kwargs)
+
+
+def catch_all(**kwargs):
+    """
+    Catches any unknown key with a value that can be cast to the given type and
+    collects them under the attribute name.
+    """
+    return ConfigurationAttributeCatcher(**kwargs)
+
+
+def unset():
+    """
+    Override and unset an inherited configuration attribute.
+    """
+    return ConfigurationAttribute(unset=True)
+
+
+def file(**kwargs):
+    """
+    Create a file dependency attribute.
+    """
+    from ..storage._files import FileDependencyNode
+
+    kwargs.setdefault("type", FileDependencyNode)
+    return attr(**kwargs)
+
+
+def _setattr(instance, name, value):
+    instance.__dict__["_" + name] = value
+
+
+def _getattr(instance, name):
+    try:
+        return instance.__dict__["_" + name]
+    except KeyError as e:
+        instance.__getattribute__(e.args[0])
+
+
+def _hasattr(instance, name):
+    return "_" + name in instance.__dict__
+
+
+def _get_root(obj):
+    parent = obj
+    while hasattr(parent, "_config_parent") and parent._config_parent is not None:
+        parent = parent._config_parent
+    return parent
+
+
+def _strict_root(obj):
+    root = _get_root(obj)
+    return root if getattr(root, "_config_isroot", False) else None
+
+
+def _booted_root(obj):
+    root = _strict_root(obj)
+    return root if root and root._config_isfinished and root._config_isbooted else None
+
+
+def _is_booted(obj):
+    return obj and obj._config_isbooted
+
+
+def _root_is_booted(obj):
+    root = _strict_root(obj)
+    return _is_booted(root)
+
+
+def _boot_nodes(top_node, scaffold):
+    for node in walk_nodes(top_node):
+        node.scaffold = scaffold
+        # Boot attributes
+        for attr in getattr(node, "_config_attrs", {}).values():
+            booted = {None}
+            for cls in type(node).__mro__:
+                cls_attr = getattr(cls, attr.attr_name, None)
+                if (boot := getattr(cls_attr, "__boot__", None)) and boot not in booted:
+                    boot(node, scaffold)
+                    booted.add(boot)
+        # Boot node hook
+        try:
+            run_hook(node, "boot")
+        except Exception as e:
+            errr.wrap(BootError, e, prepend=f"Failed to boot {node}:")
+    # fixme: why is this here? Will deadlock in case of BootError on specific node only.
+    MPI.barrier()
+
+
+def _unset_nodes(top_node):
+    for node in walk_nodes(top_node):
+        try:
+            del node.scaffold
+        except Exception:
+            pass
+        node._config_parent = None
+        node._config_key = None
+        if hasattr(node, "_config_index"):
+            node._config_index = None
+        run_hook(node, "unboot")
+
+
+class ConfigurationAttribute:
+    """
+    Base implementation of all the different configuration attributes. Call the factory
+    function :func:`.attr` instead.
+    """
+
+    def __init__(
+        self,
+        type=None,
+        default=None,
+        call_default=None,
+        required=False,
+        key=False,
+        unset=False,
+        hint=MISSING,
+    ):
+        if not callable(required):
+            self.required = lambda s: required
+        else:
+            self.required = required
+        self.key = key
+        self.default = default
+        self.call_default = call_default
+        self.type = self._set_type(type, key)
+        self.unset = unset
+        self.hint = hint
+
+    def __set_name__(self, owner, name):
+        self.attr_name = name
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        return _getattr(instance, self.attr_name)
+
+    def fset(self, instance, value):
+        return _setattr(instance, self.attr_name, value)
+
+    def __set__(self, instance, value):
+        if _hasattr(instance, self.attr_name):
+            ex_value = _getattr(instance, self.attr_name)
+            _unset_nodes(ex_value)
+        if value is None:
+            # Don't try to cast None to a value of the attribute type.
+            return self.fset(instance, None)
+        try:
+            value = self.type(value, _parent=instance, _key=self.attr_name)
+        except ValueError:
+            # This value error should only arise when users are manually setting
+            # attributes in an already bootstrapped config tree.
+            raise CastError(
+                f"'{value}' is not convertible to {self.type.__name__},"
+                f" for attribute '{self.attr_name}' of {instance}."
+            ) from None
+        except (RequirementError, CastError) as e:
+            if not hasattr(e, "node") or not e.node:
+                e.node, e.attr = instance, self.attr_name
+            raise
+        except Exception as e:
+            raise CastError(
+                f"Couldn't cast '{value}' into {self.type.__name__}: {e}",
+                instance,
+                self.attr_name,
+            ) from e
+        self.flag_dirty(instance)
+        # The value was cast to its intended type and the new value can be set.
+        self.fset(instance, value)
+        root = _strict_root(instance)
+        if _is_booted(root):
+            _boot_nodes(value, root.scaffold)
+
+    def _set_type(self, type, key):
+        self._config_type = type
+        # Determine type of the attribute
+        if not type and self.default is not None:
+            if self.should_call_default():
+                t = builtins.type(self.default())
+            else:
+                t = builtins.type(self.default)
+        else:
+            from . import types
+
+            t = type or (key and types.key()) or types.str()
+        # This call wraps the type handler so that it accepts all reserved keyword args
+        # like `_parent` and `_key`
+        t = _wrap_reserved(t)
+        return t
+
+    def get_type(self):
+        return self._config_type
+
+    def get_hint(self):
+        if self.hint is not MISSING:
+            return self.hint
+        if hasattr(self.type, "__hint__"):
+            return self.type.__hint__()
+        return MISSING
+
+    def get_node_name(self, instance):
+        return instance.get_node_name() + "." + self.attr_name
+
+    def is_node_type(self):
+        return hasattr(self._config_type, "_config_attrs")
+
+    def tree(self, instance):
+        val = _getattr(instance, self.attr_name)
+        return self.tree_of(val)
+
+    def tree_of(self, value):
+        # Allow subnodes and other class values to convert themselves to their tree
+        # representation
+        if hasattr(value, "__tree__"):
+            value = value.__tree__()
+        # Check if the type handler specifies any inversion function to convert tree
+        # values back to how they were found in the document.
+        if hasattr(self.type, "__inv__") and value is not None:
+            value = self.type.__inv__(value)
+        return value
+
+    def flag_dirty(self, instance):
+        instance._config_state[self.attr_name] = False
+        if self.attr_name not in instance._config_attr_order:
+            instance._config_attr_order.append(self.attr_name)
+
+    def is_dirty(self, instance):
+        return not instance._config_state.get(self.attr_name, True)
+
+    def flag_pristine(self, instance):
+        instance._config_state[self.attr_name] = True
+
+    def get_default(self):
+        return self.default() if self.should_call_default() else self.default
+
+    def should_call_default(self):
+        cdf = self.call_default
+        return cdf or (cdf is None and callable(self.default))
+
+
+class cfglist(builtins.list):
+    """
+    Extension of the builtin list to manipulate lists of configuration nodes.
+    """
+
+    def get_node_name(self):
+        return self._config_parent.get_node_name() + "." + self._config_attr_name
+
+    def append(self, item):
+        item = self._preset(len(self), item)
+        super().append(item)
+        self._postset((item,))
+        return self[-1]
+
+    def insert(self, index, item):
+        item = self._preset(index, item)
+        super().insert(index, item)
+        self._postset((item,))
+        self._reindex(index)
+
+    def pop(self, index=-1):
+        ex_item = super().pop(index)
+        _unset_nodes(ex_item)
+        self._reindex(index)
+        return ex_item
+
+    def clear(self):
+        for node in self:
+            _unset_nodes(node)
+        super().clear()
+
+    def sort(self, **kwargs):
+        super().sort(**kwargs)
+        self._reindex(0)
+
+    def reverse(self):
+        super().reverse()
+        self._reindex(0)
+
+    def extend(self, items):
+        items = self._fromiter(len(self), items)
+        super().extend(items)
+        self._postset(items)
+
+    def __setitem__(self, index, item):
+        if isinstance(index, int):
+            ex_items = [self[index]]
+            item = self._preset(index, item)
+            items = [item]
+            reindex_from = None
+        else:
+            ex_items = self[index]
+            reindex_from = index.indices(len(self))[0]
+            items = item = self._fromiter(reindex_from, item)
+        for ex_item in ex_items:
+            _unset_nodes(ex_item)
+        # Don't be fooled, item can be a single value or a list, depending on the index.
+        super().__setitem__(index, item)
+        if reindex_from is not None:
+            self._reindex(reindex_from)
+        self._postset(items)
+
+    def _reindex(self, start):
+        for i in range(start, len(self)):
+            self[i]._config_key = i
+            self[i]._config_index = i
+
+    def _fromiter(self, start, items):
+        return tuple(self._preset(start + i, item) for i, item in enumerate(items))
+
+    def _preset(self, index, item):
+        try:
+            item = self._elem_type(item, _parent=self, _key=index)
+            try:
+                item._config_index = index
+            except Exception as e:
+                pass
+            return item
+        except (RequirementError, CastError) as e:
+            e.args = (
+                f"Couldn't cast element {index} from '{item}'"
+                + f" into a {self._elem_type.__name__}. "
+                + e.msg,
+                *e.args,
+            )
+            if not e.node:
+                e.node, e.attr = self, index
+            raise
+        except Exception as e:
+            raise CastError(
+                f"Couldn't cast element {index} from '{item}'"
+                + f" into a {self._elem_type.__name__}: {e}"
+            )
+
+    def _postset(self, items):
+        root = _strict_root(self)
+        if root is not None:
+            for item in items:
+                _resolve_references(root, item)
+        if _is_booted(root):
+            for item in items:
+                _boot_nodes(item, root.scaffold)
+
+    @builtins.property
+    def _config_attr_name(self):
+        return self._config_attr.attr_name
+
+
+class ConfigurationListAttribute(ConfigurationAttribute):
+    def __init__(self, *args, size=None, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.size = size
+
+    def __set__(self, instance, value, _key=None):
+        _setattr(instance, self.attr_name, self.fill(value, _parent=instance))
+
+    def __populate__(self, instance, value, unique_list=False):
+        cfglist = _getattr(instance, self.attr_name)
+        if not unique_list or value not in cfglist:
+            builtins.list.append(cfglist, value)
+
+    def fill(self, value, _parent, _key=None):
+        _cfglist = cfglist()
+        _cfglist._config_parent = _parent
+        _cfglist._config_attr = self
+        _cfglist._elem_type = self.child_type
+        if isinstance(value, builtins.dict):
+            raise CastError(f"Dictionary `{value}` given where list is expected.")
+        _cfglist.extend(value or builtins.list())
+        if self.size is not None and len(_cfglist) != self.size:
+            raise CastError(
+                f"Couldn't cast {value} into a {self.size}-element list,"
+                + f" obtained {len(_cfglist)} elements"
+            )
+        return _cfglist
+
+    def _set_type(self, type, key=None):
+        self.child_type = super()._set_type(type, key=False)
+
+        @wraps(self.fill)
+        def wrapper(*args, **kwargs):
+            return self.fill(*args, **kwargs)
+
+        # Expose children __inv__ function if it exists
+        if hasattr(self.child_type, "__inv__"):
+            setattr(wrapper, "__inv__", self.child_type.__inv__)
+        return wrapper
+
+    def tree(self, instance):
+        val = _getattr(instance, self.attr_name)
+        return [self.tree_of(e) for e in val]
+
+    def get_hint(self):
+        if self.hint is not MISSING:
+            return self.hint
+        if hasattr(self.child_type, "__hint__"):
+            return [self.child_type.__hint__(), self.child_type.__hint__()]
+        return MISSING
+
+
+class cfgdict(builtins.dict):
+    """
+    Extension of the builtin dictionary to manipulate dicts of configuration nodes.
+    """
+
+    def __getattr__(self, name):
+        try:
+            return self[name]
+        except KeyError:
+            raise AttributeError(
+                self.get_node_name() + " object has no attribute '{}'".format(name)
+            )
+
+    def __setitem__(self, key, value):
+        if key in self:
+            _unset_nodes(self[key])
+        try:
+            value = self._elem_type(value, _parent=self, _key=key)
+        except (RequirementError, CastError) as e:
+            if not (hasattr(e, "node") and e.node):
+                e.node, e.attr = self, key
+            raise
+        except Exception:
+            import traceback
+
+            raise CastError(
+                "Couldn't cast {}.{} from '{}' into a {}".format(
+                    self.get_node_name(), key, value, self._elem_type.__name__
+                )
+                + "\n"
+                + traceback.format_exc()
+            )
+        else:
+            super().__setitem__(key, value)
+            root = _strict_root(value)
+            if root is not None:
+                _resolve_references(root, value)
+            if _is_booted(root):
+                _boot_nodes(value, root.scaffold)
+
+    def add(self, key, *args, **kwargs):
+        if key in self:
+            raise KeyError(
+                f"{self.get_node_name()} already contains '{key}'."
+                + " Use `node[key] = value` if you want to overwrite it."
+            )
+        self[key] = value = self._elem_type(*args, _parent=self, _key=key, **kwargs)
+        return value
+
+    def clear(self):
+        for node in self.values():
+            _unset_nodes(node)
+        super().clear()
+
+    def pop(self, key):
+        item = super().pop(key)
+        _unset_nodes(item)
+        return item
+
+    def popitem(self):
+        key, value = super().popitem()
+        _unset_nodes(value)
+        return key, value
+
+    def setdefault(self, key, value):
+        if key in self:
+            return self[key]
+        else:
+            self[key] = value
+            return self[key]
+
+    def update(self, other):
+        for ckey, value in other.items():
+            self[ckey] = value
+
+    def __ior__(self, other):
+        ex_values = tuple(self.values())
+        try:
+            merge_f = super().__ior__
+        except AttributeError:
+            # Patch for 3.8
+            merge_f = super().update
+        merge_f(other)
+        new_values = tuple(self.values())
+        for removed_node in (e for e in ex_values if e not in new_values):
+            _unset_nodes(removed_node)
+        for added_node in (a for a in new_values if a not in ex_values):
+            _boot_nodes(added_node, self.scaffold)
+        return self
+
+    def copy(self):
+        return cfgdictcopy(self)
+
+    @builtins.property
+    def _config_attr_name(self):
+        return self._config_attr.attr_name
+
+    def get_node_name(self):
+        return self._config_parent.get_node_name() + "." + self._config_attr_name
+
+
+class cfgdictcopy(builtins.dict):
+    def __init__(self, other):
+        super().__init__(other)
+        self._elem_type = other._elem_type
+        self._copied_from = other
+
+    @builtins.property
+    def _config_attr_name(self):
+        return self._copied_from._config_attr_name
+
+    def get_node_name(self):
+        return self._copied_from.get_node_name()
+
+
+class ConfigurationDictAttribute(ConfigurationAttribute):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def __set__(self, instance, value, _key=None):
+        _setattr(
+            instance,
+            self.attr_name,
+            self.fill(value, _parent=instance, _key=_key or self.attr_name),
+        )
+
+    def fill(self, value, _parent, _key=None):
+        _cfgdict = cfgdict()
+        _cfgdict._config_parent = _parent
+        _cfgdict._config_key = _key
+        _cfgdict._config_attr = self
+        _cfgdict._elem_type = self.child_type
+        _cfgdict.update(value or builtins.dict())
+        return _cfgdict
+
+    def _set_type(self, type, key=None):
+        self.child_type = super()._set_type(type, key=False)
+
+        @wraps(self.fill)
+        def wrapper(*args, **kwargs):
+            return self.fill(*args, **kwargs)
+
+        # Expose children __inv__ function if it exists
+        if hasattr(self.child_type, "__inv__"):
+            setattr(wrapper, "__inv__", self.child_type.__inv__)
+        return wrapper
+
+    def tree(self, instance):
+        val = _getattr(instance, self.attr_name).items()
+        return {k: self.tree_of(v) for k, v in val}
+
+    def get_hint(self):
+        if self.hint is not MISSING:
+            return self.hint
+        if hasattr(self.child_type, "__hint__"):
+            return {
+                "key1": self.child_type.__hint__(),
+                "key2": self.child_type.__hint__(),
+            }
+        return MISSING
+
+
+class ConfigurationReferenceAttribute(ConfigurationAttribute):
+    def __init__(
+        self,
+        reference,
+        key=None,
+        ref_type=None,
+        populate=None,
+        backref=None,
+        pop_unique=True,
+        **kwargs,
+    ):
+        self.ref_lambda = reference
+        self.ref_key = key
+        self.ref_type = ref_type
+        self.populate = populate
+        self.backref = backref
+        self.pop_unique = pop_unique
+        # No need to cast to any types: the reference we fetch will already have been cast
+        if "type" in kwargs:  # pragma: nocover
+            del kwargs["type"]
+        super().__init__(**kwargs)
+
+    def get_ref_key(self):
+        return self.ref_key or (self.attr_name + "_reference")
+
+    def __set__(self, instance, value, key=None):
+        if self.is_reference_value(value):
+            _setattr(instance, self.attr_name, value)
+        else:
+            setattr(instance, self.get_ref_key(), value)
+            if self.should_resolve_on_set(instance):
+                if hasattr(instance, "_config_root"):  # pragma: nocover
+                    raise CfgReferenceError(
+                        "Can't autoresolve references without a config root."
+                    )
+                _setattr(
+                    instance,
+                    self.attr_name,
+                    self.__ref__(instance, instance._config_root),
+                )
+
+    def is_reference_value(self, value):
+        if value is None:
+            return True
+        if self.ref_type is not None:
+            return isinstance(value, self.ref_type)
+        elif hasattr(self.ref_lambda, "is_ref"):
+            return self.ref_lambda.is_ref(value)
+        else:
+            return not isinstance(value, str)
+
+    def should_resolve_on_set(self, instance):
+        return (
+            hasattr(instance, "_config_resolve_on_set")
+            and instance._config_resolve_on_set
+        )
+
+    def __ref__(self, instance, root):
+        try:
+            remote, remote_key = self._prepare_self(instance, root)
+        except NoReferenceAttributeSignal:
+            return None
+        return self.resolve_reference(instance, remote, remote_key)
+
+    def _prepare_self(self, instance, root):
+        instance._config_root = root
+        instance._config_resolve_on_set = True
+        remote = self.ref_lambda(root, instance)
+        local_attr = self.get_ref_key()
+        if not hasattr(instance, local_attr):
+            raise NoReferenceAttributeSignal()
+        return remote, getattr(instance, local_attr)
+
+    def resolve_reference(self, instance, remote, key):
+        if key not in remote:
+            raise CfgReferenceError(
+                "Reference '{}' of {} does not exist in {}".format(
+                    key,
+                    self.get_node_name(instance),
+                    remote.get_node_name(),
+                )
+            )
+        value = remote[key]
+        if self.populate:
+            self.populate_reference(instance, value)
+        if self.backref:
+            self.back_reference(instance, value)
+
+        return value
+
+    def populate_reference(self, instance, reference):
+        # Remote descriptors can ask to handle populating itself by implementing a
+        # __populate__ method. Here we check if the method exists and if so defer to it.
+        if (pop_attr := getattr(reference.__class__, self.populate, None)) and (
+            pop_func := getattr(pop_attr, "__populate__", None)
+        ):
+            pop_func(reference, instance, unique_list=self.pop_unique)
+        elif (population := getattr(reference, self.populate, None)) is not None:
+            if not self.pop_unique or instance not in population:
+                population.append(instance)
+        else:
+            setattr(reference, self.populate, [instance])
+
+    def back_reference(self, instance, reference):
+        if (ref_attr := getattr(reference.__class__, self.backref, None)) and (
+            ref_func := getattr(ref_attr, "__backref__", None)
+        ):
+            ref_func(reference, instance)
+        else:
+            setattr(reference, self.backref, instance)
+
+    def tree(self, instance):
+        val = getattr(instance, self.get_ref_key(), None)
+        if self.is_reference_value(val) and hasattr(val, "_config_key"):
+            val = val._config_key
+        return val
+
+
+class ConfigurationReferenceListAttribute(ConfigurationReferenceAttribute):
+    def __set__(self, instance, value, key=None):
+        if value is None:
+            setattr(instance, self.get_ref_key(), [])
+            _setattr(instance, self.attr_name, [])
+            return
+        try:
+            remote_keys = builtins.list(iter(value))
+        except TypeError:
+            raise CfgReferenceError(
+                "Reference list '{}' of {} is not iterable.".format(
+                    value, self.get_node_name(instance)
+                )
+            )
+        # Store the referring values to the references key.
+        setattr(instance, self.get_ref_key(), remote_keys)
+        if self.should_resolve_on_set(instance):
+            remote = self.ref_lambda(instance._config_root, instance)
+            refs = self.resolve_reference_list(instance, remote, remote_keys)
+            _setattr(instance, self.attr_name, refs)
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        if self.should_resolve_on_set(instance):
+            return super().__get__(instance, owner)
+        else:
+            return getattr(instance, self.get_ref_key())
+
+    def get_ref_key(self):
+        return self.ref_key or (self.attr_name + "_references")
+
+    def __ref__(self, instance, root):
+        try:
+            remote, remote_keys = self._prepare_self(instance, root)
+        except NoReferenceAttributeSignal:  # pragma: nocover
+            return None
+        return self.resolve_reference_list(instance, remote, remote_keys)
+
+    def resolve_reference_list(self, instance, remote, remote_keys):
+        refs = []
+        for remote_key in remote_keys:
+            if not self.is_reference_value(remote_key):
+                reference = self.resolve_reference(instance, remote, remote_key)
+            else:
+                reference = remote_key
+                # Usually resolve_reference also populates, but since we have our ref
+                # already we skip it and should call populate_reference ourselves.
+                if self.populate:
+                    self.populate_reference(instance, reference)
+            refs.append(reference)
+        return refs
+
+    def __populate__(self, instance, value, unique_list=False):
+        has_refs = hasattr(instance, self.get_ref_key())
+        has_pop = hasattr(instance, self.attr_name)
+        if has_pop:
+            population = getattr(instance, self.attr_name)
+        if has_refs:
+            references = getattr(instance, self.get_ref_key())
+        is_new = (not has_pop or value not in population) and (
+            not has_refs or value not in references
+        )
+        should_pop = has_pop and (not unique_list or is_new)
+        should_ref = should_pop or not has_pop
+        if should_pop:
+            population.append(value)
+        if should_ref:
+            if not has_refs:
+                setattr(instance, self.get_ref_key(), [value])
+            else:
+                references.append(value)
+
+    def tree(self, instance):
+        val = getattr(instance, self.get_ref_key(), [])
+        val = [v._config_key if self._tree_should_unreference(v) else v for v in val]
+        return val
+
+    def _tree_should_unreference(self, value):
+        return self.is_reference_value(value) and hasattr(value, "_config_key")
+
+
+class ConfigurationAttributeSlot(ConfigurationAttribute):
+    def __set__(self, instance, value):  # pragma: nocover
+        raise NotImplementedError(
+            f"Configuration slot '{self.attr_name}' of {instance.get_node_name()} is"
+            f" empty. The {instance.__class__._bsb_entry_point.module_name} plugin"
+            f" provided by '{instance.__class__._bsb_entry_point.dist}' should fill the"
+            " slot with a configuration attribute."
+        )
+
+
+class ConfigurationProperty(ConfigurationAttribute):
+    def __init__(self, fget, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.fget = fget
+        self.fset = None
+
+    def setter(self, f):
+        self.fset = f
+        return self
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        return self.fget(instance)
+
+    def __set__(self, instance, value):
+        if self.fset is None:
+            e = AttributeError(f"Can't set attribute '{self.attr_name}'")
+            e.node = self
+            raise e
+        else:
+            return super().__set__(instance, value)
+
+
+def _collect_kv(n, d, k, v):
+    d[k] = v
+
+
+class ConfigurationAttributeCatcher(ConfigurationAttribute):
+    def __init__(
+        self,
+        *args,
+        type=str,
+        initial=builtins.dict,
+        catch=_collect_kv,
+        contains=None,
+        tree_cb=None,
+        **kwargs,
+    ):
+        super().__init__(*args, type=type, default=initial, call_default=True, **kwargs)
+        self.catch_callback = catch
+        if contains is not None:
+            self.contains = contains
+        if tree_cb is not None:
+            self.tree_callback = tree_cb
+
+    def __set__(self, instance, value):
+        _setattr(instance, self.attr_name, value)
+
+    def get_caught(self, instance):
+        if not hasattr(instance, f"_{self.attr_name}_caught"):
+            setattr(instance, f"_{self.attr_name}_caught", {})
+        return getattr(instance, f"_{self.attr_name}_caught")
+
+    def __catch__(self, node, key, value):
+        # Try to cast to our type, if it fails it will be caught by whoever is asking us
+        # to catch this and we don't catch this value.
+        cast = self.type(value, _parent=node, _key=key)
+        # If succesfully cast, catch this value by executing our catch callback.
+        self.catch_callback(node, _getattr(node, self.attr_name), key, cast)
+        self.get_caught(node)[key] = cast
+
+    def tree(self, instance):
+        # The default attr catcher collects what it catches in a dict. When we want to
+        # build the config tree again these values should be placed back in their
+        # original keys. We don't want to store our caught values in the config file. To
+        # do so we use the `tree_callback` instead.
+        return None
+
+    def contains(self, instance, key):
+        return key in self.get_caught(instance)
+
+    def tree_callback(self, instance, key):
+        # When building the config tree the values that were caught can't be found in the
+        # attrs and the tree builder will check all catch-attr's `contains` methods and
+        # calls the right tree_callback to fetch the value.
+        value = _getattr(instance, self.attr_name)[key]
+        if hasattr(value, "__tree__"):
+            value = value.__tree__()
+        return value
```

### Comparing `bsb_core-4.0.1/bsb/config/_compile.py` & `bsb_core-4.1.0/bsb/config/_compile.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,54 +1,54 @@
-import functools
-from inspect import signature as _inspect_signature
-
-_reserved_keywords = ["_parent", "_key"]
-
-
-def _wrap_reserved(t):
-    """
-    Wrap a type handler in a wrapper that accepts all reserved keyword arguments that
-    the config system will push into the type handler call, and pass only those that
-    the original type handler accepts. This way type handlers can accept any
-    combination of the reserved keyword args without raising TypeErrors when they do
-    not accept one.
-    """
-    from .types import TypeHandler
-
-    # Type handlers never need to be wrapped. The `__init_subclass__` of the TypeHandler
-    # class handles wrapping of `__call__` implementations so that they accept and strip
-    # _parent & _key.
-    if isinstance(t, TypeHandler):
-        return t
-
-    # Check which reserved keywords the function already takes
-    passes = _reserved_kw_passes(t)
-    if all(passes.values()):
-        return t
-
-    # Create the keyword arguments of the outer function that accepts all reserved kwargs
-    reserved_keys = "".join(f", {key}=None" for key in _reserved_keywords)
-    header = f"def type_handler(value, *args{reserved_keys}, **kwargs):\n"
-    passes = "".join(f", {key}={key}" for key in _reserved_keywords if passes[key])
-    # Create the call to the inner function that is passed only the kwargs that it accepts
-    wrap = f" return orig(value, *args{passes}, **kwargs)"
-    # Compile the code block and indicate that the function was compiled here.
-    mod = compile(header + wrap, f"{__file__}/<_wrap_reserved:compile>", "exec")
-    # Execute the code block in this local scope and pick the function out of the scope
-    exec(mod, {"orig": t}, bait := locals())
-    type_handler = bait["type_handler"]
-    # Copy over the metadata of the original function
-    type_handler = functools.wraps(t)(type_handler)
-    type_handler.__name__ = t.__name__
-    return type_handler
-
-
-def _reserved_kw_passes(f):
-    # Inspect the signature and wrap the typecast in a wrapper that will accept and
-    # strip the missing 'key' kwarg
-    try:
-        sig = _inspect_signature(f)
-        params = sig.parameters
-    except:
-        params = []
-
-    return {key: key in params for key in _reserved_keywords}
+import functools
+from inspect import signature as _inspect_signature
+
+_reserved_keywords = ["_parent", "_key"]
+
+
+def _wrap_reserved(t):
+    """
+    Wrap a type handler in a wrapper that accepts all reserved keyword arguments that
+    the config system will push into the type handler call, and pass only those that
+    the original type handler accepts. This way type handlers can accept any
+    combination of the reserved keyword args without raising TypeErrors when they do
+    not accept one.
+    """
+    from .types import TypeHandler
+
+    # Type handlers never need to be wrapped. The `__init_subclass__` of the TypeHandler
+    # class handles wrapping of `__call__` implementations so that they accept and strip
+    # _parent & _key.
+    if isinstance(t, TypeHandler):
+        return t
+
+    # Check which reserved keywords the function already takes
+    passes = _reserved_kw_passes(t)
+    if all(passes.values()):
+        return t
+
+    # Create the keyword arguments of the outer function that accepts all reserved kwargs
+    reserved_keys = "".join(f", {key}=None" for key in _reserved_keywords)
+    header = f"def type_handler(value, *args{reserved_keys}, **kwargs):\n"
+    passes = "".join(f", {key}={key}" for key in _reserved_keywords if passes[key])
+    # Create the call to the inner function that is passed only the kwargs that it accepts
+    wrap = f" return orig(value, *args{passes}, **kwargs)"
+    # Compile the code block and indicate that the function was compiled here.
+    mod = compile(header + wrap, f"{__file__}/<_wrap_reserved:compile>", "exec")
+    # Execute the code block in this local scope and pick the function out of the scope
+    exec(mod, {"orig": t}, bait := locals())
+    type_handler = bait["type_handler"]
+    # Copy over the metadata of the original function
+    type_handler = functools.wraps(t)(type_handler)
+    type_handler.__name__ = t.__name__
+    return type_handler
+
+
+def _reserved_kw_passes(f):
+    # Inspect the signature and wrap the typecast in a wrapper that will accept and
+    # strip the missing 'key' kwarg
+    try:
+        sig = _inspect_signature(f)
+        params = sig.parameters
+    except:
+        params = []
+
+    return {key: key in params for key in _reserved_keywords}
```

### Comparing `bsb_core-4.0.1/bsb/config/_hooks.py` & `bsb_core-4.1.0/bsb/config/_hooks.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,167 +1,167 @@
-def _dunder(*hooks):
-    return "__" + "_".join(hooks) + "__"
-
-
-def run_hook(obj, hook, *args, **kwargs):
-    """
-    Execute the ``hook`` hook of ``obj``.
-
-    Runs the ``hook`` method ``obj`` but also looks through the class hierarchy for
-    essential hooks with the name ``__<hook>__``.
-
-    .. note::
-
-      Essential hooks are only ran if the method is called using ``run_hook`` while
-      non-essential hooks are wrapped around the method and will always be executed when
-      the method is called (see https://github.com/dbbs-lab/bsb/issues/158).
-    """
-    # Traverse the MRO tree and execute all essential (__hook__) methods.
-    for parent in reversed(obj.__class__.__mro__):
-        if hasattr(parent, _dunder(hook)):
-            getattr(parent, _dunder(hook))(obj, *args, **kwargs)
-    # Execute the final regular hook method.
-    if hasattr(obj, hook):
-        getattr(obj, hook)(*args, **kwargs)
-
-
-def on(hook, cls, essential=False, before=False):
-    """
-    Register a class hook.
-
-    :param hook: Name of the method to hook.
-    :type hook: str
-    :param cls: Class to hook.
-    :type cls: type
-    :param essential: If the hook is essential, it will always be executed even in child
-      classes that override the hook. Essential hooks are only lost if the method on
-      ``cls`` is replaced.
-    :type essential: bool
-    :param before: If ``before`` the hook is executed before the method, otherwise
-      afterwards.
-    :type before: bool
-    """
-    if essential:
-        hook = _dunder(hook)
-    return _hook(cls, hook, before)
-
-
-def after(hook, cls, essential=False):
-    """
-    Register a class hook to run after the target method.
-
-    :param hook: Name of the method to hook.
-    :type hook: str
-    :param cls: Class to hook.
-    :type cls: type
-    :param essential: If the hook is essential, it will always be executed even in child
-      classes that override the hook. Essential hooks are only lost if the method on
-      ``cls`` is replaced.
-    :type essential: bool
-    """
-    return on(hook, cls, essential=essential, before=False)
-
-
-def before(hook, cls, essential=False):
-    """
-    Register a class hook to run before the target method.
-
-    :param hook: Name of the method to hook.
-    :type hook: str
-    :param cls: Class to hook.
-    :type cls: type
-    :param essential: If the hook is essential, it will always be executed even in child
-      classes that override the hook. Essential hooks are only lost if the method on
-      ``cls`` is replaced.
-    :type essential: bool
-    """
-    return on(hook, cls, essential=essential, before=True)
-
-
-def _super(cls, attr):
-    for c in list(cls.__mro__)[1:]:
-        try:
-            return getattr(c, attr)
-        except:
-            pass
-
-
-def has_hook(instance, hook):
-    """
-    Checks the existence of a method or essential method on the ``instance``.
-
-    :param instance: Object to inspect.
-    :type instance: object
-    :param hook: Name of the hook to look for.
-    :type hook: str
-    """
-    return hasattr(instance, hook) or hasattr(instance, _dunder(hook))
-
-
-def overrides(cls, hook, mro=False):
-    """
-    Returns ``True`` if a class has implemented a method or ``False`` if it has inherited
-    it.
-
-    :param cls: Class to inspect.
-    :type cls: class
-    :param hook: Name of the hook to look for.
-    :type hook: str
-    """
-    if not mro:
-        return hook in vars(cls)
-    else:
-
-        class NotDefined:
-            pass
-
-        return getattr(cls, hook, NotDefined) is not getattr(object, hook, NotDefined)
-
-
-def _hook(cls, hook, before):
-    # Returns a decorator that will wrap the existing implementation if it exists,
-    # otherwise returns a decorator that will simply add the decorated function to the
-    # class.
-    if overrides(cls, hook):
-        return _make_wrapper(cls, hook, before)
-    else:
-        return _make_injector(cls, hook)
-
-
-def _make_injector(cls, hook):
-    def decorator(func):
-        # Set the decorated function as the hooked function.
-        _set_hooked_func(cls, hook, func)
-
-    return decorator
-
-
-def _get_func_to_hook(cls, hook):
-    return getattr(cls, hook)
-
-
-def _set_hooked_func(cls, hook, hooked):
-    return setattr(cls, hook, hooked)
-
-
-def _make_wrapper(cls, hook, before):
-    func_to_hook = _get_func_to_hook(cls, hook)
-
-    def decorator(func):
-        if before:
-            # Wrapper that executes the decorated function before the class method
-            def hooked(*args, **kwargs):
-                func(*args, **kwargs)
-                r = func_to_hook(*args, **kwargs)
-                return r
-
-        else:
-            # Wrapper that executes the decorated function after the class method
-            def hooked(*args, **kwargs):
-                r = func_to_hook(*args, **kwargs)
-                func(*args, **kwargs)
-                return r
-
-        # Set the wrapper function as the hooked function
-        _set_hooked_func(cls, hook, hooked)
-
-    return decorator
+def _dunder(*hooks):
+    return "__" + "_".join(hooks) + "__"
+
+
+def run_hook(obj, hook, *args, **kwargs):
+    """
+    Execute the ``hook`` hook of ``obj``.
+
+    Runs the ``hook`` method ``obj`` but also looks through the class hierarchy for
+    essential hooks with the name ``__<hook>__``.
+
+    .. note::
+
+      Essential hooks are only ran if the method is called using ``run_hook`` while
+      non-essential hooks are wrapped around the method and will always be executed when
+      the method is called (see https://github.com/dbbs-lab/bsb/issues/158).
+    """
+    # Traverse the MRO tree and execute all essential (__hook__) methods.
+    for parent in reversed(obj.__class__.__mro__):
+        if hasattr(parent, _dunder(hook)):
+            getattr(parent, _dunder(hook))(obj, *args, **kwargs)
+    # Execute the final regular hook method.
+    if hasattr(obj, hook):
+        getattr(obj, hook)(*args, **kwargs)
+
+
+def on(hook, cls, essential=False, before=False):
+    """
+    Register a class hook.
+
+    :param hook: Name of the method to hook.
+    :type hook: str
+    :param cls: Class to hook.
+    :type cls: type
+    :param essential: If the hook is essential, it will always be executed even in child
+      classes that override the hook. Essential hooks are only lost if the method on
+      ``cls`` is replaced.
+    :type essential: bool
+    :param before: If ``before`` the hook is executed before the method, otherwise
+      afterwards.
+    :type before: bool
+    """
+    if essential:
+        hook = _dunder(hook)
+    return _hook(cls, hook, before)
+
+
+def after(hook, cls, essential=False):
+    """
+    Register a class hook to run after the target method.
+
+    :param hook: Name of the method to hook.
+    :type hook: str
+    :param cls: Class to hook.
+    :type cls: type
+    :param essential: If the hook is essential, it will always be executed even in child
+      classes that override the hook. Essential hooks are only lost if the method on
+      ``cls`` is replaced.
+    :type essential: bool
+    """
+    return on(hook, cls, essential=essential, before=False)
+
+
+def before(hook, cls, essential=False):
+    """
+    Register a class hook to run before the target method.
+
+    :param hook: Name of the method to hook.
+    :type hook: str
+    :param cls: Class to hook.
+    :type cls: type
+    :param essential: If the hook is essential, it will always be executed even in child
+      classes that override the hook. Essential hooks are only lost if the method on
+      ``cls`` is replaced.
+    :type essential: bool
+    """
+    return on(hook, cls, essential=essential, before=True)
+
+
+def _super(cls, attr):
+    for c in list(cls.__mro__)[1:]:
+        try:
+            return getattr(c, attr)
+        except:
+            pass
+
+
+def has_hook(instance, hook):
+    """
+    Checks the existence of a method or essential method on the ``instance``.
+
+    :param instance: Object to inspect.
+    :type instance: object
+    :param hook: Name of the hook to look for.
+    :type hook: str
+    """
+    return hasattr(instance, hook) or hasattr(instance, _dunder(hook))
+
+
+def overrides(cls, hook, mro=False):
+    """
+    Returns ``True`` if a class has implemented a method or ``False`` if it has inherited
+    it.
+
+    :param cls: Class to inspect.
+    :type cls: class
+    :param hook: Name of the hook to look for.
+    :type hook: str
+    """
+    if not mro:
+        return hook in vars(cls)
+    else:
+
+        class NotDefined:
+            pass
+
+        return getattr(cls, hook, NotDefined) is not getattr(object, hook, NotDefined)
+
+
+def _hook(cls, hook, before):
+    # Returns a decorator that will wrap the existing implementation if it exists,
+    # otherwise returns a decorator that will simply add the decorated function to the
+    # class.
+    if overrides(cls, hook):
+        return _make_wrapper(cls, hook, before)
+    else:
+        return _make_injector(cls, hook)
+
+
+def _make_injector(cls, hook):
+    def decorator(func):
+        # Set the decorated function as the hooked function.
+        _set_hooked_func(cls, hook, func)
+
+    return decorator
+
+
+def _get_func_to_hook(cls, hook):
+    return getattr(cls, hook)
+
+
+def _set_hooked_func(cls, hook, hooked):
+    return setattr(cls, hook, hooked)
+
+
+def _make_wrapper(cls, hook, before):
+    func_to_hook = _get_func_to_hook(cls, hook)
+
+    def decorator(func):
+        if before:
+            # Wrapper that executes the decorated function before the class method
+            def hooked(*args, **kwargs):
+                func(*args, **kwargs)
+                r = func_to_hook(*args, **kwargs)
+                return r
+
+        else:
+            # Wrapper that executes the decorated function after the class method
+            def hooked(*args, **kwargs):
+                r = func_to_hook(*args, **kwargs)
+                func(*args, **kwargs)
+                return r
+
+        # Set the wrapper function as the hooked function
+        _set_hooked_func(cls, hook, hooked)
+
+    return decorator
```

### Comparing `bsb_core-4.0.1/bsb/config/_make.py` & `bsb_core-4.1.0/bsb/config/_make.py`

 * *Ordering differences only*

 * *Files 25% similar despite different names*

```diff
@@ -1,738 +1,738 @@
-import functools
-import importlib
-import inspect
-import os
-import sys
-import types
-import warnings
-from collections import defaultdict
-from re import sub
-
-import errr
-
-from .._package_spec import warn_missing_packages
-from .._util import get_qualified_class_name
-from ..exceptions import (
-    BootError,
-    CastError,
-    ConfigurationError,
-    DynamicClassError,
-    DynamicClassInheritanceError,
-    DynamicObjectNotFoundError,
-    PluginError,
-    RequirementError,
-    UnfitClassCastError,
-    UnresolvedClassCastError,
-)
-from ..reporting import warn
-from ._hooks import overrides
-
-
-def _has_own_init(meta_subject, kwargs):
-    try:
-        determined_class = meta_subject.__new__.class_determinant(meta_subject, kwargs)
-        return overrides(determined_class, "__init__", mro=True)
-    except Exception:
-        return overrides(meta_subject, "__init__", mro=True)
-
-
-def make_metaclass(cls):
-    # We make a `NodeMeta` class for each decorated node class, in compliance with any
-    # metaclasses they might already have (to prevent metaclass confusion).
-    # The purpose of the metaclass is to rewrite `__new__` and `__init__` arguments,
-    # and to always call `__new__` and `__init__` in the same manner.
-    # The metaclass makes it so that there are 3 overloaded constructor forms:
-    #
-    # MyNode({ <config dict values> })
-    # MyNode(example="attr", values="here")
-    # ParentNode(me=MyNode(...))
-    #
-    # The third makes it that type handling and other types of casting opt out early
-    # and keep the object reference that the user gives them
-    class ConfigArgRewrite:
-        def __call__(meta_subject, *args, _parent=None, _key=None, **kwargs):
-            has_own_init = _has_own_init(meta_subject, kwargs)
-            # Rewrite the arguments
-            primer = args[0] if args else None
-            if isinstance(primer, meta_subject):
-                _set_pk(primer, _parent, _key)
-                return primer
-            elif isinstance(primer, dict):
-                args = args[1:]
-                primed = primer.copy()
-                primed.update(kwargs)
-                kwargs = primed
-            elif primer is not None and not has_own_init:
-                # If we're dealing with a typical config node, the primer should be a dict
-                # or already precast node. If it is not, we consider it invalid input,
-                # unless the user has specified its own `__init__` function and will deal
-                # with the input arguments there.
-                raise ValueError(f"Unexpected positional argument '{primer}'")
-            # Call the base class's new with internal arguments
-            instance = meta_subject.__new__(
-                meta_subject, *args, _parent=_parent, _key=_key, **kwargs
-            )
-            instance._config_pos_init = getattr(instance, "_config_pos_init", False)
-            # Call the end user's __init__ with the rewritten arguments, if one is defined
-            if has_own_init:
-                sig = inspect.signature(instance.__init__)
-                try:
-                    # Check whether the arguments match the signature. We use `sig.bind`
-                    # so that the function isn't actually called, as this could mask
-                    # `TypeErrors` that occur inside the function.
-                    sig.bind(*args, **kwargs)
-                except TypeError as e:
-                    # Since the user might not know where all these additional arguments
-                    # are coming from, inform them that config nodes get passed their
-                    # config attrs, and how to correctly override __init__.
-                    Param = inspect.Parameter
-                    help_params = {"self": Param("self", Param.POSITIONAL_OR_KEYWORD)}
-                    help_params.update(sig._parameters)
-                    help_params["kwargs"] = Param("kwargs", Param.VAR_KEYWORD)
-                    sig._parameters = help_params
-                    raise TypeError(
-                        f"`{instance.__init__.__module__}.__init__` {e}."
-                        + " When overriding `__init__` on config nodes, do not define"
-                        + " any positional arguments, and catch any additional"
-                        + " configuration attributes that are passed as keyword arguments"
-                        + f": e.g. 'def __init__{sig}'"
-                    ) from None
-                else:
-                    instance._config_pos_init = bool(len(args))
-                    instance.__init__(*args, **kwargs)
-            return instance
-
-    # Avoid metaclass conflicts by prepending our rewrite class to existing metaclass MRO
-    class NodeMeta(ConfigArgRewrite, *cls.__class__.__mro__):
-        def __new__(cls, *args, **kwargs):
-            rcls = super().__new__(cls, *args, **kwargs)
-            # `__init_subclass__` refused to be called with correct subclass, so call
-            # it ourselves.
-            if hasattr(rcls.__bases__[0], "_cfgnode_replaced_ics"):
-                rcls.__bases__[0]._cfgnode_replaced_ics(rcls, **kwargs)
-            return rcls
-
-    return NodeMeta
-
-
-class NodeKwargs(dict):
-    def __init__(self, instance, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.is_shortform = getattr(instance, "_config_pos_init", False)
-
-
-def compose_nodes(*node_classes):
-    """
-    Create a composite mixin class of the given classes. Inherit from the returned class
-    to inherit from more than one node class.
-
-    """
-    meta = type("ComposedMetaclass", tuple(type(cls) for cls in node_classes), {})
-    return meta("CompositionMixin", node_classes, {})
-
-
-def compile_class(cls):
-    cls_dict = dict(cls.__dict__)
-    if "__dict__" in cls_dict:
-        del cls_dict["__dict__"]
-    if "__weakref__" in cls_dict:
-        del cls_dict["__weakref__"]
-    ncls = make_metaclass(cls)(cls.__name__, cls.__bases__, cls_dict)
-    for method in ncls.__dict__.values():
-        _replace_closure_cells(method, cls, ncls)
-
-    # Shitty hack, for some reason I couldn't find a way to override the first argument
-    # of `__init_subclass__` methods, that would otherwise work on other classmethods,
-    # so we noop the actual `__init_subclass__` and we call `__init_subclass__` ourselves
-    # from the metaclass' `__new__` method, where the argument replacement works as usual.
-    if (
-        hasattr(ncls, "__init_subclass__")
-        and "__init_subclass__" in ncls.__dict__
-        and not isinstance(ncls.__init_subclass__, types.BuiltinFunctionType)
-    ):
-        ncls._cfgnode_replaced_ics = ncls.__init_subclass__.__func__
-        ncls.__init_subclass__ = lambda *args, **kwargs: None
-    classmap = getattr(ncls, "_config_dynamic_classmap", None)
-    if classmap is not None:
-        # Replace the reference to the old class with the new class.
-        # The auto classmap entry is added in `__init_subclass__`, which happens before
-        # we replace the class.
-        for k, v in classmap.items():
-            if v is cls:
-                classmap[k] = ncls
-    return ncls
-
-
-def _replace_closure_cells(method, old, new):
-    cl = getattr(method, "__closure__", None) or []
-    for cell in cl:
-        if cell.cell_contents is old:
-            cell.cell_contents = new
-        elif inspect.isfunction(cell.cell_contents):
-            _replace_closure_cells(cell.cell_contents, old, new)
-
-
-def compile_isc(node_cls, dynamic_config):
-    if not dynamic_config or not dynamic_config.auto_classmap:
-        return node_cls.__init_subclass__
-
-    from ._hooks import overrides
-
-    def dud(*args, **kwargs):
-        pass
-
-    if overrides(node_cls, "__init_subclass__"):
-        f = node_cls.__init_subclass__
-    else:
-        f = dud
-
-    def __init_subclass__(cls, classmap_entry=MISSING, **kwargs):
-        super(node_cls, cls).__init_subclass__(**kwargs)
-        if classmap_entry is MISSING:
-            classmap_entry = _snake_case(cls.__name__)
-        if classmap_entry is not None:
-            node_cls._config_dynamic_classmap[classmap_entry] = cls
-        f(**kwargs)
-
-    return classmethod(__init_subclass__)
-
-
-def _snake_case(s):
-    return "_".join(
-        sub("([A-Z][a-z]+)", r" \1", sub("([A-Z]+)", r" \1", s.replace("-", " "))).split()
-    ).lower()
-
-
-def _node_determinant(cls, kwargs):
-    return cls
-
-
-def compile_new(node_cls, dynamic=False, pluggable=False, root=False):
-    if pluggable:
-        class_determinant = _get_pluggable_class
-    elif dynamic:
-        class_determinant = _get_dynamic_class
-    else:
-        class_determinant = _node_determinant
-
-    def __new__(_cls, *args, _parent=None, _key=None, **kwargs):
-        ncls = class_determinant(_cls, kwargs)
-        instance = object.__new__(ncls)
-        instance._config_pos_init = bool(len(args))
-        _set_pk(instance, _parent, _key)
-        if root:
-            instance._config_isfinished = False
-        instance.__post_new__(**kwargs)
-        if _cls is not ncls:
-            instance.__init__(*args, **kwargs)
-        return instance
-
-    __new__.class_determinant = class_determinant
-
-    return __new__
-
-
-def _set_pk(obj, parent, key):
-    obj._config_parent = parent
-    obj._config_key = key
-    if not hasattr(obj, "_config_attr_order"):
-        obj._config_attr_order = []
-    if not hasattr(obj, "_config_state"):
-        obj._config_state = {}
-    for a in get_config_attributes(obj.__class__).values():
-        if a.key:
-            from ._attrs import _setattr
-
-            _setattr(obj, a.attr_name, key)
-
-
-def _missing_requirements(instance, attr, kwargs):
-    # We use `self.__class__`, not `cls`, to get the proper child class.
-    cls = instance.__class__
-    dynamic_root = getattr(cls, "_config_dynamic_root", None)
-    kwargs = NodeKwargs(instance, kwargs)
-    if dynamic_root is not None:
-        dynamic_attr = dynamic_root._config_dynamic_attr
-        # If we are checking the dynamic attribute, but we're already a dynamic subclass,
-        # we skip the required check.
-        return (
-            attr.attr_name == dynamic_attr
-            and cls is dynamic_root
-            and attr.required(kwargs)
-        ) or (attr.attr_name != dynamic_attr and attr.required(kwargs))
-    else:
-        return attr.required(kwargs)
-
-
-def compile_postnew(cls):
-    def __post_new__(self, _parent=None, _key=None, **kwargs):
-        attrs = get_config_attributes(self.__class__)
-        self._config_attr_order = list(kwargs.keys())
-        catch_attrs = [a for a in attrs.values() if hasattr(a, "__catch__")]
-        leftovers = kwargs.copy()
-        values = {}
-        for attr in attrs.values():
-            name = attr.attr_name
-            value = values[name] = leftovers.pop(name, None)
-            try:
-                if _missing_requirements(self, attr, kwargs) and value is None:
-                    raise RequirementError(f"Missing required attribute '{name}'")
-            except RequirementError as e:
-                # Catch both our own and possible `attr.required` RequirementErrors
-                # and set the node detail before passing it on
-                e.node = self
-                raise
-        for attr in attrs.values():
-            name = attr.attr_name
-            if attr.key and attr.attr_name not in kwargs:
-                # If this is a "key" attribute, and the user didn't overwrite it,
-                # set the attribute to the config key
-                setattr(self, name, self._config_key)
-                attr.flag_pristine(self)
-            elif (value := values[name]) is None:
-                if _is_settable_attr(attr):
-                    setattr(self, name, attr.get_default())
-                attr.flag_pristine(self)
-            else:
-                setattr(self, name, value)
-                attr.flag_dirty(self)
-        for key, value in leftovers.items():
-            try:
-                _try_catch_attrs(self, catch_attrs, key, value)
-            except UncaughtAttributeError:
-                try:
-                    setattr(self, key, value)
-                except AttributeError:
-                    raise AttributeError(
-                        f"Configuration attribute key '{key}' conflicts with"
-                        + f" readonly class attribute on `{self.__class__.__module__}"
-                        + f".{self.__class__.__name__}`."
-                    ) from None
-                raise ConfigurationError(f"Unknown attribute:  '{key}'") from None
-
-    return __post_new__
-
-
-def wrap_root_postnew(post_new):
-    def __post_new__(self, *args, _parent=None, _key=None, _store=None, **kwargs):
-        if not hasattr(self, "_meta"):
-            self._meta = {"path": None, "produced": True}
-
-        try:
-            # Root node bootstrapping sequence
-            _bootstrap_components(kwargs.get("components", []), file_store=_store)
-            warn_missing_packages(kwargs.get("packages", []))
-        except Exception as e:
-            raise BootError("Failed to bootstrap configuration.") from e
-
-        try:
-            with warnings.catch_warnings(record=True) as log:
-                try:
-                    post_new(self, *args, _parent=None, _key=None, **kwargs)
-                except (CastError, RequirementError) as e:
-                    _bubble_up_exc(e, self._meta)
-                self._config_isfinished = True
-                _resolve_references(self)
-        finally:
-            _bubble_up_warnings(log)
-
-    return __post_new__
-
-
-def _is_settable_attr(attr):
-    return not hasattr(attr, "fget") or attr.fset
-
-
-def _bubble_up_exc(exc, meta):
-    if hasattr(exc, "node") and exc.node is not None:
-        node = " in " + exc.node.get_node_name()
-    else:
-        node = ""
-    attr = f".{exc.attr}" if hasattr(exc, "attr") and exc.attr else ""
-    errr.wrap(type(exc), exc, append=node + attr)
-
-
-def _bubble_up_warnings(log):
-    for w in log:
-        m = w.message
-        if hasattr(m, "node"):
-            # Unpack the inner Warning that was passed instead of the warning msg
-            attr = f".{m.attr.attr_name}" if hasattr(m, "attr") else ""
-            warn(str(m) + " in " + m.node.get_node_name() + attr, type(m), stacklevel=4)
-        else:
-            warn(str(m), w.category, stacklevel=4)
-
-
-def _bootstrap_components(components, file_store=None):
-    from ..storage._files import CodeDependencyNode
-
-    for component in components:
-        component_node = CodeDependencyNode(component)
-        component_node.file_store = file_store
-        component_node.load_object()
-
-
-def get_config_attributes(cls):
-    attrs = {}
-    if not isinstance(cls, type):
-        cls = cls.__class__
-    for p_cls in reversed(cls.__mro__):
-        if hasattr(p_cls, "_config_attrs"):
-            attrs.update(p_cls._config_attrs)
-        else:
-            # Scrape for mixin config attributes
-            from ._attrs import ConfigurationAttribute
-
-            attrs.update(
-                {
-                    key: attr
-                    for key, attr in p_cls.__dict__.items()
-                    if isinstance(attr, ConfigurationAttribute)
-                }
-            )
-        for unset in getattr(p_cls, "_config_unset", []):
-            attrs.pop(unset, None)
-    return attrs
-
-
-def _get_node_name(self):
-    name = ".<missing>"
-    if getattr(self, "attr_name", None) is not None:
-        name = "." + str(self.attr_name)
-    if getattr(self, "_config_key", None) is not None:
-        name = "." + str(self._config_key)
-    if hasattr(self, "_config_index"):
-        if self._config_index is None:
-            return "{removed}"
-        else:
-            name = "[" + str(self._config_index) + "]"
-    if getattr(self, "name", None) is not None:
-        name = "." + self.name
-    if getattr(self, "_config_parent", None):
-        return self._config_parent.get_node_name() + name
-    else:
-        return "{standalone}" + name
-
-
-def make_get_node_name(node_cls, root):
-    if root:
-        node_cls.get_node_name = lambda self: r"{root}"
-    else:
-        node_cls.get_node_name = _get_node_name
-
-
-class UncaughtAttributeError(Exception):
-    pass
-
-
-def _try_catch_attrs(node, catchers, key, value):
-    # See if any of the attributes in the node can catch the value of an unknown key in
-    # the configuration section. If none of them catch the value, raise an
-    # `UncaughtAttributeError`
-    for attr in catchers:
-        try:
-            _try_catch(attr.__catch__, node, key, value)
-            break
-        except UncaughtAttributeError:
-            pass
-    else:
-        raise UncaughtAttributeError()
-
-
-def _try_catch(catch, node, key, value):
-    try:
-        return catch(node, key, value)
-    except Exception:
-        raise UncaughtAttributeError()
-
-
-def _get_dynamic_class(node_cls, kwargs):
-    if node_cls is not node_cls._config_dynamic_root:
-        # When the node is already a subclass of its dynamic root, we don't need to cast
-        # it anymore.
-        return node_cls
-
-    attr_name = node_cls._config_dynamic_attr
-    dynamic_attr = getattr(node_cls, attr_name)
-    if attr_name in kwargs:
-        loaded_cls_name = kwargs[attr_name]
-    elif dynamic_attr.required(kwargs):
-        raise RequirementError(f"Dynamic node must contain a '{attr_name}' attribute")
-    elif dynamic_attr.should_call_default():  # pragma: nocover
-        loaded_cls_name = dynamic_attr.default()
-    else:
-        # Fall back to the default value, or the current class.
-        loaded_cls_name = dynamic_attr.default or node_cls.__name__
-    module_path = ["__main__", node_cls.__module__]
-    classmap = get_classmap(node_cls)
-    interface = getattr(node_cls, "_config_dynamic_root")
-    try:
-        dynamic_cls = _load_class(
-            loaded_cls_name, module_path, interface=interface, classmap=classmap
-        )
-    except DynamicClassInheritanceError:
-        mapped_class_msg = _get_mapped_class_msg(loaded_cls_name, classmap)
-        raise UnfitClassCastError(
-            "'{}'{} is not a valid class as it does not inherit from {}".format(
-                loaded_cls_name,
-                mapped_class_msg,
-                node_cls.__name__,
-            )
-        ) from None
-    except DynamicClassError:
-        mapped_class_msg = _get_mapped_class_msg(loaded_cls_name, classmap)
-        raise UnresolvedClassCastError(
-            f"Could not resolve '{loaded_cls_name}'{mapped_class_msg} to a class"
-        ) from None
-    return dynamic_cls
-
-
-def _get_pluggable_class(node_cls, kwargs):
-    plugin_label = node_cls._config_plugin_name or node_cls.__name__
-    if node_cls._config_plugin_key not in kwargs:
-        raise CastError(
-            "Pluggable node must contain a '{}' attribute to select a {}".format(
-                node_cls._config_plugin_key,
-                plugin_label,
-            )
-        )
-    plugin_name = kwargs[node_cls._config_plugin_key]
-    plugins = node_cls.__plugins__()
-    if plugin_name not in plugins:
-        raise PluginError("Unknown {} '{}'".format(plugin_label, plugin_name))
-    plugin_cls = plugins[plugin_name]
-    # TODO: Enforce class inheritance
-    return plugin_cls
-
-
-def _get_mapped_class_msg(loaded_cls_name, classmap):
-    if classmap and loaded_cls_name in classmap:
-        return " (mapped to '{}')".format(classmap[loaded_cls_name])
-    else:
-        return ""
-
-
-def _load_class(cfg_classname, module_path, interface=None, classmap=None):
-    if classmap and cfg_classname in classmap:
-        cfg_classname = classmap[cfg_classname]
-    if inspect.isclass(cfg_classname):
-        class_ref = cfg_classname
-        class_name = cfg_classname.__name__
-    else:
-        class_ref = _load_object(cfg_classname, module_path)
-        class_name = class_ref.__name__
-
-    def qualname(cls):
-        return cls.__module__ + "." + cls.__name__
-
-    if interface and not issubclass(class_ref, interface):
-        raise DynamicClassInheritanceError(
-            "Dynamic class '{}' must derive from {}".format(
-                class_name, qualname(interface)
-            )
-        )
-    return class_ref
-
-
-def _load_object(object_path, module_path):
-    class_parts = object_path.split(".")
-    object_name = class_parts[-1]
-    module_name = ".".join(class_parts[:-1])
-    if not module_name:
-        object_ref = _search_module_path(object_name, module_path, object_path)
-    else:
-        object_ref = _get_module_object(object_name, module_name, object_path)
-
-    return object_ref
-
-
-def _search_module_path(class_name, module_path, cfg_classname):
-    for module_name in module_path:
-        module_dict = sys.modules[module_name].__dict__
-        if class_name in module_dict:
-            return module_dict[class_name]
-    raise DynamicObjectNotFoundError("Class not found: " + cfg_classname)
-
-
-def _get_module_object(object_name, module_name, object_path):
-    sys.path.append(os.getcwd())
-    try:
-        module_ref = importlib.import_module(module_name)
-    finally:
-        tmp = list(reversed(sys.path))
-        tmp.remove(os.getcwd())
-        sys.path = list(reversed(tmp))
-    try:
-        return getattr(module_ref, object_name)
-    except Exception:
-        raise DynamicObjectNotFoundError(f"'{object_path}' not found.")
-
-
-def make_dictable(node_cls):
-    def __contains__(self, attr):
-        return attr in get_config_attributes(self.__class__)
-
-    def __getitem__(self, attr):
-        if attr in get_config_attributes(self.__class__):
-            return getattr(self, attr)
-        else:
-            raise KeyError(attr)
-
-    def __iter__(self):
-        return (attr for attr in get_config_attributes(self.__class__))
-
-    node_cls.__contains__ = __contains__
-    node_cls.__getitem__ = __getitem__
-    node_cls.__iter__ = __iter__
-
-
-def make_tree(node_cls):
-    def get_tree(instance):
-        if hasattr(instance, "__inv__") and not getattr(instance, "_config_inv", None):
-            instance._config_inv = True
-            inv = instance.__inv__()
-            instance._config_inv = False
-            return inv
-        attrs = get_config_attributes(instance.__class__)
-        catch_attrs = [a for a in attrs.values() if hasattr(a, "__catch__")]
-        tree = {}
-        for name in instance._config_attr_order:
-            if name in attrs:
-                attr = attrs[name]
-                if attr.is_dirty(instance):
-                    value = attr.tree(instance)
-                else:
-                    value = None
-            else:
-                for catcher in catch_attrs:
-                    if catcher.contains(instance, name):
-                        value = catcher.tree_callback(instance, name)
-                        break
-                else:
-                    value = getattr(instance, name, None)
-            if value is not None:
-                tree[name] = value
-        return tree
-
-    node_cls.__tree__ = get_tree
-
-
-def make_copyable(node_cls):
-    def loc_copy(instance, memo=None):
-        return type(instance)(instance.__tree__())
-
-    node_cls.__copy__ = loc_copy
-    node_cls.__deepcopy__ = loc_copy
-
-
-def walk_node_attributes(node):
-    """
-    Walk over all of the child configuration nodes and attributes of ``node``.
-
-    :returns: attribute, node, parents
-    :rtype: Tuple[:class:`~.config.ConfigurationAttribute`, Any, Tuple]
-    """
-    attrs = get_config_attributes(node)
-    if not attrs:
-        if hasattr(node, "_config_attr"):
-            attrs = _get_walkable_iterator(node)
-        else:
-            return
-    for attr in attrs.values():
-        yield node, attr
-        # Yield but don't follow references.
-        if hasattr(attr, "__ref__"):
-            continue
-        child = attr.__get__(node, node.__class__)
-        yield from walk_node_attributes(child)
-
-
-def walk_nodes(node):
-    """
-    Walk over all of the child configuration nodes of ``node``.
-
-    :returns: node generator
-    :rtype: Any
-    """
-    if hasattr(node.__class__, "_config_attrs"):
-        attrs = node.__class__._config_attrs
-    elif hasattr(node, "_config_attr"):
-        attrs = _get_walkable_iterator(node)
-    else:
-        return
-    yield node
-    for attr in attrs.values():
-        # Yield but don't follow references.
-        if hasattr(attr, "__ref__"):
-            continue
-        child = attr.__get__(node, node.__class__)
-        yield from walk_nodes(child)
-
-
-def walk_node_values(start_node):
-    for node, attr in walk_node_attributes(start_node):
-        yield node, attr.attr_name, attr.__get__(node, node.__class__)
-
-
-def _resolve_references(root, start=None, /):
-    from ._attrs import _setattr
-
-    if start is None:
-        start = root
-    if root._config_isfinished:
-        for node, attr in walk_node_attributes(root):
-            if hasattr(attr, "__ref__"):
-                ref = attr.__ref__(node, root)
-                _setattr(node, attr.attr_name, ref)
-
-
-class WalkIterDescriptor:
-    def __init__(self, n, v):
-        self.attr_name = n
-        self.v = v
-
-    def __get__(self, instance, cls):
-        return self.v
-
-
-def _get_walkable_iterator(node):
-    if isinstance(node, dict):
-        walkiter = {}
-        for name, value in node.items():
-            walkiter[name] = WalkIterDescriptor(name, value)
-        return walkiter
-    elif isinstance(node, list):
-        walkiter = {}
-        for i, value in enumerate(node):
-            walkiter[i] = WalkIterDescriptor(i, value)
-        return walkiter
-
-
-_classmap_registry = defaultdict(dict)
-
-
-@functools.cache
-def load_component_plugins():
-    from ..plugins import discover
-
-    plugins = discover("components")
-    for plugin in plugins.values():
-        if isinstance(plugin, dict):
-            for class_name, classmap in plugin.items():
-                register_classmap(class_name, classmap)
-
-    return plugins
-
-
-def register_classmap(cls_name, classmap):
-    _classmap_registry[cls_name].update(classmap)
-
-
-def get_classmap(cls):
-    load_component_plugins()
-    classmap = getattr(cls, "_config_dynamic_classmap", {})
-    classmap.update(_classmap_registry[get_qualified_class_name(cls)])
-    return classmap
-
-
-MISSING = object()
+import functools
+import importlib
+import inspect
+import os
+import sys
+import types
+import warnings
+from collections import defaultdict
+from re import sub
+
+import errr
+
+from .._package_spec import warn_missing_packages
+from .._util import get_qualified_class_name
+from ..exceptions import (
+    BootError,
+    CastError,
+    ConfigurationError,
+    DynamicClassError,
+    DynamicClassInheritanceError,
+    DynamicObjectNotFoundError,
+    PluginError,
+    RequirementError,
+    UnfitClassCastError,
+    UnresolvedClassCastError,
+)
+from ..reporting import warn
+from ._hooks import overrides
+
+
+def _has_own_init(meta_subject, kwargs):
+    try:
+        determined_class = meta_subject.__new__.class_determinant(meta_subject, kwargs)
+        return overrides(determined_class, "__init__", mro=True)
+    except Exception:
+        return overrides(meta_subject, "__init__", mro=True)
+
+
+def make_metaclass(cls):
+    # We make a `NodeMeta` class for each decorated node class, in compliance with any
+    # metaclasses they might already have (to prevent metaclass confusion).
+    # The purpose of the metaclass is to rewrite `__new__` and `__init__` arguments,
+    # and to always call `__new__` and `__init__` in the same manner.
+    # The metaclass makes it so that there are 3 overloaded constructor forms:
+    #
+    # MyNode({ <config dict values> })
+    # MyNode(example="attr", values="here")
+    # ParentNode(me=MyNode(...))
+    #
+    # The third makes it that type handling and other types of casting opt out early
+    # and keep the object reference that the user gives them
+    class ConfigArgRewrite:
+        def __call__(meta_subject, *args, _parent=None, _key=None, **kwargs):
+            has_own_init = _has_own_init(meta_subject, kwargs)
+            # Rewrite the arguments
+            primer = args[0] if args else None
+            if isinstance(primer, meta_subject):
+                _set_pk(primer, _parent, _key)
+                return primer
+            elif isinstance(primer, dict):
+                args = args[1:]
+                primed = primer.copy()
+                primed.update(kwargs)
+                kwargs = primed
+            elif primer is not None and not has_own_init:
+                # If we're dealing with a typical config node, the primer should be a dict
+                # or already precast node. If it is not, we consider it invalid input,
+                # unless the user has specified its own `__init__` function and will deal
+                # with the input arguments there.
+                raise ValueError(f"Unexpected positional argument '{primer}'")
+            # Call the base class's new with internal arguments
+            instance = meta_subject.__new__(
+                meta_subject, *args, _parent=_parent, _key=_key, **kwargs
+            )
+            instance._config_pos_init = getattr(instance, "_config_pos_init", False)
+            # Call the end user's __init__ with the rewritten arguments, if one is defined
+            if has_own_init:
+                sig = inspect.signature(instance.__init__)
+                try:
+                    # Check whether the arguments match the signature. We use `sig.bind`
+                    # so that the function isn't actually called, as this could mask
+                    # `TypeErrors` that occur inside the function.
+                    sig.bind(*args, **kwargs)
+                except TypeError as e:
+                    # Since the user might not know where all these additional arguments
+                    # are coming from, inform them that config nodes get passed their
+                    # config attrs, and how to correctly override __init__.
+                    Param = inspect.Parameter
+                    help_params = {"self": Param("self", Param.POSITIONAL_OR_KEYWORD)}
+                    help_params.update(sig._parameters)
+                    help_params["kwargs"] = Param("kwargs", Param.VAR_KEYWORD)
+                    sig._parameters = help_params
+                    raise TypeError(
+                        f"`{instance.__init__.__module__}.__init__` {e}."
+                        + " When overriding `__init__` on config nodes, do not define"
+                        + " any positional arguments, and catch any additional"
+                        + " configuration attributes that are passed as keyword arguments"
+                        + f": e.g. 'def __init__{sig}'"
+                    ) from None
+                else:
+                    instance._config_pos_init = bool(len(args))
+                    instance.__init__(*args, **kwargs)
+            return instance
+
+    # Avoid metaclass conflicts by prepending our rewrite class to existing metaclass MRO
+    class NodeMeta(ConfigArgRewrite, *cls.__class__.__mro__):
+        def __new__(cls, *args, **kwargs):
+            rcls = super().__new__(cls, *args, **kwargs)
+            # `__init_subclass__` refused to be called with correct subclass, so call
+            # it ourselves.
+            if hasattr(rcls.__bases__[0], "_cfgnode_replaced_ics"):
+                rcls.__bases__[0]._cfgnode_replaced_ics(rcls, **kwargs)
+            return rcls
+
+    return NodeMeta
+
+
+class NodeKwargs(dict):
+    def __init__(self, instance, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.is_shortform = getattr(instance, "_config_pos_init", False)
+
+
+def compose_nodes(*node_classes):
+    """
+    Create a composite mixin class of the given classes. Inherit from the returned class
+    to inherit from more than one node class.
+
+    """
+    meta = type("ComposedMetaclass", tuple(type(cls) for cls in node_classes), {})
+    return meta("CompositionMixin", node_classes, {})
+
+
+def compile_class(cls):
+    cls_dict = dict(cls.__dict__)
+    if "__dict__" in cls_dict:
+        del cls_dict["__dict__"]
+    if "__weakref__" in cls_dict:
+        del cls_dict["__weakref__"]
+    ncls = make_metaclass(cls)(cls.__name__, cls.__bases__, cls_dict)
+    for method in ncls.__dict__.values():
+        _replace_closure_cells(method, cls, ncls)
+
+    # Shitty hack, for some reason I couldn't find a way to override the first argument
+    # of `__init_subclass__` methods, that would otherwise work on other classmethods,
+    # so we noop the actual `__init_subclass__` and we call `__init_subclass__` ourselves
+    # from the metaclass' `__new__` method, where the argument replacement works as usual.
+    if (
+        hasattr(ncls, "__init_subclass__")
+        and "__init_subclass__" in ncls.__dict__
+        and not isinstance(ncls.__init_subclass__, types.BuiltinFunctionType)
+    ):
+        ncls._cfgnode_replaced_ics = ncls.__init_subclass__.__func__
+        ncls.__init_subclass__ = lambda *args, **kwargs: None
+    classmap = getattr(ncls, "_config_dynamic_classmap", None)
+    if classmap is not None:
+        # Replace the reference to the old class with the new class.
+        # The auto classmap entry is added in `__init_subclass__`, which happens before
+        # we replace the class.
+        for k, v in classmap.items():
+            if v is cls:
+                classmap[k] = ncls
+    return ncls
+
+
+def _replace_closure_cells(method, old, new):
+    cl = getattr(method, "__closure__", None) or []
+    for cell in cl:
+        if cell.cell_contents is old:
+            cell.cell_contents = new
+        elif inspect.isfunction(cell.cell_contents):
+            _replace_closure_cells(cell.cell_contents, old, new)
+
+
+def compile_isc(node_cls, dynamic_config):
+    if not dynamic_config or not dynamic_config.auto_classmap:
+        return node_cls.__init_subclass__
+
+    from ._hooks import overrides
+
+    def dud(*args, **kwargs):
+        pass
+
+    if overrides(node_cls, "__init_subclass__"):
+        f = node_cls.__init_subclass__
+    else:
+        f = dud
+
+    def __init_subclass__(cls, classmap_entry=MISSING, **kwargs):
+        super(node_cls, cls).__init_subclass__(**kwargs)
+        if classmap_entry is MISSING:
+            classmap_entry = _snake_case(cls.__name__)
+        if classmap_entry is not None:
+            node_cls._config_dynamic_classmap[classmap_entry] = cls
+        f(**kwargs)
+
+    return classmethod(__init_subclass__)
+
+
+def _snake_case(s):
+    return "_".join(
+        sub("([A-Z][a-z]+)", r" \1", sub("([A-Z]+)", r" \1", s.replace("-", " "))).split()
+    ).lower()
+
+
+def _node_determinant(cls, kwargs):
+    return cls
+
+
+def compile_new(node_cls, dynamic=False, pluggable=False, root=False):
+    if pluggable:
+        class_determinant = _get_pluggable_class
+    elif dynamic:
+        class_determinant = _get_dynamic_class
+    else:
+        class_determinant = _node_determinant
+
+    def __new__(_cls, *args, _parent=None, _key=None, **kwargs):
+        ncls = class_determinant(_cls, kwargs)
+        instance = object.__new__(ncls)
+        instance._config_pos_init = bool(len(args))
+        _set_pk(instance, _parent, _key)
+        if root:
+            instance._config_isfinished = False
+        instance.__post_new__(**kwargs)
+        if _cls is not ncls:
+            instance.__init__(*args, **kwargs)
+        return instance
+
+    __new__.class_determinant = class_determinant
+
+    return __new__
+
+
+def _set_pk(obj, parent, key):
+    obj._config_parent = parent
+    obj._config_key = key
+    if not hasattr(obj, "_config_attr_order"):
+        obj._config_attr_order = []
+    if not hasattr(obj, "_config_state"):
+        obj._config_state = {}
+    for a in get_config_attributes(obj.__class__).values():
+        if a.key:
+            from ._attrs import _setattr
+
+            _setattr(obj, a.attr_name, key)
+
+
+def _missing_requirements(instance, attr, kwargs):
+    # We use `self.__class__`, not `cls`, to get the proper child class.
+    cls = instance.__class__
+    dynamic_root = getattr(cls, "_config_dynamic_root", None)
+    kwargs = NodeKwargs(instance, kwargs)
+    if dynamic_root is not None:
+        dynamic_attr = dynamic_root._config_dynamic_attr
+        # If we are checking the dynamic attribute, but we're already a dynamic subclass,
+        # we skip the required check.
+        return (
+            attr.attr_name == dynamic_attr
+            and cls is dynamic_root
+            and attr.required(kwargs)
+        ) or (attr.attr_name != dynamic_attr and attr.required(kwargs))
+    else:
+        return attr.required(kwargs)
+
+
+def compile_postnew(cls):
+    def __post_new__(self, _parent=None, _key=None, **kwargs):
+        attrs = get_config_attributes(self.__class__)
+        self._config_attr_order = list(kwargs.keys())
+        catch_attrs = [a for a in attrs.values() if hasattr(a, "__catch__")]
+        leftovers = kwargs.copy()
+        values = {}
+        for attr in attrs.values():
+            name = attr.attr_name
+            value = values[name] = leftovers.pop(name, None)
+            try:
+                if _missing_requirements(self, attr, kwargs) and value is None:
+                    raise RequirementError(f"Missing required attribute '{name}'")
+            except RequirementError as e:
+                # Catch both our own and possible `attr.required` RequirementErrors
+                # and set the node detail before passing it on
+                e.node = self
+                raise
+        for attr in attrs.values():
+            name = attr.attr_name
+            if attr.key and attr.attr_name not in kwargs:
+                # If this is a "key" attribute, and the user didn't overwrite it,
+                # set the attribute to the config key
+                setattr(self, name, self._config_key)
+                attr.flag_pristine(self)
+            elif (value := values[name]) is None:
+                if _is_settable_attr(attr):
+                    setattr(self, name, attr.get_default())
+                attr.flag_pristine(self)
+            else:
+                setattr(self, name, value)
+                attr.flag_dirty(self)
+        for key, value in leftovers.items():
+            try:
+                _try_catch_attrs(self, catch_attrs, key, value)
+            except UncaughtAttributeError:
+                try:
+                    setattr(self, key, value)
+                except AttributeError:
+                    raise AttributeError(
+                        f"Configuration attribute key '{key}' conflicts with"
+                        + f" readonly class attribute on `{self.__class__.__module__}"
+                        + f".{self.__class__.__name__}`."
+                    ) from None
+                raise ConfigurationError(f"Unknown attribute:  '{key}'") from None
+
+    return __post_new__
+
+
+def wrap_root_postnew(post_new):
+    def __post_new__(self, *args, _parent=None, _key=None, _store=None, **kwargs):
+        if not hasattr(self, "_meta"):
+            self._meta = {"path": None, "produced": True}
+
+        try:
+            # Root node bootstrapping sequence
+            _bootstrap_components(kwargs.get("components", []), file_store=_store)
+            warn_missing_packages(kwargs.get("packages", []))
+        except Exception as e:
+            raise BootError("Failed to bootstrap configuration.") from e
+
+        try:
+            with warnings.catch_warnings(record=True) as log:
+                try:
+                    post_new(self, *args, _parent=None, _key=None, **kwargs)
+                except (CastError, RequirementError) as e:
+                    _bubble_up_exc(e, self._meta)
+                self._config_isfinished = True
+                _resolve_references(self)
+        finally:
+            _bubble_up_warnings(log)
+
+    return __post_new__
+
+
+def _is_settable_attr(attr):
+    return not hasattr(attr, "fget") or attr.fset
+
+
+def _bubble_up_exc(exc, meta):
+    if hasattr(exc, "node") and exc.node is not None:
+        node = " in " + exc.node.get_node_name()
+    else:
+        node = ""
+    attr = f".{exc.attr}" if hasattr(exc, "attr") and exc.attr else ""
+    errr.wrap(type(exc), exc, append=node + attr)
+
+
+def _bubble_up_warnings(log):
+    for w in log:
+        m = w.message
+        if hasattr(m, "node"):
+            # Unpack the inner Warning that was passed instead of the warning msg
+            attr = f".{m.attr.attr_name}" if hasattr(m, "attr") else ""
+            warn(str(m) + " in " + m.node.get_node_name() + attr, type(m), stacklevel=4)
+        else:
+            warn(str(m), w.category, stacklevel=4)
+
+
+def _bootstrap_components(components, file_store=None):
+    from ..storage._files import CodeDependencyNode
+
+    for component in components:
+        component_node = CodeDependencyNode(component)
+        component_node.file_store = file_store
+        component_node.load_object()
+
+
+def get_config_attributes(cls):
+    attrs = {}
+    if not isinstance(cls, type):
+        cls = cls.__class__
+    for p_cls in reversed(cls.__mro__):
+        if hasattr(p_cls, "_config_attrs"):
+            attrs.update(p_cls._config_attrs)
+        else:
+            # Scrape for mixin config attributes
+            from ._attrs import ConfigurationAttribute
+
+            attrs.update(
+                {
+                    key: attr
+                    for key, attr in p_cls.__dict__.items()
+                    if isinstance(attr, ConfigurationAttribute)
+                }
+            )
+        for unset in getattr(p_cls, "_config_unset", []):
+            attrs.pop(unset, None)
+    return attrs
+
+
+def _get_node_name(self):
+    name = ".<missing>"
+    if getattr(self, "attr_name", None) is not None:
+        name = "." + str(self.attr_name)
+    if getattr(self, "_config_key", None) is not None:
+        name = "." + str(self._config_key)
+    if hasattr(self, "_config_index"):
+        if self._config_index is None:
+            return "{removed}"
+        else:
+            name = "[" + str(self._config_index) + "]"
+    if getattr(self, "name", None) is not None:
+        name = "." + self.name
+    if getattr(self, "_config_parent", None):
+        return self._config_parent.get_node_name() + name
+    else:
+        return "{standalone}" + name
+
+
+def make_get_node_name(node_cls, root):
+    if root:
+        node_cls.get_node_name = lambda self: r"{root}"
+    else:
+        node_cls.get_node_name = _get_node_name
+
+
+class UncaughtAttributeError(Exception):
+    pass
+
+
+def _try_catch_attrs(node, catchers, key, value):
+    # See if any of the attributes in the node can catch the value of an unknown key in
+    # the configuration section. If none of them catch the value, raise an
+    # `UncaughtAttributeError`
+    for attr in catchers:
+        try:
+            _try_catch(attr.__catch__, node, key, value)
+            break
+        except UncaughtAttributeError:
+            pass
+    else:
+        raise UncaughtAttributeError()
+
+
+def _try_catch(catch, node, key, value):
+    try:
+        return catch(node, key, value)
+    except Exception:
+        raise UncaughtAttributeError()
+
+
+def _get_dynamic_class(node_cls, kwargs):
+    if node_cls is not node_cls._config_dynamic_root:
+        # When the node is already a subclass of its dynamic root, we don't need to cast
+        # it anymore.
+        return node_cls
+
+    attr_name = node_cls._config_dynamic_attr
+    dynamic_attr = getattr(node_cls, attr_name)
+    if attr_name in kwargs:
+        loaded_cls_name = kwargs[attr_name]
+    elif dynamic_attr.required(kwargs):
+        raise RequirementError(f"Dynamic node must contain a '{attr_name}' attribute")
+    elif dynamic_attr.should_call_default():  # pragma: nocover
+        loaded_cls_name = dynamic_attr.default()
+    else:
+        # Fall back to the default value, or the current class.
+        loaded_cls_name = dynamic_attr.default or node_cls.__name__
+    module_path = ["__main__", node_cls.__module__]
+    classmap = get_classmap(node_cls)
+    interface = getattr(node_cls, "_config_dynamic_root")
+    try:
+        dynamic_cls = _load_class(
+            loaded_cls_name, module_path, interface=interface, classmap=classmap
+        )
+    except DynamicClassInheritanceError:
+        mapped_class_msg = _get_mapped_class_msg(loaded_cls_name, classmap)
+        raise UnfitClassCastError(
+            "'{}'{} is not a valid class as it does not inherit from {}".format(
+                loaded_cls_name,
+                mapped_class_msg,
+                node_cls.__name__,
+            )
+        ) from None
+    except DynamicClassError:
+        mapped_class_msg = _get_mapped_class_msg(loaded_cls_name, classmap)
+        raise UnresolvedClassCastError(
+            f"Could not resolve '{loaded_cls_name}'{mapped_class_msg} to a class"
+        ) from None
+    return dynamic_cls
+
+
+def _get_pluggable_class(node_cls, kwargs):
+    plugin_label = node_cls._config_plugin_name or node_cls.__name__
+    if node_cls._config_plugin_key not in kwargs:
+        raise CastError(
+            "Pluggable node must contain a '{}' attribute to select a {}".format(
+                node_cls._config_plugin_key,
+                plugin_label,
+            )
+        )
+    plugin_name = kwargs[node_cls._config_plugin_key]
+    plugins = node_cls.__plugins__()
+    if plugin_name not in plugins:
+        raise PluginError("Unknown {} '{}'".format(plugin_label, plugin_name))
+    plugin_cls = plugins[plugin_name]
+    # TODO: Enforce class inheritance
+    return plugin_cls
+
+
+def _get_mapped_class_msg(loaded_cls_name, classmap):
+    if classmap and loaded_cls_name in classmap:
+        return " (mapped to '{}')".format(classmap[loaded_cls_name])
+    else:
+        return ""
+
+
+def _load_class(cfg_classname, module_path, interface=None, classmap=None):
+    if classmap and cfg_classname in classmap:
+        cfg_classname = classmap[cfg_classname]
+    if inspect.isclass(cfg_classname):
+        class_ref = cfg_classname
+        class_name = cfg_classname.__name__
+    else:
+        class_ref = _load_object(cfg_classname, module_path)
+        class_name = class_ref.__name__
+
+    def qualname(cls):
+        return cls.__module__ + "." + cls.__name__
+
+    if interface and not issubclass(class_ref, interface):
+        raise DynamicClassInheritanceError(
+            "Dynamic class '{}' must derive from {}".format(
+                class_name, qualname(interface)
+            )
+        )
+    return class_ref
+
+
+def _load_object(object_path, module_path):
+    class_parts = object_path.split(".")
+    object_name = class_parts[-1]
+    module_name = ".".join(class_parts[:-1])
+    if not module_name:
+        object_ref = _search_module_path(object_name, module_path, object_path)
+    else:
+        object_ref = _get_module_object(object_name, module_name, object_path)
+
+    return object_ref
+
+
+def _search_module_path(class_name, module_path, cfg_classname):
+    for module_name in module_path:
+        module_dict = sys.modules[module_name].__dict__
+        if class_name in module_dict:
+            return module_dict[class_name]
+    raise DynamicObjectNotFoundError("Class not found: " + cfg_classname)
+
+
+def _get_module_object(object_name, module_name, object_path):
+    sys.path.append(os.getcwd())
+    try:
+        module_ref = importlib.import_module(module_name)
+    finally:
+        tmp = list(reversed(sys.path))
+        tmp.remove(os.getcwd())
+        sys.path = list(reversed(tmp))
+    try:
+        return getattr(module_ref, object_name)
+    except Exception:
+        raise DynamicObjectNotFoundError(f"'{object_path}' not found.")
+
+
+def make_dictable(node_cls):
+    def __contains__(self, attr):
+        return attr in get_config_attributes(self.__class__)
+
+    def __getitem__(self, attr):
+        if attr in get_config_attributes(self.__class__):
+            return getattr(self, attr)
+        else:
+            raise KeyError(attr)
+
+    def __iter__(self):
+        return (attr for attr in get_config_attributes(self.__class__))
+
+    node_cls.__contains__ = __contains__
+    node_cls.__getitem__ = __getitem__
+    node_cls.__iter__ = __iter__
+
+
+def make_tree(node_cls):
+    def get_tree(instance):
+        if hasattr(instance, "__inv__") and not getattr(instance, "_config_inv", None):
+            instance._config_inv = True
+            inv = instance.__inv__()
+            instance._config_inv = False
+            return inv
+        attrs = get_config_attributes(instance.__class__)
+        catch_attrs = [a for a in attrs.values() if hasattr(a, "__catch__")]
+        tree = {}
+        for name in instance._config_attr_order:
+            if name in attrs:
+                attr = attrs[name]
+                if attr.is_dirty(instance):
+                    value = attr.tree(instance)
+                else:
+                    value = None
+            else:
+                for catcher in catch_attrs:
+                    if catcher.contains(instance, name):
+                        value = catcher.tree_callback(instance, name)
+                        break
+                else:
+                    value = getattr(instance, name, None)
+            if value is not None:
+                tree[name] = value
+        return tree
+
+    node_cls.__tree__ = get_tree
+
+
+def make_copyable(node_cls):
+    def loc_copy(instance, memo=None):
+        return type(instance)(instance.__tree__())
+
+    node_cls.__copy__ = loc_copy
+    node_cls.__deepcopy__ = loc_copy
+
+
+def walk_node_attributes(node):
+    """
+    Walk over all of the child configuration nodes and attributes of ``node``.
+
+    :returns: attribute, node, parents
+    :rtype: Tuple[:class:`~.config.ConfigurationAttribute`, Any, Tuple]
+    """
+    attrs = get_config_attributes(node)
+    if not attrs:
+        if hasattr(node, "_config_attr"):
+            attrs = _get_walkable_iterator(node)
+        else:
+            return
+    for attr in attrs.values():
+        yield node, attr
+        # Yield but don't follow references.
+        if hasattr(attr, "__ref__"):
+            continue
+        child = attr.__get__(node, node.__class__)
+        yield from walk_node_attributes(child)
+
+
+def walk_nodes(node):
+    """
+    Walk over all of the child configuration nodes of ``node``.
+
+    :returns: node generator
+    :rtype: Any
+    """
+    if hasattr(node.__class__, "_config_attrs"):
+        attrs = node.__class__._config_attrs
+    elif hasattr(node, "_config_attr"):
+        attrs = _get_walkable_iterator(node)
+    else:
+        return
+    yield node
+    for attr in attrs.values():
+        # Yield but don't follow references.
+        if hasattr(attr, "__ref__"):
+            continue
+        child = attr.__get__(node, node.__class__)
+        yield from walk_nodes(child)
+
+
+def walk_node_values(start_node):
+    for node, attr in walk_node_attributes(start_node):
+        yield node, attr.attr_name, attr.__get__(node, node.__class__)
+
+
+def _resolve_references(root, start=None, /):
+    from ._attrs import _setattr
+
+    if start is None:
+        start = root
+    if root._config_isfinished:
+        for node, attr in walk_node_attributes(root):
+            if hasattr(attr, "__ref__"):
+                ref = attr.__ref__(node, root)
+                _setattr(node, attr.attr_name, ref)
+
+
+class WalkIterDescriptor:
+    def __init__(self, n, v):
+        self.attr_name = n
+        self.v = v
+
+    def __get__(self, instance, cls):
+        return self.v
+
+
+def _get_walkable_iterator(node):
+    if isinstance(node, dict):
+        walkiter = {}
+        for name, value in node.items():
+            walkiter[name] = WalkIterDescriptor(name, value)
+        return walkiter
+    elif isinstance(node, list):
+        walkiter = {}
+        for i, value in enumerate(node):
+            walkiter[i] = WalkIterDescriptor(i, value)
+        return walkiter
+
+
+_classmap_registry = defaultdict(dict)
+
+
+@functools.cache
+def load_component_plugins():
+    from ..plugins import discover
+
+    plugins = discover("components")
+    for plugin in plugins.values():
+        if isinstance(plugin, dict):
+            for class_name, classmap in plugin.items():
+                register_classmap(class_name, classmap)
+
+    return plugins
+
+
+def register_classmap(cls_name, classmap):
+    _classmap_registry[cls_name].update(classmap)
+
+
+def get_classmap(cls):
+    load_component_plugins()
+    classmap = getattr(cls, "_config_dynamic_classmap", {})
+    classmap.update(_classmap_registry[get_qualified_class_name(cls)])
+    return classmap
+
+
+MISSING = object()
```

### Comparing `bsb_core-4.0.1/bsb/config/types.py` & `bsb_core-4.1.0/bsb/config/types.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,828 +1,830 @@
-import abc
-import builtins
-import inspect
-import math
-from weakref import WeakKeyDictionary
-
-import numpy as np
-
-from ..exceptions import (
-    CastError,
-    ClassMapMissingError,
-    InvalidReferenceError,
-    NoneReferenceError,
-    RequirementError,
-    TypeHandlingError,
-)
-from ._compile import _reserved_kw_passes, _wrap_reserved
-from ._make import _load_object
-
-
-class TypeHandler(abc.ABC):
-    """
-    Base class for any type handler that cannot be described as a single function.
-
-    Declare the `__call__(self, value)` method to convert the given value to the
-    desired type, raising a `TypeError` if it failed in an expected manner.
-
-    Declare the `__name__(self)` method to return a name for the type handler to
-    display in messages to the user such as errors.
-
-    Declare the optional `__inv__` method to invert the given value back to its
-    original value, the type of the original value will usually be lost but the type
-    of the returned value can still serve as a suggestion.
-    """
-
-    @abc.abstractmethod
-    def __call__(self, value):  # pragma: nocover
-        pass
-
-    @property
-    @abc.abstractmethod
-    def __name__(self):  # pragma: nocover
-        return "unknown type handler"
-
-    def __inv__(self, value):  # pragma: nocover
-        return value
-
-    def __init_subclass__(cls, **kwargs):
-        super().__init_subclass__(**kwargs)
-        call = cls.__call__
-        passes = _reserved_kw_passes(call)
-        if not all(passes.values()):
-            cls.__call__ = _wrap_reserved(call)
-
-
-def any_():
-    def type_handler(value):
-        return value
-
-    type_handler.__name__ = "any"
-    return type_handler
-
-
-def in_(container):
-    """
-    Type validator. Checks whether the given value occurs in the given container.
-    Uses the `in` operator.
-
-    :param container: List of possible values
-    :type container: list
-    :returns: Type validator function
-    :rtype: Callable
-    """
-    error_msg = "a value in: " + builtins.str(container)
-
-    def type_handler(value):
-        if value in container:
-            return value
-        else:
-            raise TypeError(f"Couldn't cast '{value}' to " + error_msg)
-
-    type_handler.__name__ = error_msg
-    return type_handler
-
-
-def or_(*type_args):
-    """
-    Type validator. Attempts to cast the value to any of the given types in order.
-
-    :param type_args: Another type validator
-    :type type_args: Callable
-    :returns: Type validator function
-    :raises: TypeError if none of the given type validators can cast the value.
-    :rtype: Callable
-    """
-    handler_name = "any of: " + ", ".join(x.__name__ for x in type_args)
-    # Make sure to wrap all type handlers so that they accept the parent and key args.
-    type_args = [_wrap_reserved(t) for t in type_args]
-
-    def type_handler(value, _parent=None, _key=None):
-        type_errors = {}
-        for t in type_args:
-            try:
-                v = t(value, _parent=_parent, _key=_key)
-            except Exception as e:
-                type_error = (
-                    builtins.str(e.__class__.__module__)
-                    + "."
-                    + builtins.str(e.__class__.__name__)
-                    + ": "
-                    + builtins.str(e)
-                )
-                type_errors[t.__name__] = type_error
-            else:
-                return v
-        type_errors = "\n".join(
-            "- Casting to '{}' raised:\n{}".format(n, e) for n, e in type_errors.items()
-        )
-        # Use a CastError instead of a TypeError so that the message is passed along as is
-        # by upstream error handlers.
-        raise CastError(
-            "Couldn't cast {} into {}.\n{}".format(value, handler_name, type_errors)
-        )
-
-    type_handler.__name__ = handler_name
-    return type_handler
-
-
-class object_(TypeHandler):
-    """
-    Type validator. Attempts to import the value, absolute, or relative to the
-    `module_path` entries.
-
-    :param module_path: List of the modules that should be searched when doing a
-      relative import.
-    :type module_path: list[str]
-    :raises: TypeError when value can't be cast.
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def __init__(self, module_path=None):
-        self._module_path = module_path
-
-    def __call__(self, value):
-        msg = f"Could not import '{value}': "
-        try:
-            if isinstance(value, builtins.str):
-                obj = _load_object(value, self._module_path)
-                obj._cfg_inv = value
-            else:
-                obj = value
-        except Exception as e:
-            raise TypeError(msg + builtins.str(e))
-        return obj
-
-    def __inv__(self, value):
-        return getattr(value, "_cfg_inv", value)
-
-    @property
-    def __name__(self):
-        return "object"
-
-
-class class_(object_):
-    """
-    Type validator. Attempts to import the value as the name of a class, relative to
-    the `module_path` entries, absolute or just returning it if it is already a class.
-
-    :param module_path: List of the modules that should be searched when doing a
-      relative import.
-    :type module_path: list[str]
-    :raises: TypeError when value can't be cast.
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def __call__(self, value):
-        if inspect.isclass(value):
-            return value
-        obj = super().__call__(value)
-        if not inspect.isclass(obj):
-            raise TypeError(f"'{value}' is not a class, got {builtins.type(obj)} instead")
-        return obj
-
-    def __inv__(self, value):
-        if not inspect.isclass(value):
-            value = type(value)
-        return f"{value.__module__}.{value.__name__}"
-
-    @property
-    def __name__(self):
-        return "class"
-
-
-class function_(object_):
-    """
-    Type validator. Attempts to import the value, absolute, or relative to the
-    `module_path` entries, and verifies that it is callable.
-
-    :param module_path: List of the modules that should be searched when doing a
-      relative import.
-    :type module_path: list[str]
-    :raises: TypeError when value can't be cast.
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def __call__(self, value):
-        if callable(value):
-            return value
-        obj = super().__call__(value)
-        if not callable(obj):
-            raise TypeError(f"Could not import {value} as a callable function.")
-        return obj
-
-    def __inv__(self, value):
-        return f"{value.__module__}.{value.__name__}"
-
-    @property
-    def __name__(self):
-        return "function"
-
-
-class method(function_):
-    def __init__(self, class_name):
-        super().__init__()
-        self._class = class_name
-
-    def __call__(self, value):
-        parent = class_()(self._class)
-        try:
-            obj = getattr(parent, value)
-        except AttributeError as e:
-            raise TypeError(builtins.str(e)) from None
-        if not callable(obj):
-            raise TypeError(f"Could not import '{value}' as a method of `{self._class}`.")
-        return obj
-
-    def __inv__(self, value):
-        return value.__name__
-
-    @property
-    def __name__(self):
-        return f"method of '{self._class}'"
-
-
-class WeakInverter:
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._map = WeakKeyDictionary()
-
-    def store_value(self, value, result):
-        self._map[result] = value
-
-    def __inv__(self, value):
-        return self._map.get(value, value)
-
-
-class method_shortcut(method, function_):
-    def __call__(self, value):
-        try:
-            obj = method.__call__(self, value)
-        except TypeError:
-            try:
-                obj = function_.__call__(self, value)
-            except TypeError:
-                raise TypeError(
-                    f"Could not import '{value}' as a function or a method of `{self._class}`."
-                ) from None
-        return obj
-
-    def __inv__(self, value):
-        if inspect.isfunction(value):
-            try:
-                method.__call__(self, value.__name__)
-                return method.__inv__(self, value)
-            except TypeError:
-                return function_.__inv__(self, value)
-        else:
-            return value
-
-
-def str(strip=False, lower=False, upper=False, safe=True):
-    """
-    Type validator. Attempts to cast the value to a str, optionally with some sanitation.
-
-    :param strip: Trim whitespaces
-    :type strip: bool
-    :param lower: Convert value to lowercase
-    :type lower: bool
-    :param upper: Convert value to uppercase
-    :type upper: bool
-    :param safe: If False, checks that the type of value is string before cast.
-    :type safe: bool
-    :returns: Type validator function
-    :raises: TypeError when value can't be cast.
-    :rtype: Callable
-    """
-    handler_name = "str"
-    # Compile a custom function to sanitize the string according to args
-    safety_check = "\n if not isinstance(s, str):\n  raise TypeError()\n" if safe else ""
-    fstr = f"def f(s):{safety_check} return str(s)"
-    for add, mod in zip((strip, lower, upper), ("strip", "lower", "upper")):
-        if add:
-            fstr += f".{mod}()"
-    go_fish = builtins.dict()
-    exec(compile(fstr, "__", "exec"), go_fish)
-    f = go_fish["f"]
-
-    def type_handler(value):
-        return f(value)
-
-    type_handler.__name__ = handler_name
-    return type_handler
-
-
-def int(min=None, max=None):
-    """
-    Type validator. Attempts to cast the value to an int, optionally within some
-    bounds.
-
-    :param min: Minimum valid value
-    :type min: int
-    :param max: Maximum valid value
-    :type max: int
-    :returns: Type validator function
-    :raises: TypeError when value can't be cast.
-    :rtype: Callable
-    """
-    handler_name = "int"
-    if min is not None and max is not None:
-        handler_name += " between [{}, {}]".format(min, max)
-    elif min is not None:
-        handler_name += " >= {}".format(min)
-    elif max is not None:
-        handler_name += " <= {}".format(max)
-
-    def type_handler(value):
-        try:
-            v = builtins.int(value)
-            if min is not None and min > v or max is not None and max < v:
-                raise Exception()
-            return v
-        except Exception:
-            raise TypeError(
-                "Could not cast {} to an {}.".format(value, handler_name)
-            ) from None
-
-    type_handler.__name__ = handler_name
-    return type_handler
-
-
-_float = float
-
-
-def float(min=None, max=None):
-    """
-    Type validator. Attempts to cast the value to an float, optionally within some
-    bounds.
-
-    :param min: Minimum valid value
-    :type min: float
-    :param max: Maximum valid value
-    :type max: float
-    :returns: Type validator function
-    :raises: TypeError when value can't be cast.
-    :rtype: Callable
-    """
-    handler_name = "float"
-    if min is not None and max is not None:
-        handler_name += " between [{}, {}]".format(min, max)
-    elif min is not None:
-        handler_name += " >= {}".format(min)
-    elif max is not None:
-        handler_name += " <= {}".format(max)
-
-    def type_handler(value):
-        try:
-            v = _float(value)
-            if min is not None and min > v or max is not None and max < v:
-                raise Exception()
-            return v
-        except Exception:
-            raise TypeError("Could not cast {} to an {}.".format(value, handler_name))
-
-    type_handler.__name__ = handler_name
-    return type_handler
-
-
-def number(min=None, max=None):
-    """
-    Type validator. If the given value is an int returns an int, tries to cast to float
-    otherwise
-
-    :param min: Minimum valid value
-    :type min: float
-    :param max: Maximum valid value
-    :type max: float
-    :returns: Type validator function
-    :raises: TypeError when value can't be cast.
-    :rtype: Callable
-    """
-    handler_name = "number"
-    if min is not None and max is not None:
-        handler_name += " between [{}, {}]".format(min, max)
-    elif min is not None:
-        handler_name += " >= {}".format(min)
-    elif max is not None:
-        handler_name += " <= {}".format(max)
-
-    def type_handler(value):
-        try:
-            if isinstance(value, builtins.int):
-                v = builtins.int(value)
-                if min is not None and min > v or max is not None and max < v:
-                    raise Exception()
-            else:
-                v = _float(value)
-                if min is not None and min > v or max is not None and max < v:
-                    raise Exception()
-            return v
-        except Exception:
-            raise TypeError("Could not cast {} to a {}.".format(value, handler_name))
-
-    type_handler.__name__ = handler_name
-    return type_handler
-
-
-def key():
-    """
-    Type handler for keys in configuration trees. Keys can be either int indices of a
-    config list, or string keys of a config dict.
-
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def type_handler(value):
-        if not (isinstance(value, builtins.int) or isinstance(value, builtins.str)):
-            raise TypeError(f"{type(value)} is not an int or str")
-        else:
-            return value
-
-    type_handler.__name__ = "configuration key"
-    return type_handler
-
-
-def scalar_expand(scalar_type, size=None, expand=None):
-    """
-    Create a method that expands a scalar into an array with a specific size or uses
-    an expansion function.
-
-    :param scalar_type: Type of the scalar
-    :type scalar_type: type
-    :param size: Expand the scalar to an array of a fixed size.
-    :type size: int
-    :param expand: A function that takes the scalar value as argument and returns the
-      expanded form.
-    :type expand: Callable
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    if expand is None:
-
-        def expand(x):
-            return [1.0] * x
-
-    def type_handler(value):
-        # No try block: let it raise the cast error.
-        v = scalar_type(value)
-        # Expand the scalar.
-        return expand(v)
-
-    type_handler.__name__ = "expanded list of " + scalar_type.__name__
-    return type_handler
-
-
-def list_or_scalar(scalar_type, size=None):
-    """
-    Type validator that accepts a scalar or list of said scalars.
-
-    :param scalar_type: Type of the scalar
-    :type scalar_type: type
-    :param size: Expand the scalar to an array of a fixed size.
-    :type size: int
-    :returns: Type validator function
-    :rtype: Callable
-    """
-    type_handler = or_(list(scalar_type, size), scalar_type)
-
-    type_handler.__name__ += " or " + scalar_type.__name__
-    return type_handler
-
-
-def voxel_size():
-    return list_or_scalar(float(), 3)
-
-
-def list(type=builtins.str, size=None):
-    """
-    Type validator for lists. Type casts each element to the given type and optionally
-    validates the length of the list.
-
-    :param type: Type validator of the elements.
-    :type type: Callable
-    :param size: Mandatory length of the list.
-    :type size: int
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def type_handler(value):
-        # Simple lists default to returning None for None, while configuration lists
-        # default to an empty list.
-        if value is None:
-            return None
-        v = builtins.list(value)
-        try:
-            for i, e in enumerate(v):
-                v[i] = type(e)
-        except Exception:
-            raise TypeError(
-                "Couldn't cast element {} of {} into {}".format(i, value, type.__name__)
-            )
-        if size is not None and len(v) != size:
-            raise ValueError(
-                "Couldn't cast {} into a {} element list".format(value, size)
-            )
-        return v
-
-    type_handler.__name__ = "list{} of {}".format(
-        "[{}]".format(size) if size is not None else "", type.__name__
-    )
-    return type_handler
-
-
-def dict(type=builtins.str):
-    """
-    Type validator for dicts. Type casts each element to the given type.
-
-    :param type: Type validator of the elements.
-    :type type: Callable
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def type_handler(value):
-        if value is None:
-            return None
-        v = builtins.dict(value)
-        try:
-            for k, e in v.items():
-                v[k] = type(e)
-        except Exception:
-            raise TypeError(
-                "Couldn't cast {} of {} into {}".format(k, value, type.__name__)
-            )
-        return v
-
-    type_handler.__name__ = "dict of {}".format(type.__name__)
-    return type_handler
-
-
-def fraction():
-    """
-    Type validator. Type casts the value into a rational number between 0 and 1
-    (inclusive).
-
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def type_handler(value):
-        v = _float(value)
-        if v < 0.0 or v > 1.0:
-            raise ValueError("{} is out of the 0-1 range for a fraction.".format(value))
-        return v
-
-    type_handler.__name__ = "fraction [0.; 1.]"
-    return type_handler
-
-
-class deg_to_radian(TypeHandler):
-    """
-    Type validator. Type casts the value from degrees to radians.
-    """
-
-    def __call__(self, value):
-        v = _float(value)
-        return v * 2 * math.pi / 360
-
-    @property
-    def __name__(self):  # pragma: nocover
-        return "degrees"
-
-    def __inv__(self, value):
-        v = _float(value)
-        return v * 360 / (2 * math.pi)
-
-
-class distribution(TypeHandler):
-    """
-    Type validator. Type casts the value or node to a distribution.
-    """
-
-    def __call__(self, value, _key=None, _parent=None):
-        from ._distributions import Distribution
-
-        if not isinstance(value, builtins.list) and not isinstance(value, builtins.dict):
-            value = {"distribution": "constant", "constant": value}
-
-        return Distribution(**value, _key=_key, _parent=_parent)
-
-    @property
-    def __name__(self):  # pragma: nocover
-        return "distribution"
-
-    def __inv__(self, value):
-        if value["distribution"] == "constant":
-            return value["constant"]
-        else:
-            return value
-
-
-class evaluation(TypeHandler):
-    """
-    Type validator. Provides a structured way to evaluate a python statement from the
-    config. The evaluation context provides ``numpy`` as ``np``.
-
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def __init__(self):
-        self._references = {}
-
-    def __call__(self, value):
-        cfg = builtins.dict(value)
-        statement = builtins.str(cfg.get("statement", "None"))
-        locals_ = builtins.dict(cfg.get("variables", {}))
-        globals_ = {"np": np}
-        res = eval(statement, globals_, locals_)
-        self._references[id(res)] = value
-        return res
-
-    @property
-    def __name__(self):
-        return "evaluation"
-
-    def get_original(self, value):
-        """
-        Return the original configuration node associated with the given evaluated value.
-
-        :param value: A value that was produced by this type handler.
-        :type value: Any
-        :raises: NoneReferenceError when `value` is `None`, InvalidReferenceError when
-          there is no config associated to the object id of this value.
-        """
-        # None is a singleton, so it's not bijective, it's also the value returned when
-        # a weak reference is removed; so it's doubly unsafe to check for references to it
-        if value is None:
-            raise NoneReferenceError("Can't create bijection for NoneType value.")
-        vid = id(value)
-        # Create a set of references from our stored weak references that are still alive.
-        if vid not in self._references:
-            raise InvalidReferenceError(f"No evaluation reference found for {vid}", value)
-        return self._references[vid]
-
-    def __inv__(self, value):
-        try:
-            return self.get_original(value)
-        except TypeHandlingError:
-            # Original does not exist or can't be obtained, just return the given value.
-            return value
-
-
-def in_classmap():
-    """
-    Type validator. Checks whether the given string occurs in the class map of a
-    dynamic node.
-
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def type_handler(value, _parent, _key=None):
-        class_name = _parent.__class__.__name__
-        if not hasattr(_parent.__class__, "_config_dynamic_classmap"):
-            raise ClassMapMissingError(
-                f"Class map missing for `{class_name}`,"
-                + " required when using `in_classmap` type handler."
-            )
-        classmap = _parent.__class__._config_dynamic_classmap
-        if value not in classmap:
-            classmap_str = ", ".join(f"'{key}'" for key in classmap)
-            raise CastError(
-                f"'{value}' is not a valid classmap identifier for `{class_name}`."
-                + f" Choose from: {classmap_str}"
-            )
-        return value
-
-    type_handler.__name__ = "a classmap value"
-    return type_handler
-
-
-def mut_excl(*mutuals, required=True, max=1, shortform=False):
-    """
-    Requirement handler for mutually exclusive attributes.
-
-    :param str mutuals: The keys of the mutually exclusive attributes.
-    :param bool required: Whether at least one of the keys is required
-    :param int max: The maximum amount of keys that may occur together.
-    :param bool shortform: Allow the short form alternative.
-    :returns: Requirement function
-    :rtype: Callable
-    """
-    listed = ", ".join(f"`{m}`" for m in mutuals[:-1])
-    if len(mutuals) > 1:
-        listed += f" {{}} `{mutuals[-1]}`"
-
-    def requirement(section):
-        if shortform and section.is_shortform:
-            return False
-        bools = [m in section for m in mutuals]
-        given = sum(bools)
-        if given > max:
-            if max > 1:
-                err_msg = f"Maximum {max} of {listed} may be specified. {given} given."
-            else:
-                err_msg = f"The {listed} attributes are mutually exclusive."
-            err_msg = err_msg.format("and")
-            raise RequirementError(err_msg)
-        if not given and required:
-            err_msg = f"A {listed} attribute is required."
-            raise RequirementError(err_msg)
-        return False
-
-    return requirement
-
-
-def shortform():
-    def requirement(section):
-        return not section.is_shortform
-
-    return requirement
-
-
-class ndarray(TypeHandler):
-    """
-    Type validator numpy arrays.
-
-    :returns: Type validator function
-    :rtype: Callable
-    """
-
-    def __call__(self, value):
-        return np.array(value, copy=False)
-
-    @property
-    def __name__(self):
-        return "ndarray"
-
-    def __inv__(self, value):
-        return value.tolist()
-
-
-def none():
-    def type_handler(value, _parent, _key=None):
-        if value is not None:
-            raise TypeError("value is not None")
-        return value
-
-    type_handler.__name__ = "a None value"
-    return type_handler
-
-
-class PackageRequirement(TypeHandler):
-    def __call__(self, value):
-        from packaging.requirements import Requirement
-
-        return Requirement(value)
-
-    @property
-    def __name__(self):
-        return "package requirement"
-
-    def __inv__(self, value):
-        return str(value)
-
-    def __hint__(self):
-        return "numpy==1.24.0"
-
-
-__all__ = [
-    "PackageRequirement",
-    "TypeHandler",
-    "WeakInverter",
-    "any_",
-    "class_",
-    "deg_to_radian",
-    "dict",
-    "distribution",
-    "evaluation",
-    "float",
-    "fraction",
-    "function_",
-    "in_",
-    "in_classmap",
-    "int",
-    "key",
-    "list",
-    "list_or_scalar",
-    "method",
-    "method_shortcut",
-    "mut_excl",
-    "ndarray",
-    "none",
-    "number",
-    "object_",
-    "or_",
-    "scalar_expand",
-    "shortform",
-    "str",
-    "voxel_size",
-]
-__api__ = ["PackageRequirement", "TypeHandler", "WeakInverter"]
+import abc
+import builtins
+import inspect
+import math
+from weakref import WeakKeyDictionary
+
+import numpy as np
+
+from ..exceptions import (
+    CastError,
+    ClassMapMissingError,
+    InvalidReferenceError,
+    NoneReferenceError,
+    RequirementError,
+    TypeHandlingError,
+)
+from ._compile import _reserved_kw_passes, _wrap_reserved
+from ._make import _load_object
+
+
+class TypeHandler(abc.ABC):
+    """
+    Base class for any type handler that cannot be described as a single function.
+
+    Declare the `__call__(self, value)` method to convert the given value to the
+    desired type, raising a `TypeError` if it failed in an expected manner.
+
+    Declare the `__name__(self)` method to return a name for the type handler to
+    display in messages to the user such as errors.
+
+    Declare the optional `__inv__` method to invert the given value back to its
+    original value, the type of the original value will usually be lost but the type
+    of the returned value can still serve as a suggestion.
+    """
+
+    @abc.abstractmethod
+    def __call__(self, value):  # pragma: nocover
+        pass
+
+    @property
+    @abc.abstractmethod
+    def __name__(self):  # pragma: nocover
+        return "unknown type handler"
+
+    def __inv__(self, value):  # pragma: nocover
+        return value
+
+    def __init_subclass__(cls, **kwargs):
+        super().__init_subclass__(**kwargs)
+        call = cls.__call__
+        passes = _reserved_kw_passes(call)
+        if not all(passes.values()):
+            cls.__call__ = _wrap_reserved(call)
+
+
+def any_():
+    def type_handler(value):
+        return value
+
+    type_handler.__name__ = "any"
+    return type_handler
+
+
+def in_(container):
+    """
+    Type validator. Checks whether the given value occurs in the given container.
+    Uses the `in` operator.
+
+    :param container: List of possible values
+    :type container: list
+    :returns: Type validator function
+    :rtype: Callable
+    """
+    error_msg = "a value in: " + builtins.str(container)
+
+    def type_handler(value):
+        if value in container:
+            return value
+        else:
+            raise TypeError(f"Couldn't cast '{value}' to " + error_msg)
+
+    type_handler.__name__ = error_msg
+    return type_handler
+
+
+def or_(*type_args):
+    """
+    Type validator. Attempts to cast the value to any of the given types in order.
+
+    :param type_args: Another type validator
+    :type type_args: Callable
+    :returns: Type validator function
+    :raises: TypeError if none of the given type validators can cast the value.
+    :rtype: Callable
+    """
+    handler_name = "any of: " + ", ".join(x.__name__ for x in type_args)
+    # Make sure to wrap all type handlers so that they accept the parent and key args.
+    type_args = [_wrap_reserved(t) for t in type_args]
+
+    def type_handler(value, _parent=None, _key=None):
+        type_errors = {}
+        for t in type_args:
+            try:
+                v = t(value, _parent=_parent, _key=_key)
+            except Exception as e:
+                type_error = (
+                    builtins.str(e.__class__.__module__)
+                    + "."
+                    + builtins.str(e.__class__.__name__)
+                    + ": "
+                    + builtins.str(e)
+                )
+                type_errors[t.__name__] = type_error
+            else:
+                return v
+        type_errors = "\n".join(
+            "- Casting to '{}' raised:\n{}".format(n, e) for n, e in type_errors.items()
+        )
+        # Use a CastError instead of a TypeError so that the message is passed along as is
+        # by upstream error handlers.
+        raise CastError(
+            "Couldn't cast {} into {}.\n{}".format(value, handler_name, type_errors)
+        )
+
+    type_handler.__name__ = handler_name
+    return type_handler
+
+
+class object_(TypeHandler):
+    """
+    Type validator. Attempts to import the value, absolute, or relative to the
+    `module_path` entries.
+
+    :param module_path: List of the modules that should be searched when doing a
+      relative import.
+    :type module_path: list[str]
+    :raises: TypeError when value can't be cast.
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def __init__(self, module_path=None):
+        self._module_path = module_path
+
+    def __call__(self, value):
+        msg = f"Could not import '{value}': "
+        try:
+            if isinstance(value, builtins.str):
+                obj = _load_object(value, self._module_path)
+                obj._cfg_inv = value
+            else:
+                obj = value
+        except Exception as e:
+            raise TypeError(msg + builtins.str(e))
+        return obj
+
+    def __inv__(self, value):
+        return getattr(value, "_cfg_inv", value)
+
+    @property
+    def __name__(self):
+        return "object"
+
+
+class class_(object_):
+    """
+    Type validator. Attempts to import the value as the name of a class, relative to
+    the `module_path` entries, absolute or just returning it if it is already a class.
+
+    :param module_path: List of the modules that should be searched when doing a
+      relative import.
+    :type module_path: list[str]
+    :raises: TypeError when value can't be cast.
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def __call__(self, value):
+        if inspect.isclass(value):
+            return value
+        obj = super().__call__(value)
+        if not inspect.isclass(obj):
+            raise TypeError(f"'{value}' is not a class, got {builtins.type(obj)} instead")
+        return obj
+
+    def __inv__(self, value):
+        if not inspect.isclass(value):
+            value = type(value)
+        return f"{value.__module__}.{value.__name__}"
+
+    @property
+    def __name__(self):
+        return "class"
+
+
+class function_(object_):
+    """
+    Type validator. Attempts to import the value, absolute, or relative to the
+    `module_path` entries, and verifies that it is callable.
+
+    :param module_path: List of the modules that should be searched when doing a
+      relative import.
+    :type module_path: list[str]
+    :raises: TypeError when value can't be cast.
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def __call__(self, value):
+        if callable(value):
+            return value
+        obj = super().__call__(value)
+        if not callable(obj):
+            raise TypeError(f"Could not import {value} as a callable function.")
+        return obj
+
+    def __inv__(self, value):
+        return f"{value.__module__}.{value.__name__}"
+
+    @property
+    def __name__(self):
+        return "function"
+
+
+class method(function_):
+    def __init__(self, class_name):
+        super().__init__()
+        self._class = class_name
+
+    def __call__(self, value):
+        parent = class_()(self._class)
+        try:
+            obj = getattr(parent, value)
+        except AttributeError as e:
+            raise TypeError(builtins.str(e)) from None
+        if not callable(obj):
+            raise TypeError(f"Could not import '{value}' as a method of `{self._class}`.")
+        return obj
+
+    def __inv__(self, value):
+        return value.__name__
+
+    @property
+    def __name__(self):
+        return f"method of '{self._class}'"
+
+
+class WeakInverter:
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._map = WeakKeyDictionary()
+
+    def store_value(self, value, result):
+        self._map[result] = value
+
+    def __inv__(self, value):
+        return self._map.get(value, value)
+
+
+class method_shortcut(method, function_):
+    def __call__(self, value):
+        try:
+            obj = method.__call__(self, value)
+        except TypeError:
+            try:
+                obj = function_.__call__(self, value)
+            except TypeError:
+                raise TypeError(
+                    f"Could not import '{value}' as a function or a method of `{self._class}`."
+                ) from None
+        return obj
+
+    def __inv__(self, value):
+        if inspect.isfunction(value):
+            try:
+                method.__call__(self, value.__name__)
+                return method.__inv__(self, value)
+            except TypeError:
+                return function_.__inv__(self, value)
+        else:
+            return value
+
+
+def str(strip=False, lower=False, upper=False, safe=True):
+    """
+    Type validator. Attempts to cast the value to a str, optionally with some sanitation.
+
+    :param strip: Trim whitespaces
+    :type strip: bool
+    :param lower: Convert value to lowercase
+    :type lower: bool
+    :param upper: Convert value to uppercase
+    :type upper: bool
+    :param safe: If False, checks that the type of value is string before cast.
+    :type safe: bool
+    :returns: Type validator function
+    :raises: TypeError when value can't be cast.
+    :rtype: Callable
+    """
+    handler_name = "str"
+    # Compile a custom function to sanitize the string according to args
+    safety_check = "\n if not isinstance(s, str):\n  raise TypeError()\n" if safe else ""
+    fstr = f"def f(s):{safety_check} return str(s)"
+    for add, mod in zip((strip, lower, upper), ("strip", "lower", "upper")):
+        if add:
+            fstr += f".{mod}()"
+    go_fish = builtins.dict()
+    exec(compile(fstr, "__", "exec"), go_fish)
+    f = go_fish["f"]
+
+    def type_handler(value):
+        return f(value)
+
+    type_handler.__name__ = handler_name
+    return type_handler
+
+
+def int(min=None, max=None):
+    """
+    Type validator. Attempts to cast the value to an int, optionally within some
+    bounds.
+
+    :param min: Minimum valid value
+    :type min: int
+    :param max: Maximum valid value
+    :type max: int
+    :returns: Type validator function
+    :raises: TypeError when value can't be cast.
+    :rtype: Callable
+    """
+    handler_name = "int"
+    if min is not None and max is not None:
+        handler_name += " between [{}, {}]".format(min, max)
+    elif min is not None:
+        handler_name += " >= {}".format(min)
+    elif max is not None:
+        handler_name += " <= {}".format(max)
+
+    def type_handler(value):
+        try:
+            v = builtins.int(value)
+            if min is not None and min > v or max is not None and max < v:
+                raise Exception()
+            return v
+        except Exception:
+            raise TypeError(
+                "Could not cast {} to an {}.".format(value, handler_name)
+            ) from None
+
+    type_handler.__name__ = handler_name
+    return type_handler
+
+
+_float = float
+
+
+def float(min=None, max=None):
+    """
+    Type validator. Attempts to cast the value to an float, optionally within some
+    bounds.
+
+    :param min: Minimum valid value
+    :type min: float
+    :param max: Maximum valid value
+    :type max: float
+    :returns: Type validator function
+    :raises: TypeError when value can't be cast.
+    :rtype: Callable
+    """
+    handler_name = "float"
+    if min is not None and max is not None:
+        handler_name += " between [{}, {}]".format(min, max)
+    elif min is not None:
+        handler_name += " >= {}".format(min)
+    elif max is not None:
+        handler_name += " <= {}".format(max)
+
+    def type_handler(value):
+        try:
+            v = _float(value)
+            if min is not None and min > v or max is not None and max < v:
+                raise Exception()
+            return v
+        except Exception:
+            raise TypeError("Could not cast {} to an {}.".format(value, handler_name))
+
+    type_handler.__name__ = handler_name
+    return type_handler
+
+
+def number(min=None, max=None):
+    """
+    Type validator. If the given value is an int returns an int, tries to cast to float
+    otherwise
+
+    :param min: Minimum valid value
+    :type min: float
+    :param max: Maximum valid value
+    :type max: float
+    :returns: Type validator function
+    :raises: TypeError when value can't be cast.
+    :rtype: Callable
+    """
+    handler_name = "number"
+    if min is not None and max is not None:
+        handler_name += " between [{}, {}]".format(min, max)
+    elif min is not None:
+        handler_name += " >= {}".format(min)
+    elif max is not None:
+        handler_name += " <= {}".format(max)
+
+    def type_handler(value):
+        try:
+            if isinstance(value, builtins.int):
+                v = builtins.int(value)
+                if min is not None and min > v or max is not None and max < v:
+                    raise Exception()
+            else:
+                v = _float(value)
+                if min is not None and min > v or max is not None and max < v:
+                    raise Exception()
+            return v
+        except Exception:
+            raise TypeError("Could not cast {} to a {}.".format(value, handler_name))
+
+    type_handler.__name__ = handler_name
+    return type_handler
+
+
+def key():
+    """
+    Type handler for keys in configuration trees. Keys can be either int indices of a
+    config list, or string keys of a config dict.
+
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def type_handler(value):
+        if not (isinstance(value, builtins.int) or isinstance(value, builtins.str)):
+            raise TypeError(f"{type(value)} is not an int or str")
+        else:
+            return value
+
+    type_handler.__name__ = "configuration key"
+    return type_handler
+
+
+def scalar_expand(scalar_type, size=None, expand=None):
+    """
+    Create a method that expands a scalar into an array with a specific size or uses
+    an expansion function.
+
+    :param scalar_type: Type of the scalar
+    :type scalar_type: type
+    :param size: Expand the scalar to an array of a fixed size.
+    :type size: int
+    :param expand: A function that takes the scalar value as argument and returns the
+      expanded form.
+    :type expand: Callable
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    if expand is None:
+
+        def expand(x):
+            return [1.0] * x
+
+    def type_handler(value):
+        # No try block: let it raise the cast error.
+        v = scalar_type(value)
+        # Expand the scalar.
+        return expand(v)
+
+    type_handler.__name__ = "expanded list of " + scalar_type.__name__
+    return type_handler
+
+
+def list_or_scalar(scalar_type, size=None):
+    """
+    Type validator that accepts a scalar or list of said scalars.
+
+    :param scalar_type: Type of the scalar
+    :type scalar_type: type
+    :param size: Expand the scalar to an array of a fixed size.
+    :type size: int
+    :returns: Type validator function
+    :rtype: Callable
+    """
+    type_handler = or_(list(scalar_type, size), scalar_type)
+
+    type_handler.__name__ += " or " + scalar_type.__name__
+    return type_handler
+
+
+def voxel_size():
+    return list_or_scalar(float(), 3)
+
+
+def list(type=builtins.str, size=None):
+    """
+    Type validator for lists. Type casts each element to the given type and optionally
+    validates the length of the list.
+
+    :param type: Type validator of the elements.
+    :type type: Callable
+    :param size: Mandatory length of the list.
+    :type size: int
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def type_handler(value):
+        # Simple lists default to returning None for None, while configuration lists
+        # default to an empty list.
+        if value is None:
+            return None
+        v = builtins.list(value)
+        try:
+            for i, e in enumerate(v):
+                v[i] = type(e)
+        except Exception:
+            raise TypeError(
+                "Couldn't cast element {} of {} into {}".format(i, value, type.__name__)
+            )
+        if size is not None and len(v) != size:
+            raise ValueError(
+                "Couldn't cast {} into a {} element list".format(value, size)
+            )
+        return v
+
+    type_handler.__name__ = "list{} of {}".format(
+        "[{}]".format(size) if size is not None else "", type.__name__
+    )
+    return type_handler
+
+
+def dict(type=builtins.str):
+    """
+    Type validator for dicts. Type casts each element to the given type.
+
+    :param type: Type validator of the elements.
+    :type type: Callable
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def type_handler(value):
+        if value is None:
+            return None
+        v = builtins.dict(value)
+        try:
+            for k, e in v.items():
+                v[k] = type(e)
+        except Exception:
+            raise TypeError(
+                "Couldn't cast {} of {} into {}".format(k, value, type.__name__)
+            )
+        return v
+
+    type_handler.__name__ = "dict of {}".format(type.__name__)
+    return type_handler
+
+
+def fraction():
+    """
+    Type validator. Type casts the value into a rational number between 0 and 1
+    (inclusive).
+
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def type_handler(value):
+        v = _float(value)
+        if v < 0.0 or v > 1.0:
+            raise ValueError("{} is out of the 0-1 range for a fraction.".format(value))
+        return v
+
+    type_handler.__name__ = "fraction [0.; 1.]"
+    return type_handler
+
+
+class deg_to_radian(TypeHandler):
+    """
+    Type validator. Type casts the value from degrees to radians.
+    """
+
+    def __call__(self, value):
+        v = _float(value)
+        return v * 2 * math.pi / 360
+
+    @property
+    def __name__(self):  # pragma: nocover
+        return "degrees"
+
+    def __inv__(self, value):
+        v = _float(value)
+        return v * 360 / (2 * math.pi)
+
+
+class distribution(TypeHandler):
+    """
+    Type validator. Type casts the value or node to a distribution.
+    """
+
+    def __call__(self, value, _key=None, _parent=None):
+        from ._distributions import Distribution
+
+        if not isinstance(value, builtins.list) and not isinstance(value, builtins.dict):
+            value = {"distribution": "constant", "constant": value}
+
+        return Distribution(**value, _key=_key, _parent=_parent)
+
+    @property
+    def __name__(self):  # pragma: nocover
+        return "distribution"
+
+    def __inv__(self, value):
+        if value["distribution"] == "constant":
+            return value["constant"]
+        else:
+            return value
+
+
+class evaluation(TypeHandler):
+    """
+    Type validator. Provides a structured way to evaluate a python statement from the
+    config. The evaluation context provides ``numpy`` as ``np``.
+
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def __init__(self):
+        self._references = {}
+
+    def __call__(self, value):
+        cfg = builtins.dict(value)
+        statement = builtins.str(cfg.get("statement", "None"))
+        locals_ = builtins.dict(cfg.get("variables", {}))
+        globals_ = {"np": np}
+        res = eval(statement, globals_, locals_)
+        self._references[id(res)] = value
+        return res
+
+    @property
+    def __name__(self):
+        return "evaluation"
+
+    def get_original(self, value):
+        """
+        Return the original configuration node associated with the given evaluated value.
+
+        :param value: A value that was produced by this type handler.
+        :type value: Any
+        :raises: NoneReferenceError when `value` is `None`, InvalidReferenceError when
+          there is no config associated to the object id of this value.
+        """
+        # None is a singleton, so it's not bijective, it's also the value returned when
+        # a weak reference is removed; so it's doubly unsafe to check for references to it
+        if value is None:
+            raise NoneReferenceError("Can't create bijection for NoneType value.")
+        vid = id(value)
+        # Create a set of references from our stored weak references that are still alive.
+        if vid not in self._references:
+            raise InvalidReferenceError(f"No evaluation reference found for {vid}", value)
+        return self._references[vid]
+
+    def __inv__(self, value):
+        try:
+            return self.get_original(value)
+        except TypeHandlingError:
+            # Original does not exist or can't be obtained, just return the given value.
+            return value
+
+
+def in_classmap():
+    """
+    Type validator. Checks whether the given string occurs in the class map of a
+    dynamic node.
+
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def type_handler(value, _parent, _key=None):
+        class_name = _parent.__class__.__name__
+        if not hasattr(_parent.__class__, "_config_dynamic_classmap"):
+            raise ClassMapMissingError(
+                f"Class map missing for `{class_name}`,"
+                + " required when using `in_classmap` type handler."
+            )
+        classmap = _parent.__class__._config_dynamic_classmap
+        if value not in classmap:
+            classmap_str = ", ".join(f"'{key}'" for key in classmap)
+            raise CastError(
+                f"'{value}' is not a valid classmap identifier for `{class_name}`."
+                + f" Choose from: {classmap_str}"
+            )
+        return value
+
+    type_handler.__name__ = "a classmap value"
+    return type_handler
+
+
+def mut_excl(*mutuals, required=True, max=1, shortform=False):
+    """
+    Requirement handler for mutually exclusive attributes.
+
+    :param str mutuals: The keys of the mutually exclusive attributes.
+    :param bool required: Whether at least one of the keys is required
+    :param int max: The maximum amount of keys that may occur together.
+    :param bool shortform: Allow the short form alternative.
+    :returns: Requirement function
+    :rtype: Callable
+    """
+    listed = ", ".join(f"`{m}`" for m in mutuals[:-1])
+    if len(mutuals) > 1:
+        listed += f" {{}} `{mutuals[-1]}`"
+
+    def requirement(section):
+        if shortform and section.is_shortform:
+            return False
+        bools = [m in section for m in mutuals]
+        given = sum(bools)
+        if given > max:
+            if max > 1:
+                err_msg = f"Maximum {max} of {listed} may be specified. {given} given."
+            else:
+                err_msg = f"The {listed} attributes are mutually exclusive."
+            err_msg = err_msg.format("and")
+            raise RequirementError(err_msg)
+        if not given and required:
+            err_msg = f"A {listed} attribute is required."
+            raise RequirementError(err_msg)
+        return False
+
+    return requirement
+
+
+def shortform():
+    def requirement(section):
+        return not section.is_shortform
+
+    return requirement
+
+
+class ndarray(TypeHandler):
+    """
+    Type validator numpy arrays.
+
+    :returns: Type validator function
+    :rtype: Callable
+    """
+
+    def __call__(self, value):
+        return np.array(value, copy=False)
+
+    @property
+    def __name__(self):
+        return "ndarray"
+
+    def __inv__(self, value):
+        return value.tolist()
+
+
+def none():
+    def type_handler(value, _parent, _key=None):
+        if value is not None:
+            raise TypeError("value is not None")
+        return value
+
+    type_handler.__name__ = "a None value"
+    return type_handler
+
+
+class PackageRequirement(TypeHandler):
+    def __call__(self, value):
+        from packaging.requirements import Requirement
+
+        requirement = Requirement(value)
+        requirement._cfg_inv = value
+        return requirement
+
+    @property
+    def __name__(self):
+        return "package requirement"
+
+    def __inv__(self, value):
+        return getattr(value, "_cfg_inv", builtins.str(value))
+
+    def __hint__(self):
+        return "numpy==1.24.0"
+
+
+__all__ = [
+    "PackageRequirement",
+    "TypeHandler",
+    "WeakInverter",
+    "any_",
+    "class_",
+    "deg_to_radian",
+    "dict",
+    "distribution",
+    "evaluation",
+    "float",
+    "fraction",
+    "function_",
+    "in_",
+    "in_classmap",
+    "int",
+    "key",
+    "list",
+    "list_or_scalar",
+    "method",
+    "method_shortcut",
+    "mut_excl",
+    "ndarray",
+    "none",
+    "number",
+    "object_",
+    "or_",
+    "scalar_expand",
+    "shortform",
+    "str",
+    "voxel_size",
+]
+__api__ = ["PackageRequirement", "TypeHandler", "WeakInverter"]
```

### Comparing `bsb_core-4.0.1/bsb/connectivity/detailed/voxel_intersection.py` & `bsb_core-4.1.0/bsb/connectivity/detailed/voxel_intersection.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,161 +1,161 @@
-import itertools
-import random
-
-import numpy as np
-from numpy.random import default_rng
-
-from ... import config
-from ..._util import ichain
-from ...config import types
-from ..strategy import ConnectionStrategy
-from .shared import Intersectional
-
-_rng = default_rng()
-
-
-@config.node
-class VoxelIntersection(Intersectional, ConnectionStrategy):
-    """
-    This strategy finds overlap between voxelized morphologies.
-    """
-
-    contacts = config.attr(type=types.distribution(), default=1)
-    voxels_pre = config.attr(type=int, default=50)
-    voxels_post = config.attr(type=int, default=50)
-    cache = config.attr(type=bool, default=True)
-    favor_cache = config.attr(type=types.in_(["pre", "post"]), default="pre")
-
-    def connect(self, pre, post):
-        # Note on the caching terms: `targets` are the population that will be cached the
-        # strongest; their voxelized tree will remain in place, while the candidates are
-        # rotated and translated to overlap the target tree.
-        # The choice to make something cached harder is if they have less different
-        # morphologies, and good choices for candidates are the population with more
-        # numerous and smaller morphologies.
-        if self.favor_cache == "pre":
-            targets = pre
-            candidates = post
-            self._n_tvoxels = self.voxels_pre
-            self._n_cvoxels = self.voxels_post
-            target_morpho = self.presynaptic.morpho_loader
-            cand_morpho = self.postsynaptic.morpho_loader
-        else:
-            targets = post
-            candidates = pre
-            self._n_tvoxels = self.voxels_post
-            self._n_cvoxels = self.voxels_pre
-            target_morpho = self.postsynaptic.morpho_loader
-            cand_morpho = self.presynaptic.morpho_loader
-        combo_itr = self.candidate_intersection(targets, candidates)
-        mset_cache = {}
-        for target_set, cand_set, match_itr in combo_itr:
-            if self.cache:
-                if id(target_set) not in mset_cache:
-                    mset_cache[id(target_set)] = target_morpho(target_set)
-                if id(cand_set) not in mset_cache:
-                    mset_cache[id(cand_set)] = cand_morpho(cand_set)
-                target_mset = mset_cache[id(target_set)]
-                cand_mset = mset_cache[id(cand_set)]
-            else:
-                target_mset = target_morpho(target_set)
-                cand_mset = cand_morpho(cand_set)
-            self._match_voxel_intersection(
-                match_itr, target_set, cand_set, target_mset, cand_mset
-            )
-
-    def _match_voxel_intersection(self, matches, tset, cset, tmset, cmset):
-        # Soft-caching caches at the IO level and gives you a fresh copy of the morphology
-        # each time, the `cached_voxelize` function we need wouldn't have any effect!
-        tm_iter = tmset.iter_morphologies(cache=self.cache, hard_cache=self.cache)
-        target_itrs = zip(tset.load_positions(), tset.load_rotations().iter(), tm_iter)
-        rotations = cset.load_rotations()
-        positions = cset.load_positions()
-        data_acc = []
-        for target, candidates in enumerate(matches):
-            tpos, trot, tmor = next(target_itrs)
-            if not len(candidates):
-                # No need to load or voxelize if there's no candidates anyway
-                continue
-            # Load and voxelize the target into a box tree
-            if self.cache:
-                tvoxels = tmor.cached_voxelize(N=self._n_tvoxels)
-            else:
-                tvoxels = tmor.voxelize(N=self._n_tvoxels)
-            tree = tvoxels.as_boxtree(cache=self.cache)
-            for cand in candidates:
-                cpos = positions[cand]
-                crot = rotations[cand]
-                # Don't hard cache, as we mutate the instance we get.
-                morpho = cmset.get(cand, cache=self.cache, hard_cache=False)
-                # Transform candidate, keep target unrotated and untranslated at origin:
-                # 1) Rotate self by own rotation
-                # 2) Translate by position relative to target
-                # 3) Anti-rotate by target rotation
-                # Gives us the candidate relative to the target without having to modify,
-                # reload, recalculate or revoxelize any of the target morphologies.
-                # So in the case of a single target morphology we can keep that around.
-                morpho.rotate(crot)
-                morpho.translate(cpos - tpos)
-                morpho.rotate(trot.inv())
-                cvoxels = morpho.voxelize(N=self._n_cvoxels)
-                boxes = cvoxels.as_boxes()
-                # Filter out the candidate voxels that overlap with target voxels.
-                overlap = [(i, v) for i, v in enumerate(tree.query(boxes)) if v]
-                if overlap:
-                    locations = self._pick_locations(
-                        target, cand, tvoxels, cvoxels, overlap
-                    )
-                    data_acc.append(locations)
-
-        # Preallocating and filling is faster than `np.concatenate` :shrugs:
-        acc_idx = np.cumsum(
-            [len(a[0]) for a in data_acc],
-        )
-        # The inline if guards against the case where there's no overlap
-        tlocs = np.empty((acc_idx[-1] if len(acc_idx) else 0, 3), dtype=int)
-        clocs = np.empty((acc_idx[-1] if len(acc_idx) else 0, 3), dtype=int)
-        for (s, e), (tblock, cblock) in zip(_pairs_with_zero(acc_idx), data_acc):
-            tlocs[s:e] = tblock
-            clocs[s:e] = cblock
-
-        if self.favor_cache == "pre":
-            src_set, dest_set = tset, cset
-            src_locs, dest_locs = tlocs, clocs
-        else:
-            src_set, dest_set = cset, tset
-            src_locs, dest_locs = clocs, tlocs
-
-        self.connect_cells(src_set, dest_set, src_locs, dest_locs)
-
-    def _pick_locations(self, tid, cid, tvoxels, cvoxels, overlap):
-        n = int(self.contacts.draw(1)[0])
-        if n <= 0:
-            return np.empty((0, 3), dtype=int), np.empty((0, 3), dtype=int)
-        cpool = cvoxels.get_data([c for c, _ in overlap])
-        tpool = [tvoxels.get_data(t) for _, t in overlap]
-        pool = np.column_stack(
-            (
-                np.repeat(cpool, [len(t) for t in tpool]),
-                np.array([*ichain(tpool)], dtype=object),
-            )
-        )
-        weights = [len(c) * len(t) for c, t in pool]
-        tlocs = []
-        clocs = []
-        for cpick, tpick in random.choices(pool, weights, k=n):
-            clocs.append((cid, *random.choice(cpick)))
-            tlocs.append((tid, *random.choice(tpick)))
-        return tlocs, clocs
-
-
-def _pairs_with_zero(iterable):
-    a, b = itertools.tee(iterable)
-    try:
-        yield 0, next(b)
-    except StopIteration:
-        pass
-    else:
-        yield from zip(a, b)
-
-
-__all__ = ["VoxelIntersection"]
+import itertools
+import random
+
+import numpy as np
+from numpy.random import default_rng
+
+from ... import config
+from ..._util import ichain
+from ...config import types
+from ..strategy import ConnectionStrategy
+from .shared import Intersectional
+
+_rng = default_rng()
+
+
+@config.node
+class VoxelIntersection(Intersectional, ConnectionStrategy):
+    """
+    This strategy finds overlap between voxelized morphologies.
+    """
+
+    contacts = config.attr(type=types.distribution(), default=1)
+    voxels_pre = config.attr(type=int, default=50)
+    voxels_post = config.attr(type=int, default=50)
+    cache = config.attr(type=bool, default=True)
+    favor_cache = config.attr(type=types.in_(["pre", "post"]), default="pre")
+
+    def connect(self, pre, post):
+        # Note on the caching terms: `targets` are the population that will be cached the
+        # strongest; their voxelized tree will remain in place, while the candidates are
+        # rotated and translated to overlap the target tree.
+        # The choice to make something cached harder is if they have less different
+        # morphologies, and good choices for candidates are the population with more
+        # numerous and smaller morphologies.
+        if self.favor_cache == "pre":
+            targets = pre
+            candidates = post
+            self._n_tvoxels = self.voxels_pre
+            self._n_cvoxels = self.voxels_post
+            target_morpho = self.presynaptic.morpho_loader
+            cand_morpho = self.postsynaptic.morpho_loader
+        else:
+            targets = post
+            candidates = pre
+            self._n_tvoxels = self.voxels_post
+            self._n_cvoxels = self.voxels_pre
+            target_morpho = self.postsynaptic.morpho_loader
+            cand_morpho = self.presynaptic.morpho_loader
+        combo_itr = self.candidate_intersection(targets, candidates)
+        mset_cache = {}
+        for target_set, cand_set, match_itr in combo_itr:
+            if self.cache:
+                if id(target_set) not in mset_cache:
+                    mset_cache[id(target_set)] = target_morpho(target_set)
+                if id(cand_set) not in mset_cache:
+                    mset_cache[id(cand_set)] = cand_morpho(cand_set)
+                target_mset = mset_cache[id(target_set)]
+                cand_mset = mset_cache[id(cand_set)]
+            else:
+                target_mset = target_morpho(target_set)
+                cand_mset = cand_morpho(cand_set)
+            self._match_voxel_intersection(
+                match_itr, target_set, cand_set, target_mset, cand_mset
+            )
+
+    def _match_voxel_intersection(self, matches, tset, cset, tmset, cmset):
+        # Soft-caching caches at the IO level and gives you a fresh copy of the morphology
+        # each time, the `cached_voxelize` function we need wouldn't have any effect!
+        tm_iter = tmset.iter_morphologies(cache=self.cache, hard_cache=self.cache)
+        target_itrs = zip(tset.load_positions(), tset.load_rotations().iter(), tm_iter)
+        rotations = cset.load_rotations()
+        positions = cset.load_positions()
+        data_acc = []
+        for target, candidates in enumerate(matches):
+            tpos, trot, tmor = next(target_itrs)
+            if not len(candidates):
+                # No need to load or voxelize if there's no candidates anyway
+                continue
+            # Load and voxelize the target into a box tree
+            if self.cache:
+                tvoxels = tmor.cached_voxelize(N=self._n_tvoxels)
+            else:
+                tvoxels = tmor.voxelize(N=self._n_tvoxels)
+            tree = tvoxels.as_boxtree(cache=self.cache)
+            for cand in candidates:
+                cpos = positions[cand]
+                crot = rotations[cand]
+                # Don't hard cache, as we mutate the instance we get.
+                morpho = cmset.get(cand, cache=self.cache, hard_cache=False)
+                # Transform candidate, keep target unrotated and untranslated at origin:
+                # 1) Rotate self by own rotation
+                # 2) Translate by position relative to target
+                # 3) Anti-rotate by target rotation
+                # Gives us the candidate relative to the target without having to modify,
+                # reload, recalculate or revoxelize any of the target morphologies.
+                # So in the case of a single target morphology we can keep that around.
+                morpho.rotate(crot)
+                morpho.translate(cpos - tpos)
+                morpho.rotate(trot.inv())
+                cvoxels = morpho.voxelize(N=self._n_cvoxels)
+                boxes = cvoxels.as_boxes()
+                # Filter out the candidate voxels that overlap with target voxels.
+                overlap = [(i, v) for i, v in enumerate(tree.query(boxes)) if v]
+                if overlap:
+                    locations = self._pick_locations(
+                        target, cand, tvoxels, cvoxels, overlap
+                    )
+                    data_acc.append(locations)
+
+        # Preallocating and filling is faster than `np.concatenate` :shrugs:
+        acc_idx = np.cumsum(
+            [len(a[0]) for a in data_acc],
+        )
+        # The inline if guards against the case where there's no overlap
+        tlocs = np.empty((acc_idx[-1] if len(acc_idx) else 0, 3), dtype=int)
+        clocs = np.empty((acc_idx[-1] if len(acc_idx) else 0, 3), dtype=int)
+        for (s, e), (tblock, cblock) in zip(_pairs_with_zero(acc_idx), data_acc):
+            tlocs[s:e] = tblock
+            clocs[s:e] = cblock
+
+        if self.favor_cache == "pre":
+            src_set, dest_set = tset, cset
+            src_locs, dest_locs = tlocs, clocs
+        else:
+            src_set, dest_set = cset, tset
+            src_locs, dest_locs = clocs, tlocs
+
+        self.connect_cells(src_set, dest_set, src_locs, dest_locs)
+
+    def _pick_locations(self, tid, cid, tvoxels, cvoxels, overlap):
+        n = int(self.contacts.draw(1)[0])
+        if n <= 0:
+            return np.empty((0, 3), dtype=int), np.empty((0, 3), dtype=int)
+        cpool = cvoxels.get_data([c for c, _ in overlap])
+        tpool = [tvoxels.get_data(t) for _, t in overlap]
+        pool = np.column_stack(
+            (
+                np.repeat(cpool, [len(t) for t in tpool]),
+                np.array([*ichain(tpool)], dtype=object),
+            )
+        )
+        weights = [len(c) * len(t) for c, t in pool]
+        tlocs = []
+        clocs = []
+        for cpick, tpick in random.choices(pool, weights, k=n):
+            clocs.append((cid, *random.choice(cpick)))
+            tlocs.append((tid, *random.choice(tpick)))
+        return tlocs, clocs
+
+
+def _pairs_with_zero(iterable):
+    a, b = itertools.tee(iterable)
+    try:
+        yield 0, next(b)
+    except StopIteration:
+        pass
+    else:
+        yield from zip(a, b)
+
+
+__all__ = ["VoxelIntersection"]
```

### Comparing `bsb_core-4.0.1/bsb/connectivity/general.py` & `bsb_core-4.1.0/bsb/connectivity/general.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,77 +1,77 @@
-import typing
-
-import numpy as np
-
-from .. import config
-from ..config import types
-from ..mixins import InvertedRoI
-from .strategy import ConnectionStrategy
-
-if typing.TYPE_CHECKING:
-    from ..config import Distribution
-
-
-@config.node
-class Convergence(ConnectionStrategy):
-    """
-    Connect cells based on a convergence distribution, i.e. by connecting each source cell
-    to X target cells.
-    """
-
-    convergence: "Distribution" = config.attr(type=types.distribution(), required=True)
-
-    def connect(self):
-        raise NotImplementedError("Needs to be restored, please open an issue.")
-
-
-class AllToAll(ConnectionStrategy):
-    """
-    All to all connectivity between two neural populations
-    """
-
-    def connect(self, pre, post):
-        for from_ps in pre.placement:
-            fl = len(from_ps)
-            for to_ps in post.placement:
-                len_ = len(to_ps)
-                ml = fl * len_
-                src_locs = np.full((ml, 3), -1)
-                dest_locs = np.full((ml, 3), -1)
-                src_locs[:, 0] = np.repeat(np.arange(fl), len_)
-                dest_locs[:, 0] = np.tile(np.arange(len_), fl)
-                self.connect_cells(from_ps, to_ps, src_locs, dest_locs)
-
-
-@config.node
-class FixedIndegree(InvertedRoI, ConnectionStrategy):
-    """
-    Connect a group of postsynaptic cell types to ``indegree`` uniformly random
-    presynaptic cells from all the presynaptic cell types.
-    """
-
-    indegree: int = config.attr(type=int, required=True)
-
-    def connect(self, pre, post):
-        in_ = self.indegree
-        rng = np.random.default_rng()
-        high = sum(len(ps) for ps in pre.placement)
-        for ps in post.placement:
-            l = len(ps)
-            pre_targets = np.full((l * in_, 3), -1)
-            post_targets = np.full((l * in_, 3), -1)
-            ptr = 0
-            for i in range(l):
-                post_targets[ptr : ptr + in_, 0] = i
-                pre_targets[ptr : ptr + in_, 0] = rng.choice(high, in_, replace=False)
-                ptr += in_
-            lowmux = 0
-            for pre_ps in pre.placement:
-                highmux = lowmux + len(pre_ps)
-                demux_idx = (pre_targets[:, 0] >= lowmux) & (pre_targets[:, 0] < highmux)
-                demuxed = pre_targets[demux_idx]
-                demuxed[:, 0] -= lowmux
-                self.connect_cells(pre_ps, ps, demuxed, post_targets[demux_idx])
-                lowmux = highmux
-
-
-__all__ = ["AllToAll", "Convergence", "FixedIndegree"]
+import typing
+
+import numpy as np
+
+from .. import config
+from ..config import types
+from ..mixins import InvertedRoI
+from .strategy import ConnectionStrategy
+
+if typing.TYPE_CHECKING:
+    from ..config import Distribution
+
+
+@config.node
+class Convergence(ConnectionStrategy):
+    """
+    Connect cells based on a convergence distribution, i.e. by connecting each source cell
+    to X target cells.
+    """
+
+    convergence: "Distribution" = config.attr(type=types.distribution(), required=True)
+
+    def connect(self):
+        raise NotImplementedError("Needs to be restored, please open an issue.")
+
+
+class AllToAll(ConnectionStrategy):
+    """
+    All to all connectivity between two neural populations
+    """
+
+    def connect(self, pre, post):
+        for from_ps in pre.placement:
+            fl = len(from_ps)
+            for to_ps in post.placement:
+                len_ = len(to_ps)
+                ml = fl * len_
+                src_locs = np.full((ml, 3), -1)
+                dest_locs = np.full((ml, 3), -1)
+                src_locs[:, 0] = np.repeat(np.arange(fl), len_)
+                dest_locs[:, 0] = np.tile(np.arange(len_), fl)
+                self.connect_cells(from_ps, to_ps, src_locs, dest_locs)
+
+
+@config.node
+class FixedIndegree(InvertedRoI, ConnectionStrategy):
+    """
+    Connect a group of postsynaptic cell types to ``indegree`` uniformly random
+    presynaptic cells from all the presynaptic cell types.
+    """
+
+    indegree: int = config.attr(type=int, required=True)
+
+    def connect(self, pre, post):
+        in_ = self.indegree
+        rng = np.random.default_rng()
+        high = sum(len(ps) for ps in pre.placement)
+        for ps in post.placement:
+            l = len(ps)
+            pre_targets = np.full((l * in_, 3), -1)
+            post_targets = np.full((l * in_, 3), -1)
+            ptr = 0
+            for i in range(l):
+                post_targets[ptr : ptr + in_, 0] = i
+                pre_targets[ptr : ptr + in_, 0] = rng.choice(high, in_, replace=False)
+                ptr += in_
+            lowmux = 0
+            for pre_ps in pre.placement:
+                highmux = lowmux + len(pre_ps)
+                demux_idx = (pre_targets[:, 0] >= lowmux) & (pre_targets[:, 0] < highmux)
+                demuxed = pre_targets[demux_idx]
+                demuxed[:, 0] -= lowmux
+                self.connect_cells(pre_ps, ps, demuxed, post_targets[demux_idx])
+                lowmux = highmux
+
+
+__all__ = ["AllToAll", "Convergence", "FixedIndegree"]
```

### Comparing `bsb_core-4.0.1/bsb/connectivity/strategy.py` & `bsb_core-4.1.0/bsb/connectivity/strategy.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,274 +1,274 @@
-import abc
-import typing
-from itertools import chain
-
-from .. import config
-from .._util import ichain, obj_str_insert
-from ..config import refs, types
-from ..exceptions import ConnectivityError
-from ..mixins import HasDependencies
-from ..profiling import node_meter
-from ..reporting import warn
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-    from ..core import Scaffold
-    from ..morphologies import MorphologySet
-    from ..services import JobPool
-    from ..storage.interfaces import PlacementSet
-
-
-@config.node
-class Hemitype:
-    """
-    Class used to represent one (pre- or postsynaptic) side of a connection rule.
-    """
-
-    scaffold: "Scaffold"
-
-    cell_types: list["CellType"] = config.reflist(refs.cell_type_ref, required=True)
-    """List of cell types to use in connection."""
-    labels: list[str] = config.attr(type=types.list())
-    """List of labels to filter the placement set by."""
-    morphology_labels: list[str] = config.attr(type=types.list())
-    """List of labels to filter the morphologies by."""
-    morpho_loader: typing.Callable[["PlacementSet"], "MorphologySet"] = config.attr(
-        type=types.function_(),
-        required=False,
-        call_default=False,
-        default=(lambda ps: ps.load_morphologies()),
-    )
-    """
-    Function to load the morphologies (MorphologySet) from a PlacementSet. This override
-    can allow temporary dynamic morphology generation during the connectivity phase, from
-    a much smaller, or empty, MorphologySet. It is useful for example when the task would
-    take too much disk space or time otherwise.
-    """
-
-
-class HemitypeCollection:
-    def __init__(self, hemitype, roi):
-        self.hemitype = hemitype
-        self.roi = roi
-
-    def __iter__(self):
-        return iter(self.hemitype.cell_types)
-
-    @property
-    def placement(self):
-        return [
-            ct.get_placement_set(
-                chunks=self.roi,
-                labels=self.hemitype.labels,
-                morphology_labels=self.hemitype.morphology_labels,
-            )
-            for ct in self.hemitype.cell_types
-        ]
-
-
-@config.dynamic(attr_name="strategy", required=True)
-class ConnectionStrategy(abc.ABC, HasDependencies):
-    scaffold: "Scaffold"
-    name: str = config.attr(key=True)
-    """Name used to refer to the connectivity strategy"""
-    presynaptic: Hemitype = config.attr(type=Hemitype, required=True)
-    """Presynaptic (source) neuron population"""
-    postsynaptic: Hemitype = config.attr(type=Hemitype, required=True)
-    """Postsynaptic (target) neuron population"""
-    depends_on: list["ConnectionStrategy"] = config.reflist(refs.connectivity_ref)
-    """The list of strategies that must run before this one"""
-    output_naming: typing.Union[str, None, dict[str, dict[str, str, None, list[str]]]] = (
-        config.attr(
-            type=types.or_(
-                types.str(),
-                types.dict(
-                    type=types.dict(
-                        type=types.or_(
-                            types.str(), types.list(type=types.str()), types.none()
-                        )
-                    )
-                ),
-                types.list(type=types.str()),
-            )
-        )
-    )
-    """Specifies how to name the output ConnectivitySets in which the connections between cell type pairs are stored."""
-
-    def __init_subclass__(cls, **kwargs):
-        super(cls, cls).__init_subclass__(**kwargs)
-        # Decorate subclasses to measure performance
-        node_meter("connect")(cls)
-
-    def __hash__(self):
-        return id(self)
-
-    def __lt__(self, other):
-        # This comparison should sort connection strategies by name, via __repr__ below
-        return str(self) < str(other)
-
-    @obj_str_insert
-    def __repr__(self):
-        if not hasattr(self, "scaffold"):
-            return f"'{self.name}'"
-        pre = [ct.name for ct in self.presynaptic.cell_types]
-        post = [ct.name for ct in self.postsynaptic.cell_types]
-        return f"'{self.name}', connecting {pre} to {post}"
-
-    @abc.abstractmethod
-    def connect(self, presyn_collection, postsyn_collection):
-        pass
-
-    def get_deps(self):
-        return set(self.depends_on)
-
-    def _get_connect_args_from_job(self, pre_roi, post_roi):
-        pre = HemitypeCollection(self.presynaptic, pre_roi)
-        post = HemitypeCollection(self.postsynaptic, post_roi)
-        return pre, post
-
-    def connect_cells(self, pre_set, post_set, src_locs, dest_locs, tag=None):
-        names = self.get_output_names(pre_set.cell_type, post_set.cell_type)
-        between_msg = f"between {pre_set.cell_type.name} and {post_set.cell_type.name}"
-        if len(names) == 0:
-            raise ConnectivityError(
-                f"Connections {between_msg} have been disabled by output naming."
-            )
-        elif len(names) == 1:
-            name = names[0]
-            if tag is not None and tag != name:
-                raise ConnectivityError(
-                    f"Tag ('{tag}') and output name ('{name}') mismatch."
-                )
-        else:
-            names_msg = f"{between_msg} (names: {', '.join(names)})."
-            if tag is None:
-                raise ConnectivityError(
-                    f"No tag was given to decide between multiple output names {names_msg}"
-                )
-            elif tag not in names:
-                raise ConnectivityError(
-                    f"Tag '{tag}' is not a valid output name {names_msg}"
-                )
-            else:
-                name = tag
-
-        cs = self.scaffold.require_connectivity_set(
-            pre_set.cell_type, post_set.cell_type, name
-        )
-        cs.connect(pre_set, post_set, src_locs, dest_locs)
-
-    def get_region_of_interest(self, chunk):
-        pass
-
-    def queue(self, pool: "JobPool"):
-        """
-        Specifies how to queue this connectivity strategy into a job pool. Can
-        be overridden, the default implementation asks each partition to chunk
-        itself and creates 1 placement job per chunk.
-        """
-        # Get the queued jobs of all the strategies we depend on.
-        dep_jobs = set(
-            chain.from_iterable(
-                pool.get_submissions_of(strat) for strat in self.get_deps()
-            )
-        )
-        pre_types = self.presynaptic.cell_types
-        # Iterate over each chunk that is populated by our presynaptic cell types.
-        from_chunks = set(
-            chain.from_iterable(
-                ct.get_placement_set().get_all_chunks() for ct in pre_types
-            )
-        )
-        rois = {
-            chunk: roi
-            for chunk in from_chunks
-            if (roi := self.get_region_of_interest(chunk)) is None or len(roi)
-        }
-        if not rois:
-            warn(
-                f"No overlap found between {[pre.name for pre in pre_types]} and "
-                f"{[post.name for post in self.postsynaptic.cell_types]} "
-                f"in '{self.name}'."
-            )
-        for chunk, roi in rois.items():
-            job = pool.queue_connectivity(self, [chunk], roi, deps=dep_jobs)
-
-    def get_cell_types(self):
-        return set(self.presynaptic.cell_types) | set(self.postsynaptic.cell_types)
-
-    def get_all_pre_chunks(self):
-        all_ps = (ct.get_placement_set() for ct in self.presynaptic.cell_types)
-        chunks = set(ichain(ps.get_all_chunks() for ps in all_ps))
-        return list(chunks)
-
-    def get_all_post_chunks(self):
-        all_ps = (ct.get_placement_set() for ct in self.postsynaptic.cell_types)
-        chunks = set(ichain(ps.get_all_chunks() for ps in all_ps))
-        return list(chunks)
-
-    def get_output_names(self, pre=None, post=None):
-        if (pre is None) != (post is None):
-            raise RuntimeError("pre and post must be specified or omitted together.")
-        if pre is not None and (
-            pre not in self.presynaptic.cell_types
-            or post not in self.postsynaptic.cell_types
-        ):
-            raise ValueError(
-                f"'{pre.name}' and '{post.name}' are not a valid cell pair type for this connectivity strategy."
-            )
-        if self.output_naming is None or isinstance(self.output_naming, str):
-            return self._infer_output_name(self.output_naming or self.name, pre, post)
-        elif isinstance(self.output_naming, list):
-            # Call `_infer_output_name` for each given `base` in the list, and chain them together
-            return [
-                *ichain(
-                    self._infer_output_name(base, pre, post)
-                    for base in self.output_naming
-                )
-            ]
-        else:
-            return self._get_output_name(pre, post)
-
-    def _infer_output_name(self, base, pre, post):
-        if len(self.presynaptic.cell_types) > 1 or len(self.postsynaptic.cell_types) > 1:
-            if pre is None:
-                # All output names
-                return [
-                    *ichain(
-                        self._infer_output_name(base, pre_ct, post_ct)
-                        for pre_ct in self.presynaptic.cell_types
-                        for post_ct in self.postsynaptic.cell_types
-                    )
-                ]
-            else:
-                # Pair specific output name
-                return [f"{base}_{pre.name}_to_{post.name}"]
-        else:
-            # Single output name
-            return [base]
-
-    def _get_output_name(self, pre, post):
-        if pre is None:
-            # All output names
-            return [
-                *ichain(
-                    self._get_output_name(pre_ct, post_ct)
-                    for pre_ct in self.presynaptic.cell_types
-                    for post_ct in self.postsynaptic.cell_types
-                )
-            ]
-        else:
-            # Pair specific output name
-            MISSING = type("MISSING", (), {"get": lambda *args: MISSING})()
-            spec = self.output_naming.get(pre.name, MISSING).get(post.name, MISSING)
-            if spec is MISSING:
-                return self._infer_output_name(self.name, pre, post)
-            elif spec is None:
-                return []
-            elif isinstance(spec, str):
-                return [spec]
-            else:
-                return spec
-
-
-__all__ = ["ConnectionStrategy", "Hemitype", "HemitypeCollection"]
+import abc
+import typing
+from itertools import chain
+
+from .. import config
+from .._util import ichain, obj_str_insert
+from ..config import refs, types
+from ..exceptions import ConnectivityError
+from ..mixins import HasDependencies
+from ..profiling import node_meter
+from ..reporting import warn
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+    from ..core import Scaffold
+    from ..morphologies import MorphologySet
+    from ..services import JobPool
+    from ..storage.interfaces import PlacementSet
+
+
+@config.node
+class Hemitype:
+    """
+    Class used to represent one (pre- or postsynaptic) side of a connection rule.
+    """
+
+    scaffold: "Scaffold"
+
+    cell_types: list["CellType"] = config.reflist(refs.cell_type_ref, required=True)
+    """List of cell types to use in connection."""
+    labels: list[str] = config.attr(type=types.list())
+    """List of labels to filter the placement set by."""
+    morphology_labels: list[str] = config.attr(type=types.list())
+    """List of labels to filter the morphologies by."""
+    morpho_loader: typing.Callable[["PlacementSet"], "MorphologySet"] = config.attr(
+        type=types.function_(),
+        required=False,
+        call_default=False,
+        default=(lambda ps: ps.load_morphologies()),
+    )
+    """
+    Function to load the morphologies (MorphologySet) from a PlacementSet. This override
+    can allow temporary dynamic morphology generation during the connectivity phase, from
+    a much smaller, or empty, MorphologySet. It is useful for example when the task would
+    take too much disk space or time otherwise.
+    """
+
+
+class HemitypeCollection:
+    def __init__(self, hemitype, roi):
+        self.hemitype = hemitype
+        self.roi = roi
+
+    def __iter__(self):
+        return iter(self.hemitype.cell_types)
+
+    @property
+    def placement(self):
+        return [
+            ct.get_placement_set(
+                chunks=self.roi,
+                labels=self.hemitype.labels,
+                morphology_labels=self.hemitype.morphology_labels,
+            )
+            for ct in self.hemitype.cell_types
+        ]
+
+
+@config.dynamic(attr_name="strategy", required=True)
+class ConnectionStrategy(abc.ABC, HasDependencies):
+    scaffold: "Scaffold"
+    name: str = config.attr(key=True)
+    """Name used to refer to the connectivity strategy"""
+    presynaptic: Hemitype = config.attr(type=Hemitype, required=True)
+    """Presynaptic (source) neuron population"""
+    postsynaptic: Hemitype = config.attr(type=Hemitype, required=True)
+    """Postsynaptic (target) neuron population"""
+    depends_on: list["ConnectionStrategy"] = config.reflist(refs.connectivity_ref)
+    """The list of strategies that must run before this one"""
+    output_naming: typing.Union[str, None, dict[str, dict[str, str, None, list[str]]]] = (
+        config.attr(
+            type=types.or_(
+                types.str(),
+                types.dict(
+                    type=types.dict(
+                        type=types.or_(
+                            types.str(), types.list(type=types.str()), types.none()
+                        )
+                    )
+                ),
+                types.list(type=types.str()),
+            )
+        )
+    )
+    """Specifies how to name the output ConnectivitySets in which the connections between cell type pairs are stored."""
+
+    def __init_subclass__(cls, **kwargs):
+        super(cls, cls).__init_subclass__(**kwargs)
+        # Decorate subclasses to measure performance
+        node_meter("connect")(cls)
+
+    def __hash__(self):
+        return id(self)
+
+    def __lt__(self, other):
+        # This comparison should sort connection strategies by name, via __repr__ below
+        return str(self) < str(other)
+
+    @obj_str_insert
+    def __repr__(self):
+        if not hasattr(self, "scaffold"):
+            return f"'{self.name}'"
+        pre = [ct.name for ct in self.presynaptic.cell_types]
+        post = [ct.name for ct in self.postsynaptic.cell_types]
+        return f"'{self.name}', connecting {pre} to {post}"
+
+    @abc.abstractmethod
+    def connect(self, presyn_collection, postsyn_collection):
+        pass
+
+    def get_deps(self):
+        return set(self.depends_on)
+
+    def _get_connect_args_from_job(self, pre_roi, post_roi):
+        pre = HemitypeCollection(self.presynaptic, pre_roi)
+        post = HemitypeCollection(self.postsynaptic, post_roi)
+        return pre, post
+
+    def connect_cells(self, pre_set, post_set, src_locs, dest_locs, tag=None):
+        names = self.get_output_names(pre_set.cell_type, post_set.cell_type)
+        between_msg = f"between {pre_set.cell_type.name} and {post_set.cell_type.name}"
+        if len(names) == 0:
+            raise ConnectivityError(
+                f"Connections {between_msg} have been disabled by output naming."
+            )
+        elif len(names) == 1:
+            name = names[0]
+            if tag is not None and tag != name:
+                raise ConnectivityError(
+                    f"Tag ('{tag}') and output name ('{name}') mismatch."
+                )
+        else:
+            names_msg = f"{between_msg} (names: {', '.join(names)})."
+            if tag is None:
+                raise ConnectivityError(
+                    f"No tag was given to decide between multiple output names {names_msg}"
+                )
+            elif tag not in names:
+                raise ConnectivityError(
+                    f"Tag '{tag}' is not a valid output name {names_msg}"
+                )
+            else:
+                name = tag
+
+        cs = self.scaffold.require_connectivity_set(
+            pre_set.cell_type, post_set.cell_type, name
+        )
+        cs.connect(pre_set, post_set, src_locs, dest_locs)
+
+    def get_region_of_interest(self, chunk):
+        pass
+
+    def queue(self, pool: "JobPool"):
+        """
+        Specifies how to queue this connectivity strategy into a job pool. Can
+        be overridden, the default implementation asks each partition to chunk
+        itself and creates 1 placement job per chunk.
+        """
+        # Get the queued jobs of all the strategies we depend on.
+        dep_jobs = set(
+            chain.from_iterable(
+                pool.get_submissions_of(strat) for strat in self.get_deps()
+            )
+        )
+        pre_types = self.presynaptic.cell_types
+        # Iterate over each chunk that is populated by our presynaptic cell types.
+        from_chunks = set(
+            chain.from_iterable(
+                ct.get_placement_set().get_all_chunks() for ct in pre_types
+            )
+        )
+        rois = {
+            chunk: roi
+            for chunk in from_chunks
+            if (roi := self.get_region_of_interest(chunk)) is None or len(roi)
+        }
+        if not rois:
+            warn(
+                f"No overlap found between {[pre.name for pre in pre_types]} and "
+                f"{[post.name for post in self.postsynaptic.cell_types]} "
+                f"in '{self.name}'."
+            )
+        for chunk, roi in rois.items():
+            job = pool.queue_connectivity(self, [chunk], roi, deps=dep_jobs)
+
+    def get_cell_types(self):
+        return set(self.presynaptic.cell_types) | set(self.postsynaptic.cell_types)
+
+    def get_all_pre_chunks(self):
+        all_ps = (ct.get_placement_set() for ct in self.presynaptic.cell_types)
+        chunks = set(ichain(ps.get_all_chunks() for ps in all_ps))
+        return list(chunks)
+
+    def get_all_post_chunks(self):
+        all_ps = (ct.get_placement_set() for ct in self.postsynaptic.cell_types)
+        chunks = set(ichain(ps.get_all_chunks() for ps in all_ps))
+        return list(chunks)
+
+    def get_output_names(self, pre=None, post=None):
+        if (pre is None) != (post is None):
+            raise RuntimeError("pre and post must be specified or omitted together.")
+        if pre is not None and (
+            pre not in self.presynaptic.cell_types
+            or post not in self.postsynaptic.cell_types
+        ):
+            raise ValueError(
+                f"'{pre.name}' and '{post.name}' are not a valid cell pair type for this connectivity strategy."
+            )
+        if self.output_naming is None or isinstance(self.output_naming, str):
+            return self._infer_output_name(self.output_naming or self.name, pre, post)
+        elif isinstance(self.output_naming, list):
+            # Call `_infer_output_name` for each given `base` in the list, and chain them together
+            return [
+                *ichain(
+                    self._infer_output_name(base, pre, post)
+                    for base in self.output_naming
+                )
+            ]
+        else:
+            return self._get_output_name(pre, post)
+
+    def _infer_output_name(self, base, pre, post):
+        if len(self.presynaptic.cell_types) > 1 or len(self.postsynaptic.cell_types) > 1:
+            if pre is None:
+                # All output names
+                return [
+                    *ichain(
+                        self._infer_output_name(base, pre_ct, post_ct)
+                        for pre_ct in self.presynaptic.cell_types
+                        for post_ct in self.postsynaptic.cell_types
+                    )
+                ]
+            else:
+                # Pair specific output name
+                return [f"{base}_{pre.name}_to_{post.name}"]
+        else:
+            # Single output name
+            return [base]
+
+    def _get_output_name(self, pre, post):
+        if pre is None:
+            # All output names
+            return [
+                *ichain(
+                    self._get_output_name(pre_ct, post_ct)
+                    for pre_ct in self.presynaptic.cell_types
+                    for post_ct in self.postsynaptic.cell_types
+                )
+            ]
+        else:
+            # Pair specific output name
+            MISSING = type("MISSING", (), {"get": lambda *args: MISSING})()
+            spec = self.output_naming.get(pre.name, MISSING).get(post.name, MISSING)
+            if spec is MISSING:
+                return self._infer_output_name(self.name, pre, post)
+            elif spec is None:
+                return []
+            elif isinstance(spec, str):
+                return [spec]
+            else:
+                return spec
+
+
+__all__ = ["ConnectionStrategy", "Hemitype", "HemitypeCollection"]
```

### Comparing `bsb_core-4.0.1/bsb/core.py` & `bsb_core-4.1.0/bsb/core.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,805 +1,805 @@
-import itertools
-import os
-import sys
-import typing
-
-import numpy as np
-
-from ._util import obj_str_insert
-from .config._config import Configuration
-from .connectivity import ConnectionStrategy
-from .exceptions import (
-    InputError,
-    MissingActiveConfigError,
-    NodeNotFoundError,
-    RedoError,
-)
-from .placement import PlacementStrategy
-from .profiling import meter
-from .reporting import report
-from .services import MPI, JobPool
-from .services._pool_listeners import NonTTYTerminalListener, TTYTerminalListener
-from .services.pool import Job, Workflow
-from .simulation import get_simulation_adapter
-from .storage import Storage, open_storage
-from .storage._chunks import Chunk
-
-if typing.TYPE_CHECKING:
-    from .cell_types import CellType
-    from .config._config import NetworkNode as Network
-    from .postprocessing import AfterPlacementHook
-    from .simulation.simulation import Simulation
-    from .storage.interfaces import (
-        ConnectivitySet,
-        FileStore,
-        MorphologyRepository,
-        PlacementSet,
-    )
-    from .topology import Partition, Region
-
-
-@meter()
-def from_storage(root):
-    """
-    Load :class:`.core.Scaffold` from a storage object.
-
-    :param root: Root (usually path) pointing to the storage object.
-    :returns: A network scaffold
-    :rtype: :class:`Scaffold`
-    """
-    return open_storage(root).load()
-
-
-_cfg_props = (
-    "network",
-    "regions",
-    "partitions",
-    "cell_types",
-    "placement",
-    "after_placement",
-    "connectivity",
-    "after_connectivity",
-    "simulations",
-)
-
-
-def _config_property(name):
-    def fget(self):
-        return getattr(self.configuration, name)
-
-    def fset(self, value):
-        setattr(self.configuration, name, value)
-
-    prop = property(fget)
-    return prop.setter(fset)
-
-
-def _get_linked_config(storage=None):
-    import bsb.config
-
-    try:
-        cfg = storage.load_active_config()
-    except Exception:
-        import bsb.options
-
-        path = bsb.options.config
-    else:
-        path = cfg._meta.get("path", None)
-    if path and os.path.exists(path):
-        with open(path, "r") as f:
-            cfg = bsb.config.parse_configuration_file(f)
-            return cfg
-    else:
-        return None
-
-
-def _bad_flag(flag: bool):
-    return flag is not None and bool(flag) is not flag
-
-
-class Scaffold:
-    """
-
-    This is the main object of the bsb package, it represents a network and puts together
-    all the pieces that make up the model description such as the
-    :class:`~.config.Configuration` with the technical side like the
-    :class:`~.storage.Storage`.
-    """
-
-    network: "Network"
-    regions: typing.Dict[str, "Region"]
-    partitions: typing.Dict[str, "Partition"]
-    cell_types: typing.Dict[str, "CellType"]
-    placement: typing.Dict[str, "PlacementStrategy"]
-    after_placement: typing.Dict[str, "AfterPlacementHook"]
-    connectivity: typing.Dict[str, "ConnectionStrategy"]
-    after_connectivity: typing.Dict[str, "AfterPlacementHook"]
-    simulations: typing.Dict[str, "Simulation"]
-
-    def __init__(self, config=None, storage=None, clear=False, comm=None):
-        """
-        Bootstraps a network object.
-
-        :param config: The configuration to use for this network. If it is omitted the
-          :ref:`default configuration <default-config>` is used.
-        :type config: :class:`~.config.Configuration`
-        :param storage: The storage to use to read and write data for this network. If it
-          is omitted the configuration's ``Storage`` node is used to construct one.
-        :type storage: :class:`~.storage.Storage`
-        :param clear: Start with a new network, clearing any previously stored information
-        :type clear: bool
-        :returns: A network object
-        :rtype: :class:`~.core.Scaffold`
-        """
-        self._pool_listeners: list[tuple[typing.Callable[[list["Job"]], None], float]] = (
-            []
-        )
-        self._configuration = None
-        self._storage = None
-        self._comm = comm or MPI
-        self._bootstrap(config, storage, clear=clear)
-
-    def __contains__(self, component):
-        return getattr(component, "scaffold", None) is self
-
-    @obj_str_insert
-    def __repr__(self):
-        file = os.path.abspath(self.storage.root)
-        cells_placed = len(self.cell_types)
-        n_types = len(self.connectivity)
-        return f"'{file}' with {cells_placed} cell types, and {n_types} connection_types"
-
-    def is_main_process(self) -> bool:
-        return not MPI.get_rank()
-
-    def is_worker_process(self) -> bool:
-        return bool(MPI.get_rank())
-
-    def _bootstrap(self, config, storage, clear=False):
-        if config is None:
-            # No config given, check for linked configs, or stored configs, otherwise
-            # make default config.
-            linked = _get_linked_config(storage)
-            if linked:
-                report(f"Pulling configuration from linked {linked}.", level=2)
-                config = linked
-            elif storage is not None:
-                try:
-                    config = storage.load_active_config()
-                except MissingActiveConfigError:
-                    config = Configuration.default()
-            else:
-                config = Configuration.default()
-        if not storage:
-            # No storage given, create one.
-            report("Creating storage from config.", level=4)
-            storage = Storage(config.storage.engine, config.storage.root)
-        if clear:
-            # Storage given, but asked to clear it before use.
-            storage.remove()
-            storage.create()
-        # Synchronize the scaffold, config and storage objects for use together
-        self._configuration = config
-        # Make sure the storage config node reflects the storage we are using
-        config._update_storage_node(storage)
-        # Give the scaffold access to the unitialized storage object (for use during
-        # config bootstrapping).
-        self._storage = storage
-        # First, the scaffold is passed to each config node, and their boot methods called
-        self._configuration._bootstrap(self)
-        # Then, `storage` is initted for the scaffold, and `config` is stored (happens
-        # inside the `storage` property).
-        self.storage = storage
-
-    storage_cfg = _config_property("storage")
-    for attr in _cfg_props:
-        vars()[attr] = _config_property(attr)
-
-    @property
-    def configuration(self) -> Configuration:
-        return self._configuration
-
-    @configuration.setter
-    def configuration(self, cfg: Configuration):
-        self._configuration = cfg
-        cfg._bootstrap(self)
-        self.storage.store_active_config(cfg)
-
-    @property
-    def storage(self) -> Storage:
-        return self._storage
-
-    @storage.setter
-    def storage(self, storage: Storage):
-        self._storage = storage
-        storage.init(self)
-
-    @property
-    def morphologies(self) -> "MorphologyRepository":
-        return self.storage.morphologies
-
-    @property
-    def files(self) -> "FileStore":
-        return self.storage.files
-
-    def clear(self):
-        """
-        Clears the storage. This deletes any existing network data!
-        """
-        self.storage.renew(self)
-
-    def clear_placement(self):
-        """
-        Clears the placement storage.
-        """
-        self.storage.clear_placement(self)
-
-    def clear_connectivity(self):
-        """
-        Clears the connectivity storage.
-        """
-        self.storage.clear_connectivity()
-
-    def resize(self, x=None, y=None, z=None):
-        """
-        Updates the topology boundary indicators. Use before placement, updates
-        only the abstract topology tree, does not rescale, prune or otherwise
-        alter already existing placement data.
-        """
-        from .topology._layout import box_layout
-
-        if x is not None:
-            self.network.x = x
-        if y is not None:
-            self.network.y = y
-        if z is not None:
-            self.network.z = z
-        self.topology.do_layout(
-            box_layout(
-                self.network.origin,
-                np.array(self.network.origin)
-                + [self.network.x, self.network.y, self.network.z],
-            )
-        )
-
-    @meter()
-    def run_placement(self, strategies=None, fail_fast=True, pipelines=True):
-        """
-        Run placement strategies.
-        """
-        if pipelines:
-            self.run_pipelines()
-        if strategies is None:
-            strategies = [*self.placement.values()]
-        strategies = PlacementStrategy.sort_deps(strategies)
-        with self.create_job_pool(fail_fast=fail_fast) as pool:
-            if pool.is_main():
-
-                def scheduler(strategy):
-                    strategy.queue(pool, self.network.chunk_size)
-
-                pool.schedule(strategies, scheduler)
-            pool.execute()
-
-    @meter()
-    def run_connectivity(self, strategies=None, fail_fast=True, pipelines=True):
-        """
-        Run connection strategies.
-        """
-        if pipelines:
-            self.run_pipelines()
-        if strategies is None:
-            strategies = set(self.connectivity.values())
-        strategies = ConnectionStrategy.sort_deps(strategies)
-        with self.create_job_pool(fail_fast=fail_fast) as pool:
-            if pool.is_main():
-                pool.schedule(strategies)
-            pool.execute()
-
-    @meter()
-    def run_placement_strategy(self, strategy):
-        """
-        Run a single placement strategy.
-        """
-        self.run_placement([strategy])
-
-    @meter()
-    def run_after_placement(self, hooks=None, fail_fast=None, pipelines=True):
-        """
-        Run after placement hooks.
-        """
-        if hooks is None:
-            hooks = self.after_placement
-        with self.create_job_pool(fail_fast) as pool:
-            if pool.is_main():
-                pool.schedule(hooks)
-            pool.execute()
-
-    @meter()
-    def run_after_connectivity(self, hooks=None, fail_fast=None, pipelines=True):
-        """
-        Run after placement hooks.
-        """
-        if hooks is None:
-            hooks = self.after_placement
-        with self.create_job_pool(fail_fast) as pool:
-            if pool.is_main():
-                pool.schedule(hooks)
-            pool.execute()
-
-    @meter()
-    def compile(
-        self,
-        skip_placement=False,
-        skip_connectivity=False,
-        skip_after_placement=False,
-        skip_after_connectivity=False,
-        only=None,
-        skip=None,
-        clear=False,
-        append=False,
-        redo=False,
-        force=False,
-        fail_fast=True,
-    ):
-        """
-        Run reconstruction steps in the scaffold sequence to obtain a full network.
-        """
-        existed = self.storage.preexisted
-        if skip_placement:
-            p_strats = []
-        else:
-            p_strats = self.get_placement(skip=skip, only=only)
-        if skip_connectivity:
-            c_strats = []
-        else:
-            c_strats = self.get_connectivity(skip=skip, only=only)
-        todo_list_str = ", ".join(s.name for s in itertools.chain(p_strats, c_strats))
-        report(f"Compiling the following strategies: {todo_list_str}", level=2)
-        if _bad_flag(clear) or _bad_flag(redo) or _bad_flag(append):
-            raise InputError(
-                "`clear`, `redo` and `append` are strictly boolean flags. "
-                "Pass the strategies to run to the skip/only options instead."
-            )
-        if sum((bool(clear), bool(redo), bool(append))) > 1:
-            raise InputError("`clear`, `redo` and `append` are mutually exclusive.")
-        if existed:
-            if not (clear or append or redo):
-                raise FileExistsError(
-                    f"The `{self.storage.format}` storage"
-                    + f" at `{self.storage.root}` already exists. Either move/delete it,"
-                    + " or pass one of the `clear`, `append` or `redo` arguments"
-                    + " to pick what to do with the existing data."
-                )
-            if clear:
-                report("Clearing data", level=2)
-                # Clear the placement and connectivity data, but leave any cached files
-                # and morphologies intact.
-                self.clear_placement()
-                self.clear_connectivity()
-            elif redo:
-                # In order to properly redo things, we clear some placement and connection
-                # data, but since multiple placement/connection strategies can contribute
-                # to the same sets we might be wiping their data too, and they will need
-                # to be cleared and reran as well.
-                p_strats, c_strats = self._redo_chain(p_strats, c_strats, skip, force)
-            # else:
-            #   append mode is luckily simpler, just don't clear anything :)
-
-        phases = ["pipelines"]
-        if not skip_placement:
-            phases.append("placement")
-        if not skip_after_placement:
-            phases.append("after_placement")
-        if not skip_connectivity:
-            phases.append("connectivity")
-        if not skip_after_connectivity:
-            phases.append("after_connectivity")
-        self._workflow = Workflow(phases)
-        try:
-            self.run_pipelines(fail_fast=fail_fast)
-            self._workflow.next_phase()
-            if not skip_placement:
-                placement_todo = ", ".join(s.name for s in p_strats)
-                report(f"Starting placement strategies: {placement_todo}", level=2)
-                self.run_placement(p_strats, fail_fast=fail_fast, pipelines=False)
-                self._workflow.next_phase()
-            if not skip_after_placement:
-                self.run_after_placement(pipelines=False, fail_fast=fail_fast)
-                self._workflow.next_phase()
-            if not skip_connectivity:
-                connectivity_todo = ", ".join(s.name for s in c_strats)
-                report(f"Starting connectivity strategies: {connectivity_todo}", level=2)
-                self.run_connectivity(c_strats, fail_fast=fail_fast, pipelines=False)
-                self._workflow.next_phase()
-            if not skip_after_connectivity:
-                self.run_after_connectivity(pipelines=False)
-                self._workflow.next_phase()
-        finally:
-            # After compilation we should flag the storage as having existed before so that
-            # the `clear`, `redo` and `append` flags take effect on a second `compile` pass.
-            self.storage._preexisted = True
-            del self._workflow
-
-    @meter()
-    def run_pipelines(self, fail_fast=True, pipelines=None):
-        if pipelines is None:
-            pipelines = self.get_dependency_pipelines()
-        with self.create_job_pool(fail_fast=fail_fast) as pool:
-            if pool.is_main():
-                pool.schedule(pipelines)
-            pool.execute()
-
-    @meter()
-    def run_simulation(self, simulation_name: str):
-        """
-        Run a simulation starting from the default single-instance adapter.
-
-        :param simulation_name: Name of the simulation in the configuration.
-        :type simulation_name: str
-        """
-        simulation = self.get_simulation(simulation_name)
-        adapter = get_simulation_adapter(simulation.simulator)
-        return adapter.simulate(simulation)[0]
-
-    def get_simulation(self, sim_name: str) -> "Simulation":
-        """
-        Retrieve the default single-instance adapter for a simulation.
-        """
-        if sim_name not in self.simulations:
-            simstr = ", ".join(f"'{s}'" for s in self.simulations.keys())
-            raise NodeNotFoundError(
-                f"Unknown simulation '{sim_name}', choose from: {simstr}"
-            )
-        return self.configuration.simulations[sim_name]
-
-    def place_cells(
-        self,
-        cell_type,
-        positions,
-        morphologies=None,
-        rotations=None,
-        additional=None,
-        chunk=None,
-    ):
-        """
-        Place cells inside of the scaffold
-
-        .. code-block:: python
-
-            # Add one granule cell at position 0, 0, 0
-            cell_type = scaffold.get_cell_type("granule_cell")
-            scaffold.place_cells(cell_type, cell_type.layer_instance, [[0., 0., 0.]])
-
-        :param cell_type: The type of the cells to place.
-        :type cell_type: ~bsb.cell_types.CellType
-        :param positions: A collection of xyz positions to place the cells on.
-        :type positions: Any `np.concatenate` type of shape (N, 3).
-        """
-        if chunk is None:
-            chunk = Chunk([0, 0, 0], self.network.chunk_size)
-        if hasattr(chunk, "dimensions") and np.any(np.isnan(chunk.dimensions)):
-            chunk.dimensions = self.network.chunk_size
-        self.get_placement_set(cell_type).append_data(
-            chunk,
-            positions=positions,
-            morphologies=morphologies,
-            rotations=rotations,
-            additional=additional,
-        )
-
-    def create_entities(self, cell_type, count):
-        """
-        Create entities in the simulation space.
-
-        Entities are different from cells because they have no positional data and
-        don't influence the placement step. They do have a representation in the
-        connection and simulation step.
-
-        :param cell_type: The cell type of the entities
-        :type cell_type: ~bsb.cell_types.CellType
-        :param count: Number of entities to place
-        :type count: int
-        :todo: Allow `additional` data for entities
-        """
-        if count == 0:
-            return
-        ps = self.get_placement_set(cell_type)
-        # Append entity data to the default chunk 000
-        chunk = Chunk([0, 0, 0], self.network.chunk_size)
-        ps.append_entities(chunk, count)
-
-    def get_placement(
-        self, cell_types=None, skip=None, only=None
-    ) -> typing.List["PlacementStrategy"]:
-        if cell_types is not None:
-            cell_types = [
-                self.cell_types[ct] if isinstance(ct, str) else ct for ct in cell_types
-            ]
-        return [
-            val
-            for key, val in self.placement.items()
-            if (cell_types is None or any(ct in cell_types for ct in val.cell_types))
-            and (only is None or key in only)
-            and (skip is None or key not in skip)
-        ]
-
-    def get_placement_of(self, *cell_types):
-        """
-        Find all of the placement strategies that given certain cell types.
-
-        :param cell_types: Cell types (or their names) of interest.
-        :type cell_types: Union[~bsb.cell_types.CellType, str]
-        """
-        return self.get_placement(cell_types=cell_types)
-
-    def get_placement_set(
-        self, type, chunks=None, labels=None, morphology_labels=None
-    ) -> "PlacementSet":
-        """
-        Return a cell type's placement set from the output formatter.
-
-        :param tag: Unique identifier of the placement set in the storage
-        :type tag: str
-        :returns: A placement set
-        :param labels: Labels to filter the placement set by.
-        :type labels: list[str]
-        :param morphology_labels: Subcellular labels to apply to the morphologies.
-        :type morphology_labels: list[str]
-        :rtype: :class:`~.storage.interfaces.PlacementSet`
-        """
-        if isinstance(type, str):
-            type = self.cell_types[type]
-        return self.storage.get_placement_set(
-            type, chunks=chunks, labels=labels, morphology_labels=morphology_labels
-        )
-
-    def get_placement_sets(self) -> typing.List["PlacementSet"]:
-        """
-        Return all of the placement sets present in the network.
-
-        :rtype: List[~bsb.storage.interfaces.PlacementSet]
-        """
-        return [cell_type.get_placement_set() for cell_type in self.cell_types.values()]
-
-    def get_connectivity(
-        self, anywhere=None, presynaptic=None, postsynaptic=None, skip=None, only=None
-    ) -> typing.List["ConnectivitySet"]:
-        conntype_filtered = self._connectivity_query(
-            any_query=set(self._sanitize_ct(anywhere)),
-            pre_query=set(self._sanitize_ct(presynaptic)),
-            post_query=set(self._sanitize_ct(postsynaptic)),
-        )
-        return [
-            ct
-            for ct in conntype_filtered
-            if (only is None or ct.name in only) and (skip is None or ct.name not in skip)
-        ]
-
-    def get_connectivity_sets(self) -> typing.List["ConnectivitySet"]:
-        """
-        Return all connectivity sets from the output formatter.
-
-        :param tag: Unique identifier of the connectivity set in the output formatter
-        :type tag: str
-        :returns: All connectivity sets
-        """
-        return [self._load_cs_types(cs) for cs in self.storage.get_connectivity_sets()]
-
-    def require_connectivity_set(self, pre, post, tag=None) -> "ConnectivitySet":
-        return self._load_cs_types(
-            self.storage.require_connectivity_set(pre, post, tag), pre, post
-        )
-
-    def get_connectivity_set(self, tag=None, pre=None, post=None) -> "ConnectivitySet":
-        """
-        Return a connectivity set from the output formatter.
-
-        :param tag: Unique identifier of the connectivity set in the output formatter
-        :type tag: str
-        :returns: A connectivity set
-        :rtype: :class:`~.storage.interfaces.ConnectivitySet`
-        """
-        if tag is None:
-            try:
-                tag = f"{pre.name}_to_{post.name}"
-            except Exception:
-                raise ValueError("Supply either `tag` or a valid pre and post cell type.")
-        return self._load_cs_types(self.storage.get_connectivity_set(tag), pre, post)
-
-    def get_cell_types(self) -> typing.List["CellType"]:
-        """
-        Return a list of all cell types in the network.
-        """
-        return [*self.configuration.cell_types.values()]
-
-    def merge(self, other, label=None):
-        raise NotImplementedError("Revisit: merge CT, PS & CS, done?")
-
-    def _sanitize_ct(self, seq_str_or_none):
-        if seq_str_or_none is None:
-            return []
-        try:
-            if isinstance(seq_str_or_none, str):
-                return [self.cell_types[seq_str_or_none]]
-            return [
-                self.cell_types[s] if isinstance(s, str) else s for s in seq_str_or_none
-            ]
-        except KeyError as e:
-            raise NodeNotFoundError(f"Cell type `{e.args[0]}` not found.")
-
-    def _connectivity_query(self, any_query=set(), pre_query=set(), post_query=set()):
-        # Filter network connection types for any type that satisfies both
-        # the presynaptic and postsynaptic query. Empty queries satisfy all
-        # types. The presynaptic query is satisfied if the conn type contains
-        # any of the queried cell types presynaptically, and same for post.
-        # The any query is satisfied if a cell type is found either pre or post.
-
-        def partial_query(types, query):
-            return not query or any(cell_type in query for cell_type in types)
-
-        def query(conn_type):
-            pre_match = partial_query(conn_type.presynaptic.cell_types, pre_query)
-            post_match = partial_query(conn_type.postsynaptic.cell_types, post_query)
-            any_match = partial_query(
-                conn_type.presynaptic.cell_types, any_query
-            ) or partial_query(conn_type.postsynaptic.cell_types, any_query)
-            return any_match or (pre_match and post_match)
-
-        types = self.connectivity.values()
-        return [*filter(query, types)]
-
-    def _redo_chain(self, p_strats, c_strats, skip, force):
-        p_contrib = set(p_strats)
-        while True:
-            # Get all the placement strategies that effect the current set of CT.
-            full_wipe = set(itertools.chain(*(ps.cell_types for ps in p_contrib)))
-            contrib = set(self.get_placement(full_wipe))
-            # Keep repeating until no new contributors are fished up.
-            if contrib.issubset(p_contrib):
-                break
-            # Grow the placement chain
-            p_contrib.update(contrib)
-        report(
-            "Redo-affected placement: " + " ".join(ps.name for ps in p_contrib), level=2
-        )
-
-        c_contrib = set(c_strats)
-        conn_wipe = full_wipe.copy()
-        if full_wipe:
-            while True:
-                contrib = set(self.get_connectivity(anywhere=conn_wipe))
-                conn_wipe.update(
-                    itertools.chain(*(ct.get_cell_types() for ct in contrib))
-                )
-                if contrib.issubset(c_contrib):
-                    break
-                c_contrib.update(contrib)
-        report(
-            "Redo-affected connectivity: " + " ".join(cs.name for cs in c_contrib),
-            level=2,
-        )
-        # Don't do greedy things without `force`
-        if not force:
-            # Error if we need to redo things the user asked to skip
-            if skip is not None:
-                unskipped = [p.name for p in p_contrib if p.name in skip]
-                if unskipped:
-                    chainstr = ", ".join(f"'{s.name}'" for s in (p_strats + c_strats))
-                    skipstr = ", ".join(f"'{s.name}'" for s in unskipped)
-                    raise RedoError(
-                        f"Can't skip {skipstr}. Redoing {chainstr} requires to redo them."
-                        + f" Omit {skipstr} from `skip` or use `force` (not recommended)."
-                    )
-            # Error if we need to redo things the user didn't ask for
-            for label, chain, og in zip(
-                ("placement", "connection"), (p_contrib, c_contrib), (p_strats, c_strats)
-            ):
-                if len(chain) > len(og):
-                    new = chain.difference(og)
-                    raise RedoError(
-                        f"Need to redo additional {label} strategies: "
-                        + ", ".join(n.name for n in new)
-                        + ". Include them or use `force` (not recommended)."
-                    )
-
-        for ct in full_wipe:
-            report(f"Clearing all data of {ct.name}", level=2)
-            ct.clear()
-
-        for ct in conn_wipe:
-            report(f"Clearing connectivity data of {ct.name}", level=2)
-            ct.clear_connections()
-
-        return p_contrib, c_contrib
-
-    def get_dependency_pipelines(self):
-        return [*self.configuration.morphologies]
-
-    def get_config_diagram(self):
-        from .config import make_configuration_diagram
-
-        return make_configuration_diagram(self.configuration)
-
-    def get_storage_diagram(self):
-        dot = f'digraph "{self.configuration.name or "network"}" {{'
-        for ps in self.get_placement_sets():
-            dot += f'\n  {ps.tag}[label="{ps.tag} ({len(ps)} {ps.cell_type.name})"]'
-        for conn in self.get_connectivity_sets():
-            dot += f"\n  {conn.pre_type.name} -> {conn.post_type.name}"
-            dot += f'[label="{conn.tag} ({len(conn)})"];'
-
-        dot += "\n}\n"
-        return dot
-
-    def _load_cs_types(
-        self, cs: "ConnectivitySet", pre=None, post=None
-    ) -> "ConnectivitySet":
-        if pre and pre.name != cs.pre_type_name:
-            raise ValueError(
-                "Given and stored type mismatch:" + f" {pre.name} vs {cs.pre_type_name}"
-            )
-        if post and post.name != cs.post_type_name:
-            raise ValueError(
-                "Given and stored type mismatch:" + f" {post.name} vs {cs.post_type_name}"
-            )
-        try:
-            cs.pre_type = self.cell_types[cs.pre_type_name]
-            cs.post_type = self.cell_types[cs.post_type_name]
-        except KeyError as e:
-            raise NodeNotFoundError(
-                f"Couldn't load '{cs.tag}' connections, missing cell type '{e.args[0]}'."
-            ) from None
-        return cs
-
-    def create_job_pool(self, fail_fast=None, quiet=False):
-        pool = JobPool(
-            self, fail_fast=fail_fast, workflow=getattr(self, "_workflow", None)
-        )
-        try:
-            # Check whether stdout is a TTY, and that it is larger than 0x0
-            # (e.g. MPI sets it to 0x0 unless an xterm is emulated.
-            tty = os.isatty(sys.stdout.fileno()) and sum(os.get_terminal_size())
-        except Exception:
-            tty = False
-        if tty:
-            fps = 25
-            default_listener = TTYTerminalListener(fps)
-            default_max_wait = 1 / fps
-        else:
-            default_listener = NonTTYTerminalListener()
-            default_max_wait = None
-        if self._pool_listeners:
-            for listener, max_wait in self._pool_listeners:
-                pool.add_listener(listener, max_wait=max_wait)
-        elif not quiet:
-            pool.add_listener(default_listener, max_wait=default_max_wait)
-        return pool
-
-    def register_listener(self, listener, max_wait=None):
-        self._pool_listeners.append((listener, max_wait))
-
-    def remove_listener(self, listener):
-        for i, (l, _) in enumerate(self._pool_listeners):
-            if l is listener:
-                self._pool_listeners.pop(i)
-                break
-
-
-class ReportListener:
-    def __init__(self, scaffold, file):
-        self.file = file
-        self.scaffold = scaffold
-
-    def __call__(self, progress):
-        report(
-            str(progress.progression)
-            + "+"
-            + str(progress.duration)
-            + "+"
-            + str(progress.time),
-            token="simulation_progress",
-        )
-
-
-__all__ = ["ReportListener", "Scaffold", "from_storage"]
+import itertools
+import os
+import sys
+import typing
+
+import numpy as np
+
+from ._util import obj_str_insert
+from .config._config import Configuration
+from .connectivity import ConnectionStrategy
+from .exceptions import (
+    InputError,
+    MissingActiveConfigError,
+    NodeNotFoundError,
+    RedoError,
+)
+from .placement import PlacementStrategy
+from .profiling import meter
+from .reporting import report
+from .services import MPI, JobPool
+from .services._pool_listeners import NonTTYTerminalListener, TTYTerminalListener
+from .services.pool import Job, Workflow
+from .simulation import get_simulation_adapter
+from .storage import Storage, open_storage
+from .storage._chunks import Chunk
+
+if typing.TYPE_CHECKING:
+    from .cell_types import CellType
+    from .config._config import NetworkNode as Network
+    from .postprocessing import AfterPlacementHook
+    from .simulation.simulation import Simulation
+    from .storage.interfaces import (
+        ConnectivitySet,
+        FileStore,
+        MorphologyRepository,
+        PlacementSet,
+    )
+    from .topology import Partition, Region
+
+
+@meter()
+def from_storage(root):
+    """
+    Load :class:`.core.Scaffold` from a storage object.
+
+    :param root: Root (usually path) pointing to the storage object.
+    :returns: A network scaffold
+    :rtype: :class:`Scaffold`
+    """
+    return open_storage(root).load()
+
+
+_cfg_props = (
+    "network",
+    "regions",
+    "partitions",
+    "cell_types",
+    "placement",
+    "after_placement",
+    "connectivity",
+    "after_connectivity",
+    "simulations",
+)
+
+
+def _config_property(name):
+    def fget(self):
+        return getattr(self.configuration, name)
+
+    def fset(self, value):
+        setattr(self.configuration, name, value)
+
+    prop = property(fget)
+    return prop.setter(fset)
+
+
+def _get_linked_config(storage=None):
+    import bsb.config
+
+    try:
+        cfg = storage.load_active_config()
+    except Exception:
+        import bsb.options
+
+        path = bsb.options.config
+    else:
+        path = cfg._meta.get("path", None)
+    if path and os.path.exists(path):
+        with open(path, "r") as f:
+            cfg = bsb.config.parse_configuration_file(f, path=path)
+            return cfg
+    else:
+        return None
+
+
+def _bad_flag(flag: bool):
+    return flag is not None and bool(flag) is not flag
+
+
+class Scaffold:
+    """
+
+    This is the main object of the bsb package, it represents a network and puts together
+    all the pieces that make up the model description such as the
+    :class:`~.config.Configuration` with the technical side like the
+    :class:`~.storage.Storage`.
+    """
+
+    network: "Network"
+    regions: typing.Dict[str, "Region"]
+    partitions: typing.Dict[str, "Partition"]
+    cell_types: typing.Dict[str, "CellType"]
+    placement: typing.Dict[str, "PlacementStrategy"]
+    after_placement: typing.Dict[str, "AfterPlacementHook"]
+    connectivity: typing.Dict[str, "ConnectionStrategy"]
+    after_connectivity: typing.Dict[str, "AfterPlacementHook"]
+    simulations: typing.Dict[str, "Simulation"]
+
+    def __init__(self, config=None, storage=None, clear=False, comm=None):
+        """
+        Bootstraps a network object.
+
+        :param config: The configuration to use for this network. If it is omitted the
+          :ref:`default configuration <default-config>` is used.
+        :type config: :class:`~.config.Configuration`
+        :param storage: The storage to use to read and write data for this network. If it
+          is omitted the configuration's ``Storage`` node is used to construct one.
+        :type storage: :class:`~.storage.Storage`
+        :param clear: Start with a new network, clearing any previously stored information
+        :type clear: bool
+        :returns: A network object
+        :rtype: :class:`~.core.Scaffold`
+        """
+        self._pool_listeners: list[tuple[typing.Callable[[list["Job"]], None], float]] = (
+            []
+        )
+        self._configuration = None
+        self._storage = None
+        self._comm = comm or MPI
+        self._bootstrap(config, storage, clear=clear)
+
+    def __contains__(self, component):
+        return getattr(component, "scaffold", None) is self
+
+    @obj_str_insert
+    def __repr__(self):
+        file = os.path.abspath(self.storage.root)
+        cells_placed = len(self.cell_types)
+        n_types = len(self.connectivity)
+        return f"'{file}' with {cells_placed} cell types, and {n_types} connection_types"
+
+    def is_main_process(self) -> bool:
+        return not MPI.get_rank()
+
+    def is_worker_process(self) -> bool:
+        return bool(MPI.get_rank())
+
+    def _bootstrap(self, config, storage, clear=False):
+        if config is None:
+            # No config given, check for linked configs, or stored configs, otherwise
+            # make default config.
+            linked = _get_linked_config(storage)
+            if linked:
+                report(f"Pulling configuration from linked {linked}.", level=2)
+                config = linked
+            elif storage is not None:
+                try:
+                    config = storage.load_active_config()
+                except MissingActiveConfigError:
+                    config = Configuration.default()
+            else:
+                config = Configuration.default()
+        if not storage:
+            # No storage given, create one.
+            report("Creating storage from config.", level=4)
+            storage = Storage(config.storage.engine, config.storage.root)
+        if clear:
+            # Storage given, but asked to clear it before use.
+            storage.remove()
+            storage.create()
+        # Synchronize the scaffold, config and storage objects for use together
+        self._configuration = config
+        # Make sure the storage config node reflects the storage we are using
+        config._update_storage_node(storage)
+        # Give the scaffold access to the unitialized storage object (for use during
+        # config bootstrapping).
+        self._storage = storage
+        # First, the scaffold is passed to each config node, and their boot methods called
+        self._configuration._bootstrap(self)
+        # Then, `storage` is initted for the scaffold, and `config` is stored (happens
+        # inside the `storage` property).
+        self.storage = storage
+
+    storage_cfg = _config_property("storage")
+    for attr in _cfg_props:
+        vars()[attr] = _config_property(attr)
+
+    @property
+    def configuration(self) -> Configuration:
+        return self._configuration
+
+    @configuration.setter
+    def configuration(self, cfg: Configuration):
+        self._configuration = cfg
+        cfg._bootstrap(self)
+        self.storage.store_active_config(cfg)
+
+    @property
+    def storage(self) -> Storage:
+        return self._storage
+
+    @storage.setter
+    def storage(self, storage: Storage):
+        self._storage = storage
+        storage.init(self)
+
+    @property
+    def morphologies(self) -> "MorphologyRepository":
+        return self.storage.morphologies
+
+    @property
+    def files(self) -> "FileStore":
+        return self.storage.files
+
+    def clear(self):
+        """
+        Clears the storage. This deletes any existing network data!
+        """
+        self.storage.renew(self)
+
+    def clear_placement(self):
+        """
+        Clears the placement storage.
+        """
+        self.storage.clear_placement(self)
+
+    def clear_connectivity(self):
+        """
+        Clears the connectivity storage.
+        """
+        self.storage.clear_connectivity()
+
+    def resize(self, x=None, y=None, z=None):
+        """
+        Updates the topology boundary indicators. Use before placement, updates
+        only the abstract topology tree, does not rescale, prune or otherwise
+        alter already existing placement data.
+        """
+        from .topology._layout import box_layout
+
+        if x is not None:
+            self.network.x = x
+        if y is not None:
+            self.network.y = y
+        if z is not None:
+            self.network.z = z
+        self.topology.do_layout(
+            box_layout(
+                self.network.origin,
+                np.array(self.network.origin)
+                + [self.network.x, self.network.y, self.network.z],
+            )
+        )
+
+    @meter()
+    def run_placement(self, strategies=None, fail_fast=True, pipelines=True):
+        """
+        Run placement strategies.
+        """
+        if pipelines:
+            self.run_pipelines()
+        if strategies is None:
+            strategies = [*self.placement.values()]
+        strategies = PlacementStrategy.sort_deps(strategies)
+        with self.create_job_pool(fail_fast=fail_fast) as pool:
+            if pool.is_main():
+
+                def scheduler(strategy):
+                    strategy.queue(pool, self.network.chunk_size)
+
+                pool.schedule(strategies, scheduler)
+            pool.execute()
+
+    @meter()
+    def run_connectivity(self, strategies=None, fail_fast=True, pipelines=True):
+        """
+        Run connection strategies.
+        """
+        if pipelines:
+            self.run_pipelines()
+        if strategies is None:
+            strategies = set(self.connectivity.values())
+        strategies = ConnectionStrategy.sort_deps(strategies)
+        with self.create_job_pool(fail_fast=fail_fast) as pool:
+            if pool.is_main():
+                pool.schedule(strategies)
+            pool.execute()
+
+    @meter()
+    def run_placement_strategy(self, strategy):
+        """
+        Run a single placement strategy.
+        """
+        self.run_placement([strategy])
+
+    @meter()
+    def run_after_placement(self, hooks=None, fail_fast=None, pipelines=True):
+        """
+        Run after placement hooks.
+        """
+        if hooks is None:
+            hooks = self.after_placement
+        with self.create_job_pool(fail_fast) as pool:
+            if pool.is_main():
+                pool.schedule(hooks)
+            pool.execute()
+
+    @meter()
+    def run_after_connectivity(self, hooks=None, fail_fast=None, pipelines=True):
+        """
+        Run after placement hooks.
+        """
+        if hooks is None:
+            hooks = self.after_placement
+        with self.create_job_pool(fail_fast) as pool:
+            if pool.is_main():
+                pool.schedule(hooks)
+            pool.execute()
+
+    @meter()
+    def compile(
+        self,
+        skip_placement=False,
+        skip_connectivity=False,
+        skip_after_placement=False,
+        skip_after_connectivity=False,
+        only=None,
+        skip=None,
+        clear=False,
+        append=False,
+        redo=False,
+        force=False,
+        fail_fast=True,
+    ):
+        """
+        Run reconstruction steps in the scaffold sequence to obtain a full network.
+        """
+        existed = self.storage.preexisted
+        if skip_placement:
+            p_strats = []
+        else:
+            p_strats = self.get_placement(skip=skip, only=only)
+        if skip_connectivity:
+            c_strats = []
+        else:
+            c_strats = self.get_connectivity(skip=skip, only=only)
+        todo_list_str = ", ".join(s.name for s in itertools.chain(p_strats, c_strats))
+        report(f"Compiling the following strategies: {todo_list_str}", level=2)
+        if _bad_flag(clear) or _bad_flag(redo) or _bad_flag(append):
+            raise InputError(
+                "`clear`, `redo` and `append` are strictly boolean flags. "
+                "Pass the strategies to run to the skip/only options instead."
+            )
+        if sum((bool(clear), bool(redo), bool(append))) > 1:
+            raise InputError("`clear`, `redo` and `append` are mutually exclusive.")
+        if existed:
+            if not (clear or append or redo):
+                raise FileExistsError(
+                    f"The `{self.storage.format}` storage"
+                    + f" at `{self.storage.root}` already exists. Either move/delete it,"
+                    + " or pass one of the `clear`, `append` or `redo` arguments"
+                    + " to pick what to do with the existing data."
+                )
+            if clear:
+                report("Clearing data", level=2)
+                # Clear the placement and connectivity data, but leave any cached files
+                # and morphologies intact.
+                self.clear_placement()
+                self.clear_connectivity()
+            elif redo:
+                # In order to properly redo things, we clear some placement and connection
+                # data, but since multiple placement/connection strategies can contribute
+                # to the same sets we might be wiping their data too, and they will need
+                # to be cleared and reran as well.
+                p_strats, c_strats = self._redo_chain(p_strats, c_strats, skip, force)
+            # else:
+            #   append mode is luckily simpler, just don't clear anything :)
+
+        phases = ["pipelines"]
+        if not skip_placement:
+            phases.append("placement")
+        if not skip_after_placement:
+            phases.append("after_placement")
+        if not skip_connectivity:
+            phases.append("connectivity")
+        if not skip_after_connectivity:
+            phases.append("after_connectivity")
+        self._workflow = Workflow(phases)
+        try:
+            self.run_pipelines(fail_fast=fail_fast)
+            self._workflow.next_phase()
+            if not skip_placement:
+                placement_todo = ", ".join(s.name for s in p_strats)
+                report(f"Starting placement strategies: {placement_todo}", level=2)
+                self.run_placement(p_strats, fail_fast=fail_fast, pipelines=False)
+                self._workflow.next_phase()
+            if not skip_after_placement:
+                self.run_after_placement(pipelines=False, fail_fast=fail_fast)
+                self._workflow.next_phase()
+            if not skip_connectivity:
+                connectivity_todo = ", ".join(s.name for s in c_strats)
+                report(f"Starting connectivity strategies: {connectivity_todo}", level=2)
+                self.run_connectivity(c_strats, fail_fast=fail_fast, pipelines=False)
+                self._workflow.next_phase()
+            if not skip_after_connectivity:
+                self.run_after_connectivity(pipelines=False)
+                self._workflow.next_phase()
+        finally:
+            # After compilation we should flag the storage as having existed before so that
+            # the `clear`, `redo` and `append` flags take effect on a second `compile` pass.
+            self.storage._preexisted = True
+            del self._workflow
+
+    @meter()
+    def run_pipelines(self, fail_fast=True, pipelines=None):
+        if pipelines is None:
+            pipelines = self.get_dependency_pipelines()
+        with self.create_job_pool(fail_fast=fail_fast) as pool:
+            if pool.is_main():
+                pool.schedule(pipelines)
+            pool.execute()
+
+    @meter()
+    def run_simulation(self, simulation_name: str):
+        """
+        Run a simulation starting from the default single-instance adapter.
+
+        :param simulation_name: Name of the simulation in the configuration.
+        :type simulation_name: str
+        """
+        simulation = self.get_simulation(simulation_name)
+        adapter = get_simulation_adapter(simulation.simulator)
+        return adapter.simulate(simulation)[0]
+
+    def get_simulation(self, sim_name: str) -> "Simulation":
+        """
+        Retrieve the default single-instance adapter for a simulation.
+        """
+        if sim_name not in self.simulations:
+            simstr = ", ".join(f"'{s}'" for s in self.simulations.keys())
+            raise NodeNotFoundError(
+                f"Unknown simulation '{sim_name}', choose from: {simstr}"
+            )
+        return self.configuration.simulations[sim_name]
+
+    def place_cells(
+        self,
+        cell_type,
+        positions,
+        morphologies=None,
+        rotations=None,
+        additional=None,
+        chunk=None,
+    ):
+        """
+        Place cells inside of the scaffold
+
+        .. code-block:: python
+
+            # Add one granule cell at position 0, 0, 0
+            cell_type = scaffold.get_cell_type("granule_cell")
+            scaffold.place_cells(cell_type, cell_type.layer_instance, [[0., 0., 0.]])
+
+        :param cell_type: The type of the cells to place.
+        :type cell_type: ~bsb.cell_types.CellType
+        :param positions: A collection of xyz positions to place the cells on.
+        :type positions: Any `np.concatenate` type of shape (N, 3).
+        """
+        if chunk is None:
+            chunk = Chunk([0, 0, 0], self.network.chunk_size)
+        if hasattr(chunk, "dimensions") and np.any(np.isnan(chunk.dimensions)):
+            chunk.dimensions = self.network.chunk_size
+        self.get_placement_set(cell_type).append_data(
+            chunk,
+            positions=positions,
+            morphologies=morphologies,
+            rotations=rotations,
+            additional=additional,
+        )
+
+    def create_entities(self, cell_type, count):
+        """
+        Create entities in the simulation space.
+
+        Entities are different from cells because they have no positional data and
+        don't influence the placement step. They do have a representation in the
+        connection and simulation step.
+
+        :param cell_type: The cell type of the entities
+        :type cell_type: ~bsb.cell_types.CellType
+        :param count: Number of entities to place
+        :type count: int
+        :todo: Allow `additional` data for entities
+        """
+        if count == 0:
+            return
+        ps = self.get_placement_set(cell_type)
+        # Append entity data to the default chunk 000
+        chunk = Chunk([0, 0, 0], self.network.chunk_size)
+        ps.append_entities(chunk, count)
+
+    def get_placement(
+        self, cell_types=None, skip=None, only=None
+    ) -> typing.List["PlacementStrategy"]:
+        if cell_types is not None:
+            cell_types = [
+                self.cell_types[ct] if isinstance(ct, str) else ct for ct in cell_types
+            ]
+        return [
+            val
+            for key, val in self.placement.items()
+            if (cell_types is None or any(ct in cell_types for ct in val.cell_types))
+            and (only is None or key in only)
+            and (skip is None or key not in skip)
+        ]
+
+    def get_placement_of(self, *cell_types):
+        """
+        Find all of the placement strategies that given certain cell types.
+
+        :param cell_types: Cell types (or their names) of interest.
+        :type cell_types: Union[~bsb.cell_types.CellType, str]
+        """
+        return self.get_placement(cell_types=cell_types)
+
+    def get_placement_set(
+        self, type, chunks=None, labels=None, morphology_labels=None
+    ) -> "PlacementSet":
+        """
+        Return a cell type's placement set from the output formatter.
+
+        :param tag: Unique identifier of the placement set in the storage
+        :type tag: str
+        :returns: A placement set
+        :param labels: Labels to filter the placement set by.
+        :type labels: list[str]
+        :param morphology_labels: Subcellular labels to apply to the morphologies.
+        :type morphology_labels: list[str]
+        :rtype: :class:`~.storage.interfaces.PlacementSet`
+        """
+        if isinstance(type, str):
+            type = self.cell_types[type]
+        return self.storage.get_placement_set(
+            type, chunks=chunks, labels=labels, morphology_labels=morphology_labels
+        )
+
+    def get_placement_sets(self) -> typing.List["PlacementSet"]:
+        """
+        Return all of the placement sets present in the network.
+
+        :rtype: List[~bsb.storage.interfaces.PlacementSet]
+        """
+        return [cell_type.get_placement_set() for cell_type in self.cell_types.values()]
+
+    def get_connectivity(
+        self, anywhere=None, presynaptic=None, postsynaptic=None, skip=None, only=None
+    ) -> typing.List["ConnectivitySet"]:
+        conntype_filtered = self._connectivity_query(
+            any_query=set(self._sanitize_ct(anywhere)),
+            pre_query=set(self._sanitize_ct(presynaptic)),
+            post_query=set(self._sanitize_ct(postsynaptic)),
+        )
+        return [
+            ct
+            for ct in conntype_filtered
+            if (only is None or ct.name in only) and (skip is None or ct.name not in skip)
+        ]
+
+    def get_connectivity_sets(self) -> typing.List["ConnectivitySet"]:
+        """
+        Return all connectivity sets from the output formatter.
+
+        :param tag: Unique identifier of the connectivity set in the output formatter
+        :type tag: str
+        :returns: All connectivity sets
+        """
+        return [self._load_cs_types(cs) for cs in self.storage.get_connectivity_sets()]
+
+    def require_connectivity_set(self, pre, post, tag=None) -> "ConnectivitySet":
+        return self._load_cs_types(
+            self.storage.require_connectivity_set(pre, post, tag), pre, post
+        )
+
+    def get_connectivity_set(self, tag=None, pre=None, post=None) -> "ConnectivitySet":
+        """
+        Return a connectivity set from the output formatter.
+
+        :param tag: Unique identifier of the connectivity set in the output formatter
+        :type tag: str
+        :returns: A connectivity set
+        :rtype: :class:`~.storage.interfaces.ConnectivitySet`
+        """
+        if tag is None:
+            try:
+                tag = f"{pre.name}_to_{post.name}"
+            except Exception:
+                raise ValueError("Supply either `tag` or a valid pre and post cell type.")
+        return self._load_cs_types(self.storage.get_connectivity_set(tag), pre, post)
+
+    def get_cell_types(self) -> typing.List["CellType"]:
+        """
+        Return a list of all cell types in the network.
+        """
+        return [*self.configuration.cell_types.values()]
+
+    def merge(self, other, label=None):
+        raise NotImplementedError("Revisit: merge CT, PS & CS, done?")
+
+    def _sanitize_ct(self, seq_str_or_none):
+        if seq_str_or_none is None:
+            return []
+        try:
+            if isinstance(seq_str_or_none, str):
+                return [self.cell_types[seq_str_or_none]]
+            return [
+                self.cell_types[s] if isinstance(s, str) else s for s in seq_str_or_none
+            ]
+        except KeyError as e:
+            raise NodeNotFoundError(f"Cell type `{e.args[0]}` not found.")
+
+    def _connectivity_query(self, any_query=set(), pre_query=set(), post_query=set()):
+        # Filter network connection types for any type that satisfies both
+        # the presynaptic and postsynaptic query. Empty queries satisfy all
+        # types. The presynaptic query is satisfied if the conn type contains
+        # any of the queried cell types presynaptically, and same for post.
+        # The any query is satisfied if a cell type is found either pre or post.
+
+        def partial_query(types, query):
+            return not query or any(cell_type in query for cell_type in types)
+
+        def query(conn_type):
+            pre_match = partial_query(conn_type.presynaptic.cell_types, pre_query)
+            post_match = partial_query(conn_type.postsynaptic.cell_types, post_query)
+            any_match = partial_query(
+                conn_type.presynaptic.cell_types, any_query
+            ) or partial_query(conn_type.postsynaptic.cell_types, any_query)
+            return any_match or (pre_match and post_match)
+
+        types = self.connectivity.values()
+        return [*filter(query, types)]
+
+    def _redo_chain(self, p_strats, c_strats, skip, force):
+        p_contrib = set(p_strats)
+        while True:
+            # Get all the placement strategies that effect the current set of CT.
+            full_wipe = set(itertools.chain(*(ps.cell_types for ps in p_contrib)))
+            contrib = set(self.get_placement(full_wipe))
+            # Keep repeating until no new contributors are fished up.
+            if contrib.issubset(p_contrib):
+                break
+            # Grow the placement chain
+            p_contrib.update(contrib)
+        report(
+            "Redo-affected placement: " + " ".join(ps.name for ps in p_contrib), level=2
+        )
+
+        c_contrib = set(c_strats)
+        conn_wipe = full_wipe.copy()
+        if full_wipe:
+            while True:
+                contrib = set(self.get_connectivity(anywhere=conn_wipe))
+                conn_wipe.update(
+                    itertools.chain(*(ct.get_cell_types() for ct in contrib))
+                )
+                if contrib.issubset(c_contrib):
+                    break
+                c_contrib.update(contrib)
+        report(
+            "Redo-affected connectivity: " + " ".join(cs.name for cs in c_contrib),
+            level=2,
+        )
+        # Don't do greedy things without `force`
+        if not force:
+            # Error if we need to redo things the user asked to skip
+            if skip is not None:
+                unskipped = [p.name for p in p_contrib if p.name in skip]
+                if unskipped:
+                    chainstr = ", ".join(f"'{s.name}'" for s in (p_strats + c_strats))
+                    skipstr = ", ".join(f"'{s.name}'" for s in unskipped)
+                    raise RedoError(
+                        f"Can't skip {skipstr}. Redoing {chainstr} requires to redo them."
+                        + f" Omit {skipstr} from `skip` or use `force` (not recommended)."
+                    )
+            # Error if we need to redo things the user didn't ask for
+            for label, chain, og in zip(
+                ("placement", "connection"), (p_contrib, c_contrib), (p_strats, c_strats)
+            ):
+                if len(chain) > len(og):
+                    new = chain.difference(og)
+                    raise RedoError(
+                        f"Need to redo additional {label} strategies: "
+                        + ", ".join(n.name for n in new)
+                        + ". Include them or use `force` (not recommended)."
+                    )
+
+        for ct in full_wipe:
+            report(f"Clearing all data of {ct.name}", level=2)
+            ct.clear()
+
+        for ct in conn_wipe:
+            report(f"Clearing connectivity data of {ct.name}", level=2)
+            ct.clear_connections()
+
+        return p_contrib, c_contrib
+
+    def get_dependency_pipelines(self):
+        return [*self.configuration.morphologies]
+
+    def get_config_diagram(self):
+        from .config import make_configuration_diagram
+
+        return make_configuration_diagram(self.configuration)
+
+    def get_storage_diagram(self):
+        dot = f'digraph "{self.configuration.name or "network"}" {{'
+        for ps in self.get_placement_sets():
+            dot += f'\n  {ps.tag}[label="{ps.tag} ({len(ps)} {ps.cell_type.name})"]'
+        for conn in self.get_connectivity_sets():
+            dot += f"\n  {conn.pre_type.name} -> {conn.post_type.name}"
+            dot += f'[label="{conn.tag} ({len(conn)})"];'
+
+        dot += "\n}\n"
+        return dot
+
+    def _load_cs_types(
+        self, cs: "ConnectivitySet", pre=None, post=None
+    ) -> "ConnectivitySet":
+        if pre and pre.name != cs.pre_type_name:
+            raise ValueError(
+                "Given and stored type mismatch:" + f" {pre.name} vs {cs.pre_type_name}"
+            )
+        if post and post.name != cs.post_type_name:
+            raise ValueError(
+                "Given and stored type mismatch:" + f" {post.name} vs {cs.post_type_name}"
+            )
+        try:
+            cs.pre_type = self.cell_types[cs.pre_type_name]
+            cs.post_type = self.cell_types[cs.post_type_name]
+        except KeyError as e:
+            raise NodeNotFoundError(
+                f"Couldn't load '{cs.tag}' connections, missing cell type '{e.args[0]}'."
+            ) from None
+        return cs
+
+    def create_job_pool(self, fail_fast=None, quiet=False):
+        pool = JobPool(
+            self, fail_fast=fail_fast, workflow=getattr(self, "_workflow", None)
+        )
+        try:
+            # Check whether stdout is a TTY, and that it is larger than 0x0
+            # (e.g. MPI sets it to 0x0 unless an xterm is emulated.
+            tty = os.isatty(sys.stdout.fileno()) and sum(os.get_terminal_size())
+        except Exception:
+            tty = False
+        if tty:
+            fps = 25
+            default_listener = TTYTerminalListener(fps)
+            default_max_wait = 1 / fps
+        else:
+            default_listener = NonTTYTerminalListener()
+            default_max_wait = None
+        if self._pool_listeners:
+            for listener, max_wait in self._pool_listeners:
+                pool.add_listener(listener, max_wait=max_wait)
+        elif not quiet:
+            pool.add_listener(default_listener, max_wait=default_max_wait)
+        return pool
+
+    def register_listener(self, listener, max_wait=None):
+        self._pool_listeners.append((listener, max_wait))
+
+    def remove_listener(self, listener):
+        for i, (l, _) in enumerate(self._pool_listeners):
+            if l is listener:
+                self._pool_listeners.pop(i)
+                break
+
+
+class ReportListener:
+    def __init__(self, scaffold, file):
+        self.file = file
+        self.scaffold = scaffold
+
+    def __call__(self, progress):
+        report(
+            str(progress.progression)
+            + "+"
+            + str(progress.duration)
+            + "+"
+            + str(progress.time),
+            token="simulation_progress",
+        )
+
+
+__all__ = ["ReportListener", "Scaffold", "from_storage"]
```

### Comparing `bsb_core-4.0.1/bsb/mixins.py` & `bsb_core-4.1.0/bsb/mixins.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-import abc as _abc
-import itertools
-import typing
-from graphlib import TopologicalSorter
-
-from . import _util as _gutil
-from .reporting import warn
-from .storage._chunks import Chunk
-
-if typing.TYPE_CHECKING:
-    from .services import JobPool
-
-
-def _queue_placement(self, pool: "JobPool", chunk_size):
-    # Get the queued jobs of all the strategies we depend on.
-    deps = set(
-        itertools.chain(*(pool.get_submissions_of(strat) for strat in self.get_deps()))
-    )
-    # todo: perhaps pass the volume or partition boundaries as chunk size
-    pool.queue_placement(self, Chunk([0, 0, 0], None), deps=deps)
-
-
-def _all_chunks(iter_):
-    return _gutil.unique(
-        _gutil.ichain(ct.get_placement_set().get_all_chunks() for ct in iter_)
-    )
-
-
-def _queue_connectivity(self, pool: "JobPool"):
-    # Get the queued jobs of all the strategies we depend on.
-    deps = set(_gutil.ichain(pool.get_submissions_of(strat) for strat in self.get_deps()))
-    # Schedule all chunks in 1 job
-    pre_chunks = _all_chunks(self.presynaptic.cell_types)
-    post_chunks = _all_chunks(self.postsynaptic.cell_types)
-    job = pool.queue_connectivity(self, pre_chunks, post_chunks, deps=deps)
-
-
-def _raise_na(*args, **kwargs):
-    raise NotImplementedError("NotParallel connection strategies have no RoI.")
-
-
-class HasDependencies:
-    """
-    Mixin class to mark that this node may depend on other nodes.
-    """
-
-    @_abc.abstractmethod
-    def get_deps(self):
-        pass
-
-    @_abc.abstractmethod
-    def __lt__(self, other):
-        raise NotImplementedError(f"{type(self).__name__} must implement __lt__.")
-
-    @_abc.abstractmethod
-    def __hash__(self):
-        raise NotImplementedError(f"{type(self).__name__} must implement __hash__.")
-
-    @classmethod
-    def sort_deps(cls, objects):
-        """
-        Orders a given dictionary of objects by the class's default mechanism and
-        then apply the `after` attribute for further restrictions.
-        """
-        objects = set(objects)
-        ordered = []
-        sorter = TopologicalSorter(
-            {o: set(d for d in o.get_deps() if d in objects) for o in objects}
-        )
-        sorter.prepare()
-        while sorter.is_active():
-            node_group = sorter.get_ready()
-            ordered.extend(sorted(node_group))
-            sorter.done(*node_group)
-        return ordered
-
-
-class NotParallel:
-    def __init_subclass__(cls, **kwargs):
-        from .connectivity import ConnectionStrategy
-        from .placement import PlacementStrategy
-
-        super().__init_subclass__(**kwargs)
-        if PlacementStrategy in cls.__mro__:
-            cls.queue = _queue_placement
-        elif ConnectionStrategy in cls.__mro__:
-            cls.queue = _queue_connectivity
-            if "get_region_of_interest" not in cls.__dict__:
-                cls.get_region_of_interest = _raise_na
-        else:
-            raise Exception(
-                "NotParallel can only be applied to placement or "
-                "connectivity strategies"
-            )
-
-
-class InvertedRoI:
-    """
-    This mixin inverts the perspective of the ``get_region_of_interest`` interface and
-    lets you find presynaptic regions of interest for a postsynaptic chunk.
-
-    Usage:
-
-    ..code-block:: python
-
-        class MyConnStrat(InvertedRoI, ConnectionStrategy):
-          def get_region_of_interest(post_chunk):
-            return [pre_chunk1, pre_chunk2]
-    """
-
-    def queue(self, pool):
-        # Get the queued jobs of all the strategies we depend on.
-        deps = set(
-            _gutil.ichain(pool.get_submissions_of(strat) for strat in self.get_deps())
-        )
-        post_types = self.postsynaptic.cell_types
-        # Iterate over each chunk that is populated by our postsynaptic cell types.
-        to_chunks = set(
-            _gutil.ichain(ct.get_placement_set().get_all_chunks() for ct in post_types)
-        )
-        rois = {
-            chunk: roi
-            for chunk in to_chunks
-            if (roi := self.get_region_of_interest(chunk)) is None or len(roi)
-        }
-        if not rois:
-            warn(
-                f"No overlap found between {[post.name for post in post_types]} and "
-                f"{[pre.name for pre in self.presynaptic.cell_types]} "
-                f"in '{self.name}'."
-            )
-        for chunk, roi in rois.items():
-            pool.queue_connectivity(self, roi, [chunk], deps=deps)
-
-
-__all__ = ["HasDependencies", "InvertedRoI", "NotParallel"]
+import abc as _abc
+import itertools
+import typing
+from graphlib import TopologicalSorter
+
+from . import _util as _gutil
+from .reporting import warn
+from .storage._chunks import Chunk
+
+if typing.TYPE_CHECKING:
+    from .services import JobPool
+
+
+def _queue_placement(self, pool: "JobPool", chunk_size):
+    # Get the queued jobs of all the strategies we depend on.
+    deps = set(
+        itertools.chain(*(pool.get_submissions_of(strat) for strat in self.get_deps()))
+    )
+    # todo: perhaps pass the volume or partition boundaries as chunk size
+    pool.queue_placement(self, Chunk([0, 0, 0], None), deps=deps)
+
+
+def _all_chunks(iter_):
+    return _gutil.unique(
+        _gutil.ichain(ct.get_placement_set().get_all_chunks() for ct in iter_)
+    )
+
+
+def _queue_connectivity(self, pool: "JobPool"):
+    # Get the queued jobs of all the strategies we depend on.
+    deps = set(_gutil.ichain(pool.get_submissions_of(strat) for strat in self.get_deps()))
+    # Schedule all chunks in 1 job
+    pre_chunks = _all_chunks(self.presynaptic.cell_types)
+    post_chunks = _all_chunks(self.postsynaptic.cell_types)
+    job = pool.queue_connectivity(self, pre_chunks, post_chunks, deps=deps)
+
+
+def _raise_na(*args, **kwargs):
+    raise NotImplementedError("NotParallel connection strategies have no RoI.")
+
+
+class HasDependencies:
+    """
+    Mixin class to mark that this node may depend on other nodes.
+    """
+
+    @_abc.abstractmethod
+    def get_deps(self):
+        pass
+
+    @_abc.abstractmethod
+    def __lt__(self, other):
+        raise NotImplementedError(f"{type(self).__name__} must implement __lt__.")
+
+    @_abc.abstractmethod
+    def __hash__(self):
+        raise NotImplementedError(f"{type(self).__name__} must implement __hash__.")
+
+    @classmethod
+    def sort_deps(cls, objects):
+        """
+        Orders a given dictionary of objects by the class's default mechanism and
+        then apply the `after` attribute for further restrictions.
+        """
+        objects = set(objects)
+        ordered = []
+        sorter = TopologicalSorter(
+            {o: set(d for d in o.get_deps() if d in objects) for o in objects}
+        )
+        sorter.prepare()
+        while sorter.is_active():
+            node_group = sorter.get_ready()
+            ordered.extend(sorted(node_group))
+            sorter.done(*node_group)
+        return ordered
+
+
+class NotParallel:
+    def __init_subclass__(cls, **kwargs):
+        from .connectivity import ConnectionStrategy
+        from .placement import PlacementStrategy
+
+        super().__init_subclass__(**kwargs)
+        if PlacementStrategy in cls.__mro__:
+            cls.queue = _queue_placement
+        elif ConnectionStrategy in cls.__mro__:
+            cls.queue = _queue_connectivity
+            if "get_region_of_interest" not in cls.__dict__:
+                cls.get_region_of_interest = _raise_na
+        else:
+            raise Exception(
+                "NotParallel can only be applied to placement or "
+                "connectivity strategies"
+            )
+
+
+class InvertedRoI:
+    """
+    This mixin inverts the perspective of the ``get_region_of_interest`` interface and
+    lets you find presynaptic regions of interest for a postsynaptic chunk.
+
+    Usage:
+
+    ..code-block:: python
+
+        class MyConnStrat(InvertedRoI, ConnectionStrategy):
+          def get_region_of_interest(post_chunk):
+            return [pre_chunk1, pre_chunk2]
+    """
+
+    def queue(self, pool):
+        # Get the queued jobs of all the strategies we depend on.
+        deps = set(
+            _gutil.ichain(pool.get_submissions_of(strat) for strat in self.get_deps())
+        )
+        post_types = self.postsynaptic.cell_types
+        # Iterate over each chunk that is populated by our postsynaptic cell types.
+        to_chunks = set(
+            _gutil.ichain(ct.get_placement_set().get_all_chunks() for ct in post_types)
+        )
+        rois = {
+            chunk: roi
+            for chunk in to_chunks
+            if (roi := self.get_region_of_interest(chunk)) is None or len(roi)
+        }
+        if not rois:
+            warn(
+                f"No overlap found between {[post.name for post in post_types]} and "
+                f"{[pre.name for pre in self.presynaptic.cell_types]} "
+                f"in '{self.name}'."
+            )
+        for chunk, roi in rois.items():
+            pool.queue_connectivity(self, roi, [chunk], deps=deps)
+
+
+__all__ = ["HasDependencies", "InvertedRoI", "NotParallel"]
```

### Comparing `bsb_core-4.0.1/bsb/morphologies/__init__.py` & `bsb_core-4.1.0/bsb/morphologies/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1714 +1,1727 @@
-"""
-Morphology module
-"""
-
-# This is a note to myself, should expand into docs:
-#
-# It goes ``morphology-on-file`` into ``repository`` that the ``storage`` needs to provide
-# support for. Then after a placement job has placed cells for a chunk, the positions are
-# sent to a ``distributor`` that is supposed to use the ``indicators`` to ask the
-# ``storage.morphology_repository`` which ``loaders`` are appropriate for the given
-# ``selectors``, then, still hopefully using just morpho metadata the  ``distributor``
-# generates indices and rotations. In more complex cases the ``selector`` and
-# ``distributor`` can both load the morphologies but this will slow things down.
-#
-# In the simulation step, these (possibly dynamically modified) morphologies are passed
-# to the cell model instantiators.
-
-import functools
-import inspect
-import itertools
-from collections import deque
-from pathlib import Path
-from pickle import UnpicklingError
-
-import numpy as np
-from scipy.spatial.transform import Rotation
-
-from .. import _util as _gutil
-from .._encoding import EncodedLabels
-from ..exceptions import EmptyBranchError, MorphologyDataError, MorphologyError
-from ..voxels import VoxelSet
-
-
-def parse_morphology_file(file, **kwargs):
-    from .parsers import parse_morphology_file
-
-    return parse_morphology_file(file, **kwargs)
-
-
-class MorphologySet:
-    """
-    Associates a set of :class:`StoredMorphologies
-    <.storage.interfaces.StoredMorphology>` to cells
-    """
-
-    def __init__(self, loaders, m_indices=None, /, labels=None):
-        """
-        :param loaders: list of Morphology loader functions.
-        :type loaders: List[Callable[[], bsb.storage.interfaces.StoredMorphology]]
-        :param m_indices: indices of the loaders for each of the morphologies.
-        :type: List[int]
-        """
-        if m_indices is None:
-            loaders, m_indices = np.unique(loaders, return_inverse=True)
-        self._m_indices = np.array(m_indices, copy=False, dtype=int)
-        self._loaders = list(loaders)
-        check_max = np.max(m_indices, initial=-1)
-        if check_max >= len(loaders):
-            raise IndexError(f"Index {check_max} out of range for {len(loaders)}.")
-        self._cached = {}
-        self._labels = labels
-
-    def set_label_filter(self, labels):
-        self._cached = {}
-        for loader in self._loaders:
-            loader._cached_load.cache_clear()
-        self._labels = labels
-
-    @_gutil.obj_str_insert
-    def __repr__(self):
-        return f"{len(self)} cells, {len(self._loaders)} morphologies"
-
-    def __contains__(self, value):
-        return value in [loader.name for loader in self._loaders]
-
-    def count_morphologies(self):
-        return len(self._loaders)
-
-    def count_unique(self):
-        uniques = []
-        count = 0
-        for m in (m.load() for m in self._loaders):
-            if not any(f == m for f in uniques):
-                uniques.append(m)
-                count += 1
-        return count
-
-    def __len__(self):
-        return len(self._m_indices)
-
-    def __iter__(self):
-        return self.iter_morphologies()
-
-    @property
-    def names(self):
-        return [loader.name for loader in self._loaders]
-
-    def get_indices(self, copy=True):
-        return self._m_indices.copy() if copy else self._m_indices
-
-    def get(self, index, cache=True, hard_cache=False):
-        data = self._m_indices[index]
-        if data.ndim:
-            return self._get_many(data, cache, hard_cache)
-        else:
-            return self._get_one(data, cache, hard_cache)
-
-    def _get_one(self, idx, cache, hard_cache):
-        if cache:
-            if hard_cache:
-                return self._loaders[idx].cached_load(self._labels)
-
-            if idx not in self._cached:
-                self._cached[idx] = (
-                    self._loaders[idx].load().set_label_filter(self._labels).as_filtered()
-                )
-            return self._cached[idx].copy()
-        else:
-            return self._loaders[idx].load().set_label_filter(self._labels).as_filtered()
-
-    def _get_many(self, data, cache, hard_cache):
-        if hard_cache:
-            return np.array([self._loaders[idx].cached_load() for idx in data])
-        elif cache:
-            res = []
-            for idx in data:
-                if idx not in self._cached:
-                    self._cached[idx] = self._loaders[idx].load()
-                res.append(self._cached[idx].copy())
-        else:
-            res = np.array([self._loaders[idx].load() for idx in data])
-        return res
-
-    def clear_soft_cache(self):
-        self._cached = {}
-
-    def iter_morphologies(self, cache=True, unique=False, hard_cache=False):
-        """
-        Iterate over the morphologies in a MorphologySet with full control over caching.
-
-        :param cache: Use :ref:`soft-caching` (1 copy stored in mem per cache miss, 1 copy
-          created from that per cache hit).
-        :type cache: bool
-        :param hard_cache: Use :ref:`hard-caching` (1 copy stored on the loader, always
-          same copy returned from that loader forever).
-        """
-        if hard_cache:
-
-            def _load(loader):
-                return loader.cached_load(self._labels)
-
-        elif self._labels is not None:
-
-            def _load(loader):
-                return loader.load().set_label_filter(self._labels).as_filtered()
-
-        else:
-
-            def _load(loader):
-                return loader.load()
-
-        if unique:
-            yield from map(_load, self._loaders)
-        elif not cache or hard_cache:
-            yield from map(_load, (self._loaders[idx] for idx in self._m_indices))
-        else:
-            _cached = {}
-            for idx in self._m_indices:
-                if idx not in _cached:
-                    _cached[idx] = _load(self._loaders[idx])
-                yield _cached[idx].copy()
-
-    def iter_meta(self, unique=False):
-        if unique:
-            yield from (loader.get_meta() for loader in self._loaders)
-        else:
-            yield from (self._loaders[idx].get_meta() for idx in self._m_indices)
-
-    def _serialize_loaders(self):
-        return [loader.name for loader in self._loaders]
-
-    @classmethod
-    def empty(cls):
-        return cls([], np.empty(0, dtype=int))
-
-    def merge(self, other):
-        merged_loaders = self._loaders.copy()
-        previous_set = set(merged_loaders)
-        if any(loader in previous_set for loader in other._loaders):
-            # There is overlap between the sets, and mapping is required
-            id_map = dict(
-                (i, merged_loaders.index(loader))
-                for i, loader in enumerate(other._loaders)
-                if loader in previous_set
-            )
-            if all(k == v for k, v in id_map.items()):
-                mapped_indices = other._m_indices
-            else:
-
-                def map_ids(id):
-                    mapped_id = id_map.get(id, None)
-                    if mapped_id is None:
-                        mapped_id = id_map[id] = len(merged_loaders)
-                        merged_loaders.append(other._loaders[id])
-                    return mapped_id
-
-                mapped_indices = np.vectorize(map_ids)(other._m_indices)
-            merged_indices = np.concatenate((self._m_indices, mapped_indices))
-        else:
-            # No overlap, we can just offset the new dataset
-            merge_offset = len(self._loaders)
-            merged_loaders = self._loaders + other._loaders
-            merged_indices = np.concatenate(
-                (self._m_indices, other._m_indices + merge_offset)
-            )
-        return MorphologySet(merged_loaders, merged_indices)
-
-    def _mapback(self, locs):
-        if self._labels is None:
-            raise RuntimeError("Mapback requested on unfiltered morphology set.")
-        locs = locs.copy()
-        for i, loader in enumerate(self._loaders):
-            rows = self._m_indices[locs[:, 0]] == i
-            if np.any(rows):
-                morpho = loader.load()
-                filtered = morpho.set_label_filter(self._labels).as_filtered()
-                # Using np.vectorize is Python speed O(n), worst case in numpy is C speed
-                # O(n^2) (that is if every point is on another branch), not sure.
-                branchmap = np.vectorize(
-                    {
-                        bid: b._copied_from_branch
-                        for bid, b in enumerate(filtered.branches)
-                    }.get
-                )
-                pointmap = np.vectorize(
-                    {
-                        bid: b._copied_points_offset
-                        for bid, b in enumerate(filtered.branches)
-                    }.get
-                )
-                # Map points first, then branches, since points depend on unmapped branch.
-                locs[rows, 2] = locs[rows, 2] + pointmap(locs[rows, 1])
-                locs[rows, 1] = branchmap(locs[rows, 1])
-        return locs
-
-
-class RotationSet:
-    """
-    Set of rotations. Returned rotations are of :class:`scipy.spatial.transform.Rotation`
-    """
-
-    def __init__(self, data):
-        if not isinstance(data, np.ndarray):
-            self._data = np.array(
-                [
-                    v.as_euler("xyz", degrees=True) if isinstance(v, Rotation) else v
-                    for v in data
-                ]
-            )
-        else:
-            self._data = data
-        if self._data.ndim != 2 or self._data.shape[1] != 3:
-            raise ValueError("Input should be an (Nx3) matrix of rotations.")
-
-    def __array__(self, dtype=None, *args, **kwargs):
-        return self._data.__array__(dtype, *args, **kwargs)
-
-    def __iter__(self):
-        return self.iter()
-
-    def __getitem__(self, index):
-        data = self._data[index]
-        if data.ndim == 2:
-            return np.array([self._rot(d) for d in data])
-        else:
-            return self._rot(data)
-
-    def __len__(self):
-        return len(self._data)
-
-    def iter(self, cache=False):
-        if cache:
-            yield from (self._cached_rot(tuple(d)) for d in self._data)
-        else:
-            yield from (self._rot(d) for d in self._data)
-
-    @functools.cache
-    def _cached_rot(self, angles):
-        return self._rot(angles)
-
-    def _rot(self, angles):
-        return Rotation.from_euler("xyz", angles)
-
-
-def branch_iter(branch):
-    """
-    Iterate over a branch and all of its children depth first.
-    """
-    yield branch
-    for child in branch._children:
-        yield from branch_iter(child)
-
-
-class SubTree:
-    """
-    Collection of branches, not necesarily all connected.
-    """
-
-    def __init__(self, branches, sanitize=True):
-        if sanitize:
-            # Find the roots of the full subtree(s) emanating from the given, possibly
-            # overlapping branches.
-            if len(branches) < 2:
-                # The roots of the subtrees of 0 or 1 branches is eaqual to the 0 or 1
-                # branches themselves.
-                self.roots = branches
-            else:
-                # Collect the deduplicated subtree emanating from all given branches; use
-                # dict.fromkeys and .keys to preserve insertion order (i.e. DFS order)
-                sub = dict.fromkeys(
-                    itertools.chain(*(b.get_branches() for b in branches))
-                ).keys()
-                # Find the root branches whose parents are not part of the subtrees
-                self.roots = [b for b in sub if b.parent not in sub]
-        else:
-            # No subtree sanitizing: Assume the whole tree is given, or only the roots
-            # have been given, and just take all literal root (non-parent-having)
-            # branches.
-            self.roots = [b for b in branches if b.parent is None]
-        self._is_shared = False
-
-    def __getattr__(self, attr):
-        if not hasattr(self, "_is_shared"):
-            raise UnpicklingError("Morphology class does not support pickling.")
-        if self._is_shared:
-            if attr in self._shared._prop:
-                return self._shared._prop[attr]
-            else:
-                super().__getattribute__(attr)
-        else:
-            return np.concatenate([getattr(b, attr) for b in self.branches])
-
-    def __len__(self):
-        if self._is_shared:
-            return len(self._shared._points)
-        else:
-            return sum(b.size for b in self.get_branches())
-
-    @property
-    def size(self):
-        return len(self)
-
-    @property
-    def branches(self):
-        """
-        Return a depth-first flattened array of all branches.
-        """
-        return self.get_branches()
-
-    @property
-    def points(self):
-        return self.flatten()
-
-    @points.setter
-    def points(self, value):
-        arr = np.array(value, copy=False, dtype=float)
-        if self._is_shared:
-            self.points[:] = arr
-        else:
-            ptr = 0
-            for b in self.branches:
-                b.points = arr[ptr : (ptr := ptr + len(b))]
-
-    @property
-    def radii(self):
-        return self.flatten_radii()
-
-    @radii.setter
-    def radii(self, value):
-        arr = np.array(value, copy=False, dtype=float)
-        if self._is_shared:
-            self.radii[:] = arr
-        else:
-            ptr = 0
-            for b in self.branches:
-                b.radii = arr[ptr : (ptr := ptr + len(b))]
-
-    @property
-    def labels(self):
-        return self.flatten_labels()
-
-    @property
-    def properties(self):
-        return self.flatten_properties()
-
-    @property
-    def bounds(self):
-        f = self.flatten()
-        if not len(f):
-            return np.zeros(3), np.zeros(3)
-        return np.min(f, axis=0), np.max(f, axis=0)
-
-    @property
-    def branch_adjacency(self):
-        """
-        Return a dictonary containing mapping the id of the branch to its children.
-        """
-        idmap = {b: n for n, b in enumerate(self.branches)}
-        return {n: list(map(idmap.get, b.children)) for n, b in enumerate(self.branches)}
-
-    @property
-    def path_length(self):
-        """
-        Return the total path length as the sum of the euclidian distances between
-        consecutive points.
-        """
-        return sum(b.path_length for b in self.branches)
-
-    def subtree(self, labels=None):
-        return SubTree(self.get_branches(labels))
-
-    def get_branches(self, labels=None):
-        """
-        Return a depth-first flattened array of all or the selected branches.
-
-        :param labels: Names of the labels to select.
-        :type labels: list
-        :returns: List of all branches, or the ones fully labelled with any of
-          the given labels.
-        :rtype: list
-        """
-        root_iter = (branch_iter(root) for root in self.roots)
-        all_branch = itertools.chain(*root_iter)
-        if labels is None:
-            return [*all_branch]
-        else:
-            return [b for b in all_branch if b.contains_labels(labels)]
-
-    def flatten(self):
-        """
-        Return the flattened points of the morphology or subtree.
-
-        :rtype: numpy.ndarray
-        """
-        if self._is_shared:
-            return self._shared._points
-        branches = self.get_branches()
-        if not branches:
-            return np.empty((0, 3))
-        return np.vstack(tuple(b.points for b in branches))
-
-    def flatten_radii(self):
-        """
-        Return the flattened radii of the morphology or subtree.
-
-        :rtype: numpy.ndarray
-        """
-        if self._is_shared:
-            return self._shared._radii
-        branches = self.get_branches()
-        if not branches:
-            return np.empty(0)
-        return np.concatenate(tuple(b.radii for b in branches))
-
-    def flatten_labels(self):
-        """
-        Return the flattened labels of the morphology or subtree.
-
-        :rtype: numpy.ndarray
-        """
-        if self._is_shared:
-            return self._shared._labels
-        else:
-            return EncodedLabels.concatenate(*(b._labels for b in self.get_branches()))
-
-    def flatten_properties(self):
-        """
-        Return the flattened properties of the morphology or subtree.
-
-        :rtype: numpy.ndarray
-        """
-        if self._is_shared:
-            if not self._shared._prop:
-                return {}
-            return self._shared._prop.copy()
-        else:
-            branches = self.get_branches()
-            len_ = sum(len(b) for b in branches)
-            all_props = [*set(_gutil.ichain(b._properties.keys() for b in branches))]
-            props = {k: np.empty(len_) for k in all_props}
-            ptr = 0
-            for branch in self.branches:
-                nptr = ptr + len(branch)
-                for k, v in props.items():
-                    prop = branch._properties.get(k, None)
-                    if prop is None:
-                        prop = np.full(len(branch), np.nan)
-                    v[ptr:nptr] = prop
-                ptr = nptr
-            return props
-
-    def label(self, labels, points=None):
-        """
-        Add labels to the morphology or subtree.
-
-        :param labels: Labels to add to the subtree.
-        :type labels: list[str]
-        :param points: Optional boolean or integer mask for the points to be labelled.
-        :type points: numpy.ndarray
-        """
-        if points is None:
-            points = np.ones(len(self), dtype=bool)
-        points = np.array(points, copy=False)
-        if self._is_shared:
-            self.labels.label(labels, points)
-        else:
-            if len(points) == len(self) and points.dtype == bool:
-                ctr = 0
-                for b in self.branches:
-                    b.label(labels, points[ctr : ctr + len(b)])
-                    ctr += len(b)
-            elif np.can_cast(points.dtype, int):
-                points = np.array(points, copy=False, dtype=int)
-                for b in self.branches:
-                    mux = points < len(b)
-                    b.label(labels, points[mux])
-                    points = points[~mux]
-            else:
-                raise ValueError(
-                    "Label indices must be a boolean or integer mask."
-                    f" {points.dtype} given."
-                )
-
-        return self
-
-    def rotate(self, rotation, center=None):
-        """
-        Point rotation
-
-        :param rot: Scipy rotation
-        :type: Union[scipy.spatial.transform.Rotation, List[float,float,float]]
-        :param center: rotation offset point.
-        :type center: numpy.ndarray
-        """
-        if not isinstance(rotation, Rotation):
-            rotation = Rotation.from_euler("xyz", rotation, degrees=True)
-        if self._is_shared:
-            self._shared._points[:] = self._rotate(self._shared._points, rotation, center)
-        else:
-            for b in self.branches:
-                b.points[:] = self._rotate(b.points, rotation, center)
-        return self
-
-    def _rotate(self, points, rot, center):
-        if center is not None:
-            points = points - center
-            rotated_points = rot.apply(points)
-            rotated_points = rotated_points + center
-        else:
-            rotated_points = rot.apply(points)
-        return rotated_points
-
-    def root_rotate(self, rot, downstream_of=0):
-        """
-        Rotate the subtree emanating from each root around the start of that root
-        If downstream_of is provided, will rotate points starting from the index provided (only for
-        subtrees with a single root).
-
-        :param rot: Scipy rotation to apply to the subtree.
-        :type rot: scipy.spatial.transform.Rotation
-        :param downstream_of: index of the point in the subtree from which the rotation should be
-            applied. This feature works only when the subtree has only one root branch.
-        :returns: rotated Morphology
-        :rtype: bsb.morphologies.SubTree
-        """
-
-        if downstream_of != 0:
-            if len(self.roots) > 1:
-                raise ValueError(
-                    "Can't rotate with subbranch precision with multiple roots"
-                )
-            elif type(downstream_of) == int and 0 < downstream_of < len(
-                self.roots[0].points
-            ):
-                b = self.roots[0]
-                group = SubTree([b])
-                upstream = np.copy(b.points[:downstream_of])
-                group.rotate(rot, b.points[downstream_of])
-                b.points[:downstream_of] = upstream
-        else:
-            for b in self.roots:
-                group = SubTree([b])
-                group.rotate(rot, group.origin)
-        return self
-
-    def translate(self, point):
-        """
-        Translate the subtree by a 3D vector.
-
-        :param numpy.ndarray point: 3D vector to translate the subtree.
-        :returns: the translated subtree
-        :rtype: bsb.morphologies.SubTree
-        """
-        if len(point) != 3:
-            raise ValueError("Point must be a sequence of x, y and z coordinates")
-        if self._is_shared:
-            self._shared._points[:] = self._shared._points + point
-        else:
-            for branch in self.branches:
-                branch.points[:] = branch.points[:] + point
-        return self
-
-    @property
-    def origin(self):
-        return np.mean([r.points[0] for r in self.roots], axis=0)
-
-    def center(self):
-        """
-        Center the morphology on the origin
-        """
-        self.translate(-self.origin)
-        return self
-
-    def close_gaps(self):
-        """
-        Close any head-to-tail gaps between parent and child branches.
-        """
-        for branch in self.branches:
-            if branch.parent is not None:
-                gap_offset = branch.parent.points[-1] - branch.points[0]
-                if not np.allclose(gap_offset, 0):
-                    SubTree([branch]).translate(gap_offset)
-        return self
-
-    def collapse(self, on=None):
-        """
-        Collapse all the roots of the morphology or subtree onto a single point.
-
-        :param on: Index of the root to collapse on. Collapses onto the origin by default.
-        :type on: int
-        """
-        if on is None:
-            on = self.origin
-        for root in self.roots:
-            root.translate(on - root.points[0])
-        return self
-
-    def simplify_branches(self, epsilon):
-        """
-        Apply RamerDouglasPeucker algorithm to all points of all branches of the SubTree.
-        :param epsilon: Epsilon to be used in the algorithm.
-        """
-        for branch in self.branches:
-            branch.simplify(epsilon)
-
-    def voxelize(self, N):
-        """
-        Turn the morphology or subtree into an approximating set of axis-aligned cuboids.
-
-        :rtype: bsb.voxels.VoxelSet
-        """
-        return VoxelSet.from_morphology(self, N)
-
-    @functools.cache
-    def cached_voxelize(self, N):
-        """
-        Turn the morphology or subtree into an approximating set of axis-aligned cuboids
-        and cache the result.
-
-        :rtype: bsb.voxels.VoxelSet
-        """
-        return self.voxelize(N)
-
-
-class _SharedBuffers:
-    def __init__(self, points, radii, labels, properties):
-        self._points = points
-        self._radii = radii
-        self._labels = labels if labels is not None else EncodedLabels.none(len(radii))
-        self._prop = properties
-
-    def copy(self):
-        copied_props = {k: v.copy() for k, v in self._prop.items()}
-        return self.__class__(
-            self._points.copy(), self._radii.copy(), self._labels.copy(), copied_props
-        )
-
-    def points_shared(self, branches):
-        return all(b.points.base is self._points for b in branches)
-
-    def radii_shared(self, branches):
-        return all(b.radii.base is self._radii for b in branches)
-
-    def labels_shared(self, branches):
-        return all(b.labels.base is self._labels for b in branches)
-
-    def properties_shared(self, branches):
-        return all(
-            (
-                b._properties.keys() == self._prop.keys() and all(c.base is self._prop[c])
-                for a, c in b._properties.items()
-            )
-            for b in branches
-        )
-
-    def all_buffers_shared(self, branches):
-        return (
-            self.points_shared(branches)
-            and self.radii_shared(branches)
-            and self.labels_shared(branches)
-            and self.properties_shared(branches)
-        )
-
-    def get_shared(self, start, end):
-        copied_props = {k: v[start:end] for k, v in self._prop.items()}
-        return (
-            self._points[start:end],
-            self._radii[start:end],
-            self._labels[start:end],
-            copied_props,
-        )
-
-
-class Morphology(SubTree):
-    """
-    A multicompartmental spatial representation of a cell based on a directed acyclic
-    graph of branches whom consist of data vectors, each element of a vector being a
-    coordinate or other associated data of a point on the branch.
-    """
-
-    def __init__(self, roots, meta=None, shared_buffers=None, sanitize=False):
-        super().__init__(roots, sanitize=sanitize)
-        self._meta = meta if meta is not None else {}
-        self._filter = None
-        if shared_buffers is None:
-            self._shared = None
-            self._is_shared = False
-        else:
-            if isinstance(shared_buffers, _SharedBuffers):
-                self._shared = shared_buffers
-            else:
-                self._shared = _SharedBuffers(*shared_buffers)
-            self._is_shared = self._check_shared()
-            for branch in self.branches:
-                branch._on_mutate = self._mutnotif
-
-    @_gutil.obj_str_insert
-    def __repr__(self):
-        return (
-            f"{len(self.roots)} roots, {len(self)} points,"
-            f" from {self.bounds[0]} to {self.bounds[1]}"
-        )
-
-    def __eq__(self, other):
-        return len(self.branches) == len(other.branches) and all(
-            b1.is_terminal == b2.is_terminal and (not b1.is_terminal or b1 == b2)
-            for b1, b2 in zip(self.branches, other.branches)
-        )
-
-    def __lt__(self, other):
-        # Sorting compares using lt, so we use id for useless but stable comparison.
-        return id(self) < id(other)
-
-    def __hash__(self):
-        return id(self)
-
-    def _check_shared(self):
-        if self._shared is None:
-            return False
-        return self._shared.all_buffers_shared(self.branches)
-
-    def _mutnotif(self):
-        self._is_shared = False
-
-    @property
-    def is_optimized(self):
-        return self._shared
-
-    def optimize(self, force=False):
-        if force or not self._is_shared:
-            branches = self.branches
-            len_ = sum(len(b) for b in branches)
-            points = np.empty((len_, 3))
-            radii = np.empty(len_)
-            all_props = [*set(_gutil.ichain(b._properties.keys() for b in branches))]
-            types = [
-                next(_p[k].dtype for b in branches if k in (_p := b._properties))
-                for k in all_props
-            ]
-            props = {k: np.empty(len_, dtype=t) for k, t in zip(all_props, types)}
-            labels = EncodedLabels.concatenate(*(b._labels for b in branches))
-            ptr = 0
-            for branch in self.branches:
-                nptr = ptr + len(branch)
-                points[ptr:nptr] = branch.points
-                branch._points = points[ptr:nptr]
-                radii[ptr:nptr] = branch.radii
-                branch._radii = radii[ptr:nptr]
-                for k, v in props.items():
-                    prop = branch._properties.get(k, None)
-                    if prop is None:
-                        prop = np.full(len(branch), np.nan)
-                    v[ptr:nptr] = prop
-                    branch._properties[k] = v[ptr:nptr]
-                branch._labels = labels[ptr:nptr]
-                ptr = nptr
-            self._shared = _SharedBuffers(points, radii, labels, props)
-            self._is_shared = True
-            assert self._check_shared(), "optimize should result in shared buffers"
-
-    @property
-    def meta(self):
-        return self._meta
-
-    @meta.setter
-    def meta(self, value):
-        self._meta = value
-
-    @property
-    def adjacency_dictionary(self):
-        """
-        Return a dictonary associating to each key (branch index) a list of adjacent branch indices
-        """
-        branches = self.branches
-        idmap = {b: n for n, b in enumerate(branches)}
-        return {n: list(map(idmap.get, b.children)) for n, b in enumerate(branches)}
-
-    @property
-    def labelsets(self):
-        """
-        Return the sets of labels associated to each numerical label.
-        """
-        self.optimize()
-        return self._shared._labels.labels
-
-    def list_labels(self):
-        """
-        Return a list of labels present on the morphology.
-        """
-        self.optimize()
-        return sorted(set(_gutil.ichain(self._shared._labels.labels.values())))
-
-    def set_label_filter(self, labels):
-        """
-        Set a label filter, so that `as_filtered` returns copies filtered by these labels.
-        """
-        self._filter = labels
-        return self
-
-    def get_label_mask(self, labels):
-        """
-        Get a mask corresponding to all the points labelled with 1 or more of the given
-        labels
-        """
-        self.optimize()
-        return self.labels.get_mask(labels)
-
-    @classmethod
-    def empty(cls):
-        return cls([])
-
-    def copy(self):
-        """
-        Copy the morphology.
-        """
-        # Make sure to optimize so that we use 1 shared buffer.
-        self.optimize(force=False)
-        # Copy that buffer into a new one
-        buffers = self._shared.copy()
-        roots = []
-        branch_copy_map = {}
-        ptr = 0
-        # For each branch, create a copy, and assign a piece of the copied buffer to it.
-        # Also attach it to its intended parent. Since we iterate DFS, each parent occurs
-        # before their children, so we can always find it in `branch_copy_map` from a
-        # previous iteration.
-        for branch in self.branches:
-            nptr = ptr + len(branch)
-            nbranch = Branch(*buffers.get_shared(ptr, nptr))
-            branch_copy_map[branch] = nbranch
-            if not branch.is_root:
-                branch_copy_map[branch.parent].attach_child(nbranch)
-            else:
-                roots.append(nbranch)
-            ptr = nptr
-        # Construct the morphology
-        return self.__class__(roots, shared_buffers=buffers, meta=self.meta.copy())
-
-    def as_filtered(self, labels=None):
-        """
-        Return a filtered copy of the morphology that includes only points that match the
-        current label filter, or the specified labels.
-        """
-        filter = labels if labels is not None else self._filter
-        if filter is None:
-            return self.copy()
-        self.optimize(force=False)
-        buffers = self._shared.copy()
-        roots = []
-        branch_copy_map = {None: None}
-        ptr = 0
-        # Iterate over each branch, and turn it into 0 or more branches.
-        for og_id, branch in enumerate(self.branches):
-            # Using the filter mask, figure out where to split the branch into pieces.
-            # Parts, or the entire branch, may be excluded if there are no labelled points
-            filtered = branch.get_label_mask(filter)
-            # Each boolean in filtered represents a point, either included or excluded.
-            # Every subbranch begins where a point is excluded and the next point is
-            # included, and ends where a point is included, and the next point is excluded
-            starts = (np.nonzero(filtered[1:] & ~filtered[:-1])[0] + 1).tolist()
-            ends = (np.nonzero(filtered[:-1] & ~filtered[1:])[0] + 1).tolist()
-            # Treat the boundary.
-            if len(filtered) and filtered[0]:
-                starts.insert(0, 0)
-            if len(filtered) and filtered[-1]:
-                ends.append(len(filtered))
-            prev = None
-            nbranch = None
-            # Make all the sub branches. Connect the first to the parent, and store the
-            # last in the map, for children to be connected to.
-            for start, end in zip(starts, ends):
-                nbranch = Branch(*buffers.get_shared(ptr + start, ptr + end))
-                # Store where this branch came from, for loc mapping.
-                nbranch._copied_from_branch = og_id
-                # Store where the points map to
-                nbranch._copied_points_offset = start
-                if not prev:
-                    if branch.is_root or branch_copy_map[branch.parent] is None:
-                        roots.append(nbranch)
-                    else:
-                        branch_copy_map[branch.parent].attach_child(nbranch)
-                else:
-                    prev.attach_child(nbranch)
-                prev = nbranch
-            ptr = ptr + len(branch)
-            if nbranch is None:
-                # If an entire branch is unlabelled, skip it, and map our children's
-                # parent to their grandparent, since we, the parent, don't exist.
-                branch_copy_map[branch] = branch_copy_map[branch.parent]
-            else:
-                # Did we create some branches? Use the last iteration value as parent for
-                # our children.
-                branch_copy_map[branch] = nbranch
-        # Construct and return the morphology
-        return self.__class__(roots, meta=self.meta.copy())
-
-    def simplify(self, *args, optimize=True, **kwargs):
-        super().simplify_branches(*args, **kwargs)
-        if optimize:
-            self.optimize()
-
-    def to_swc(self, file):
-        """
-        Create a SWC file from a Morphology.
-        :param file: path to write to
-        """
-        file_data = _morpho_to_swc(self)
-        if isinstance(file, str) or isinstance(file, Path):
-            np.savetxt(
-                file,
-                file_data,
-                fmt="%d %d %f %f %f %f %d",
-                delimiter="\t",
-                newline="\n",
-                header="",
-                footer="",
-                comments="# ",
-                encoding=None,
-            )
-
-    def to_graph_array(self):
-        """
-        Create a SWC-like numpy array from a Morphology.
-
-        .. warning::
-
-            Custom SWC tags (above 3) won't work and throw an error
-
-        :returns: a numpy array with columns storing the standard SWC attributes
-        :rtype: numpy.ndarray
-        """
-        data = _morpho_to_swc(self)
-        return data
-
-
-def _copy_api(cls, wrap=lambda self: self):
-    # Wraps functions so they are called with `self` wrapped in `wrap`
-    def make_wrapper(f):
-        @functools.wraps(f)
-        def wrapper(self, *args, **kwargs):
-            return f(wrap(self), *args, **kwargs)
-
-        return wrapper
-
-    # Decorates a class so that it copies and wraps (see above) the public API of `cls`
-    def decorator(decorated_cls):
-        for key, f in vars(cls).items():
-            if (
-                inspect.isfunction(f)
-                and not key.startswith("_")
-                and key not in vars(decorated_cls)
-            ):
-                setattr(decorated_cls, key, make_wrapper(f))
-
-        return decorated_cls
-
-    return decorator
-
-
-# For every `SubTree.f` there is a `Branch.f` == `SubTree([branch]).f` so we copy and wrap
-# the public API of `SubTree` onto `Branch`, with the `SubTree([self])` wrapped into it.
-@_copy_api(SubTree, lambda self: SubTree([self]))
-class Branch:
-    """
-    A vector based representation of a series of point in space. Can be a root or
-    connected to a parent branch. Can be a terminal branch or have multiple children.
-    """
-
-    def __init__(self, points, radii, labels=None, properties=None, children=None):
-        """
-        :param points: Array of 3D coordinates defining the point of the branch
-        :type points: list | numpy.ndarray
-        :param radii: Array of radii associated to each point
-        :type radii: list | numpy.ndarray
-        :param labels: Array of labels to associate to each point
-        :type labels: EncodedLabels | List[str] | set | numpy.ndarray
-        :param properties: dictionary of per-point data to store in the branch
-        :type properties: dict
-        :param children: list of child branches to attach to the branch
-        :type children: List[bsb.morphologies.Branch]
-        :raises bsb.exceptions.MorphologyError: if a property of the branch does not have the same
-            size as its points
-        """
-
-        self._points = _gutil.sanitize_ndarray(points, (-1, 3), float)
-        self._radii = _gutil.sanitize_ndarray(radii, (-1,), float)
-        _gutil.assert_samelen(self._points, self._radii)
-        self._children = []
-        if labels is None:
-            labels = EncodedLabels.none(len(points))
-        elif not isinstance(labels, EncodedLabels):
-            labels = EncodedLabels.from_labelset(len(points), labels)
-        self._labels = labels
-        if properties is None:
-            properties = {}
-        mismatched = [str(k) for k, v in properties.items() if len(v) != len(points)]
-        if mismatched:
-            raise MorphologyError(
-                f"Morphology properties {', '.join(mismatched)} are not length {len(points)}"
-            )
-        self._properties = {
-            k: v if isinstance(v, np.ndarray) else np.array(v)
-            for k, v in properties.items()
-        }
-        self._parent = None
-        self._on_mutate = lambda: None
-        if children is not None:
-            for child in children:
-                self.attach_child(child)
-
-    def set_properties(self, **kwargs):
-        for prop, values in kwargs.items():
-            if len(values) != len(self):
-                raise ValueError(f"Expected {len(self)} {prop}, got {len(values)}.")
-            if prop in self._properties:
-                self._properties[prop][:] = values
-            else:
-                self._on_mutate()
-                self._properties[prop] = values
-
-    def __getattr__(self, attr):
-        if not hasattr(self, "_properties"):
-            raise UnpicklingError("Branch class does not support pickling.")
-        if attr in self._properties:
-            return self._properties[attr]
-        else:
-            super().__getattribute__(attr)
-
-    def __copy__(self):
-        return self.copy()
-
-    def __bool__(self):
-        # Without this, empty branches are False, and `if branch.parent:` checks fail.
-        return True
-
-    def __eq__(self, other):
-        if isinstance(other, Branch):
-            return (
-                self.points.shape == other.points.shape
-                and np.allclose(self.points, other.points)
-                and self.labels == other.labels
-            )
-        else:
-            return np.allclose(self.points, other)
-
-    def __hash__(self):
-        return id(self)
-
-    @property
-    def parent(self):
-        return self._parent
-
-    @property
-    def size(self):
-        """
-        Returns the amount of points on this branch
-
-        :returns: Number of points on the branch.
-        :rtype: int
-        """
-        return len(self._points)
-
-    def __len__(self):
-        return self.size
-
-    @property
-    def points(self):
-        """
-        Return the spatial coordinates of the points on this branch.
-        """
-        return self._points
-
-    @points.setter
-    def points(self, value):
-        arr = np.array(value, copy=False, dtype=float)
-        if arr.shape == self._points.shape:
-            self._points[:] = arr
-        elif arr.ndim != 2 or arr.shape[1] != 3:
-            raise ValueError(f"Point data must have (N, 3) shape, {arr.shape} given.")
-        else:
-            self._points = arr
-
-    @property
-    def _kd_tree(self):
-        """
-        Return a `scipy.spatial.cKDTree` of this branch points for fast spatial queries.
-
-        .. warning::
-
-           Constructing a kd-tree takes time and should only be used for repeat queries.
-
-        """
-        import scipy.spatial
-
-        return scipy.spatial.cKDTree(self._points)
-
-    @property
-    def point_vectors(self):
-        """
-        Return the individual vectors between consecutive points on this branch.
-        """
-        return np.diff(self.points, axis=0)
-
-    @property
-    def segments(self):
-        """
-        Return the start and end points of vectors between consecutive points on this
-        branch.
-        """
-        return np.hstack(
-            (self.points[:-1], self.points[:-1] + self.point_vectors)
-        ).reshape(-1, 2, 3)
-
-    @property
-    def start(self):
-        """
-        Return the spatial coordinates of the starting point of this branch.
-        """
-        try:
-            return self._points[0]
-        except IndexError:
-            raise EmptyBranchError("Empty branch has no starting point") from None
-
-    @property
-    def end(self):
-        """
-        Return the spatial coordinates of the terminal point of this branch.
-        """
-        try:
-            return self._points[-1]
-        except IndexError:
-            raise EmptyBranchError("Empty branch has no ending point") from None
-
-    @property
-    def vector(self):
-        """
-        Return the vector of the axis connecting the start and terminal points.
-        """
-        try:
-            return self.end - self.start
-        except IndexError:
-            raise EmptyBranchError("Empty branch has no vector") from None
-
-    @property
-    def versor(self):
-        """
-        Return the normalized vector of the axis connecting the start and terminal points.
-        """
-        versor = (self.end - self.start) / np.linalg.norm(self.end - self.start)
-        if np.any(np.isnan(versor)):
-            raise EmptyBranchError("Empty and single-point branched have no versor")
-        else:
-            return versor
-
-    @property
-    def euclidean_dist(self):
-        """
-        Return the Euclidean distance from the start to the terminal point of this branch.
-        """
-        try:
-            return np.sqrt(np.sum((self.end - self.start) ** 2))
-        except IndexError:
-            raise EmptyBranchError("Empty branch has no Euclidean distance") from None
-
-    @property
-    def max_displacement(self):
-        """
-        Return the max displacement of the branch points from its axis vector.
-        """
-        try:
-            displacements = np.linalg.norm(
-                np.cross(self.versor, (self.points - self.start)), axis=1
-            )
-            return np.max(displacements)
-        except IndexError:
-            raise EmptyBranchError(
-                "Impossible to compute max_displacement in branches with 0 or 1 points."
-            ) from None
-
-    @property
-    def path_length(self):
-        """
-        Return the sum of the euclidean distances between the points on the branch.
-        """
-        return np.sum(np.sqrt(np.sum(self.point_vectors**2, axis=1)))
-
-    @property
-    def fractal_dim(self):
-        """
-        Return the fractal dimension of this branch, computed as the coefficient
-        of the line fitting the log-log plot of path vs euclidean distances of its points.
-        """
-        if len(self.points) == 0:
-            raise EmptyBranchError("Empty branch has no fractal dimension") from None
-        else:
-            euclidean = np.sqrt(np.sum((self.points - self.start) ** 2, axis=1))
-            path = np.cumsum(np.sqrt(np.sum(self.point_vectors**2, axis=1)))
-            log_e = np.log(euclidean[1:])
-            log_p = np.log(path)
-            if len(self.points) <= 2:
-                return 1.0
-            return np.polyfit(log_e, log_p, 1)[0]
-
-    @property
-    def radii(self):
-        """
-        Return the radii of the points on this branch.
-        """
-        return self._radii
-
-    @radii.setter
-    def radii(self, value):
-        arr = np.array(value, copy=False, dtype=float)
-        if arr.shape == self._radii.shape:
-            self._radii[:] = arr
-        else:
-            self._radii = arr.ravel()
-
-    @property
-    def labels(self):
-        """
-        Return the labels of the points on this branch. Labels are represented as a number
-        that is associated to a set of labels. See :ref:`morphology_labels` for more info.
-        """
-        return self._labels
-
-    @property
-    def labelsets(self):
-        """
-        Return the sets of labels associated to each numerical label.
-        """
-        return self._labels.labels
-
-    def list_labels(self):
-        """
-        Return a list of labels present on the branch.
-        """
-        lookup = np.vectorize(self._labels.labels.get)
-        labels = np.unique(lookup(self._labels.raw))
-        return sorted(set(_gutil.ichain(labels)))
-
-    @property
-    def is_root(self):
-        """
-        Returns whether this branch is root or if it has a parent.
-
-        :returns: True if this branch has no parent, False otherwise.
-        :rtype: bool
-        """
-        return not self._parent
-
-    @property
-    def is_terminal(self):
-        """
-        Returns whether this branch is terminal or if it has children.
-
-        :returns: True if this branch has no children, False otherwise.
-        :rtype: bool
-        """
-        return not self._children
-
-    def copy(self, branch_class=None):
-        """
-        Return a parentless and childless copy of the branch.
-
-        :param branch_class: Custom branch creation class
-        :type branch_class: type
-        :returns: A branch, or `branch_class` if given, without parents or children.
-        :rtype: bsb.morphologies.Branch
-        """
-        cls = branch_class or type(self)
-        props = {k: v.copy() for k, v in self._properties.items()}
-        return cls(self._points.copy(), self._radii.copy(), self._labels.copy(), props)
-
-    def label(self, labels, points=None):
-        """
-        Add labels to the branch.
-
-        :param labels: Label(s) for the branch
-        :type labels: List[str]
-        :param points: An integer or boolean mask to select the points to label.
-        """
-        if points is None:
-            points = np.ones(len(self), dtype=bool)
-        self._labels.label(labels, points)
-
-    @property
-    def children(self):
-        """
-        Collection of the child branches of this branch.
-
-        :returns: list of :class:`Branches <.morphologies.Branch>`
-        :rtype: list
-        """
-        return self._children.copy()
-
-    def attach_child(self, branch):
-        """
-        Attach a branch as a child to this branch.
-
-        :param branch: Child branch
-        :type branch: :class:`Branch <.morphologies.Branch>`
-        """
-        self._on_mutate()
-        if branch._parent is not None:
-            branch._parent.detach_child(branch)
-        self._children.append(branch)
-        branch._parent = self
-
-    def find_closest_point(self, coord):
-        """
-        Return the index of the closest on this branch to a desired coordinate.
-
-        :param coord: The coordinate to find the nearest point to
-        :type: :class:`numpy.ndarray`
-        """
-        diff = np.sqrt(np.sum((self._points - coord) ** 2, axis=1))
-        return np.argmin(diff)
-
-    def insert_branch(self, branch, index):
-        """
-        Split this branch and insert the given ``branch`` at the specified ``index``.
-
-        :param branch: Branch to be attached
-        :type branch: :class:`Branch <.morphologies.Branch>`
-        :param index: Index or coordinates of the cutpoint; if coordinates are given, the closest point to the coordinates is used.
-        :type: Union[:class:`numpy.ndarray`, int]
-        """
-        index = np.array(index, copy=False)
-        if index.ndim != 0:
-            index = self.find_closest_point(index)
-
-        if index < 0 or index >= len(self):
-            raise IndexError(
-                f"Cannot insert branch at cutpoint: index {index} is out of range ({len(self)})"
-            )
-
-        if index == len(self.points) - 1:
-            self.attach_child(branch)
-        elif index == 0:
-            self.parent.attach_child(branch)
-        else:
-            first_segment = Branch(
-                self._points.copy()[: index + 1],
-                self._radii.copy()[: index + 1],
-                self._labels.copy()[: index + 1],
-                {k: v.copy()[: index + 1] for k, v in self._properties.items()},
-            )
-            self.parent.attach_child(first_segment)
-            self.parent.detach_child(self)
-            first_segment.attach_child(branch)
-            second_segment = Branch(
-                self._points.copy()[index:],
-                self._radii.copy()[index:],
-                self._labels.copy()[index:],
-                {k: v.copy()[index:] for k, v in self._properties.items()},
-            )
-            for b in self.children:
-                self.detach_child(b)
-                second_segment.attach_child(b)
-            first_segment.attach_child(second_segment)
-
-    def detach(self):
-        """
-        Detach the branch from its parent, if one exists.
-        """
-        if self.parent:
-            self.parent.detach_child(self)
-
-    def detach_child(self, branch):
-        """
-        Remove a branch as a child from this branch.
-
-        :param branch: Child branch
-        :type branch: :class:`Branch <.morphologies.Branch>`
-        """
-        if branch._parent is not self:
-            raise ValueError(f"Can't detach {branch} from {self}, not a child branch.")
-        self._on_mutate()
-        self._children = [b for b in self._children if b is not branch]
-        branch._parent = None
-
-    def walk(self):
-        """
-        Iterate over the points in the branch.
-        """
-        return zip(
-            self.points[:, 0],
-            self.points[:, 1],
-            self.points[:, 2],
-            self.radii,
-            self.labels.walk(),
-            *self._properties.values(),
-        )
-
-    def contains_labels(self, labels):
-        """
-        Check if this branch contains any points labelled with any of the given labels.
-
-        :param labels: The labels to check for.
-        :type labels: List[str]
-        :rtype: bool
-        """
-        return self.labels.contains(labels)
-
-    def get_points_labelled(self, labels):
-        """
-        Filter out all points with certain labels
-
-        :param labels: The labels to check for.
-        :type labels: List[str] | numpy.ndarray[str]
-        :returns: All points with the labels.
-        :rtype: List[numpy.ndarray]
-        """
-        return self.points[self.get_label_mask(labels)]
-
-    def get_label_mask(self, labels):
-        """
-        Return a mask for the specified labels
-
-        :param labels: The labels to check for.
-        :type labels: List[str] | numpy.ndarray[str]
-        :returns: A boolean mask that selects out the points that match the label.
-        :rtype: List[numpy.ndarray]
-        """
-        return self.labels.get_mask(labels)
-
-    def introduce_point(self, index, *args, labels=None):
-        """
-        Insert a new point at ``index``, before the existing point at ``index``.
-
-        :param index: Index of the new point.
-        :type index: int
-        :param args: Vector coordinates of the new point
-        :type args: float
-        :param labels: The labels to assign to the point.
-        :type labels: list
-        """
-        self._on_mutate()
-        for v, vector_name in enumerate(type(self).vectors):
-            vector = getattr(self, vector_name)
-            new_vector = np.concatenate((vector[:index], [args[v]], vector[index:]))
-            setattr(self, vector_name, new_vector)
-        if labels is None:
-            labels = set()
-        for label, mask in self._label_masks.items():
-            has_label = label in labels
-            new_mask = np.concatenate((mask[:index], [has_label], mask[index:]))
-            self._label_masks[label] = new_mask
-
-    def introduce_arc_point(self, arc_val):
-        """
-        Introduce a new point at the given arc length.
-
-        :param arc_val: Arc length between 0 and 1 to introduce new point at.
-        :type arc_val: float
-        :returns: The index of the new point.
-        :rtype: int
-        """
-        arc = self.as_arc()
-        arc_point_floor = self.floor_arc_point(arc_val)
-        arc_point_ceil = self.ceil_arc_point(arc_val)
-        arc_floor = arc[arc_point_floor]
-        arc_ceil = arc[arc_point_ceil]
-        point_floor = self[arc_point_floor]
-        point_ceil = self[arc_point_ceil]
-        rem = (arc_val - arc_floor) / (arc_ceil - arc_floor)
-        new_point = (point_ceil - point_floor) * rem + point_floor
-        new_index = arc_point_floor + 1
-        self.introduce_point(new_index, *new_point)
-        return new_index
-
-    def get_arc_point(self, arc, eps=1e-10):
-        """
-        Strict search for an arc point within an epsilon.
-
-        :param arc: Arclength position to look for.
-        :type arc: float
-        :param eps: Maximum distance/tolerance to accept an arc point as a match.
-        :type eps: float
-        :returns: The matched arc point index, or ``None`` if no match is found
-        :rtype: Union[int, None]
-        """
-        arc_values = self.as_arc()
-        arc_match = (i for i, arc_p in enumerate(arc_values) if abs(arc_p - arc) < eps)
-        return next(arc_match, None)
-
-    def as_arc(self):
-        """
-        Return the branch as a vector of arclengths in the closed interval [0, 1]. An
-        arclength is the distance each point to the start of the branch along the branch
-        axis, normalized by total branch length. A point at the start will have an
-        arclength close to 0, and a point near the end an arclength close to 1
-
-        :returns: Vector of branch points as arclengths.
-        :rtype: :class:`numpy.ndarray`
-        """
-        arc_distances = np.sqrt(np.sum(np.diff(self.points, axis=0) ** 2, axis=1))
-        arc_length = np.sum(arc_distances)
-        return np.cumsum(np.concatenate(([0], arc_distances))) / arc_length
-
-    def floor_arc_point(self, arc):
-        """
-        Get the index of the nearest proximal arc point.
-        """
-        p = 0
-        for i, a in enumerate(self.as_arc()):
-            if a <= arc:
-                p = i
-            else:
-                break
-        return p
-
-    def ceil_arc_point(self, arc):
-        """
-        Get the index of the nearest distal arc point.
-        """
-        for i, a in enumerate(self.as_arc()):
-            if a >= arc:
-                return i
-        return len(self) - 1
-
-    def get_axial_distances(self, idx_start=0, idx_end=-1, return_max=False):
-        """
-        Return the displacements or its max value of a subset of branch points from its axis vector.
-        :param idx_start = 0: index of the first point of the subset.
-        :param idx_end = -1: index of the last point of the subset.
-        :param return_max = False: if True the function only returns the max value of displacements, otherwise the entire array.
-        """
-        start = self.points[idx_start]
-        end = self.points[idx_end]
-        versor = (end - start) / np.linalg.norm(end - start)
-        displacements = np.linalg.norm(
-            np.cross(
-                versor,
-                (self.points[idx_start : idx_end + 1] - self.points[idx_start]),
-            ),
-            axis=1,
-        )
-        if return_max:
-            try:
-                return np.max(displacements)
-            except IndexError:
-                raise EmptyBranchError("Selected an empty subset of points") from None
-        else:
-            return displacements
-
-    def delete_point(self, index):
-        """
-        Remove a point from the branch
-
-        :param int index: index position of the point to remove
-        :returns: the branch where the point has been removed
-        :rtype: bsb.morphologies.Branch
-        """
-        self._points = np.delete(self._points, index, axis=0)
-        self._labels = np.delete(self._labels, index, axis=0)
-        self._radii = np.delete(self._radii, index, axis=0)
-        for k, v in self._properties.items():
-            self._properties[k] = np.delete(v, index, axis=0)
-        return self
-
-    def simplify(self, epsilon, idx_start=0, idx_end=-1):
-        """
-        Apply RamerDouglasPeucker algorithm to all points or a subset of points of the branch.
-        :param epsilon: Epsilon to be used in the algorithm.
-        :param idx_start = 0: Index of the first element of the subset of points to be reduced.
-        :param epsilon = -1: Index of the last element of the subset of points to be reduced.
-        """
-        if len(self.points) < 3:
-            return
-        if idx_end == -1:
-            idx_end = len(self.points) - 1
-        if epsilon < 0:
-            raise ValueError(f"Epsilon must be >= 0")
-
-        reduced = []
-        skipped = deque()
-
-        while True:
-            dists = self.get_axial_distances(idx_start, idx_end)
-            try:
-                idx_max = np.argmax(dists)
-                dmax = dists[idx_max]
-                idx_max = idx_start + idx_max
-            except ValueError:
-                dmax = 0
-
-            reduced.append(idx_start)
-            reduced.append(idx_end)
-            if dmax > epsilon and len(dists) > 2:
-                skipped.append((idx_max, idx_end))
-                idx_end = idx_max - 1
-            else:
-                try:
-                    idx_start, idx_end = skipped.pop()
-                except IndexError:
-                    break
-
-        # sorted because indexes are appended to reduced from the middle of the list (the first point with dist > epsilon)
-        # then all points with smaller index  until 0, then all points with bigger index
-        reduced = np.sort(np.unique(reduced))
-        self.points = self.points[reduced]
-        self.radii = self.radii[reduced]
-
-    @functools.wraps(SubTree.cached_voxelize)
-    @functools.cache
-    def cached_voxelize(self, *args, **kwargs):
-        return SubTree([self]).voxelize(*args, **kwargs)
-
-
-def _morpho_to_swc(morpho):
-    # Initialize an empty data array
-    data = np.empty((len(morpho.points), 7), dtype=object)
-    swc_tags = {"soma": 1, "axon": 2, "dendrites": 3}
-    bmap = {}
-    nid = 0
-    offset = 0
-    # Convert labels to tags
-    if not hasattr(morpho, "tags"):
-        tags = np.full(len(morpho.points), -1, dtype=int)
-        for key in swc_tags.keys():
-            mask = morpho.get_label_mask([key])
-            tags[mask] = swc_tags[key]
-    else:
-        tags = morpho.tags
-    if np.any(tags == -1):
-        raise NotImplementedError("Can't store morphologies with custom SWC tags")
-    # Iterate over the morphology branches
-    for b in morpho.branches:
-        ids = (
-            np.arange(nid, nid + len(b) - 1)
-            if len(b) > 1
-            else np.arange(nid, nid + len(b))
-        )
-        samples = ids + 1
-        data[ids, 0] = samples
-        data[ids, 1] = tags[ids + offset]
-        data[ids, 2:5] = morpho.points[ids + offset]
-        try:
-            data[ids, 5] = morpho.radii[ids + offset]
-        except Exception as e:
-            raise MorphologyDataError(
-                f"Couldn't convert morphology radii to SWC: {e}."
-                " Note that SWC files cannot store multi-dimensional radii"
-            )
-        nid += len(b) - 1 if len(b) > 1 else len(b)
-        offset += 1
-        bmap[b] = ids[-1]
-        data[ids, 6] = ids
-        data[ids[0], 6] = -1 if b.parent is None else bmap[b.parent] + 1
-
-    return data[data != np.array(None)].reshape(-1, 7)
-
-
-__all__ = [
-    "Branch",
-    "Morphology",
-    "MorphologySet",
-    "RotationSet",
-    "SubTree",
-    "branch_iter",
-    "parse_morphology_file",
-]
+"""
+Morphology module
+"""
+
+# This is a note to myself, should expand into docs:
+#
+# It goes ``morphology-on-file`` into ``repository`` that the ``storage`` needs to provide
+# support for. Then after a placement job has placed cells for a chunk, the positions are
+# sent to a ``distributor`` that is supposed to use the ``indicators`` to ask the
+# ``storage.morphology_repository`` which ``loaders`` are appropriate for the given
+# ``selectors``, then, still hopefully using just morpho metadata the  ``distributor``
+# generates indices and rotations. In more complex cases the ``selector`` and
+# ``distributor`` can both load the morphologies but this will slow things down.
+#
+# In the simulation step, these (possibly dynamically modified) morphologies are passed
+# to the cell model instantiators.
+
+import functools
+import inspect
+import itertools
+from collections import deque
+from pathlib import Path
+from pickle import UnpicklingError
+
+import numpy as np
+from scipy.spatial.transform import Rotation
+
+from .. import _util as _gutil
+from .._encoding import EncodedLabels
+from ..exceptions import EmptyBranchError, MorphologyDataError, MorphologyError
+from ..voxels import VoxelSet
+
+
+class MorphologySet:
+    """
+    Associates a set of :class:`StoredMorphologies
+    <.storage.interfaces.StoredMorphology>` to cells
+    """
+
+    def __init__(self, loaders, m_indices=None, /, labels=None):
+        """
+        :param loaders: list of Morphology loader functions.
+        :type loaders: List[Callable[[], bsb.storage.interfaces.StoredMorphology]]
+        :param m_indices: indices of the loaders for each of the morphologies.
+        :type: List[int]
+        """
+        if m_indices is None:
+            loaders, m_indices = np.unique(loaders, return_inverse=True)
+        self._m_indices = np.array(m_indices, copy=False, dtype=int)
+        self._loaders = list(loaders)
+        check_max = np.max(m_indices, initial=-1)
+        if check_max >= len(loaders):
+            raise IndexError(f"Index {check_max} out of range for {len(loaders)}.")
+        self._cached = {}
+        self._labels = labels
+
+    def set_label_filter(self, labels):
+        self._cached = {}
+        for loader in self._loaders:
+            loader._cached_load.cache_clear()
+        self._labels = labels
+
+    @_gutil.obj_str_insert
+    def __repr__(self):
+        return f"{len(self)} cells, {len(self._loaders)} morphologies"
+
+    def __contains__(self, value):
+        return value in [loader.name for loader in self._loaders]
+
+    def count_morphologies(self):
+        return len(self._loaders)
+
+    def count_unique(self):
+        uniques = []
+        count = 0
+        for m in (m.load() for m in self._loaders):
+            if not any(f == m for f in uniques):
+                uniques.append(m)
+                count += 1
+        return count
+
+    def __len__(self):
+        return len(self._m_indices)
+
+    def __iter__(self):
+        return self.iter_morphologies()
+
+    @property
+    def names(self):
+        return [loader.name for loader in self._loaders]
+
+    def get_indices(self, copy=True):
+        return self._m_indices.copy() if copy else self._m_indices
+
+    def get(self, index, cache=True, hard_cache=False):
+        data = self._m_indices[index]
+        if data.ndim:
+            return self._get_many(data, cache, hard_cache)
+        else:
+            return self._get_one(data, cache, hard_cache)
+
+    def _get_one(self, idx, cache, hard_cache):
+        if cache:
+            if hard_cache:
+                return self._loaders[idx].cached_load(self._labels)
+
+            if idx not in self._cached:
+                self._cached[idx] = (
+                    self._loaders[idx].load().set_label_filter(self._labels).as_filtered()
+                )
+            return self._cached[idx].copy()
+        else:
+            return self._loaders[idx].load().set_label_filter(self._labels).as_filtered()
+
+    def _get_many(self, data, cache, hard_cache):
+        if hard_cache:
+            return np.array([self._loaders[idx].cached_load() for idx in data])
+        elif cache:
+            res = []
+            for idx in data:
+                if idx not in self._cached:
+                    self._cached[idx] = self._loaders[idx].load()
+                res.append(self._cached[idx].copy())
+        else:
+            res = np.array([self._loaders[idx].load() for idx in data])
+        return res
+
+    def clear_soft_cache(self):
+        self._cached = {}
+
+    def iter_morphologies(self, cache=True, unique=False, hard_cache=False):
+        """
+        Iterate over the morphologies in a MorphologySet with full control over caching.
+
+        :param cache: Use :ref:`soft-caching` (1 copy stored in mem per cache miss, 1 copy
+          created from that per cache hit).
+        :type cache: bool
+        :param hard_cache: Use :ref:`hard-caching` (1 copy stored on the loader, always
+          same copy returned from that loader forever).
+        """
+        if hard_cache:
+
+            def _load(loader):
+                return loader.cached_load(self._labels)
+
+        elif self._labels is not None:
+
+            def _load(loader):
+                return loader.load().set_label_filter(self._labels).as_filtered()
+
+        else:
+
+            def _load(loader):
+                return loader.load()
+
+        if unique:
+            yield from map(_load, self._loaders)
+        elif not cache or hard_cache:
+            yield from map(_load, (self._loaders[idx] for idx in self._m_indices))
+        else:
+            _cached = {}
+            for idx in self._m_indices:
+                if idx not in _cached:
+                    _cached[idx] = _load(self._loaders[idx])
+                yield _cached[idx].copy()
+
+    def iter_meta(self, unique=False):
+        if unique:
+            yield from (loader.get_meta() for loader in self._loaders)
+        else:
+            yield from (self._loaders[idx].get_meta() for idx in self._m_indices)
+
+    def _serialize_loaders(self):
+        return [loader.name for loader in self._loaders]
+
+    @classmethod
+    def empty(cls):
+        return cls([], np.empty(0, dtype=int))
+
+    def merge(self, other):
+        merged_loaders = self._loaders.copy()
+        previous_set = set(merged_loaders)
+        if any(loader in previous_set for loader in other._loaders):
+            # There is overlap between the sets, and mapping is required
+            id_map = dict(
+                (i, merged_loaders.index(loader))
+                for i, loader in enumerate(other._loaders)
+                if loader in previous_set
+            )
+            if all(k == v for k, v in id_map.items()):
+                mapped_indices = other._m_indices
+            else:
+
+                def map_ids(id):
+                    mapped_id = id_map.get(id, None)
+                    if mapped_id is None:
+                        mapped_id = id_map[id] = len(merged_loaders)
+                        merged_loaders.append(other._loaders[id])
+                    return mapped_id
+
+                mapped_indices = np.vectorize(map_ids)(other._m_indices)
+            merged_indices = np.concatenate((self._m_indices, mapped_indices))
+        else:
+            # No overlap, we can just offset the new dataset
+            merge_offset = len(self._loaders)
+            merged_loaders = self._loaders + other._loaders
+            merged_indices = np.concatenate(
+                (self._m_indices, other._m_indices + merge_offset)
+            )
+        return MorphologySet(merged_loaders, merged_indices)
+
+    def _mapback(self, locs):
+        if self._labels is None:
+            raise RuntimeError("Mapback requested on unfiltered morphology set.")
+        locs = locs.copy()
+        for i, loader in enumerate(self._loaders):
+            rows = self._m_indices[locs[:, 0]] == i
+            if np.any(rows):
+                morpho = loader.load()
+                filtered = morpho.set_label_filter(self._labels).as_filtered()
+                # Using np.vectorize is Python speed O(n), worst case in numpy is C speed
+                # O(n^2) (that is if every point is on another branch), not sure.
+                branchmap = np.vectorize(
+                    {
+                        bid: b._copied_from_branch
+                        for bid, b in enumerate(filtered.branches)
+                    }.get
+                )
+                pointmap = np.vectorize(
+                    {
+                        bid: b._copied_points_offset
+                        for bid, b in enumerate(filtered.branches)
+                    }.get
+                )
+                # Map points first, then branches, since points depend on unmapped branch.
+                locs[rows, 2] = locs[rows, 2] + pointmap(locs[rows, 1])
+                locs[rows, 1] = branchmap(locs[rows, 1])
+        return locs
+
+
+class RotationSet:
+    """
+    Set of rotations. Returned rotations are of :class:`scipy.spatial.transform.Rotation`
+    """
+
+    def __init__(self, data):
+        if not isinstance(data, np.ndarray):
+            self._data = np.array(
+                [
+                    v.as_euler("xyz", degrees=True) if isinstance(v, Rotation) else v
+                    for v in data
+                ]
+            )
+        else:
+            self._data = data
+        if self._data.ndim != 2 or self._data.shape[1] != 3:
+            raise ValueError("Input should be an (Nx3) matrix of rotations.")
+
+    def __array__(self, dtype=None, *args, **kwargs):
+        return self._data.__array__(dtype, *args, **kwargs)
+
+    def __iter__(self):
+        return self.iter()
+
+    def __getitem__(self, index):
+        data = self._data[index]
+        if data.ndim == 2:
+            return np.array([self._rot(d) for d in data])
+        else:
+            return self._rot(data)
+
+    def __len__(self):
+        return len(self._data)
+
+    def iter(self, cache=False):
+        if cache:
+            yield from (self._cached_rot(tuple(d)) for d in self._data)
+        else:
+            yield from (self._rot(d) for d in self._data)
+
+    @functools.cache
+    def _cached_rot(self, angles):
+        return self._rot(angles)
+
+    def _rot(self, angles):
+        return Rotation.from_euler("xyz", angles)
+
+
+def branch_iter(branch):
+    """
+    Iterate over a branch and all of its children depth first.
+    """
+    yield branch
+    for child in branch._children:
+        yield from branch_iter(child)
+
+
+class SubTree:
+    """
+    Collection of branches, not necessarily all connected.
+    """
+
+    def __init__(self, branches, sanitize=True):
+        if sanitize:
+            # Find the roots of the full subtree(s) emanating from the given, possibly
+            # overlapping branches.
+            if len(branches) < 2:
+                # The roots of the subtrees of 0 or 1 branches is eaqual to the 0 or 1
+                # branches themselves.
+                self.roots = branches
+            else:
+                # Collect the deduplicated subtree emanating from all given branches; use
+                # dict.fromkeys and .keys to preserve insertion order (i.e. DFS order)
+                sub = dict.fromkeys(
+                    itertools.chain(*(b.get_branches() for b in branches))
+                ).keys()
+                # Find the root branches whose parents are not part of the subtrees
+                self.roots = [b for b in sub if b.parent not in sub]
+        else:
+            # No subtree sanitizing: Assume the whole tree is given, or only the roots
+            # have been given, and just take all literal root (non-parent-having)
+            # branches.
+            self.roots = [b for b in branches if b.parent is None]
+        self._is_shared = False
+
+    def __getattr__(self, attr):
+        if not hasattr(self, "_is_shared"):
+            raise UnpicklingError("Morphology class does not support pickling.")
+        if self._is_shared:
+            if attr in self._shared._prop:
+                return self._shared._prop[attr]
+            else:
+                super().__getattribute__(attr)
+        else:
+            return np.concatenate([getattr(b, attr) for b in self.branches])
+
+    def __len__(self):
+        if self._is_shared:
+            return len(self._shared._points)
+        else:
+            return sum(b.size for b in self.get_branches())
+
+    @property
+    def size(self):
+        return len(self)
+
+    @property
+    def branches(self):
+        """
+        Return a depth-first flattened array of all branches.
+        """
+        return self.get_branches()
+
+    @property
+    def points(self):
+        return self.flatten()
+
+    @points.setter
+    def points(self, value):
+        arr = np.array(value, copy=False, dtype=float)
+        if self._is_shared:
+            self.points[:] = arr
+        else:
+            ptr = 0
+            for b in self.branches:
+                b.points = arr[ptr : (ptr := ptr + len(b))]
+
+    @property
+    def radii(self):
+        return self.flatten_radii()
+
+    @radii.setter
+    def radii(self, value):
+        arr = np.array(value, copy=False, dtype=float)
+        if self._is_shared:
+            self.radii[:] = arr
+        else:
+            ptr = 0
+            for b in self.branches:
+                b.radii = arr[ptr : (ptr := ptr + len(b))]
+
+    @property
+    def labels(self):
+        return self.flatten_labels()
+
+    @property
+    def properties(self):
+        return self.flatten_properties()
+
+    @property
+    def bounds(self):
+        f = self.flatten()
+        if not len(f):
+            return np.zeros(3), np.zeros(3)
+        return np.min(f, axis=0), np.max(f, axis=0)
+
+    @property
+    def branch_adjacency(self):
+        """
+        Return a dictonary containing mapping the id of the branch to its children.
+        """
+        idmap = {b: n for n, b in enumerate(self.branches)}
+        return {n: list(map(idmap.get, b.children)) for n, b in enumerate(self.branches)}
+
+    @property
+    def path_length(self):
+        """
+        Return the total path length as the sum of the euclidian distances between
+        consecutive points.
+        """
+        return sum(b.path_length for b in self.branches)
+
+    def subtree(self, labels=None):
+        return SubTree(self.get_branches(labels))
+
+    def get_branches(self, labels=None):
+        """
+        Return a depth-first flattened array of all or the selected branches.
+
+        :param labels: Names of the labels to select.
+        :type labels: list
+        :returns: List of all branches, or the ones fully labelled with any of
+          the given labels.
+        :rtype: list
+        """
+        root_iter = (branch_iter(root) for root in self.roots)
+        all_branch = itertools.chain(*root_iter)
+        if labels is None:
+            return [*all_branch]
+        else:
+            return [b for b in all_branch if b.contains_labels(labels)]
+
+    def flatten(self):
+        """
+        Return the flattened points of the morphology or subtree.
+
+        :rtype: numpy.ndarray
+        """
+        if self._is_shared:
+            return self._shared._points
+        branches = self.get_branches()
+        if not branches:
+            return np.empty((0, 3))
+        return np.vstack(tuple(b.points for b in branches))
+
+    def flatten_radii(self):
+        """
+        Return the flattened radii of the morphology or subtree.
+
+        :rtype: numpy.ndarray
+        """
+        if self._is_shared:
+            return self._shared._radii
+        branches = self.get_branches()
+        if not branches:
+            return np.empty(0)
+        return np.concatenate(tuple(b.radii for b in branches))
+
+    def flatten_labels(self):
+        """
+        Return the flattened labels of the morphology or subtree.
+
+        :rtype: numpy.ndarray
+        """
+        if self._is_shared:
+            return self._shared._labels
+        else:
+            return EncodedLabels.concatenate(*(b._labels for b in self.get_branches()))
+
+    def flatten_properties(self):
+        """
+        Return the flattened properties of the morphology or subtree.
+
+        :rtype: numpy.ndarray
+        """
+        if self._is_shared:
+            if not self._shared._prop:
+                return {}
+            return self._shared._prop.copy()
+        else:
+            branches = self.get_branches()
+            len_ = sum(len(b) for b in branches)
+            all_props = [*set(_gutil.ichain(b._properties.keys() for b in branches))]
+            props = {k: np.empty(len_) for k in all_props}
+            ptr = 0
+            for branch in self.branches:
+                nptr = ptr + len(branch)
+                for k, v in props.items():
+                    prop = branch._properties.get(k, None)
+                    if prop is None:
+                        prop = np.full(len(branch), np.nan)
+                    v[ptr:nptr] = prop
+                ptr = nptr
+            return props
+
+    def label(self, labels, points=None):
+        """
+        Add labels to the morphology or subtree.
+
+        :param labels: Labels to add to the subtree.
+        :type labels: list[str]
+        :param points: Optional boolean or integer mask for the points to be labelled.
+        :type points: numpy.ndarray
+        """
+        if points is None:
+            points = np.ones(len(self), dtype=bool)
+        points = np.array(points, copy=False)
+        if self._is_shared:
+            self.labels.label(labels, points)
+        else:
+            if len(points) == len(self) and points.dtype == bool:
+                ctr = 0
+                for b in self.branches:
+                    b.label(labels, points[ctr : ctr + len(b)])
+                    ctr += len(b)
+            elif np.can_cast(points.dtype, int):
+                points = np.array(points, copy=False, dtype=int)
+                for b in self.branches:
+                    mux = points < len(b)
+                    b.label(labels, points[mux])
+                    points = points[~mux]
+            else:
+                raise ValueError(
+                    "Label indices must be a boolean or integer mask."
+                    f" {points.dtype} given."
+                )
+
+        return self
+
+    def rotate(self, rotation, center=None):
+        """
+        Point rotation
+
+        :param rot: Scipy rotation
+        :type: Union[scipy.spatial.transform.Rotation, List[float,float,float]]
+        :param center: rotation offset point.
+        :type center: numpy.ndarray
+        """
+        if not isinstance(rotation, Rotation):
+            rotation = Rotation.from_euler("xyz", rotation, degrees=True)
+        if self._is_shared:
+            self._shared._points[:] = self._rotate(self._shared._points, rotation, center)
+        else:
+            for b in self.branches:
+                b.points[:] = self._rotate(b.points, rotation, center)
+        return self
+
+    def _rotate(self, points, rot, center):
+        if center is not None:
+            points = points - center
+            rotated_points = rot.apply(points)
+            rotated_points = rotated_points + center
+        else:
+            rotated_points = rot.apply(points)
+        return rotated_points
+
+    def root_rotate(self, rot, downstream_of=0):
+        """
+        Rotate the subtree emanating from each root around the start of that root
+        If downstream_of is provided, will rotate points starting from the index provided (only for
+        subtrees with a single root).
+
+        :param rot: Scipy rotation to apply to the subtree.
+        :type rot: scipy.spatial.transform.Rotation
+        :param downstream_of: index of the point in the subtree from which the rotation should be
+            applied. This feature works only when the subtree has only one root branch.
+        :returns: rotated Morphology
+        :rtype: bsb.morphologies.SubTree
+        """
+
+        if downstream_of != 0:
+            if len(self.roots) > 1:
+                raise ValueError(
+                    "Can't rotate with subbranch precision with multiple roots"
+                )
+            elif type(downstream_of) == int and 0 < downstream_of < len(
+                self.roots[0].points
+            ):
+                b = self.roots[0]
+                group = SubTree([b])
+                upstream = np.copy(b.points[:downstream_of])
+                group.rotate(rot, b.points[downstream_of])
+                b.points[:downstream_of] = upstream
+        else:
+            for b in self.roots:
+                group = SubTree([b])
+                group.rotate(rot, group.origin)
+        return self
+
+    def translate(self, point):
+        """
+        Translate the subtree by a 3D vector.
+
+        :param numpy.ndarray point: 3D vector to translate the subtree.
+        :returns: the translated subtree
+        :rtype: bsb.morphologies.SubTree
+        """
+        if len(point) != 3:
+            raise ValueError("Point must be a sequence of x, y and z coordinates")
+        if self._is_shared:
+            self._shared._points[:] = self._shared._points + point
+        else:
+            for branch in self.branches:
+                branch.points[:] = branch.points[:] + point
+        return self
+
+    @property
+    def origin(self):
+        return np.mean([r.points[0] for r in self.roots], axis=0)
+
+    def center(self):
+        """
+        Center the morphology on the origin
+        """
+        self.translate(-self.origin)
+        return self
+
+    def close_gaps(self):
+        """
+        Close any head-to-tail gaps between parent and child branches.
+        """
+        for branch in self.branches:
+            if branch.parent is not None:
+                gap_offset = branch.parent.points[-1] - branch.points[0]
+                if not np.allclose(gap_offset, 0):
+                    SubTree([branch]).translate(gap_offset)
+        return self
+
+    def collapse(self, on=None):
+        """
+        Collapse all the roots of the morphology or subtree onto a single point.
+
+        :param on: Index of the root to collapse on. Collapses onto the origin by default.
+        :type on: int
+        """
+        if on is None:
+            on = self.origin
+        for root in self.roots:
+            root.translate(on - root.points[0])
+        return self
+
+    def simplify_branches(self, epsilon):
+        """
+        Apply RamerDouglasPeucker algorithm to all points of all branches of the SubTree.
+        :param epsilon: Epsilon to be used in the algorithm.
+        """
+        for branch in self.branches:
+            branch.simplify(epsilon)
+
+    def voxelize(self, N):
+        """
+        Turn the morphology or subtree into an approximating set of axis-aligned cuboids.
+
+        :rtype: bsb.voxels.VoxelSet
+        """
+        return VoxelSet.from_morphology(self, N)
+
+    @functools.cache
+    def cached_voxelize(self, N):
+        """
+        Turn the morphology or subtree into an approximating set of axis-aligned cuboids
+        and cache the result.
+
+        :rtype: bsb.voxels.VoxelSet
+        """
+        return self.voxelize(N)
+
+
+class _SharedBuffers:
+    def __init__(self, points, radii, labels, properties):
+        self._points = points
+        self._radii = radii
+        self._labels = labels if labels is not None else EncodedLabels.none(len(radii))
+        self._prop = properties
+
+    def copy(self):
+        copied_props = {k: v.copy() for k, v in self._prop.items()}
+        return self.__class__(
+            self._points.copy(), self._radii.copy(), self._labels.copy(), copied_props
+        )
+
+    def points_shared(self, branches):
+        return all(b.points.base is self._points for b in branches)
+
+    def radii_shared(self, branches):
+        return all(b.radii.base is self._radii for b in branches)
+
+    def labels_shared(self, branches):
+        return all(b.labels.base is self._labels for b in branches)
+
+    def properties_shared(self, branches):
+        return all(
+            (
+                b._properties.keys() == self._prop.keys() and all(c.base is self._prop[c])
+                for a, c in b._properties.items()
+            )
+            for b in branches
+        )
+
+    def all_buffers_shared(self, branches):
+        return (
+            self.points_shared(branches)
+            and self.radii_shared(branches)
+            and self.labels_shared(branches)
+            and self.properties_shared(branches)
+        )
+
+    def get_shared(self, start, end):
+        copied_props = {k: v[start:end] for k, v in self._prop.items()}
+        return (
+            self._points[start:end],
+            self._radii[start:end],
+            self._labels[start:end],
+            copied_props,
+        )
+
+
+class Morphology(SubTree):
+    """
+    A multicompartmental spatial representation of a cell based on a directed acyclic
+    graph of branches whom consist of data vectors, each element of a vector being a
+    coordinate or other associated data of a point on the branch.
+    """
+
+    def __init__(self, roots, meta=None, shared_buffers=None, sanitize=False):
+        super().__init__(roots, sanitize=sanitize)
+        self._meta = meta if meta is not None else {}
+        self._filter = None
+        if shared_buffers is None:
+            self._shared = None
+            self._is_shared = False
+        else:
+            if isinstance(shared_buffers, _SharedBuffers):
+                self._shared = shared_buffers
+            else:
+                self._shared = _SharedBuffers(*shared_buffers)
+            self._is_shared = self._check_shared()
+            for branch in self.branches:
+                branch._on_mutate = self._mutnotif
+
+    @_gutil.obj_str_insert
+    def __repr__(self):
+        return (
+            f"{len(self.roots)} roots, {len(self)} points,"
+            f" from {self.bounds[0]} to {self.bounds[1]}"
+        )
+
+    def __eq__(self, other):
+        return len(self.branches) == len(other.branches) and all(
+            b1.is_terminal == b2.is_terminal and (not b1.is_terminal or b1 == b2)
+            for b1, b2 in zip(self.branches, other.branches)
+        )
+
+    def __lt__(self, other):
+        # Sorting compares using lt, so we use id for useless but stable comparison.
+        return id(self) < id(other)
+
+    def __hash__(self):
+        return id(self)
+
+    def _check_shared(self):
+        if self._shared is None:
+            return False
+        return self._shared.all_buffers_shared(self.branches)
+
+    def _mutnotif(self):
+        self._is_shared = False
+
+    @property
+    def is_optimized(self):
+        return self._shared
+
+    def optimize(self, force=False):
+        if force or not self._is_shared:
+            branches = self.branches
+            len_ = sum(len(b) for b in branches)
+            points = np.empty((len_, 3))
+            radii = np.empty(len_)
+            all_props = [*set(_gutil.ichain(b._properties.keys() for b in branches))]
+            types = [
+                next(_p[k].dtype for b in branches if k in (_p := b._properties))
+                for k in all_props
+            ]
+            props = {k: np.empty(len_, dtype=t) for k, t in zip(all_props, types)}
+            labels = EncodedLabels.concatenate(*(b._labels for b in branches))
+            ptr = 0
+            for branch in self.branches:
+                nptr = ptr + len(branch)
+                points[ptr:nptr] = branch.points
+                branch._points = points[ptr:nptr]
+                radii[ptr:nptr] = branch.radii
+                branch._radii = radii[ptr:nptr]
+                for k, v in props.items():
+                    prop = branch._properties.get(k, None)
+                    if prop is None:
+                        prop = np.full(len(branch), np.nan)
+                    v[ptr:nptr] = prop
+                    branch._properties[k] = v[ptr:nptr]
+                branch._labels = labels[ptr:nptr]
+                ptr = nptr
+            self._shared = _SharedBuffers(points, radii, labels, props)
+            self._is_shared = True
+            assert self._check_shared(), "optimize should result in shared buffers"
+
+    @property
+    def meta(self):
+        return self._meta
+
+    @meta.setter
+    def meta(self, value):
+        self._meta = value
+
+    @property
+    def adjacency_dictionary(self):
+        """
+        Return a dictonary associating to each key (branch index) a list of adjacent branch indices
+        """
+        branches = self.branches
+        idmap = {b: n for n, b in enumerate(branches)}
+        return {n: list(map(idmap.get, b.children)) for n, b in enumerate(branches)}
+
+    @property
+    def labelsets(self):
+        """
+        Return the sets of labels associated to each numerical label.
+        """
+        self.optimize()
+        return self._shared._labels.labels
+
+    def list_labels(self):
+        """
+        Return a list of labels present on the morphology.
+        """
+        self.optimize()
+        return sorted(set(_gutil.ichain(self._shared._labels.labels.values())))
+
+    def set_label_filter(self, labels):
+        """
+        Set a label filter, so that `as_filtered` returns copies filtered by these labels.
+        """
+        self._filter = labels
+        return self
+
+    def get_label_mask(self, labels):
+        """
+        Get a mask corresponding to all the points labelled with 1 or more of the given
+        labels
+        """
+        self.optimize()
+        return self.labels.get_mask(labels)
+
+    @classmethod
+    def empty(cls):
+        return cls([])
+
+    def copy(self):
+        """
+        Copy the morphology.
+        """
+        # Make sure to optimize so that we use 1 shared buffer.
+        self.optimize(force=False)
+        # Copy that buffer into a new one
+        buffers = self._shared.copy()
+        roots = []
+        branch_copy_map = {}
+        ptr = 0
+        # For each branch, create a copy, and assign a piece of the copied buffer to it.
+        # Also attach it to its intended parent. Since we iterate DFS, each parent occurs
+        # before their children, so we can always find it in `branch_copy_map` from a
+        # previous iteration.
+        for branch in self.branches:
+            nptr = ptr + len(branch)
+            nbranch = Branch(*buffers.get_shared(ptr, nptr))
+            branch_copy_map[branch] = nbranch
+            if not branch.is_root:
+                branch_copy_map[branch.parent].attach_child(nbranch)
+            else:
+                roots.append(nbranch)
+            ptr = nptr
+        # Construct the morphology
+        return self.__class__(roots, shared_buffers=buffers, meta=self.meta.copy())
+
+    def as_filtered(self, labels=None):
+        """
+        Return a filtered copy of the morphology that includes only points that match the
+        current label filter, or the specified labels.
+        """
+        filter = labels if labels is not None else self._filter
+        if filter is None:
+            return self.copy()
+        self.optimize(force=False)
+        buffers = self._shared.copy()
+        roots = []
+        branch_copy_map = {None: None}
+        ptr = 0
+        # Iterate over each branch, and turn it into 0 or more branches.
+        for og_id, branch in enumerate(self.branches):
+            # Using the filter mask, figure out where to split the branch into pieces.
+            # Parts, or the entire branch, may be excluded if there are no labelled points
+            filtered = branch.get_label_mask(filter)
+            # Each boolean in filtered represents a point, either included or excluded.
+            # Every subbranch begins where a point is excluded and the next point is
+            # included, and ends where a point is included, and the next point is excluded
+            starts = (np.nonzero(filtered[1:] & ~filtered[:-1])[0] + 1).tolist()
+            ends = (np.nonzero(filtered[:-1] & ~filtered[1:])[0] + 1).tolist()
+            # Treat the boundary.
+            if len(filtered) and filtered[0]:
+                starts.insert(0, 0)
+            if len(filtered) and filtered[-1]:
+                ends.append(len(filtered))
+            prev = None
+            nbranch = None
+            # Make all the sub branches. Connect the first to the parent, and store the
+            # last in the map, for children to be connected to.
+            for start, end in zip(starts, ends):
+                nbranch = Branch(*buffers.get_shared(ptr + start, ptr + end))
+                # Store where this branch came from, for loc mapping.
+                nbranch._copied_from_branch = og_id
+                # Store where the points map to
+                nbranch._copied_points_offset = start
+                if not prev:
+                    if branch.is_root or branch_copy_map[branch.parent] is None:
+                        roots.append(nbranch)
+                    else:
+                        branch_copy_map[branch.parent].attach_child(nbranch)
+                else:
+                    prev.attach_child(nbranch)
+                prev = nbranch
+            ptr = ptr + len(branch)
+            if nbranch is None:
+                # If an entire branch is unlabelled, skip it, and map our children's
+                # parent to their grandparent, since we, the parent, don't exist.
+                branch_copy_map[branch] = branch_copy_map[branch.parent]
+            else:
+                # Did we create some branches? Use the last iteration value as parent for
+                # our children.
+                branch_copy_map[branch] = nbranch
+        # Construct and return the morphology
+        return self.__class__(roots, meta=self.meta.copy())
+
+    def swap_axes(self, axis1: int, axis2: int):
+        """
+        Interchange two axes of a morphology points.
+
+        :param int axis1: index of the first axis to exchange
+        :param int axis2: index of the second axis to exchange
+        :return: the modified morphology
+        :rtype: bsb.morphologies.Morphology
+        """
+        if not 0 <= axis1 < 3 or not 0 <= axis2 < 3:
+            raise ValueError(
+                f"Axes values should be in [0, 1, 2], {axis1}, {axis2} given."
+            )
+        for b in self.branches:
+            old_column = np.copy(b.points[:, axis1])
+            b.points[:, axis1] = b.points[:, axis2]
+            b.points[:, axis2] = old_column
+
+        return self
+
+    def simplify(self, *args, optimize=True, **kwargs):
+        super().simplify_branches(*args, **kwargs)
+        if optimize:
+            self.optimize()
+
+    def to_swc(self, file):
+        """
+        Create a SWC file from a Morphology.
+        :param file: path to write to
+        """
+        file_data = _morpho_to_swc(self)
+        if isinstance(file, str) or isinstance(file, Path):
+            np.savetxt(
+                file,
+                file_data,
+                fmt="%d %d %f %f %f %f %d",
+                delimiter="\t",
+                newline="\n",
+                header="",
+                footer="",
+                comments="# ",
+                encoding=None,
+            )
+
+    def to_graph_array(self):
+        """
+        Create a SWC-like numpy array from a Morphology.
+
+        .. warning::
+
+            Custom SWC tags (above 3) won't work and throw an error
+
+        :returns: a numpy array with columns storing the standard SWC attributes
+        :rtype: numpy.ndarray
+        """
+        data = _morpho_to_swc(self)
+        return data
+
+
+def _copy_api(cls, wrap=lambda self: self):
+    # Wraps functions so they are called with `self` wrapped in `wrap`
+    def make_wrapper(f):
+        @functools.wraps(f)
+        def wrapper(self, *args, **kwargs):
+            return f(wrap(self), *args, **kwargs)
+
+        return wrapper
+
+    # Decorates a class so that it copies and wraps (see above) the public API of `cls`
+    def decorator(decorated_cls):
+        for key, f in vars(cls).items():
+            if (
+                inspect.isfunction(f)
+                and not key.startswith("_")
+                and key not in vars(decorated_cls)
+            ):
+                setattr(decorated_cls, key, make_wrapper(f))
+
+        return decorated_cls
+
+    return decorator
+
+
+# For every `SubTree.f` there is a `Branch.f` == `SubTree([branch]).f` so we copy and wrap
+# the public API of `SubTree` onto `Branch`, with the `SubTree([self])` wrapped into it.
+@_copy_api(SubTree, lambda self: SubTree([self]))
+class Branch:
+    """
+    A vector based representation of a series of point in space. Can be a root or
+    connected to a parent branch. Can be a terminal branch or have multiple children.
+    """
+
+    def __init__(self, points, radii, labels=None, properties=None, children=None):
+        """
+        :param points: Array of 3D coordinates defining the point of the branch
+        :type points: list | numpy.ndarray
+        :param radii: Array of radii associated to each point
+        :type radii: list | numpy.ndarray
+        :param labels: Array of labels to associate to each point
+        :type labels: EncodedLabels | List[str] | set | numpy.ndarray
+        :param properties: dictionary of per-point data to store in the branch
+        :type properties: dict
+        :param children: list of child branches to attach to the branch
+        :type children: List[bsb.morphologies.Branch]
+        :raises bsb.exceptions.MorphologyError: if a property of the branch does not have the same
+            size as its points
+        """
+
+        self._points = _gutil.sanitize_ndarray(points, (-1, 3), float)
+        self._radii = _gutil.sanitize_ndarray(radii, (-1,), float)
+        _gutil.assert_samelen(self._points, self._radii)
+        self._children = []
+        if labels is None:
+            labels = EncodedLabels.none(len(points))
+        elif not isinstance(labels, EncodedLabels):
+            labels = EncodedLabels.from_labelset(len(points), labels)
+        self._labels = labels
+        if properties is None:
+            properties = {}
+        mismatched = [str(k) for k, v in properties.items() if len(v) != len(points)]
+        if mismatched:
+            raise MorphologyError(
+                f"Morphology properties {', '.join(mismatched)} are not length {len(points)}"
+            )
+        self._properties = {
+            k: v if isinstance(v, np.ndarray) else np.array(v)
+            for k, v in properties.items()
+        }
+        self._parent = None
+        self._on_mutate = lambda: None
+        if children is not None:
+            for child in children:
+                self.attach_child(child)
+
+    def set_properties(self, **kwargs):
+        for prop, values in kwargs.items():
+            if len(values) != len(self):
+                raise ValueError(f"Expected {len(self)} {prop}, got {len(values)}.")
+            if prop in self._properties:
+                self._properties[prop][:] = values
+            else:
+                self._on_mutate()
+                self._properties[prop] = values
+
+    def __getattr__(self, attr):
+        if not hasattr(self, "_properties"):
+            raise UnpicklingError("Branch class does not support pickling.")
+        if attr in self._properties:
+            return self._properties[attr]
+        else:
+            super().__getattribute__(attr)
+
+    def __copy__(self):
+        return self.copy()
+
+    def __bool__(self):
+        # Without this, empty branches are False, and `if branch.parent:` checks fail.
+        return True
+
+    def __eq__(self, other):
+        if isinstance(other, Branch):
+            return (
+                self.points.shape == other.points.shape
+                and np.allclose(self.points, other.points)
+                and self.labels == other.labels
+            )
+        else:
+            return np.allclose(self.points, other)
+
+    def __hash__(self):
+        return id(self)
+
+    @property
+    def parent(self):
+        return self._parent
+
+    @property
+    def size(self):
+        """
+        Returns the amount of points on this branch
+
+        :returns: Number of points on the branch.
+        :rtype: int
+        """
+        return len(self._points)
+
+    def __len__(self):
+        return self.size
+
+    @property
+    def points(self):
+        """
+        Return the spatial coordinates of the points on this branch.
+        """
+        return self._points
+
+    @points.setter
+    def points(self, value):
+        arr = np.array(value, copy=False, dtype=float)
+        if arr.shape == self._points.shape:
+            self._points[:] = arr
+        elif arr.ndim != 2 or arr.shape[1] != 3:
+            raise ValueError(f"Point data must have (N, 3) shape, {arr.shape} given.")
+        else:
+            self._points = arr
+
+    @property
+    def _kd_tree(self):
+        """
+        Return a `scipy.spatial.cKDTree` of this branch points for fast spatial queries.
+
+        .. warning::
+
+           Constructing a kd-tree takes time and should only be used for repeat queries.
+
+        """
+        import scipy.spatial
+
+        return scipy.spatial.cKDTree(self._points)
+
+    @property
+    def point_vectors(self):
+        """
+        Return the individual vectors between consecutive points on this branch.
+        """
+        return np.diff(self.points, axis=0)
+
+    @property
+    def segments(self):
+        """
+        Return the start and end points of vectors between consecutive points on this
+        branch.
+        """
+        return np.hstack(
+            (self.points[:-1], self.points[:-1] + self.point_vectors)
+        ).reshape(-1, 2, 3)
+
+    @property
+    def start(self):
+        """
+        Return the spatial coordinates of the starting point of this branch.
+        """
+        try:
+            return self._points[0]
+        except IndexError:
+            raise EmptyBranchError("Empty branch has no starting point") from None
+
+    @property
+    def end(self):
+        """
+        Return the spatial coordinates of the terminal point of this branch.
+        """
+        try:
+            return self._points[-1]
+        except IndexError:
+            raise EmptyBranchError("Empty branch has no ending point") from None
+
+    @property
+    def vector(self):
+        """
+        Return the vector of the axis connecting the start and terminal points.
+        """
+        try:
+            return self.end - self.start
+        except IndexError:
+            raise EmptyBranchError("Empty branch has no vector") from None
+
+    @property
+    def versor(self):
+        """
+        Return the normalized vector of the axis connecting the start and terminal points.
+        """
+        versor = (self.end - self.start) / np.linalg.norm(self.end - self.start)
+        if np.any(np.isnan(versor)):
+            raise EmptyBranchError("Empty and single-point branched have no versor")
+        else:
+            return versor
+
+    @property
+    def euclidean_dist(self):
+        """
+        Return the Euclidean distance from the start to the terminal point of this branch.
+        """
+        try:
+            return np.sqrt(np.sum((self.end - self.start) ** 2))
+        except IndexError:
+            raise EmptyBranchError("Empty branch has no Euclidean distance") from None
+
+    @property
+    def max_displacement(self):
+        """
+        Return the max displacement of the branch points from its axis vector.
+        """
+        try:
+            displacements = np.linalg.norm(
+                np.cross(self.versor, (self.points - self.start)), axis=1
+            )
+            return np.max(displacements)
+        except IndexError:
+            raise EmptyBranchError(
+                "Impossible to compute max_displacement in branches with 0 or 1 points."
+            ) from None
+
+    @property
+    def path_length(self):
+        """
+        Return the sum of the euclidean distances between the points on the branch.
+        """
+        return np.sum(np.sqrt(np.sum(self.point_vectors**2, axis=1)))
+
+    @property
+    def fractal_dim(self):
+        """
+        Return the fractal dimension of this branch, computed as the coefficient
+        of the line fitting the log-log plot of path vs euclidean distances of its points.
+        """
+        if len(self.points) == 0:
+            raise EmptyBranchError("Empty branch has no fractal dimension") from None
+        else:
+            euclidean = np.sqrt(np.sum((self.points - self.start) ** 2, axis=1))
+            path = np.cumsum(np.sqrt(np.sum(self.point_vectors**2, axis=1)))
+            log_e = np.log(euclidean[1:])
+            log_p = np.log(path)
+            if len(self.points) <= 2:
+                return 1.0
+            return np.polyfit(log_e, log_p, 1)[0]
+
+    @property
+    def radii(self):
+        """
+        Return the radii of the points on this branch.
+        """
+        return self._radii
+
+    @radii.setter
+    def radii(self, value):
+        arr = np.array(value, copy=False, dtype=float)
+        if arr.shape == self._radii.shape:
+            self._radii[:] = arr
+        else:
+            self._radii = arr.ravel()
+
+    @property
+    def labels(self):
+        """
+        Return the labels of the points on this branch. Labels are represented as a number
+        that is associated to a set of labels. See :ref:`morphology_labels` for more info.
+        """
+        return self._labels
+
+    @property
+    def labelsets(self):
+        """
+        Return the sets of labels associated to each numerical label.
+        """
+        return self._labels.labels
+
+    def list_labels(self):
+        """
+        Return a list of labels present on the branch.
+        """
+        lookup = np.vectorize(self._labels.labels.get)
+        labels = np.unique(lookup(self._labels.raw))
+        return sorted(set(_gutil.ichain(labels)))
+
+    @property
+    def is_root(self):
+        """
+        Returns whether this branch is root or if it has a parent.
+
+        :returns: True if this branch has no parent, False otherwise.
+        :rtype: bool
+        """
+        return not self._parent
+
+    @property
+    def is_terminal(self):
+        """
+        Returns whether this branch is terminal or if it has children.
+
+        :returns: True if this branch has no children, False otherwise.
+        :rtype: bool
+        """
+        return not self._children
+
+    def copy(self, branch_class=None):
+        """
+        Return a parentless and childless copy of the branch.
+
+        :param branch_class: Custom branch creation class
+        :type branch_class: type
+        :returns: A branch, or `branch_class` if given, without parents or children.
+        :rtype: bsb.morphologies.Branch
+        """
+        cls = branch_class or type(self)
+        props = {k: v.copy() for k, v in self._properties.items()}
+        return cls(self._points.copy(), self._radii.copy(), self._labels.copy(), props)
+
+    def label(self, labels, points=None):
+        """
+        Add labels to the branch.
+
+        :param labels: Label(s) for the branch
+        :type labels: List[str]
+        :param points: An integer or boolean mask to select the points to label.
+        """
+        if points is None:
+            points = np.ones(len(self), dtype=bool)
+        self._labels.label(labels, points)
+
+    @property
+    def children(self):
+        """
+        Collection of the child branches of this branch.
+
+        :returns: list of :class:`Branches <.morphologies.Branch>`
+        :rtype: list
+        """
+        return self._children.copy()
+
+    def attach_child(self, branch):
+        """
+        Attach a branch as a child to this branch.
+
+        :param branch: Child branch
+        :type branch: :class:`Branch <.morphologies.Branch>`
+        """
+        self._on_mutate()
+        if branch._parent is not None:
+            branch._parent.detach_child(branch)
+        self._children.append(branch)
+        branch._parent = self
+
+    def find_closest_point(self, coord):
+        """
+        Return the index of the closest on this branch to a desired coordinate.
+
+        :param coord: The coordinate to find the nearest point to
+        :type: :class:`numpy.ndarray`
+        """
+        diff = np.sqrt(np.sum((self._points - coord) ** 2, axis=1))
+        return np.argmin(diff)
+
+    def insert_branch(self, branch, index):
+        """
+        Split this branch and insert the given ``branch`` at the specified ``index``.
+
+        :param branch: Branch to be attached
+        :type branch: :class:`Branch <.morphologies.Branch>`
+        :param index: Index or coordinates of the cutpoint; if coordinates are given, the closest point to the coordinates is used.
+        :type: Union[:class:`numpy.ndarray`, int]
+        """
+        index = np.array(index, copy=False)
+        if index.ndim != 0:
+            index = self.find_closest_point(index)
+
+        if index < 0 or index >= len(self):
+            raise IndexError(
+                f"Cannot insert branch at cutpoint: index {index} is out of range ({len(self)})"
+            )
+
+        if index == len(self.points) - 1:
+            self.attach_child(branch)
+        elif index == 0:
+            self.parent.attach_child(branch)
+        else:
+            first_segment = Branch(
+                self._points.copy()[: index + 1],
+                self._radii.copy()[: index + 1],
+                self._labels.copy()[: index + 1],
+                {k: v.copy()[: index + 1] for k, v in self._properties.items()},
+            )
+            self.parent.attach_child(first_segment)
+            self.parent.detach_child(self)
+            first_segment.attach_child(branch)
+            second_segment = Branch(
+                self._points.copy()[index:],
+                self._radii.copy()[index:],
+                self._labels.copy()[index:],
+                {k: v.copy()[index:] for k, v in self._properties.items()},
+            )
+            for b in self.children:
+                self.detach_child(b)
+                second_segment.attach_child(b)
+            first_segment.attach_child(second_segment)
+
+    def detach(self):
+        """
+        Detach the branch from its parent, if one exists.
+        """
+        if self.parent:
+            self.parent.detach_child(self)
+
+    def detach_child(self, branch):
+        """
+        Remove a branch as a child from this branch.
+
+        :param branch: Child branch
+        :type branch: :class:`Branch <.morphologies.Branch>`
+        """
+        if branch._parent is not self:
+            raise ValueError(f"Can't detach {branch} from {self}, not a child branch.")
+        self._on_mutate()
+        self._children = [b for b in self._children if b is not branch]
+        branch._parent = None
+
+    def walk(self):
+        """
+        Iterate over the points in the branch.
+        """
+        return zip(
+            self.points[:, 0],
+            self.points[:, 1],
+            self.points[:, 2],
+            self.radii,
+            self.labels.walk(),
+            *self._properties.values(),
+        )
+
+    def contains_labels(self, labels):
+        """
+        Check if this branch contains any points labelled with any of the given labels.
+
+        :param labels: The labels to check for.
+        :type labels: List[str]
+        :rtype: bool
+        """
+        return self.labels.contains(labels)
+
+    def get_points_labelled(self, labels):
+        """
+        Filter out all points with certain labels
+
+        :param labels: The labels to check for.
+        :type labels: List[str] | numpy.ndarray[str]
+        :returns: All points with the labels.
+        :rtype: List[numpy.ndarray]
+        """
+        return self.points[self.get_label_mask(labels)]
+
+    def get_label_mask(self, labels):
+        """
+        Return a mask for the specified labels
+
+        :param labels: The labels to check for.
+        :type labels: List[str] | numpy.ndarray[str]
+        :returns: A boolean mask that selects out the points that match the label.
+        :rtype: List[numpy.ndarray]
+        """
+        return self.labels.get_mask(labels)
+
+    def introduce_point(self, index, *args, labels=None):
+        """
+        Insert a new point at ``index``, before the existing point at ``index``.
+
+        :param index: Index of the new point.
+        :type index: int
+        :param args: Vector coordinates of the new point
+        :type args: float
+        :param labels: The labels to assign to the point.
+        :type labels: list
+        """
+        self._on_mutate()
+        for v, vector_name in enumerate(type(self).vectors):
+            vector = getattr(self, vector_name)
+            new_vector = np.concatenate((vector[:index], [args[v]], vector[index:]))
+            setattr(self, vector_name, new_vector)
+        if labels is None:
+            labels = set()
+        for label, mask in self._label_masks.items():
+            has_label = label in labels
+            new_mask = np.concatenate((mask[:index], [has_label], mask[index:]))
+            self._label_masks[label] = new_mask
+
+    def introduce_arc_point(self, arc_val):
+        """
+        Introduce a new point at the given arc length.
+
+        :param arc_val: Arc length between 0 and 1 to introduce new point at.
+        :type arc_val: float
+        :returns: The index of the new point.
+        :rtype: int
+        """
+        arc = self.as_arc()
+        arc_point_floor = self.floor_arc_point(arc_val)
+        arc_point_ceil = self.ceil_arc_point(arc_val)
+        arc_floor = arc[arc_point_floor]
+        arc_ceil = arc[arc_point_ceil]
+        point_floor = self[arc_point_floor]
+        point_ceil = self[arc_point_ceil]
+        rem = (arc_val - arc_floor) / (arc_ceil - arc_floor)
+        new_point = (point_ceil - point_floor) * rem + point_floor
+        new_index = arc_point_floor + 1
+        self.introduce_point(new_index, *new_point)
+        return new_index
+
+    def get_arc_point(self, arc, eps=1e-10):
+        """
+        Strict search for an arc point within an epsilon.
+
+        :param arc: Arclength position to look for.
+        :type arc: float
+        :param eps: Maximum distance/tolerance to accept an arc point as a match.
+        :type eps: float
+        :returns: The matched arc point index, or ``None`` if no match is found
+        :rtype: Union[int, None]
+        """
+        arc_values = self.as_arc()
+        arc_match = (i for i, arc_p in enumerate(arc_values) if abs(arc_p - arc) < eps)
+        return next(arc_match, None)
+
+    def as_arc(self):
+        """
+        Return the branch as a vector of arclengths in the closed interval [0, 1]. An
+        arclength is the distance each point to the start of the branch along the branch
+        axis, normalized by total branch length. A point at the start will have an
+        arclength close to 0, and a point near the end an arclength close to 1
+
+        :returns: Vector of branch points as arclengths.
+        :rtype: :class:`numpy.ndarray`
+        """
+        arc_distances = np.sqrt(np.sum(np.diff(self.points, axis=0) ** 2, axis=1))
+        arc_length = np.sum(arc_distances)
+        return np.cumsum(np.concatenate(([0], arc_distances))) / arc_length
+
+    def floor_arc_point(self, arc):
+        """
+        Get the index of the nearest proximal arc point.
+        """
+        p = 0
+        for i, a in enumerate(self.as_arc()):
+            if a <= arc:
+                p = i
+            else:
+                break
+        return p
+
+    def ceil_arc_point(self, arc):
+        """
+        Get the index of the nearest distal arc point.
+        """
+        for i, a in enumerate(self.as_arc()):
+            if a >= arc:
+                return i
+        return len(self) - 1
+
+    def get_axial_distances(self, idx_start=0, idx_end=-1, return_max=False):
+        """
+        Return the displacements or its max value of a subset of branch points from its axis vector.
+        :param idx_start = 0: index of the first point of the subset.
+        :param idx_end = -1: index of the last point of the subset.
+        :param return_max = False: if True the function only returns the max value of displacements, otherwise the entire array.
+        """
+        start = self.points[idx_start]
+        end = self.points[idx_end]
+        versor = (end - start) / np.linalg.norm(end - start)
+        displacements = np.linalg.norm(
+            np.cross(
+                versor,
+                (self.points[idx_start : idx_end + 1] - self.points[idx_start]),
+            ),
+            axis=1,
+        )
+        if return_max:
+            try:
+                return np.max(displacements)
+            except IndexError:
+                raise EmptyBranchError("Selected an empty subset of points") from None
+        else:
+            return displacements
+
+    def delete_point(self, index):
+        """
+        Remove a point from the branch
+
+        :param int index: index position of the point to remove
+        :returns: the branch where the point has been removed
+        :rtype: bsb.morphologies.Branch
+        """
+        self._points = np.delete(self._points, index, axis=0)
+        self._labels = np.delete(self._labels, index, axis=0)
+        self._radii = np.delete(self._radii, index, axis=0)
+        for k, v in self._properties.items():
+            self._properties[k] = np.delete(v, index, axis=0)
+        return self
+
+    def simplify(self, epsilon, idx_start=0, idx_end=-1):
+        """
+        Apply RamerDouglasPeucker algorithm to all points or a subset of points of the branch.
+        :param epsilon: Epsilon to be used in the algorithm.
+        :param idx_start = 0: Index of the first element of the subset of points to be reduced.
+        :param epsilon = -1: Index of the last element of the subset of points to be reduced.
+        """
+        if len(self.points) < 3:
+            return
+        if idx_end == -1:
+            idx_end = len(self.points) - 1
+        if epsilon < 0:
+            raise ValueError(f"Epsilon must be >= 0")
+
+        reduced = []
+        skipped = deque()
+
+        while True:
+            dists = self.get_axial_distances(idx_start, idx_end)
+            try:
+                idx_max = np.argmax(dists)
+                dmax = dists[idx_max]
+                idx_max = idx_start + idx_max
+            except ValueError:
+                dmax = 0
+
+            reduced.append(idx_start)
+            reduced.append(idx_end)
+            if dmax > epsilon and len(dists) > 2:
+                skipped.append((idx_max, idx_end))
+                idx_end = idx_max - 1
+            else:
+                try:
+                    idx_start, idx_end = skipped.pop()
+                except IndexError:
+                    break
+
+        # sorted because indexes are appended to reduced from the middle of the list (the first point with dist > epsilon)
+        # then all points with smaller index  until 0, then all points with bigger index
+        reduced = np.sort(np.unique(reduced))
+        self.points = self.points[reduced]
+        self.radii = self.radii[reduced]
+
+    @functools.wraps(SubTree.cached_voxelize)
+    @functools.cache
+    def cached_voxelize(self, *args, **kwargs):
+        return SubTree([self]).voxelize(*args, **kwargs)
+
+
+def _morpho_to_swc(morpho):
+    # Initialize an empty data array
+    data = np.empty((len(morpho.points), 7), dtype=object)
+    swc_tags = {"soma": 1, "axon": 2, "dendrites": 3}
+    bmap = {}
+    nid = 0
+    offset = 0
+    # Convert labels to tags
+    if not hasattr(morpho, "tags"):
+        tags = np.full(len(morpho.points), -1, dtype=int)
+        for key in swc_tags.keys():
+            mask = morpho.get_label_mask([key])
+            tags[mask] = swc_tags[key]
+    else:
+        tags = morpho.tags
+    if np.any(tags == -1):
+        raise NotImplementedError("Can't store morphologies with custom SWC tags")
+    # Iterate over the morphology branches
+    for b in morpho.branches:
+        ids = (
+            np.arange(nid, nid + len(b) - 1)
+            if len(b) > 1
+            else np.arange(nid, nid + len(b))
+        )
+        samples = ids + 1
+        data[ids, 0] = samples
+        data[ids, 1] = tags[ids + offset]
+        data[ids, 2:5] = morpho.points[ids + offset]
+        try:
+            data[ids, 5] = morpho.radii[ids + offset]
+        except Exception as e:
+            raise MorphologyDataError(
+                f"Couldn't convert morphology radii to SWC: {e}."
+                " Note that SWC files cannot store multi-dimensional radii"
+            )
+        nid += len(b) - 1 if len(b) > 1 else len(b)
+        offset += 1
+        bmap[b] = ids[-1]
+        data[ids, 6] = ids
+        data[ids[0], 6] = -1 if b.parent is None else bmap[b.parent] + 1
+
+    return data[data != np.array(None)].reshape(-1, 7)
+
+
+__all__ = [
+    "Branch",
+    "Morphology",
+    "MorphologySet",
+    "RotationSet",
+    "SubTree",
+    "branch_iter",
+]
```

### Comparing `bsb_core-4.0.1/bsb/morphologies/parsers/parser.py` & `bsb_core-4.1.0/bsb/morphologies/parsers/parser.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,269 +1,269 @@
-import abc
-import itertools
-import typing
-from collections import deque
-from functools import reduce
-
-import morphio
-import numpy as np
-
-from ... import config
-from ..._encoding import EncodedLabels
-from ...config import types
-from .. import Morphology
-
-if typing.TYPE_CHECKING:
-    from ...storage._files import FileDependency
-
-
-@config.dynamic(attr_name="parser", auto_classmap=True, default="bsb")
-class MorphologyParser:
-    cls: type = config.attr(type=types.class_(), default="bsb.morphologies.Morphology")
-    branch_cls: type = config.attr(type=types.class_(), default="bsb.morphologies.Branch")
-
-    @abc.abstractmethod
-    def parse(self, file: typing.Union["FileDependency", str]) -> Morphology:
-        """Parse the morphology"""
-        pass
-
-
-@config.node
-class BsbParser(MorphologyParser, classmap_entry="bsb"):
-    tags: dict[typing.Union[str, list[str]]] = config.attr(
-        type=types.dict(type=types.or_(types.str(), types.list(str)))
-    )
-    """
-    Dictionary mapping SWC tags to sets of morphology labels.
-    """
-    skip_boundary_labels: list[str] = config.attr(type=types.list(str))
-    """
-    A set of labels that is used to create gaps in a morphology at certain boundaries.
-    No point will be inferred between a child branch of a branch labelled with the given
-    labels; usually used to skip points between the soma and its child branches.
-    """
-
-    def parse(self, file: typing.Union["FileDependency", str]):
-        from ...storage._files import FileDependency
-
-        if not isinstance(file, FileDependency):
-            file = FileDependency(file)
-        content, encoding = file.get_content(check_store=False)
-        return self.parse_content(content.decode(encoding or "utf8"))
-
-    def parse_content(self, content: str):
-        data = self._swc_parse(content)
-        return self._swc_data_to_morpho(data)
-
-    def _swc_parse(self, content: str):
-        try:
-            data = [
-                swc_data
-                for line in content.split("\n")
-                if not line.strip().startswith("#")
-                and (swc_data := [float(x) for x in line.split() if x != ""])
-            ]
-        except Exception:
-            raise RuntimeError(f"Could not parse SWC content")
-        err_lines = ", ".join(str(i) for i, d in enumerate(data) if len(d) != 7)
-        if err_lines:
-            raise ValueError(f"SWC incorrect on lines: {err_lines}")
-        return np.array(data)
-
-    def _swc_data_to_morpho(self, data):
-        data = np.array(data, copy=False)
-        tag_map = {1: "soma", 2: "axon", 3: "dendrites"}
-        if self.tags is not None:
-            tag_map.update((int(k), v) for (k, v) in self.tags.items())
-        # `data` is the raw SWC data, `samples` and `parents` are the graph nodes and edges.
-        samples = data[:, 0].astype(int)
-        # Map possibly irregular sample IDs (SWC spec allows this) to an ordered 0 to N map.
-        id_map = dict(zip(samples, itertools.count()))
-        id_map[-1] = -1
-        # Create an adjacency list of the graph described in the SWC data
-        adjacency = {n: [] for n in range(len(samples))}
-        adjacency[-1] = []
-        map_ids = np.vectorize(id_map.get)
-        parents = map_ids(data[:, 6])
-        for s, p in enumerate(parents):
-            adjacency[p].append(s)
-        # Now turn the adjacency list into a list of unbranching stretches of the graph.
-        # Call these `node_branches` because they only contain the sample/node ids.
-        node_branches = []
-        for root_node in adjacency[-1]:
-            self._swc_branch_dfs(adjacency, node_branches, root_node, data, tag_map)
-        branches = []
-        roots = []
-        _len = sum(len(s[1]) for s in node_branches)
-        points = np.empty((_len, 3))
-        radii = np.empty(_len)
-        tags = np.empty(_len, dtype=int)
-        labels = EncodedLabels.none(_len)
-        # Now turn each "node branch" into an actual branch by looking up the node data in the
-        # samples array. We copy over the node data into several contiguous matrices that will
-        # form the basis of the Morphology data structure.
-        ptr = 0
-        for parent, branch_nodes in node_branches:
-            node_data = data[branch_nodes]
-            nptr = ptr + len(node_data)
-            # Example with the points data matrix: copy over the swc data into contiguous arr
-            points[ptr:nptr] = node_data[:, 2:5]
-            # Then create a partial view into that data matrix for the branch
-            branch_points = points[ptr:nptr]
-            # Same here for radius,
-            radii[ptr:nptr] = node_data[:, 5]
-            branch_radii = radii[ptr:nptr]
-            # the SWC tags
-            tags[ptr:nptr] = node_data[:, 1]
-            if len(branch_nodes) > 1:
-                # Since we add an extra point we have to copy its tag from the next point.
-                tags[ptr] = tags[ptr + 1]
-            branch_tags = tags[ptr:nptr]
-            # And the labels
-            branch_labels = labels[ptr:nptr]
-            for v in np.unique(branch_tags):
-                u_tags = tag_map.get(v, f"tag_{v}")
-                branch_labels.label(
-                    [u_tags] if isinstance(u_tags, str) else u_tags, branch_tags == v
-                )
-            ptr = nptr
-            # Use the views to construct the branch
-            branch = self.branch_cls(branch_points, branch_radii, branch_labels)
-            branch.set_properties(tags=branch_tags)
-            branches.append(branch)
-            if parent is not None:
-                branches[parent].attach_child(branch)
-            else:
-                roots.append(branch)
-        # Then save the shared data matrices on the morphology
-        morpho = self.cls(roots, shared_buffers=(points, radii, labels, {"tags": tags}))
-        # And assert that this shared buffer mode succeeded
-        assert morpho._check_shared(), "SWC import didn't result in shareable buffers."
-        return morpho
-
-    def _swc_branch_dfs(self, adjacency, branches, node, data, tags):
-        boundaries = set()
-        if self.skip_boundary_labels:
-            tset = set(self.skip_boundary_labels)
-            for tag, labels in tags.items():
-                lset = set(labels if not isinstance(labels, str) else [labels])
-                if tset.issuperset(lset):
-                    boundaries.add(tag)
-        branch = []
-        branch_id = len(branches)
-        branches.append((None, branch))
-        node_stack = deque()
-        while True:
-            if node is not None:
-                # Append the current node to the current branch, and get its child nodes
-                branch.append(node)
-                child_nodes = adjacency[node]
-
-            if not child_nodes:
-                # No children, pop next branch
-                try:
-                    parent_bid, parent, node, skip = node_stack.pop()
-                except IndexError:
-                    # No next branch, we're done
-                    break
-                else:
-                    # Start the next branch
-                    branch = [] if skip else [parent]
-                    branch_id = len(branches)
-                    branches.append((parent_bid, branch))
-            elif len(child_nodes) == 1 and not (
-                data[node, 1] in boundaries and data[child_nodes[0], 1] not in boundaries
-            ):
-                # One child, and not a skipped boundary: grow the branch
-                node = child_nodes[0]
-            else:
-                # Branch point: create 1 new branch per child point
-                # If skip is False we add the current node to all the child branches.
-                skip = data[node, 1] in boundaries
-                node_stack.extend(
-                    (branch_id, node, child, skip) for child in reversed(child_nodes)
-                )
-                child_nodes = []
-                node = None
-
-
-# Wrapper to append our own attributes to morphio somas and treat it like any other branch
-class _MorphIoSomaWrapper:
-    def __init__(self, obj):
-        self._o = obj
-
-    def __getattr__(self, attr):
-        return getattr(self._o, attr)
-
-
-@config.node
-class MorphIOParser(MorphologyParser, classmap_entry="morphio"):
-    @config.property(type=types.list(type=types.in_(morphio.Option.__members__)))
-    def flags(self):
-        return getattr(self, "_flags", morphio.Option.no_modifier)
-
-    @flags.setter
-    def flags(self, values):
-        self._flags = reduce(
-            morphio.Option.__or__,
-            [getattr(morphio.Option, flag) for flag in values or []],
-            morphio.Option.no_modifier,
-        )
-
-    def parse(self, file: typing.Union["FileDependency", str]) -> Morphology:
-        from ...storage._files import FileDependency
-
-        if isinstance(file, str):
-            file = FileDependency(file)
-
-        with file.provide_locally() as (fp, encoding):
-            morpho_io = morphio.Morphology(fp, self.flags)
-        # We create shared buffers for the entire morphology, which optimize operations on the
-        # entire morphology such as `.flatten`, subtree transformations and IO.  The branches
-        # have views on those buffers, and as long as no points are added or removed, we can
-        # keep working in shared buffer mode.
-        soma = _MorphIoSomaWrapper(morpho_io.soma)
-        _len = len(morpho_io.points) + len(soma.points)
-        points = np.empty((_len, 3))
-        radii = np.empty(_len)
-        tags = np.empty(_len, dtype=int)
-        labels = EncodedLabels.none(_len)
-        soma.children = morpho_io.root_sections
-        section_stack = deque([(None, soma)])
-        branch = None
-        roots = []
-        ptr = 0
-        while True:
-            try:
-                parent, section = section_stack.pop()
-            except IndexError:
-                break
-            else:
-                nptr = ptr + len(section.points)
-                # Fill the branch data into the shared buffers and create views into them.
-                points[ptr:nptr] = section.points
-                branch_points = points[ptr:nptr]
-                radii[ptr:nptr] = section.diameters / 2
-                branch_radii = radii[ptr:nptr]
-                tags[ptr:nptr] = np.ones(len(section.points), dtype=int) * int(
-                    section.type
-                )
-                branch_tags = tags[ptr:nptr]
-                branch_labels = labels[ptr:nptr]
-                ptr = nptr
-                # Pass the shared buffer views to the branch
-                branch = self.branch_cls(branch_points, branch_radii, branch_labels)
-                branch.set_properties(tags=branch_tags)
-                if parent:
-                    parent.attach_child(branch)
-                else:
-                    roots.append(branch)
-                children = reversed([(branch, child) for child in section.children])
-                section_stack.extend(children)
-        morpho = self.cls(roots, shared_buffers=(points, radii, labels, {"tags": tags}))
-        assert (
-            morpho._check_shared()
-        ), "MorphIO import didn't result in shareable buffers."
-        return morpho
-
-
-__all__ = ["BsbParser", "MorphIOParser", "MorphologyParser"]
+import abc
+import itertools
+import typing
+from collections import deque
+from functools import reduce
+
+import morphio
+import numpy as np
+
+from ... import config
+from ..._encoding import EncodedLabels
+from ...config import types
+from .. import Morphology
+
+if typing.TYPE_CHECKING:
+    from ...storage._files import FileDependency
+
+
+@config.dynamic(attr_name="parser", auto_classmap=True, default="bsb")
+class MorphologyParser:
+    cls: type = config.attr(type=types.class_(), default="bsb.morphologies.Morphology")
+    branch_cls: type = config.attr(type=types.class_(), default="bsb.morphologies.Branch")
+
+    @abc.abstractmethod
+    def parse(self, file: typing.Union["FileDependency", str]) -> Morphology:
+        """Parse the morphology"""
+        pass
+
+
+@config.node
+class BsbParser(MorphologyParser, classmap_entry="bsb"):
+    tags: dict[typing.Union[str, list[str]]] = config.attr(
+        type=types.dict(type=types.or_(types.str(), types.list(str)))
+    )
+    """
+    Dictionary mapping SWC tags to sets of morphology labels.
+    """
+    skip_boundary_labels: list[str] = config.attr(type=types.list(str))
+    """
+    A set of labels that is used to create gaps in a morphology at certain boundaries.
+    No point will be inferred between a child branch of a branch labelled with the given
+    labels; usually used to skip points between the soma and its child branches.
+    """
+
+    def parse(self, file: typing.Union["FileDependency", str]):
+        from ...storage._files import FileDependency
+
+        if not isinstance(file, FileDependency):
+            file = FileDependency(file)
+        content, encoding = file.get_content(check_store=False)
+        return self.parse_content(content.decode(encoding or "utf8"))
+
+    def parse_content(self, content: str):
+        data = self._swc_parse(content)
+        return self._swc_data_to_morpho(data)
+
+    def _swc_parse(self, content: str):
+        try:
+            data = [
+                swc_data
+                for line in content.split("\n")
+                if not line.strip().startswith("#")
+                and (swc_data := [float(x) for x in line.split() if x != ""])
+            ]
+        except Exception:
+            raise RuntimeError(f"Could not parse SWC content")
+        err_lines = ", ".join(str(i) for i, d in enumerate(data) if len(d) != 7)
+        if err_lines:
+            raise ValueError(f"SWC incorrect on lines: {err_lines}")
+        return np.array(data)
+
+    def _swc_data_to_morpho(self, data):
+        data = np.array(data, copy=False)
+        tag_map = {1: "soma", 2: "axon", 3: "dendrites"}
+        if self.tags is not None:
+            tag_map.update((int(k), v) for (k, v) in self.tags.items())
+        # `data` is the raw SWC data, `samples` and `parents` are the graph nodes and edges.
+        samples = data[:, 0].astype(int)
+        # Map possibly irregular sample IDs (SWC spec allows this) to an ordered 0 to N map.
+        id_map = dict(zip(samples, itertools.count()))
+        id_map[-1] = -1
+        # Create an adjacency list of the graph described in the SWC data
+        adjacency = {n: [] for n in range(len(samples))}
+        adjacency[-1] = []
+        map_ids = np.vectorize(id_map.get)
+        parents = map_ids(data[:, 6])
+        for s, p in enumerate(parents):
+            adjacency[p].append(s)
+        # Now turn the adjacency list into a list of unbranching stretches of the graph.
+        # Call these `node_branches` because they only contain the sample/node ids.
+        node_branches = []
+        for root_node in adjacency[-1]:
+            self._swc_branch_dfs(adjacency, node_branches, root_node, data, tag_map)
+        branches = []
+        roots = []
+        _len = sum(len(s[1]) for s in node_branches)
+        points = np.empty((_len, 3))
+        radii = np.empty(_len)
+        tags = np.empty(_len, dtype=int)
+        labels = EncodedLabels.none(_len)
+        # Now turn each "node branch" into an actual branch by looking up the node data in the
+        # samples array. We copy over the node data into several contiguous matrices that will
+        # form the basis of the Morphology data structure.
+        ptr = 0
+        for parent, branch_nodes in node_branches:
+            node_data = data[branch_nodes]
+            nptr = ptr + len(node_data)
+            # Example with the points data matrix: copy over the swc data into contiguous arr
+            points[ptr:nptr] = node_data[:, 2:5]
+            # Then create a partial view into that data matrix for the branch
+            branch_points = points[ptr:nptr]
+            # Same here for radius,
+            radii[ptr:nptr] = node_data[:, 5]
+            branch_radii = radii[ptr:nptr]
+            # the SWC tags
+            tags[ptr:nptr] = node_data[:, 1]
+            if len(branch_nodes) > 1:
+                # Since we add an extra point we have to copy its tag from the next point.
+                tags[ptr] = tags[ptr + 1]
+            branch_tags = tags[ptr:nptr]
+            # And the labels
+            branch_labels = labels[ptr:nptr]
+            for v in np.unique(branch_tags):
+                u_tags = tag_map.get(v, f"tag_{v}")
+                branch_labels.label(
+                    [u_tags] if isinstance(u_tags, str) else u_tags, branch_tags == v
+                )
+            ptr = nptr
+            # Use the views to construct the branch
+            branch = self.branch_cls(branch_points, branch_radii, branch_labels)
+            branch.set_properties(tags=branch_tags)
+            branches.append(branch)
+            if parent is not None:
+                branches[parent].attach_child(branch)
+            else:
+                roots.append(branch)
+        # Then save the shared data matrices on the morphology
+        morpho = self.cls(roots, shared_buffers=(points, radii, labels, {"tags": tags}))
+        # And assert that this shared buffer mode succeeded
+        assert morpho._check_shared(), "SWC import didn't result in shareable buffers."
+        return morpho
+
+    def _swc_branch_dfs(self, adjacency, branches, node, data, tags):
+        boundaries = set()
+        if self.skip_boundary_labels:
+            tset = set(self.skip_boundary_labels)
+            for tag, labels in tags.items():
+                lset = set(labels if not isinstance(labels, str) else [labels])
+                if tset.issuperset(lset):
+                    boundaries.add(tag)
+        branch = []
+        branch_id = len(branches)
+        branches.append((None, branch))
+        node_stack = deque()
+        while True:
+            if node is not None:
+                # Append the current node to the current branch, and get its child nodes
+                branch.append(node)
+                child_nodes = adjacency[node]
+
+            if not child_nodes:
+                # No children, pop next branch
+                try:
+                    parent_bid, parent, node, skip = node_stack.pop()
+                except IndexError:
+                    # No next branch, we're done
+                    break
+                else:
+                    # Start the next branch
+                    branch = [] if skip else [parent]
+                    branch_id = len(branches)
+                    branches.append((parent_bid, branch))
+            elif len(child_nodes) == 1 and not (
+                data[node, 1] in boundaries and data[child_nodes[0], 1] not in boundaries
+            ):
+                # One child, and not a skipped boundary: grow the branch
+                node = child_nodes[0]
+            else:
+                # Branch point: create 1 new branch per child point
+                # If skip is False we add the current node to all the child branches.
+                skip = data[node, 1] in boundaries
+                node_stack.extend(
+                    (branch_id, node, child, skip) for child in reversed(child_nodes)
+                )
+                child_nodes = []
+                node = None
+
+
+# Wrapper to append our own attributes to morphio somas and treat it like any other branch
+class _MorphIoSomaWrapper:
+    def __init__(self, obj):
+        self._o = obj
+
+    def __getattr__(self, attr):
+        return getattr(self._o, attr)
+
+
+@config.node
+class MorphIOParser(MorphologyParser, classmap_entry="morphio"):
+    @config.property(type=types.list(type=types.in_(morphio.Option.__members__)))
+    def flags(self):
+        return getattr(self, "_flags", morphio.Option.no_modifier)
+
+    @flags.setter
+    def flags(self, values):
+        self._flags = reduce(
+            morphio.Option.__or__,
+            [getattr(morphio.Option, flag) for flag in values or []],
+            morphio.Option.no_modifier,
+        )
+
+    def parse(self, file: typing.Union["FileDependency", str]) -> Morphology:
+        from ...storage._files import FileDependency
+
+        if isinstance(file, str):
+            file = FileDependency(file)
+
+        with file.provide_locally() as (fp, encoding):
+            morpho_io = morphio.Morphology(fp, self.flags)
+        # We create shared buffers for the entire morphology, which optimize operations on the
+        # entire morphology such as `.flatten`, subtree transformations and IO.  The branches
+        # have views on those buffers, and as long as no points are added or removed, we can
+        # keep working in shared buffer mode.
+        soma = _MorphIoSomaWrapper(morpho_io.soma)
+        _len = len(morpho_io.points) + len(soma.points)
+        points = np.empty((_len, 3))
+        radii = np.empty(_len)
+        tags = np.empty(_len, dtype=int)
+        labels = EncodedLabels.none(_len)
+        soma.children = morpho_io.root_sections
+        section_stack = deque([(None, soma)])
+        branch = None
+        roots = []
+        ptr = 0
+        while True:
+            try:
+                parent, section = section_stack.pop()
+            except IndexError:
+                break
+            else:
+                nptr = ptr + len(section.points)
+                # Fill the branch data into the shared buffers and create views into them.
+                points[ptr:nptr] = section.points
+                branch_points = points[ptr:nptr]
+                radii[ptr:nptr] = section.diameters / 2
+                branch_radii = radii[ptr:nptr]
+                tags[ptr:nptr] = np.ones(len(section.points), dtype=int) * int(
+                    section.type
+                )
+                branch_tags = tags[ptr:nptr]
+                branch_labels = labels[ptr:nptr]
+                ptr = nptr
+                # Pass the shared buffer views to the branch
+                branch = self.branch_cls(branch_points, branch_radii, branch_labels)
+                branch.set_properties(tags=branch_tags)
+                if parent:
+                    parent.attach_child(branch)
+                else:
+                    roots.append(branch)
+                children = reversed([(branch, child) for child in section.children])
+                section_stack.extend(children)
+        morpho = self.cls(roots, shared_buffers=(points, radii, labels, {"tags": tags}))
+        assert (
+            morpho._check_shared()
+        ), "MorphIO import didn't result in shareable buffers."
+        return morpho
+
+
+__all__ = ["BsbParser", "MorphIOParser", "MorphologyParser"]
```

### Comparing `bsb_core-4.0.1/bsb/morphologies/selector.py` & `bsb_core-4.1.0/bsb/morphologies/selector.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,158 +1,158 @@
-import abc
-import concurrent
-import re
-import tempfile
-import typing
-import urllib
-import warnings
-from concurrent.futures import ThreadPoolExecutor
-
-import requests
-
-from .. import config
-from ..config import types
-from ..config._attrs import cfglist
-from ..exceptions import MissingMorphologyError, SelectorError
-from ..services import MPI
-from . import Morphology
-
-if typing.TYPE_CHECKING:
-    from ..core import Scaffold
-
-
-@config.dynamic(
-    attr_name="select",
-    auto_classmap=True,
-    required=False,
-    default="by_name",
-)
-class MorphologySelector(abc.ABC):
-    scaffold: "Scaffold"
-
-    @abc.abstractmethod
-    def validate(self, all_morphos):
-        pass
-
-    @abc.abstractmethod
-    def pick(self, morphology):
-        pass
-
-
-@config.node
-class NameSelector(MorphologySelector, classmap_entry="by_name"):
-    names: cfglist[str] = config.list(type=str, required=types.shortform())
-
-    def __init__(self, name=None, /, **kwargs):
-        if name is not None:
-            self.names = [name]
-
-    def __inv__(self):
-        if self._config_pos_init:
-            return self.names[0]
-        return self.__tree__()
-
-    def _cache_patterns(self):
-        self._pnames = {n: n.replace("*", r".*").replace("|", "\\|") for n in self.names}
-        self._patterns = {n: re.compile(f"^{pat}$") for n, pat in self._pnames.items()}
-        self._empty = not self.names
-        self._match = re.compile(f"^({'|'.join(self._pnames.values())})$")
-
-    def validate(self, all_morphos):
-        self._cache_patterns()
-        repo_names = {m.get_meta()["name"] for m in all_morphos}
-        missing = [
-            n
-            for n, pat in self._patterns.items()
-            if not any(pat.match(rn) for rn in repo_names)
-        ]
-        if missing:
-            err = "Morphology repository misses the following morphologies"
-            if self._config_parent is not None:
-                node = self._config_parent._config_parent
-                err += f" required by {node.get_node_name()}"
-            err += f": {', '.join(missing)}"
-            raise MissingMorphologyError(err)
-
-    def pick(self, morphology):
-        self._cache_patterns()
-        return (
-            not self._empty
-            and self._match.match(morphology.get_meta()["name"]) is not None
-        )
-
-
-@config.node
-class NeuroMorphoSelector(NameSelector, classmap_entry="from_neuromorpho"):
-    _url = "https://neuromorpho.org/"
-    _meta = "api/neuron/select?q=neuron_name:"
-    _files = "dableFiles/"
-
-    def __boot__(self):
-        if self.scaffold.is_main_process():
-            try:
-                morphos = self._scrape_nm(self.names)
-            except:
-                MPI.barrier()
-                raise
-            for name, morpho in morphos.items():
-                self.scaffold.morphologies.save(name, morpho, overwrite=True)
-        MPI.barrier()
-
-    @classmethod
-    def _swc_url(cls, archive, name):
-        return f"{cls._url}{cls._files}{urllib.parse.quote(archive.lower())}/CNG%20version/{name}.CNG.swc"
-
-    @classmethod
-    def _scrape_nm(cls, names):
-        # Weak DH key on neuromorpho.org
-        # https://stackoverflow.com/questions/38015537/python-requests-exceptions-sslerror-dh-key-too-small
-        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ":HIGH:!DH:!aNULL"
-        try:
-            requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += (
-                ":HIGH:!DH:!aNULL"
-            )
-        except AttributeError:
-            # no pyopenssl support used / needed / available
-            pass
-        with warnings.catch_warnings():
-            warnings.simplefilter("ignore")
-            with ThreadPoolExecutor() as executor:
-                # Certificate issues with neuromorpho --> verify=False
-                res = requests.get(cls._url + cls._meta + ",".join(names), verify=False)
-                if res.status_code == 404:
-                    raise SelectorError(f"'{names[0]}' is not a valid NeuroMorpho name.")
-                elif res.status_code != 200:
-                    raise SelectorError("NeuroMorpho API error: " + res.message)
-                metas = {n: None for n in names}
-                for meta in res.json()["_embedded"]["neuronResources"]:
-                    del meta["_links"]
-                    metas[meta["neuron_name"]] = meta
-                missing = [name for name, meta in metas.items() if meta is None]
-                if missing:
-                    raise SelectorError(
-                        ", ".join(f"'{n}'" for n in missing)
-                        + " are not valid NeuroMorpho names."
-                    )
-                swc_urls = {n: cls._swc_url(metas[n]["archive"], n) for n in names}
-                req = lambda n: requests.get(swc_urls[n], verify=False)
-                sub = lambda n: (executor.submit(req, n), n)
-                futures = dict(map(sub, names))
-                morphos = {n: None for n in names}
-                with tempfile.TemporaryDirectory() as tempdir:
-                    for future in concurrent.futures.as_completed(futures.keys()):
-                        name = futures[future]
-                        path = tempdir + f"/{name}.swc"
-                        with open(path, "w") as f:
-                            f.write(future.result().text)
-                        morphos[name] = Morphology.from_swc(path, meta=metas[name])
-                missing = [name for name, m in morphos.items() if m is None]
-                if missing:  # pragma: nocover
-                    raise SelectorError(
-                        "Downloading NeuroMorpho failed for "
-                        + ", ".join(f"'{n}'" for n in missing)
-                        + "."
-                    )
-                return morphos
-
-
-__all__ = ["MorphologySelector", "NameSelector", "NeuroMorphoSelector"]
+import abc
+import concurrent
+import re
+import tempfile
+import typing
+import urllib
+import warnings
+from concurrent.futures import ThreadPoolExecutor
+
+import requests
+
+from .. import config
+from ..config import types
+from ..config._attrs import cfglist
+from ..exceptions import MissingMorphologyError, SelectorError
+from ..services import MPI
+from . import Morphology
+
+if typing.TYPE_CHECKING:
+    from ..core import Scaffold
+
+
+@config.dynamic(
+    attr_name="select",
+    auto_classmap=True,
+    required=False,
+    default="by_name",
+)
+class MorphologySelector(abc.ABC):
+    scaffold: "Scaffold"
+
+    @abc.abstractmethod
+    def validate(self, all_morphos):
+        pass
+
+    @abc.abstractmethod
+    def pick(self, morphology):
+        pass
+
+
+@config.node
+class NameSelector(MorphologySelector, classmap_entry="by_name"):
+    names: cfglist[str] = config.list(type=str, required=types.shortform())
+
+    def __init__(self, name=None, /, **kwargs):
+        if name is not None:
+            self.names = [name]
+
+    def __inv__(self):
+        if self._config_pos_init:
+            return self.names[0]
+        return self.__tree__()
+
+    def _cache_patterns(self):
+        self._pnames = {n: n.replace("*", r".*").replace("|", "\\|") for n in self.names}
+        self._patterns = {n: re.compile(f"^{pat}$") for n, pat in self._pnames.items()}
+        self._empty = not self.names
+        self._match = re.compile(f"^({'|'.join(self._pnames.values())})$")
+
+    def validate(self, all_morphos):
+        self._cache_patterns()
+        repo_names = {m.get_meta()["name"] for m in all_morphos}
+        missing = [
+            n
+            for n, pat in self._patterns.items()
+            if not any(pat.match(rn) for rn in repo_names)
+        ]
+        if missing:
+            err = "Morphology repository misses the following morphologies"
+            if self._config_parent is not None:
+                node = self._config_parent._config_parent
+                err += f" required by {node.get_node_name()}"
+            err += f": {', '.join(missing)}"
+            raise MissingMorphologyError(err)
+
+    def pick(self, morphology):
+        self._cache_patterns()
+        return (
+            not self._empty
+            and self._match.match(morphology.get_meta()["name"]) is not None
+        )
+
+
+@config.node
+class NeuroMorphoSelector(NameSelector, classmap_entry="from_neuromorpho"):
+    _url = "https://neuromorpho.org/"
+    _meta = "api/neuron/select?q=neuron_name:"
+    _files = "dableFiles/"
+
+    def __boot__(self):
+        if self.scaffold.is_main_process():
+            try:
+                morphos = self._scrape_nm(self.names)
+            except:
+                MPI.barrier()
+                raise
+            for name, morpho in morphos.items():
+                self.scaffold.morphologies.save(name, morpho, overwrite=True)
+        MPI.barrier()
+
+    @classmethod
+    def _swc_url(cls, archive, name):
+        return f"{cls._url}{cls._files}{urllib.parse.quote(archive.lower())}/CNG%20version/{name}.CNG.swc"
+
+    @classmethod
+    def _scrape_nm(cls, names):
+        # Weak DH key on neuromorpho.org
+        # https://stackoverflow.com/questions/38015537/python-requests-exceptions-sslerror-dh-key-too-small
+        requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ":HIGH:!DH:!aNULL"
+        try:
+            requests.packages.urllib3.contrib.pyopenssl.util.ssl_.DEFAULT_CIPHERS += (
+                ":HIGH:!DH:!aNULL"
+            )
+        except AttributeError:
+            # no pyopenssl support used / needed / available
+            pass
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            with ThreadPoolExecutor() as executor:
+                # Certificate issues with neuromorpho --> verify=False
+                res = requests.get(cls._url + cls._meta + ",".join(names), verify=False)
+                if res.status_code == 404:
+                    raise SelectorError(f"'{names[0]}' is not a valid NeuroMorpho name.")
+                elif res.status_code != 200:
+                    raise SelectorError("NeuroMorpho API error: " + res.message)
+                metas = {n: None for n in names}
+                for meta in res.json()["_embedded"]["neuronResources"]:
+                    del meta["_links"]
+                    metas[meta["neuron_name"]] = meta
+                missing = [name for name, meta in metas.items() if meta is None]
+                if missing:
+                    raise SelectorError(
+                        ", ".join(f"'{n}'" for n in missing)
+                        + " are not valid NeuroMorpho names."
+                    )
+                swc_urls = {n: cls._swc_url(metas[n]["archive"], n) for n in names}
+                req = lambda n: requests.get(swc_urls[n], verify=False)
+                sub = lambda n: (executor.submit(req, n), n)
+                futures = dict(map(sub, names))
+                morphos = {n: None for n in names}
+                with tempfile.TemporaryDirectory() as tempdir:
+                    for future in concurrent.futures.as_completed(futures.keys()):
+                        name = futures[future]
+                        path = tempdir + f"/{name}.swc"
+                        with open(path, "w") as f:
+                            f.write(future.result().text)
+                        morphos[name] = Morphology.from_swc(path, meta=metas[name])
+                missing = [name for name, m in morphos.items() if m is None]
+                if missing:  # pragma: nocover
+                    raise SelectorError(
+                        "Downloading NeuroMorpho failed for "
+                        + ", ".join(f"'{n}'" for n in missing)
+                        + "."
+                    )
+                return morphos
+
+
+__all__ = ["MorphologySelector", "NameSelector", "NeuroMorphoSelector"]
```

### Comparing `bsb_core-4.0.1/bsb/option.py` & `bsb_core-4.1.0/bsb/option.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,410 +1,410 @@
-"""
-This module contains the classes required to construct options.
-"""
-
-import argparse
-import functools
-import os
-import pathlib
-
-import toml
-
-from .exceptions import OptionError
-from .reporting import warn
-
-
-class OptionDescriptor:
-    """
-    Base option property descriptor. Can be inherited from to create a cascading property
-    such as the default CLI, env & script descriptors.
-    """
-
-    def __init_subclass__(cls, slug=None, **kwargs):
-        super().__init_subclass__(**kwargs)
-        cls.slug = slug
-
-    def __init__(self, *tags):
-        self.tags = tags
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        return getattr(instance, f"_bsbopt_{self.slug}_value", instance.get_default())
-
-    def __set__(self, instance, value):
-        set_value = getattr(instance, "setter", lambda x: x)(value)
-        setattr(instance, f"_bsbopt_{self.slug}_value", set_value)
-
-    def __delete__(self, instance):
-        try:
-            delattr(instance, f"_bsbopt_{self.slug}_value")
-        except AttributeError:
-            pass
-
-    def is_set(self, instance):
-        return hasattr(instance, f"_bsbopt_{self.slug}_value")
-
-
-class CLIOptionDescriptor(OptionDescriptor, slug="cli"):
-    """
-    Descriptor that retrieves its value from the given CLI command arguments.
-    """
-
-    pass
-
-
-class EnvOptionDescriptor(OptionDescriptor, slug="env"):
-    """
-    Descriptor that retrieves its value from the environment variables.
-    """
-
-    def __init__(self, *args, flag=False):
-        super().__init__(*args)
-        self.flag = flag
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        getter = getattr(instance, "getter", lambda x: x)
-        # Iterate the env for all tags, if none are set this returns `None`
-        for tag in self.tags:
-            if tag in os.environ:
-                return getter(self._parse(os.environ[tag]))
-
-    def __set__(self, instance, value):
-        value = getattr(instance, "setter", lambda x: x)(value)
-        parsed = self._rev_parse(value)
-        for tag in self.tags:
-            os.environ[tag] = parsed
-
-    def __delete__(self, instance):
-        for tag in self.tags:
-            try:
-                del os.environ[tag]
-            except KeyError:
-                pass
-
-    def is_set(self, instance):
-        return any(tag in os.environ for tag in self.tags)
-
-    def _parse(self, value):
-        if self.flag:
-            if value is True or str(value).strip().upper() in ("ON", "TRUE", "1", "YES"):
-                return True
-            else:
-                return False
-        else:
-            return value
-
-    def _rev_parse(self, value):
-        if self.flag:
-            return "ON" if value else "OFF"
-        else:
-            return str(value)
-
-
-class ScriptOptionDescriptor(OptionDescriptor, slug="script"):
-    """
-    Descriptor that retrieves and sets its value from/to the :mod:`bsb.options` module.
-    """
-
-    # This class uses `self.tags[0]`, because all tags are aliases of eachother in the
-    # options module.
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        if not self.tags:
-            # This option has no options module binding
-            return None
-        from .options import get_module_option
-
-        return get_module_option(self.tags[0])
-
-    def __set__(self, instance, value):
-        if not self.tags:
-            # This option has no options module binding
-            raise OptionError(f"{instance.name} can't be set through `bsb.options`.")
-        from .options import set_module_option
-
-        set_value = getattr(instance, "setter", lambda x: x)(value)
-        return set_module_option(self.tags[0], set_value)
-
-    def __delete__(self, instance):
-        from .options import reset_module_option
-
-        for tag in self.tags:
-            reset_module_option(tag)
-
-    def is_set(self, instance):
-        from .options import is_module_option_set
-
-        return any(is_module_option_set(tag) for tag in self.tags)
-
-
-class ProjectOptionDescriptor(OptionDescriptor, slug="project"):
-    """
-    Descriptor that retrieves and stores values in the `pyproject.toml` file. Traverses
-    up the filesystem tree until one is found.
-    """
-
-    def __init__(self, *tags):
-        if len(tags) > 1:  # pragma: nocover
-            raise OptionError(f"Project option can have only 1 tag, got {tags}.")
-        super().__init__(*(tags[0].split(".") if tags else ()))
-
-    def __get__(self, instance, owner):
-        if instance is None:
-            return self
-        if self.tags:
-            _, proj = _pyproject_bsb()
-            for tag in self.tags[:-1]:
-                proj = proj.get(tag, None)
-                if proj is None:
-                    return None
-            return proj.get(self.tags[-1], None)
-
-    def __set__(self, instance, value):
-        if self.tags:
-            path, proj = _pyproject_bsb()
-            deeper = proj
-            for tag in self.tags[:-1]:
-                deeper = deeper.setdefault(tag, {})
-            deeper[self.tags[-1]] = value
-            _save_pyproject_bsb(proj)
-
-    def __delete__(self, instance):
-        if self.tags:
-            path, proj = _pyproject_bsb()
-            for tag in self.tags[:-1]:
-                proj = proj.get(tag, None)
-                if proj is None:
-                    return None
-            try:
-                del proj[self.tags[-1]]
-            except KeyError:
-                pass
-            else:
-                _save_pyproject_bsb(proj)
-
-    def is_set(self, instance):
-        if self.tags:
-            _, proj = _pyproject_bsb()
-            for tag in self.tags[:-1]:
-                proj = proj.get(tag, None)
-                if proj is None:
-                    return False
-            return self.tags[-1] in proj
-        else:
-            return False
-
-
-class BsbOption:
-    """
-    Base option class. Can be subclassed to create new options.
-    """
-
-    def __init__(self, positional=False):
-        self.positional = positional
-
-    def __init_subclass__(
-        cls,
-        name=None,
-        env=(),
-        project=(),
-        cli=(),
-        script=(),
-        description=None,
-        flag=False,
-        inverted=False,
-        list=False,
-        readonly=False,
-        action=False,
-    ):
-        """
-        Subclass hook that defines the characteristics of the subclassed option class.
-
-        :param name: Unique name for identification
-        :type name: str
-        :param cli: Positional arguments for the :class:`.CLIOptionDescriptor` constructor.
-        :type cli: iterable
-        :param cli: Positional arguments for the :class:`.CLIOptionDescriptor` constructor.
-        :type cli: iterable
-        :param env: Positional arguments for the :class:`.EnvOptionDescriptor` constructor.
-        :type env: iterable
-        :param script: Positional arguments for the :class:`.ScriptOptionDescriptor` constructor.
-        :type script: iterable
-        :param description: Description of the option's purpose for the user.
-        :type description: str
-        :param flag: Indicates that the option is a flag and should toggle on a default off boolean when given.
-        :type flag: boolean
-        :param inverted: Used only for flags. Indicates that the flag is default on and is toggled off when given.
-        :param list: Indicates that the option takes multiple values.
-        :type list: boolean
-        :param readonly: Indicates that an option can be accessed but not be altered from the ``bsb.options`` module.
-        :type readonly: boolean
-        :param action: Indicates that the option should execute its ``action`` method.
-        :type action: boolean
-        """
-        if name is None:
-            raise OptionError("Options must be given a name in the class argument list.")
-        cls.name = name
-        cls.env = EnvOptionDescriptor(*env, flag=flag)
-        cls.project = ProjectOptionDescriptor(*project)
-        cls.cli = CLIOptionDescriptor(*cli)
-        cls.script = ScriptOptionDescriptor(*script)
-        cls.description = description
-        cls.is_flag = flag
-        cls.inverted_flag = inverted
-        cls.use_extend = list
-        cls.readonly = readonly
-        cls.use_action = action
-        cls.positional = False
-
-    def get(self, prio=None):
-        """
-        Get the option's value. Cascades the script, cli, env & default descriptors together.
-
-        :returns: option value
-        """
-        try:
-            if prio is not None:
-                return getattr(self, prio)
-
-            cls = self.__class__
-            if cls.script.is_set(self):
-                return self.script
-            if cls.cli.is_set(self):
-                return self.cli
-            if cls.project.is_set(self):
-                return self.project
-            if cls.env.is_set(self):
-                return self.env
-            return self.get_default()
-        except Exception as e:
-            warn(f"Error retrieving option '{self.name}'.", log_exc=e)
-            return self.get_default()
-
-    def is_set(self, slug):
-        if descriptor := getattr(type(self), slug, None):
-            return descriptor.is_set(self)
-        else:
-            return False
-
-    def get_default(self):
-        """
-        Override to specify the default value of the option.
-        """
-        return None
-
-    def get_cli_tags(self):
-        """
-        Return the ``argparse`` positional arguments from the tags.
-
-        :returns: ``-x`` or ``--xxx`` for each CLI tag.
-        :rtype: list
-        """
-        if self.positional:
-            longest = ""
-            for t in type(self).cli.tags:
-                if len(t) >= len(longest):
-                    longest = t
-            return [longest]
-        else:
-            return [("--" if len(t) != 1 else "-") + t for t in type(self).cli.tags]
-
-    def add_to_parser(self, parser, level):
-        """
-        Register this option into an ``argparse`` parser.
-        """
-        if not self.get_cli_tags():
-            return
-
-        kwargs = {}
-        kwargs["help"] = self.description
-        kwargs["dest"] = level * "_" + self.name
-        kwargs["action"] = "store"
-        if self.positional:
-            kwargs["nargs"] = "?"
-            kwargs["metavar"] = self.get_cli_tags()
-            args = []
-        else:
-            args = self.get_cli_tags()
-        if self.is_flag:
-            kwargs["action"] += "_false" if self.inverted_flag else "_true"
-            kwargs["default"] = argparse.SUPPRESS
-        if self.use_extend:
-            kwargs["action"] = "extend"
-            kwargs["nargs"] = "+"
-        if self.use_action:
-            kwargs["dest"] = "internal_action_list"
-            kwargs["action"] = "append_const"
-            kwargs["const"] = self.action
-
-        parser.add_argument(*args, **kwargs)
-
-    @classmethod
-    def register(cls):
-        """
-        Register this option class into the :mod:`bsb.options` module.
-        """
-        from . import options
-
-        opt = cls()
-        options.register_option(cls.name, opt)
-        return opt
-
-    def unregister(self):
-        """
-        Remove this option class from the :mod:`bsb.options` module, not part of the
-        public API as removing options is undefined behavior but useful for testing.
-        """
-        from . import options
-
-        options.unregister_option(self)
-
-
-@functools.cache
-def _pyproject_path():
-    path = pathlib.Path.cwd()
-    while str(path)[len(path.drive) :] != path.root:
-        proj = path / "pyproject.toml"
-        if proj.exists():
-            return proj
-        path = path.parent
-
-
-def _pyproject_content():
-    path = _pyproject_path()
-    if path:
-        with open(path, "r") as f:
-            return path.resolve(), toml.load(f)
-    else:
-        return None, {}  # pragma: nocover
-
-
-def _pyproject_bsb():
-    path, content = _pyproject_content()
-    return path, content.get("tools", {}).get("bsb", {})
-
-
-def _save_pyproject_bsb(project):
-    path, content = _pyproject_content()
-    if path is None:
-        raise OptionError(
-            "No 'pyproject.toml' in current dir or parents,"
-            + " can't set project settings."
-        )
-    content.setdefault("tools", {})["bsb"] = project
-    with open(path, "w") as f:
-        toml.dump(content, f)
-
-
-__all__ = [
-    "BsbOption",
-    "CLIOptionDescriptor",
-    "EnvOptionDescriptor",
-    "OptionDescriptor",
-    "ProjectOptionDescriptor",
-    "ScriptOptionDescriptor",
-]
+"""
+This module contains the classes required to construct options.
+"""
+
+import argparse
+import functools
+import os
+import pathlib
+
+import toml
+
+from .exceptions import OptionError
+from .reporting import warn
+
+
+class OptionDescriptor:
+    """
+    Base option property descriptor. Can be inherited from to create a cascading property
+    such as the default CLI, env & script descriptors.
+    """
+
+    def __init_subclass__(cls, slug=None, **kwargs):
+        super().__init_subclass__(**kwargs)
+        cls.slug = slug
+
+    def __init__(self, *tags):
+        self.tags = tags
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        return getattr(instance, f"_bsbopt_{self.slug}_value", instance.get_default())
+
+    def __set__(self, instance, value):
+        set_value = getattr(instance, "setter", lambda x: x)(value)
+        setattr(instance, f"_bsbopt_{self.slug}_value", set_value)
+
+    def __delete__(self, instance):
+        try:
+            delattr(instance, f"_bsbopt_{self.slug}_value")
+        except AttributeError:
+            pass
+
+    def is_set(self, instance):
+        return hasattr(instance, f"_bsbopt_{self.slug}_value")
+
+
+class CLIOptionDescriptor(OptionDescriptor, slug="cli"):
+    """
+    Descriptor that retrieves its value from the given CLI command arguments.
+    """
+
+    pass
+
+
+class EnvOptionDescriptor(OptionDescriptor, slug="env"):
+    """
+    Descriptor that retrieves its value from the environment variables.
+    """
+
+    def __init__(self, *args, flag=False):
+        super().__init__(*args)
+        self.flag = flag
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        getter = getattr(instance, "getter", lambda x: x)
+        # Iterate the env for all tags, if none are set this returns `None`
+        for tag in self.tags:
+            if tag in os.environ:
+                return getter(self._parse(os.environ[tag]))
+
+    def __set__(self, instance, value):
+        value = getattr(instance, "setter", lambda x: x)(value)
+        parsed = self._rev_parse(value)
+        for tag in self.tags:
+            os.environ[tag] = parsed
+
+    def __delete__(self, instance):
+        for tag in self.tags:
+            try:
+                del os.environ[tag]
+            except KeyError:
+                pass
+
+    def is_set(self, instance):
+        return any(tag in os.environ for tag in self.tags)
+
+    def _parse(self, value):
+        if self.flag:
+            if value is True or str(value).strip().upper() in ("ON", "TRUE", "1", "YES"):
+                return True
+            else:
+                return False
+        else:
+            return value
+
+    def _rev_parse(self, value):
+        if self.flag:
+            return "ON" if value else "OFF"
+        else:
+            return str(value)
+
+
+class ScriptOptionDescriptor(OptionDescriptor, slug="script"):
+    """
+    Descriptor that retrieves and sets its value from/to the :mod:`bsb.options` module.
+    """
+
+    # This class uses `self.tags[0]`, because all tags are aliases of eachother in the
+    # options module.
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        if not self.tags:
+            # This option has no options module binding
+            return None
+        from .options import get_module_option
+
+        return get_module_option(self.tags[0])
+
+    def __set__(self, instance, value):
+        if not self.tags:
+            # This option has no options module binding
+            raise OptionError(f"{instance.name} can't be set through `bsb.options`.")
+        from .options import set_module_option
+
+        set_value = getattr(instance, "setter", lambda x: x)(value)
+        return set_module_option(self.tags[0], set_value)
+
+    def __delete__(self, instance):
+        from .options import reset_module_option
+
+        for tag in self.tags:
+            reset_module_option(tag)
+
+    def is_set(self, instance):
+        from .options import is_module_option_set
+
+        return any(is_module_option_set(tag) for tag in self.tags)
+
+
+class ProjectOptionDescriptor(OptionDescriptor, slug="project"):
+    """
+    Descriptor that retrieves and stores values in the `pyproject.toml` file. Traverses
+    up the filesystem tree until one is found.
+    """
+
+    def __init__(self, *tags):
+        if len(tags) > 1:  # pragma: nocover
+            raise OptionError(f"Project option can have only 1 tag, got {tags}.")
+        super().__init__(*(tags[0].split(".") if tags else ()))
+
+    def __get__(self, instance, owner):
+        if instance is None:
+            return self
+        if self.tags:
+            _, proj = _pyproject_bsb()
+            for tag in self.tags[:-1]:
+                proj = proj.get(tag, None)
+                if proj is None:
+                    return None
+            return proj.get(self.tags[-1], None)
+
+    def __set__(self, instance, value):
+        if self.tags:
+            path, proj = _pyproject_bsb()
+            deeper = proj
+            for tag in self.tags[:-1]:
+                deeper = deeper.setdefault(tag, {})
+            deeper[self.tags[-1]] = value
+            _save_pyproject_bsb(proj)
+
+    def __delete__(self, instance):
+        if self.tags:
+            path, proj = _pyproject_bsb()
+            for tag in self.tags[:-1]:
+                proj = proj.get(tag, None)
+                if proj is None:
+                    return None
+            try:
+                del proj[self.tags[-1]]
+            except KeyError:
+                pass
+            else:
+                _save_pyproject_bsb(proj)
+
+    def is_set(self, instance):
+        if self.tags:
+            _, proj = _pyproject_bsb()
+            for tag in self.tags[:-1]:
+                proj = proj.get(tag, None)
+                if proj is None:
+                    return False
+            return self.tags[-1] in proj
+        else:
+            return False
+
+
+class BsbOption:
+    """
+    Base option class. Can be subclassed to create new options.
+    """
+
+    def __init__(self, positional=False):
+        self.positional = positional
+
+    def __init_subclass__(
+        cls,
+        name=None,
+        env=(),
+        project=(),
+        cli=(),
+        script=(),
+        description=None,
+        flag=False,
+        inverted=False,
+        list=False,
+        readonly=False,
+        action=False,
+    ):
+        """
+        Subclass hook that defines the characteristics of the subclassed option class.
+
+        :param name: Unique name for identification
+        :type name: str
+        :param cli: Positional arguments for the :class:`.CLIOptionDescriptor` constructor.
+        :type cli: iterable
+        :param cli: Positional arguments for the :class:`.CLIOptionDescriptor` constructor.
+        :type cli: iterable
+        :param env: Positional arguments for the :class:`.EnvOptionDescriptor` constructor.
+        :type env: iterable
+        :param script: Positional arguments for the :class:`.ScriptOptionDescriptor` constructor.
+        :type script: iterable
+        :param description: Description of the option's purpose for the user.
+        :type description: str
+        :param flag: Indicates that the option is a flag and should toggle on a default off boolean when given.
+        :type flag: boolean
+        :param inverted: Used only for flags. Indicates that the flag is default on and is toggled off when given.
+        :param list: Indicates that the option takes multiple values.
+        :type list: boolean
+        :param readonly: Indicates that an option can be accessed but not be altered from the ``bsb.options`` module.
+        :type readonly: boolean
+        :param action: Indicates that the option should execute its ``action`` method.
+        :type action: boolean
+        """
+        if name is None:
+            raise OptionError("Options must be given a name in the class argument list.")
+        cls.name = name
+        cls.env = EnvOptionDescriptor(*env, flag=flag)
+        cls.project = ProjectOptionDescriptor(*project)
+        cls.cli = CLIOptionDescriptor(*cli)
+        cls.script = ScriptOptionDescriptor(*script)
+        cls.description = description
+        cls.is_flag = flag
+        cls.inverted_flag = inverted
+        cls.use_extend = list
+        cls.readonly = readonly
+        cls.use_action = action
+        cls.positional = False
+
+    def get(self, prio=None):
+        """
+        Get the option's value. Cascades the script, cli, env & default descriptors together.
+
+        :returns: option value
+        """
+        try:
+            if prio is not None:
+                return getattr(self, prio)
+
+            cls = self.__class__
+            if cls.script.is_set(self):
+                return self.script
+            if cls.cli.is_set(self):
+                return self.cli
+            if cls.project.is_set(self):
+                return self.project
+            if cls.env.is_set(self):
+                return self.env
+            return self.get_default()
+        except Exception as e:
+            warn(f"Error retrieving option '{self.name}'.", log_exc=e)
+            return self.get_default()
+
+    def is_set(self, slug):
+        if descriptor := getattr(type(self), slug, None):
+            return descriptor.is_set(self)
+        else:
+            return False
+
+    def get_default(self):
+        """
+        Override to specify the default value of the option.
+        """
+        return None
+
+    def get_cli_tags(self):
+        """
+        Return the ``argparse`` positional arguments from the tags.
+
+        :returns: ``-x`` or ``--xxx`` for each CLI tag.
+        :rtype: list
+        """
+        if self.positional:
+            longest = ""
+            for t in type(self).cli.tags:
+                if len(t) >= len(longest):
+                    longest = t
+            return [longest]
+        else:
+            return [("--" if len(t) != 1 else "-") + t for t in type(self).cli.tags]
+
+    def add_to_parser(self, parser, level):
+        """
+        Register this option into an ``argparse`` parser.
+        """
+        if not self.get_cli_tags():
+            return
+
+        kwargs = {}
+        kwargs["help"] = self.description
+        kwargs["dest"] = level * "_" + self.name
+        kwargs["action"] = "store"
+        if self.positional:
+            kwargs["nargs"] = "?"
+            kwargs["metavar"] = self.get_cli_tags()
+            args = []
+        else:
+            args = self.get_cli_tags()
+        if self.is_flag:
+            kwargs["action"] += "_false" if self.inverted_flag else "_true"
+            kwargs["default"] = argparse.SUPPRESS
+        if self.use_extend:
+            kwargs["action"] = "extend"
+            kwargs["nargs"] = "+"
+        if self.use_action:
+            kwargs["dest"] = "internal_action_list"
+            kwargs["action"] = "append_const"
+            kwargs["const"] = self.action
+
+        parser.add_argument(*args, **kwargs)
+
+    @classmethod
+    def register(cls):
+        """
+        Register this option class into the :mod:`bsb.options` module.
+        """
+        from . import options
+
+        opt = cls()
+        options.register_option(cls.name, opt)
+        return opt
+
+    def unregister(self):
+        """
+        Remove this option class from the :mod:`bsb.options` module, not part of the
+        public API as removing options is undefined behavior but useful for testing.
+        """
+        from . import options
+
+        options.unregister_option(self)
+
+
+@functools.cache
+def _pyproject_path():
+    path = pathlib.Path.cwd()
+    while str(path)[len(path.drive) :] != path.root:
+        proj = path / "pyproject.toml"
+        if proj.exists():
+            return proj
+        path = path.parent
+
+
+def _pyproject_content():
+    path = _pyproject_path()
+    if path:
+        with open(path, "r") as f:
+            return path.resolve(), toml.load(f)
+    else:
+        return None, {}  # pragma: nocover
+
+
+def _pyproject_bsb():
+    path, content = _pyproject_content()
+    return path, content.get("tools", {}).get("bsb", {})
+
+
+def _save_pyproject_bsb(project):
+    path, content = _pyproject_content()
+    if path is None:
+        raise OptionError(
+            "No 'pyproject.toml' in current dir or parents,"
+            + " can't set project settings."
+        )
+    content.setdefault("tools", {})["bsb"] = project
+    with open(path, "w") as f:
+        toml.dump(content, f)
+
+
+__all__ = [
+    "BsbOption",
+    "CLIOptionDescriptor",
+    "EnvOptionDescriptor",
+    "OptionDescriptor",
+    "ProjectOptionDescriptor",
+    "ScriptOptionDescriptor",
+]
```

### Comparing `bsb_core-4.0.1/bsb/placement/arrays.py` & `bsb_core-4.1.0/bsb/placement/arrays.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,103 +1,103 @@
-import math
-
-import numpy as np
-
-from .. import config
-from ..config import types
-from ..mixins import NotParallel
-from ..reporting import report
-from .strategy import PlacementStrategy
-
-
-@config.node
-class ParallelArrayPlacement(NotParallel, PlacementStrategy):
-    """
-    Implementation of the placement of cells in parallel arrays.
-    """
-
-    spacing_x: float = config.attr(type=float, required=True)
-    angle: float = config.attr(type=types.deg_to_radian(), required=True)
-
-    def place(self, chunk, indicators):
-        """
-        Cell placement: Create a lattice of parallel arrays/lines in the layer's surface.
-        """
-        for indicator in indicators.values():
-            cell_type = indicator.cell_type
-            radius = indicator.get_radius()
-            for prt in self.partitions:
-                width, depth, height = prt.data.mdc - prt.data.ldc
-                ldc = prt.data.ldc
-                # Extension of a single array in the X dimension
-                spacing_x = self.spacing_x
-                # Add a random shift to the starting points of the arrays for variation.
-                x_shift = np.random.rand() * spacing_x
-                # Place purkinje cells equally spaced over the entire length of the X axis kept apart by their dendritic trees.
-                # They are placed in straight lines, tilted by a certain angle by adding a shifting value.
-                x_pos = np.arange(start=0.0, stop=width, step=spacing_x) + x_shift
-                if x_pos.shape[0] == 0:
-                    # When the spacing_x of is larger than the simulation volume,
-                    # place a single row on a random position along the x axis
-                    x_pos = np.array([x_shift])
-                # Amount of parallel arrays of cells
-                n_arrays = x_pos.shape[0]
-                # Number of cells
-                n = np.sum(indicator.guess(prt.data))
-                # Add extra cells to fill the lattice error volume which will be pruned
-                n += int((n_arrays * spacing_x % width) / width * n)
-                # cells to distribute along the rows
-                cells_per_row = round(n / n_arrays)
-                # The rounded amount of cells that will be placed
-                cells_placed = cells_per_row * n_arrays
-                # Calculate the position of the cells along the z-axis.
-                y_pos, y_axis_distance = np.linspace(
-                    start=0.0,
-                    stop=depth - radius,
-                    num=cells_per_row,
-                    retstep=True,
-                    endpoint=False,
-                )
-                # Center the cell soma center to the middle of the unit cell
-                y_pos += radius + y_axis_distance / 2
-                # The length of the X axis rounded up to a multiple of the unit cell size.
-                lattice_x = n_arrays * spacing_x
-                # The length of the X axis where cells can be placed in.
-                bounded_x = lattice_x - radius * 2
-                # Epsilon: open space in the unit cell along the z-axis
-                epsilon = y_axis_distance - radius * 2
-                # Storage array for the cells
-                cells = np.empty((cells_placed, 3))
-                for i in range(y_pos.shape[0]):
-                    # Shift the arrays at an angle
-                    angleShift = y_pos[i] * math.tan(self.angle)
-                    # Apply shift and offset
-                    x = x_pos + angleShift
-                    # Place the cells in a bounded lattice with a little modulus magic
-                    x = ldc[0] + x % bounded_x + radius
-                    # Place the cells in their y-position with jitter
-                    y = ldc[1] + y_pos[i] + epsilon * (np.random.rand(x.shape[0]) - 0.5)
-                    # Place them at a uniformly random height throughout the partition.
-                    z = ldc[2] + np.random.uniform(radius, height - radius, x.shape[0])
-                    # Store this stack's cells
-                    cells[(i * len(x)) : ((i + 1) * len(x)), 0] = x
-                    cells[(i * len(x)) : ((i + 1) * len(x)), 1] = y
-                    cells[(i * len(x)) : ((i + 1) * len(x)), 2] = z
-                # Place all the cells in 1 batch (more efficient)
-                positions = cells[cells[:, 0] < width - radius]
-
-                # Determine in which chunks the cells must be placed
-                cs = self.scaffold.configuration.network.chunk_size
-                chunks_list = np.array(
-                    [chunk.data + np.floor_divide(p, cs[0]) for p in positions]
-                )
-                unique_chunks_list = np.unique(chunks_list, axis=0)
-
-                # For each chunk, place the cells
-                for c in unique_chunks_list:
-                    idx = np.where((chunks_list == c).all(axis=1))
-                    pos_current_chunk = positions[idx]
-                    self.place_cells(indicator, pos_current_chunk, chunk=c)
-                report(f"Placed {len(positions)} {cell_type.name} in {prt.name}", level=3)
-
-
-__all__ = ["ParallelArrayPlacement"]
+import math
+
+import numpy as np
+
+from .. import config
+from ..config import types
+from ..mixins import NotParallel
+from ..reporting import report
+from .strategy import PlacementStrategy
+
+
+@config.node
+class ParallelArrayPlacement(NotParallel, PlacementStrategy):
+    """
+    Implementation of the placement of cells in parallel arrays.
+    """
+
+    spacing_x: float = config.attr(type=float, required=True)
+    angle: float = config.attr(type=types.deg_to_radian(), required=True)
+
+    def place(self, chunk, indicators):
+        """
+        Cell placement: Create a lattice of parallel arrays/lines in the layer's surface.
+        """
+        for indicator in indicators.values():
+            cell_type = indicator.cell_type
+            radius = indicator.get_radius()
+            for prt in self.partitions:
+                width, depth, height = prt.data.mdc - prt.data.ldc
+                ldc = prt.data.ldc
+                # Extension of a single array in the X dimension
+                spacing_x = self.spacing_x
+                # Add a random shift to the starting points of the arrays for variation.
+                x_shift = np.random.rand() * spacing_x
+                # Place purkinje cells equally spaced over the entire length of the X axis kept apart by their dendritic trees.
+                # They are placed in straight lines, tilted by a certain angle by adding a shifting value.
+                x_pos = np.arange(start=0.0, stop=width, step=spacing_x) + x_shift
+                if x_pos.shape[0] == 0:
+                    # When the spacing_x of is larger than the simulation volume,
+                    # place a single row on a random position along the x axis
+                    x_pos = np.array([x_shift])
+                # Amount of parallel arrays of cells
+                n_arrays = x_pos.shape[0]
+                # Number of cells
+                n = np.sum(indicator.guess(prt.data))
+                # Add extra cells to fill the lattice error volume which will be pruned
+                n += int((n_arrays * spacing_x % width) / width * n)
+                # cells to distribute along the rows
+                cells_per_row = round(n / n_arrays)
+                # The rounded amount of cells that will be placed
+                cells_placed = cells_per_row * n_arrays
+                # Calculate the position of the cells along the z-axis.
+                y_pos, y_axis_distance = np.linspace(
+                    start=0.0,
+                    stop=depth - radius,
+                    num=cells_per_row,
+                    retstep=True,
+                    endpoint=False,
+                )
+                # Center the cell soma center to the middle of the unit cell
+                y_pos += radius + y_axis_distance / 2
+                # The length of the X axis rounded up to a multiple of the unit cell size.
+                lattice_x = n_arrays * spacing_x
+                # The length of the X axis where cells can be placed in.
+                bounded_x = lattice_x - radius * 2
+                # Epsilon: open space in the unit cell along the z-axis
+                epsilon = y_axis_distance - radius * 2
+                # Storage array for the cells
+                cells = np.empty((cells_placed, 3))
+                for i in range(y_pos.shape[0]):
+                    # Shift the arrays at an angle
+                    angleShift = y_pos[i] * math.tan(self.angle)
+                    # Apply shift and offset
+                    x = x_pos + angleShift
+                    # Place the cells in a bounded lattice with a little modulus magic
+                    x = ldc[0] + x % bounded_x + radius
+                    # Place the cells in their y-position with jitter
+                    y = ldc[1] + y_pos[i] + epsilon * (np.random.rand(x.shape[0]) - 0.5)
+                    # Place them at a uniformly random height throughout the partition.
+                    z = ldc[2] + np.random.uniform(radius, height - radius, x.shape[0])
+                    # Store this stack's cells
+                    cells[(i * len(x)) : ((i + 1) * len(x)), 0] = x
+                    cells[(i * len(x)) : ((i + 1) * len(x)), 1] = y
+                    cells[(i * len(x)) : ((i + 1) * len(x)), 2] = z
+                # Place all the cells in 1 batch (more efficient)
+                positions = cells[cells[:, 0] < width - radius]
+
+                # Determine in which chunks the cells must be placed
+                cs = self.scaffold.configuration.network.chunk_size
+                chunks_list = np.array(
+                    [chunk.data + np.floor_divide(p, cs[0]) for p in positions]
+                )
+                unique_chunks_list = np.unique(chunks_list, axis=0)
+
+                # For each chunk, place the cells
+                for c in unique_chunks_list:
+                    idx = np.where((chunks_list == c).all(axis=1))
+                    pos_current_chunk = positions[idx]
+                    self.place_cells(indicator, pos_current_chunk, chunk=c)
+                report(f"Placed {len(positions)} {cell_type.name} in {prt.name}", level=3)
+
+
+__all__ = ["ParallelArrayPlacement"]
```

### Comparing `bsb_core-4.0.1/bsb/placement/distributor.py` & `bsb_core-4.1.0/bsb/placement/distributor.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,367 +1,367 @@
-import abc
-import uuid
-from dataclasses import dataclass
-from typing import List
-
-import numpy as np
-from scipy.spatial.transform import Rotation
-
-from .. import config
-from .._util import rotation_matrix_from_vectors
-from ..config.types import ndarray
-from ..exceptions import EmptySelectionError
-from ..morphologies import MorphologySet, RotationSet
-from ..profiling import node_meter
-from ..storage._files import NrrdDependencyNode
-from ..topology.partition import Partition
-from .indicator import PlacementIndications
-
-
-@dataclass
-class DistributionContext:
-    indicator: PlacementIndications
-    partitions: List[Partition]
-
-
-@config.dynamic(attr_name="strategy", required=True)
-class Distributor(abc.ABC):
-    def __init_subclass__(cls, **kwargs):
-        super(cls, cls).__init_subclass__(**kwargs)
-        # Decorate subclasses to measure performance
-        node_meter("distribute")(cls)
-
-    @abc.abstractmethod
-    def distribute(self, positions, context):
-        """
-        Is called to distribute cell properties.
-
-        :param partitions: The partitions the cells were placed in.
-        :type context: Additional context information such as the placement indications
-          and partitions.
-        :type context: ~bsb.placement.distributor.DistributionContext
-        :returns: An array with the property data
-        :rtype: numpy.ndarray
-        """
-        pass
-
-
-@config.dynamic(
-    attr_name="strategy", required=False, default="random", auto_classmap=True
-)
-class MorphologyDistributor(Distributor):
-    may_be_empty = config.attr(type=bool, default=False)
-
-    @abc.abstractmethod
-    def distribute(self, positions, morphologies, context):
-        """
-        Is called to distribute cell morphologies and optionally rotations.
-
-        :param positions: Placed positions under consideration
-        :type positions: numpy.ndarray
-        :param morphologies: The template morphology loaders. You can decide to use them
-          and/or generate new ones in the MorphologySet that you produce. If you produce
-          any new morphologies, don't forget to encapsulate them in a
-          :class:`~bsb.storage.interfaces.StoredMorphology` loader, or better yet, use
-          the :class:`~bsb.placement.distributor.MorphologyGenerator`.
-        :param context: The placement indicator and partitions.
-        :type context: ~bsb.placement.distributor.DistributionContext
-        :returns: A MorphologySet with assigned morphologies, and optionally a RotationSet
-        :rtype: Union[~bsb.morphologies.MorphologySet, Tuple[
-          ~bsb.morphologies.MorphologySet, ~bsb.morphologies.RotationSet]]
-        """
-        pass
-
-
-@config.node
-class RandomMorphologies(MorphologyDistributor, classmap_entry="random"):
-    """
-    Distributes selected morphologies randomly without rotating them.
-
-    .. code-block:: json
-
-      { "placement": { "place_XY": {
-        "distribute": {
-            "morphologies": {"strategy": "random"}
-        }
-      }}}
-    """
-
-    may_be_empty = config.provide(False)
-
-    def distribute(self, positions, morphologies, context):
-        """
-        Uses the morphology selection indicators to select morphologies and
-        returns a MorphologySet of randomly assigned morphologies
-        """
-        return np.random.default_rng().integers(len(morphologies), size=len(positions))
-
-
-@config.node
-class RoundRobinMorphologies(MorphologyDistributor, classmap_entry="roundrobin"):
-    """
-    Distributes selected morphologies round robin, values are looped and assigned one by
-    one in order.
-
-    .. code-block:: json
-
-      { "placement": { "place_XY": {
-        "distribute": {
-            "morphologies": {"strategy": "roundrobin"}
-        }
-      }}}
-    """
-
-    may_be_empty = config.provide(False)
-
-    def distribute(self, positions, morphologies, context):
-        ll = len(morphologies)
-        lp = len(positions)
-        return np.tile(np.arange(ll), lp // ll + 1)[:lp]
-
-
-@config.node
-class MorphologyGenerator(MorphologyDistributor, classmap_entry=None):
-    """
-    Special case of the morphology distributor that provides extra convenience when
-    generating new morphologies.
-    """
-
-    may_be_empty = config.attr(type=bool, default=True)
-
-    def __init_subclass__(cls, **kwargs):
-        super(cls, cls).__init_subclass__(**kwargs)
-        # Decorate subclasses to measure performance
-        node_meter("generate")(cls)
-
-    def distribute(self, positions, morphologies, context):
-        pass
-
-    @abc.abstractmethod
-    def generate(self, positions, morphologies, context):
-        pass
-
-
-@config.dynamic(attr_name="strategy", required=False, default="none", auto_classmap=True)
-class RotationDistributor(Distributor):
-    """
-    Rotates everything by nothing!
-    """
-
-    @abc.abstractmethod
-    def distribute(self, positions, context):
-        pass
-
-
-@config.node
-class ExplicitNoRotations(RotationDistributor, classmap_entry="explicitly_none"):
-    def distribute(self, positions, context):
-        return np.zeros((len(positions), 3))
-
-
-# Sentinel mixin class to tag RotationDistributor as being overridable by morphology
-# distributor.
-class Implicit:
-    pass
-
-
-@config.node
-class ImplicitNoRotations(ExplicitNoRotations, Implicit, classmap_entry="none"):
-    pass
-
-
-@config.node
-class RandomRotations(RotationDistributor, classmap_entry="random"):
-    def distribute(self, positions, context):
-        return np.random.rand(len(positions), 3) * 360
-
-
-@config.node
-class VolumetricRotations(RotationDistributor, classmap_entry="orientation_field"):
-    orientation_path = config.attr(required=True, type=NrrdDependencyNode)
-    """Path to the nrrd file containing the volumetric orientation field. It provides a rotation 
-    for each voxel considered. Its shape should be (3, L, W, D) where L, W and D are the sizes of 
-    the field."""
-    orientation_resolution = config.attr(required=False, default=25.0, type=float)
-    """Voxel size resolution of the orientation field.
-    """
-    default_vector = config.attr(
-        required=False,
-        default=lambda: np.array([0.0, -1.0, 0.0]),
-        call_default=True,
-        type=ndarray(),
-    )
-    """Default orientation vector of each position.
-    """
-    space_origin = config.attr(
-        required=False,
-        default=lambda: np.array([0.0, 0.0, 0.0]),
-        call_default=True,
-        type=ndarray(),
-    )
-    """Origin point for the orientation field.
-    """
-
-    def distribute(self, positions, context):
-        """
-        Rotates according to a volumetric orientation field of specific resolution.
-        For each position, find the equivalent voxel in the volumetric orientation field and apply
-        the rotation from the default_vector to the corresponding orientation vector.
-        Positions outside the orientation field will not be rotated.
-
-        :param positions: Placed positions under consideration. Its shape is (N, 3) where N is the
-            number of positions.
-        :param context: The placement indicator and partitions.
-        :type context: ~bsb.placement.distributor.DistributionContext
-        :returns: A RotationSet object containing the 3D Euler angles in degrees for the rotation
-            of each position.
-        :rtype: RotationSet
-        """
-
-        orientation_field = self.orientation_path.load_object()
-        voxel_pos = np.asarray(
-            np.floor((positions - self.space_origin) / self.orientation_resolution),
-            dtype=int,
-        )
-
-        # filter for positions inside the orientation field.
-        filter_inside = (
-            np.all(voxel_pos >= 0, axis=1)
-            * (voxel_pos[:, 0] < orientation_field.shape[1])
-            * (voxel_pos[:, 1] < orientation_field.shape[2])
-            * (voxel_pos[:, 2] < orientation_field.shape[3])
-        )
-
-        # By default, positions outside the field should not rotate.
-        # So their target orientation vector will be set to the default_vector,
-        # from which the rotation is processed.
-        orientations = np.full((positions.shape[0], 3), self.default_vector, dtype=float)
-        # Expected orientation_field shape is (3, L, W, D) where L, W and D are the sizes
-        # of the field. Here we want to filter on the space dimensions, so we move the axes.
-        if filter_inside.any():
-            orientations[filter_inside] = np.moveaxis(orientation_field, 0, -1)[
-                voxel_pos[filter_inside, 0],
-                voxel_pos[filter_inside, 1],
-                voxel_pos[filter_inside, 2],
-            ]
-            orientations[
-                np.isnan(orientations).any(axis=1) + ~orientations.any(axis=1)
-            ] = self.default_vector
-
-        return RotationSet(
-            Rotation.from_matrix(
-                rotation_matrix_from_vectors(self.default_vector, v)
-            ).as_euler("xyz", degrees=True)
-            for v in orientations
-        )
-
-
-@config.node
-class DistributorsNode:
-    morphologies: MorphologyDistributor = config.attr(
-        type=MorphologyDistributor, default=dict, call_default=True
-    )
-    rotations: RotationDistributor = config.attr(
-        type=RotationDistributor, default=dict, call_default=True
-    )
-    properties: dict[Distributor] = config.catch_all(type=Distributor)
-
-    def __call__(self, key, partitions, indicator, positions, loaders=None):
-        context = DistributionContext(indicator, partitions)
-        if key == "morphologies":
-            distributor = getattr(self, key)
-            if hasattr(distributor, "generate"):
-                distribute = distributor.generate
-            else:
-                distribute = distributor.distribute
-            values = distribute(positions, loaders, context)
-            if isinstance(values, tuple):
-                # Check for accidental tuple return values. If you return a 2 sized
-                # tuple you're still fucked, but the Gods must truly hate you.
-                try:
-                    morphologies, rotations = values
-                except TypeError:
-                    raise ValueError(
-                        "Morphology distributors may only return tuples when they are"
-                        + " to be unpacked as (morphologies, rotations)"
-                    ) from None
-            else:
-                values = (values, None)
-            return values
-        elif key == "rotations":
-            distribute = getattr(self, key).distribute
-        else:
-            distribute = self.properties[key].distribute
-        return distribute(positions, context)
-
-    def _curry(self, partitions, indicator, positions, loaders=None):
-        def curried(key):
-            return self(key, partitions, indicator, positions, loaders)
-
-        return curried
-
-    def _specials(self, partitions, indicator, positions):
-        sel = indicator.assert_indication("morphologies")
-        loaders = self.scaffold.storage.morphologies.select(*sel)
-        if not loaders and not self.morphologies.may_be_empty:
-            raise EmptySelectionError(
-                f"Given {len(sel)} selectors: did not find any suitable morphologies", sel
-            )
-        distr = self._curry(partitions, indicator, positions, loaders)
-        morphologies, rotations = distr("morphologies")
-        if morphologies is not None and (
-            rotations is None or not isinstance(self.rotations, Implicit)
-        ):
-            # If a RotationDistributor is not explicitly marked as `Implicit`, it
-            # overrides the MorphologyDistributor's implicit rotations.
-            rotations = distr("rotations")
-        if hasattr(self.morphologies, "generate"):
-            prefix = self._config_parent.name
-            generated = {}
-            indices = []
-            # Get all the unique morphology objects from the return value and map to them
-            for m in morphologies:
-                idx = generated.setdefault(m, len(generated))
-                indices.append(idx)
-            mr = self.scaffold.morphologies
-            uid = uuid.uuid4()
-            loaders = []
-            all_meta = {}
-            # Save morphologies one by one
-            for gen_morpho, i in generated.items():
-                name = f"{prefix}-{uid}-{i}"
-                # `update_metadata=False` so morphology is incompletely stored.
-                saved = mr.save(name, gen_morpho, update_meta=False)
-                all_meta[name] = saved.get_meta()
-                loaders.append(saved)
-            # Finish morphologies by bulk saving their collective metadata
-            mr.update_all_meta(all_meta)
-            morphologies = MorphologySet(loaders, indices)
-        if not isinstance(morphologies, MorphologySet) and morphologies is not None:
-            morphologies = MorphologySet(loaders, morphologies)
-
-        return morphologies, rotations
-
-    def _has_mdistr(self):
-        # This function checks if this distributor node has specified a morpho distributor
-        return self.__class__.morphologies.is_dirty(self)
-
-    def _has_rdistr(self):
-        # This function checks if this distributor node has specified a rotation distributor
-        return self.__class__.rotations.is_dirty(self)
-
-
-__all__ = [
-    "DistributionContext",
-    "Distributor",
-    "DistributorsNode",
-    "ExplicitNoRotations",
-    "Implicit",
-    "ImplicitNoRotations",
-    "MorphologyDistributor",
-    "MorphologyGenerator",
-    "RandomMorphologies",
-    "RandomRotations",
-    "RotationDistributor",
-    "RoundRobinMorphologies",
-    "VolumetricRotations",
-]
+import abc
+import uuid
+from dataclasses import dataclass
+from typing import List
+
+import numpy as np
+from scipy.spatial.transform import Rotation
+
+from .. import config
+from .._util import rotation_matrix_from_vectors
+from ..config.types import ndarray
+from ..exceptions import EmptySelectionError
+from ..morphologies import MorphologySet, RotationSet
+from ..profiling import node_meter
+from ..storage._files import NrrdDependencyNode
+from ..topology.partition import Partition
+from .indicator import PlacementIndications
+
+
+@dataclass
+class DistributionContext:
+    indicator: PlacementIndications
+    partitions: List[Partition]
+
+
+@config.dynamic(attr_name="strategy", required=True)
+class Distributor(abc.ABC):
+    def __init_subclass__(cls, **kwargs):
+        super(cls, cls).__init_subclass__(**kwargs)
+        # Decorate subclasses to measure performance
+        node_meter("distribute")(cls)
+
+    @abc.abstractmethod
+    def distribute(self, positions, context):
+        """
+        Is called to distribute cell properties.
+
+        :param partitions: The partitions the cells were placed in.
+        :type context: Additional context information such as the placement indications
+          and partitions.
+        :type context: ~bsb.placement.distributor.DistributionContext
+        :returns: An array with the property data
+        :rtype: numpy.ndarray
+        """
+        pass
+
+
+@config.dynamic(
+    attr_name="strategy", required=False, default="random", auto_classmap=True
+)
+class MorphologyDistributor(Distributor):
+    may_be_empty = config.attr(type=bool, default=False)
+
+    @abc.abstractmethod
+    def distribute(self, positions, morphologies, context):
+        """
+        Is called to distribute cell morphologies and optionally rotations.
+
+        :param positions: Placed positions under consideration
+        :type positions: numpy.ndarray
+        :param morphologies: The template morphology loaders. You can decide to use them
+          and/or generate new ones in the MorphologySet that you produce. If you produce
+          any new morphologies, don't forget to encapsulate them in a
+          :class:`~bsb.storage.interfaces.StoredMorphology` loader, or better yet, use
+          the :class:`~bsb.placement.distributor.MorphologyGenerator`.
+        :param context: The placement indicator and partitions.
+        :type context: ~bsb.placement.distributor.DistributionContext
+        :returns: A MorphologySet with assigned morphologies, and optionally a RotationSet
+        :rtype: Union[~bsb.morphologies.MorphologySet, Tuple[
+          ~bsb.morphologies.MorphologySet, ~bsb.morphologies.RotationSet]]
+        """
+        pass
+
+
+@config.node
+class RandomMorphologies(MorphologyDistributor, classmap_entry="random"):
+    """
+    Distributes selected morphologies randomly without rotating them.
+
+    .. code-block:: json
+
+      { "placement": { "place_XY": {
+        "distribute": {
+            "morphologies": {"strategy": "random"}
+        }
+      }}}
+    """
+
+    may_be_empty = config.provide(False)
+
+    def distribute(self, positions, morphologies, context):
+        """
+        Uses the morphology selection indicators to select morphologies and
+        returns a MorphologySet of randomly assigned morphologies
+        """
+        return np.random.default_rng().integers(len(morphologies), size=len(positions))
+
+
+@config.node
+class RoundRobinMorphologies(MorphologyDistributor, classmap_entry="roundrobin"):
+    """
+    Distributes selected morphologies round robin, values are looped and assigned one by
+    one in order.
+
+    .. code-block:: json
+
+      { "placement": { "place_XY": {
+        "distribute": {
+            "morphologies": {"strategy": "roundrobin"}
+        }
+      }}}
+    """
+
+    may_be_empty = config.provide(False)
+
+    def distribute(self, positions, morphologies, context):
+        ll = len(morphologies)
+        lp = len(positions)
+        return np.tile(np.arange(ll), lp // ll + 1)[:lp]
+
+
+@config.node
+class MorphologyGenerator(MorphologyDistributor, classmap_entry=None):
+    """
+    Special case of the morphology distributor that provides extra convenience when
+    generating new morphologies.
+    """
+
+    may_be_empty = config.attr(type=bool, default=True)
+
+    def __init_subclass__(cls, **kwargs):
+        super(cls, cls).__init_subclass__(**kwargs)
+        # Decorate subclasses to measure performance
+        node_meter("generate")(cls)
+
+    def distribute(self, positions, morphologies, context):
+        pass
+
+    @abc.abstractmethod
+    def generate(self, positions, morphologies, context):
+        pass
+
+
+@config.dynamic(attr_name="strategy", required=False, default="none", auto_classmap=True)
+class RotationDistributor(Distributor):
+    """
+    Rotates everything by nothing!
+    """
+
+    @abc.abstractmethod
+    def distribute(self, positions, context):
+        pass
+
+
+@config.node
+class ExplicitNoRotations(RotationDistributor, classmap_entry="explicitly_none"):
+    def distribute(self, positions, context):
+        return np.zeros((len(positions), 3))
+
+
+# Sentinel mixin class to tag RotationDistributor as being overridable by morphology
+# distributor.
+class Implicit:
+    pass
+
+
+@config.node
+class ImplicitNoRotations(ExplicitNoRotations, Implicit, classmap_entry="none"):
+    pass
+
+
+@config.node
+class RandomRotations(RotationDistributor, classmap_entry="random"):
+    def distribute(self, positions, context):
+        return np.random.rand(len(positions), 3) * 360
+
+
+@config.node
+class VolumetricRotations(RotationDistributor, classmap_entry="orientation_field"):
+    orientation_path = config.attr(required=True, type=NrrdDependencyNode)
+    """Path to the nrrd file containing the volumetric orientation field. It provides a rotation 
+    for each voxel considered. Its shape should be (3, L, W, D) where L, W and D are the sizes of 
+    the field."""
+    orientation_resolution = config.attr(required=False, default=25.0, type=float)
+    """Voxel size resolution of the orientation field.
+    """
+    default_vector = config.attr(
+        required=False,
+        default=lambda: np.array([0.0, -1.0, 0.0]),
+        call_default=True,
+        type=ndarray(),
+    )
+    """Default orientation vector of each position.
+    """
+    space_origin = config.attr(
+        required=False,
+        default=lambda: np.array([0.0, 0.0, 0.0]),
+        call_default=True,
+        type=ndarray(),
+    )
+    """Origin point for the orientation field.
+    """
+
+    def distribute(self, positions, context):
+        """
+        Rotates according to a volumetric orientation field of specific resolution.
+        For each position, find the equivalent voxel in the volumetric orientation field and apply
+        the rotation from the default_vector to the corresponding orientation vector.
+        Positions outside the orientation field will not be rotated.
+
+        :param positions: Placed positions under consideration. Its shape is (N, 3) where N is the
+            number of positions.
+        :param context: The placement indicator and partitions.
+        :type context: ~bsb.placement.distributor.DistributionContext
+        :returns: A RotationSet object containing the 3D Euler angles in degrees for the rotation
+            of each position.
+        :rtype: RotationSet
+        """
+
+        orientation_field = self.orientation_path.load_object()
+        voxel_pos = np.asarray(
+            np.floor((positions - self.space_origin) / self.orientation_resolution),
+            dtype=int,
+        )
+
+        # filter for positions inside the orientation field.
+        filter_inside = (
+            np.all(voxel_pos >= 0, axis=1)
+            * (voxel_pos[:, 0] < orientation_field.shape[1])
+            * (voxel_pos[:, 1] < orientation_field.shape[2])
+            * (voxel_pos[:, 2] < orientation_field.shape[3])
+        )
+
+        # By default, positions outside the field should not rotate.
+        # So their target orientation vector will be set to the default_vector,
+        # from which the rotation is processed.
+        orientations = np.full((positions.shape[0], 3), self.default_vector, dtype=float)
+        # Expected orientation_field shape is (3, L, W, D) where L, W and D are the sizes
+        # of the field. Here we want to filter on the space dimensions, so we move the axes.
+        if filter_inside.any():
+            orientations[filter_inside] = np.moveaxis(orientation_field, 0, -1)[
+                voxel_pos[filter_inside, 0],
+                voxel_pos[filter_inside, 1],
+                voxel_pos[filter_inside, 2],
+            ]
+            orientations[
+                np.isnan(orientations).any(axis=1) + ~orientations.any(axis=1)
+            ] = self.default_vector
+
+        return RotationSet(
+            Rotation.from_matrix(
+                rotation_matrix_from_vectors(self.default_vector, v)
+            ).as_euler("xyz", degrees=True)
+            for v in orientations
+        )
+
+
+@config.node
+class DistributorsNode:
+    morphologies: MorphologyDistributor = config.attr(
+        type=MorphologyDistributor, default=dict, call_default=True
+    )
+    rotations: RotationDistributor = config.attr(
+        type=RotationDistributor, default=dict, call_default=True
+    )
+    properties: dict[Distributor] = config.catch_all(type=Distributor)
+
+    def __call__(self, key, partitions, indicator, positions, loaders=None):
+        context = DistributionContext(indicator, partitions)
+        if key == "morphologies":
+            distributor = getattr(self, key)
+            if hasattr(distributor, "generate"):
+                distribute = distributor.generate
+            else:
+                distribute = distributor.distribute
+            values = distribute(positions, loaders, context)
+            if isinstance(values, tuple):
+                # Check for accidental tuple return values. If you return a 2 sized
+                # tuple you're still fucked, but the Gods must truly hate you.
+                try:
+                    morphologies, rotations = values
+                except TypeError:
+                    raise ValueError(
+                        "Morphology distributors may only return tuples when they are"
+                        + " to be unpacked as (morphologies, rotations)"
+                    ) from None
+            else:
+                values = (values, None)
+            return values
+        elif key == "rotations":
+            distribute = getattr(self, key).distribute
+        else:
+            distribute = self.properties[key].distribute
+        return distribute(positions, context)
+
+    def _curry(self, partitions, indicator, positions, loaders=None):
+        def curried(key):
+            return self(key, partitions, indicator, positions, loaders)
+
+        return curried
+
+    def _specials(self, partitions, indicator, positions):
+        sel = indicator.assert_indication("morphologies")
+        loaders = self.scaffold.storage.morphologies.select(*sel)
+        if not loaders and not self.morphologies.may_be_empty:
+            raise EmptySelectionError(
+                f"Given {len(sel)} selectors: did not find any suitable morphologies", sel
+            )
+        distr = self._curry(partitions, indicator, positions, loaders)
+        morphologies, rotations = distr("morphologies")
+        if morphologies is not None and (
+            rotations is None or not isinstance(self.rotations, Implicit)
+        ):
+            # If a RotationDistributor is not explicitly marked as `Implicit`, it
+            # overrides the MorphologyDistributor's implicit rotations.
+            rotations = distr("rotations")
+        if hasattr(self.morphologies, "generate"):
+            prefix = self._config_parent.name
+            generated = {}
+            indices = []
+            # Get all the unique morphology objects from the return value and map to them
+            for m in morphologies:
+                idx = generated.setdefault(m, len(generated))
+                indices.append(idx)
+            mr = self.scaffold.morphologies
+            uid = uuid.uuid4()
+            loaders = []
+            all_meta = {}
+            # Save morphologies one by one
+            for gen_morpho, i in generated.items():
+                name = f"{prefix}-{uid}-{i}"
+                # `update_metadata=False` so morphology is incompletely stored.
+                saved = mr.save(name, gen_morpho, update_meta=False)
+                all_meta[name] = saved.get_meta()
+                loaders.append(saved)
+            # Finish morphologies by bulk saving their collective metadata
+            mr.update_all_meta(all_meta)
+            morphologies = MorphologySet(loaders, indices)
+        if not isinstance(morphologies, MorphologySet) and morphologies is not None:
+            morphologies = MorphologySet(loaders, morphologies)
+
+        return morphologies, rotations
+
+    def _has_mdistr(self):
+        # This function checks if this distributor node has specified a morpho distributor
+        return self.__class__.morphologies.is_dirty(self)
+
+    def _has_rdistr(self):
+        # This function checks if this distributor node has specified a rotation distributor
+        return self.__class__.rotations.is_dirty(self)
+
+
+__all__ = [
+    "DistributionContext",
+    "Distributor",
+    "DistributorsNode",
+    "ExplicitNoRotations",
+    "Implicit",
+    "ImplicitNoRotations",
+    "MorphologyDistributor",
+    "MorphologyGenerator",
+    "RandomMorphologies",
+    "RandomRotations",
+    "RotationDistributor",
+    "RoundRobinMorphologies",
+    "VolumetricRotations",
+]
```

### Comparing `bsb_core-4.0.1/bsb/placement/import_.py` & `bsb_core-4.1.0/bsb/placement/import_.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,148 +1,148 @@
-import abc
-import csv
-import io
-import typing
-from collections import defaultdict
-
-import numpy as np
-import psutil
-from tqdm import tqdm
-
-from .. import config
-from ..config import refs
-from ..exceptions import ConfigurationError
-from ..mixins import NotParallel
-from ..storage._chunks import Chunk
-from .strategy import PlacementStrategy
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-    from ..storage._files import FileDependencyNode
-    from ..topology import Partition
-
-
-@config.node
-class ImportPlacement(NotParallel, PlacementStrategy, abc.ABC, classmap_entry=None):
-    source: "FileDependencyNode" = config.file(required=True)
-    cell_types: list["CellType"] = config.reflist(refs.cell_type_ref, required=False)
-    partitions: list["Partition"] = config.reflist(refs.partition_ref, required=False)
-
-    @config.property(default=False)
-    def cache(self):
-        return self.source.cache
-
-    @cache.setter
-    def cache(self, value):
-        self.source.cache = bool(value)
-
-    def place(self, chunk, indicators):
-        self.parse_source(indicators)
-
-    @abc.abstractmethod
-    def parse_source(self, indicators):
-        pass
-
-
-@config.node
-class CsvImportPlacement(ImportPlacement):
-    x_header: str = config.attr(default="x")
-    y_header: str = config.attr(default="y")
-    z_header: str = config.attr(default="z")
-    type_header: str = config.attr()
-    delimiter: str = config.attr(default=",")
-    progress_bar: bool = config.attr(type=bool, default=True)
-
-    def __boot__(self):
-        if not self.type_header and len(self.get_considered_cell_types()) > 1:
-            raise ConfigurationError(
-                "Must set `type_header` to import multiple cell types from single CSV."
-            )
-
-    def parse_source(self, indicators):
-        self._reset_cache()
-        chunk_size = np.array(self.scaffold.network.chunk_size)
-        with self.source.provide_stream() as (fp, encoding):
-            text = io.TextIOWrapper(fp, encoding=encoding, newline="")
-            reader = csv.reader(text)
-            headers = [h.strip().lower() for h in next(reader)]
-            type_col = headers.index(self.type_header) if self.type_header else None
-            coord_cols = [
-                *map(headers.index, (self.x_header, self.y_header, self.z_header))
-            ]
-            other_cols = [
-                r for r in range(len(headers)) if r not in coord_cols and r != type_col
-            ]
-            self._other_colnames = [headers[c] for c in other_cols]
-            cts = self.get_considered_cell_types()
-            if len(cts) == 1:
-                name = cts[0].name
-            i = 0
-            if self.progress_bar:
-                reader = tqdm(reader, desc="imported", unit=" lines")
-            for line in reader:
-                ct_cache = self._cache[line[type_col] if type_col is not None else name]
-                coords = [_safe_float(line[c]) for c in coord_cols]
-                others = [_safe_float(line[c]) for c in other_cols]
-                cache = ct_cache[tuple(coords // chunk_size)]
-                cache[0].append(coords)
-                cache[1].append(others)
-
-                if i % 10000 == 0:
-                    est_memsize = (len(others) + 3) * i * 8
-                    av_mem = psutil.virtual_memory().available
-                    if est_memsize > av_mem / 10:
-                        print(
-                            "FLUSHING, AVAILABLE MEM:",
-                        )
-                        self._flush(indicators)
-                i += 1
-            self._flush(indicators)
-
-    def get_considered_cell_types(self):
-        return self.cell_types or self.scaffold.cell_types.values()
-
-    def _reset_cache(self):
-        self._cache = {
-            ct.name: defaultdict(lambda: [[], []])
-            for ct in self.get_considered_cell_types()
-        }
-
-    def _flush(self, indicators):
-        cell_types = self.get_considered_cell_types()
-        iter = zip(cell_types, self._cache.values())
-        if self.progress_bar:
-            iter = tqdm(
-                iter,
-                desc="cell types",
-                total=len(cell_types),
-            )
-        for ct, chunked_cache in iter:
-            inner = ((Chunk(c, None), data) for c, data in chunked_cache.items())
-            if self.progress_bar:
-                inner = tqdm(
-                    inner,
-                    desc="saved",
-                    total=len(chunked_cache),
-                    bar_format="{l_bar}{bar} [ {n_fmt}/{total_fmt} time left: {remaining}, time spent: {elapsed}]",
-                )
-            for chunk, data in inner:
-                additional = np.array(data[1], dtype=float).T
-                self.place_cells(
-                    indicators[ct.name],
-                    np.array(data[0]),
-                    chunk,
-                    additional={
-                        name: col for name, col in zip(self._other_colnames, additional)
-                    },
-                )
-        self._reset_cache()
-
-
-def _safe_float(value: typing.Any) -> float:
-    try:
-        return float(value)
-    except ValueError:
-        return float("nan")
-
-
-__all__ = ["CsvImportPlacement", "ImportPlacement"]
+import abc
+import csv
+import io
+import typing
+from collections import defaultdict
+
+import numpy as np
+import psutil
+from tqdm import tqdm
+
+from .. import config
+from ..config import refs
+from ..exceptions import ConfigurationError
+from ..mixins import NotParallel
+from ..storage._chunks import Chunk
+from .strategy import PlacementStrategy
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+    from ..storage._files import FileDependencyNode
+    from ..topology import Partition
+
+
+@config.node
+class ImportPlacement(NotParallel, PlacementStrategy, abc.ABC, classmap_entry=None):
+    source: "FileDependencyNode" = config.file(required=True)
+    cell_types: list["CellType"] = config.reflist(refs.cell_type_ref, required=False)
+    partitions: list["Partition"] = config.reflist(refs.partition_ref, required=False)
+
+    @config.property(default=False)
+    def cache(self):
+        return self.source.cache
+
+    @cache.setter
+    def cache(self, value):
+        self.source.cache = bool(value)
+
+    def place(self, chunk, indicators):
+        self.parse_source(indicators)
+
+    @abc.abstractmethod
+    def parse_source(self, indicators):
+        pass
+
+
+@config.node
+class CsvImportPlacement(ImportPlacement):
+    x_header: str = config.attr(default="x")
+    y_header: str = config.attr(default="y")
+    z_header: str = config.attr(default="z")
+    type_header: str = config.attr()
+    delimiter: str = config.attr(default=",")
+    progress_bar: bool = config.attr(type=bool, default=True)
+
+    def __boot__(self):
+        if not self.type_header and len(self.get_considered_cell_types()) > 1:
+            raise ConfigurationError(
+                "Must set `type_header` to import multiple cell types from single CSV."
+            )
+
+    def parse_source(self, indicators):
+        self._reset_cache()
+        chunk_size = np.array(self.scaffold.network.chunk_size)
+        with self.source.provide_stream() as (fp, encoding):
+            text = io.TextIOWrapper(fp, encoding=encoding, newline="")
+            reader = csv.reader(text)
+            headers = [h.strip().lower() for h in next(reader)]
+            type_col = headers.index(self.type_header) if self.type_header else None
+            coord_cols = [
+                *map(headers.index, (self.x_header, self.y_header, self.z_header))
+            ]
+            other_cols = [
+                r for r in range(len(headers)) if r not in coord_cols and r != type_col
+            ]
+            self._other_colnames = [headers[c] for c in other_cols]
+            cts = self.get_considered_cell_types()
+            if len(cts) == 1:
+                name = cts[0].name
+            i = 0
+            if self.progress_bar:
+                reader = tqdm(reader, desc="imported", unit=" lines")
+            for line in reader:
+                ct_cache = self._cache[line[type_col] if type_col is not None else name]
+                coords = [_safe_float(line[c]) for c in coord_cols]
+                others = [_safe_float(line[c]) for c in other_cols]
+                cache = ct_cache[tuple(coords // chunk_size)]
+                cache[0].append(coords)
+                cache[1].append(others)
+
+                if i % 10000 == 0:
+                    est_memsize = (len(others) + 3) * i * 8
+                    av_mem = psutil.virtual_memory().available
+                    if est_memsize > av_mem / 10:
+                        print(
+                            "FLUSHING, AVAILABLE MEM:",
+                        )
+                        self._flush(indicators)
+                i += 1
+            self._flush(indicators)
+
+    def get_considered_cell_types(self):
+        return self.cell_types or self.scaffold.cell_types.values()
+
+    def _reset_cache(self):
+        self._cache = {
+            ct.name: defaultdict(lambda: [[], []])
+            for ct in self.get_considered_cell_types()
+        }
+
+    def _flush(self, indicators):
+        cell_types = self.get_considered_cell_types()
+        iter = zip(cell_types, self._cache.values())
+        if self.progress_bar:
+            iter = tqdm(
+                iter,
+                desc="cell types",
+                total=len(cell_types),
+            )
+        for ct, chunked_cache in iter:
+            inner = ((Chunk(c, None), data) for c, data in chunked_cache.items())
+            if self.progress_bar:
+                inner = tqdm(
+                    inner,
+                    desc="saved",
+                    total=len(chunked_cache),
+                    bar_format="{l_bar}{bar} [ {n_fmt}/{total_fmt} time left: {remaining}, time spent: {elapsed}]",
+                )
+            for chunk, data in inner:
+                additional = np.array(data[1], dtype=float).T
+                self.place_cells(
+                    indicators[ct.name],
+                    np.array(data[0]),
+                    chunk,
+                    additional={
+                        name: col for name, col in zip(self._other_colnames, additional)
+                    },
+                )
+        self._reset_cache()
+
+
+def _safe_float(value: typing.Any) -> float:
+    try:
+        return float(value)
+    except ValueError:
+        return float("nan")
+
+
+__all__ = ["CsvImportPlacement", "ImportPlacement"]
```

### Comparing `bsb_core-4.0.1/bsb/placement/indicator.py` & `bsb_core-4.1.0/bsb/placement/indicator.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,200 +1,200 @@
-import typing
-
-import numpy as np
-
-from .. import config
-from ..config import refs, types
-from ..config._attrs import cfglist
-from ..exceptions import IndicatorError, PlacementError, PlacementRelationError
-from ..morphologies.selector import MorphologySelector
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-    from ..core import Scaffold
-
-
-@config.node
-class PlacementIndications:
-    scaffold: "Scaffold"
-    radius: float = config.attr(type=float)
-    density: float = config.attr(type=float)
-    planar_density: float = config.attr(type=float)
-    count_ratio: float = config.attr(type=float)
-    density_ratio: float = config.attr(type=float)
-    relative_to: "CellType" = config.ref(refs.cell_type_ref)
-    count: int = config.attr(type=int)
-    geometry: dict = config.dict(type=types.any_())
-    morphologies: cfglist[MorphologySelector] = config.list(type=MorphologySelector)
-    density_key: str = config.attr(type=str)
-
-
-class _Noner:
-    def __getattr__(self, attr):
-        return None
-
-
-class PlacementIndicator:
-    def __init__(self, strat, cell_type):
-        self._strat = strat
-        self._cell_type = cell_type
-
-    @property
-    def cell_type(self):
-        return self._cell_type
-
-    def get_radius(self):
-        return self.assert_indication("radius")
-
-    def use_morphologies(self):
-        return bool(self.indication("morphologies"))
-
-    def indication(self, attr):
-        ind_strat = self._strat.overrides.get(self._cell_type.name) or _Noner()
-        ind_ct = self._cell_type.spatial
-        strat = getattr(ind_strat, attr)
-        ct = getattr(ind_ct, attr)
-        if strat is not None:
-            return strat
-        return ct
-
-    def assert_indication(self, attr):
-        ind = self.indication(attr)
-        if ind is None:
-            raise IndicatorError(
-                f"No configuration indicators found for the {attr}"
-                f" of '{self._cell_type.name}' in '{self._strat.name}'"
-            )
-        return ind
-
-    def guess(self, chunk=None, voxels=None):
-        """
-        Estimate the count of cell to place based on the cell_type's PlacementIndications.
-        Float estimates are converted to int using an acceptance-rejection method.
-
-        :param chunk: if provided, will estimate the number of cell within the Chunk.
-        :type chunk: bsb.storage._chunks.Chunk
-        :param voxels: if provided, will estimate the number of cell within the VoxelSet.
-            Only for cells with the indication "density_key" set or with the indication
-            "relative_to" set and the target cell has the indication "density_key" set.
-        :type voxels: bsb.voxels.VoxelSet
-        :returns: Cell counts for each chunk or voxel.
-        :rtype: numpy.ndarray[int]
-        """
-        count = self.indication("count")
-        density = self.indication("density")
-        density_key = self.indication("density_key")
-        planar_density = self.indication("planar_density")
-        relative_to = self.indication("relative_to")
-        density_ratio = self.indication("density_ratio")
-        count_ratio = self.indication("count_ratio")
-        if count is not None:
-            estimate = self._estim_for_chunk(chunk, count)
-        if density is not None:
-            estimate = self._density_to_estim(density, chunk)
-        if planar_density is not None:
-            estimate = self._pdensity_to_estim(planar_density, chunk)
-        if relative_to is not None:
-            relation = relative_to
-            if count_ratio is not None:
-                strats = self._strat.scaffold.get_placement_of(relation)
-                estimate = (
-                    sum(
-                        PlacementIndicator(s, relation).guess(chunk, voxels)
-                        for s in strats
-                    )
-                    * count_ratio
-                )
-            elif density_ratio is not None:
-                # Create an indicator based on this strategy for the related CT.
-                # This means we'll read only the CT indications, and ignore any
-                # overrides of other strats, but one can set overrides for the
-                # related type in this strat.
-                rel_ind = PlacementIndicator(self._strat, relation)
-                rel_density = rel_ind.indication("density")
-                rel_pl_density = rel_ind.indication("planar_density")
-                rel_pl_density_key = rel_ind.indication("density_key")
-                if rel_density is not None:
-                    estimate = self._density_to_estim(rel_density * density_ratio, chunk)
-                elif rel_pl_density is not None:
-                    estimate = self._pdensity_to_estim(
-                        rel_pl_density * density_ratio, chunk
-                    )
-                elif rel_pl_density_key is not None:
-                    # Use the relation's `guess` to guess according to the relation's density key
-                    estimate = rel_ind.guess(chunk, voxels) * density_ratio
-                else:
-                    raise PlacementRelationError(
-                        f"{self.cell_type.name} requires relation {relation.name}"
-                        + " to specify density information."
-                    )
-            else:
-                raise PlacementError(
-                    f"Relation specified but no ratio indications provided."
-                )
-        if density_key is not None:
-            if voxels is None:
-                raise Exception("Can't guess voxel density without a voxelset.")
-            elif density_key in voxels.data_keys:
-                estimate = self._estim_for_voxels(voxels, density_key)
-            else:
-                raise RuntimeError(
-                    f"No voxel density data column '{density_key}' found in any of the"
-                    " following partitions:\n"
-                    + "\n".join(
-                        f"* {p.name}: "
-                        + (
-                            fstr
-                            if (
-                                fstr := ", ".join(
-                                    f"'{col}'" for col in p.voxelset.data_keys
-                                )
-                            )
-                            else "no data"
-                        )
-                        for p in self._strat.partitions
-                        if hasattr(p, "voxelset")
-                    )
-                    + "\n".join(
-                        f"* {p.name} contains no voxelsets"
-                        for p in self._strat.partitions
-                        if not hasattr(p, "voxelset")
-                    )
-                )
-        try:
-            estimate = np.array(estimate)
-        except NameError:
-            # If `estimate` is undefined after all this then error out.
-            raise IndicatorError(
-                "No configuration indicators found for the number of"
-                + f"'{self._cell_type.name}' in '{self._strat.name}'"
-            )
-        if not np.allclose(estimate, estimate // 1):
-            # 1.2 cells == 0.8 probability for 1, 0.2 probability for 2
-            return (
-                np.floor(estimate) + (np.random.rand(estimate.size) < estimate % 1)
-            ).astype(int)
-        else:
-            return np.round(estimate).astype(int)
-
-    def _density_to_estim(self, density, chunk=None):
-        return sum(p.volume(chunk) * density for p in self._strat.partitions)
-
-    def _pdensity_to_estim(self, planar_density, chunk=None):
-        return sum(p.surface(chunk) * planar_density for p in self._strat.partitions)
-
-    def _estim_for_chunk(self, chunk, count):
-        if chunk is None:
-            return count
-        # When getting with absolute count for a chunk give back the count
-        # proportional to the volume in this chunk vs total volume
-        chunk_volume = sum(p.volume(chunk) for p in self._strat.partitions)
-        total_volume = sum(p.volume() for p in self._strat.partitions)
-        return count * chunk_volume / total_volume
-
-    def _estim_for_voxels(self, voxels, key):
-        return voxels.get_data(key).ravel().astype(float) * np.prod(
-            voxels.get_size_matrix(copy=False), axis=1
-        )
-
-
-__all__ = ["PlacementIndicator"]
+import typing
+
+import numpy as np
+
+from .. import config
+from ..config import refs, types
+from ..config._attrs import cfglist
+from ..exceptions import IndicatorError, PlacementError, PlacementRelationError
+from ..morphologies.selector import MorphologySelector
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+    from ..core import Scaffold
+
+
+@config.node
+class PlacementIndications:
+    scaffold: "Scaffold"
+    radius: float = config.attr(type=float)
+    density: float = config.attr(type=float)
+    planar_density: float = config.attr(type=float)
+    count_ratio: float = config.attr(type=float)
+    density_ratio: float = config.attr(type=float)
+    relative_to: "CellType" = config.ref(refs.cell_type_ref)
+    count: int = config.attr(type=int)
+    geometry: dict = config.dict(type=types.any_())
+    morphologies: cfglist[MorphologySelector] = config.list(type=MorphologySelector)
+    density_key: str = config.attr(type=str)
+
+
+class _Noner:
+    def __getattr__(self, attr):
+        return None
+
+
+class PlacementIndicator:
+    def __init__(self, strat, cell_type):
+        self._strat = strat
+        self._cell_type = cell_type
+
+    @property
+    def cell_type(self):
+        return self._cell_type
+
+    def get_radius(self):
+        return self.assert_indication("radius")
+
+    def use_morphologies(self):
+        return bool(self.indication("morphologies"))
+
+    def indication(self, attr):
+        ind_strat = self._strat.overrides.get(self._cell_type.name) or _Noner()
+        ind_ct = self._cell_type.spatial
+        strat = getattr(ind_strat, attr)
+        ct = getattr(ind_ct, attr)
+        if strat is not None:
+            return strat
+        return ct
+
+    def assert_indication(self, attr):
+        ind = self.indication(attr)
+        if ind is None:
+            raise IndicatorError(
+                f"No configuration indicators found for the {attr}"
+                f" of '{self._cell_type.name}' in '{self._strat.name}'"
+            )
+        return ind
+
+    def guess(self, chunk=None, voxels=None):
+        """
+        Estimate the count of cell to place based on the cell_type's PlacementIndications.
+        Float estimates are converted to int using an acceptance-rejection method.
+
+        :param chunk: if provided, will estimate the number of cell within the Chunk.
+        :type chunk: bsb.storage._chunks.Chunk
+        :param voxels: if provided, will estimate the number of cell within the VoxelSet.
+            Only for cells with the indication "density_key" set or with the indication
+            "relative_to" set and the target cell has the indication "density_key" set.
+        :type voxels: bsb.voxels.VoxelSet
+        :returns: Cell counts for each chunk or voxel.
+        :rtype: numpy.ndarray[int]
+        """
+        count = self.indication("count")
+        density = self.indication("density")
+        density_key = self.indication("density_key")
+        planar_density = self.indication("planar_density")
+        relative_to = self.indication("relative_to")
+        density_ratio = self.indication("density_ratio")
+        count_ratio = self.indication("count_ratio")
+        if count is not None:
+            estimate = self._estim_for_chunk(chunk, count)
+        if density is not None:
+            estimate = self._density_to_estim(density, chunk)
+        if planar_density is not None:
+            estimate = self._pdensity_to_estim(planar_density, chunk)
+        if relative_to is not None:
+            relation = relative_to
+            if count_ratio is not None:
+                strats = self._strat.scaffold.get_placement_of(relation)
+                estimate = (
+                    sum(
+                        PlacementIndicator(s, relation).guess(chunk, voxels)
+                        for s in strats
+                    )
+                    * count_ratio
+                )
+            elif density_ratio is not None:
+                # Create an indicator based on this strategy for the related CT.
+                # This means we'll read only the CT indications, and ignore any
+                # overrides of other strats, but one can set overrides for the
+                # related type in this strat.
+                rel_ind = PlacementIndicator(self._strat, relation)
+                rel_density = rel_ind.indication("density")
+                rel_pl_density = rel_ind.indication("planar_density")
+                rel_pl_density_key = rel_ind.indication("density_key")
+                if rel_density is not None:
+                    estimate = self._density_to_estim(rel_density * density_ratio, chunk)
+                elif rel_pl_density is not None:
+                    estimate = self._pdensity_to_estim(
+                        rel_pl_density * density_ratio, chunk
+                    )
+                elif rel_pl_density_key is not None:
+                    # Use the relation's `guess` to guess according to the relation's density key
+                    estimate = rel_ind.guess(chunk, voxels) * density_ratio
+                else:
+                    raise PlacementRelationError(
+                        f"{self.cell_type.name} requires relation {relation.name}"
+                        + " to specify density information."
+                    )
+            else:
+                raise PlacementError(
+                    f"Relation specified but no ratio indications provided."
+                )
+        if density_key is not None:
+            if voxels is None:
+                raise Exception("Can't guess voxel density without a voxelset.")
+            elif density_key in voxels.data_keys:
+                estimate = self._estim_for_voxels(voxels, density_key)
+            else:
+                raise RuntimeError(
+                    f"No voxel density data column '{density_key}' found in any of the"
+                    " following partitions:\n"
+                    + "\n".join(
+                        f"* {p.name}: "
+                        + (
+                            fstr
+                            if (
+                                fstr := ", ".join(
+                                    f"'{col}'" for col in p.voxelset.data_keys
+                                )
+                            )
+                            else "no data"
+                        )
+                        for p in self._strat.partitions
+                        if hasattr(p, "voxelset")
+                    )
+                    + "\n".join(
+                        f"* {p.name} contains no voxelsets"
+                        for p in self._strat.partitions
+                        if not hasattr(p, "voxelset")
+                    )
+                )
+        try:
+            estimate = np.array(estimate)
+        except NameError:
+            # If `estimate` is undefined after all this then error out.
+            raise IndicatorError(
+                "No configuration indicators found for the number of"
+                + f"'{self._cell_type.name}' in '{self._strat.name}'"
+            )
+        if not np.allclose(estimate, estimate // 1):
+            # 1.2 cells == 0.8 probability for 1, 0.2 probability for 2
+            return (
+                np.floor(estimate) + (np.random.rand(estimate.size) < estimate % 1)
+            ).astype(int)
+        else:
+            return np.round(estimate).astype(int)
+
+    def _density_to_estim(self, density, chunk=None):
+        return sum(p.volume(chunk) * density for p in self._strat.partitions)
+
+    def _pdensity_to_estim(self, planar_density, chunk=None):
+        return sum(p.surface(chunk) * planar_density for p in self._strat.partitions)
+
+    def _estim_for_chunk(self, chunk, count):
+        if chunk is None:
+            return count
+        # When getting with absolute count for a chunk give back the count
+        # proportional to the volume in this chunk vs total volume
+        chunk_volume = sum(p.volume(chunk) for p in self._strat.partitions)
+        total_volume = sum(p.volume() for p in self._strat.partitions)
+        return count * chunk_volume / total_volume
+
+    def _estim_for_voxels(self, voxels, key):
+        return voxels.get_data(key).ravel().astype(float) * np.prod(
+            voxels.get_size_matrix(copy=False), axis=1
+        )
+
+
+__all__ = ["PlacementIndicator"]
```

### Comparing `bsb_core-4.0.1/bsb/placement/random.py` & `bsb_core-4.1.0/bsb/placement/random.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,286 +1,286 @@
-import numpy as np
-
-from .. import config
-from ..exceptions import PackingError, PackingWarning
-from ..reporting import report, warn
-from ..voxels import VoxelSet
-from .strategy import PlacementStrategy
-
-
-class _VoxelBasedFiller:
-    """
-    Internal mixin for filler placement strategies
-    """
-
-    def _fill_system(self, chunk, indicators, check_pack=True):
-        voxels = VoxelSet.concatenate(
-            *(p.chunk_to_voxels(chunk) for p in self.partitions)
-        )
-        # Define the particles for the particle system.
-        particles = [
-            {
-                "name": name,
-                # Place particles in all voxels
-                "voxels": list(range(len(voxels))),
-                "radius": indicator.get_radius(),
-                # Indicator guesses either a global value, or a per voxel value.
-                "count": indicator.guess(chunk, voxels),
-            }
-            for name, indicator in indicators.items()
-        ]
-        # Create and fill the particle system.
-        system = VolumeFiller(track_displaced=False, scaffold=self.scaffold, strat=self)
-        system.fill(voxels, particles, check_pack=check_pack)
-        return system
-
-    def _extract_system(self, system, chunk, indicators):
-        if len(system.particles) == 0:
-            return
-
-        for pt in system.particle_types:
-            cell_type = self.scaffold.cell_types[pt["name"]]
-            indicator = indicators[pt["name"]]
-            particle_positions = [p.position for p in system.particles if p.type is pt]
-            if len(particle_positions) == 0:
-                continue
-            positions = np.empty((len(particle_positions), 3))
-            positions[:] = particle_positions
-            report(f"Placing {len(positions)} {cell_type.name} in {chunk}", level=3)
-            self.place_cells(indicator, positions, chunk)
-
-
-@config.node
-class RandomPlacement(PlacementStrategy, _VoxelBasedFiller):
-    """
-    Place cells in random positions.
-    """
-
-    def place(self, chunk, indicators):
-        system = self._fill_system(chunk, indicators, check_pack=True)
-        self._extract_system(system, chunk, indicators)
-
-
-class Particle:
-    def __init__(self, radius, position):
-        self.radius = radius
-        self.position = position
-
-
-class Neighbourhood:
-    def __init__(self, epicenter, neighbours, neighbour_radius, partners, partner_radius):
-        self.epicenter = epicenter
-        self.neighbours = neighbours
-        self.neighbour_radius = neighbour_radius
-        self.partners = partners
-        self.partner_radius = partner_radius
-
-    def get_overlap(self):
-        overlap = 0
-        neighbours = self.neighbours
-        n_neighbours = len(neighbours)
-        for partner in self.partners:
-            for neighbour in self.neighbours:
-                if partner.id == neighbour.id:
-                    continue
-                overlap -= min(
-                    0,
-                    distance(partner.position, neighbour.position)
-                    - partner.radius
-                    - neighbour.radius,
-                )
-        return overlap
-
-    def colliding(self):
-        neighbours = self.neighbours
-        for partner in self.partners:
-            for neighbour in self.neighbours:
-                if partner.id == neighbour.id:
-                    continue
-                if (
-                    distance(partner.position, neighbour.position)
-                    - partner.radius
-                    - neighbour.radius
-                    < 0
-                ):
-                    return True
-        return False
-
-
-class ParticleVoxel:
-    def __init__(self, origin, dimensions):
-        self.origin = np.array(origin)
-        self.size = np.array(dimensions)
-
-
-class VolumeFiller:
-    def __init__(self, track_displaced=False, scaffold=None, strat=None):
-        self.particle_types = []
-        self.voxels = []
-        self.track_displaced = track_displaced
-        self.scaffold = scaffold
-        self.strat = strat
-
-    def fill(self, voxels, particles, check_pack=True):
-        """
-        Fill a list of voxels with Particles.
-
-        :param bsb.voxels.VoxelSet voxels: List of voxels in which to place the particles.
-        :param List[dict] particles: List of dictionary for each particle to place.
-            Each dictionary needs to contain the "name" (str), "radius" (float) of particle.
-            It should also store the "count" of particle (int | List[int]) either in total or
-            for each voxel and "voxels" (List[int]) which gives in which voxel the placement will
-            append.
-        :param bool check_pack: If True, will check the packing factor before placing particles in
-            the voxels.
-        :raise PackingError: If check_pack is True and the resulting packing factor is greater than
-            0.4.
-        """
-        # Amount of spatial dimensions
-        self.dimensions = voxels.get_raw(copy=False).shape[1]
-        # Extend list of particle types in the system
-        self.particle_types.extend(particles)
-        # Max particle type radius
-        self.max_radius = max([pt["radius"] for pt in self.particle_types])
-        self.min_radius = min([pt["radius"] for pt in self.particle_types])
-        # Set initial radius for collision/rearrangement to 2 times the largest particle type radius
-        self.search_radius = self.max_radius * 2
-        # Create a list of voxels where the particles can be placed.
-        self.voxels.extend(
-            ParticleVoxel(ldc, size)
-            for ldc, size in zip(
-                voxels.as_spatial_coords(copy=False), voxels.get_size_matrix(copy=False)
-            )
-        )
-        if check_pack:
-            pf = self.get_packing_factor()
-            if self.strat is not None:
-                strat_name = type(self.strat).__name__
-            else:
-                strat_name = "particle system"
-            msg = f"Packing factor {round(pf, 2)}"
-            if pf > 0.4:
-                if pf > 0.64:
-                    msg += " exceeds geometrical maximum packing for spheres (0.64)"
-                elif pf > 0.4:
-                    msg += f" too high to resolve with {strat_name}"
-
-                count, pvol, vol = self._get_packing_factors()
-                raise PackingError(
-                    f"{msg}. Can not fit {round(count)} particles for a total of "
-                    f"{round(pvol, 3)}m micrometers into {round(vol, 3)}m."
-                )
-            elif pf > 0.2:
-                warn(
-                    f"{msg} is too high for good performance.",
-                    PackingWarning,
-                )
-        # Reset particles
-        self.particles = []
-        for particle_type in self.particle_types:
-            radius = particle_type["radius"]
-            count = particle_type["count"]
-            if count.size == 1:
-                self._fill_global(particle_type)
-            else:
-                self._fill_per_voxel(particle_type)
-
-    def _fill_per_voxel(self, particle_type):
-        voxel_counts = particle_type["count"]
-        radius = particle_type["radius"]
-        if len(voxel_counts) != len(self.voxels):
-            raise Exception(
-                f"Particle system voxel mismatch. Given {len(voxel_counts)} expected {len(self.voxels)}"
-            )
-        for voxel, count in zip(self.voxels, voxel_counts):
-            particle_type["placed"] = particle_type.get("placed", 0) + count
-            placement_matrix = np.random.rand(count, self.dimensions)
-            for in_voxel_pos in placement_matrix:
-                particle_position = voxel.origin + in_voxel_pos * voxel.size
-                self.add_particle(radius, particle_position, type=particle_type)
-
-    def _fill_global(self, particle_type):
-        particle_count = int(particle_type["count"])
-        particle_type["placed"] = particle_type.get("placed", 0) + particle_count
-        radius = particle_type["radius"]
-        # Generate a matrix with random positions for the particles
-        # Add an extra dimension to determine in which voxels to place the particles
-        placement_matrix = np.random.rand(particle_count, self.dimensions + 1)
-        # Generate each particle
-        for row in placement_matrix:
-            # Determine the voxel to be placed in.
-            voxel_id = int(row[0] * len(self.voxels))
-            voxel = self.voxels[voxel_id]
-            # Translate the particle into the voxel based on the remaining dimensions
-            particle_position = voxel.origin + row[1:] * voxel.size
-            # Store the particle object
-            self.add_particle(radius, particle_position, type=particle_type)
-
-    @property
-    def positions(self):
-        x = np.array([p.position for p in self.particles])
-        return x
-
-    def add_particle(self, radius, position, type=None):
-        particle = Particle(radius, position)
-        particle.id = len(self.particles)
-        particle.type = type
-        self.particles.append(particle)
-
-    def add_particles(self, radius, positions, type=None):
-        for position in positions:
-            self.add_particle(radius, position, type=type)
-
-    def remove_particles(self, particles_id):
-        # Remove particles with a certain id
-        for index in sorted(particles_id, reverse=True):
-            del self.particles[index]
-
-    def get_packing_factor(self, particles=None, volume=None):
-        """
-        Calculate the packing factor of the volume where particles will be placed.
-        It corresponds to the ratio of the sum of the particles' volume over the volume itself.
-
-        :param List[bsb.placement.particle.Particle] | None particles: List of Particle to place.
-            If None, it will use the ParticleSystem particle_types list.
-        :param float | None volume: Size of the volume in which the particles will be placed.
-            If None, it will use the total volume of the voxels of the ParticleSystem.
-        :return: Packing factor
-        :rtype: float
-        """
-        if particles is None:
-            particles_volume = sum(
-                np.sum(p["count"]) * sphere_volume(p["radius"])
-                for p in self.particle_types
-            )
-        else:
-            particles_volume = sum(sphere_volume(p.radius) for p in particles)
-        if volume is None:
-            volume = sum(
-                sum(np.prod(v.size) for v in np.array(self.voxels)[np.array(p["voxels"])])
-                for p in self.particle_types
-            )
-        return particles_volume / volume
-
-    def _get_packing_factors(self, particles=None, volume=None):
-        if particles is None:
-            particles_volume = np.sum(
-                [p["count"] * sphere_volume(p["radius"]) for p in self.particle_types]
-            )
-            particles_count = np.sum([p["count"] for p in self.particle_types])
-        else:
-            particles_volume = np.sum([sphere_volume(p.radius) for p in particles])
-            particles_count = len(particles)
-        if volume is None:
-            volume = np.sum([np.prod(v.size) for v in self.voxels])
-        return particles_count, particles_volume, volume
-
-
-def sphere_volume(radius):
-    return 4 / 3 * np.pi * radius**3
-
-
-def distance(a, b):
-    return np.sqrt(np.sum((b - a) ** 2))
-
-
-__all__ = ["RandomPlacement"]
+import numpy as np
+
+from .. import config
+from ..exceptions import PackingError, PackingWarning
+from ..reporting import report, warn
+from ..voxels import VoxelSet
+from .strategy import PlacementStrategy
+
+
+class _VoxelBasedFiller:
+    """
+    Internal mixin for filler placement strategies
+    """
+
+    def _fill_system(self, chunk, indicators, check_pack=True):
+        voxels = VoxelSet.concatenate(
+            *(p.chunk_to_voxels(chunk) for p in self.partitions)
+        )
+        # Define the particles for the particle system.
+        particles = [
+            {
+                "name": name,
+                # Place particles in all voxels
+                "voxels": list(range(len(voxels))),
+                "radius": indicator.get_radius(),
+                # Indicator guesses either a global value, or a per voxel value.
+                "count": indicator.guess(chunk, voxels),
+            }
+            for name, indicator in indicators.items()
+        ]
+        # Create and fill the particle system.
+        system = VolumeFiller(track_displaced=False, scaffold=self.scaffold, strat=self)
+        system.fill(voxels, particles, check_pack=check_pack)
+        return system
+
+    def _extract_system(self, system, chunk, indicators):
+        if len(system.particles) == 0:
+            return
+
+        for pt in system.particle_types:
+            cell_type = self.scaffold.cell_types[pt["name"]]
+            indicator = indicators[pt["name"]]
+            particle_positions = [p.position for p in system.particles if p.type is pt]
+            if len(particle_positions) == 0:
+                continue
+            positions = np.empty((len(particle_positions), 3))
+            positions[:] = particle_positions
+            report(f"Placing {len(positions)} {cell_type.name} in {chunk}", level=3)
+            self.place_cells(indicator, positions, chunk)
+
+
+@config.node
+class RandomPlacement(PlacementStrategy, _VoxelBasedFiller):
+    """
+    Place cells in random positions.
+    """
+
+    def place(self, chunk, indicators):
+        system = self._fill_system(chunk, indicators, check_pack=True)
+        self._extract_system(system, chunk, indicators)
+
+
+class Particle:
+    def __init__(self, radius, position):
+        self.radius = radius
+        self.position = position
+
+
+class Neighbourhood:
+    def __init__(self, epicenter, neighbours, neighbour_radius, partners, partner_radius):
+        self.epicenter = epicenter
+        self.neighbours = neighbours
+        self.neighbour_radius = neighbour_radius
+        self.partners = partners
+        self.partner_radius = partner_radius
+
+    def get_overlap(self):
+        overlap = 0
+        neighbours = self.neighbours
+        n_neighbours = len(neighbours)
+        for partner in self.partners:
+            for neighbour in self.neighbours:
+                if partner.id == neighbour.id:
+                    continue
+                overlap -= min(
+                    0,
+                    distance(partner.position, neighbour.position)
+                    - partner.radius
+                    - neighbour.radius,
+                )
+        return overlap
+
+    def colliding(self):
+        neighbours = self.neighbours
+        for partner in self.partners:
+            for neighbour in self.neighbours:
+                if partner.id == neighbour.id:
+                    continue
+                if (
+                    distance(partner.position, neighbour.position)
+                    - partner.radius
+                    - neighbour.radius
+                    < 0
+                ):
+                    return True
+        return False
+
+
+class ParticleVoxel:
+    def __init__(self, origin, dimensions):
+        self.origin = np.array(origin)
+        self.size = np.array(dimensions)
+
+
+class VolumeFiller:
+    def __init__(self, track_displaced=False, scaffold=None, strat=None):
+        self.particle_types = []
+        self.voxels = []
+        self.track_displaced = track_displaced
+        self.scaffold = scaffold
+        self.strat = strat
+
+    def fill(self, voxels, particles, check_pack=True):
+        """
+        Fill a list of voxels with Particles.
+
+        :param bsb.voxels.VoxelSet voxels: List of voxels in which to place the particles.
+        :param List[dict] particles: List of dictionary for each particle to place.
+            Each dictionary needs to contain the "name" (str), "radius" (float) of particle.
+            It should also store the "count" of particle (int | List[int]) either in total or
+            for each voxel and "voxels" (List[int]) which gives in which voxel the placement will
+            append.
+        :param bool check_pack: If True, will check the packing factor before placing particles in
+            the voxels.
+        :raise PackingError: If check_pack is True and the resulting packing factor is greater than
+            0.4.
+        """
+        # Amount of spatial dimensions
+        self.dimensions = voxels.get_raw(copy=False).shape[1]
+        # Extend list of particle types in the system
+        self.particle_types.extend(particles)
+        # Max particle type radius
+        self.max_radius = max([pt["radius"] for pt in self.particle_types])
+        self.min_radius = min([pt["radius"] for pt in self.particle_types])
+        # Set initial radius for collision/rearrangement to 2 times the largest particle type radius
+        self.search_radius = self.max_radius * 2
+        # Create a list of voxels where the particles can be placed.
+        self.voxels.extend(
+            ParticleVoxel(ldc, size)
+            for ldc, size in zip(
+                voxels.as_spatial_coords(copy=False), voxels.get_size_matrix(copy=False)
+            )
+        )
+        if check_pack:
+            pf = self.get_packing_factor()
+            if self.strat is not None:
+                strat_name = type(self.strat).__name__
+            else:
+                strat_name = "particle system"
+            msg = f"Packing factor {round(pf, 2)}"
+            if pf > 0.4:
+                if pf > 0.64:
+                    msg += " exceeds geometrical maximum packing for spheres (0.64)"
+                elif pf > 0.4:
+                    msg += f" too high to resolve with {strat_name}"
+
+                count, pvol, vol = self._get_packing_factors()
+                raise PackingError(
+                    f"{msg}. Can not fit {round(count)} particles for a total of "
+                    f"{round(pvol, 3)}m micrometers into {round(vol, 3)}m."
+                )
+            elif pf > 0.2:
+                warn(
+                    f"{msg} is too high for good performance.",
+                    PackingWarning,
+                )
+        # Reset particles
+        self.particles = []
+        for particle_type in self.particle_types:
+            radius = particle_type["radius"]
+            count = particle_type["count"]
+            if count.size == 1:
+                self._fill_global(particle_type)
+            else:
+                self._fill_per_voxel(particle_type)
+
+    def _fill_per_voxel(self, particle_type):
+        voxel_counts = particle_type["count"]
+        radius = particle_type["radius"]
+        if len(voxel_counts) != len(self.voxels):
+            raise Exception(
+                f"Particle system voxel mismatch. Given {len(voxel_counts)} expected {len(self.voxels)}"
+            )
+        for voxel, count in zip(self.voxels, voxel_counts):
+            particle_type["placed"] = particle_type.get("placed", 0) + count
+            placement_matrix = np.random.rand(count, self.dimensions)
+            for in_voxel_pos in placement_matrix:
+                particle_position = voxel.origin + in_voxel_pos * voxel.size
+                self.add_particle(radius, particle_position, type=particle_type)
+
+    def _fill_global(self, particle_type):
+        particle_count = int(particle_type["count"])
+        particle_type["placed"] = particle_type.get("placed", 0) + particle_count
+        radius = particle_type["radius"]
+        # Generate a matrix with random positions for the particles
+        # Add an extra dimension to determine in which voxels to place the particles
+        placement_matrix = np.random.rand(particle_count, self.dimensions + 1)
+        # Generate each particle
+        for row in placement_matrix:
+            # Determine the voxel to be placed in.
+            voxel_id = int(row[0] * len(self.voxels))
+            voxel = self.voxels[voxel_id]
+            # Translate the particle into the voxel based on the remaining dimensions
+            particle_position = voxel.origin + row[1:] * voxel.size
+            # Store the particle object
+            self.add_particle(radius, particle_position, type=particle_type)
+
+    @property
+    def positions(self):
+        x = np.array([p.position for p in self.particles])
+        return x
+
+    def add_particle(self, radius, position, type=None):
+        particle = Particle(radius, position)
+        particle.id = len(self.particles)
+        particle.type = type
+        self.particles.append(particle)
+
+    def add_particles(self, radius, positions, type=None):
+        for position in positions:
+            self.add_particle(radius, position, type=type)
+
+    def remove_particles(self, particles_id):
+        # Remove particles with a certain id
+        for index in sorted(particles_id, reverse=True):
+            del self.particles[index]
+
+    def get_packing_factor(self, particles=None, volume=None):
+        """
+        Calculate the packing factor of the volume where particles will be placed.
+        It corresponds to the ratio of the sum of the particles' volume over the volume itself.
+
+        :param List[bsb.placement.particle.Particle] | None particles: List of Particle to place.
+            If None, it will use the ParticleSystem particle_types list.
+        :param float | None volume: Size of the volume in which the particles will be placed.
+            If None, it will use the total volume of the voxels of the ParticleSystem.
+        :return: Packing factor
+        :rtype: float
+        """
+        if particles is None:
+            particles_volume = sum(
+                np.sum(p["count"]) * sphere_volume(p["radius"])
+                for p in self.particle_types
+            )
+        else:
+            particles_volume = sum(sphere_volume(p.radius) for p in particles)
+        if volume is None:
+            volume = sum(
+                sum(np.prod(v.size) for v in np.array(self.voxels)[np.array(p["voxels"])])
+                for p in self.particle_types
+            )
+        return particles_volume / volume
+
+    def _get_packing_factors(self, particles=None, volume=None):
+        if particles is None:
+            particles_volume = np.sum(
+                [p["count"] * sphere_volume(p["radius"]) for p in self.particle_types]
+            )
+            particles_count = np.sum([p["count"] for p in self.particle_types])
+        else:
+            particles_volume = np.sum([sphere_volume(p.radius) for p in particles])
+            particles_count = len(particles)
+        if volume is None:
+            volume = np.sum([np.prod(v.size) for v in self.voxels])
+        return particles_count, particles_volume, volume
+
+
+def sphere_volume(radius):
+    return 4 / 3 * np.pi * radius**3
+
+
+def distance(a, b):
+    return np.sqrt(np.sum((b - a) ** 2))
+
+
+__all__ = ["RandomPlacement"]
```

### Comparing `bsb_core-4.0.1/bsb/placement/strategy.py` & `bsb_core-4.1.0/bsb/placement/strategy.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,218 +1,218 @@
-import abc
-import itertools
-import typing
-
-import numpy as np
-
-from .. import config
-from .._util import obj_str_insert
-from ..config import refs, types
-from ..config._attrs import cfgdict
-from ..exceptions import DistributorError, EmptySelectionError
-from ..mixins import HasDependencies
-from ..profiling import node_meter
-from ..reporting import warn
-from ..storage._chunks import Chunk
-from ..voxels import VoxelSet
-from .distributor import DistributorsNode
-from .indicator import PlacementIndications, PlacementIndicator
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-    from ..core import Scaffold
-    from ..topology import Partition
-
-
-@config.dynamic(attr_name="strategy", required=True)
-class PlacementStrategy(abc.ABC, HasDependencies):
-    """
-    Quintessential interface of the placement module. Each placement strategy defines an
-    approach to placing neurons into a volume.
-    """
-
-    scaffold: "Scaffold"
-
-    name: str = config.attr(key=True)
-    cell_types: list["CellType"] = config.reflist(refs.cell_type_ref, required=True)
-    partitions: list["Partition"] = config.reflist(refs.partition_ref, required=True)
-    overrides: cfgdict["PlacementIndications"] = config.dict(type=PlacementIndications)
-    depends_on: list["PlacementStrategy"] = config.reflist(refs.placement_ref)
-    distribute: DistributorsNode = config.attr(
-        type=DistributorsNode, default=dict, call_default=True
-    )
-    indicator_class = PlacementIndicator
-
-    def __init_subclass__(cls, **kwargs):
-        super(cls, cls).__init_subclass__(**kwargs)
-        # Decorate subclasses to measure performance
-        node_meter("place")(cls)
-
-    def __hash__(self):
-        return id(self)
-
-    def __lt__(self, other):
-        # This comparison should sort placement strategies by name, via __repr__ below
-        return str(self) < str(other)
-
-    @obj_str_insert
-    def __repr__(self):
-        config_name = self.name
-        if not hasattr(self, "scaffold"):
-            return f"'{config_name}'"
-        part_str = ""
-        if len(self.partitions):
-            partition_names = [p.name for p in self.partitions]
-            part_str = f" into {partition_names}"
-        ct_names = [ct.name for ct in self.cell_types]
-        return f"'{config_name}', placing {ct_names}{part_str}"
-
-    @abc.abstractmethod
-    def place(self, chunk, indicators):
-        """
-        Central method of each placement strategy. Given a chunk, should fill that chunk
-        with cells by calling the scaffold's (available as ``self.scaffold``)
-        :func:`~bsb.core.Scaffold.place_cells` method.
-
-        :param chunk: Chunk to fill
-        :type chunk: bsb.storage._chunks.Chunk
-        :param indicators: Dictionary of each cell type to its PlacementIndicator
-        :type indicators: Mapping[str, bsb.placement.indicator.PlacementIndicator]
-        """
-        pass
-
-    def place_cells(self, indicator, positions, chunk, additional=None):
-        if additional is None:
-            additional = {}
-        if self.distribute._has_mdistr() or indicator.use_morphologies():
-            selector_error = None
-            try:
-                morphologies, rotations = self.distribute._specials(
-                    self.partitions, indicator, positions
-                )
-            except EmptySelectionError as e:
-                selector_error = ", ".join(str(s) for s in e.selectors)
-            if selector_error:
-                # Starting from Python 3.11, even though we raise from None, the original
-                # EmptySelectionError somehow still gets pickled and contains unpicklable
-                # elements. So we work around by raising here, outside of the exception
-                # context.
-                raise DistributorError(
-                    "Morphology distribution couldn't find any"
-                    + f" morphologies with the following selector(s): {selector_error}"
-                )
-        elif self.distribute._has_rdistr():
-            rotations = self.distribute(
-                "rotations", self.partitions, indicator, positions
-            )
-            morphologies = None
-        else:
-            morphologies, rotations = None, None
-
-        distr = self.distribute._curry(self.partitions, indicator, positions)
-        additional.update(
-            {prop: distr(prop) for prop in self.distribute.properties.keys()}
-        )
-        self.scaffold.place_cells(
-            indicator.cell_type,
-            positions=positions,
-            rotations=rotations,
-            morphologies=morphologies,
-            additional=additional,
-            chunk=chunk,
-        )
-
-    def queue(self, pool, chunk_size):
-        """
-        Specifies how to queue this placement strategy into a job pool. Can be overridden,
-        the default implementation asks each partition to chunk itself and creates 1
-        placement job per chunk.
-        """
-        # Get the queued jobs of all the strategies we depend on.
-        deps = set(
-            itertools.chain(
-                *(pool.get_submissions_of(strat) for strat in self.get_deps())
-            )
-        )
-        for p in self.partitions:
-            chunks = p.to_chunks(chunk_size)
-            for chunk in chunks:
-                job = pool.queue_placement(self, Chunk(chunk, chunk_size), deps=deps)
-
-    def is_entities(self):
-        return "entities" in self.__class__.__dict__ and self.__class__.entities
-
-    def get_indicators(self):
-        """
-        Return indicators per cell type. Indicators collect all configuration information
-        into objects that can produce guesses as to how many cells of a type should be
-        placed in a volume.
-        """
-        return {
-            ct.name: self.__class__.indicator_class(self, ct) for ct in self.cell_types
-        }
-
-    def guess_cell_count(self):
-        return sum(ind.guess() for ind in self.get_indicators().values())
-
-    def get_deps(self):
-        return set(self.depends_on)
-
-
-@config.node
-class FixedPositions(PlacementStrategy):
-    positions: np.ndarray = config.attr(type=types.ndarray())
-
-    def place(self, chunk, indicators):
-        if self.positions is None:
-            raise ValueError(
-                f"Please set `.positions` on '{self.name}' before placement."
-            )
-        if not len(self.positions):
-            warn(f"No positions given to {self.get_node_name()}.")
-            return
-        for indicator in indicators.values():
-            inside_chunk = VoxelSet([chunk], chunk.dimensions).inside(self.positions)
-            self.place_cells(indicator, self.positions[inside_chunk], chunk)
-
-    def guess_cell_count(self):
-        if self.positions is None:
-            raise ValueError(f"Please set `.positions` on '{self.name}'.")
-        return len(self.positions)
-
-    def queue(self, pool, chunk_size):
-        if self.positions is None:
-            raise ValueError(f"Please set `.positions` on '{self.name}'.")
-        # Get the queued jobs of all the strategies we depend on.
-        deps = set(
-            itertools.chain(
-                *(pool.get_submissions_of(strat) for strat in self.get_deps())
-            )
-        )
-        for chunk in VoxelSet.fill(self.positions, chunk_size):
-            pool.queue_placement(self, Chunk(chunk, chunk_size), deps=deps)
-
-
-@config.node
-class Entities(PlacementStrategy):
-    """
-    Implementation of the placement of entities that do not have a 3D position,
-    but that need to be connected with other cells of the network.
-    """
-
-    def queue(self, pool, chunk_size):
-        # Entities ignore chunks since they don't intrinsically store any data.
-        pool.queue_placement(self, Chunk([0, 0, 0], chunk_size))
-
-    def place(self, chunk, indicators):
-        for indicator in indicators.values():
-            cell_type = indicator.cell_type
-            # Guess total number, not chunk number, as entities bypass chunking.
-            n = sum(
-                # Pass the voxelset if it exists
-                np.sum(indicator.guess(voxels=getattr(p, "voxelset", None)))
-                for p in self.partitions
-            )
-            self.scaffold.create_entities(cell_type, n)
-
-
-__all__ = ["Entities", "FixedPositions", "PlacementStrategy"]
+import abc
+import itertools
+import typing
+
+import numpy as np
+
+from .. import config
+from .._util import obj_str_insert
+from ..config import refs, types
+from ..config._attrs import cfgdict
+from ..exceptions import DistributorError, EmptySelectionError
+from ..mixins import HasDependencies
+from ..profiling import node_meter
+from ..reporting import warn
+from ..storage._chunks import Chunk
+from ..voxels import VoxelSet
+from .distributor import DistributorsNode
+from .indicator import PlacementIndications, PlacementIndicator
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+    from ..core import Scaffold
+    from ..topology import Partition
+
+
+@config.dynamic(attr_name="strategy", required=True)
+class PlacementStrategy(abc.ABC, HasDependencies):
+    """
+    Quintessential interface of the placement module. Each placement strategy defines an
+    approach to placing neurons into a volume.
+    """
+
+    scaffold: "Scaffold"
+
+    name: str = config.attr(key=True)
+    cell_types: list["CellType"] = config.reflist(refs.cell_type_ref, required=True)
+    partitions: list["Partition"] = config.reflist(refs.partition_ref, required=True)
+    overrides: cfgdict["PlacementIndications"] = config.dict(type=PlacementIndications)
+    depends_on: list["PlacementStrategy"] = config.reflist(refs.placement_ref)
+    distribute: DistributorsNode = config.attr(
+        type=DistributorsNode, default=dict, call_default=True
+    )
+    indicator_class = PlacementIndicator
+
+    def __init_subclass__(cls, **kwargs):
+        super(cls, cls).__init_subclass__(**kwargs)
+        # Decorate subclasses to measure performance
+        node_meter("place")(cls)
+
+    def __hash__(self):
+        return id(self)
+
+    def __lt__(self, other):
+        # This comparison should sort placement strategies by name, via __repr__ below
+        return str(self) < str(other)
+
+    @obj_str_insert
+    def __repr__(self):
+        config_name = self.name
+        if not hasattr(self, "scaffold"):
+            return f"'{config_name}'"
+        part_str = ""
+        if len(self.partitions):
+            partition_names = [p.name for p in self.partitions]
+            part_str = f" into {partition_names}"
+        ct_names = [ct.name for ct in self.cell_types]
+        return f"'{config_name}', placing {ct_names}{part_str}"
+
+    @abc.abstractmethod
+    def place(self, chunk, indicators):
+        """
+        Central method of each placement strategy. Given a chunk, should fill that chunk
+        with cells by calling the scaffold's (available as ``self.scaffold``)
+        :func:`~bsb.core.Scaffold.place_cells` method.
+
+        :param chunk: Chunk to fill
+        :type chunk: bsb.storage._chunks.Chunk
+        :param indicators: Dictionary of each cell type to its PlacementIndicator
+        :type indicators: Mapping[str, bsb.placement.indicator.PlacementIndicator]
+        """
+        pass
+
+    def place_cells(self, indicator, positions, chunk, additional=None):
+        if additional is None:
+            additional = {}
+        if self.distribute._has_mdistr() or indicator.use_morphologies():
+            selector_error = None
+            try:
+                morphologies, rotations = self.distribute._specials(
+                    self.partitions, indicator, positions
+                )
+            except EmptySelectionError as e:
+                selector_error = ", ".join(str(s) for s in e.selectors)
+            if selector_error:
+                # Starting from Python 3.11, even though we raise from None, the original
+                # EmptySelectionError somehow still gets pickled and contains unpicklable
+                # elements. So we work around by raising here, outside of the exception
+                # context.
+                raise DistributorError(
+                    "Morphology distribution couldn't find any"
+                    + f" morphologies with the following selector(s): {selector_error}"
+                )
+        elif self.distribute._has_rdistr():
+            rotations = self.distribute(
+                "rotations", self.partitions, indicator, positions
+            )
+            morphologies = None
+        else:
+            morphologies, rotations = None, None
+
+        distr = self.distribute._curry(self.partitions, indicator, positions)
+        additional.update(
+            {prop: distr(prop) for prop in self.distribute.properties.keys()}
+        )
+        self.scaffold.place_cells(
+            indicator.cell_type,
+            positions=positions,
+            rotations=rotations,
+            morphologies=morphologies,
+            additional=additional,
+            chunk=chunk,
+        )
+
+    def queue(self, pool, chunk_size):
+        """
+        Specifies how to queue this placement strategy into a job pool. Can be overridden,
+        the default implementation asks each partition to chunk itself and creates 1
+        placement job per chunk.
+        """
+        # Get the queued jobs of all the strategies we depend on.
+        deps = set(
+            itertools.chain(
+                *(pool.get_submissions_of(strat) for strat in self.get_deps())
+            )
+        )
+        for p in self.partitions:
+            chunks = p.to_chunks(chunk_size)
+            for chunk in chunks:
+                job = pool.queue_placement(self, Chunk(chunk, chunk_size), deps=deps)
+
+    def is_entities(self):
+        return "entities" in self.__class__.__dict__ and self.__class__.entities
+
+    def get_indicators(self):
+        """
+        Return indicators per cell type. Indicators collect all configuration information
+        into objects that can produce guesses as to how many cells of a type should be
+        placed in a volume.
+        """
+        return {
+            ct.name: self.__class__.indicator_class(self, ct) for ct in self.cell_types
+        }
+
+    def guess_cell_count(self):
+        return sum(ind.guess() for ind in self.get_indicators().values())
+
+    def get_deps(self):
+        return set(self.depends_on)
+
+
+@config.node
+class FixedPositions(PlacementStrategy):
+    positions: np.ndarray = config.attr(type=types.ndarray())
+
+    def place(self, chunk, indicators):
+        if self.positions is None:
+            raise ValueError(
+                f"Please set `.positions` on '{self.name}' before placement."
+            )
+        if not len(self.positions):
+            warn(f"No positions given to {self.get_node_name()}.")
+            return
+        for indicator in indicators.values():
+            inside_chunk = VoxelSet([chunk], chunk.dimensions).inside(self.positions)
+            self.place_cells(indicator, self.positions[inside_chunk], chunk)
+
+    def guess_cell_count(self):
+        if self.positions is None:
+            raise ValueError(f"Please set `.positions` on '{self.name}'.")
+        return len(self.positions)
+
+    def queue(self, pool, chunk_size):
+        if self.positions is None:
+            raise ValueError(f"Please set `.positions` on '{self.name}'.")
+        # Get the queued jobs of all the strategies we depend on.
+        deps = set(
+            itertools.chain(
+                *(pool.get_submissions_of(strat) for strat in self.get_deps())
+            )
+        )
+        for chunk in VoxelSet.fill(self.positions, chunk_size):
+            pool.queue_placement(self, Chunk(chunk, chunk_size), deps=deps)
+
+
+@config.node
+class Entities(PlacementStrategy):
+    """
+    Implementation of the placement of entities that do not have a 3D position,
+    but that need to be connected with other cells of the network.
+    """
+
+    def queue(self, pool, chunk_size):
+        # Entities ignore chunks since they don't intrinsically store any data.
+        pool.queue_placement(self, Chunk([0, 0, 0], chunk_size))
+
+    def place(self, chunk, indicators):
+        for indicator in indicators.values():
+            cell_type = indicator.cell_type
+            # Guess total number, not chunk number, as entities bypass chunking.
+            n = sum(
+                # Pass the voxelset if it exists
+                np.sum(indicator.guess(voxels=getattr(p, "voxelset", None)))
+                for p in self.partitions
+            )
+            self.scaffold.create_entities(cell_type, n)
+
+
+__all__ = ["Entities", "FixedPositions", "PlacementStrategy"]
```

### Comparing `bsb_core-4.0.1/bsb/profiling.py` & `bsb_core-4.1.0/bsb/profiling.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,185 +1,185 @@
-import atexit
-import cProfile
-import functools
-import pickle
-import sys
-import traceback
-import warnings
-from functools import cache
-from time import time
-from uuid import uuid4
-
-from .services import MPI
-
-
-class Meter:
-    def __init__(self, name, data):
-        self.name = name
-        self.data = data
-        self._starts = []
-        self._stops = []
-
-    def __enter__(self):
-        self.start()
-
-    def start(self):
-        self._starts.append(time())
-
-    def __exit__(self, exc, exc_type, tb):
-        self.stop()
-        self._exc = exc, exc_type, traceback.format_tb(tb)
-
-    def stop(self):
-        self._stops.append(time())
-
-    @property
-    def calls(self):
-        return len(self._starts)
-
-    @property
-    def elapsed(self):
-        return sum(e - s for s, e in zip(self._starts, self._stops))
-
-
-class ProfilingSession:
-    def __init__(self):
-        self._started = False
-        self._meters = []
-        self.name = "bsb_profiling"
-        self._current_f = None
-        self._flushcounter = 0
-
-    def set_name(self, name):
-        self.name = name
-
-    @property
-    def meters(self):
-        return self._meters.copy()
-
-    def start(self):
-        if not self._started:
-            self._started = True
-            self.profile = cProfile.Profile()
-            self.profile.enable()
-            atexit.register(self.flush)
-
-    def stop(self):
-        if self._started:
-            self._started = False
-            self.profile.disable()
-            atexit.unregister(self.flush)
-
-    def meter(self, name, **data):
-        meter = Meter(name, data)
-        self._meters.append(meter)
-        return meter
-
-    def flush(self, stats=True):
-        profile = self.profile
-        if self._current_f is None:
-            uuid = uuid4()
-            self._current_f = f"{self.name}_{MPI.get_rank()}_{uuid}"
-        if stats:
-            self.profile.dump_stats(f"{self._current_f}_{self._flushcounter}.prf")
-            self._flushcounter += 1
-        try:
-            del self.profile
-            with open(f"{self._current_f}.pkl", "wb") as f:
-                pickle.dump(self, f)
-        except Exception as e:
-            warnings.warn(f"Could not store profile: {e}")
-        finally:
-            self.profile = profile
-
-    def view(self):
-        try:
-            from snakeviz.cli import main as snakeviz
-        except ImportError:
-            raise ImportError("Please `pip install snakeviz` to view profiles.") from None
-
-        args = sys.argv
-        if self._current_f is None:
-            self.flush()
-        sys.argv = ["snakeviz", f"{self._current_f}.prf"]
-
-        snakeviz()
-
-        sys.argv = args
-
-    @staticmethod
-    def load(fstem):
-        with open(f"{fstem}.pkl", "rb") as f:
-            return pickle.load(f)
-
-
-@cache
-def get_active_session():
-    return ProfilingSession()
-
-
-def activate_session(name=None):
-    session = get_active_session()
-    if name is not None:
-        session.set_name(name)
-    session.start()
-    return session
-
-
-def node_meter(*methods):
-    def get_node_method_name(method, args, kwargs):
-        return (
-            f"{args[0].get_node_name()}[{args[0].__class__.__name__}].{method.__name__}"
-        )
-
-    def decorator(node_cls):
-        for method_name in methods:
-            if method := getattr(node_cls, method_name, None):
-                setattr(node_cls, method_name, meter(method, name_f=get_node_method_name))
-
-        return node_cls
-
-    return decorator
-
-
-def meter(f=None, *, name_f=None):
-    def decorated(*args, **kwargs):
-        import bsb.options
-
-        if bsb.options.profiling:
-            session = get_active_session()
-            if name_f:
-                name = name_f(f, args, kwargs)
-            else:
-                name = f.__name__
-            with session.meter(name, args=str(args), kwargs=str(kwargs)):
-                r = f(*args, **kwargs)
-            session.flush(stats=False)
-            return r
-        else:
-            return f(*args, **kwargs)
-
-    if f is None:
-
-        def decorator(g):
-            nonlocal f
-            f = g
-            return functools.wraps(f)(decorated)
-
-        return decorator
-    else:
-        return functools.wraps(f)(decorated)
-
-
-def view_profile(fstem):
-    ProfilingSession.load(fstem).view()
-
-
-__all__ = [
-    "Meter",
-    "ProfilingSession",
-    "activate_session",
-    "get_active_session",
-    "meter",
-    "node_meter",
-    "view_profile",
-]
+import atexit
+import cProfile
+import functools
+import pickle
+import sys
+import traceback
+import warnings
+from functools import cache
+from time import time
+from uuid import uuid4
+
+from .services import MPI
+
+
+class Meter:
+    def __init__(self, name, data):
+        self.name = name
+        self.data = data
+        self._starts = []
+        self._stops = []
+
+    def __enter__(self):
+        self.start()
+
+    def start(self):
+        self._starts.append(time())
+
+    def __exit__(self, exc, exc_type, tb):
+        self.stop()
+        self._exc = exc, exc_type, traceback.format_tb(tb)
+
+    def stop(self):
+        self._stops.append(time())
+
+    @property
+    def calls(self):
+        return len(self._starts)
+
+    @property
+    def elapsed(self):
+        return sum(e - s for s, e in zip(self._starts, self._stops))
+
+
+class ProfilingSession:
+    def __init__(self):
+        self._started = False
+        self._meters = []
+        self.name = "bsb_profiling"
+        self._current_f = None
+        self._flushcounter = 0
+
+    def set_name(self, name):
+        self.name = name
+
+    @property
+    def meters(self):
+        return self._meters.copy()
+
+    def start(self):
+        if not self._started:
+            self._started = True
+            self.profile = cProfile.Profile()
+            self.profile.enable()
+            atexit.register(self.flush)
+
+    def stop(self):
+        if self._started:
+            self._started = False
+            self.profile.disable()
+            atexit.unregister(self.flush)
+
+    def meter(self, name, **data):
+        meter = Meter(name, data)
+        self._meters.append(meter)
+        return meter
+
+    def flush(self, stats=True):
+        profile = self.profile
+        if self._current_f is None:
+            uuid = uuid4()
+            self._current_f = f"{self.name}_{MPI.get_rank()}_{uuid}"
+        if stats:
+            self.profile.dump_stats(f"{self._current_f}_{self._flushcounter}.prf")
+            self._flushcounter += 1
+        try:
+            del self.profile
+            with open(f"{self._current_f}.pkl", "wb") as f:
+                pickle.dump(self, f)
+        except Exception as e:
+            warnings.warn(f"Could not store profile: {e}")
+        finally:
+            self.profile = profile
+
+    def view(self):
+        try:
+            from snakeviz.cli import main as snakeviz
+        except ImportError:
+            raise ImportError("Please `pip install snakeviz` to view profiles.") from None
+
+        args = sys.argv
+        if self._current_f is None:
+            self.flush()
+        sys.argv = ["snakeviz", f"{self._current_f}.prf"]
+
+        snakeviz()
+
+        sys.argv = args
+
+    @staticmethod
+    def load(fstem):
+        with open(f"{fstem}.pkl", "rb") as f:
+            return pickle.load(f)
+
+
+@cache
+def get_active_session():
+    return ProfilingSession()
+
+
+def activate_session(name=None):
+    session = get_active_session()
+    if name is not None:
+        session.set_name(name)
+    session.start()
+    return session
+
+
+def node_meter(*methods):
+    def get_node_method_name(method, args, kwargs):
+        return (
+            f"{args[0].get_node_name()}[{args[0].__class__.__name__}].{method.__name__}"
+        )
+
+    def decorator(node_cls):
+        for method_name in methods:
+            if method := getattr(node_cls, method_name, None):
+                setattr(node_cls, method_name, meter(method, name_f=get_node_method_name))
+
+        return node_cls
+
+    return decorator
+
+
+def meter(f=None, *, name_f=None):
+    def decorated(*args, **kwargs):
+        import bsb.options
+
+        if bsb.options.profiling:
+            session = get_active_session()
+            if name_f:
+                name = name_f(f, args, kwargs)
+            else:
+                name = f.__name__
+            with session.meter(name, args=str(args), kwargs=str(kwargs)):
+                r = f(*args, **kwargs)
+            session.flush(stats=False)
+            return r
+        else:
+            return f(*args, **kwargs)
+
+    if f is None:
+
+        def decorator(g):
+            nonlocal f
+            f = g
+            return functools.wraps(f)(decorated)
+
+        return decorator
+    else:
+        return functools.wraps(f)(decorated)
+
+
+def view_profile(fstem):
+    ProfilingSession.load(fstem).view()
+
+
+__all__ = [
+    "Meter",
+    "ProfilingSession",
+    "activate_session",
+    "get_active_session",
+    "meter",
+    "node_meter",
+    "view_profile",
+]
```

### Comparing `bsb_core-4.0.1/bsb/reporting.py` & `bsb_core-4.1.0/bsb/reporting.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,72 +1,72 @@
-import sys
-import warnings
-
-
-def in_notebook():
-    try:
-        from IPython import get_ipython
-
-        if "IPKernelApp" not in get_ipython().config:  # pragma: no cover
-            return False
-    except ImportError:
-        return False
-    except AttributeError:
-        return False
-    return True
-
-
-def in_pytest():
-    return "pytest" in sys.modules
-
-
-def report(*message, level=2, ongoing=False, nodes=None, all_nodes=False):
-    """
-    Send a message to the appropriate output channel.
-
-    :param message: Text message to send.
-    :type message: str
-    :param level: Verbosity level of the message.
-    :type level: int
-    :param ongoing: The message is part of an ongoing progress report.
-    :type ongoing: bool
-    """
-    from . import options
-    from .services import MPI
-
-    message = " ".join(map(str, message))
-    rank = MPI.get_rank()
-    if (
-        (not rank and nodes is None) or all_nodes or (nodes is not None and rank in nodes)
-    ) and options.verbosity >= level:
-        print(message, end="\n" if not ongoing else "\r", flush=True)
-
-
-def warn(message, category=None, stacklevel=2, log_exc=None):
-    """
-    Send a warning.
-
-    :param message: Warning message
-    :type message: str
-    :param category: The class of the warning.
-    """
-    from . import options
-
-    if log_exc:
-        import traceback
-
-        from .storage._util import cache
-
-        log = f"{message}\n\n{traceback.format_exception(type(log_exc), log_exc, log_exc.__traceback__)}"
-        id = cache.files.store(log)
-        path = cache.files.id_to_file_path(id)
-        message += f" See '{path}' for full error log."
-
-    # Avoid infinite loop looking up verbosity when verbosity option is broken.
-    if "Error retrieving option 'verbosity'" in message or options.verbosity > 0:
-        warnings.warn(message, category, stacklevel=stacklevel)
-
-
-__all__ = [
-    "report",
-    "warn",
-]
+import sys
+import warnings
+
+
+def in_notebook():
+    try:
+        from IPython import get_ipython
+
+        if "IPKernelApp" not in get_ipython().config:  # pragma: no cover
+            return False
+    except ImportError:
+        return False
+    except AttributeError:
+        return False
+    return True
+
+
+def in_pytest():
+    return "pytest" in sys.modules
+
+
+def report(*message, level=2, ongoing=False, nodes=None, all_nodes=False):
+    """
+    Send a message to the appropriate output channel.
+
+    :param message: Text message to send.
+    :type message: str
+    :param level: Verbosity level of the message.
+    :type level: int
+    :param ongoing: The message is part of an ongoing progress report.
+    :type ongoing: bool
+    """
+    from . import options
+    from .services import MPI
+
+    message = " ".join(map(str, message))
+    rank = MPI.get_rank()
+    if (
+        (not rank and nodes is None) or all_nodes or (nodes is not None and rank in nodes)
+    ) and options.verbosity >= level:
+        print(message, end="\n" if not ongoing else "\r", flush=True)
+
+
+def warn(message, category=None, stacklevel=2, log_exc=None):
+    """
+    Send a warning.
+
+    :param message: Warning message
+    :type message: str
+    :param category: The class of the warning.
+    """
+    from . import options
+
+    if log_exc:
+        import traceback
+
+        from .storage._util import cache
+
+        log = f"{message}\n\n{traceback.format_exception(type(log_exc), log_exc, log_exc.__traceback__)}"
+        id = cache.files.store(log)
+        path = cache.files.id_to_file_path(id)
+        message += f" See '{path}' for full error log."
+
+    # Avoid infinite loop looking up verbosity when verbosity option is broken.
+    if "Error retrieving option 'verbosity'" in message or options.verbosity > 0:
+        warnings.warn(message, category, stacklevel=stacklevel)
+
+
+__all__ = [
+    "report",
+    "warn",
+]
```

### Comparing `bsb_core-4.0.1/bsb/services/__init__.py` & `bsb_core-4.1.0/bsb/services/__init__.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,36 +1,36 @@
-"""
-Service module. Register or access interfaces that may be provided, mocked or missing, but
-should always behave neatly on import.
-"""
-
-from ._util import ErrorModule as _ErrorModule
-from .mpi import MPIService as _MPIService
-from .mpilock import MPILockModule as _MPILockModule
-
-MPI = _MPIService()
-"""
-MPI service
-"""
-MPILock = _MPILockModule("mpilock")
-"""
-MPILock service
-"""
-
-from .pool import JobPool as _JobPool  # noqa
-from .pool import WorkflowError
-
-JobPool = _JobPool
-"""
-JobPool service
-"""
-
-
-def __getattr__(attr):
-    return _ErrorModule(f"{attr} is not a registered service.")
-
-
-def register_service(attr, provider):
-    globals()[attr] = provider
-
-
-__all__ = ["MPI", "MPILock", "JobPool", "register_service", "WorkflowError"]
+"""
+Service module. Register or access interfaces that may be provided, mocked or missing, but
+should always behave neatly on import.
+"""
+
+from ._util import ErrorModule as _ErrorModule
+from .mpi import MPIService as _MPIService
+from .mpilock import MPILockModule as _MPILockModule
+
+MPI = _MPIService()
+"""
+MPI service
+"""
+MPILock = _MPILockModule("mpilock")
+"""
+MPILock service
+"""
+
+from .pool import JobPool as _JobPool  # noqa
+from .pool import WorkflowError
+
+JobPool = _JobPool
+"""
+JobPool service
+"""
+
+
+def __getattr__(attr):
+    return _ErrorModule(f"{attr} is not a registered service.")
+
+
+def register_service(attr, provider):
+    globals()[attr] = provider
+
+
+__all__ = ["MPI", "MPILock", "JobPool", "register_service", "WorkflowError"]
```

### Comparing `bsb_core-4.0.1/bsb/services/mpi.py` & `bsb_core-4.1.0/bsb/services/mpi.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,70 +1,70 @@
-import functools
-import os
-
-from ..exceptions import DependencyError
-from ._util import MockModule
-
-
-class MPIService:
-    def __init__(self):
-        self._mpi = MPIModule("mpi4py.MPI")
-        self._comm = self._mpi.COMM_WORLD
-
-    def get_communicator(self):
-        return self._comm
-
-    def get_rank(self):
-        if self._comm:
-            return self._comm.Get_rank()
-        return 0
-
-    def get_size(self):
-        if self._comm:
-            return self._comm.Get_size()
-        return 1
-
-    def barrier(self):
-        if self._comm:
-            return self._comm.Barrier()
-        pass
-
-    def abort(self, errorcode=1):
-        if self._comm:
-            return self._comm.Abort(errorcode)
-        print("MPI Abort called on MockCommunicator", flush=True)
-        exit(errorcode)
-
-    def bcast(self, obj, root=0):
-        if self._comm:
-            return self._comm.bcast(obj, root=root)
-        return obj
-
-    def gather(self, obj, root=0):
-        if self._comm:
-            return self._comm.gather(obj, root=root)
-        return [obj]
-
-    def allgather(self, obj):
-        if self._comm:
-            return self._comm.allgather(obj)
-        return [obj]
-
-
-class MPIModule(MockModule):
-    """
-    Module provider of the MPI interface.
-    """
-
-    @property
-    @functools.cache
-    def COMM_WORLD(self):
-        if (
-            any("mpi" in key.lower() for key in os.environ)
-            and "BSB_IGNORE_PARALLEL" not in os.environ
-        ):
-            raise DependencyError(
-                "MPI runtime detected without parallel support."
-                + " Please install with `pip install bsb[parallel]`."
-                + " Set `BSB_IGNORE_PARALLEL` to ignore this error."
-            )
-        return None
+import functools
+import os
+
+from ..exceptions import DependencyError
+from ._util import MockModule
+
+
+class MPIService:
+    def __init__(self):
+        self._mpi = MPIModule("mpi4py.MPI")
+        self._comm = self._mpi.COMM_WORLD
+
+    def get_communicator(self):
+        return self._comm
+
+    def get_rank(self):
+        if self._comm:
+            return self._comm.Get_rank()
+        return 0
+
+    def get_size(self):
+        if self._comm:
+            return self._comm.Get_size()
+        return 1
+
+    def barrier(self):
+        if self._comm:
+            return self._comm.Barrier()
+        pass
+
+    def abort(self, errorcode=1):
+        if self._comm:
+            return self._comm.Abort(errorcode)
+        print("MPI Abort called on MockCommunicator", flush=True)
+        exit(errorcode)
+
+    def bcast(self, obj, root=0):
+        if self._comm:
+            return self._comm.bcast(obj, root=root)
+        return obj
+
+    def gather(self, obj, root=0):
+        if self._comm:
+            return self._comm.gather(obj, root=root)
+        return [obj]
+
+    def allgather(self, obj):
+        if self._comm:
+            return self._comm.allgather(obj)
+        return [obj]
+
+
+class MPIModule(MockModule):
+    """
+    Module provider of the MPI interface.
+    """
+
+    @property
+    @functools.cache
+    def COMM_WORLD(self):
+        if (
+            any("mpi" in key.lower() for key in os.environ)
+            and "BSB_IGNORE_PARALLEL" not in os.environ
+        ):
+            raise DependencyError(
+                "MPI runtime detected without parallel support."
+                + " Please install with `pip install bsb[parallel]`."
+                + " Set `BSB_IGNORE_PARALLEL` to ignore this error."
+            )
+        return None
```

### Comparing `bsb_core-4.0.1/bsb/services/mpilock.py` & `bsb_core-4.1.0/bsb/services/mpilock.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,126 +1,126 @@
-# TODO: check for parallel support in the hdf5 provider, if it has it, provide noop
-
-from ._util import MockModule
-
-
-class MockedWindowController:
-    def __init__(self, comm=None, master=0):
-        from . import MPI
-
-        if comm is None:
-            comm = MPI
-        self._comm = comm
-        self._size = comm.get_size()
-        self._rank = comm.get_rank()
-        self._master = master
-        self._mocked = True
-        self._closed = False
-
-    @property
-    def master(self):
-        return self._master
-
-    @property
-    def rank(self):
-        return self._rank
-
-    @property
-    def closed(self):
-        return self._closed
-
-    def close(self):
-        self._closed = True
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, *args):
-        self.close()
-
-    def read(self):
-        return _NoopLock()
-
-    def write(self):
-        return _NoopLock()
-
-    def single_write(self, handle=None, rank=None):
-        if rank is None:
-            rank = self._master
-        fence = Fence(self._rank == rank)
-        if self._rank == rank:
-            return _NoopLock(handle=handle, fence=fence)
-        elif handle:
-            return _NoHandle()
-        else:
-            return fence
-
-
-class _NoopLock:
-    def __init__(self, handle=None, fence=None):
-        self._locked = 0
-        self._mocked = True
-        self._handle = handle
-        self._fence = fence
-
-    def locked(self):
-        return self._locked > 0
-
-    def __enter__(self):
-        self._locked += 1
-        return self._acquire_lock()
-
-    def _acquire_lock(self):
-        if self._handle is not None:
-            return self._handle
-        elif self._fence is not None:
-            return self._fence
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        self._locked -= 1
-
-
-class Fence:
-    def __init__(self, access):
-        self._access = access
-        self._obj = None
-        self._mocked = True
-
-    def guard(self):
-        if not self._access:
-            raise FencedSignal()
-
-    def share(self, obj):
-        self._obj = obj
-
-    def collect(self):
-        return self._obj
-
-    def __enter__(self):
-        return self
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        if exc_type is FencedSignal:
-            return True
-
-
-class _NoHandle:
-    def __init__(self):
-        self._mocked = True
-
-    def __enter__(self):
-        return None
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        pass
-
-
-class FencedSignal(Exception):
-    pass
-
-
-class MPILockModule(MockModule):
-    def sync(self, comm=None, master=0):
-        return MockedWindowController(comm, master)
-
-    def __call__(self, comm=None, master=None):
-        return MockedWindowController(comm, master)
+# TODO: check for parallel support in the hdf5 provider, if it has it, provide noop
+
+from ._util import MockModule
+
+
+class MockedWindowController:
+    def __init__(self, comm=None, master=0):
+        from . import MPI
+
+        if comm is None:
+            comm = MPI
+        self._comm = comm
+        self._size = comm.get_size()
+        self._rank = comm.get_rank()
+        self._master = master
+        self._mocked = True
+        self._closed = False
+
+    @property
+    def master(self):
+        return self._master
+
+    @property
+    def rank(self):
+        return self._rank
+
+    @property
+    def closed(self):
+        return self._closed
+
+    def close(self):
+        self._closed = True
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, *args):
+        self.close()
+
+    def read(self):
+        return _NoopLock()
+
+    def write(self):
+        return _NoopLock()
+
+    def single_write(self, handle=None, rank=None):
+        if rank is None:
+            rank = self._master
+        fence = Fence(self._rank == rank)
+        if self._rank == rank:
+            return _NoopLock(handle=handle, fence=fence)
+        elif handle:
+            return _NoHandle()
+        else:
+            return fence
+
+
+class _NoopLock:
+    def __init__(self, handle=None, fence=None):
+        self._locked = 0
+        self._mocked = True
+        self._handle = handle
+        self._fence = fence
+
+    def locked(self):
+        return self._locked > 0
+
+    def __enter__(self):
+        self._locked += 1
+        return self._acquire_lock()
+
+    def _acquire_lock(self):
+        if self._handle is not None:
+            return self._handle
+        elif self._fence is not None:
+            return self._fence
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        self._locked -= 1
+
+
+class Fence:
+    def __init__(self, access):
+        self._access = access
+        self._obj = None
+        self._mocked = True
+
+    def guard(self):
+        if not self._access:
+            raise FencedSignal()
+
+    def share(self, obj):
+        self._obj = obj
+
+    def collect(self):
+        return self._obj
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        if exc_type is FencedSignal:
+            return True
+
+
+class _NoHandle:
+    def __init__(self):
+        self._mocked = True
+
+    def __enter__(self):
+        return None
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        pass
+
+
+class FencedSignal(Exception):
+    pass
+
+
+class MPILockModule(MockModule):
+    def sync(self, comm=None, master=0):
+        return MockedWindowController(comm, master)
+
+    def __call__(self, comm=None, master=None):
+        return MockedWindowController(comm, master)
```

### Comparing `bsb_core-4.0.1/bsb/services/pool.py` & `bsb_core-4.1.0/bsb/services/pool.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,848 +1,848 @@
-"""
-Job pooling module.
-
-Jobs derive from the base :class:`.Job` class which can be put on the queue of a
-:class:`.JobPool`. In order to submit themselves to the pool Jobs will
-:meth:`~.Job.serialize` themselves into a predefined set of variables::
-
-   job.serialize() -> (job_type, f, args, kwargs)
-
-* ``job_type`` should be a string that is a class name defined in this module.
-   (e.g. ``"PlacementJob"``)
-
-* ``f`` should be the function object that the job's ``execute`` method should
-   execute.
-
-* ``args`` and ``kwargs`` are the args to be passed to that ``f``.
-
-The :meth:`.Job.execute` handler can help interpret ``args`` and ``kwargs``
-before running ``f``. The execute handler has access to the scaffold on the MPI
-process so one best serializes just the name of some part of the configuration,
-rather than trying to pickle the complex objects. For example, the
-:class:`.PlacementJob` uses the first ``args`` element to store the
-:class:`~bsb.placement.strategy.PlacementStrategy` name and then retrieve it from the
-scaffold:
-
-.. code-block:: python
-
-    @staticmethod
-    def execute(job_owner, f, args, kwargs):
-        placement = job_owner.placement[args[0]]
-        indicators = placement.get_indicators()
-        return f(placement, *args[1:], indicators, **kwargs)
-
-A job has a couple of display variables that can be set: ``_cname`` for the
-class name, ``_name`` for the job name and ``_c`` for the chunk. These are used
-to display what the workers are doing during parallel execution. This is an experimental
-API and subject to sudden change in the future.
-
-"""
-
-import abc
-import concurrent.futures
-import logging
-import pickle
-import tempfile
-import threading
-import typing
-import warnings
-from contextlib import ExitStack
-from enum import Enum, auto
-
-from exceptiongroup import ExceptionGroup
-
-from .._util import obj_str_insert
-from ..exceptions import (
-    JobCancelledError,
-    JobPoolContextError,
-    JobPoolError,
-    JobSchedulingError,
-)
-from . import MPI
-from ._util import ErrorModule, MockModule
-
-if typing.TYPE_CHECKING:
-    from mpipool import MPIExecutor
-
-
-class WorkflowError(ExceptionGroup):
-    pass
-
-
-class JobErroredError(Exception):
-    def __init__(self, message, error):
-        super().__init__(message)
-        self.error = error
-
-
-class JobStatus(Enum):
-    # Job has not been queued yet, waiting for dependencies to resolve.
-    PENDING = "pending"
-    # Job is on the queue.
-    QUEUED = "queued"
-    # Job is currently running on a worker.
-    RUNNING = "running"
-    # Job ran successfully.
-    SUCCESS = "success"
-    # Job failed (an exception was raised).
-    FAILED = "failed"
-    # Job was cancelled before it started running.
-    CANCELLED = "cancelled"
-    # Job was killed for some reason.
-    ABORTED = "aborted"
-
-
-class PoolStatus(Enum):
-    # Pool has been initialized and jobs can be scheduled.
-    SCHEDULING = "scheduling"
-    # Pool started execution.
-    EXECUTING = "executing"
-    # Pool is closing down.
-    CLOSING = "closing"
-
-
-class PoolProgressReason(Enum):
-    POOL_STATUS_CHANGE = auto()
-    JOB_ADDED = auto()
-    JOB_STATUS_CHANGE = auto()
-    MAX_TIMEOUT_PING = auto()
-
-
-class Workflow:
-    def __init__(self, phases: list[str]):
-        self._phases = phases
-        self._phase = 0
-
-    @property
-    def phases(self):
-        return [*self._phases]
-
-    @property
-    def finished(self):
-        return self._phase >= len(self._phases)
-
-    @property
-    def phase(self):
-        if self.finished:
-            return "finished"
-        else:
-            return self._phases[self._phase]
-
-    def next_phase(self):
-        self._phase += 1
-        return self.phase
-
-
-class PoolProgress:
-    """
-    Class used to report pool progression to listeners.
-    """
-
-    def __init__(self, pool: "JobPool", reason: PoolProgressReason):
-        self._pool = pool
-        self._reason = reason
-
-    @property
-    def reason(self):
-        return self._reason
-
-    @property
-    def workflow(self):
-        return self._pool.workflow
-
-    @property
-    def jobs(self):
-        return self._pool.jobs
-
-    @property
-    def status(self):
-        return self._pool.status
-
-
-class PoolJobAddedProgress(PoolProgress):
-    def __init__(self, pool: "JobPool", job: "Job"):
-        super().__init__(pool, PoolProgressReason.JOB_ADDED)
-        self._job = job
-
-    @property
-    def job(self):
-        return self._job
-
-
-class PoolJobUpdateProgress(PoolProgress):
-    def __init__(self, pool: "JobPool", job: "Job", old_status: "JobStatus"):
-        super().__init__(pool, PoolProgressReason.JOB_STATUS_CHANGE)
-        self._job = job
-        self._old_status = old_status
-
-    @property
-    def job(self):
-        return self._job
-
-    @property
-    def old_status(self):
-        return self._old_status
-
-    @property
-    def status(self):
-        return self._job.status
-
-
-class PoolStatusProgress(PoolProgress):
-    def __init__(self, pool: "JobPool", old_status: PoolStatus):
-        super().__init__(pool, PoolProgressReason.POOL_STATUS_CHANGE)
-        self._old_status = old_status
-
-
-class _MissingMPIExecutor(ErrorModule):
-    pass
-
-
-class _MPIPoolModule(MockModule):
-    @property
-    def MPIExecutor(self) -> typing.Type["MPIExecutor"]:
-        return _MissingMPIExecutor(
-            "This is not a public interface. Use `.services.JobPool` instead."
-        )
-
-    def enable_serde_logging(self):
-        import mpipool
-
-        mpipool.enable_serde_logging()
-
-
-_MPIPool = _MPIPoolModule("mpipool")
-
-
-def dispatcher(pool_id, job_args):
-    job_type, args, kwargs = job_args
-    # Get the static job execution handler from this module
-    handler = globals()[job_type].execute
-    owner = JobPool.get_owner(pool_id)
-    # Execute it.
-    return handler(owner, args, kwargs)
-
-
-class SubmissionContext:
-    """
-    Context information on who submitted a certain job.
-    """
-
-    def __init__(self, submitter, chunks=None, **kwargs):
-        self._submitter = submitter
-        self._chunks = chunks
-        self._context = kwargs
-
-    @property
-    def name(self):
-        if hasattr(self._submitter, "get_node_name"):
-            name = self._submitter.get_node_name()
-        else:
-            name = str(self._submitter)
-        return name
-
-    @property
-    def submitter(self):
-        return self._submitter
-
-    @property
-    def chunks(self):
-        from ..storage._chunks import chunklist
-
-        return chunklist(self._chunks) if self._chunks is not None else None
-
-    @property
-    def context(self):
-        return {**self._context}
-
-    def __getattr__(self, key):
-        if key in self._context:
-            return self._context[key]
-        else:
-            return self.__getattribute__(key)
-
-
-class Job(abc.ABC):
-    """
-    Dispatches the execution of a function through a JobPool
-    """
-
-    def __init__(
-        self, pool, submission_context: SubmissionContext, args, kwargs, deps=None
-    ):
-        self.pool_id = pool.id
-        self._args = args
-        self._kwargs = kwargs
-        self._deps = set(deps or [])
-        self._submit_ctx = submission_context
-        self._completion_cbs = []
-        self._status = JobStatus.PENDING
-        self._future: typing.Optional[concurrent.futures.Future] = None
-        self._thread: typing.Optional[threading.Thread] = None
-        self._res_file = None
-        self._error = None
-
-        for j in self._deps:
-            j.on_completion(self._dep_completed)
-
-    @obj_str_insert
-    def __str__(self):
-        return self.description
-
-    @property
-    def name(self):
-        return self._submit_ctx.name
-
-    @property
-    def description(self):
-        descr = self.name
-        if self.context:
-            descr += " (" + ", ".join(f"{k}={v}" for k, v in self.context.items()) + ")"
-        return descr
-
-    @property
-    def submitter(self):
-        return self._submit_ctx.submitter
-
-    @property
-    def context(self):
-        return self._submit_ctx.context
-
-    @property
-    def status(self):
-        return self._status
-
-    @property
-    def result(self):
-        try:
-            with open(self._res_file, "rb") as f:
-                return pickle.load(f)
-        except Exception:
-            raise JobPoolError(f"Result of {self} is not available.") from None
-
-    @property
-    def error(self):
-        return self._error
-
-    def serialize(self):
-        """
-        Convert the job to a (de)serializable representation
-        """
-        name = self.__class__.__name__
-        # First arg is to find the static `execute` method so that we don't have to
-        # serialize any of the job objects themselves but can still use different handlers
-        # for different job types.
-        return (name, self._args, self._kwargs)
-
-    @staticmethod
-    @abc.abstractmethod
-    def execute(job_owner, args, kwargs):
-        """
-        Job handler
-        """
-        pass
-
-    def run(self, timeout=None):
-        """
-        Execute the job on the current process, in a thread, and return whether the job is still running.
-        """
-        if self._thread is None:
-
-            def target():
-                try:
-                    # Execute the static handler
-                    result = self.execute(self._pool.owner, self._args, self._kwargs)
-                except Exception as e:
-                    self._future.set_exception(e)
-                else:
-                    self._future.set_result(result)
-
-            self._thread = threading.Thread(target=target, daemon=True)
-            self._thread.start()
-        self._thread.join(timeout=timeout)
-        if not self._thread.is_alive():
-            self._completed()
-            return False
-        return True
-
-    def on_completion(self, cb):
-        self._completion_cbs.append(cb)
-
-    def set_result(self, value):
-        dirname = JobPool.get_tmp_folder(self.pool_id)
-        try:
-            with tempfile.NamedTemporaryFile(
-                prefix=dirname + "/", delete=False, mode="wb"
-            ) as fp:
-                pickle.dump(value, fp)
-                self._res_file = fp.name
-        except FileNotFoundError as e:
-            self.set_exception(e)
-        else:
-            self.change_status(JobStatus.SUCCESS)
-
-    def set_exception(self, e: Exception):
-        self._error = e
-        self.change_status(JobStatus.FAILED)
-
-    def _completed(self):
-        if self._status != JobStatus.CANCELLED:
-            try:
-                result = self._future.result()
-            except Exception as e:
-                self.set_exception(e)
-            else:
-                self.set_result(result)
-        for cb in self._completion_cbs:
-            cb(self)
-
-    def _dep_completed(self, dep):
-        # Earlier we registered this callback on the completion of our dependencies.
-        # When a dep completes we end up here and we discard it as a dependency as it has
-        # finished. If the dep returns an error remove the job from the pool, since the dependency have failed.
-        self._deps.discard(dep)
-        if dep._status is not JobStatus.SUCCESS:
-            self.cancel("Job killed for dependency failure")
-        else:
-            # When all our dependencies have been discarded we can queue ourselves. Unless the
-            # pool is serial, then the pool itself just runs all jobs in order.
-            if not self._deps and MPI.get_size() > 1:
-                # self._pool is set when the pool first tried to enqueue us, but we were still
-                # waiting for deps, in the `_enqueue` method below.
-                self._enqueue(self._pool)
-
-    def _enqueue(self, pool):
-        if not self._deps and self._status is not JobStatus.CANCELLED:
-            # Go ahead and submit ourselves to the pool, no dependencies to wait for
-            # The dispatcher is run on the remote worker and unpacks the data required
-            # to execute the job contents.
-            self.change_status(JobStatus.QUEUED)
-            self._future = pool._submit(dispatcher, self.pool_id, self.serialize())
-        else:
-            # We have unfinished dependencies and should wait until we can enqueue
-            # ourselves when our dependencies haved all notified us of their completion.
-            # Store the reference to the pool though, so later in `_dep_completed` we can
-            # call `_enqueue` again ourselves!
-            self._pool = pool
-
-    def cancel(self, reason: typing.Optional[str] = None):
-        self.change_status(JobStatus.CANCELLED)
-        self._error = JobCancelledError() if reason is None else JobCancelledError(reason)
-        if self._future:
-            if not self._future.cancel():
-                warnings.warn(f"Could not cancel {self}, the job is already running.")
-
-    def change_status(self, status: JobStatus):
-        old_status = self._status
-        self._status = status
-        try:
-            # Closed pools may have been removed from this map already.
-            pool = JobPool._pools[self.pool_id]
-        except KeyError:
-            pass
-        else:
-            progress = PoolJobUpdateProgress(pool, self, old_status)
-            pool.add_notification(progress)
-
-
-class PlacementJob(Job):
-    """
-    Dispatches the execution of a chunk of a placement strategy through a JobPool.
-    """
-
-    def __init__(self, pool, strategy, chunk, deps=None):
-        args = (strategy.name, chunk)
-        context = SubmissionContext(strategy, [chunk])
-        super().__init__(pool, context, args, {}, deps=deps)
-
-    @staticmethod
-    def execute(job_owner, args, kwargs):
-        name, chunk = args
-        placement = job_owner.placement[name]
-        indicators = placement.get_indicators()
-        return placement.place(chunk, indicators, **kwargs)
-
-
-class ConnectivityJob(Job):
-    """
-    Dispatches the execution of a chunk of a connectivity strategy through a JobPool.
-    """
-
-    def __init__(self, pool, strategy, pre_roi, post_roi, deps=None):
-        from ..storage._chunks import chunklist
-
-        args = (strategy.name, pre_roi, post_roi)
-        context = SubmissionContext(
-            strategy, chunks=chunklist((*(pre_roi or []), *(post_roi or [])))
-        )
-        super().__init__(pool, context, args, {}, deps=deps)
-
-    @staticmethod
-    def execute(job_owner, args, kwargs):
-        name = args[0]
-        connectivity = job_owner.connectivity[name]
-        collections = connectivity._get_connect_args_from_job(*args[1:])
-        return connectivity.connect(*collections, **kwargs)
-
-
-class FunctionJob(Job):
-    def __init__(self, pool, f, args, kwargs, deps=None, **context):
-        # Pack the function into the args
-        args = (f, args)
-        # If no submitter was given, set the function as submitter
-        context.setdefault("submitter", f)
-        super().__init__(pool, SubmissionContext(**context), args, kwargs, deps=deps)
-
-    @staticmethod
-    def execute(job_owner, args, kwargs):
-        # Unpack the function from the args
-        f, args = args
-        return f(job_owner, *args, **kwargs)
-
-
-class JobPool:
-    _next_pool_id = 0
-    _pools = {}
-    _pool_owners = {}
-    _tmp_folders = {}
-
-    def __init__(self, scaffold, fail_fast=False, workflow: "Workflow" = None):
-        self._schedulers: list[concurrent.futures.Future] = []
-        self.id: int = None
-        self._scaffold = scaffold
-        self._unhandled_errors = []
-        self._running_futures: list[concurrent.futures.Future] = []
-        self._mpipool: typing.Optional["MPIExecutor"] = None
-        self._job_queue: list[Job] = []
-        self._listeners = []
-        self._max_wait = 60
-        self._status: PoolStatus = None
-        self._progress_notifications: list["PoolProgress"] = []
-        self._workers_raise_unhandled = False
-        self._fail_fast = fail_fast
-        self._workflow = workflow
-
-    def __enter__(self):
-        self._context = ExitStack()
-        tmp_dirname = self._context.enter_context(tempfile.TemporaryDirectory())
-
-        self.id = JobPool._next_pool_id
-        JobPool._next_pool_id += 1
-        JobPool._pool_owners[self.id] = self._scaffold
-        JobPool._pools[self.id] = self
-        JobPool._tmp_folders[self.id] = tmp_dirname
-        del self._scaffold
-
-        for listener in self._listeners:
-            try:
-                self._context.enter_context(listener)
-            except (TypeError, AttributeError):
-                # Listener is not a context manager
-                pass
-        self.change_status(PoolStatus.SCHEDULING)
-        return self
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        self._context.__exit__(exc_type, exc_val, exc_tb)
-        # Clean up pool/job references
-        self._job_queue = []
-        del JobPool._pools[self.id]
-        del JobPool._pool_owners[self.id]
-        del JobPool._tmp_folders[self.id]
-        self.id = None
-
-    def add_listener(self, listener, max_wait=None):
-        self._max_wait = min(self._max_wait, max_wait or float("+inf"))
-        self._listeners.append(listener)
-
-    @property
-    def workflow(self):
-        return self._workflow
-
-    @property
-    def status(self):
-        return self._status
-
-    @property
-    def jobs(self) -> list[Job]:
-        return [*self._job_queue]
-
-    @property
-    def parallel(self):
-        return MPI.get_size() > 1
-
-    @classmethod
-    def get_owner(cls, id):
-        return cls._pool_owners[id]
-
-    @classmethod
-    def get_tmp_folder(cls, id):
-        return cls._tmp_folders[id]
-
-    @property
-    def owner(self):
-        return self.get_owner(self.id)
-
-    def is_main(self):
-        return MPI.get_rank() == 0
-
-    def get_submissions_of(self, submitter):
-        return [job for job in self._job_queue if job.submitter is submitter]
-
-    def _put(self, job):
-        """
-        Puts a job onto our internal queue.
-        """
-        if self._mpipool and not self._mpipool.open:
-            raise JobPoolError("No job pool available for job submission.")
-        else:
-            self.add_notification(PoolJobAddedProgress(self, job))
-            self._job_queue.append(job)
-            if self._mpipool:
-                # This job was scheduled after the MPIPool was opened, so immediately
-                # put it on the MPIPool's queue.
-                job._enqueue(self)
-
-    def _submit(self, fn, *args, **kwargs):
-        if not self._mpipool or not self._mpipool.open:
-            raise JobPoolError("No job pool available for job submission.")
-        else:
-            future = self._mpipool.submit(fn, *args, **kwargs)
-            self._running_futures.append(future)
-            return future
-
-    def _schedule(self, future: concurrent.futures.Future, nodes, scheduler):
-        _failed_nodes = []
-        if not future.set_running_or_notify_cancel():
-            return
-        try:
-            for node in nodes:
-                failed_deps = [
-                    n for n in getattr(node, "depends_on", []) if n in _failed_nodes
-                ]
-                if failed_deps:
-                    _failed_nodes.append(node)
-                    ctx = SubmissionContext(
-                        node,
-                        error=JobSchedulingError(
-                            f"Depends on {failed_deps}, whom failed."
-                        ),
-                    )
-                    self._unhandled_errors.append(ctx)
-                    continue
-                try:
-                    scheduler(node)
-                except Exception as e:
-                    _failed_nodes.append(node)
-                    ctx = SubmissionContext(node, error=e)
-                    self._unhandled_errors.append(ctx)
-        finally:
-            future.set_result(None)
-
-    def schedule(self, nodes, scheduler=None):
-        if scheduler is None:
-
-            def scheduler(node):
-                node.queue(self)
-
-        future = concurrent.futures.Future()
-        self._schedulers.append(future)
-        thread = threading.Thread(target=self._schedule, args=(future, nodes, scheduler))
-        thread.start()
-
-    @property
-    def scheduling(self):
-        return any(not f.done() for f in self._schedulers)
-
-    def queue(self, f, args=None, kwargs=None, deps=None, **context):
-        job = FunctionJob(self, f, args or [], kwargs or {}, deps, **context)
-        self._put(job)
-        return job
-
-    def queue_placement(self, strategy, chunk, deps=None):
-        job = PlacementJob(self, strategy, chunk, deps)
-        self._put(job)
-        return job
-
-    def queue_connectivity(self, strategy, pre_roi, post_roi, deps=None):
-        job = ConnectivityJob(self, strategy, pre_roi, post_roi, deps)
-        self._put(job)
-        return job
-
-    def execute(self, return_results=False):
-        """
-        Execute the jobs in the queue
-
-        In serial execution this runs all of the jobs in the queue in First In First Out
-        order. In parallel execution this enqueues all jobs into the MPIPool unless they
-        have dependencies that need to complete first.
-        """
-
-        if self.id is None:
-            raise JobPoolContextError("Job pools must use a context manager.")
-
-        if self.parallel:
-            self._execute_parallel()
-        else:
-            self._execute_serial()
-
-        if return_results:
-            return {
-                job: job.result
-                for job in self._job_queue
-                if job.status == JobStatus.SUCCESS
-            }
-
-    def _execute_parallel(self):
-        import bsb.options
-
-        # Enable full mpipool debugging
-        if bsb.options.debug_pool:
-            _MPIPool.enable_serde_logging()
-        # Create the MPI pool
-        self._mpipool = _MPIPool.MPIExecutor(
-            loglevel=logging.DEBUG if bsb.options.debug_pool else logging.CRITICAL
-        )
-
-        if self._mpipool.is_worker():
-            # The workers will return out of the pool constructor when they receive
-            # the shutdown signal from the master, they return here skipping the
-            # master logic.
-
-            # Check if we need to abort our process due to errors etc.
-            abort = MPI.bcast(None)
-            if abort:
-                raise WorkflowError(
-                    "Unhandled exceptions during parallel execution.",
-                    [JobPoolError("See main node logs for details.")],
-                )
-            return
-
-        try:
-            # Tell the listeners execution is running
-            self.change_status(PoolStatus.EXECUTING)
-            # Kickstart the workers with the queued jobs
-            for job in self._job_queue:
-                job._enqueue(self)
-            # Add the scheduling futures to the running futures, to await them.
-            self._running_futures.extend(self._schedulers)
-
-            # Keep executing as long as any of the schedulers or jobs aren't done yet.
-            while self.scheduling or any(
-                job.status == JobStatus.PENDING or job.status == JobStatus.QUEUED
-                for job in self._job_queue
-            ):
-                try:
-                    done, not_done = concurrent.futures.wait(
-                        self._running_futures,
-                        timeout=self._max_wait,
-                        return_when="FIRST_COMPLETED",
-                    )
-                except ValueError:
-                    # Sometimes a ValueError is raised here, perhaps because we modify
-                    # the list below?
-                    continue
-
-                # Complete any jobs that are done
-                for job in self._job_queue:
-                    if job._future in done:
-                        job._completed()
-                # Remove running futures that are done
-                for future in done:
-                    self._running_futures.remove(future)
-                # If nothing finished, post a timeout notification.
-                if not len(done):
-                    self.ping()
-                # Notify all the listeners, and store/raise any unhandled errors
-                self.notify()
-
-            # Notify listeners that execution is over
-            self.change_status(PoolStatus.CLOSING)
-            # Raise any unhandled errors
-            self.raise_unhandled()
-        except:
-            # If any exception (including SystemExit and KeyboardInterrupt) happen on main, we should
-            # broadcast the abort to all worker nodes.
-            self._workers_raise_unhandled = True
-            raise
-        finally:
-            # Shut down our internal pool
-            self._mpipool.shutdown(wait=False, cancel_futures=True)
-            # Broadcast whether the worker nodes should raise an unhandled error.
-            MPI.bcast(self._workers_raise_unhandled)
-
-    def _execute_serial(self):
-        # Wait for jobs to finish scheduling
-        while concurrent.futures.wait(
-            self._schedulers, timeout=self._max_wait, return_when="FIRST_COMPLETED"
-        )[1]:
-            self.ping()
-            self.notify()
-        # Prepare jobs for local execution
-        for job in self._job_queue:
-            job._future = concurrent.futures.Future()
-            job._pool = self
-            if job.status != JobStatus.CANCELLED and job.status != JobStatus.ABORTED:
-                job._status = JobStatus.QUEUED
-            else:
-                job._future.cancel()
-
-        self.change_status(PoolStatus.EXECUTING)
-        # Just run each job serially
-        for job in self._job_queue:
-            if not job._future.set_running_or_notify_cancel():
-                continue
-            job.change_status(JobStatus.RUNNING)
-            self.notify()
-            while job.run(timeout=self._max_wait):
-                self.ping()
-                self.notify()
-            self.notify()
-        # Raise any unhandled errors
-        self.raise_unhandled()
-
-        self.change_status(PoolStatus.CLOSING)
-
-    def change_status(self, status: PoolStatus):
-        old_status = self._status
-        self._status = status
-        self.add_notification(PoolStatusProgress(self, old_status))
-        self.notify()
-
-    def add_notification(self, notification: PoolProgress):
-        self._progress_notifications.append(notification)
-
-    def ping(self):
-        self.add_notification(PoolProgress(self, PoolProgressReason.MAX_TIMEOUT_PING))
-
-    def notify(self):
-        for notification in self._progress_notifications:
-            job = getattr(notification, "job", None)
-            job_error = getattr(job, "error", None)
-            has_error = job_error is not None and type(job_error) is not JobCancelledError
-            handled_error = [bool(listener(notification)) for listener in self._listeners]
-            if has_error and not any(handled_error):
-                self._unhandled_errors.append(job)
-        if self._fail_fast:
-            self.raise_unhandled()
-        self._progress_notifications = []
-
-    def raise_unhandled(self):
-        if not self._unhandled_errors:
-            return
-        errors = []
-        # Raise and catch for nicer traceback
-        for job in self._unhandled_errors:
-            try:
-                if isinstance(job, SubmissionContext):
-                    raise JobSchedulingError(
-                        f"{job.name} failed to schedule its jobs."
-                    ) from job.context["error"]
-                raise JobErroredError(f"{job} failed", job.error) from job.error
-            except (JobErroredError, JobSchedulingError) as e:
-                errors.append(e)
-        self._unhandled_errors = []
-        raise WorkflowError(
-            f"Your workflow encountered errors.",
-            errors,
-        )
+"""
+Job pooling module.
+
+Jobs derive from the base :class:`.Job` class which can be put on the queue of a
+:class:`.JobPool`. In order to submit themselves to the pool Jobs will
+:meth:`~.Job.serialize` themselves into a predefined set of variables::
+
+   job.serialize() -> (job_type, f, args, kwargs)
+
+* ``job_type`` should be a string that is a class name defined in this module.
+   (e.g. ``"PlacementJob"``)
+
+* ``f`` should be the function object that the job's ``execute`` method should
+   execute.
+
+* ``args`` and ``kwargs`` are the args to be passed to that ``f``.
+
+The :meth:`.Job.execute` handler can help interpret ``args`` and ``kwargs``
+before running ``f``. The execute handler has access to the scaffold on the MPI
+process so one best serializes just the name of some part of the configuration,
+rather than trying to pickle the complex objects. For example, the
+:class:`.PlacementJob` uses the first ``args`` element to store the
+:class:`~bsb.placement.strategy.PlacementStrategy` name and then retrieve it from the
+scaffold:
+
+.. code-block:: python
+
+    @staticmethod
+    def execute(job_owner, f, args, kwargs):
+        placement = job_owner.placement[args[0]]
+        indicators = placement.get_indicators()
+        return f(placement, *args[1:], indicators, **kwargs)
+
+A job has a couple of display variables that can be set: ``_cname`` for the
+class name, ``_name`` for the job name and ``_c`` for the chunk. These are used
+to display what the workers are doing during parallel execution. This is an experimental
+API and subject to sudden change in the future.
+
+"""
+
+import abc
+import concurrent.futures
+import logging
+import pickle
+import tempfile
+import threading
+import typing
+import warnings
+from contextlib import ExitStack
+from enum import Enum, auto
+
+from exceptiongroup import ExceptionGroup
+
+from .._util import obj_str_insert
+from ..exceptions import (
+    JobCancelledError,
+    JobPoolContextError,
+    JobPoolError,
+    JobSchedulingError,
+)
+from . import MPI
+from ._util import ErrorModule, MockModule
+
+if typing.TYPE_CHECKING:
+    from mpipool import MPIExecutor
+
+
+class WorkflowError(ExceptionGroup):
+    pass
+
+
+class JobErroredError(Exception):
+    def __init__(self, message, error):
+        super().__init__(message)
+        self.error = error
+
+
+class JobStatus(Enum):
+    # Job has not been queued yet, waiting for dependencies to resolve.
+    PENDING = "pending"
+    # Job is on the queue.
+    QUEUED = "queued"
+    # Job is currently running on a worker.
+    RUNNING = "running"
+    # Job ran successfully.
+    SUCCESS = "success"
+    # Job failed (an exception was raised).
+    FAILED = "failed"
+    # Job was cancelled before it started running.
+    CANCELLED = "cancelled"
+    # Job was killed for some reason.
+    ABORTED = "aborted"
+
+
+class PoolStatus(Enum):
+    # Pool has been initialized and jobs can be scheduled.
+    SCHEDULING = "scheduling"
+    # Pool started execution.
+    EXECUTING = "executing"
+    # Pool is closing down.
+    CLOSING = "closing"
+
+
+class PoolProgressReason(Enum):
+    POOL_STATUS_CHANGE = auto()
+    JOB_ADDED = auto()
+    JOB_STATUS_CHANGE = auto()
+    MAX_TIMEOUT_PING = auto()
+
+
+class Workflow:
+    def __init__(self, phases: list[str]):
+        self._phases = phases
+        self._phase = 0
+
+    @property
+    def phases(self):
+        return [*self._phases]
+
+    @property
+    def finished(self):
+        return self._phase >= len(self._phases)
+
+    @property
+    def phase(self):
+        if self.finished:
+            return "finished"
+        else:
+            return self._phases[self._phase]
+
+    def next_phase(self):
+        self._phase += 1
+        return self.phase
+
+
+class PoolProgress:
+    """
+    Class used to report pool progression to listeners.
+    """
+
+    def __init__(self, pool: "JobPool", reason: PoolProgressReason):
+        self._pool = pool
+        self._reason = reason
+
+    @property
+    def reason(self):
+        return self._reason
+
+    @property
+    def workflow(self):
+        return self._pool.workflow
+
+    @property
+    def jobs(self):
+        return self._pool.jobs
+
+    @property
+    def status(self):
+        return self._pool.status
+
+
+class PoolJobAddedProgress(PoolProgress):
+    def __init__(self, pool: "JobPool", job: "Job"):
+        super().__init__(pool, PoolProgressReason.JOB_ADDED)
+        self._job = job
+
+    @property
+    def job(self):
+        return self._job
+
+
+class PoolJobUpdateProgress(PoolProgress):
+    def __init__(self, pool: "JobPool", job: "Job", old_status: "JobStatus"):
+        super().__init__(pool, PoolProgressReason.JOB_STATUS_CHANGE)
+        self._job = job
+        self._old_status = old_status
+
+    @property
+    def job(self):
+        return self._job
+
+    @property
+    def old_status(self):
+        return self._old_status
+
+    @property
+    def status(self):
+        return self._job.status
+
+
+class PoolStatusProgress(PoolProgress):
+    def __init__(self, pool: "JobPool", old_status: PoolStatus):
+        super().__init__(pool, PoolProgressReason.POOL_STATUS_CHANGE)
+        self._old_status = old_status
+
+
+class _MissingMPIExecutor(ErrorModule):
+    pass
+
+
+class _MPIPoolModule(MockModule):
+    @property
+    def MPIExecutor(self) -> typing.Type["MPIExecutor"]:
+        return _MissingMPIExecutor(
+            "This is not a public interface. Use `.services.JobPool` instead."
+        )
+
+    def enable_serde_logging(self):
+        import mpipool
+
+        mpipool.enable_serde_logging()
+
+
+_MPIPool = _MPIPoolModule("mpipool")
+
+
+def dispatcher(pool_id, job_args):
+    job_type, args, kwargs = job_args
+    # Get the static job execution handler from this module
+    handler = globals()[job_type].execute
+    owner = JobPool.get_owner(pool_id)
+    # Execute it.
+    return handler(owner, args, kwargs)
+
+
+class SubmissionContext:
+    """
+    Context information on who submitted a certain job.
+    """
+
+    def __init__(self, submitter, chunks=None, **kwargs):
+        self._submitter = submitter
+        self._chunks = chunks
+        self._context = kwargs
+
+    @property
+    def name(self):
+        if hasattr(self._submitter, "get_node_name"):
+            name = self._submitter.get_node_name()
+        else:
+            name = str(self._submitter)
+        return name
+
+    @property
+    def submitter(self):
+        return self._submitter
+
+    @property
+    def chunks(self):
+        from ..storage._chunks import chunklist
+
+        return chunklist(self._chunks) if self._chunks is not None else None
+
+    @property
+    def context(self):
+        return {**self._context}
+
+    def __getattr__(self, key):
+        if key in self._context:
+            return self._context[key]
+        else:
+            return self.__getattribute__(key)
+
+
+class Job(abc.ABC):
+    """
+    Dispatches the execution of a function through a JobPool
+    """
+
+    def __init__(
+        self, pool, submission_context: SubmissionContext, args, kwargs, deps=None
+    ):
+        self.pool_id = pool.id
+        self._args = args
+        self._kwargs = kwargs
+        self._deps = set(deps or [])
+        self._submit_ctx = submission_context
+        self._completion_cbs = []
+        self._status = JobStatus.PENDING
+        self._future: typing.Optional[concurrent.futures.Future] = None
+        self._thread: typing.Optional[threading.Thread] = None
+        self._res_file = None
+        self._error = None
+
+        for j in self._deps:
+            j.on_completion(self._dep_completed)
+
+    @obj_str_insert
+    def __str__(self):
+        return self.description
+
+    @property
+    def name(self):
+        return self._submit_ctx.name
+
+    @property
+    def description(self):
+        descr = self.name
+        if self.context:
+            descr += " (" + ", ".join(f"{k}={v}" for k, v in self.context.items()) + ")"
+        return descr
+
+    @property
+    def submitter(self):
+        return self._submit_ctx.submitter
+
+    @property
+    def context(self):
+        return self._submit_ctx.context
+
+    @property
+    def status(self):
+        return self._status
+
+    @property
+    def result(self):
+        try:
+            with open(self._res_file, "rb") as f:
+                return pickle.load(f)
+        except Exception:
+            raise JobPoolError(f"Result of {self} is not available.") from None
+
+    @property
+    def error(self):
+        return self._error
+
+    def serialize(self):
+        """
+        Convert the job to a (de)serializable representation
+        """
+        name = self.__class__.__name__
+        # First arg is to find the static `execute` method so that we don't have to
+        # serialize any of the job objects themselves but can still use different handlers
+        # for different job types.
+        return (name, self._args, self._kwargs)
+
+    @staticmethod
+    @abc.abstractmethod
+    def execute(job_owner, args, kwargs):
+        """
+        Job handler
+        """
+        pass
+
+    def run(self, timeout=None):
+        """
+        Execute the job on the current process, in a thread, and return whether the job is still running.
+        """
+        if self._thread is None:
+
+            def target():
+                try:
+                    # Execute the static handler
+                    result = self.execute(self._pool.owner, self._args, self._kwargs)
+                except Exception as e:
+                    self._future.set_exception(e)
+                else:
+                    self._future.set_result(result)
+
+            self._thread = threading.Thread(target=target, daemon=True)
+            self._thread.start()
+        self._thread.join(timeout=timeout)
+        if not self._thread.is_alive():
+            self._completed()
+            return False
+        return True
+
+    def on_completion(self, cb):
+        self._completion_cbs.append(cb)
+
+    def set_result(self, value):
+        dirname = JobPool.get_tmp_folder(self.pool_id)
+        try:
+            with tempfile.NamedTemporaryFile(
+                prefix=dirname + "/", delete=False, mode="wb"
+            ) as fp:
+                pickle.dump(value, fp)
+                self._res_file = fp.name
+        except FileNotFoundError as e:
+            self.set_exception(e)
+        else:
+            self.change_status(JobStatus.SUCCESS)
+
+    def set_exception(self, e: Exception):
+        self._error = e
+        self.change_status(JobStatus.FAILED)
+
+    def _completed(self):
+        if self._status != JobStatus.CANCELLED:
+            try:
+                result = self._future.result()
+            except Exception as e:
+                self.set_exception(e)
+            else:
+                self.set_result(result)
+        for cb in self._completion_cbs:
+            cb(self)
+
+    def _dep_completed(self, dep):
+        # Earlier we registered this callback on the completion of our dependencies.
+        # When a dep completes we end up here and we discard it as a dependency as it has
+        # finished. If the dep returns an error remove the job from the pool, since the dependency have failed.
+        self._deps.discard(dep)
+        if dep._status is not JobStatus.SUCCESS:
+            self.cancel("Job killed for dependency failure")
+        else:
+            # When all our dependencies have been discarded we can queue ourselves. Unless the
+            # pool is serial, then the pool itself just runs all jobs in order.
+            if not self._deps and MPI.get_size() > 1:
+                # self._pool is set when the pool first tried to enqueue us, but we were still
+                # waiting for deps, in the `_enqueue` method below.
+                self._enqueue(self._pool)
+
+    def _enqueue(self, pool):
+        if not self._deps and self._status is not JobStatus.CANCELLED:
+            # Go ahead and submit ourselves to the pool, no dependencies to wait for
+            # The dispatcher is run on the remote worker and unpacks the data required
+            # to execute the job contents.
+            self.change_status(JobStatus.QUEUED)
+            self._future = pool._submit(dispatcher, self.pool_id, self.serialize())
+        else:
+            # We have unfinished dependencies and should wait until we can enqueue
+            # ourselves when our dependencies haved all notified us of their completion.
+            # Store the reference to the pool though, so later in `_dep_completed` we can
+            # call `_enqueue` again ourselves!
+            self._pool = pool
+
+    def cancel(self, reason: typing.Optional[str] = None):
+        self.change_status(JobStatus.CANCELLED)
+        self._error = JobCancelledError() if reason is None else JobCancelledError(reason)
+        if self._future:
+            if not self._future.cancel():
+                warnings.warn(f"Could not cancel {self}, the job is already running.")
+
+    def change_status(self, status: JobStatus):
+        old_status = self._status
+        self._status = status
+        try:
+            # Closed pools may have been removed from this map already.
+            pool = JobPool._pools[self.pool_id]
+        except KeyError:
+            pass
+        else:
+            progress = PoolJobUpdateProgress(pool, self, old_status)
+            pool.add_notification(progress)
+
+
+class PlacementJob(Job):
+    """
+    Dispatches the execution of a chunk of a placement strategy through a JobPool.
+    """
+
+    def __init__(self, pool, strategy, chunk, deps=None):
+        args = (strategy.name, chunk)
+        context = SubmissionContext(strategy, [chunk])
+        super().__init__(pool, context, args, {}, deps=deps)
+
+    @staticmethod
+    def execute(job_owner, args, kwargs):
+        name, chunk = args
+        placement = job_owner.placement[name]
+        indicators = placement.get_indicators()
+        return placement.place(chunk, indicators, **kwargs)
+
+
+class ConnectivityJob(Job):
+    """
+    Dispatches the execution of a chunk of a connectivity strategy through a JobPool.
+    """
+
+    def __init__(self, pool, strategy, pre_roi, post_roi, deps=None):
+        from ..storage._chunks import chunklist
+
+        args = (strategy.name, pre_roi, post_roi)
+        context = SubmissionContext(
+            strategy, chunks=chunklist((*(pre_roi or []), *(post_roi or [])))
+        )
+        super().__init__(pool, context, args, {}, deps=deps)
+
+    @staticmethod
+    def execute(job_owner, args, kwargs):
+        name = args[0]
+        connectivity = job_owner.connectivity[name]
+        collections = connectivity._get_connect_args_from_job(*args[1:])
+        return connectivity.connect(*collections, **kwargs)
+
+
+class FunctionJob(Job):
+    def __init__(self, pool, f, args, kwargs, deps=None, **context):
+        # Pack the function into the args
+        args = (f, args)
+        # If no submitter was given, set the function as submitter
+        context.setdefault("submitter", f)
+        super().__init__(pool, SubmissionContext(**context), args, kwargs, deps=deps)
+
+    @staticmethod
+    def execute(job_owner, args, kwargs):
+        # Unpack the function from the args
+        f, args = args
+        return f(job_owner, *args, **kwargs)
+
+
+class JobPool:
+    _next_pool_id = 0
+    _pools = {}
+    _pool_owners = {}
+    _tmp_folders = {}
+
+    def __init__(self, scaffold, fail_fast=False, workflow: "Workflow" = None):
+        self._schedulers: list[concurrent.futures.Future] = []
+        self.id: int = None
+        self._scaffold = scaffold
+        self._unhandled_errors = []
+        self._running_futures: list[concurrent.futures.Future] = []
+        self._mpipool: typing.Optional["MPIExecutor"] = None
+        self._job_queue: list[Job] = []
+        self._listeners = []
+        self._max_wait = 60
+        self._status: PoolStatus = None
+        self._progress_notifications: list["PoolProgress"] = []
+        self._workers_raise_unhandled = False
+        self._fail_fast = fail_fast
+        self._workflow = workflow
+
+    def __enter__(self):
+        self._context = ExitStack()
+        tmp_dirname = self._context.enter_context(tempfile.TemporaryDirectory())
+
+        self.id = JobPool._next_pool_id
+        JobPool._next_pool_id += 1
+        JobPool._pool_owners[self.id] = self._scaffold
+        JobPool._pools[self.id] = self
+        JobPool._tmp_folders[self.id] = tmp_dirname
+        del self._scaffold
+
+        for listener in self._listeners:
+            try:
+                self._context.enter_context(listener)
+            except (TypeError, AttributeError):
+                # Listener is not a context manager
+                pass
+        self.change_status(PoolStatus.SCHEDULING)
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self._context.__exit__(exc_type, exc_val, exc_tb)
+        # Clean up pool/job references
+        self._job_queue = []
+        del JobPool._pools[self.id]
+        del JobPool._pool_owners[self.id]
+        del JobPool._tmp_folders[self.id]
+        self.id = None
+
+    def add_listener(self, listener, max_wait=None):
+        self._max_wait = min(self._max_wait, max_wait or float("+inf"))
+        self._listeners.append(listener)
+
+    @property
+    def workflow(self):
+        return self._workflow
+
+    @property
+    def status(self):
+        return self._status
+
+    @property
+    def jobs(self) -> list[Job]:
+        return [*self._job_queue]
+
+    @property
+    def parallel(self):
+        return MPI.get_size() > 1
+
+    @classmethod
+    def get_owner(cls, id):
+        return cls._pool_owners[id]
+
+    @classmethod
+    def get_tmp_folder(cls, id):
+        return cls._tmp_folders[id]
+
+    @property
+    def owner(self):
+        return self.get_owner(self.id)
+
+    def is_main(self):
+        return MPI.get_rank() == 0
+
+    def get_submissions_of(self, submitter):
+        return [job for job in self._job_queue if job.submitter is submitter]
+
+    def _put(self, job):
+        """
+        Puts a job onto our internal queue.
+        """
+        if self._mpipool and not self._mpipool.open:
+            raise JobPoolError("No job pool available for job submission.")
+        else:
+            self.add_notification(PoolJobAddedProgress(self, job))
+            self._job_queue.append(job)
+            if self._mpipool:
+                # This job was scheduled after the MPIPool was opened, so immediately
+                # put it on the MPIPool's queue.
+                job._enqueue(self)
+
+    def _submit(self, fn, *args, **kwargs):
+        if not self._mpipool or not self._mpipool.open:
+            raise JobPoolError("No job pool available for job submission.")
+        else:
+            future = self._mpipool.submit(fn, *args, **kwargs)
+            self._running_futures.append(future)
+            return future
+
+    def _schedule(self, future: concurrent.futures.Future, nodes, scheduler):
+        _failed_nodes = []
+        if not future.set_running_or_notify_cancel():
+            return
+        try:
+            for node in nodes:
+                failed_deps = [
+                    n for n in getattr(node, "depends_on", []) if n in _failed_nodes
+                ]
+                if failed_deps:
+                    _failed_nodes.append(node)
+                    ctx = SubmissionContext(
+                        node,
+                        error=JobSchedulingError(
+                            f"Depends on {failed_deps}, whom failed."
+                        ),
+                    )
+                    self._unhandled_errors.append(ctx)
+                    continue
+                try:
+                    scheduler(node)
+                except Exception as e:
+                    _failed_nodes.append(node)
+                    ctx = SubmissionContext(node, error=e)
+                    self._unhandled_errors.append(ctx)
+        finally:
+            future.set_result(None)
+
+    def schedule(self, nodes, scheduler=None):
+        if scheduler is None:
+
+            def scheduler(node):
+                node.queue(self)
+
+        future = concurrent.futures.Future()
+        self._schedulers.append(future)
+        thread = threading.Thread(target=self._schedule, args=(future, nodes, scheduler))
+        thread.start()
+
+    @property
+    def scheduling(self):
+        return any(not f.done() for f in self._schedulers)
+
+    def queue(self, f, args=None, kwargs=None, deps=None, **context):
+        job = FunctionJob(self, f, args or [], kwargs or {}, deps, **context)
+        self._put(job)
+        return job
+
+    def queue_placement(self, strategy, chunk, deps=None):
+        job = PlacementJob(self, strategy, chunk, deps)
+        self._put(job)
+        return job
+
+    def queue_connectivity(self, strategy, pre_roi, post_roi, deps=None):
+        job = ConnectivityJob(self, strategy, pre_roi, post_roi, deps)
+        self._put(job)
+        return job
+
+    def execute(self, return_results=False):
+        """
+        Execute the jobs in the queue
+
+        In serial execution this runs all of the jobs in the queue in First In First Out
+        order. In parallel execution this enqueues all jobs into the MPIPool unless they
+        have dependencies that need to complete first.
+        """
+
+        if self.id is None:
+            raise JobPoolContextError("Job pools must use a context manager.")
+
+        if self.parallel:
+            self._execute_parallel()
+        else:
+            self._execute_serial()
+
+        if return_results:
+            return {
+                job: job.result
+                for job in self._job_queue
+                if job.status == JobStatus.SUCCESS
+            }
+
+    def _execute_parallel(self):
+        import bsb.options
+
+        # Enable full mpipool debugging
+        if bsb.options.debug_pool:
+            _MPIPool.enable_serde_logging()
+        # Create the MPI pool
+        self._mpipool = _MPIPool.MPIExecutor(
+            loglevel=logging.DEBUG if bsb.options.debug_pool else logging.CRITICAL
+        )
+
+        if self._mpipool.is_worker():
+            # The workers will return out of the pool constructor when they receive
+            # the shutdown signal from the master, they return here skipping the
+            # master logic.
+
+            # Check if we need to abort our process due to errors etc.
+            abort = MPI.bcast(None)
+            if abort:
+                raise WorkflowError(
+                    "Unhandled exceptions during parallel execution.",
+                    [JobPoolError("See main node logs for details.")],
+                )
+            return
+
+        try:
+            # Tell the listeners execution is running
+            self.change_status(PoolStatus.EXECUTING)
+            # Kickstart the workers with the queued jobs
+            for job in self._job_queue:
+                job._enqueue(self)
+            # Add the scheduling futures to the running futures, to await them.
+            self._running_futures.extend(self._schedulers)
+
+            # Keep executing as long as any of the schedulers or jobs aren't done yet.
+            while self.scheduling or any(
+                job.status == JobStatus.PENDING or job.status == JobStatus.QUEUED
+                for job in self._job_queue
+            ):
+                try:
+                    done, not_done = concurrent.futures.wait(
+                        self._running_futures,
+                        timeout=self._max_wait,
+                        return_when="FIRST_COMPLETED",
+                    )
+                except ValueError:
+                    # Sometimes a ValueError is raised here, perhaps because we modify
+                    # the list below?
+                    continue
+
+                # Complete any jobs that are done
+                for job in self._job_queue:
+                    if job._future in done:
+                        job._completed()
+                # Remove running futures that are done
+                for future in done:
+                    self._running_futures.remove(future)
+                # If nothing finished, post a timeout notification.
+                if not len(done):
+                    self.ping()
+                # Notify all the listeners, and store/raise any unhandled errors
+                self.notify()
+
+            # Notify listeners that execution is over
+            self.change_status(PoolStatus.CLOSING)
+            # Raise any unhandled errors
+            self.raise_unhandled()
+        except:
+            # If any exception (including SystemExit and KeyboardInterrupt) happen on main, we should
+            # broadcast the abort to all worker nodes.
+            self._workers_raise_unhandled = True
+            raise
+        finally:
+            # Shut down our internal pool
+            self._mpipool.shutdown(wait=False, cancel_futures=True)
+            # Broadcast whether the worker nodes should raise an unhandled error.
+            MPI.bcast(self._workers_raise_unhandled)
+
+    def _execute_serial(self):
+        # Wait for jobs to finish scheduling
+        while concurrent.futures.wait(
+            self._schedulers, timeout=self._max_wait, return_when="FIRST_COMPLETED"
+        )[1]:
+            self.ping()
+            self.notify()
+        # Prepare jobs for local execution
+        for job in self._job_queue:
+            job._future = concurrent.futures.Future()
+            job._pool = self
+            if job.status != JobStatus.CANCELLED and job.status != JobStatus.ABORTED:
+                job._status = JobStatus.QUEUED
+            else:
+                job._future.cancel()
+
+        self.change_status(PoolStatus.EXECUTING)
+        # Just run each job serially
+        for job in self._job_queue:
+            if not job._future.set_running_or_notify_cancel():
+                continue
+            job.change_status(JobStatus.RUNNING)
+            self.notify()
+            while job.run(timeout=self._max_wait):
+                self.ping()
+                self.notify()
+            self.notify()
+        # Raise any unhandled errors
+        self.raise_unhandled()
+
+        self.change_status(PoolStatus.CLOSING)
+
+    def change_status(self, status: PoolStatus):
+        old_status = self._status
+        self._status = status
+        self.add_notification(PoolStatusProgress(self, old_status))
+        self.notify()
+
+    def add_notification(self, notification: PoolProgress):
+        self._progress_notifications.append(notification)
+
+    def ping(self):
+        self.add_notification(PoolProgress(self, PoolProgressReason.MAX_TIMEOUT_PING))
+
+    def notify(self):
+        for notification in self._progress_notifications:
+            job = getattr(notification, "job", None)
+            job_error = getattr(job, "error", None)
+            has_error = job_error is not None and type(job_error) is not JobCancelledError
+            handled_error = [bool(listener(notification)) for listener in self._listeners]
+            if has_error and not any(handled_error):
+                self._unhandled_errors.append(job)
+        if self._fail_fast:
+            self.raise_unhandled()
+        self._progress_notifications = []
+
+    def raise_unhandled(self):
+        if not self._unhandled_errors:
+            return
+        errors = []
+        # Raise and catch for nicer traceback
+        for job in self._unhandled_errors:
+            try:
+                if isinstance(job, SubmissionContext):
+                    raise JobSchedulingError(
+                        f"{job.name} failed to schedule its jobs."
+                    ) from job.context["error"]
+                raise JobErroredError(f"{job} failed", job.error) from job.error
+            except (JobErroredError, JobSchedulingError) as e:
+                errors.append(e)
+        self._unhandled_errors = []
+        raise WorkflowError(
+            f"Your workflow encountered errors.",
+            errors,
+        )
```

### Comparing `bsb_core-4.0.1/bsb/simulation/_backends.py` & `bsb_core-4.1.0/bsb/simulation/_backends.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-import functools
-import typing
-
-from .. import plugins
-
-if typing.TYPE_CHECKING:
-    from . import SimulationBackendPlugin
-    from .adapter import SimulatorAdapter
-    from .simulation import Simulation
-
-
-@functools.cache
-def get_backends() -> dict[str, "SimulationBackendPlugin"]:
-    backends = plugins.discover("simulation_backends")
-    for backend in backends.values():
-        plugins._decorate_advert(backend.Simulation, backend._bsb_entry_point)
-        plugins._decorate_advert(backend.Adapter, backend._bsb_entry_point)
-    return backends
-
-
-@functools.cache
-def get_simulation_nodes() -> dict[str, "Simulation"]:
-    return {name: plugin.Simulation for name, plugin in get_backends().items()}
-
-
-@functools.cache
-def get_simulation_adapters() -> dict[str, "SimulatorAdapter"]:
-    return {name: plugin.Adapter() for name, plugin in get_backends().items()}
+import functools
+import typing
+
+from .. import plugins
+
+if typing.TYPE_CHECKING:
+    from . import SimulationBackendPlugin
+    from .adapter import SimulatorAdapter
+    from .simulation import Simulation
+
+
+@functools.cache
+def get_backends() -> dict[str, "SimulationBackendPlugin"]:
+    backends = plugins.discover("simulation_backends")
+    for backend in backends.values():
+        plugins._decorate_advert(backend.Simulation, backend._bsb_entry_point)
+        plugins._decorate_advert(backend.Adapter, backend._bsb_entry_point)
+    return backends
+
+
+@functools.cache
+def get_simulation_nodes() -> dict[str, "Simulation"]:
+    return {name: plugin.Simulation for name, plugin in get_backends().items()}
+
+
+@functools.cache
+def get_simulation_adapters() -> dict[str, "SimulatorAdapter"]:
+    return {name: plugin.Adapter() for name, plugin in get_backends().items()}
```

### Comparing `bsb_core-4.0.1/bsb/simulation/adapter.py` & `bsb_core-4.1.0/bsb/simulation/adapter.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,118 +1,118 @@
-import abc
-import itertools
-import types
-import typing
-from contextlib import ExitStack
-from time import time
-
-import numpy as np
-
-from .results import SimulationResult
-
-if typing.TYPE_CHECKING:
-    from ..storage import PlacementSet
-    from .cell import CellModel
-    from .simulation import Simulation
-
-
-class AdapterProgress:
-    def __init__(self, duration):
-        self._duration = duration
-        self._start = self._last_tick = time()
-        self._ticks = 0
-
-    def tick(self, step):
-        """
-        Report simulation progress.
-        """
-        now = time()
-        tic = now - self._last_tick
-        self._ticks += 1
-        el = now - self._start
-        progress = types.SimpleNamespace(
-            progression=step, duration=self._duration, time=time(), tick=tic, elapsed=el
-        )
-        self._last_tick = now
-        return progress
-
-    def steps(self, step=1):
-        steps = itertools.chain(np.arange(0, self._duration, step), (self._duration,))
-        a, b = itertools.tee(steps)
-        next(b, None)
-        yield from zip(a, b)
-
-    def complete(self):
-        return
-
-
-class SimulationData:
-    def __init__(self, simulation: "Simulation", result=None):
-        self.chunks = None
-        self.populations = dict()
-        self.placement: dict["CellModel", "PlacementSet"] = {
-            model: model.get_placement_set() for model in simulation.cell_models.values()
-        }
-        self.connections = dict()
-        self.devices = dict()
-        if result is None:
-            result = SimulationResult(simulation)
-        self.result: SimulationResult = result
-
-
-class SimulatorAdapter(abc.ABC):
-    def __init__(self):
-        self._progress_listeners = []
-        self.simdata: dict["Simulation", "SimulationData"] = dict()
-
-    def simulate(self, *simulations, post_prepare=None, comm=None):
-        """
-        Simulate the given simulations.
-        """
-        with ExitStack() as context:
-            for simulation in simulations:
-                context.enter_context(simulation.scaffold.storage.read_only())
-            alldata = []
-            for simulation in simulations:
-                data = self.prepare(simulation)
-                alldata.append(data)
-                for hook in simulation.post_prepare:
-                    hook(self, simulation, data)
-            if post_prepare:
-                post_prepare(self, simulations, alldata)
-            results = self.run(*simulations)
-            return [
-                self.collect(simulation, data, result)
-                for simulation, result in zip(simulations, results)
-            ]
-
-    @abc.abstractmethod
-    def prepare(self, simulation, comm=None):
-        """
-        Reset the simulation backend and prepare for the given simulation.
-
-        :param simulation: The simulation configuration to prepare.
-        :type simulation: ~bsb.simulation.simulation.Simulation
-        :param comm: The mpi4py MPI communicator to use. Only nodes in the communicator
-          will participate in the simulation. The first node will idle as the main node.
-        """
-        pass
-
-    @abc.abstractmethod
-    def run(self, *simulations, comm=None):
-        """
-        Fire up the prepared adapter.
-        """
-        pass
-
-    def collect(self, simulation, simdata, simresult, comm=None):
-        """
-        Collect the output of a simulation that completed
-        """
-        simresult.flush()
-        return simresult
-
-    def add_progress_listener(self, listener):
-        self._progress_listeners.append(listener)
-
-
-__all__ = ["AdapterProgress", "SimulationData", "SimulatorAdapter"]
+import abc
+import itertools
+import types
+import typing
+from contextlib import ExitStack
+from time import time
+
+import numpy as np
+
+from .results import SimulationResult
+
+if typing.TYPE_CHECKING:
+    from ..storage import PlacementSet
+    from .cell import CellModel
+    from .simulation import Simulation
+
+
+class AdapterProgress:
+    def __init__(self, duration):
+        self._duration = duration
+        self._start = self._last_tick = time()
+        self._ticks = 0
+
+    def tick(self, step):
+        """
+        Report simulation progress.
+        """
+        now = time()
+        tic = now - self._last_tick
+        self._ticks += 1
+        el = now - self._start
+        progress = types.SimpleNamespace(
+            progression=step, duration=self._duration, time=time(), tick=tic, elapsed=el
+        )
+        self._last_tick = now
+        return progress
+
+    def steps(self, step=1):
+        steps = itertools.chain(np.arange(0, self._duration, step), (self._duration,))
+        a, b = itertools.tee(steps)
+        next(b, None)
+        yield from zip(a, b)
+
+    def complete(self):
+        return
+
+
+class SimulationData:
+    def __init__(self, simulation: "Simulation", result=None):
+        self.chunks = None
+        self.populations = dict()
+        self.placement: dict["CellModel", "PlacementSet"] = {
+            model: model.get_placement_set() for model in simulation.cell_models.values()
+        }
+        self.connections = dict()
+        self.devices = dict()
+        if result is None:
+            result = SimulationResult(simulation)
+        self.result: SimulationResult = result
+
+
+class SimulatorAdapter(abc.ABC):
+    def __init__(self):
+        self._progress_listeners = []
+        self.simdata: dict["Simulation", "SimulationData"] = dict()
+
+    def simulate(self, *simulations, post_prepare=None, comm=None):
+        """
+        Simulate the given simulations.
+        """
+        with ExitStack() as context:
+            for simulation in simulations:
+                context.enter_context(simulation.scaffold.storage.read_only())
+            alldata = []
+            for simulation in simulations:
+                data = self.prepare(simulation)
+                alldata.append(data)
+                for hook in simulation.post_prepare:
+                    hook(self, simulation, data)
+            if post_prepare:
+                post_prepare(self, simulations, alldata)
+            results = self.run(*simulations)
+            return [
+                self.collect(simulation, data, result)
+                for simulation, result in zip(simulations, results)
+            ]
+
+    @abc.abstractmethod
+    def prepare(self, simulation, comm=None):
+        """
+        Reset the simulation backend and prepare for the given simulation.
+
+        :param simulation: The simulation configuration to prepare.
+        :type simulation: ~bsb.simulation.simulation.Simulation
+        :param comm: The mpi4py MPI communicator to use. Only nodes in the communicator
+          will participate in the simulation. The first node will idle as the main node.
+        """
+        pass
+
+    @abc.abstractmethod
+    def run(self, *simulations, comm=None):
+        """
+        Fire up the prepared adapter.
+        """
+        pass
+
+    def collect(self, simulation, simdata, simresult, comm=None):
+        """
+        Collect the output of a simulation that completed
+        """
+        simresult.flush()
+        return simresult
+
+    def add_progress_listener(self, listener):
+        self._progress_listeners.append(listener)
+
+
+__all__ = ["AdapterProgress", "SimulationData", "SimulatorAdapter"]
```

### Comparing `bsb_core-4.0.1/bsb/simulation/cell.py` & `bsb_core-4.1.0/bsb/simulation/cell.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-import typing
-
-from .. import config
-from ..config import refs
-from ..config._attrs import cfglist
-from .component import SimulationComponent
-from .parameter import Parameter
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-
-
-@config.node
-class CellModel(SimulationComponent):
-    """
-    Cell models are simulator specific representations of a cell type.
-    """
-
-    cell_type: "CellType" = config.ref(refs.cell_type_ref, key="name")
-    """
-    The cell type that this model represents
-    """
-    parameters: cfglist[Parameter] = config.list(type=Parameter)
-    """
-    The parameters of the model.
-    """
-
-    def __lt__(self, other):
-        try:
-            return self.name < other.name
-        except Exception:
-            return True
-
-    def get_placement_set(self, chunks=None):
-        return self.cell_type.get_placement_set(chunks=chunks)
-
-
-__all__ = ["CellModel"]
+import typing
+
+from .. import config
+from ..config import refs
+from ..config._attrs import cfglist
+from .component import SimulationComponent
+from .parameter import Parameter
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+
+
+@config.node
+class CellModel(SimulationComponent):
+    """
+    Cell models are simulator specific representations of a cell type.
+    """
+
+    cell_type: "CellType" = config.ref(refs.cell_type_ref, key="name")
+    """
+    The cell type that this model represents
+    """
+    parameters: cfglist[Parameter] = config.list(type=Parameter)
+    """
+    The parameters of the model.
+    """
+
+    def __lt__(self, other):
+        try:
+            return self.name < other.name
+        except Exception:
+            return True
+
+    def get_placement_set(self, chunks=None):
+        return self.cell_type.get_placement_set(chunks=chunks)
+
+
+__all__ = ["CellModel"]
```

### Comparing `bsb_core-4.0.1/bsb/simulation/results.py` & `bsb_core-4.1.0/bsb/simulation/results.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,62 +1,62 @@
-import traceback
-import typing
-
-from ..reporting import warn
-
-if typing.TYPE_CHECKING:
-    import neo
-
-
-class SimulationResult:
-    def __init__(self, simulation):
-        from neo import Block
-
-        tree = simulation.__tree__()
-        try:
-            del tree["post_prepare"]
-        except KeyError:
-            pass
-        self.block = Block(name=simulation.name, config=tree)
-        self.recorders = []
-
-    @property
-    def spiketrains(self):
-        return self.block.segments[0].spiketrains
-
-    @property
-    def analogsignals(self):
-        return self.block.segments[0].analogsignals
-
-    def add(self, recorder):
-        self.recorders.append(recorder)
-
-    def create_recorder(self, flush: typing.Callable[["neo.core.Segment"], None]):
-        recorder = SimulationRecorder()
-        recorder.flush = flush
-        self.add(recorder)
-        return recorder
-
-    def flush(self):
-        from neo import Segment
-
-        segment = Segment()
-        self.block.segments.append(segment)
-        for recorder in self.recorders:
-            try:
-                recorder.flush(segment)
-            except Exception as e:
-                traceback.print_exc()
-                warn("Recorder errored out!")
-
-    def write(self, filename, mode):
-        from neo import io
-
-        io.NixIO(filename, mode=mode).write(self.block)
-
-
-class SimulationRecorder:
-    def flush(self, segment: "neo.core.Segment"):
-        raise NotImplementedError("Recorders need to implement the `flush` function.")
-
-
-__all__ = ["SimulationResult", "SimulationRecorder"]
+import traceback
+import typing
+
+from ..reporting import warn
+
+if typing.TYPE_CHECKING:
+    import neo
+
+
+class SimulationResult:
+    def __init__(self, simulation):
+        from neo import Block
+
+        tree = simulation.__tree__()
+        try:
+            del tree["post_prepare"]
+        except KeyError:
+            pass
+        self.block = Block(name=simulation.name, config=tree)
+        self.recorders = []
+
+    @property
+    def spiketrains(self):
+        return self.block.segments[0].spiketrains
+
+    @property
+    def analogsignals(self):
+        return self.block.segments[0].analogsignals
+
+    def add(self, recorder):
+        self.recorders.append(recorder)
+
+    def create_recorder(self, flush: typing.Callable[["neo.core.Segment"], None]):
+        recorder = SimulationRecorder()
+        recorder.flush = flush
+        self.add(recorder)
+        return recorder
+
+    def flush(self):
+        from neo import Segment
+
+        segment = Segment()
+        self.block.segments.append(segment)
+        for recorder in self.recorders:
+            try:
+                recorder.flush(segment)
+            except Exception as e:
+                traceback.print_exc()
+                warn("Recorder errored out!")
+
+    def write(self, filename, mode):
+        from neo import io
+
+        io.NixIO(filename, mode=mode).write(self.block)
+
+
+class SimulationRecorder:
+    def flush(self, segment: "neo.core.Segment"):
+        raise NotImplementedError("Recorders need to implement the `flush` function.")
+
+
+__all__ = ["SimulationResult", "SimulationRecorder"]
```

### Comparing `bsb_core-4.0.1/bsb/simulation/simulation.py` & `bsb_core-4.1.0/bsb/simulation/simulation.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,66 +1,66 @@
-import typing
-from time import time
-
-from .. import config
-from ..config import types as cfgtypes
-from ..config._attrs import cfgdict, cfglist
-from ._backends import get_simulation_nodes
-from .cell import CellModel
-from .connection import ConnectionModel
-from .device import DeviceModel
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-    from ..connectivity import ConnectionStrategy
-    from ..core import Scaffold
-    from ..storage.interfaces import ConnectivitySet
-
-
-class ProgressEvent:
-    def __init__(self, progression, duration, time):
-        self.progression = progression
-        self.duration = duration
-        self.time = time
-
-
-@config.pluggable(key="simulator", plugin_name="simulation backend")
-class Simulation:
-    scaffold: "Scaffold"
-    simulator: str
-    name: str = config.attr(key=True)
-    duration: float = config.attr(type=float, required=True)
-    cell_models: cfgdict[CellModel] = config.slot(type=CellModel, required=True)
-    connection_models: cfgdict[ConnectionModel] = config.slot(
-        type=ConnectionModel, required=True
-    )
-    devices: cfgdict[DeviceModel] = config.slot(type=DeviceModel, required=True)
-    post_prepare: cfglist[typing.Callable[["Simulation", typing.Any], None]] = (
-        config.list(type=cfgtypes.function_())
-    )
-
-    @staticmethod
-    def __plugins__():
-        return get_simulation_nodes()
-
-    def get_model_of(
-        self, type: typing.Union["CellType", "ConnectionStrategy"]
-    ) -> typing.Optional[typing.Union["CellModel", "ConnectionModel"]]:
-        cell_models = [cm for cm in self.cell_models.values() if cm.cell_type is type]
-        if cell_models:
-            return cell_models[0]
-        conn_models = [
-            cm for cm in self.connection_models.values() if cm.connection_type is type
-        ]
-        if conn_models:
-            return conn_models[0]
-
-    def get_connectivity_sets(
-        self,
-    ) -> typing.Mapping["ConnectionModel", "ConnectivitySet"]:
-        return {
-            model: self.scaffold.get_connectivity_set(model.name)
-            for model in sorted(self.connection_models.values())
-        }
-
-
-__all__ = ["ProgressEvent", "Simulation"]
+import typing
+from time import time
+
+from .. import config
+from ..config import types as cfgtypes
+from ..config._attrs import cfgdict, cfglist
+from ._backends import get_simulation_nodes
+from .cell import CellModel
+from .connection import ConnectionModel
+from .device import DeviceModel
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+    from ..connectivity import ConnectionStrategy
+    from ..core import Scaffold
+    from ..storage.interfaces import ConnectivitySet
+
+
+class ProgressEvent:
+    def __init__(self, progression, duration, time):
+        self.progression = progression
+        self.duration = duration
+        self.time = time
+
+
+@config.pluggable(key="simulator", plugin_name="simulation backend")
+class Simulation:
+    scaffold: "Scaffold"
+    simulator: str
+    name: str = config.attr(key=True)
+    duration: float = config.attr(type=float, required=True)
+    cell_models: cfgdict[CellModel] = config.slot(type=CellModel, required=True)
+    connection_models: cfgdict[ConnectionModel] = config.slot(
+        type=ConnectionModel, required=True
+    )
+    devices: cfgdict[DeviceModel] = config.slot(type=DeviceModel, required=True)
+    post_prepare: cfglist[typing.Callable[["Simulation", typing.Any], None]] = (
+        config.list(type=cfgtypes.function_())
+    )
+
+    @staticmethod
+    def __plugins__():
+        return get_simulation_nodes()
+
+    def get_model_of(
+        self, type: typing.Union["CellType", "ConnectionStrategy"]
+    ) -> typing.Optional[typing.Union["CellModel", "ConnectionModel"]]:
+        cell_models = [cm for cm in self.cell_models.values() if cm.cell_type is type]
+        if cell_models:
+            return cell_models[0]
+        conn_models = [
+            cm for cm in self.connection_models.values() if cm.connection_type is type
+        ]
+        if conn_models:
+            return conn_models[0]
+
+    def get_connectivity_sets(
+        self,
+    ) -> typing.Mapping["ConnectionModel", "ConnectivitySet"]:
+        return {
+            model: self.scaffold.get_connectivity_set(model.name)
+            for model in sorted(self.connection_models.values())
+        }
+
+
+__all__ = ["ProgressEvent", "Simulation"]
```

### Comparing `bsb_core-4.0.1/bsb/simulation/targetting.py` & `bsb_core-4.1.0/bsb/simulation/targetting.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,304 +1,304 @@
-import functools
-import math
-import typing
-
-import numpy as np
-from numpy.random import default_rng
-
-from .. import config
-from ..config import refs, types
-
-if typing.TYPE_CHECKING:
-    from .cell import CellModel
-
-
-@config.dynamic(attr_name="strategy", default="all", auto_classmap=True)
-class Targetting:
-    type: typing.Union[typing.Literal["cell"], typing.Literal["connection"]] = (
-        config.attr(type=types.in_(["cell", "connection"]), default="cell")
-    )
-
-    def get_targets(self, adapter, simulation, simdata):
-        if self.type == "cell":
-            return simdata.populations
-        elif self.type == "connection":
-            return simdata.connections
-
-
-@config.node
-class CellTargetting(Targetting, classmap_entry="all"):
-    @config.property
-    def type(self):
-        return "cell"
-
-    def get_targets(self, adapter, simulation, simdata):
-        return simdata.populations
-
-
-@config.node
-class ConnectionTargetting(Targetting, classmap_entry="all_connections"):
-    @config.property
-    def type(self):
-        return "connection"
-
-    def get_targets(self, adapter, simulation, simdata):
-        return simdata.connections
-
-
-class CellModelFilter:
-    cell_models: list["CellModel"] = config.reflist(
-        refs.sim_cell_model_ref, required=False
-    )
-
-    def get_targets(self, adapter, simulation, simdata):
-        return {
-            model: pop
-            for model, pop in simdata.populations.items()
-            if not self.cell_models or model in self.cell_models
-        }
-
-
-class FractionFilter:
-    count = config.attr(
-        type=int, required=types.mut_excl("fraction", "count", required=False)
-    )
-    fraction = config.attr(
-        type=types.fraction(),
-        required=types.mut_excl("fraction", "count", required=False),
-    )
-
-    def satisfy_fractions(self, targets):
-        return {model: self._frac(data) for model, data in targets.items()}
-
-    def _frac(self, data):
-        take = None
-        if self.count is not None:
-            take = self.count
-        if self.fraction is not None:
-            take = math.floor(len(data) * self.fraction)
-        if take is None:
-            return data
-        else:
-            # Select `take` elements from data with a boolean mask (otherwise a sorted
-            # integer mask would be required)
-            idx = np.zeros(len(data), dtype=bool)
-            idx[np.random.default_rng().integers(0, len(data), take)] = True
-            return data[idx]
-
-    @staticmethod
-    def filter(f):
-        @functools.wraps(f)
-        def wrapper(self, *args, **kwargs):
-            return self.satisfy_fractions(f(self, *args, **kwargs))
-
-        return wrapper
-
-
-@config.node
-class CellModelTargetting(
-    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="cell_model"
-):
-    """
-    Targets all cells of certain cell models.
-    """
-
-    cell_models: list["CellModel"] = config.reflist(
-        refs.sim_cell_model_ref, required=True
-    )
-
-    @FractionFilter.filter
-    def get_targets(self, adapter, simulation, simdata):
-        return super().get_targets(adapter, simulation, simdata)
-
-
-@config.node
-class RepresentativesTargetting(
-    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="representatives"
-):
-    """
-    Targets all identifiers of certain cell types.
-    """
-
-    n: int = config.attr(type=int, default=1)
-
-    @FractionFilter.filter
-    def get_targets(self, adapter, simulation, simdata):
-        return {
-            model: default_rng().choice(len(pop), size=self.n, replace=False)
-            for model, pop in super().get_targets(adapter, simulation, simdata)
-        }
-
-
-@config.node
-class ByIdTargetting(FractionFilter, CellTargetting, classmap_entry="by_id"):
-    """
-    Targets all given identifiers.
-    """
-
-    ids: dict[str, list[int]] = config.attr(
-        type=types.dict(type=types.list(type=int)), required=True
-    )
-
-    @FractionFilter.filter
-    def get_targets(self, adapter, simulation, simdata):
-        by_name = {model.name: model for model in simdata.populations.keys()}
-        return {
-            model: simdata.populations[model][ids]
-            for model_name, ids in self.ids.items()
-            if (model := by_name.get(model_name)) is not None
-        }
-
-
-@config.node
-class ByLabelTargetting(
-    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="by_label"
-):
-    """
-    Targets all given labels.
-    """
-
-    labels: list[str] = config.attr(type=types.list(type=str), required=True)
-
-    @FractionFilter.filter
-    def get_targets(self, adapter, simulation, simdata):
-        return {
-            model: simdata.populations[
-                simdata.placement[model].get_label_mask(self.labels)
-            ]
-            for model in super().get_targets(adapter, simulation, simdata).keys()
-        }
-
-
-@config.node
-class CylindricalTargetting(
-    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="cylinder"
-):
-    """
-    Targets all cells in a cylinder along specified axis.
-    """
-
-    origin: list[float] = config.attr(type=types.list(type=float, size=2))
-    axis: typing.Union[typing.Literal["x"], typing.Literal["y"], typing.Literal["z"]] = (
-        config.attr(type=types.in_(["x", "y", "z"]), default="y")
-    )
-    radius: float = config.attr(type=float, required=True)
-
-    @FractionFilter.filter
-    def get_targets(self, adapter, simulation, simdata):
-        """
-        Target all or certain cells within a cylinder of specified radius.
-        """
-        if self.axis == "x":
-            axes = [1, 2]
-        elif self.axis == "y":
-            axes = [0, 2]
-        else:
-            axes = [0, 1]
-        return {
-            model: simdata.populations[model][
-                np.sum(
-                    simdata.placement[model].load_positions()[:, axes] - self.origin**2,
-                    axis=0,
-                )
-                < self.radius**2
-            ]
-            for model in super().get_targets(adapter, simulation, simdata).keys()
-        }
-
-
-@config.node
-class SphericalTargetting(
-    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="sphere"
-):
-    """
-    Targets all cells in a sphere.
-    """
-
-    origin: list[float] = config.attr(type=types.list(type=float, size=3), required=True)
-    radius: float = config.attr(type=float, required=True)
-
-    @FractionFilter.filter
-    def get_targets(self, adapter, simulation, simdata):
-        """
-        Target all or certain cells within a sphere of specified radius.
-        """
-        return {
-            model: simdata.populations[model][
-                (
-                    np.sum(
-                        (simdata.placement[model].load_positions() - self.origin) ** 2,
-                        axis=1,
-                    )
-                    < self.radius**2
-                )
-            ]
-            for model in super().get_targets(adapter, simulation, simdata).keys()
-        }
-
-
-@config.dynamic(
-    attr_name="strategy",
-    default="everywhere",
-    auto_classmap=True,
-    classmap_entry="everywhere",
-)
-class LocationTargetting:
-    def get_locations(self, cell):
-        return cell.locations
-
-
-@config.node
-class SomaTargetting(LocationTargetting, classmap_entry="soma"):
-    def get_locations(self, cell):
-        return [cell.locations[(0, 0)]]
-
-
-@config.node
-class LabelTargetting(LocationTargetting, classmap_entry="label"):
-    labels = config.list(required=True)
-
-    def get_locations(self, cell):
-        locs = [
-            loc
-            for loc in cell.locations.values()
-            if all(l in loc.section.labels for l in self.labels)
-        ]
-        return locs
-
-
-@config.node
-class BranchLocTargetting(LabelTargetting, classmap_entry="branch"):
-    x = config.attr(type=types.fraction(), default=0.5)
-
-    def get_locations(self, cell):
-        locations = super().get_locations(cell)
-        branches = set()
-        selected = []
-        for loc in locations:
-            if (
-                loc._loc[0] not in branches
-                and loc.arc(0) <= self.x
-                and loc.arc(1) > self.x
-            ):
-                selected.append(loc)
-                branches.add(loc._loc[0])
-        return selected
-
-
-__all__ = [
-    "BranchLocTargetting",
-    "ByIdTargetting",
-    "ByLabelTargetting",
-    "CellModelFilter",
-    "CellModelTargetting",
-    "CellTargetting",
-    "ConnectionTargetting",
-    "CylindricalTargetting",
-    "FractionFilter",
-    "LabelTargetting",
-    "LocationTargetting",
-    "RepresentativesTargetting",
-    "SomaTargetting",
-    "SphericalTargetting",
-    "Targetting",
-]
+import functools
+import math
+import typing
+
+import numpy as np
+from numpy.random import default_rng
+
+from .. import config
+from ..config import refs, types
+
+if typing.TYPE_CHECKING:
+    from .cell import CellModel
+
+
+@config.dynamic(attr_name="strategy", default="all", auto_classmap=True)
+class Targetting:
+    type: typing.Union[typing.Literal["cell"], typing.Literal["connection"]] = (
+        config.attr(type=types.in_(["cell", "connection"]), default="cell")
+    )
+
+    def get_targets(self, adapter, simulation, simdata):
+        if self.type == "cell":
+            return simdata.populations
+        elif self.type == "connection":
+            return simdata.connections
+
+
+@config.node
+class CellTargetting(Targetting, classmap_entry="all"):
+    @config.property
+    def type(self):
+        return "cell"
+
+    def get_targets(self, adapter, simulation, simdata):
+        return simdata.populations
+
+
+@config.node
+class ConnectionTargetting(Targetting, classmap_entry="all_connections"):
+    @config.property
+    def type(self):
+        return "connection"
+
+    def get_targets(self, adapter, simulation, simdata):
+        return simdata.connections
+
+
+class CellModelFilter:
+    cell_models: list["CellModel"] = config.reflist(
+        refs.sim_cell_model_ref, required=False
+    )
+
+    def get_targets(self, adapter, simulation, simdata):
+        return {
+            model: pop
+            for model, pop in simdata.populations.items()
+            if not self.cell_models or model in self.cell_models
+        }
+
+
+class FractionFilter:
+    count = config.attr(
+        type=int, required=types.mut_excl("fraction", "count", required=False)
+    )
+    fraction = config.attr(
+        type=types.fraction(),
+        required=types.mut_excl("fraction", "count", required=False),
+    )
+
+    def satisfy_fractions(self, targets):
+        return {model: self._frac(data) for model, data in targets.items()}
+
+    def _frac(self, data):
+        take = None
+        if self.count is not None:
+            take = self.count
+        if self.fraction is not None:
+            take = math.floor(len(data) * self.fraction)
+        if take is None:
+            return data
+        else:
+            # Select `take` elements from data with a boolean mask (otherwise a sorted
+            # integer mask would be required)
+            idx = np.zeros(len(data), dtype=bool)
+            idx[np.random.default_rng().integers(0, len(data), take)] = True
+            return data[idx]
+
+    @staticmethod
+    def filter(f):
+        @functools.wraps(f)
+        def wrapper(self, *args, **kwargs):
+            return self.satisfy_fractions(f(self, *args, **kwargs))
+
+        return wrapper
+
+
+@config.node
+class CellModelTargetting(
+    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="cell_model"
+):
+    """
+    Targets all cells of certain cell models.
+    """
+
+    cell_models: list["CellModel"] = config.reflist(
+        refs.sim_cell_model_ref, required=True
+    )
+
+    @FractionFilter.filter
+    def get_targets(self, adapter, simulation, simdata):
+        return super().get_targets(adapter, simulation, simdata)
+
+
+@config.node
+class RepresentativesTargetting(
+    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="representatives"
+):
+    """
+    Targets all identifiers of certain cell types.
+    """
+
+    n: int = config.attr(type=int, default=1)
+
+    @FractionFilter.filter
+    def get_targets(self, adapter, simulation, simdata):
+        return {
+            model: default_rng().choice(len(pop), size=self.n, replace=False)
+            for model, pop in super().get_targets(adapter, simulation, simdata)
+        }
+
+
+@config.node
+class ByIdTargetting(FractionFilter, CellTargetting, classmap_entry="by_id"):
+    """
+    Targets all given identifiers.
+    """
+
+    ids: dict[str, list[int]] = config.attr(
+        type=types.dict(type=types.list(type=int)), required=True
+    )
+
+    @FractionFilter.filter
+    def get_targets(self, adapter, simulation, simdata):
+        by_name = {model.name: model for model in simdata.populations.keys()}
+        return {
+            model: simdata.populations[model][ids]
+            for model_name, ids in self.ids.items()
+            if (model := by_name.get(model_name)) is not None
+        }
+
+
+@config.node
+class ByLabelTargetting(
+    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="by_label"
+):
+    """
+    Targets all given labels.
+    """
+
+    labels: list[str] = config.attr(type=types.list(type=str), required=True)
+
+    @FractionFilter.filter
+    def get_targets(self, adapter, simulation, simdata):
+        return {
+            model: simdata.populations[
+                simdata.placement[model].get_label_mask(self.labels)
+            ]
+            for model in super().get_targets(adapter, simulation, simdata).keys()
+        }
+
+
+@config.node
+class CylindricalTargetting(
+    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="cylinder"
+):
+    """
+    Targets all cells in a cylinder along specified axis.
+    """
+
+    origin: list[float] = config.attr(type=types.list(type=float, size=2))
+    axis: typing.Union[typing.Literal["x"], typing.Literal["y"], typing.Literal["z"]] = (
+        config.attr(type=types.in_(["x", "y", "z"]), default="y")
+    )
+    radius: float = config.attr(type=float, required=True)
+
+    @FractionFilter.filter
+    def get_targets(self, adapter, simulation, simdata):
+        """
+        Target all or certain cells within a cylinder of specified radius.
+        """
+        if self.axis == "x":
+            axes = [1, 2]
+        elif self.axis == "y":
+            axes = [0, 2]
+        else:
+            axes = [0, 1]
+        return {
+            model: simdata.populations[model][
+                np.sum(
+                    simdata.placement[model].load_positions()[:, axes] - self.origin**2,
+                    axis=0,
+                )
+                < self.radius**2
+            ]
+            for model in super().get_targets(adapter, simulation, simdata).keys()
+        }
+
+
+@config.node
+class SphericalTargetting(
+    CellModelFilter, FractionFilter, CellTargetting, classmap_entry="sphere"
+):
+    """
+    Targets all cells in a sphere.
+    """
+
+    origin: list[float] = config.attr(type=types.list(type=float, size=3), required=True)
+    radius: float = config.attr(type=float, required=True)
+
+    @FractionFilter.filter
+    def get_targets(self, adapter, simulation, simdata):
+        """
+        Target all or certain cells within a sphere of specified radius.
+        """
+        return {
+            model: simdata.populations[model][
+                (
+                    np.sum(
+                        (simdata.placement[model].load_positions() - self.origin) ** 2,
+                        axis=1,
+                    )
+                    < self.radius**2
+                )
+            ]
+            for model in super().get_targets(adapter, simulation, simdata).keys()
+        }
+
+
+@config.dynamic(
+    attr_name="strategy",
+    default="everywhere",
+    auto_classmap=True,
+    classmap_entry="everywhere",
+)
+class LocationTargetting:
+    def get_locations(self, cell):
+        return cell.locations
+
+
+@config.node
+class SomaTargetting(LocationTargetting, classmap_entry="soma"):
+    def get_locations(self, cell):
+        return [cell.locations[(0, 0)]]
+
+
+@config.node
+class LabelTargetting(LocationTargetting, classmap_entry="label"):
+    labels = config.list(required=True)
+
+    def get_locations(self, cell):
+        locs = [
+            loc
+            for loc in cell.locations.values()
+            if all(l in loc.section.labels for l in self.labels)
+        ]
+        return locs
+
+
+@config.node
+class BranchLocTargetting(LabelTargetting, classmap_entry="branch"):
+    x = config.attr(type=types.fraction(), default=0.5)
+
+    def get_locations(self, cell):
+        locations = super().get_locations(cell)
+        branches = set()
+        selected = []
+        for loc in locations:
+            if (
+                loc._loc[0] not in branches
+                and loc.arc(0) <= self.x
+                and loc.arc(1) > self.x
+            ):
+                selected.append(loc)
+                branches.add(loc._loc[0])
+        return selected
+
+
+__all__ = [
+    "BranchLocTargetting",
+    "ByIdTargetting",
+    "ByLabelTargetting",
+    "CellModelFilter",
+    "CellModelTargetting",
+    "CellTargetting",
+    "ConnectionTargetting",
+    "CylindricalTargetting",
+    "FractionFilter",
+    "LabelTargetting",
+    "LocationTargetting",
+    "RepresentativesTargetting",
+    "SomaTargetting",
+    "SphericalTargetting",
+    "Targetting",
+]
```

### Comparing `bsb_core-4.0.1/bsb/storage/_files.py` & `bsb_core-4.1.0/bsb/storage/_files.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,641 +1,661 @@
-import abc as _abc
-import contextlib as _cl
-import datetime as _dt
-import email.utils as _eml
-import functools as _ft
-import hashlib as _hl
-import os as _os
-import pathlib as _pl
-import tempfile as _tf
-import time as _t
-import typing
-import typing as _tp
-import urllib.parse as _up
-import urllib.request as _ur
-
-import nrrd as _nrrd
-import requests as _rq
-
-from .. import config
-from .._util import obj_str_insert
-from ..config import types
-from ..config._attrs import cfglist
-from ..morphologies.parsers import MorphologyParser
-from ..reporting import warn
-
-if _tp.TYPE_CHECKING:
-    from ..core import Scaffold
-    from ..morphologies import Morphology
-    from ..storage.interfaces import FileStore
-
-
-def _is_uri(url):
-    return _up.urlparse(url).scheme != ""
-
-
-def _uri_to_path(uri):
-    parsed = _up.urlparse(uri)
-    host = "{0}{0}{mnt}{0}".format(_os.path.sep, mnt=parsed.netloc)
-    return _os.path.relpath(
-        _os.path.normpath(_os.path.join(host, _ur.url2pathname(_up.unquote(parsed.path))))
-    )
-
-
-class FileDependency:
-    def __init__(
-        self,
-        source: typing.Union[str, _os.PathLike],
-        file_store: "FileStore" = None,
-        ext: str = None,
-        cache=True,
-    ):
-        self._given_source: str = str(source)
-        if _is_uri(self._given_source):
-            self._uri = self._given_source
-        else:
-            path = _pl.Path(source).absolute()
-            self._uri: str = path.as_uri()
-            ext = ext or path.suffix[1:] or None
-        self._scheme: "FileScheme" = _get_scheme(_up.urlparse(self._uri).scheme)
-        self.file_store = file_store
-        self.extension = ext
-        self.cache = cache
-
-    @property
-    def uri(self):
-        return self._uri
-
-    def __hash__(self):
-        return hash(self._uri)
-
-    def __inv__(self):
-        return self._given_source
-
-    @obj_str_insert
-    def __str__(self):
-        return f"'{self._uri}'"
-
-    def get_content(self, check_store=True):
-        if check_store:
-            stored = self.get_stored_file()
-            if stored is None or self._scheme.should_update(self, stored):
-                content = self._get_content()
-                if self.cache:
-                    self.store_content(content, meta=self._scheme.get_meta(self))
-                return content
-            else:
-                return stored.load()
-        else:
-            return self._get_content()
-
-    def get_meta(self, check_store=True):
-        if (
-            not check_store or ((stored := self.get_stored_file()) is None)
-        ) or self._scheme.should_update(self, stored):
-            return self._scheme.get_meta(self)
-        else:
-            return stored.meta
-
-    @_cl.contextmanager
-    def provide_locally(self):
-        try:
-            path = self._scheme.get_local_path(self)
-            if _os.path.exists(path):
-                yield (path, None)
-            else:
-                raise FileNotFoundError()
-        except (TypeError, FileNotFoundError):
-            if self.file_store is None:
-                raise FileNotFoundError(f"Can't find {self}")
-            content = self.get_content()
-            with _tf.TemporaryDirectory() as dirpath:
-                name = "file"
-                if self.extension:
-                    name = f"{name}.{self.extension}"
-                filepath = _os.path.join(dirpath, name)
-                with open(filepath, "wb") as f:
-                    f.write(content[0])
-                yield (filepath, content[1])
-
-    def provide_stream(self):
-        return self._scheme.provide_stream(self)
-
-    def get_stored_file(self):
-        if not self.file_store:
-            raise ValueError(
-                "Can't check for file dependency in store before scaffold is ready."
-            )
-        return self.file_store.find_meta("source", self._given_source)
-
-    def store_content(self, content, encoding=None, meta=None):
-        if not self.file_store:
-            raise ValueError(
-                "Can't store file dependency in store before scaffold is ready."
-            )
-        if isinstance(content, tuple):
-            content, encoding = content
-        # Save the file under the same id if it already exists
-        id_ = getattr(self.get_stored_file(), "id", None)
-        if meta is None:
-            meta = {}
-        meta.update(self._scheme.get_meta(self))
-        meta["source"] = self._given_source
-        return self.file_store.store(
-            content, meta=meta, encoding=encoding, id=id_, overwrite=True
-        )
-
-    def should_update(self):
-        if not self.file_store:
-            raise ValueError(
-                "Can't update file dependency in store before scaffold is ready."
-            )
-        stored = self.get_stored_file()
-        return stored is None or self._scheme.should_update(self, stored)
-
-    def _get_content(self):
-        if not self._scheme.find(self):
-            raise FileNotFoundError(f"Couldn't find {self._uri}")
-        return self._scheme.get_content(self)
-
-    def update(self, force=False):
-        if force or self.should_update():
-            self.get_content()
-
-
-class UriScheme(_abc.ABC):
-    @_abc.abstractmethod
-    def find(self, file: FileDependency):
-        path = _uri_to_path(file.uri)
-        return _os.path.exists(path)
-
-    @_abc.abstractmethod
-    def should_update(self, file: FileDependency, stored_file):
-        path = _uri_to_path(file.uri)
-        try:
-            file_mtime = _os.path.getmtime(path)
-        except FileNotFoundError:
-            return False
-        try:
-            stored_mtime = stored_file.mtime
-        except Exception:
-            return True
-        return file_mtime > stored_mtime
-
-    @_abc.abstractmethod
-    def get_content(self, file: FileDependency):
-        with self.provide_stream(file) as (fp, encoding):
-            fp: _tp.BinaryIO
-            return (fp.read(), encoding)
-
-    @_cl.contextmanager
-    @_abc.abstractmethod
-    def provide_stream(self, file):
-        path = _uri_to_path(file.uri)
-        with open(path, "rb") as fp:
-            yield (fp, None)
-
-    @_abc.abstractmethod
-    def get_meta(self, file: FileDependency):
-        return {}
-
-    @_abc.abstractmethod
-    def get_local_path(self, file: FileDependency):
-        raise RuntimeError(f"{type(self)} has no local path representation")
-
-
-class FileScheme(UriScheme):
-    def find(self, file: FileDependency):
-        return super().find(file)
-
-    def should_update(self, file: FileDependency, stored_file):
-        return super().should_update(file, stored_file)
-
-    def get_content(self, file: FileDependency):
-        return super().get_content(file)
-
-    def provide_stream(self, file: FileDependency):
-        return super().provide_stream(file)
-
-    def get_meta(self, file: FileDependency):
-        return super().get_meta(file)
-
-    def get_local_path(self, file: FileDependency):
-        return _uri_to_path(file.uri)
-
-
-class UrlScheme(UriScheme):
-    def resolve_uri(self, file: FileDependency):
-        return file.uri
-
-    def find(self, file: FileDependency):
-        with self.create_session() as session:
-            response = session.head(self.resolve_uri(file))
-        return response.status_code == 200
-
-    def should_update(self, file: FileDependency, stored_file):
-        mtime = stored_file.mtime
-        headers = self.get_meta(file).get("headers", {})
-        stored_headers = stored_file.meta.get("headers", {})
-        if "ETag" in headers:
-            new_etag = headers["ETag"]
-            old_etag = stored_headers.get("ETag")
-            # Check if we have the latest ETag
-            return old_etag != new_etag
-        elif "Last-Modified" in headers:
-            their_mtime = _dt.datetime(
-                *_eml.parsedate(headers["Last-Modified"])[:6]
-            ).timestamp()
-            return their_mtime > mtime
-        # 100h default expiration
-        return _t.time() > mtime + 360000
-
-    def get_content(self, file: FileDependency):
-        with self.create_session() as session:
-            response = session.get(self.resolve_uri(file))
-        return (response.content, response.encoding)
-
-    def get_meta(self, file: FileDependency):
-        with self.create_session() as session:
-            response = session.head(self.resolve_uri(file))
-        return {"headers": dict(response.headers)}
-
-    def get_local_path(self, file: FileDependency):
-        raise TypeError("URL schemes don't have a local path")
-
-    @_cl.contextmanager
-    def provide_stream(self, file):
-        with self.create_session() as session:
-            response = session.get(self.resolve_uri(file), stream=True)
-        response.raw.decode_content = True
-        response.raw.auto_close = False
-        yield (response.raw, response.encoding)
-
-    def create_session(self):
-        return _rq.Session()
-
-    def get_base_url(self):
-        raise NotImplementedError("Base UrlScheme has no fixed base URL.")
-
-
-class NeuroMorphoScheme(UrlScheme):
-    _nm_url = "https://neuromorpho.org/"
-    _meta = "api/neuron/name/"
-    _files = "dableFiles/"
-
-    def resolve_uri(self, file: FileDependency):
-        meta = self.get_nm_meta(file)
-        return self._swc_url(meta["archive"], meta["neuron_name"])
-
-    @_ft.cache
-    def get_nm_meta(self, file: FileDependency):
-        name = _up.urlparse(file.uri).hostname
-        # urlparse gives lowercase, so slice out the original cased NM name
-        idx = file.uri.lower().find(name)
-        name = file.uri[idx : (idx + len(name))]
-        with self.create_session() as session:
-            try:
-                res = session.get(self._nm_url + self._meta + name)
-            except Exception as e:
-                return {"archive": "none", "neuron_name": "none"}
-            if res.status_code == 404:
-                res = session.get(self._nm_url)
-                if res.status_code != 200 or "Service Interruption Notice" in res.text:
-                    warn(f"NeuroMorpho.org is down, can't retrieve morphology '{name}'.")
-                    return {"archive": "none", "neuron_name": "none"}
-                raise IOError(f"'{name}' is not a valid NeuroMorpho name.")
-            elif res.status_code != 200:
-                raise IOError("NeuroMorpho API error: " + res.text)
-        return res.json()
-
-    def get_base_url(self):
-        return self._nm_url
-
-    def get_meta(self, file: FileDependency):
-        meta = super().get_meta(file)
-        meta["neuromorpho_data"] = self.get_nm_meta(file)
-        return meta
-
-    @classmethod
-    def _swc_url(cls, archive, name):
-        base_url = f"{cls._nm_url}{cls._files}{_up.quote(archive.lower())}"
-        return f"{base_url}/CNG%20version/{name}.CNG.swc"
-
-    def create_session(self):
-        # Weak DH key on neuromorpho.org
-        # https://stackoverflow.com/a/76217135/1016004
-        from requests.adapters import HTTPAdapter
-        from urllib3 import PoolManager
-        from urllib3.util import create_urllib3_context
-
-        class DHAdapter(HTTPAdapter):
-            def init_poolmanager(self, connections, maxsize, block=False, **kwargs):
-                ctx = create_urllib3_context(ciphers=":HIGH:!DH:!aNULL")
-                self.poolmanager = PoolManager(
-                    num_pools=connections,
-                    maxsize=maxsize,
-                    block=block,
-                    ssl_context=ctx,
-                    **kwargs,
-                )
-
-        session = _rq.Session()
-        session.mount(self._nm_url, DHAdapter())
-        return session
-
-
-@_ft.cache
-def _get_schemes() -> _tp.Mapping[str, typing.Type[FileScheme]]:
-    from ..plugins import discover
-
-    schemes = discover("storage.schemes")
-    schemes["file"] = FileScheme
-    schemes["http"] = schemes["https"] = UrlScheme
-    schemes["nm"] = NeuroMorphoScheme
-    return schemes
-
-
-def _get_scheme(scheme: str) -> FileScheme:
-    schemes = _get_schemes()
-    try:
-        return schemes[scheme]()
-    except KeyError:
-        raise KeyError(f"{scheme} is not a known file scheme.")
-
-
-@config.node
-class FileDependencyNode:
-    scaffold: "Scaffold"
-    file: "FileDependency" = config.attr(type=FileDependency)
-
-    def __init__(self, value=None, **kwargs):
-        if value is not None:
-            self.file = value
-
-    def __boot__(self):
-        self.file.file_store = self.scaffold.files
-        if self.scaffold.is_main_process():
-            self.file.update()
-
-    def __inv__(self):
-        if not isinstance(self, FileDependencyNode):
-            return self
-        if self._config_pos_init:
-            return self.file._given_source
-        else:
-            tree = self.__tree__()
-            return tree
-
-    def load_object(self):
-        return self.file.get_content()
-
-    def provide_locally(self):
-        return self.file.provide_locally()
-
-    def provide_stream(self):
-        return self.file.provide_stream()
-
-    def get_stored_file(self):
-        return self.file.get_stored_file()
-
-
-@config.node
-class CodeDependencyNode(FileDependencyNode):
-    module: str = config.attr(type=str, required=types.shortform())
-    attr: str = config.attr(type=str)
-
-    @config.property
-    def file(self):
-        if getattr(self, "scaffold", None) is not None:
-            file_store = self.scaffold.files
-        else:
-            file_store = None
-        return FileDependency(
-            self.module.replace(".", _os.sep) + ".py", file_store=file_store
-        )
-
-    def __init__(self, module=None, **kwargs):
-        super().__init__(**kwargs)
-        if module is not None:
-            self.module = module
-
-    def load_object(self):
-        import importlib.util
-        import sys
-
-        sys.path.append(_os.getcwd())
-        try:
-            with self.file.provide_locally() as (path, encoding):
-                spec = importlib.util.spec_from_file_location(self.module, path)
-                module = importlib.util.module_from_spec(spec)
-                sys.modules[self.module] = module
-                spec.loader.exec_module(module)
-                return module if self.attr is None else module[self.attr]
-        finally:
-            tmp = list(reversed(sys.path))
-            tmp.remove(_os.getcwd())
-            sys.path = list(reversed(tmp))
-
-
-class OperationCallable(typing.Protocol):
-    def __call__(self, obj: object, **kwargs: typing.Any) -> object:
-        pass
-
-
-@config.node
-class Operation:
-    func: OperationCallable = config.attr(type=types.function_())
-    parameters: dict[typing.Any] = config.catch_all(type=types.any_())
-
-    def __init__(self, value=None, /, **kwargs):
-        if value is not None:
-            self.func = value
-
-    def __call__(self, obj):
-        return self.func(obj, **self.parameters)
-
-
-class FilePipelineMixin:
-    pipeline: cfglist[Operation] = config.list(type=Operation)
-
-    def pipe(self, input):
-        return _ft.reduce(lambda state, func: func(state), self.pipeline, input)
-
-
-@config.node
-class NrrdDependencyNode(FilePipelineMixin, FileDependencyNode):
-    """
-    Configuration dependency node to load NRRD files.
-    """
-
-    def get_header(self):
-        with self.file.provide_locally() as (path, encoding):
-            return _nrrd.read_header(path)
-
-    def get_data(self):
-        with self.file.provide_locally() as (path, encoding):
-            return _nrrd.read(path)[0]
-
-    def load_object(self):
-        return self.pipe(self.get_data())
-
-
-class MorphologyOperationCallable(OperationCallable):
-    """
-    Hello
-    """
-
-    def __call__(self, obj: "Morphology", **kwargs: typing.Any) -> "Morphology":
-        pass
-
-
-@config.node
-class MorphologyOperation(Operation):
-    func: MorphologyOperationCallable = config.attr(
-        type=types.method_shortcut("bsb.morphologies.Morphology")
-    )
-
-
-@config.node
-class MorphologyDependencyNode(FilePipelineMixin, FileDependencyNode):
-    """
-    Configuration dependency node to load morphology files.
-    The content of these files will be stored in bsb.morphologies.Morphology instances.
-    """
-
-    pipeline: cfglist[MorphologyOperation] = config.list(type=MorphologyOperation)
-    name: str = config.attr(type=str, default=None, required=False)
-    parser: MorphologyParser = config.attr(type=MorphologyParser, default={})
-    """
-    Name associated to the morphology. If not provided, the program will use the name of the file 
-    in which the morphology is stored. 
-    """
-
-    def store_content(self, content, *args, encoding=None, meta=None):
-        if meta is None:
-            meta = {}
-        meta["_hash"] = self._hash(content)
-        meta["_stale"] = True
-        stored = super().store_content(content, *args, encoding=encoding, meta=meta)
-        return stored
-
-    def load_object(self, parser=None, save=True) -> "Morphology":
-        if parser is None or self.__class__.parser.is_dirty(self):
-            parser = self.parser
-        self.file.update()
-        stored = self.get_stored_file()
-        meta = stored.meta
-        if meta.get("_stale", True):
-            content, meta = stored.load()
-            if hasattr(parser, "parse_content"):
-                morpho = parser.parse_content(
-                    content.decode(meta.get("encoding", "utf8"))
-                )
-            else:
-                morpho = parser.parse(self.file)
-            morpho.meta = meta
-            morpho = self.pipe(morpho)
-            meta["hash"] = self._hash(content)
-            meta["_stale"] = False
-            morpho.meta = meta
-            if save:
-                self.scaffold.morphologies.save(
-                    self.get_morphology_name(), morpho, overwrite=True
-                )
-            return morpho
-        else:
-            return self.scaffold.morphologies.load(self.get_morphology_name())
-
-    def get_morphology_name(self):
-        """
-        Returns morphology name provided by the user or extract it from its file name.
-
-        :returns: Morphology name
-        :rtype: str
-        """
-        return self.name if self.name is not None else _pl.Path(self.file.uri).stem
-
-    def store_object(self, morpho, hash_):
-        """
-        Save a morphology into the circuit file under the name of this instance morphology.
-
-        :param hash_: Hash key to store as metadata with the morphology
-        :type hash_: str
-        :param morpho: Morphology to store
-        :type morpho: bsb.morphologies.Morphology
-        """
-        self.scaffold.morphologies.save(
-            self.get_morphology_name(), morpho, meta={"hash": hash_}
-        )
-
-    def _hash(self, content):
-        md5 = _hl.new("md5", usedforsecurity=False)
-        if isinstance(content, str):
-            md5.update(content.encode("utf-8"))
-        else:
-            md5.update(content)
-        return md5.hexdigest()
-
-    def queue(self, pool):
-        """
-        Add the loading of the current morphology to a job queue.
-
-        :param pool: Queue of jobs.
-        :type pool: bsb.services.pool.JobPool
-        """
-
-        def create_morphology(scaffold, i):
-            scaffold.configuration.morphologies[i].load_object()
-
-        pool.queue(
-            create_morphology, (self._config_index,), submitter=self, uri=self.file.uri
-        )
-
-
-@config.node
-class MorphologyPipelineNode(FilePipelineMixin):
-    files: list[MorphologyDependencyNode] = config.list(
-        type=MorphologyDependencyNode, required=True
-    )
-    pipeline: cfglist[MorphologyOperation] = config.list(type=MorphologyOperation)
-    parser: MorphologyParser = config.attr(type=MorphologyParser, required=False)
-
-    def queue(self, pool):
-        """
-        Add the loading of the current morphology to a job queue.
-
-        :param pool: Queue of jobs.
-        :type pool:bsb.services.pool.JobPool
-        """
-
-        def job(scaffold, i, j):
-            me = scaffold.configuration.morphologies[i]
-            fnode = me.files[j]
-            morpho = fnode.load_object(parser=me.parser, save=False)
-            morpho_out = me.pipe(morpho)
-            scaffold.morphologies.save(
-                fnode.get_morphology_name(), morpho_out, overwrite=True
-            )
-
-        for k in range(len(self.files)):
-            pool.queue(
-                job,
-                (self._config_index, k),
-                submitter=self,
-                node=k,
-                uri=self.files[k].file.uri,
-            )
-
-
-__all__ = [
-    "CodeDependencyNode",
-    "FileDependency",
-    "FileDependencyNode",
-    "FileScheme",
-    "MorphologyDependencyNode",
-    "MorphologyOperation",
-    "NeuroMorphoScheme",
-    "NrrdDependencyNode",
-    "Operation",
-    "UriScheme",
-    "UrlScheme",
-]
+import abc as _abc
+import contextlib as _cl
+import datetime as _dt
+import email.utils as _eml
+import functools as _ft
+import hashlib as _hl
+import os as _os
+import pathlib as _pl
+import tempfile as _tf
+import time as _t
+import typing
+import typing as _tp
+import urllib.parse as _up
+import urllib.request as _ur
+
+import nrrd as _nrrd
+import requests as _rq
+
+from .. import config
+from .._util import obj_str_insert
+from ..config import types
+from ..config._attrs import cfglist
+from ..morphologies.parsers import MorphologyParser
+from ..reporting import warn
+
+if _tp.TYPE_CHECKING:
+    from ..core import Scaffold
+    from ..morphologies import Morphology
+    from ..storage.interfaces import FileStore
+
+
+def _is_uri(url):
+    return _up.urlparse(url).scheme != ""
+
+
+def _uri_to_path(uri):
+    parsed = _up.urlparse(uri)
+    host = "{0}{0}{mnt}{0}".format(_os.path.sep, mnt=parsed.netloc)
+    return _os.path.relpath(
+        _os.path.normpath(_os.path.join(host, _ur.url2pathname(_up.unquote(parsed.path))))
+    )
+
+
+class FileDependency:
+    def __init__(
+        self,
+        source: typing.Union[str, _os.PathLike],
+        file_store: "FileStore" = None,
+        ext: str = None,
+        cache=True,
+    ):
+        self._given_source: str = str(source)
+        if _is_uri(self._given_source):
+            self._uri = self._given_source
+        else:
+            path = _pl.Path(source).absolute()
+            self._uri: str = path.as_uri()
+            ext = ext or path.suffix[1:] or None
+        self._scheme: "FileScheme" = _get_scheme(_up.urlparse(self._uri).scheme)
+        self.file_store = file_store
+        self.extension = ext
+        self.cache = cache
+
+    @property
+    def uri(self):
+        return self._uri
+
+    def __hash__(self):
+        return hash(self._uri)
+
+    def __inv__(self):
+        return self._given_source
+
+    @obj_str_insert
+    def __str__(self):
+        return f"'{self._uri}'"
+
+    def get_content(self, check_store=True):
+        if check_store:
+            stored = self.get_stored_file()
+            if stored is None or self._scheme.should_update(self, stored):
+                content = self._get_content()
+                if self.cache:
+                    self.store_content(content, meta=self._scheme.get_meta(self))
+                return content
+            else:
+                return stored.load()
+        else:
+            return self._get_content()
+
+    def get_meta(self, check_store=True):
+        if (
+            not check_store or ((stored := self.get_stored_file()) is None)
+        ) or self._scheme.should_update(self, stored):
+            return self._scheme.get_meta(self)
+        else:
+            return stored.meta
+
+    @_cl.contextmanager
+    def provide_locally(self):
+        try:
+            path = self._scheme.get_local_path(self)
+            if _os.path.exists(path):
+                yield (path, None)
+            else:
+                raise FileNotFoundError()
+        except (TypeError, FileNotFoundError):
+            if self.file_store is None:
+                raise FileNotFoundError(f"Can't find {self}")
+            content = self.get_content()
+            with _tf.TemporaryDirectory() as dirpath:
+                name = "file"
+                if self.extension:
+                    name = f"{name}.{self.extension}"
+                filepath = _os.path.join(dirpath, name)
+                with open(filepath, "wb") as f:
+                    f.write(content[0])
+                yield (filepath, content[1])
+
+    def provide_stream(self):
+        return self._scheme.provide_stream(self)
+
+    def get_stored_file(self):
+        if not self.file_store:
+            raise ValueError(
+                "Can't check for file dependency in store before scaffold is ready."
+            )
+        return self.file_store.find_meta("source", self._given_source)
+
+    def store_content(self, content, encoding=None, meta=None):
+        if not self.file_store:
+            raise ValueError(
+                "Can't store file dependency in store before scaffold is ready."
+            )
+        if isinstance(content, tuple):
+            content, encoding = content
+        # Save the file under the same id if it already exists
+        id_ = getattr(self.get_stored_file(), "id", None)
+        if meta is None:
+            meta = {}
+        meta.update(self._scheme.get_meta(self))
+        meta["source"] = self._given_source
+        return self.file_store.store(
+            content, meta=meta, encoding=encoding, id=id_, overwrite=True
+        )
+
+    def should_update(self):
+        if not self.file_store:
+            raise ValueError(
+                "Can't update file dependency in store before scaffold is ready."
+            )
+        stored = self.get_stored_file()
+        return stored is None or self._scheme.should_update(self, stored)
+
+    def _get_content(self):
+        if not self._scheme.find(self):
+            raise FileNotFoundError(f"Couldn't find {self._uri}")
+        return self._scheme.get_content(self)
+
+    def update(self, force=False):
+        if force or self.should_update():
+            self.get_content()
+
+
+class UriScheme(_abc.ABC):
+    @_abc.abstractmethod
+    def find(self, file: FileDependency):
+        path = _uri_to_path(file.uri)
+        return _os.path.exists(path)
+
+    @_abc.abstractmethod
+    def should_update(self, file: FileDependency, stored_file):
+        path = _uri_to_path(file.uri)
+        try:
+            file_mtime = _os.path.getmtime(path)
+        except FileNotFoundError:
+            return False
+        try:
+            stored_mtime = stored_file.mtime
+        except Exception:
+            return True
+        return file_mtime > stored_mtime
+
+    @_abc.abstractmethod
+    def get_content(self, file: FileDependency):
+        with self.provide_stream(file) as (fp, encoding):
+            fp: _tp.BinaryIO
+            return (fp.read(), encoding)
+
+    @_cl.contextmanager
+    @_abc.abstractmethod
+    def provide_stream(self, file):
+        path = _uri_to_path(file.uri)
+        with open(path, "rb") as fp:
+            yield (fp, None)
+
+    @_abc.abstractmethod
+    def get_meta(self, file: FileDependency):
+        return {}
+
+    @_abc.abstractmethod
+    def get_local_path(self, file: FileDependency):
+        raise RuntimeError(f"{type(self)} has no local path representation")
+
+
+class FileScheme(UriScheme):
+    def find(self, file: FileDependency):
+        return super().find(file)
+
+    def should_update(self, file: FileDependency, stored_file):
+        return super().should_update(file, stored_file)
+
+    def get_content(self, file: FileDependency):
+        return super().get_content(file)
+
+    def provide_stream(self, file: FileDependency):
+        return super().provide_stream(file)
+
+    def get_meta(self, file: FileDependency):
+        return super().get_meta(file)
+
+    def get_local_path(self, file: FileDependency):
+        return _uri_to_path(file.uri)
+
+
+class UrlScheme(UriScheme):
+    def resolve_uri(self, file: FileDependency):
+        return file.uri
+
+    def find(self, file: FileDependency):
+        with self.create_session() as session:
+            response = session.head(self.resolve_uri(file))
+        return response.status_code == 200
+
+    def should_update(self, file: FileDependency, stored_file):
+        mtime = stored_file.mtime
+        headers = self.get_meta(file).get("headers", {})
+        stored_headers = stored_file.meta.get("headers", {})
+        if "ETag" in headers:
+            new_etag = headers["ETag"]
+            old_etag = stored_headers.get("ETag")
+            # Check if we have the latest ETag
+            return old_etag != new_etag
+        elif "Last-Modified" in headers:
+            their_mtime = _dt.datetime(
+                *_eml.parsedate(headers["Last-Modified"])[:6]
+            ).timestamp()
+            return their_mtime > mtime
+        # 100h default expiration
+        return _t.time() > mtime + 360000
+
+    def get_content(self, file: FileDependency):
+        with self.create_session() as session:
+            response = session.get(self.resolve_uri(file))
+        return (response.content, response.encoding)
+
+    def get_meta(self, file: FileDependency):
+        with self.create_session() as session:
+            response = session.head(self.resolve_uri(file))
+        return {"headers": dict(response.headers)}
+
+    def get_local_path(self, file: FileDependency):
+        raise TypeError("URL schemes don't have a local path")
+
+    @_cl.contextmanager
+    def provide_stream(self, file):
+        with self.create_session() as session:
+            response = session.get(self.resolve_uri(file), stream=True)
+        response.raw.decode_content = True
+        response.raw.auto_close = False
+        yield (response.raw, response.encoding)
+
+    def create_session(self):
+        return _rq.Session()
+
+    def get_base_url(self):
+        raise NotImplementedError("Base UrlScheme has no fixed base URL.")
+
+
+class NeuroMorphoScheme(UrlScheme):
+    _nm_url = "https://neuromorpho.org/"
+    _meta = "api/neuron/name/"
+    _files = "dableFiles/"
+
+    def resolve_uri(self, file: FileDependency):
+        meta = self.get_nm_meta(file)
+        return self._swc_url(meta["archive"], meta["neuron_name"])
+
+    @_ft.cache
+    def get_nm_meta(self, file: FileDependency):
+        name = _up.urlparse(file.uri).hostname
+        # urlparse gives lowercase, so slice out the original cased NM name
+        idx = file.uri.lower().find(name)
+        name = file.uri[idx : (idx + len(name))]
+        with self.create_session() as session:
+            try:
+                res = session.get(self._nm_url + self._meta + name)
+            except Exception as e:
+                return {"archive": "none", "neuron_name": "none"}
+            if res.status_code == 404:
+                res = session.get(self._nm_url)
+                if res.status_code != 200 or "Service Interruption Notice" in res.text:
+                    warn(f"NeuroMorpho.org is down, can't retrieve morphology '{name}'.")
+                    return {"archive": "none", "neuron_name": "none"}
+                raise IOError(f"'{name}' is not a valid NeuroMorpho name.")
+            elif res.status_code != 200:
+                raise IOError("NeuroMorpho API error: " + res.text)
+        return res.json()
+
+    def get_base_url(self):
+        return self._nm_url
+
+    def get_meta(self, file: FileDependency):
+        meta = super().get_meta(file)
+        meta["neuromorpho_data"] = self.get_nm_meta(file)
+        return meta
+
+    @classmethod
+    def _swc_url(cls, archive, name):
+        base_url = f"{cls._nm_url}{cls._files}{_up.quote(archive.lower())}"
+        return f"{base_url}/CNG%20version/{name}.CNG.swc"
+
+    def create_session(self):
+        # Weak DH key on neuromorpho.org
+        # https://stackoverflow.com/a/76217135/1016004
+        from requests.adapters import HTTPAdapter
+        from urllib3 import PoolManager
+        from urllib3.util import create_urllib3_context
+
+        class DHAdapter(HTTPAdapter):
+            def init_poolmanager(self, connections, maxsize, block=False, **kwargs):
+                ctx = create_urllib3_context(ciphers=":HIGH:!DH:!aNULL")
+                self.poolmanager = PoolManager(
+                    num_pools=connections,
+                    maxsize=maxsize,
+                    block=block,
+                    ssl_context=ctx,
+                    **kwargs,
+                )
+
+        session = _rq.Session()
+        session.mount(self._nm_url, DHAdapter())
+        return session
+
+
+@_ft.cache
+def _get_schemes() -> _tp.Mapping[str, typing.Type[FileScheme]]:
+    from ..plugins import discover
+
+    schemes = discover("storage.schemes")
+    schemes["file"] = FileScheme
+    schemes["http"] = schemes["https"] = UrlScheme
+    schemes["nm"] = NeuroMorphoScheme
+    return schemes
+
+
+def _get_scheme(scheme: str) -> FileScheme:
+    schemes = _get_schemes()
+    try:
+        return schemes[scheme]()
+    except KeyError:
+        raise KeyError(f"{scheme} is not a known file scheme.")
+
+
+@config.node
+class FileDependencyNode:
+    scaffold: "Scaffold"
+    file: "FileDependency" = config.attr(type=FileDependency)
+
+    def __init__(self, value=None, **kwargs):
+        if value is not None:
+            self.file = value
+
+    def __boot__(self):
+        self.file.file_store = self.scaffold.files
+        if self.scaffold.is_main_process():
+            self.file.update()
+
+    def __inv__(self):
+        if not isinstance(self, FileDependencyNode):
+            return self
+        if self._config_pos_init:
+            return self.file._given_source
+        else:
+            tree = self.__tree__()
+            return tree
+
+    def load_object(self):
+        return self.file.get_content()
+
+    def provide_locally(self):
+        return self.file.provide_locally()
+
+    def provide_stream(self):
+        return self.file.provide_stream()
+
+    def get_stored_file(self):
+        return self.file.get_stored_file()
+
+
+@config.node
+class CodeDependencyNode(FileDependencyNode):
+    """
+    Allow the loading of external code during network loading.
+    """
+
+    module: str = config.attr(type=str, required=types.shortform())
+    """Should be either the path to a python file or a import like string"""
+    attr: str = config.attr(type=str)
+    """Attribute to extract from the loaded script"""
+
+    @config.property
+    def file(self):
+        import os
+
+        if getattr(self, "scaffold", None) is not None:
+            file_store = self.scaffold.files
+        else:
+            file_store = None
+        if os.path.isfile(self.module):
+            # Convert potential relative path to absolute path
+            module_file = os.path.abspath(os.path.join(os.getcwd(), self.module))
+        else:
+            # Module like string converted to a path string relative to current folder
+            module_file = "./" + self.module.replace(".", _os.sep) + ".py"
+        return FileDependency(module_file, file_store=file_store)
+
+    def __init__(self, module=None, **kwargs):
+        super().__init__(**kwargs)
+        if module is not None:
+            self.module = module
+
+    def __inv__(self):
+        if not isinstance(self, CodeDependencyNode):
+            return self
+        res = {"module": getattr(self, "module")}
+        if self.attr is not None:
+            res["attr"] = self.attr
+        return res
+
+    def load_object(self):
+        import importlib.util
+        import sys
+
+        sys.path.append(_os.getcwd())
+        try:
+            with self.file.provide_locally() as (path, encoding):
+                spec = importlib.util.spec_from_file_location(self.module, path)
+                module = importlib.util.module_from_spec(spec)
+                sys.modules[self.module] = module
+                spec.loader.exec_module(module)
+                return module if self.attr is None else getattr(module, self.attr)
+        finally:
+            tmp = list(reversed(sys.path))
+            tmp.remove(_os.getcwd())
+            sys.path = list(reversed(tmp))
+
+
+class OperationCallable(typing.Protocol):
+    def __call__(self, obj: object, **kwargs: typing.Any) -> object:
+        pass
+
+
+@config.node
+class Operation:
+    func: OperationCallable = config.attr(type=types.function_())
+    parameters: dict[typing.Any] = config.catch_all(type=types.any_())
+
+    def __init__(self, value=None, /, **kwargs):
+        if value is not None:
+            self.func = value
+
+    def __call__(self, obj):
+        return self.func(obj, **self.parameters)
+
+
+class FilePipelineMixin:
+    pipeline: cfglist[Operation] = config.list(type=Operation)
+
+    def pipe(self, input):
+        return _ft.reduce(lambda state, func: func(state), self.pipeline, input)
+
+
+@config.node
+class NrrdDependencyNode(FilePipelineMixin, FileDependencyNode):
+    """
+    Configuration dependency node to load NRRD files.
+    """
+
+    def get_header(self):
+        with self.file.provide_locally() as (path, encoding):
+            return _nrrd.read_header(path)
+
+    def get_data(self):
+        with self.file.provide_locally() as (path, encoding):
+            return _nrrd.read(path)[0]
+
+    def load_object(self):
+        return self.pipe(self.get_data())
+
+
+class MorphologyOperationCallable(OperationCallable):
+    """
+    Hello
+    """
+
+    def __call__(self, obj: "Morphology", **kwargs: typing.Any) -> "Morphology":
+        pass
+
+
+@config.node
+class MorphologyOperation(Operation):
+    func: MorphologyOperationCallable = config.attr(
+        type=types.method_shortcut("bsb.morphologies.Morphology")
+    )
+
+
+@config.node
+class MorphologyDependencyNode(FilePipelineMixin, FileDependencyNode):
+    """
+    Configuration dependency node to load morphology files.
+    The content of these files will be stored in bsb.morphologies.Morphology instances.
+    """
+
+    pipeline: cfglist[MorphologyOperation] = config.list(type=MorphologyOperation)
+    name: str = config.attr(type=str, default=None, required=False)
+    parser: MorphologyParser = config.attr(type=MorphologyParser, default={})
+    """
+    Name associated to the morphology. If not provided, the program will use the name of the file 
+    in which the morphology is stored. 
+    """
+
+    def store_content(self, content, *args, encoding=None, meta=None):
+        if meta is None:
+            meta = {}
+        meta["_hash"] = self._hash(content)
+        meta["_stale"] = True
+        stored = super().store_content(content, *args, encoding=encoding, meta=meta)
+        return stored
+
+    def load_object(self, parser=None, save=True) -> "Morphology":
+        if parser is None or self.__class__.parser.is_dirty(self):
+            parser = self.parser
+        self.file.update()
+        stored = self.get_stored_file()
+        meta = stored.meta
+        if meta.get("_stale", True):
+            content, meta = stored.load()
+            if hasattr(parser, "parse_content"):
+                morpho = parser.parse_content(
+                    content.decode(meta.get("encoding", "utf8"))
+                )
+            else:
+                morpho = parser.parse(self.file)
+            morpho.meta = meta
+            morpho = self.pipe(morpho)
+            meta["hash"] = self._hash(content)
+            meta["_stale"] = False
+            morpho.meta = meta
+            if save:
+                self.scaffold.morphologies.save(
+                    self.get_morphology_name(), morpho, overwrite=True
+                )
+            return morpho
+        else:
+            return self.scaffold.morphologies.load(self.get_morphology_name())
+
+    def get_morphology_name(self):
+        """
+        Returns morphology name provided by the user or extract it from its file name.
+
+        :returns: Morphology name
+        :rtype: str
+        """
+        return self.name if self.name is not None else _pl.Path(self.file.uri).stem
+
+    def store_object(self, morpho, hash_):
+        """
+        Save a morphology into the circuit file under the name of this instance morphology.
+
+        :param hash_: Hash key to store as metadata with the morphology
+        :type hash_: str
+        :param morpho: Morphology to store
+        :type morpho: bsb.morphologies.Morphology
+        """
+        self.scaffold.morphologies.save(
+            self.get_morphology_name(), morpho, meta={"hash": hash_}
+        )
+
+    def _hash(self, content):
+        md5 = _hl.new("md5", usedforsecurity=False)
+        if isinstance(content, str):
+            md5.update(content.encode("utf-8"))
+        else:
+            md5.update(content)
+        return md5.hexdigest()
+
+    def queue(self, pool):
+        """
+        Add the loading of the current morphology to a job queue.
+
+        :param pool: Queue of jobs.
+        :type pool: bsb.services.pool.JobPool
+        """
+
+        def create_morphology(scaffold, i):
+            scaffold.configuration.morphologies[i].load_object()
+
+        pool.queue(
+            create_morphology, (self._config_index,), submitter=self, uri=self.file.uri
+        )
+
+
+@config.node
+class MorphologyPipelineNode(FilePipelineMixin):
+    files: list[MorphologyDependencyNode] = config.list(
+        type=MorphologyDependencyNode, required=True
+    )
+    pipeline: cfglist[MorphologyOperation] = config.list(type=MorphologyOperation)
+    parser: MorphologyParser = config.attr(type=MorphologyParser, required=False)
+
+    def queue(self, pool):
+        """
+        Add the loading of the current morphology to a job queue.
+
+        :param pool: Queue of jobs.
+        :type pool:bsb.services.pool.JobPool
+        """
+
+        def job(scaffold, i, j):
+            me = scaffold.configuration.morphologies[i]
+            fnode = me.files[j]
+            morpho = fnode.load_object(parser=me.parser, save=False)
+            morpho_out = me.pipe(morpho)
+            scaffold.morphologies.save(
+                fnode.get_morphology_name(), morpho_out, overwrite=True
+            )
+
+        for k in range(len(self.files)):
+            pool.queue(
+                job,
+                (self._config_index, k),
+                submitter=self,
+                node=k,
+                uri=self.files[k].file.uri,
+            )
+
+
+__all__ = [
+    "CodeDependencyNode",
+    "FileDependency",
+    "FileDependencyNode",
+    "FileScheme",
+    "MorphologyDependencyNode",
+    "MorphologyOperation",
+    "NeuroMorphoScheme",
+    "NrrdDependencyNode",
+    "Operation",
+    "UriScheme",
+    "UrlScheme",
+]
```

### Comparing `bsb_core-4.0.1/bsb/storage/_util.py` & `bsb_core-4.1.0/bsb/storage/_util.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,33 +1,33 @@
-import functools
-import pathlib
-import typing
-
-import appdirs
-
-from ._files import FileDependency
-
-if typing.TYPE_CHECKING:
-    from .interfaces import Storage
-
-_bsb_dirs = appdirs.AppDirs("bsb")
-_cache_path = pathlib.Path(_bsb_dirs.user_cache_dir)
-
-cache: "Storage"
-
-
-def __getattr__(name):
-    if name == "cache":
-        return _get_cache_storage()
-    else:
-        raise AttributeError(f"{__name__} has no attribute '{name}.")
-
-
-@functools.cache
-def _get_cache_storage():
-    from ..storage import Storage
-
-    return Storage("fs", _cache_path)
-
-
-def _cached_file(source, cache=True):
-    return FileDependency(source, file_store=_get_cache_storage().files, cache=cache)
+import functools
+import pathlib
+import typing
+
+import appdirs
+
+from ._files import FileDependency
+
+if typing.TYPE_CHECKING:
+    from .interfaces import Storage
+
+_bsb_dirs = appdirs.AppDirs("bsb")
+_cache_path = pathlib.Path(_bsb_dirs.user_cache_dir)
+
+cache: "Storage"
+
+
+def __getattr__(name):
+    if name == "cache":
+        return _get_cache_storage()
+    else:
+        raise AttributeError(f"{__name__} has no attribute '{name}.")
+
+
+@functools.cache
+def _get_cache_storage():
+    from ..storage import Storage
+
+    return Storage("fs", _cache_path)
+
+
+def _cached_file(source, cache=True):
+    return FileDependency(source, file_store=_get_cache_storage().files, cache=cache)
```

### Comparing `bsb_core-4.0.1/bsb/storage/decorators.py` & `bsb_core-4.1.0/bsb/storage/decorators.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,44 +1,44 @@
-def on_main(prep=None, ret=None):
-    def decorator(f):
-        def wrapper(self, *args, **kwargs):
-            r = None
-            self.comm.barrier()
-            if self.comm.get_rank() == 0:
-                r = f(self, *args, **kwargs)
-            elif prep:
-                prep(self, *args, **kwargs)
-            self.comm.barrier()
-            if not ret:
-                return self.comm.bcast(r, root=0)
-            else:
-                return ret(self, *args, **kwargs)
-
-        return wrapper
-
-    return decorator
-
-
-def on_main_until(until, prep=None, ret=None):
-    def decorator(f):
-        def wrapper(self, *args, **kwargs):
-            global _procpass
-            r = None
-            self.comm.barrier()
-            if self.comm.get_rank() == 0:
-                r = f(self, *args, **kwargs)
-            elif prep:
-                prep(self, *args, **kwargs)
-            self.comm.barrier()
-            while not until(self, *args, **kwargs):
-                pass
-            if not ret:
-                return self.comm.bcast(r, root=0)
-            else:
-                return ret(self, *args, **kwargs)
-
-        return wrapper
-
-    return decorator
-
-
-__all__ = ["on_main", "on_main_until"]
+def on_main(prep=None, ret=None):
+    def decorator(f):
+        def wrapper(self, *args, **kwargs):
+            r = None
+            self.comm.barrier()
+            if self.comm.get_rank() == 0:
+                r = f(self, *args, **kwargs)
+            elif prep:
+                prep(self, *args, **kwargs)
+            self.comm.barrier()
+            if not ret:
+                return self.comm.bcast(r, root=0)
+            else:
+                return ret(self, *args, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+def on_main_until(until, prep=None, ret=None):
+    def decorator(f):
+        def wrapper(self, *args, **kwargs):
+            global _procpass
+            r = None
+            self.comm.barrier()
+            if self.comm.get_rank() == 0:
+                r = f(self, *args, **kwargs)
+            elif prep:
+                prep(self, *args, **kwargs)
+            self.comm.barrier()
+            while not until(self, *args, **kwargs):
+                pass
+            if not ret:
+                return self.comm.bcast(r, root=0)
+            else:
+                return ret(self, *args, **kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+__all__ = ["on_main", "on_main_until"]
```

### Comparing `bsb_core-4.0.1/bsb/storage/interfaces.py` & `bsb_core-4.1.0/bsb/storage/interfaces.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,1218 +1,1218 @@
-import abc
-import functools
-import typing
-from pathlib import Path
-
-import numpy as np
-
-from .. import config, plugins
-from .._util import immutable, obj_str_insert
-from ..trees import BoxTree
-from ._chunks import Chunk
-
-if typing.TYPE_CHECKING:
-    from ..cell_types import CellType
-
-
-@config.pluggable(key="engine", plugin_name="storage engine")
-class StorageNode:
-    root: typing.Any = config.slot()
-
-    @classmethod
-    def __plugins__(cls):
-        if not hasattr(cls, "_plugins"):
-            cls._plugins = {
-                name: plugin.StorageNode
-                for name, plugin in plugins.discover("storage.engines").items()
-            }
-        return cls._plugins
-
-
-class Interface(abc.ABC):
-    _iface_engine_key = None
-
-    def __init__(self, engine):
-        self._engine = engine
-
-    def __init_subclass__(cls, **kwargs):
-        # Only change engine key if explicitly given.
-        if "engine_key" in kwargs:
-            cls._iface_engine_key = kwargs["engine_key"]
-
-
-class NoopLock:
-    def __enter__(self):
-        pass
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        pass
-
-
-class Engine(Interface):
-    """
-    Engines perform the transactions that come from the storage object, and read/write
-    data in a specific format. They can perform collective or individual actions.
-
-    .. warning::
-
-      Collective actions can only be performed from all nodes, or deadlocks occur. This
-      means in particular that they may not be called from component code.
-
-    """
-
-    def __init__(self, root, comm):
-        self._root = comm.bcast(root, root=0)
-        self._comm = comm
-        self._readonly = False
-
-    def __eq__(self, other):
-        eq_format = self._format == getattr(other, "_format", None)
-        eq_root = self._root == getattr(other, "_root", None)
-        return eq_format and eq_root
-
-    @property
-    def root(self):
-        """
-        The unique identifier for the storage. Usually pathlike, but can be anything.
-        """
-        return self._root
-
-    @property
-    def comm(self):
-        """
-        The communicator in charge of collective operations.
-        """
-        return self._comm
-
-    def set_comm(self, comm):
-        """
-        :guilabel:`collective` Set a new communicator in charge of collective operations.
-        """
-        self._comm = comm
-
-    @property
-    def format(self):
-        """
-        Name of the type of engine. Automatically set through the plugin system.
-        """
-        return self._format
-
-    @property
-    @abc.abstractmethod
-    def versions(self):
-        """
-        Must return a dictionary containing the version of the engine package, and bsb
-        package, used to last write to this storage object.
-        """
-        pass
-
-    @property
-    @abc.abstractmethod
-    def root_slug(self):
-        """
-        Must return a pathlike unique identifier for the root of the storage object.
-        """
-        pass
-
-    @abc.abstractmethod
-    def recognizes(self, root):
-        """
-        Must return whether the given argument is recognized as a valid storage object.
-        """
-        pass
-
-    @classmethod
-    def peek_exists(cls, root):
-        """
-        Must peek at the existence of the given root, without instantiating anything.
-        """
-        try:
-            return Path(root).exists()
-        except Exception:
-            return False
-
-    @abc.abstractmethod
-    def exists(self):
-        """
-        Must check existence of the storage object.
-        """
-        pass
-
-    @abc.abstractmethod
-    def create(self):
-        """
-        :guilabel:`collective` Must create the storage engine.
-        """
-        pass
-
-    @abc.abstractmethod
-    def move(self, new_root):
-        """
-        :guilabel:`collective` Must move the storage object to the new root.
-        """
-        pass
-
-    @abc.abstractmethod
-    def copy(self, new_root):
-        """
-        :guilabel:`collective` Must copy the storage object to the new root.
-        """
-        pass
-
-    @abc.abstractmethod
-    def remove(self):
-        """
-        :guilabel:`collective` Must remove the storage object.
-        """
-        pass
-
-    @abc.abstractmethod
-    def clear_placement(self):
-        """
-        :guilabel:`collective` Must clear existing placement data.
-        """
-        pass
-
-    @abc.abstractmethod
-    def clear_connectivity(self):
-        """
-        :guilabel:`collective` Must clear existing connectivity data.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_chunk_stats(self):
-        """
-        :guilabel:`readonly` Must return a dictionary with all chunk statistics.
-        """
-        pass
-
-    def read_only(self):
-        """
-        A context manager that enters the engine into readonly mode. In
-        readonly mode the engine does not perform any locking, write-operations or network
-        synchronization, and errors out if a write operation is attempted.
-        """
-        self._readonly = True
-        return ReadOnlyManager(self)
-
-    def readwrite(self):
-        self._readonly = False
-
-
-class ReadOnlyManager:
-    def __init__(self, engine):
-        self._e = engine
-
-    def __enter__(self):
-        self._e._readonly = True
-
-    def __exit__(self, *args):
-        self._e._readonly = False
-
-
-class NetworkDescription(Interface):
-    pass
-
-
-class FileStore(Interface, engine_key="files"):
-    """
-    Interface for the storage and retrieval of files essential to the network description.
-    """
-
-    @abc.abstractmethod
-    def all(self):
-        """
-        Return all ids and associated metadata in the file store.
-        """
-        pass
-
-    @abc.abstractmethod
-    def store(self, content, id=None, meta=None, encoding=None, overwrite=False):
-        """
-        Store content in the file store. Should also store the current timestamp as
-        `mtime` meta.
-
-        :param content: Content to be stored
-        :type content: str
-        :param id: Optional specific id for the content to be stored under.
-        :type id: str
-        :param meta: Metadata for the content
-        :type meta: dict
-        :param encoding: Optional encoding
-        :type encoding: str
-        :param overwrite: Overwrite existing file
-        :type overwrite: bool
-        :returns: The id the content was stored under
-        :rtype: str
-        """
-        pass
-
-    @abc.abstractmethod
-    def load(self, id):
-        """
-        Load the content of an object in the file store.
-
-        :param id: id of the content to be loaded.
-        :type id: str
-        :returns: The content of the stored object
-        :rtype: str
-        :raises FileNotFoundError: The given id doesn't exist in the file store.
-        """
-        pass
-
-    @abc.abstractmethod
-    def remove(self, id):
-        """
-        Remove the content of an object in the file store.
-
-        :param id: id of the content to be removed.
-        :type id: str
-        :raises FileNotFoundError: The given id doesn't exist in the file store.
-        """
-        pass
-
-    @abc.abstractmethod
-    def store_active_config(self, config):
-        """
-        Store configuration in the file store and mark it as the active configuration of
-        the stored network.
-
-        :param config: Configuration to be stored
-        :type config: :class:`~.config.Configuration`
-        :returns: The id the config was stored under
-        :rtype: str
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_active_config(self):
-        """
-        Load the active configuration stored in the file store.
-
-        :returns: The active configuration
-        :rtype: :class:`~.config.Configuration`
-        :raises Exception: When there's no active configuration in the file store.
-        """
-        pass
-
-    @abc.abstractmethod
-    def has(self, id):
-        """
-        Must return whether the file store has a file with the given id.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_mtime(self, id):
-        """
-        Must return the last modified timestamp of file with the given id.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_encoding(self, id):
-        """
-        Must return the encoding of the file with the given id, or None if it is
-        unspecified binary data.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_meta(self, id) -> typing.Mapping[str, typing.Any]:
-        """
-        Must return the metadata of the given id.
-        """
-        pass
-
-    def get(self, id) -> "StoredFile":
-        """
-        Return a StoredFile wrapper
-        """
-        if not self.has(id):
-            raise FileNotFoundError(f"File with id '{id}' not found.")
-        return StoredFile(self, id)
-
-    def find_files(self, predicate):
-        return (
-            StoredFile(self, id_) for id_, m in self.all().items() if predicate(id_, m)
-        )
-
-    def find_file(self, predicate):
-        return next(self.find_files(predicate), None)
-
-    def find_id(self, id):
-        return self.find_file(lambda id_, _: id_ == id)
-
-    def find_meta(self, key, value):
-        return self.find_file(lambda _, meta: meta.get(key, None) == value)
-
-
-class StoredFile:
-    def __init__(self, store, id):
-        self.store = store
-        self.id = id
-
-    @property
-    def meta(self):
-        return self.store.get_meta(self.id)
-
-    @property
-    def mtime(self):
-        return self.store.get_mtime(self.id)
-
-    def load(self):
-        return self.store.load(self.id)
-
-
-class PlacementSet(Interface):
-    """
-    Interface for the storage of placement data of a cell type.
-    """
-
-    @abc.abstractmethod
-    def __init__(self, engine, cell_type):
-        self._engine = engine
-        self._type = cell_type
-        self._tag = cell_type.name
-
-    @abc.abstractmethod
-    def __len__(self):
-        pass
-
-    @obj_str_insert
-    def __repr__(self):
-        cell_type = self.cell_type
-        try:
-            ms = self.load_morphologies()
-        except Exception:
-            return f"cell type: '{cell_type.name}'"
-        if not len(ms):
-            mstr = "without morphologies"
-        else:
-            mstr = f"with {len(ms._loaders)} morphologies"
-        return f"cell type: '{cell_type.name}', {len(self)} cells, {mstr}"
-
-    @property
-    def cell_type(self):
-        """
-        The associated cell type.
-
-        :returns: The cell type
-        :rtype: ~bsb.cell_types.CellType
-        """
-        return self._type
-
-    @property
-    def tag(self):
-        """
-        The unique identifier of the placement set.
-
-        :returns: Unique identifier
-        :rtype: str
-        """
-        return self._tag
-
-    @classmethod
-    @abc.abstractmethod
-    def create(cls, engine, cell_type):
-        """
-        Create a placement set.
-
-        :param engine: The engine that governs this PlacementSet.
-        :type engine: `bsb.storage.interfaces.Engine`
-        :param cell_type: The cell type whose data is stored in the placement set.
-        :type cell_type: bsb.cell_types.CellType
-        :returns: A placement set
-        :rtype: bsb.storage.interfaces.PlacementSet
-        """
-        pass
-
-    @staticmethod
-    @abc.abstractmethod
-    def exists(engine, cell_type):
-        """
-        Check existence of a placement set.
-
-        :param engine: The engine that governs the existence check.
-        :type engine: `bsb.storage.interfaces.Engine`
-        :param cell_type: The cell type to look for.
-        :type cell_type: bsb.cell_types.CellType
-        :returns: Whether the placement set exists.
-        :rtype: bool
-        """
-        pass
-
-    @classmethod
-    def require(cls, engine, type):
-        """
-        Return and create a placement set, if it didn't exist before.
-
-        The default implementation uses the
-        :meth:`~bsb.storage.interfaces.PlacementSet.exists` and
-        :meth:`~bsb.storage.interfaces.PlacementSet.create` methods.
-
-        :param engine: The engine that governs this PlacementSet.
-        :type engine: `bsb.storage.interfaces.Engine`
-        :param cell_type: The cell type whose data is stored in the placement set.
-        :type cell_type: bsb.cell_types.CellType
-        :returns: A placement set
-        :rtype: bsb.storage.interfaces.PlacementSet
-        """
-        if not cls.exists(engine, type):
-            cls.create(engine, type)
-        return cls(engine, type)
-
-    @abc.abstractmethod
-    def clear(self, chunks=None):
-        """
-        Clear (some chunks of) the placement set.
-
-        :param chunks: If given, the specific chunks to clear.
-        :type chunks: List[bsb.storage._chunks.Chunk]
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_all_chunks(self):
-        """
-        Get all the chunks that exist in the placement set.
-
-        :returns: List of existing chunks.
-        :rtype: List[bsb.storage._chunks.Chunk]
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_ids(self):
-        pass
-
-    @abc.abstractmethod
-    def load_positions(self):
-        """
-        Return a dataset of cell positions.
-
-        :returns: An (Nx3) dataset of positions.
-        :rtype: numpy.ndarray
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_rotations(self):
-        """
-        Load the rotation data of the placement set
-        :returns: A rotation set
-        :rtype: ~bsb.morphologies.RotationSet
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_morphologies(self, allow_empty=False):
-        """
-        Return a :class:`~.morphologies.MorphologySet` associated to the cells. Raises an
-        error if there is no morphology data, unless `allow_empty=True`.
-
-        :param bool allow_empty: Silence missing morphology data error, and return an
-          empty morphology set.
-        :returns: Set of morphologies
-        :rtype: :class:`~.morphologies.MorphologySet`
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_additional(self, key=None):
-        pass
-
-    def count_morphologies(self):
-        """
-        Must return the number of different morphologies used in the set.
-        """
-        return self.load_morphologies(allow_empty=True).count_morphologies()
-
-    @abc.abstractmethod
-    def __iter__(self):
-        pass
-
-    @abc.abstractmethod
-    def __len__(self):
-        pass
-
-    @abc.abstractmethod
-    def append_data(
-        self,
-        chunk,
-        positions=None,
-        morphologies=None,
-        rotations=None,
-        additional=None,
-        count=None,
-    ):
-        """
-        Append data to the placement set. If any of ``positions``, ``morphologies``, or
-        ``rotations`` is given, the arguments to its left must also be given (e.g. passing
-        morphologies, but no positions, is not allowed, passing just positions is allowed)
-
-        :param chunk: The chunk to store data in.
-        :type chunk: ~bsb.storage._chunks.Chunk
-        :param positions: Cell positions
-        :type positions: numpy.ndarray
-        :param rotations: Cell rotations
-        :type rotations: ~bsb.morphologies.RotationSet
-        :param morphologies: Cell morphologies
-        :type morphologies: ~bsb.morphologies.MorphologySet
-        :param additional: Additional datasets with 1 value per cell, will be stored
-          under its key in the dictionary
-        :type additional: Dict[str, numpy.ndarray]
-        :param count: Amount of entities to place. Excludes the use of any positional,
-          rotational or morphological data.
-        :type count: int
-        """
-        pass
-
-    @abc.abstractmethod
-    def append_additional(self, name, chunk, data):
-        """
-        Append arbitrary user data to the placement set. The length of the data must match
-        that of the placement set, and must be storable by the engine.
-
-        :param name:
-        :param chunk: The chunk to store data in.
-        :type chunk: ~bsb.storage._chunks.Chunk
-        :param data: Arbitrary user data. You decide |:heart:|
-        :type data: numpy.ndarray
-        """
-        pass
-
-    @abc.abstractmethod
-    def chunk_context(self, chunks):
-        pass
-
-    @abc.abstractmethod
-    def set_chunk_filter(self, chunks):
-        """
-        Should limit the scope of the placement set to the given chunks.
-
-        :param chunks: List of chunks
-        :type chunks: list[bsb.storage._chunks.Chunk]
-        """
-        pass
-
-    @abc.abstractmethod
-    def set_label_filter(self, labels):
-        """
-        Should limit the scope of the placement set to the given labels.
-
-        :param labels: List of labels
-        :type labels: list[str]
-        """
-        pass
-
-    @abc.abstractmethod
-    def set_morphology_label_filter(self, morphology_labels):
-        """
-        Should limit the scope of the placement set to the given sub-cellular labels. The
-        morphologies returned by
-        :meth:`~.storage.interfaces.PlacementSet.load_morphologies` should return a
-        filtered form of themselves if :meth:`~.morphologies.Morphology.as_filtered` is
-        called on them.
-
-        :param morphology_labels: List of labels
-        :type morphology_labels: list[str]
-        """
-        pass
-
-    @abc.abstractmethod
-    def label(self, labels, cells):
-        """
-        Should label the cells with given labels.
-
-        :param cells: Array of cells in this set to label.
-        :type cells: numpy.ndarray
-        :param labels: List of labels
-        :type labels: list[str]
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_labelled(self, labels):
-        """
-        Should return the cells labelled with given labels.
-
-        :param cells: Array of cells in this set to label.
-        :type cells: numpy.ndarray
-        :param labels: List of labels
-        :type labels: list[str]
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_label_mask(self, labels):
-        """
-        Should return a mask that fits the placement set for the cells with given labels.
-
-        :param cells: Array of cells in this set to label.
-        :type cells: numpy.ndarray
-        :param labels: List of labels
-        :type labels: list[str]
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_chunk_stats(self):
-        """
-        Should return how many cells were placed in each chunk.
-        """
-        pass
-
-    def load_boxes(self, morpho_cache=None):
-        """
-        Load the cells as axis aligned bounding box rhomboids matching the extension,
-        orientation and position in space. This function loads morphologies, unless a
-        `morpho_cache` is given, then that is used.
-
-        :param morpho_cache: If you've previously loaded morphologies with soft or hard
-          caching enabled, you can pass the resulting morphology set here to reuse it. If
-          afterwards you need the morphology set, you best call :meth:`.load_morphologies`
-          first and reuse it here.
-        :type morpho_cache: ~bsb.morphologies.MorphologySet
-        :returns: An iterator with 6 coordinates per cell: 3 min and 3 max coords, the
-          bounding box of that cell's translated and rotated morphology.
-        :rtype: Iterator[Tuple[float, float, float, float, float, float]]
-        :raises: DatasetNotFoundError if no morphologies are found.
-        """
-        if morpho_cache is None:
-            mset = self.load_morphologies()
-        else:
-            mset = morpho_cache
-        expansion = [*zip([0] * 4 + [1] * 4, ([0] * 2 + [1] * 2) * 2, [0, 1] * 4)]
-
-        def _box_of(m, o, r):
-            oo = (m["ldc"], m["mdc"])
-            # Make the 8 corners of the box
-            corners = np.array([[oo[x][0], oo[y][1], oo[z][2]] for x, y, z in expansion])
-            # Rotate them
-            rotbox = r.apply(corners)
-            # Find outer box, by rotating and translating the starting box
-            return np.concatenate(
-                (np.min(rotbox, axis=0) + o, np.max(rotbox, axis=0) + o)
-            )
-
-        iters = (mset.iter_meta(), self.load_positions(), self.load_rotations())
-        return map(_box_of, *iters)
-
-    def load_box_tree(self, morpho_cache=None):
-        """
-        Load boxes, and form an RTree with them, for fast spatial lookup of rhomboid
-        intersection.
-
-        :param morpho_cache: See :meth:`~bsb.storage.interfaces.PlacementSet.load_boxes`.
-        :returns: A boxtree
-        :rtype: bsb.trees.BoxTree
-        """
-        return BoxTree(list(self.load_boxes(morpho_cache=morpho_cache)))
-
-    def _requires_morpho_mapping(self):
-        return self._morphology_labels is not None and self.count_morphologies()
-
-    def _morpho_backmap(self, locs):
-        locs = locs.copy()
-        cols = locs[:, 1:]
-        ign_b = cols[:, 0] == -1
-        ign_p = cols[:, 1] == -1
-        semi = ign_b != ign_p
-        if np.any(semi):
-            raise ValueError(
-                f"Invalid data at {np.nonzero(semi)[0]}. -1 needs to occur in "
-                "either none or both columns to make point neuron connections."
-            )
-        to_map = ~ign_b
-        if np.any(locs[to_map, 1:] < 0):
-            raise ValueError(
-                f"Invalid data at {np.nonzero(locs[to_map, 1:] < 0)[0]}, "
-                "negative values are not valid morphology locations."
-            )
-        locs[to_map] = self.load_morphologies()._mapback(locs[to_map])
-        return locs
-
-
-class MorphologyRepository(Interface, engine_key="morphologies"):
-    @abc.abstractmethod
-    def all(self):
-        """
-        Fetch all of the stored morphologies.
-
-        :returns: List of the stored morphologies.
-        :rtype: List[~bsb.storage.interfaces.StoredMorphology]
-        """
-        pass
-
-    @abc.abstractmethod
-    def select(self, *selectors):
-        """
-        Select stored morphologies.
-
-        :param selectors: Any number of morphology selectors.
-        :type selectors: List[bsb.morphologies.selector.MorphologySelector]
-        :returns: All stored morphologies that match at least one selector.
-        :rtype: List[~bsb.storage.interfaces.StoredMorphology]
-        """
-        pass
-
-    @abc.abstractmethod
-    def save(self, name, morphology, overwrite=False):
-        """
-        Store a morphology
-
-        :param name: Key to store the morphology under.
-        :type name: str
-        :param morphology: Morphology to store
-        :type morphology: bsb.morphologies.Morphology
-        :param overwrite: Overwrite any stored morphology that already exists under that
-          name
-        :type overwrite: bool
-        :returns: The stored morphology
-        :rtype: ~bsb.storage.interfaces.StoredMorphology
-        """
-        pass
-
-    @abc.abstractmethod
-    def has(self, name):
-        """
-        Check whether a morphology under the given name exists
-
-        :param name: Key of the stored morphology.
-        :type name: str
-        :returns: Whether the key exists in the repo.
-        :rtype: bool
-        """
-        pass
-
-    def __contains__(self, item):
-        return self.has(item)
-
-    @abc.abstractmethod
-    def preload(self, name):
-        """
-        Load a stored morphology as a morphology loader.
-
-        :param name: Key of the stored morphology.
-        :type name: str
-        :returns: The stored morphology
-        :rtype: ~bsb.storage.interfaces.StoredMorphology
-        """
-        pass
-
-    @abc.abstractmethod
-    def load(self, name):
-        """
-        Load a stored morphology as a constructed morphology object.
-
-        :param name: Key of the stored morphology.
-        :type name: str
-        :returns: A morphology
-        :rtype: ~bsb.morphologies.Morphology
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_meta(self, name):
-        """
-        Get the metadata of a stored morphology.
-
-        :param name: Key of the stored morphology.
-        :type name: str
-        :returns: Metadata dictionary
-        :rtype: dict
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_all_meta(self):
-        """
-        Get the metadata of all stored morphologies.
-        :returns: Metadata dictionary
-        :rtype: dict
-        """
-        pass
-
-    @abc.abstractmethod
-    def set_all_meta(self, all_meta):
-        """
-        Set the metadata of all stored morphologies.
-        :param all_meta: Metadata dictionary.
-        :type all_meta: dict
-        """
-        pass
-
-    @abc.abstractmethod
-    def update_all_meta(self, meta):
-        """
-        Update the metadata of stored morphologies with the provided key values
-
-        :param meta: Metadata dictionary.
-        :type meta: str
-        """
-        pass
-
-    def list(self):
-        """
-        List all the names of the morphologies in the repository.
-        """
-        return [loader.name for loader in self.all()]
-
-
-class ConnectivitySet(Interface):
-    """
-    Stores the connections between 2 types of cell as ``local`` and ``global`` locations.
-    A location is a cell id, referring to the n-th cell in the chunk, a branch id, and a
-    point id, to specify the location on the morphology. Local locations refer to cells on
-    this chunk, while global locations can come from any chunk and is associated to a
-    certain chunk id as well.
-
-    Locations are either placement-context or chunk dependent: You may form connections
-    between the n-th cells of a placement set (using
-    :meth:`~.storage.interfaces.ConnectivitySet.connect`), or of the n-th cells of 2
-    chunks (using :meth:`~.storage.interfaces.ConnectivitySet.chunk_connect`).
-
-    A cell has both incoming and outgoing connections; when speaking of incoming
-    connections, the local locations are the postsynaptic cells, and when speaking of
-    outgoing connections they are the presynaptic cells. Vice versa for the global
-    connections.
-    """
-
-    # The following attributes must be set on each ConnectivitySet by the engine:
-    tag: str
-    pre_type_name: str
-    post_type_name: str
-    pre_type: "CellType"
-    post_type: "CellType"
-
-    @abc.abstractmethod
-    def __len__(self):
-        pass
-
-    @classmethod
-    @abc.abstractmethod
-    def create(cls, engine, tag):
-        """
-        Must create the placement set.
-        """
-        pass
-
-    @obj_str_insert
-    def __repr__(self):
-        cstr = f"with {len(self)} connections" if len(self) else "without connections"
-        return f"'{self.tag}' {cstr}"
-
-    @staticmethod
-    @abc.abstractmethod
-    def exists(engine, tag):
-        """
-        Must check the existence of the connectivity set
-        """
-        pass
-
-    def require(self, engine, tag):
-        """
-        Must make sure the connectivity set exists. The default
-        implementation uses the class's ``exists`` and ``create`` methods.
-        """
-        if not self.exists(engine, tag):
-            self.create(engine, tag)
-
-    @abc.abstractmethod
-    def clear(self, chunks=None):
-        """
-        Must clear (some chunks of) the placement set
-        """
-        pass
-
-    @classmethod
-    @abc.abstractmethod
-    def get_tags(cls, engine):
-        """
-        Must return the tags of all existing connectivity sets.
-
-        :param engine: Storage engine to inspect.
-        """
-        pass
-
-    @abc.abstractmethod
-    def connect(self, pre_set, post_set, src_locs, dest_locs):
-        """
-        Must connect the ``src_locs`` to the ``dest_locs``, interpreting the cell ids
-        (first column of the locs) as the cell rank in the placement set.
-        """
-        pass
-
-    @abc.abstractmethod
-    def chunk_connect(self, src_chunk, dst_chunk, src_locs, dst_locs):
-        """
-        Must connect the ``src_locs`` to the ``dest_locs``, interpreting the cell ids
-        (first column of the locs) as the cell rank in the chunk.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_local_chunks(self, direction):
-        """
-        Must list all the local chunks that contain data in the given ``direction``
-        (``"inc"`` or ``"out"``).
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_global_chunks(self, direction, local_):
-        """
-        Must list all the global chunks that contain data coming from a ``local`` chunk
-        in the given ``direction``
-        """
-        pass
-
-    @abc.abstractmethod
-    def nested_iter_connections(self, direction=None, local_=None, global_=None):
-        """
-        Must iterate over the connectivity data, leaving room for the end-user to set up
-        nested for loops:
-
-        .. code-block:: python
-
-          for dir, itr in self.nested_iter_connections():
-              for lchunk, itr in itr:
-                  for gchunk, data in itr:
-                      print(f"Nested {dir} block between {lchunk} and {gchunk}")
-
-        If a keyword argument is given, that axis is not iterated over, and the amount of
-        nested loops is reduced.
-        """
-        pass
-
-    @abc.abstractmethod
-    def flat_iter_connections(self, direction=None, local_=None, global_=None):
-        """
-        Must iterate over the connectivity data, yielding the direction, local chunk,
-        global chunk, and data:
-
-        .. code-block:: python
-
-          for dir, lchunk, gchunk, data in self.flat_iter_connections():
-              print(f"Flat {dir} block between {lchunk} and {gchunk}")
-
-        If a keyword argument is given, that axis is not iterated over, and the value is
-        fixed in each iteration.
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_block_connections(self, direction, local_, global_):
-        """
-        Must load the connections from ``direction`` perspective between ``local_`` and
-        ``global_``.
-
-        :returns: The local and global connections locations
-        :rtype: Tuple[numpy.ndarray, numpy.ndarray]
-        """
-        pass
-
-    @abc.abstractmethod
-    def load_local_connections(self, direction, local_):
-        """
-        Must load all the connections from ``direction`` perspective in ``local_``.
-
-        :returns: The local connection locations, a vector of the global connection chunks
-          (1 chunk id per connection), and the global connections locations. To identify a
-          cell in the global connections, use the corresponding chunk id from the second
-          return value.
-        :rtype: Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]
-        """
-        pass
-
-    def load_connections(self):
-        """
-        Loads connections as a ``CSIterator``.
-
-        :returns: A connectivity set iterator, that will load data
-        """
-        return ConnectivityIterator(self, "out")
-
-
-class ConnectivityIterator:
-    def __init__(
-        self, cs: ConnectivitySet, direction, lchunks=None, gchunks=None, scoped=True
-    ):
-        self._cs = cs
-        self._dir = direction
-        self._lchunks = lchunks
-        self._scoped = scoped
-        self._gchunks = gchunks
-
-    def __copy__(self):
-        lchunks = self._lchunks.copy() if self._lchunks is not None else None
-        gchunks = self._gchunks.copy() if self._gchunks is not None else None
-        return ConnectivityIterator(self._cs, self._dir, lchunks, gchunks)
-
-    def __len__(self):
-        return len(self.all()[0])
-
-    def __iter__(self):
-        """
-        Iterate over the connection locations chunk by chunk.
-
-        :returns: The presynaptic location matrix and postsynaptic location matrix.
-        :rtype: Tuple[numpy.ndarray, numpy.ndarray]
-        """
-        for _, pre_locs, _, post_locs in self.chunk_iter():
-            yield from zip(pre_locs, post_locs)
-
-    def chunk_iter(self):
-        """
-        Iterate over the connection data chunk by chunk.
-
-        :returns: The presynaptic chunk, presynaptic locations, postsynaptic chunk,
-          and postsynaptic locations.
-        :rtype: Tuple[~bsb.storage._chunks.Chunk, numpy.ndarray, ~bsb.storage._chunks.Chunk, numpy.ndarray]
-        """
-        yield from (
-            self._offset_block(*data)
-            for data in self._cs.flat_iter_connections(
-                self._dir, self._lchunks, self._gchunks
-            )
-        )
-
-    @immutable()
-    def as_globals(self):
-        self._scoped = False
-
-    @immutable()
-    def as_scoped(self):
-        self._scoped = True
-
-    @immutable()
-    def outgoing(self):
-        self._dir = "out"
-        self._lchunks, self._gchunks = self._gchunks, self._lchunks
-
-    @immutable()
-    def incoming(self):
-        self._dir = "inc"
-        self._lchunks, self._gchunks = self._gchunks, self._lchunks
-
-    @immutable()
-    def to(self, chunks):
-        if isinstance(chunks, Chunk) and chunks.ndim == 1:
-            chunks = [chunks]
-        if self._dir == "inc":
-            self._lchunks = chunks
-        else:
-            self._gchunks = chunks
-
-    @immutable()
-    def from_(self, chunks):
-        if isinstance(chunks, Chunk) and chunks.ndim == 1:
-            chunks = [chunks]
-        if self._dir == "out":
-            self._lchunks = chunks
-        else:
-            self._gchunks = chunks
-
-    def all(self):
-        pre_blocks = []
-        post_blocks = []
-        lens = []
-        for _, pre_block, _, post_block in self.chunk_iter():
-            pre_blocks.append(pre_block)
-            post_blocks.append(post_block)
-            lens.append(len(pre_block))
-        pre_locs = np.empty((sum(lens), 3), dtype=int)
-        post_locs = np.empty((sum(lens), 3), dtype=int)
-        ptr = 0
-        for len_, pre_block, post_block in zip(lens, pre_blocks, post_blocks):
-            pre_locs[ptr : ptr + len_] = pre_block
-            post_locs[ptr : ptr + len_] = post_block
-            ptr += len_
-        return pre_locs, post_locs
-
-    def _offset_block(self, direction: str, lchunk, gchunk, data):
-        loff = self._local_chunk_offsets()
-        goff = self._global_chunk_offsets()
-        llocs, glocs = data
-        llocs[:, 0] += loff[lchunk]
-        glocs[:, 0] += goff[gchunk]
-        if direction == "out":
-            return lchunk, llocs, gchunk, glocs
-        else:
-            return gchunk, glocs, lchunk, llocs
-
-    @functools.cache
-    def _local_chunk_offsets(self):
-        source = self._cs.post_type if self._dir == "inc" else self._cs.pre_type
-        return self._chunk_offsets(source, self._lchunks)
-
-    @functools.cache
-    def _global_chunk_offsets(self):
-        source = self._cs.pre_type if self._dir == "inc" else self._cs.post_type
-        return self._chunk_offsets(source, self._gchunks)
-
-    def _chunk_offsets(self, source, chunks):
-        stats = source.get_placement_set().get_chunk_stats()
-        if self._scoped and chunks is not None:
-            stats = {chunk: item for chunk, item in stats.items() if int(chunk) in chunks}
-        offsets = {}
-        ctr = 0
-        for chunk, len_ in sorted(
-            stats.items(), key=lambda k: Chunk.from_id(int(k[0]), None).id
-        ):
-            offsets[Chunk.from_id(int(chunk), None)] = ctr
-            ctr += len_
-        return offsets
-
-
-class StoredMorphology:
-    def __init__(self, name, loader, meta):
-        self.name = name
-        self._loader = loader
-        self._meta = meta
-
-    def __eq__(self, other):
-        return self.name == other.name
-
-    def __hash__(self):
-        return hash(self.name)
-
-    def get_meta(self):
-        return self._meta.copy()
-
-    def load(self):
-        return self._loader()
-
-    def cached_load(self, labels=None):
-        if labels is not None:
-            labels = tuple(labels)
-        return self._cached_load(labels)
-
-    @functools.cache
-    def _cached_load(self, labels):
-        return self.load().set_label_filter(labels).as_filtered()
-
-
-class GeneratedMorphology(StoredMorphology):
-    def __init__(self, name, generated, meta):
-        super().__init__(name, lambda: generated, meta)
-
-
-__all__ = [
-    "ConnectivityIterator",
-    "ConnectivitySet",
-    "Engine",
-    "FileStore",
-    "GeneratedMorphology",
-    "Interface",
-    "MorphologyRepository",
-    "NetworkDescription",
-    "NoopLock",
-    "PlacementSet",
-    "ReadOnlyManager",
-    "StorageNode",
-    "StoredFile",
-    "StoredMorphology",
-]
+import abc
+import functools
+import typing
+from pathlib import Path
+
+import numpy as np
+
+from .. import config, plugins
+from .._util import immutable, obj_str_insert
+from ..trees import BoxTree
+from ._chunks import Chunk
+
+if typing.TYPE_CHECKING:
+    from ..cell_types import CellType
+
+
+@config.pluggable(key="engine", plugin_name="storage engine")
+class StorageNode:
+    root: typing.Any = config.slot()
+
+    @classmethod
+    def __plugins__(cls):
+        if not hasattr(cls, "_plugins"):
+            cls._plugins = {
+                name: plugin.StorageNode
+                for name, plugin in plugins.discover("storage.engines").items()
+            }
+        return cls._plugins
+
+
+class Interface(abc.ABC):
+    _iface_engine_key = None
+
+    def __init__(self, engine):
+        self._engine = engine
+
+    def __init_subclass__(cls, **kwargs):
+        # Only change engine key if explicitly given.
+        if "engine_key" in kwargs:
+            cls._iface_engine_key = kwargs["engine_key"]
+
+
+class NoopLock:
+    def __enter__(self):
+        pass
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        pass
+
+
+class Engine(Interface):
+    """
+    Engines perform the transactions that come from the storage object, and read/write
+    data in a specific format. They can perform collective or individual actions.
+
+    .. warning::
+
+      Collective actions can only be performed from all nodes, or deadlocks occur. This
+      means in particular that they may not be called from component code.
+
+    """
+
+    def __init__(self, root, comm):
+        self._root = comm.bcast(root, root=0)
+        self._comm = comm
+        self._readonly = False
+
+    def __eq__(self, other):
+        eq_format = self._format == getattr(other, "_format", None)
+        eq_root = self._root == getattr(other, "_root", None)
+        return eq_format and eq_root
+
+    @property
+    def root(self):
+        """
+        The unique identifier for the storage. Usually pathlike, but can be anything.
+        """
+        return self._root
+
+    @property
+    def comm(self):
+        """
+        The communicator in charge of collective operations.
+        """
+        return self._comm
+
+    def set_comm(self, comm):
+        """
+        :guilabel:`collective` Set a new communicator in charge of collective operations.
+        """
+        self._comm = comm
+
+    @property
+    def format(self):
+        """
+        Name of the type of engine. Automatically set through the plugin system.
+        """
+        return self._format
+
+    @property
+    @abc.abstractmethod
+    def versions(self):
+        """
+        Must return a dictionary containing the version of the engine package, and bsb
+        package, used to last write to this storage object.
+        """
+        pass
+
+    @property
+    @abc.abstractmethod
+    def root_slug(self):
+        """
+        Must return a pathlike unique identifier for the root of the storage object.
+        """
+        pass
+
+    @abc.abstractmethod
+    def recognizes(self, root):
+        """
+        Must return whether the given argument is recognized as a valid storage object.
+        """
+        pass
+
+    @classmethod
+    def peek_exists(cls, root):
+        """
+        Must peek at the existence of the given root, without instantiating anything.
+        """
+        try:
+            return Path(root).exists()
+        except Exception:
+            return False
+
+    @abc.abstractmethod
+    def exists(self):
+        """
+        Must check existence of the storage object.
+        """
+        pass
+
+    @abc.abstractmethod
+    def create(self):
+        """
+        :guilabel:`collective` Must create the storage engine.
+        """
+        pass
+
+    @abc.abstractmethod
+    def move(self, new_root):
+        """
+        :guilabel:`collective` Must move the storage object to the new root.
+        """
+        pass
+
+    @abc.abstractmethod
+    def copy(self, new_root):
+        """
+        :guilabel:`collective` Must copy the storage object to the new root.
+        """
+        pass
+
+    @abc.abstractmethod
+    def remove(self):
+        """
+        :guilabel:`collective` Must remove the storage object.
+        """
+        pass
+
+    @abc.abstractmethod
+    def clear_placement(self):
+        """
+        :guilabel:`collective` Must clear existing placement data.
+        """
+        pass
+
+    @abc.abstractmethod
+    def clear_connectivity(self):
+        """
+        :guilabel:`collective` Must clear existing connectivity data.
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_chunk_stats(self):
+        """
+        :guilabel:`readonly` Must return a dictionary with all chunk statistics.
+        """
+        pass
+
+    def read_only(self):
+        """
+        A context manager that enters the engine into readonly mode. In
+        readonly mode the engine does not perform any locking, write-operations or network
+        synchronization, and errors out if a write operation is attempted.
+        """
+        self._readonly = True
+        return ReadOnlyManager(self)
+
+    def readwrite(self):
+        self._readonly = False
+
+
+class ReadOnlyManager:
+    def __init__(self, engine):
+        self._e = engine
+
+    def __enter__(self):
+        self._e._readonly = True
+
+    def __exit__(self, *args):
+        self._e._readonly = False
+
+
+class NetworkDescription(Interface):
+    pass
+
+
+class FileStore(Interface, engine_key="files"):
+    """
+    Interface for the storage and retrieval of files essential to the network description.
+    """
+
+    @abc.abstractmethod
+    def all(self):
+        """
+        Return all ids and associated metadata in the file store.
+        """
+        pass
+
+    @abc.abstractmethod
+    def store(self, content, id=None, meta=None, encoding=None, overwrite=False):
+        """
+        Store content in the file store. Should also store the current timestamp as
+        `mtime` meta.
+
+        :param content: Content to be stored
+        :type content: str
+        :param id: Optional specific id for the content to be stored under.
+        :type id: str
+        :param meta: Metadata for the content
+        :type meta: dict
+        :param encoding: Optional encoding
+        :type encoding: str
+        :param overwrite: Overwrite existing file
+        :type overwrite: bool
+        :returns: The id the content was stored under
+        :rtype: str
+        """
+        pass
+
+    @abc.abstractmethod
+    def load(self, id):
+        """
+        Load the content of an object in the file store.
+
+        :param id: id of the content to be loaded.
+        :type id: str
+        :returns: The content of the stored object
+        :rtype: str
+        :raises FileNotFoundError: The given id doesn't exist in the file store.
+        """
+        pass
+
+    @abc.abstractmethod
+    def remove(self, id):
+        """
+        Remove the content of an object in the file store.
+
+        :param id: id of the content to be removed.
+        :type id: str
+        :raises FileNotFoundError: The given id doesn't exist in the file store.
+        """
+        pass
+
+    @abc.abstractmethod
+    def store_active_config(self, config):
+        """
+        Store configuration in the file store and mark it as the active configuration of
+        the stored network.
+
+        :param config: Configuration to be stored
+        :type config: :class:`~.config.Configuration`
+        :returns: The id the config was stored under
+        :rtype: str
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_active_config(self):
+        """
+        Load the active configuration stored in the file store.
+
+        :returns: The active configuration
+        :rtype: :class:`~.config.Configuration`
+        :raises Exception: When there's no active configuration in the file store.
+        """
+        pass
+
+    @abc.abstractmethod
+    def has(self, id):
+        """
+        Must return whether the file store has a file with the given id.
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_mtime(self, id):
+        """
+        Must return the last modified timestamp of file with the given id.
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_encoding(self, id):
+        """
+        Must return the encoding of the file with the given id, or None if it is
+        unspecified binary data.
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_meta(self, id) -> typing.Mapping[str, typing.Any]:
+        """
+        Must return the metadata of the given id.
+        """
+        pass
+
+    def get(self, id) -> "StoredFile":
+        """
+        Return a StoredFile wrapper
+        """
+        if not self.has(id):
+            raise FileNotFoundError(f"File with id '{id}' not found.")
+        return StoredFile(self, id)
+
+    def find_files(self, predicate):
+        return (
+            StoredFile(self, id_) for id_, m in self.all().items() if predicate(id_, m)
+        )
+
+    def find_file(self, predicate):
+        return next(self.find_files(predicate), None)
+
+    def find_id(self, id):
+        return self.find_file(lambda id_, _: id_ == id)
+
+    def find_meta(self, key, value):
+        return self.find_file(lambda _, meta: meta.get(key, None) == value)
+
+
+class StoredFile:
+    def __init__(self, store, id):
+        self.store = store
+        self.id = id
+
+    @property
+    def meta(self):
+        return self.store.get_meta(self.id)
+
+    @property
+    def mtime(self):
+        return self.store.get_mtime(self.id)
+
+    def load(self):
+        return self.store.load(self.id)
+
+
+class PlacementSet(Interface):
+    """
+    Interface for the storage of placement data of a cell type.
+    """
+
+    @abc.abstractmethod
+    def __init__(self, engine, cell_type):
+        self._engine = engine
+        self._type = cell_type
+        self._tag = cell_type.name
+
+    @abc.abstractmethod
+    def __len__(self):
+        pass
+
+    @obj_str_insert
+    def __repr__(self):
+        cell_type = self.cell_type
+        try:
+            ms = self.load_morphologies()
+        except Exception:
+            return f"cell type: '{cell_type.name}'"
+        if not len(ms):
+            mstr = "without morphologies"
+        else:
+            mstr = f"with {len(ms._loaders)} morphologies"
+        return f"cell type: '{cell_type.name}', {len(self)} cells, {mstr}"
+
+    @property
+    def cell_type(self):
+        """
+        The associated cell type.
+
+        :returns: The cell type
+        :rtype: ~bsb.cell_types.CellType
+        """
+        return self._type
+
+    @property
+    def tag(self):
+        """
+        The unique identifier of the placement set.
+
+        :returns: Unique identifier
+        :rtype: str
+        """
+        return self._tag
+
+    @classmethod
+    @abc.abstractmethod
+    def create(cls, engine, cell_type):
+        """
+        Create a placement set.
+
+        :param engine: The engine that governs this PlacementSet.
+        :type engine: `bsb.storage.interfaces.Engine`
+        :param cell_type: The cell type whose data is stored in the placement set.
+        :type cell_type: bsb.cell_types.CellType
+        :returns: A placement set
+        :rtype: bsb.storage.interfaces.PlacementSet
+        """
+        pass
+
+    @staticmethod
+    @abc.abstractmethod
+    def exists(engine, cell_type):
+        """
+        Check existence of a placement set.
+
+        :param engine: The engine that governs the existence check.
+        :type engine: `bsb.storage.interfaces.Engine`
+        :param cell_type: The cell type to look for.
+        :type cell_type: bsb.cell_types.CellType
+        :returns: Whether the placement set exists.
+        :rtype: bool
+        """
+        pass
+
+    @classmethod
+    def require(cls, engine, type):
+        """
+        Return and create a placement set, if it didn't exist before.
+
+        The default implementation uses the
+        :meth:`~bsb.storage.interfaces.PlacementSet.exists` and
+        :meth:`~bsb.storage.interfaces.PlacementSet.create` methods.
+
+        :param engine: The engine that governs this PlacementSet.
+        :type engine: `bsb.storage.interfaces.Engine`
+        :param cell_type: The cell type whose data is stored in the placement set.
+        :type cell_type: bsb.cell_types.CellType
+        :returns: A placement set
+        :rtype: bsb.storage.interfaces.PlacementSet
+        """
+        if not cls.exists(engine, type):
+            cls.create(engine, type)
+        return cls(engine, type)
+
+    @abc.abstractmethod
+    def clear(self, chunks=None):
+        """
+        Clear (some chunks of) the placement set.
+
+        :param chunks: If given, the specific chunks to clear.
+        :type chunks: List[bsb.storage._chunks.Chunk]
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_all_chunks(self):
+        """
+        Get all the chunks that exist in the placement set.
+
+        :returns: List of existing chunks.
+        :rtype: List[bsb.storage._chunks.Chunk]
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_ids(self):
+        pass
+
+    @abc.abstractmethod
+    def load_positions(self):
+        """
+        Return a dataset of cell positions.
+
+        :returns: An (Nx3) dataset of positions.
+        :rtype: numpy.ndarray
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_rotations(self):
+        """
+        Load the rotation data of the placement set
+        :returns: A rotation set
+        :rtype: ~bsb.morphologies.RotationSet
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_morphologies(self, allow_empty=False):
+        """
+        Return a :class:`~.morphologies.MorphologySet` associated to the cells. Raises an
+        error if there is no morphology data, unless `allow_empty=True`.
+
+        :param bool allow_empty: Silence missing morphology data error, and return an
+          empty morphology set.
+        :returns: Set of morphologies
+        :rtype: :class:`~.morphologies.MorphologySet`
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_additional(self, key=None):
+        pass
+
+    def count_morphologies(self):
+        """
+        Must return the number of different morphologies used in the set.
+        """
+        return self.load_morphologies(allow_empty=True).count_morphologies()
+
+    @abc.abstractmethod
+    def __iter__(self):
+        pass
+
+    @abc.abstractmethod
+    def __len__(self):
+        pass
+
+    @abc.abstractmethod
+    def append_data(
+        self,
+        chunk,
+        positions=None,
+        morphologies=None,
+        rotations=None,
+        additional=None,
+        count=None,
+    ):
+        """
+        Append data to the placement set. If any of ``positions``, ``morphologies``, or
+        ``rotations`` is given, the arguments to its left must also be given (e.g. passing
+        morphologies, but no positions, is not allowed, passing just positions is allowed)
+
+        :param chunk: The chunk to store data in.
+        :type chunk: ~bsb.storage._chunks.Chunk
+        :param positions: Cell positions
+        :type positions: numpy.ndarray
+        :param rotations: Cell rotations
+        :type rotations: ~bsb.morphologies.RotationSet
+        :param morphologies: Cell morphologies
+        :type morphologies: ~bsb.morphologies.MorphologySet
+        :param additional: Additional datasets with 1 value per cell, will be stored
+          under its key in the dictionary
+        :type additional: Dict[str, numpy.ndarray]
+        :param count: Amount of entities to place. Excludes the use of any positional,
+          rotational or morphological data.
+        :type count: int
+        """
+        pass
+
+    @abc.abstractmethod
+    def append_additional(self, name, chunk, data):
+        """
+        Append arbitrary user data to the placement set. The length of the data must match
+        that of the placement set, and must be storable by the engine.
+
+        :param name:
+        :param chunk: The chunk to store data in.
+        :type chunk: ~bsb.storage._chunks.Chunk
+        :param data: Arbitrary user data. You decide |:heart:|
+        :type data: numpy.ndarray
+        """
+        pass
+
+    @abc.abstractmethod
+    def chunk_context(self, chunks):
+        pass
+
+    @abc.abstractmethod
+    def set_chunk_filter(self, chunks):
+        """
+        Should limit the scope of the placement set to the given chunks.
+
+        :param chunks: List of chunks
+        :type chunks: list[bsb.storage._chunks.Chunk]
+        """
+        pass
+
+    @abc.abstractmethod
+    def set_label_filter(self, labels):
+        """
+        Should limit the scope of the placement set to the given labels.
+
+        :param labels: List of labels
+        :type labels: list[str]
+        """
+        pass
+
+    @abc.abstractmethod
+    def set_morphology_label_filter(self, morphology_labels):
+        """
+        Should limit the scope of the placement set to the given sub-cellular labels. The
+        morphologies returned by
+        :meth:`~.storage.interfaces.PlacementSet.load_morphologies` should return a
+        filtered form of themselves if :meth:`~.morphologies.Morphology.as_filtered` is
+        called on them.
+
+        :param morphology_labels: List of labels
+        :type morphology_labels: list[str]
+        """
+        pass
+
+    @abc.abstractmethod
+    def label(self, labels, cells):
+        """
+        Should label the cells with given labels.
+
+        :param cells: Array of cells in this set to label.
+        :type cells: numpy.ndarray
+        :param labels: List of labels
+        :type labels: list[str]
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_labelled(self, labels):
+        """
+        Should return the cells labelled with given labels.
+
+        :param cells: Array of cells in this set to label.
+        :type cells: numpy.ndarray
+        :param labels: List of labels
+        :type labels: list[str]
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_label_mask(self, labels):
+        """
+        Should return a mask that fits the placement set for the cells with given labels.
+
+        :param cells: Array of cells in this set to label.
+        :type cells: numpy.ndarray
+        :param labels: List of labels
+        :type labels: list[str]
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_chunk_stats(self):
+        """
+        Should return how many cells were placed in each chunk.
+        """
+        pass
+
+    def load_boxes(self, morpho_cache=None):
+        """
+        Load the cells as axis aligned bounding box rhomboids matching the extension,
+        orientation and position in space. This function loads morphologies, unless a
+        `morpho_cache` is given, then that is used.
+
+        :param morpho_cache: If you've previously loaded morphologies with soft or hard
+          caching enabled, you can pass the resulting morphology set here to reuse it. If
+          afterwards you need the morphology set, you best call :meth:`.load_morphologies`
+          first and reuse it here.
+        :type morpho_cache: ~bsb.morphologies.MorphologySet
+        :returns: An iterator with 6 coordinates per cell: 3 min and 3 max coords, the
+          bounding box of that cell's translated and rotated morphology.
+        :rtype: Iterator[Tuple[float, float, float, float, float, float]]
+        :raises: DatasetNotFoundError if no morphologies are found.
+        """
+        if morpho_cache is None:
+            mset = self.load_morphologies()
+        else:
+            mset = morpho_cache
+        expansion = [*zip([0] * 4 + [1] * 4, ([0] * 2 + [1] * 2) * 2, [0, 1] * 4)]
+
+        def _box_of(m, o, r):
+            oo = (m["ldc"], m["mdc"])
+            # Make the 8 corners of the box
+            corners = np.array([[oo[x][0], oo[y][1], oo[z][2]] for x, y, z in expansion])
+            # Rotate them
+            rotbox = r.apply(corners)
+            # Find outer box, by rotating and translating the starting box
+            return np.concatenate(
+                (np.min(rotbox, axis=0) + o, np.max(rotbox, axis=0) + o)
+            )
+
+        iters = (mset.iter_meta(), self.load_positions(), self.load_rotations())
+        return map(_box_of, *iters)
+
+    def load_box_tree(self, morpho_cache=None):
+        """
+        Load boxes, and form an RTree with them, for fast spatial lookup of rhomboid
+        intersection.
+
+        :param morpho_cache: See :meth:`~bsb.storage.interfaces.PlacementSet.load_boxes`.
+        :returns: A boxtree
+        :rtype: bsb.trees.BoxTree
+        """
+        return BoxTree(list(self.load_boxes(morpho_cache=morpho_cache)))
+
+    def _requires_morpho_mapping(self):
+        return self._morphology_labels is not None and self.count_morphologies()
+
+    def _morpho_backmap(self, locs):
+        locs = locs.copy()
+        cols = locs[:, 1:]
+        ign_b = cols[:, 0] == -1
+        ign_p = cols[:, 1] == -1
+        semi = ign_b != ign_p
+        if np.any(semi):
+            raise ValueError(
+                f"Invalid data at {np.nonzero(semi)[0]}. -1 needs to occur in "
+                "either none or both columns to make point neuron connections."
+            )
+        to_map = ~ign_b
+        if np.any(locs[to_map, 1:] < 0):
+            raise ValueError(
+                f"Invalid data at {np.nonzero(locs[to_map, 1:] < 0)[0]}, "
+                "negative values are not valid morphology locations."
+            )
+        locs[to_map] = self.load_morphologies()._mapback(locs[to_map])
+        return locs
+
+
+class MorphologyRepository(Interface, engine_key="morphologies"):
+    @abc.abstractmethod
+    def all(self):
+        """
+        Fetch all of the stored morphologies.
+
+        :returns: List of the stored morphologies.
+        :rtype: List[~bsb.storage.interfaces.StoredMorphology]
+        """
+        pass
+
+    @abc.abstractmethod
+    def select(self, *selectors):
+        """
+        Select stored morphologies.
+
+        :param selectors: Any number of morphology selectors.
+        :type selectors: List[bsb.morphologies.selector.MorphologySelector]
+        :returns: All stored morphologies that match at least one selector.
+        :rtype: List[~bsb.storage.interfaces.StoredMorphology]
+        """
+        pass
+
+    @abc.abstractmethod
+    def save(self, name, morphology, overwrite=False):
+        """
+        Store a morphology
+
+        :param name: Key to store the morphology under.
+        :type name: str
+        :param morphology: Morphology to store
+        :type morphology: bsb.morphologies.Morphology
+        :param overwrite: Overwrite any stored morphology that already exists under that
+          name
+        :type overwrite: bool
+        :returns: The stored morphology
+        :rtype: ~bsb.storage.interfaces.StoredMorphology
+        """
+        pass
+
+    @abc.abstractmethod
+    def has(self, name):
+        """
+        Check whether a morphology under the given name exists
+
+        :param name: Key of the stored morphology.
+        :type name: str
+        :returns: Whether the key exists in the repo.
+        :rtype: bool
+        """
+        pass
+
+    def __contains__(self, item):
+        return self.has(item)
+
+    @abc.abstractmethod
+    def preload(self, name):
+        """
+        Load a stored morphology as a morphology loader.
+
+        :param name: Key of the stored morphology.
+        :type name: str
+        :returns: The stored morphology
+        :rtype: ~bsb.storage.interfaces.StoredMorphology
+        """
+        pass
+
+    @abc.abstractmethod
+    def load(self, name):
+        """
+        Load a stored morphology as a constructed morphology object.
+
+        :param name: Key of the stored morphology.
+        :type name: str
+        :returns: A morphology
+        :rtype: ~bsb.morphologies.Morphology
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_meta(self, name):
+        """
+        Get the metadata of a stored morphology.
+
+        :param name: Key of the stored morphology.
+        :type name: str
+        :returns: Metadata dictionary
+        :rtype: dict
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_all_meta(self):
+        """
+        Get the metadata of all stored morphologies.
+        :returns: Metadata dictionary
+        :rtype: dict
+        """
+        pass
+
+    @abc.abstractmethod
+    def set_all_meta(self, all_meta):
+        """
+        Set the metadata of all stored morphologies.
+        :param all_meta: Metadata dictionary.
+        :type all_meta: dict
+        """
+        pass
+
+    @abc.abstractmethod
+    def update_all_meta(self, meta):
+        """
+        Update the metadata of stored morphologies with the provided key values
+
+        :param meta: Metadata dictionary.
+        :type meta: str
+        """
+        pass
+
+    def list(self):
+        """
+        List all the names of the morphologies in the repository.
+        """
+        return [loader.name for loader in self.all()]
+
+
+class ConnectivitySet(Interface):
+    """
+    Stores the connections between 2 types of cell as ``local`` and ``global`` locations.
+    A location is a cell id, referring to the n-th cell in the chunk, a branch id, and a
+    point id, to specify the location on the morphology. Local locations refer to cells on
+    this chunk, while global locations can come from any chunk and is associated to a
+    certain chunk id as well.
+
+    Locations are either placement-context or chunk dependent: You may form connections
+    between the n-th cells of a placement set (using
+    :meth:`~.storage.interfaces.ConnectivitySet.connect`), or of the n-th cells of 2
+    chunks (using :meth:`~.storage.interfaces.ConnectivitySet.chunk_connect`).
+
+    A cell has both incoming and outgoing connections; when speaking of incoming
+    connections, the local locations are the postsynaptic cells, and when speaking of
+    outgoing connections they are the presynaptic cells. Vice versa for the global
+    connections.
+    """
+
+    # The following attributes must be set on each ConnectivitySet by the engine:
+    tag: str
+    pre_type_name: str
+    post_type_name: str
+    pre_type: "CellType"
+    post_type: "CellType"
+
+    @abc.abstractmethod
+    def __len__(self):
+        pass
+
+    @classmethod
+    @abc.abstractmethod
+    def create(cls, engine, tag):
+        """
+        Must create the placement set.
+        """
+        pass
+
+    @obj_str_insert
+    def __repr__(self):
+        cstr = f"with {len(self)} connections" if len(self) else "without connections"
+        return f"'{self.tag}' {cstr}"
+
+    @staticmethod
+    @abc.abstractmethod
+    def exists(engine, tag):
+        """
+        Must check the existence of the connectivity set
+        """
+        pass
+
+    def require(self, engine, tag):
+        """
+        Must make sure the connectivity set exists. The default
+        implementation uses the class's ``exists`` and ``create`` methods.
+        """
+        if not self.exists(engine, tag):
+            self.create(engine, tag)
+
+    @abc.abstractmethod
+    def clear(self, chunks=None):
+        """
+        Must clear (some chunks of) the placement set
+        """
+        pass
+
+    @classmethod
+    @abc.abstractmethod
+    def get_tags(cls, engine):
+        """
+        Must return the tags of all existing connectivity sets.
+
+        :param engine: Storage engine to inspect.
+        """
+        pass
+
+    @abc.abstractmethod
+    def connect(self, pre_set, post_set, src_locs, dest_locs):
+        """
+        Must connect the ``src_locs`` to the ``dest_locs``, interpreting the cell ids
+        (first column of the locs) as the cell rank in the placement set.
+        """
+        pass
+
+    @abc.abstractmethod
+    def chunk_connect(self, src_chunk, dst_chunk, src_locs, dst_locs):
+        """
+        Must connect the ``src_locs`` to the ``dest_locs``, interpreting the cell ids
+        (first column of the locs) as the cell rank in the chunk.
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_local_chunks(self, direction):
+        """
+        Must list all the local chunks that contain data in the given ``direction``
+        (``"inc"`` or ``"out"``).
+        """
+        pass
+
+    @abc.abstractmethod
+    def get_global_chunks(self, direction, local_):
+        """
+        Must list all the global chunks that contain data coming from a ``local`` chunk
+        in the given ``direction``
+        """
+        pass
+
+    @abc.abstractmethod
+    def nested_iter_connections(self, direction=None, local_=None, global_=None):
+        """
+        Must iterate over the connectivity data, leaving room for the end-user to set up
+        nested for loops:
+
+        .. code-block:: python
+
+          for dir, itr in self.nested_iter_connections():
+              for lchunk, itr in itr:
+                  for gchunk, data in itr:
+                      print(f"Nested {dir} block between {lchunk} and {gchunk}")
+
+        If a keyword argument is given, that axis is not iterated over, and the amount of
+        nested loops is reduced.
+        """
+        pass
+
+    @abc.abstractmethod
+    def flat_iter_connections(self, direction=None, local_=None, global_=None):
+        """
+        Must iterate over the connectivity data, yielding the direction, local chunk,
+        global chunk, and data:
+
+        .. code-block:: python
+
+          for dir, lchunk, gchunk, data in self.flat_iter_connections():
+              print(f"Flat {dir} block between {lchunk} and {gchunk}")
+
+        If a keyword argument is given, that axis is not iterated over, and the value is
+        fixed in each iteration.
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_block_connections(self, direction, local_, global_):
+        """
+        Must load the connections from ``direction`` perspective between ``local_`` and
+        ``global_``.
+
+        :returns: The local and global connections locations
+        :rtype: Tuple[numpy.ndarray, numpy.ndarray]
+        """
+        pass
+
+    @abc.abstractmethod
+    def load_local_connections(self, direction, local_):
+        """
+        Must load all the connections from ``direction`` perspective in ``local_``.
+
+        :returns: The local connection locations, a vector of the global connection chunks
+          (1 chunk id per connection), and the global connections locations. To identify a
+          cell in the global connections, use the corresponding chunk id from the second
+          return value.
+        :rtype: Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]
+        """
+        pass
+
+    def load_connections(self):
+        """
+        Loads connections as a ``CSIterator``.
+
+        :returns: A connectivity set iterator, that will load data
+        """
+        return ConnectivityIterator(self, "out")
+
+
+class ConnectivityIterator:
+    def __init__(
+        self, cs: ConnectivitySet, direction, lchunks=None, gchunks=None, scoped=True
+    ):
+        self._cs = cs
+        self._dir = direction
+        self._lchunks = lchunks
+        self._scoped = scoped
+        self._gchunks = gchunks
+
+    def __copy__(self):
+        lchunks = self._lchunks.copy() if self._lchunks is not None else None
+        gchunks = self._gchunks.copy() if self._gchunks is not None else None
+        return ConnectivityIterator(self._cs, self._dir, lchunks, gchunks)
+
+    def __len__(self):
+        return len(self.all()[0])
+
+    def __iter__(self):
+        """
+        Iterate over the connection locations chunk by chunk.
+
+        :returns: The presynaptic location matrix and postsynaptic location matrix.
+        :rtype: Tuple[numpy.ndarray, numpy.ndarray]
+        """
+        for _, pre_locs, _, post_locs in self.chunk_iter():
+            yield from zip(pre_locs, post_locs)
+
+    def chunk_iter(self):
+        """
+        Iterate over the connection data chunk by chunk.
+
+        :returns: The presynaptic chunk, presynaptic locations, postsynaptic chunk,
+          and postsynaptic locations.
+        :rtype: Tuple[~bsb.storage._chunks.Chunk, numpy.ndarray, ~bsb.storage._chunks.Chunk, numpy.ndarray]
+        """
+        yield from (
+            self._offset_block(*data)
+            for data in self._cs.flat_iter_connections(
+                self._dir, self._lchunks, self._gchunks
+            )
+        )
+
+    @immutable()
+    def as_globals(self):
+        self._scoped = False
+
+    @immutable()
+    def as_scoped(self):
+        self._scoped = True
+
+    @immutable()
+    def outgoing(self):
+        self._dir = "out"
+        self._lchunks, self._gchunks = self._gchunks, self._lchunks
+
+    @immutable()
+    def incoming(self):
+        self._dir = "inc"
+        self._lchunks, self._gchunks = self._gchunks, self._lchunks
+
+    @immutable()
+    def to(self, chunks):
+        if isinstance(chunks, Chunk) and chunks.ndim == 1:
+            chunks = [chunks]
+        if self._dir == "inc":
+            self._lchunks = chunks
+        else:
+            self._gchunks = chunks
+
+    @immutable()
+    def from_(self, chunks):
+        if isinstance(chunks, Chunk) and chunks.ndim == 1:
+            chunks = [chunks]
+        if self._dir == "out":
+            self._lchunks = chunks
+        else:
+            self._gchunks = chunks
+
+    def all(self):
+        pre_blocks = []
+        post_blocks = []
+        lens = []
+        for _, pre_block, _, post_block in self.chunk_iter():
+            pre_blocks.append(pre_block)
+            post_blocks.append(post_block)
+            lens.append(len(pre_block))
+        pre_locs = np.empty((sum(lens), 3), dtype=int)
+        post_locs = np.empty((sum(lens), 3), dtype=int)
+        ptr = 0
+        for len_, pre_block, post_block in zip(lens, pre_blocks, post_blocks):
+            pre_locs[ptr : ptr + len_] = pre_block
+            post_locs[ptr : ptr + len_] = post_block
+            ptr += len_
+        return pre_locs, post_locs
+
+    def _offset_block(self, direction: str, lchunk, gchunk, data):
+        loff = self._local_chunk_offsets()
+        goff = self._global_chunk_offsets()
+        llocs, glocs = data
+        llocs[:, 0] += loff[lchunk]
+        glocs[:, 0] += goff[gchunk]
+        if direction == "out":
+            return lchunk, llocs, gchunk, glocs
+        else:
+            return gchunk, glocs, lchunk, llocs
+
+    @functools.cache
+    def _local_chunk_offsets(self):
+        source = self._cs.post_type if self._dir == "inc" else self._cs.pre_type
+        return self._chunk_offsets(source, self._lchunks)
+
+    @functools.cache
+    def _global_chunk_offsets(self):
+        source = self._cs.pre_type if self._dir == "inc" else self._cs.post_type
+        return self._chunk_offsets(source, self._gchunks)
+
+    def _chunk_offsets(self, source, chunks):
+        stats = source.get_placement_set().get_chunk_stats()
+        if self._scoped and chunks is not None:
+            stats = {chunk: item for chunk, item in stats.items() if int(chunk) in chunks}
+        offsets = {}
+        ctr = 0
+        for chunk, len_ in sorted(
+            stats.items(), key=lambda k: Chunk.from_id(int(k[0]), None).id
+        ):
+            offsets[Chunk.from_id(int(chunk), None)] = ctr
+            ctr += len_
+        return offsets
+
+
+class StoredMorphology:
+    def __init__(self, name, loader, meta):
+        self.name = name
+        self._loader = loader
+        self._meta = meta
+
+    def __eq__(self, other):
+        return self.name == other.name
+
+    def __hash__(self):
+        return hash(self.name)
+
+    def get_meta(self):
+        return self._meta.copy()
+
+    def load(self):
+        return self._loader()
+
+    def cached_load(self, labels=None):
+        if labels is not None:
+            labels = tuple(labels)
+        return self._cached_load(labels)
+
+    @functools.cache
+    def _cached_load(self, labels):
+        return self.load().set_label_filter(labels).as_filtered()
+
+
+class GeneratedMorphology(StoredMorphology):
+    def __init__(self, name, generated, meta):
+        super().__init__(name, lambda: generated, meta)
+
+
+__all__ = [
+    "ConnectivityIterator",
+    "ConnectivitySet",
+    "Engine",
+    "FileStore",
+    "GeneratedMorphology",
+    "Interface",
+    "MorphologyRepository",
+    "NetworkDescription",
+    "NoopLock",
+    "PlacementSet",
+    "ReadOnlyManager",
+    "StorageNode",
+    "StoredFile",
+    "StoredMorphology",
+]
```

### Comparing `bsb_core-4.0.1/bsb/topology/__init__.py` & `bsb_core-4.1.0/bsb/topology/__init__.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,114 +1,114 @@
-"""
-Topology module
-"""
-
-from ._layout import box_layout
-from .partition import AllenStructure, Layer, NrrdVoxels, Partition
-from .region import Region, RegionGroup, Stack
-
-
-def create_topology(regions, ldc, mdc):
-    """
-    Create a topology from group of regions. Will check for root regions, if there's not
-    exactly 1 root region a :class:`~.topology.region.RegionGroup` will be created as new
-    root.
-
-    :param regions: Any iterable of regions.
-    :type regions: Iterable
-    :param ldc: Least dominant corner of the topology. Forms the suggested outer bounds of
-      the topology together with the `mdc`.
-    :param mdc: Most dominant corner of the topology. Forms the suggested outer bounds of
-      the topology together with the `mdc`.
-    """
-    regions = list(regions)
-    if len(roots := get_root_regions(regions)) == 1:
-        topology = roots[0]
-    else:
-        topology = RegionGroup(children=roots, name="topology")
-    hint = box_layout(ldc, mdc)
-    topology.do_layout(hint)
-    return topology
-
-
-def get_partitions(regions):
-    """
-    Get all of the partitions belonging to the group of regions and their subregions.
-
-    :param regions: Any iterable of regions.
-    :type regions: Iterable
-    """
-
-    def collect_deps(region, ignore):
-        # get_dependencies can be overridden, so `list` it to avoid mutation of userdata
-        deps = list(region.get_dependencies())
-        regions = [d for d in deps if not is_partition(d)]
-        parts = set(p for p in deps if is_partition(p))
-        ignore.update(parts)
-        for r in regions:
-            # Only check unchecked regions
-            if r not in ignore:
-                ignore.add(r)
-                parts.update(collect_deps(r, ignore))
-        return parts
-
-    partitions = set()
-    for region in regions:
-        partitions.update(p := collect_deps(region, set()))
-
-    return partitions
-
-
-def is_partition(obj):
-    """
-    Checks if an object is a partition.
-    """
-    return (
-        hasattr(obj, "get_layout")
-        and hasattr(obj, "to_chunks")
-        and hasattr(obj, "chunk_to_voxels")
-    )
-
-
-def is_region(obj):
-    """
-    Checks if an object is a region.
-    """
-    return hasattr(obj, "get_layout") and hasattr(obj, "do_layout")
-
-
-def get_root_regions(regions):
-    """
-    Get all of the root regions, not belonging to any other region in the given group.
-
-    :param regions: Any iterable of regions.
-    :type regions: Iterable
-    """
-    managed = set()
-
-    def collect_deps(region, ignore):
-        # get_dependencies can be overridden, so `list` it to avoid mutation of userdata
-        deps = list(region.get_dependencies()).copy()
-        for dep in deps:
-            # Only check unchecked regions, ignore visited & partitions
-            if dep not in ignore and is_region(dep):
-                ignore.add(dep)
-                extra_deps = collect_deps(dep, ignore)
-                ignore.update(extra_deps)
-        return deps
-
-    # Give `managed` as the mutable ignore arg so that it is filled with all regionals
-    # encountered as dependencies by the `collect_deps` recursive function.
-    for region in regions:
-        collect_deps(region, managed)
-
-    return list(set(list(regions)) - managed)
-
-
-__all__ = [
-    "box_layout",
-    "create_topology",
-    "get_partitions",
-    "get_root_regions",
-    "is_partition",
-    "is_region",
-]
+"""
+Topology module
+"""
+
+from ._layout import box_layout
+from .partition import AllenStructure, Layer, NrrdVoxels, Partition
+from .region import Region, RegionGroup, Stack
+
+
+def create_topology(regions, ldc, mdc):
+    """
+    Create a topology from group of regions. Will check for root regions, if there's not
+    exactly 1 root region a :class:`~.topology.region.RegionGroup` will be created as new
+    root.
+
+    :param regions: Any iterable of regions.
+    :type regions: Iterable
+    :param ldc: Least dominant corner of the topology. Forms the suggested outer bounds of
+      the topology together with the `mdc`.
+    :param mdc: Most dominant corner of the topology. Forms the suggested outer bounds of
+      the topology together with the `mdc`.
+    """
+    regions = list(regions)
+    if len(roots := get_root_regions(regions)) == 1:
+        topology = roots[0]
+    else:
+        topology = RegionGroup(children=roots, name="topology")
+    hint = box_layout(ldc, mdc)
+    topology.do_layout(hint)
+    return topology
+
+
+def get_partitions(regions):
+    """
+    Get all of the partitions belonging to the group of regions and their subregions.
+
+    :param regions: Any iterable of regions.
+    :type regions: Iterable
+    """
+
+    def collect_deps(region, ignore):
+        # get_dependencies can be overridden, so `list` it to avoid mutation of userdata
+        deps = list(region.get_dependencies())
+        regions = [d for d in deps if not is_partition(d)]
+        parts = set(p for p in deps if is_partition(p))
+        ignore.update(parts)
+        for r in regions:
+            # Only check unchecked regions
+            if r not in ignore:
+                ignore.add(r)
+                parts.update(collect_deps(r, ignore))
+        return parts
+
+    partitions = set()
+    for region in regions:
+        partitions.update(p := collect_deps(region, set()))
+
+    return partitions
+
+
+def is_partition(obj):
+    """
+    Checks if an object is a partition.
+    """
+    return (
+        hasattr(obj, "get_layout")
+        and hasattr(obj, "to_chunks")
+        and hasattr(obj, "chunk_to_voxels")
+    )
+
+
+def is_region(obj):
+    """
+    Checks if an object is a region.
+    """
+    return hasattr(obj, "get_layout") and hasattr(obj, "do_layout")
+
+
+def get_root_regions(regions):
+    """
+    Get all of the root regions, not belonging to any other region in the given group.
+
+    :param regions: Any iterable of regions.
+    :type regions: Iterable
+    """
+    managed = set()
+
+    def collect_deps(region, ignore):
+        # get_dependencies can be overridden, so `list` it to avoid mutation of userdata
+        deps = list(region.get_dependencies()).copy()
+        for dep in deps:
+            # Only check unchecked regions, ignore visited & partitions
+            if dep not in ignore and is_region(dep):
+                ignore.add(dep)
+                extra_deps = collect_deps(dep, ignore)
+                ignore.update(extra_deps)
+        return deps
+
+    # Give `managed` as the mutable ignore arg so that it is filled with all regionals
+    # encountered as dependencies by the `collect_deps` recursive function.
+    for region in regions:
+        collect_deps(region, managed)
+
+    return list(set(list(regions)) - managed)
+
+
+__all__ = [
+    "box_layout",
+    "create_topology",
+    "get_partitions",
+    "get_root_regions",
+    "is_partition",
+    "is_region",
+]
```

### Comparing `bsb_core-4.0.1/bsb/topology/_layout.py` & `bsb_core-4.1.0/bsb/topology/_layout.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,149 +1,149 @@
-import abc
-
-import numpy as np
-
-
-class Layout:
-    """
-    Container class for all types of partition data. The layout swaps the data of the
-    partition with temporary layout associated data, and tries out experimental changes
-    to the partition data, if the layout process fails, the original partition data is
-    reinstated.
-    """
-
-    def __init__(self, data, owner=None, children=None, frozen=False):
-        if children is None:
-            children = []
-        self._data = data
-        self._owner = owner
-        self._children = children
-        self._frozen = frozen
-
-    @property
-    def data(self):
-        return self._data
-
-    @property
-    def children(self):
-        return self._children
-
-    def copy(self):
-        return Layout(
-            data=self._data.copy(),
-            owner=self._owner,
-            children=self._children,
-            frozen=self._frozen,
-        )
-
-    def accept(self):
-        self.swap()
-
-    def __getattr__(self, attr):
-        if attr.startswith("propose_"):
-            f = getattr(self._owner, attr[8:])
-
-            def swapped_execute(*args, **kwargs):
-                self.swap()
-                f(*args, **kwargs)
-                self.swap()
-
-            return swapped_execute
-
-        super().__getattribute__(attr)
-
-    def swap(self):
-        if self._owner is not None:
-            old = getattr(self._owner, "_data", None)
-            self._owner._data = self._data
-            self._data = old
-        for child in self._children:
-            child.swap()
-
-
-class PartitionData(abc.ABC):
-    """
-    The partition data is a class that stores the description of a partition for a
-    partition. This allows the Partition interface to define mutating operations such as
-    translate, rotate, scale; for a dry-run we only have to swap out the actual data with
-    temporary data, and the mutation is prevented.
-    """
-
-    @abc.abstractmethod
-    def copy(self):
-        pass
-
-
-class RhomboidData(PartitionData):
-    def __init__(self, ldc, mdc):
-        # Least dominant corner
-        self.ldc = np.array(ldc, dtype=float, copy=False)
-        # Most dominant corner
-        self.mdc = np.array(mdc, dtype=float, copy=False)
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}({self.ldc}, {self.mdc})"
-
-    def copy(self):
-        """
-        Copy this boundary to a new instance.
-        """
-        return self.__class__(self.ldc.copy(), self.mdc.copy())
-
-    @property
-    def x(self):
-        return self.ldc[0]
-
-    @x.setter
-    def x(self, value):
-        self.mdc[0] += value - self.ldc[0]
-        self.ldc[0] = value
-
-    @property
-    def y(self):
-        return self.ldc[1]
-
-    @y.setter
-    def y(self, value):
-        self.mdc[1] += value - self.ldc[1]
-        self.ldc[1] = value
-
-    @property
-    def z(self):
-        return self.ldc[2]
-
-    @z.setter
-    def z(self, value):
-        self.mdc[2] += value - self.ldc[2]
-        self.ldc[2] = value
-
-    @property
-    def dimensions(self):
-        return self.mdc - self.ldc
-
-    @property
-    def width(self):
-        return self.mdc[0] - self.ldc[0]
-
-    @width.setter
-    def width(self, value):
-        self.mdc[0] = self.ldc[0] + value
-
-    @property
-    def height(self):
-        return self.mdc[2] - self.ldc[2]
-
-    @height.setter
-    def height(self, value):
-        self.mdc[2] = self.ldc[2] + value
-
-    @property
-    def depth(self):
-        return self.mdc[1] - self.ldc[1]
-
-    @depth.setter
-    def depth(self, value):
-        self.mdc[1] = self.ldc[1] + value
-
-
-def box_layout(ldc, mdc):
-    return Layout(RhomboidData(ldc, mdc), frozen=True)
+import abc
+
+import numpy as np
+
+
+class Layout:
+    """
+    Container class for all types of partition data. The layout swaps the data of the
+    partition with temporary layout associated data, and tries out experimental changes
+    to the partition data, if the layout process fails, the original partition data is
+    reinstated.
+    """
+
+    def __init__(self, data, owner=None, children=None, frozen=False):
+        if children is None:
+            children = []
+        self._data = data
+        self._owner = owner
+        self._children = children
+        self._frozen = frozen
+
+    @property
+    def data(self):
+        return self._data
+
+    @property
+    def children(self):
+        return self._children
+
+    def copy(self):
+        return Layout(
+            data=self._data.copy(),
+            owner=self._owner,
+            children=self._children,
+            frozen=self._frozen,
+        )
+
+    def accept(self):
+        self.swap()
+
+    def __getattr__(self, attr):
+        if attr.startswith("propose_"):
+            f = getattr(self._owner, attr[8:])
+
+            def swapped_execute(*args, **kwargs):
+                self.swap()
+                f(*args, **kwargs)
+                self.swap()
+
+            return swapped_execute
+
+        super().__getattribute__(attr)
+
+    def swap(self):
+        if self._owner is not None:
+            old = getattr(self._owner, "_data", None)
+            self._owner._data = self._data
+            self._data = old
+        for child in self._children:
+            child.swap()
+
+
+class PartitionData(abc.ABC):
+    """
+    The partition data is a class that stores the description of a partition for a
+    partition. This allows the Partition interface to define mutating operations such as
+    translate, rotate, scale; for a dry-run we only have to swap out the actual data with
+    temporary data, and the mutation is prevented.
+    """
+
+    @abc.abstractmethod
+    def copy(self):
+        pass
+
+
+class RhomboidData(PartitionData):
+    def __init__(self, ldc, mdc):
+        # Least dominant corner
+        self.ldc = np.array(ldc, dtype=float, copy=False)
+        # Most dominant corner
+        self.mdc = np.array(mdc, dtype=float, copy=False)
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}({self.ldc}, {self.mdc})"
+
+    def copy(self):
+        """
+        Copy this boundary to a new instance.
+        """
+        return self.__class__(self.ldc.copy(), self.mdc.copy())
+
+    @property
+    def x(self):
+        return self.ldc[0]
+
+    @x.setter
+    def x(self, value):
+        self.mdc[0] += value - self.ldc[0]
+        self.ldc[0] = value
+
+    @property
+    def y(self):
+        return self.ldc[1]
+
+    @y.setter
+    def y(self, value):
+        self.mdc[1] += value - self.ldc[1]
+        self.ldc[1] = value
+
+    @property
+    def z(self):
+        return self.ldc[2]
+
+    @z.setter
+    def z(self, value):
+        self.mdc[2] += value - self.ldc[2]
+        self.ldc[2] = value
+
+    @property
+    def dimensions(self):
+        return self.mdc - self.ldc
+
+    @property
+    def width(self):
+        return self.mdc[0] - self.ldc[0]
+
+    @width.setter
+    def width(self, value):
+        self.mdc[0] = self.ldc[0] + value
+
+    @property
+    def height(self):
+        return self.mdc[2] - self.ldc[2]
+
+    @height.setter
+    def height(self, value):
+        self.mdc[2] = self.ldc[2] + value
+
+    @property
+    def depth(self):
+        return self.mdc[1] - self.ldc[1]
+
+    @depth.setter
+    def depth(self, value):
+        self.mdc[1] = self.ldc[1] + value
+
+
+def box_layout(ldc, mdc):
+    return Layout(RhomboidData(ldc, mdc), frozen=True)
```

### Comparing `bsb_core-4.0.1/bsb/topology/region.py` & `bsb_core-4.1.0/bsb/topology/region.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,130 +1,130 @@
-"""
-Module for the Region types.
-"""
-
-import abc
-import typing
-
-import numpy as np
-
-from .. import config
-from ..config import refs, types
-from ..reporting import warn
-from ._layout import Layout
-
-if typing.TYPE_CHECKING:
-    from ..core import Scaffold
-    from .partition import Partition
-
-
-@config.dynamic(attr_name="type", required=False, default="group", auto_classmap=True)
-class Region(abc.ABC):
-    """
-    Base region.
-
-    When arranging will simply call arrange/layout on its children but won't cause any
-    changes itself.
-    """
-
-    scaffold: "Scaffold"
-
-    name: str = config.attr(key=True)
-    children: list[typing.Union["Region", "Partition"]] = config.reflist(
-        refs.regional_ref, backref="region", required=True
-    )
-
-    @property
-    def data(self):
-        # The data property is read-only to users, but `_data` is assigned
-        # during the layout process
-        return self._data
-
-    def get_dependencies(self):
-        return self.children.copy()
-
-    def __boot__(self):
-        pass
-
-    def get_layout(self, hint):
-        layouts = [dep.get_layout(hint) for dep in self.get_dependencies()]
-        return Layout(hint.data.copy(), owner=self, children=layouts)
-
-    def do_layout(self, hint):
-        layout = self.get_layout(hint)
-        layout.accept()
-
-    @abc.abstractmethod
-    def rotate(self, rotation):  # pragma: nocover
-        pass
-
-    @abc.abstractmethod
-    def translate(self, offset):  # pragma: nocover
-        pass
-
-    @abc.abstractmethod
-    def scale(self, factors):  # pragma: nocover
-        pass
-
-
-@config.node
-class RegionGroup(Region, classmap_entry="group"):
-    def rotate(self, rotation):
-        for child in self.children:
-            child.rotate(rotation)
-
-    def translate(self, offset):
-        for child in self.children:
-            child.translate(offset)
-
-    def scale(self, factors):
-        for child in self.children:
-            child.scale(factors)
-
-
-@config.node
-class Stack(RegionGroup, classmap_entry="stack"):
-    """
-    Stack components on top of each other based on their ``stack_index`` and adjust its
-    own height accordingly.
-    """
-
-    axis: typing.Union[typing.Literal["x"], typing.Literal["y"], typing.Literal["z"]] = (
-        config.attr(type=types.in_(["x", "y", "z"]), default="z")
-    )
-
-    def get_layout(self, hint):
-        layout = super().get_layout(hint)
-        stack_size = 0
-        axis_idx = ("x", "y", "z").index(self.axis)
-        trans_eye = np.zeros(3)
-        trans_eye[axis_idx] = 1
-
-        for child in layout.children:
-            if child.data is None:
-                warn(f"Skipped layout arrangement of {child._owner.name} in {self.name}")
-                continue
-            translation = (
-                layout.data.ldc[axis_idx] + stack_size - child.data.ldc
-            ) * trans_eye
-            if not np.allclose(0, translation):
-                child.propose_translate(translation)
-            stack_size += child.data.dimensions[axis_idx]
-        ldc = layout.data.ldc
-        mdc = layout.data.mdc
-        mdc[axis_idx] = ldc[axis_idx] + stack_size
-        return layout
-
-    def rotate(self, rotation):
-        for child in self.children:
-            child.rotate(rotation)
-
-    def translate(self, offset):
-        for child in self.children:
-            child.translate(offset)
-
-    def scale(self, factors):
-        for child in self.children:
-            child.scale(factors)
-
-
-__all__ = ["Region", "RegionGroup", "Stack"]
+"""
+Module for the Region types.
+"""
+
+import abc
+import typing
+
+import numpy as np
+
+from .. import config
+from ..config import refs, types
+from ..reporting import warn
+from ._layout import Layout
+
+if typing.TYPE_CHECKING:
+    from ..core import Scaffold
+    from .partition import Partition
+
+
+@config.dynamic(attr_name="type", required=False, default="group", auto_classmap=True)
+class Region(abc.ABC):
+    """
+    Base region.
+
+    When arranging will simply call arrange/layout on its children but won't cause any
+    changes itself.
+    """
+
+    scaffold: "Scaffold"
+
+    name: str = config.attr(key=True)
+    children: list[typing.Union["Region", "Partition"]] = config.reflist(
+        refs.regional_ref, backref="region", required=True
+    )
+
+    @property
+    def data(self):
+        # The data property is read-only to users, but `_data` is assigned
+        # during the layout process
+        return self._data
+
+    def get_dependencies(self):
+        return self.children.copy()
+
+    def __boot__(self):
+        pass
+
+    def get_layout(self, hint):
+        layouts = [dep.get_layout(hint) for dep in self.get_dependencies()]
+        return Layout(hint.data.copy(), owner=self, children=layouts)
+
+    def do_layout(self, hint):
+        layout = self.get_layout(hint)
+        layout.accept()
+
+    @abc.abstractmethod
+    def rotate(self, rotation):  # pragma: nocover
+        pass
+
+    @abc.abstractmethod
+    def translate(self, offset):  # pragma: nocover
+        pass
+
+    @abc.abstractmethod
+    def scale(self, factors):  # pragma: nocover
+        pass
+
+
+@config.node
+class RegionGroup(Region, classmap_entry="group"):
+    def rotate(self, rotation):
+        for child in self.children:
+            child.rotate(rotation)
+
+    def translate(self, offset):
+        for child in self.children:
+            child.translate(offset)
+
+    def scale(self, factors):
+        for child in self.children:
+            child.scale(factors)
+
+
+@config.node
+class Stack(RegionGroup, classmap_entry="stack"):
+    """
+    Stack components on top of each other based on their ``stack_index`` and adjust its
+    own height accordingly.
+    """
+
+    axis: typing.Union[typing.Literal["x"], typing.Literal["y"], typing.Literal["z"]] = (
+        config.attr(type=types.in_(["x", "y", "z"]), default="z")
+    )
+
+    def get_layout(self, hint):
+        layout = super().get_layout(hint)
+        stack_size = 0
+        axis_idx = ("x", "y", "z").index(self.axis)
+        trans_eye = np.zeros(3)
+        trans_eye[axis_idx] = 1
+
+        for child in layout.children:
+            if child.data is None:
+                warn(f"Skipped layout arrangement of {child._owner.name} in {self.name}")
+                continue
+            translation = (
+                layout.data.ldc[axis_idx] + stack_size - child.data.ldc
+            ) * trans_eye
+            if not np.allclose(0, translation):
+                child.propose_translate(translation)
+            stack_size += child.data.dimensions[axis_idx]
+        ldc = layout.data.ldc
+        mdc = layout.data.mdc
+        mdc[axis_idx] = ldc[axis_idx] + stack_size
+        return layout
+
+    def rotate(self, rotation):
+        for child in self.children:
+            child.rotate(rotation)
+
+    def translate(self, offset):
+        for child in self.children:
+            child.translate(offset)
+
+    def scale(self, factors):
+        for child in self.children:
+            child.scale(factors)
+
+
+__all__ = ["Region", "RegionGroup", "Stack"]
```

### Comparing `bsb_core-4.0.1/bsb/trees.py` & `bsb_core-4.1.0/bsb/trees.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,76 +1,76 @@
-"""
-Module for binary space partitioning, to facilitate optimal runtime complexity for n-point
-problems.
-"""
-
-import abc
-
-from rtree import index as rtree
-
-
-class BoxTreeInterface(abc.ABC):
-    """
-    Tree for fast lookup of queries of axis aligned rhomboids.
-    """
-
-    @abc.abstractmethod
-    def query(self, boxes, unique=False):
-        """
-        Should return a generator that yields lists of intersecting IDs per query box if
-        ``unique=False``. If ``unique=True``, yield a flat list of unique intersecting box
-        IDs for all queried boxes.
-        """
-        pass
-
-    @abc.abstractmethod
-    def __len__(self):
-        pass
-
-
-class _BoxRTree(BoxTreeInterface):
-    """
-    Tree for fast lookup of queries of axis aligned rhomboids using the Rtree package.
-    """
-
-    def __init__(self, boxes):
-        self._rtree = rtree.Index(properties=rtree.Property(dimension=3))
-        for id, box in enumerate(boxes):
-            self._rtree.insert(id, box)
-
-    def __len__(self):
-        return len(self._rtree)
-
-    def query(self, boxes, unique=False):
-        """
-        Given an iterable of ``(min_x, min_y, min_z, max_x, max_y, max_z)`` box tuples,
-        find all the boxes that intersect with them.
-
-        :param boxes: Boxes to look for intersections with.
-        :type boxes: Iterable[Tuple[float, float, float, float, float, float]]
-        :param unique: If ``True``, return a flat generator of unique results. If ``False``
-            (default), per box in ``boxes``, return a list of intersecting boxes.
-        :returns: See ``unique``.
-        :rtype: Union[Iterator[List[Tuple[float, float, float, float, float, float]]],
-            Iterator[Tuple[float, float, float, float, float, float]]]
-        """
-        all_ = (list(self._rtree.intersection(box, objects=False)) for box in boxes)
-        if unique:
-            seen = set()
-            # Double for loop over results, skipping those that have been seen before.
-            yield from (
-                seen.add(elem) or elem for arr in all_ for elem in arr if elem not in seen
-            )
-        else:
-            yield from all_
-
-
-# Cheapo provider pattern.
-class BoxTree(_BoxRTree):
-    """
-    Tree for fast lookup of repeat queries of axis aligned rhomboids.
-    """
-
-    pass
-
-
-__all__ = ["BoxTreeInterface", "BoxTree"]
+"""
+Module for binary space partitioning, to facilitate optimal runtime complexity for n-point
+problems.
+"""
+
+import abc
+
+from rtree import index as rtree
+
+
+class BoxTreeInterface(abc.ABC):
+    """
+    Tree for fast lookup of queries of axis aligned rhomboids.
+    """
+
+    @abc.abstractmethod
+    def query(self, boxes, unique=False):
+        """
+        Should return a generator that yields lists of intersecting IDs per query box if
+        ``unique=False``. If ``unique=True``, yield a flat list of unique intersecting box
+        IDs for all queried boxes.
+        """
+        pass
+
+    @abc.abstractmethod
+    def __len__(self):
+        pass
+
+
+class _BoxRTree(BoxTreeInterface):
+    """
+    Tree for fast lookup of queries of axis aligned rhomboids using the Rtree package.
+    """
+
+    def __init__(self, boxes):
+        self._rtree = rtree.Index(properties=rtree.Property(dimension=3))
+        for id, box in enumerate(boxes):
+            self._rtree.insert(id, box)
+
+    def __len__(self):
+        return len(self._rtree)
+
+    def query(self, boxes, unique=False):
+        """
+        Given an iterable of ``(min_x, min_y, min_z, max_x, max_y, max_z)`` box tuples,
+        find all the boxes that intersect with them.
+
+        :param boxes: Boxes to look for intersections with.
+        :type boxes: Iterable[Tuple[float, float, float, float, float, float]]
+        :param unique: If ``True``, return a flat generator of unique results. If ``False``
+            (default), per box in ``boxes``, return a list of intersecting boxes.
+        :returns: See ``unique``.
+        :rtype: Union[Iterator[List[Tuple[float, float, float, float, float, float]]],
+            Iterator[Tuple[float, float, float, float, float, float]]]
+        """
+        all_ = (list(self._rtree.intersection(box, objects=False)) for box in boxes)
+        if unique:
+            seen = set()
+            # Double for loop over results, skipping those that have been seen before.
+            yield from (
+                seen.add(elem) or elem for arr in all_ for elem in arr if elem not in seen
+            )
+        else:
+            yield from all_
+
+
+# Cheapo provider pattern.
+class BoxTree(_BoxRTree):
+    """
+    Tree for fast lookup of repeat queries of axis aligned rhomboids.
+    """
+
+    pass
+
+
+__all__ = ["BoxTreeInterface", "BoxTree"]
```

### Comparing `bsb_core-4.0.1/bsb/voxels.py` & `bsb_core-4.1.0/bsb/voxels.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,640 +1,640 @@
-import functools
-import itertools
-
-import numpy as np
-
-from .exceptions import EmptyVoxelSetError
-from .trees import BoxTree
-
-
-class VoxelData(np.ndarray):
-    """
-    Chunk identifier, consisting of chunk coordinates and size.
-    """
-
-    def __new__(cls, data, keys=None):
-        if data.ndim < 2:
-            return super().__new__(np.ndarray, data.shape, dtype=object)
-        obj = super().__new__(cls, data.shape, dtype=object)
-        obj[:] = data
-        if keys is not None:
-            keys = [str(k) for k in keys]
-            if len(set(keys)) != len(keys):
-                raise ValueError("Data keys must be unique")
-            if len(keys) != data.shape[1]:
-                raise ValueError("Amount of data keys must match amount of data columns")
-            obj._keys = keys
-        else:
-            obj._keys = []
-        return obj
-
-    def __getitem__(self, index):
-        index, keys = self._rewrite_index(index)
-        vd = super().__getitem__(index)
-        if isinstance(vd, VoxelData):
-            if len(keys) > 0 and len(vd) != vd.size / len(keys):
-                vd = vd.reshape(-1, len(keys))
-            vd._keys = keys
-        return vd
-
-    def __array_finalize__(self, obj):
-        if obj is not None:
-            self._keys = []
-
-    @property
-    def keys(self):
-        """
-        Returns the keys, or column labels, associated to each data column.
-        """
-        return self._keys.copy()
-
-    def copy(self):
-        """
-        Return a new copy of the voxel data
-        """
-        new = super().copy()
-        new._keys = self._keys.copy()
-        return new
-
-    def _split_index(self, index):
-        try:
-            if isinstance(index, tuple):
-                cols = [self._keys.index(idx) for idx in index if isinstance(idx, str)]
-                keys = [self._keys[c] for c in cols]
-                index = tuple(idx for idx in index if not isinstance(idx, str))
-            elif isinstance(index, str):
-                cols = [self._keys.index(index)]
-                keys = [self._keys[cols[0]]]
-                index = (slice(None),)
-            else:
-                index = (index,)
-                cols = None
-                keys = getattr(self, "_keys", [])
-        except ValueError as e:
-            key = str(e).split("'")[1]
-            raise IndexError(f"Voxel data key '{key}' does not exist.") from None
-        return index, cols, keys
-
-    def _rewrite_index(self, index):
-        index, cols, keys = self._split_index(index)
-        if cols:
-            return (*index, cols), keys
-        else:
-            return index, keys
-
-
-class VoxelSet:
-    def __init__(self, voxels, size, data=None, data_keys=None, irregular=False):
-        """
-        Constructs a voxel set from the given voxel indices or coordinates.
-
-        :param voxels: The spatial coordinates, or grid indices of each voxel. Nx3 matrix
-        :type voxels: numpy.ndarray
-        :param size: The global or individual size of the voxels. If it has 2 dimensions
-          it needs to have the same length as `voxels`, and will be used as individual
-          voxels.
-        :type size: numpy.ndarray
-        :param data:
-
-        .. warning::
-
-            If :class:`numpy.ndarray` are passed, they will not be copied in order to save
-            memory and time. You may accidentally change a voxelset if you later change
-            the same array.
-        """
-        voxels = np.array(voxels, copy=False)
-        voxel_size = np.array(size, copy=False)
-        if voxels.dtype.name == "object":
-            raise ValueError("Couldn't convert given `voxels` to a voxel matrix")
-        if voxels.ndim != 2:
-            if not len(voxels):
-                # Try some massaging in case of empty arrays
-                voxels = voxels.reshape(-1, 3)
-                if not len(voxel_size):
-                    voxel_size = voxel_size.reshape(-1, 3)
-            else:
-                raise ValueError("`voxels` needs to be convertable to a 2D matrix")
-        if voxels.ndim == 2 and voxels.shape[1] != 3:
-            raise ValueError("`voxels` needs to have 3 columns, 1 for each spatial dim.")
-        if not _is_broadcastable(voxels.shape, voxel_size.shape):
-            raise ValueError(
-                f"Shape {voxel_size.shape} of `size` is"
-                + f" invalid for voxel shape {voxels.shape}"
-            )
-        if data is not None:
-            if isinstance(data, VoxelData):
-                if data_keys is None:
-                    self._data = data
-                else:
-                    self._data = VoxelData(data, keys=data_keys)
-            else:
-                data = np.array(data, copy=False)
-                if data.ndim < 2:
-                    cols = len(data_keys) if data_keys else 1
-                    data = data.reshape(-1, cols)
-                self._data = VoxelData(data, keys=data_keys)
-            if len(self._data) != len(voxels):
-                raise ValueError("`voxels` and `data` length unequal.")
-        else:
-            self._data = None
-
-        if not len(voxel_size.shape):
-            self._cubic = True
-        if voxel_size.ndim > 1:
-            if voxel_size.size != voxels.size:
-                raise ValueError("`voxels` and `size` length unequal.")
-            # Voxels given in spatial coords with individual size
-            self._sizes = voxel_size
-            self._coords = voxels
-            self._regular = False
-        elif irregular:
-            # Voxels given in spatial coords but of equal size
-            self._size = voxel_size
-            self._coords = voxels
-            self._regular = False
-        else:
-            # Voxels given in index coords
-            self._size = voxel_size
-            self._indices = np.array(voxels, copy=False, dtype=int)
-            self._regular = True
-
-    def __iter__(self):
-        return iter(self.get_raw(copy=False))
-
-    def __array__(self):
-        return self.get_raw(copy=False)
-
-    def __len__(self):
-        return len(self.get_raw(copy=False))
-
-    def __getitem__(self, index):
-        if self.has_data:
-            data = self._data[index]
-            index, _, _ = self._data._split_index(index)
-        else:
-            data, _ = None, None
-        if isinstance(index, tuple) and len(index) > 1:
-            raise IndexError("Too many indices for VoxelSet, maximum 1.")
-        voxels = self.get_raw(copy=False)[index]
-        if self._single_size:
-            voxel_size = self._size.copy()
-        else:
-            voxel_size = self._sizes[index]
-        if voxels.ndim == 1:
-            voxels = voxels.reshape(-1, 3)
-        return VoxelSet(voxels, voxel_size, data)
-
-    def __getattr__(self, key):
-        if key in self._data._keys:
-            return self.get_data(key)
-        else:
-            return super().__getattribute__(key)
-
-    def __str__(self):
-        cls = type(self)
-        obj = f"<{cls.__module__}.{cls.__name__} object at {hex(id(self))}>"
-        if self.is_empty:
-            insert = "[EMPTY] "
-        else:
-            insert = f"with {len(self)} voxels from "
-            insert += f"{tuple(self.bounds[0])} to {tuple(self.bounds[1])}, "
-            if self.regular:
-                insert += f"same size {self.size}, "
-            else:
-                insert += "individual sizes, "
-            if self.has_data:
-                if self._data.keys:
-                    insert += f"with keyed data ({', '.join(self._data.keys)}) "
-                else:
-                    insert += f"with {self._data.shape[-1]} data columns "
-            else:
-                insert += "without data "
-        return obj.replace("at 0x", insert + "at 0x")
-
-    __repr__ = __str__
-
-    def __bool__(self):
-        return not self.is_empty
-
-    @property
-    def is_empty(self):
-        """
-        Whether the set contain any voxels
-
-        :rtype: bool
-        """
-        return not len(self)
-
-    @property
-    def has_data(self):
-        """
-        Whether the set has any data associated to the voxels
-
-        :rtype: bool
-        """
-        return self._data is not None
-
-    @property
-    def regular(self):
-        """
-        Whether the voxels are placed on a regular grid.
-
-        :rtype: bool
-        """
-        return self._regular
-
-    @property
-    def of_equal_size(self):
-        return self._single_size or len(np.unique(self._sizes, axis=0)) < 2
-
-    @property
-    def equilateral(self):
-        """
-        Whether all sides of all voxels have the same lengths.
-
-        :rtype: bool
-        """
-        return np.unique(self.get_size(copy=False)).shape == (1,)
-
-    @property
-    def size(self):
-        """
-        The size of the voxels. When it is 0D or 1D it counts as the size for all voxels,
-        if it is 2D it is 1 an individual size per voxel.
-
-        :rtype: numpy.ndarray
-        """
-        return self.get_size()
-
-    @property
-    def volume(self):
-        if self._single_size:
-            voxel_volume = np.abs(np.prod(self.get_size(copy=False) * np.ones(3)))
-            return voxel_volume * len(self)
-        else:
-            return np.sum(np.abs(np.prod(self.get_size_matrix(copy=False), axis=1)))
-
-    @property
-    def data(self):
-        """
-        The size of the voxels. When it is 0D or 1D it counts as the size for all voxels,
-        if it is 2D it is 1 an individual size per voxel.
-
-        :rtype: Union[numpy.ndarray, None]
-        """
-        return self.get_data()
-
-    @property
-    def data_keys(self):
-        return self._data.keys
-
-    @property
-    def raw(self):
-        return self.get_raw()
-
-    @property
-    @functools.cache
-    def bounds(self):
-        """
-        The minimum and maximum coordinates of this set.
-
-        :rtype: tuple[numpy.ndarray, numpy.ndarray]
-        """
-        if self.is_empty:
-            raise EmptyVoxelSetError("Empty VoxelSet has no bounds.")
-        boxes = self.as_boxes()
-        dims = boxes.shape[1] // 2
-        return (
-            np.min(boxes[:, :dims], axis=0),
-            np.max(boxes[:, dims:], axis=0),
-        )
-
-    @classmethod
-    def empty(cls, size=None):
-        return cls(np.empty((0, 3)), np.empty((0, 3)))
-
-    @classmethod
-    def one(cls, ldc, mdc, data=None):
-        ldc = np.array(ldc, copy=False).reshape(-1)
-        mdc = np.array(mdc, copy=False).reshape(-1)
-        if ldc.shape != (3,) or mdc.shape != (3,):
-            raise ValueError(
-                "Arguments to `VoxelSet.one` should be shape (3,) minmax coords of voxel."
-            )
-        return cls(np.array([ldc]), np.array([mdc - ldc]), np.array([data]))
-
-    @classmethod
-    def concatenate(cls, *sets):
-        # Short circuit "stupid" concat requests
-        if not sets:
-            return cls.empty()
-        elif len(sets) == 1:
-            return sets[0].copy()
-
-        primer = None
-        # Check which sets we are concatenating, maybe we can keep them in reduced data
-        # forms. If they don't line up, we expand and concatenate the expanded forms.
-        if any(
-            # `primer` is assigned the first non-empty set, all sizes must match sizes can
-            # still be 0D, 1D or 2D, but if they're allclose broadcasted it is fine! :)
-            not np.allclose(s.get_size(copy=False), primer.get_size(copy=False))
-            for s in sets
-            if s and (primer := primer or s)
-        ):
-            sizes = primer.get_size()
-            if len(sizes.shape) > 1:
-                # We happened to pick a VoxelSet that has a size matrix of equal sizes,
-                # so we take the opportunity to reduce it.
-                sizes = sizes[0]
-            if len(sizes.shape) > 0 and np.allclose(sizes, sizes[0]):
-                # Voxelset is actually even cubic regular!
-                sizes = sizes[0]
-            if all(s.regular for s in sets):
-                # Index coords with same sizes can simply be stacked
-                voxels = np.concatenate([s.get_raw(copy=False) for s in sets])
-                irregular = False
-            else:
-                voxels = np.concatenate([s.as_spatial_coords(copy=False) for s in sets])
-                irregular = True
-        else:
-            # We can't keep a single size, so expand into a matrix where needed and concat
-            sizes = np.concatenate([s.get_size_matrix(copy=False) for s in sets])
-            voxels = np.concatenate([s.as_spatial_coords(copy=False) for s in sets])
-            irregular = True
-
-        if any(s.has_data for s in sets):
-            fillers = [s.get_data(copy=False) for s in sets]
-            # Find all keys among data to concatenate
-            all_keys = set(itertools.chain(*(f.keys for f in fillers if f is not None)))
-            # Create an index for each key
-            keys = [*sorted(set(all_keys), key=str)]
-            # Allocate enough columns for all keys, or a data array with more unlabelled
-            # columns than that.
-            md = max(len(keys), *(f.shape[1] for f in fillers if f is not None))
-            if not keys:
-                keys = None
-            elif md > len(keys):
-                # Find and pad `keys` with labels for the extra numerical columns.
-                extra = md - len(keys)
-                new_nums = (s for c in itertools.count() if (s := str(c)) not in keys)
-                keys.extend(itertools.islice(new_nums, extra))
-                keys.extend(range(len(keys), md))
-                keys = sorted(keys)
-            ln = [len(s) for s in sets]
-            data = np.empty((sum(ln), md), dtype=object)
-            ptr = 0
-            for len_, fill in zip(ln, fillers):
-                if fill is not None:
-                    if not fill.keys:
-                        cols = slice(None, fill.shape[1])
-                    else:
-                        cols = [keys.index(key) for key in fill.keys]
-                    data[ptr : (ptr + len_), cols] = fill
-                    ptr += len_
-        else:
-            data = None
-            keys = None
-        return VoxelSet(voxels, sizes, data=data, data_keys=keys, irregular=irregular)
-
-    def copy(self):
-        if self.is_empty:
-            return VoxelSet.empty()
-        else:
-            return VoxelSet(
-                self.raw,
-                self.get_size(copy=True),
-                self.get_data(copy=True) if self.has_data else None,
-                irregular=not self.regular,
-            )
-
-    def get_raw(self, copy=True):
-        coords = self._indices if self.regular else self._coords
-        if copy:
-            coords = coords.copy()
-        return coords
-
-    def get_data(self, index=None, /, copy=True):
-        if self.has_data:
-            if index is not None:
-                return self._data[index]
-            else:
-                return self._data.copy()
-        else:
-            return None
-
-    def get_size(self, copy=True):
-        if self._single_size:
-            return np.array(self._size, copy=copy)
-        else:
-            return np.array(self._sizes, copy=copy)
-
-    def get_size_matrix(self, copy=True):
-        if self._single_size:
-            size = np.ones(3) * self._size
-            sizes = np.tile(size, (len(self.get_raw(copy=False)), 1))
-        else:
-            sizes = self._sizes
-            if copy:
-                sizes = sizes.copy()
-        return sizes
-
-    def as_spatial_coords(self, copy=True):
-        if self.regular:
-            coords = self._to_spatial_coords()
-        else:
-            coords = self._coords
-            if copy:
-                coords = coords.copy()
-        return coords
-
-    def as_boxes(self, cache=False):
-        if cache:
-            return self._boxes_cache()
-        else:
-            return self._boxes()
-
-    def as_boxtree(self, cache=False):
-        if cache:
-            return self._boxtree_cache()
-        else:
-            return self._boxtree()
-
-    def snap_to_grid(self, voxel_size, unique=False):
-        if self.regular:
-            grid = self._indices // _squash_zero(voxel_size / _squash_zero(self._size))
-        else:
-            grid = self._coords // _squash_zero(voxel_size)
-        data = self._data
-        if unique:
-            if self.has_data:
-                grid, id = np.unique(grid, return_index=True, axis=0)
-                data = data[id]
-            else:
-                grid = np.unique(grid, axis=0)
-        return VoxelSet(grid, voxel_size, data)
-
-    def resize(self, size):
-        val = np.array(size, copy=False)
-        if val.dtype.name == "object":
-            raise ValueError("Size must be number type")
-        if val.ndim > 1:
-            if len(val) != len(self):
-                raise ValueError("Individual voxel sizes must match amount of voxels.")
-            if self.regular:
-                self._coords = self.as_spatial_coords()
-                del self._indices
-                self._regular = False
-        self._size = size
-
-    def crop(self, ldc, mdc):
-        coords = self.as_spatial_coords(copy=False)
-        inside = np.all(np.logical_and(ldc <= coords, coords < mdc), axis=1)
-        return self[inside]
-
-    def crop_chunk(self, chunk):
-        return self.crop(chunk.ldc, chunk.mdc)
-
-    @classmethod
-    def fill(cls, positions, voxel_size, unique=True):
-        return cls(positions, 0, irregular=True).snap_to_grid(voxel_size, unique=unique)
-
-    def coordinates_of(self, positions):
-        if not self.regular:
-            raise ValueError("Cannot find a unique voxel index in irregular VoxelSet.")
-        return positions // self.get_size()
-
-    def index_of(self, positions):
-        coords = self.coordinates_of(positions)
-        map_ = {tuple(vox_coord): i for i, vox_coord in enumerate(self)}
-        return np.array([map_.get(tuple(coord), np.nan) for coord in coords])
-
-    def inside(self, positions):
-        mask = np.zeros(len(positions), dtype=bool)
-        ldc, mdc = self._box_bounds()
-        for voxel in zip(ldc, mdc):
-            mask |= np.all((positions >= ldc) & (positions < mdc), axis=1)
-        return mask
-
-    def unique(self):
-        raise NotImplementedError("and another one")
-
-    @property
-    def _single_size(self):
-        # One size fits all
-        return hasattr(self, "_size")
-
-    def _to_spatial_coords(self):
-        return self._indices * self._size
-
-    @functools.cache
-    def _boxtree_cache(self):
-        return self._boxtree()
-
-    def _boxtree(self):
-        return BoxTree(self.as_boxes())
-
-    @functools.cache
-    def _boxes_cache(self):
-        return self._boxes()
-
-    def _box_bounds(self):
-        base = self.as_spatial_coords(copy=False)
-        sizes = self.get_size(copy=False)
-        shifted = base + sizes
-        lt0 = sizes < 0
-        if np.any(lt0):
-            mdc = np.where(lt0, base, shifted)
-            ldc = np.where(lt0, shifted, base)
-        else:
-            ldc = base
-            mdc = shifted
-        return ldc, mdc
-
-    def _boxes(self):
-        return np.column_stack(self._box_bounds())
-
-    @classmethod
-    def from_morphology(cls, morphology, estimate_n, with_data=True):
-        meta = morphology.meta
-        if "mdc" in meta and "ldc" in meta:
-            ldc, mdc = meta["ldc"], meta["mdc"]
-        else:
-            ldc, mdc = morphology.bounds
-        # Find a good distribution of amount of voxels per side
-        size = mdc - ldc
-        per_side = _eq_sides(size, estimate_n)
-        voxel_size = size / per_side
-        _squash_temp = _squash_zero(voxel_size)
-        branch_vcs = [b.points // _squash_temp for b in morphology.branches]
-        if with_data:
-            voxel_reduce = {}
-            for branch, point_vcs in enumerate(branch_vcs):
-                for point, point_vc in enumerate(point_vcs):
-                    voxel_reduce.setdefault(tuple(point_vc), []).append((branch, point))
-            voxels = np.array(tuple(voxel_reduce.keys()))
-            # Transfer the voxel data into an object array
-            voxel_data_data = tuple(voxel_reduce.values())
-            # We need a bit of a workaround so that numpy doesn't make a regular from the
-            # `voxel_data_data` list of lists, when it has a matrix shape.
-            voxel_data = np.empty(len(voxel_data_data), dtype=object)
-            for i in range(len(voxel_data_data)):
-                voxel_data[i] = voxel_data_data[i]
-            return cls(voxels, voxel_size, data=voxel_data)
-        else:
-            voxels = np.unique(np.concatenate(branch_vcs), axis=0)
-            return cls(voxels, voxel_size)
-
-
-def _eq_sides(sides, n):
-    zeros = np.isclose(sides, 0)
-    if all(zeros):
-        # An empty or 1 point morphology should make only 1 (empty) voxel
-        return np.array([1])
-    elif any(zeros):
-        # Remove any zeros, by fixing their dimensions to 1 (zero-width) partition
-        solution = np.ones(len(sides))
-        solution[~zeros] = _eq_sides(sides[~zeros], n)
-        return solution
-    elif len(sides) == 1:
-        # Only 1 dimension, only 1 solution: all voxels along that dimension.
-        return np.array([n])
-
-    # Use the relative magnitudes of each side
-    norm = sides / max(sides)
-    # Find out how many divisions each side should to form a grid with `n` rhomboids.
-    per_side = norm * (n / np.prod(norm)) ** (1 / len(sides))
-    # Divisions should be integers, and minimum 1
-    solution = np.maximum(np.floor(per_side), 1)
-    order = np.argsort(sides)
-    smallest = order[0]
-    if len(sides) > 2:
-        # Because of the integer rounding the product isn't necesarily optimal, so we keep
-        # the safest (smallest) value, and solve the problem again in 1 less dimension.
-        solved = solution[smallest]
-        look_for = n / solved
-        others = sides[order[1:]]
-        solution[order[1:]] = _eq_sides(others, look_for)
-    else:
-        # In the final 2-dimensional case the remainder of the division is rounded off
-        # to the nearest integer, giving the smallest error on the product and final
-        # number of rhomboids in the grid.
-        largest = order[1]
-        solution[largest] = round(n / solution[smallest])
-    return solution
-
-
-# https://stackoverflow.com/a/24769712/1016004
-def _is_broadcastable(shape1, shape2):
-    for a, b in zip(shape1[::-1], shape2[::-1]):
-        if a == 1 or b == 1 or a == b:
-            pass
-        else:
-            return False
-    return True
-
-
-def _squash_zero(arr):
-    return np.where(np.isclose(arr, 0), np.finfo(float).max, arr)
-
-
-__all__ = ["BoxTree", "VoxelData", "VoxelSet"]
+import functools
+import itertools
+
+import numpy as np
+
+from .exceptions import EmptyVoxelSetError
+from .trees import BoxTree
+
+
+class VoxelData(np.ndarray):
+    """
+    Chunk identifier, consisting of chunk coordinates and size.
+    """
+
+    def __new__(cls, data, keys=None):
+        if data.ndim < 2:
+            return super().__new__(np.ndarray, data.shape, dtype=object)
+        obj = super().__new__(cls, data.shape, dtype=object)
+        obj[:] = data
+        if keys is not None:
+            keys = [str(k) for k in keys]
+            if len(set(keys)) != len(keys):
+                raise ValueError("Data keys must be unique")
+            if len(keys) != data.shape[1]:
+                raise ValueError("Amount of data keys must match amount of data columns")
+            obj._keys = keys
+        else:
+            obj._keys = []
+        return obj
+
+    def __getitem__(self, index):
+        index, keys = self._rewrite_index(index)
+        vd = super().__getitem__(index)
+        if isinstance(vd, VoxelData):
+            if len(keys) > 0 and len(vd) != vd.size / len(keys):
+                vd = vd.reshape(-1, len(keys))
+            vd._keys = keys
+        return vd
+
+    def __array_finalize__(self, obj):
+        if obj is not None:
+            self._keys = []
+
+    @property
+    def keys(self):
+        """
+        Returns the keys, or column labels, associated to each data column.
+        """
+        return self._keys.copy()
+
+    def copy(self):
+        """
+        Return a new copy of the voxel data
+        """
+        new = super().copy()
+        new._keys = self._keys.copy()
+        return new
+
+    def _split_index(self, index):
+        try:
+            if isinstance(index, tuple):
+                cols = [self._keys.index(idx) for idx in index if isinstance(idx, str)]
+                keys = [self._keys[c] for c in cols]
+                index = tuple(idx for idx in index if not isinstance(idx, str))
+            elif isinstance(index, str):
+                cols = [self._keys.index(index)]
+                keys = [self._keys[cols[0]]]
+                index = (slice(None),)
+            else:
+                index = (index,)
+                cols = None
+                keys = getattr(self, "_keys", [])
+        except ValueError as e:
+            key = str(e).split("'")[1]
+            raise IndexError(f"Voxel data key '{key}' does not exist.") from None
+        return index, cols, keys
+
+    def _rewrite_index(self, index):
+        index, cols, keys = self._split_index(index)
+        if cols:
+            return (*index, cols), keys
+        else:
+            return index, keys
+
+
+class VoxelSet:
+    def __init__(self, voxels, size, data=None, data_keys=None, irregular=False):
+        """
+        Constructs a voxel set from the given voxel indices or coordinates.
+
+        :param voxels: The spatial coordinates, or grid indices of each voxel. Nx3 matrix
+        :type voxels: numpy.ndarray
+        :param size: The global or individual size of the voxels. If it has 2 dimensions
+          it needs to have the same length as `voxels`, and will be used as individual
+          voxels.
+        :type size: numpy.ndarray
+        :param data:
+
+        .. warning::
+
+            If :class:`numpy.ndarray` are passed, they will not be copied in order to save
+            memory and time. You may accidentally change a voxelset if you later change
+            the same array.
+        """
+        voxels = np.array(voxels, copy=False)
+        voxel_size = np.array(size, copy=False)
+        if voxels.dtype.name == "object":
+            raise ValueError("Couldn't convert given `voxels` to a voxel matrix")
+        if voxels.ndim != 2:
+            if not len(voxels):
+                # Try some massaging in case of empty arrays
+                voxels = voxels.reshape(-1, 3)
+                if not len(voxel_size):
+                    voxel_size = voxel_size.reshape(-1, 3)
+            else:
+                raise ValueError("`voxels` needs to be convertable to a 2D matrix")
+        if voxels.ndim == 2 and voxels.shape[1] != 3:
+            raise ValueError("`voxels` needs to have 3 columns, 1 for each spatial dim.")
+        if not _is_broadcastable(voxels.shape, voxel_size.shape):
+            raise ValueError(
+                f"Shape {voxel_size.shape} of `size` is"
+                + f" invalid for voxel shape {voxels.shape}"
+            )
+        if data is not None:
+            if isinstance(data, VoxelData):
+                if data_keys is None:
+                    self._data = data
+                else:
+                    self._data = VoxelData(data, keys=data_keys)
+            else:
+                data = np.array(data, copy=False)
+                if data.ndim < 2:
+                    cols = len(data_keys) if data_keys else 1
+                    data = data.reshape(-1, cols)
+                self._data = VoxelData(data, keys=data_keys)
+            if len(self._data) != len(voxels):
+                raise ValueError("`voxels` and `data` length unequal.")
+        else:
+            self._data = None
+
+        if not len(voxel_size.shape):
+            self._cubic = True
+        if voxel_size.ndim > 1:
+            if voxel_size.size != voxels.size:
+                raise ValueError("`voxels` and `size` length unequal.")
+            # Voxels given in spatial coords with individual size
+            self._sizes = voxel_size
+            self._coords = voxels
+            self._regular = False
+        elif irregular:
+            # Voxels given in spatial coords but of equal size
+            self._size = voxel_size
+            self._coords = voxels
+            self._regular = False
+        else:
+            # Voxels given in index coords
+            self._size = voxel_size
+            self._indices = np.array(voxels, copy=False, dtype=int)
+            self._regular = True
+
+    def __iter__(self):
+        return iter(self.get_raw(copy=False))
+
+    def __array__(self):
+        return self.get_raw(copy=False)
+
+    def __len__(self):
+        return len(self.get_raw(copy=False))
+
+    def __getitem__(self, index):
+        if self.has_data:
+            data = self._data[index]
+            index, _, _ = self._data._split_index(index)
+        else:
+            data, _ = None, None
+        if isinstance(index, tuple) and len(index) > 1:
+            raise IndexError("Too many indices for VoxelSet, maximum 1.")
+        voxels = self.get_raw(copy=False)[index]
+        if self._single_size:
+            voxel_size = self._size.copy()
+        else:
+            voxel_size = self._sizes[index]
+        if voxels.ndim == 1:
+            voxels = voxels.reshape(-1, 3)
+        return VoxelSet(voxels, voxel_size, data)
+
+    def __getattr__(self, key):
+        if key in self._data._keys:
+            return self.get_data(key)
+        else:
+            return super().__getattribute__(key)
+
+    def __str__(self):
+        cls = type(self)
+        obj = f"<{cls.__module__}.{cls.__name__} object at {hex(id(self))}>"
+        if self.is_empty:
+            insert = "[EMPTY] "
+        else:
+            insert = f"with {len(self)} voxels from "
+            insert += f"{tuple(self.bounds[0])} to {tuple(self.bounds[1])}, "
+            if self.regular:
+                insert += f"same size {self.size}, "
+            else:
+                insert += "individual sizes, "
+            if self.has_data:
+                if self._data.keys:
+                    insert += f"with keyed data ({', '.join(self._data.keys)}) "
+                else:
+                    insert += f"with {self._data.shape[-1]} data columns "
+            else:
+                insert += "without data "
+        return obj.replace("at 0x", insert + "at 0x")
+
+    __repr__ = __str__
+
+    def __bool__(self):
+        return not self.is_empty
+
+    @property
+    def is_empty(self):
+        """
+        Whether the set contain any voxels
+
+        :rtype: bool
+        """
+        return not len(self)
+
+    @property
+    def has_data(self):
+        """
+        Whether the set has any data associated to the voxels
+
+        :rtype: bool
+        """
+        return self._data is not None
+
+    @property
+    def regular(self):
+        """
+        Whether the voxels are placed on a regular grid.
+
+        :rtype: bool
+        """
+        return self._regular
+
+    @property
+    def of_equal_size(self):
+        return self._single_size or len(np.unique(self._sizes, axis=0)) < 2
+
+    @property
+    def equilateral(self):
+        """
+        Whether all sides of all voxels have the same lengths.
+
+        :rtype: bool
+        """
+        return np.unique(self.get_size(copy=False)).shape == (1,)
+
+    @property
+    def size(self):
+        """
+        The size of the voxels. When it is 0D or 1D it counts as the size for all voxels,
+        if it is 2D it is 1 an individual size per voxel.
+
+        :rtype: numpy.ndarray
+        """
+        return self.get_size()
+
+    @property
+    def volume(self):
+        if self._single_size:
+            voxel_volume = np.abs(np.prod(self.get_size(copy=False) * np.ones(3)))
+            return voxel_volume * len(self)
+        else:
+            return np.sum(np.abs(np.prod(self.get_size_matrix(copy=False), axis=1)))
+
+    @property
+    def data(self):
+        """
+        The size of the voxels. When it is 0D or 1D it counts as the size for all voxels,
+        if it is 2D it is 1 an individual size per voxel.
+
+        :rtype: Union[numpy.ndarray, None]
+        """
+        return self.get_data()
+
+    @property
+    def data_keys(self):
+        return self._data.keys
+
+    @property
+    def raw(self):
+        return self.get_raw()
+
+    @property
+    @functools.cache
+    def bounds(self):
+        """
+        The minimum and maximum coordinates of this set.
+
+        :rtype: tuple[numpy.ndarray, numpy.ndarray]
+        """
+        if self.is_empty:
+            raise EmptyVoxelSetError("Empty VoxelSet has no bounds.")
+        boxes = self.as_boxes()
+        dims = boxes.shape[1] // 2
+        return (
+            np.min(boxes[:, :dims], axis=0),
+            np.max(boxes[:, dims:], axis=0),
+        )
+
+    @classmethod
+    def empty(cls, size=None):
+        return cls(np.empty((0, 3)), np.empty((0, 3)))
+
+    @classmethod
+    def one(cls, ldc, mdc, data=None):
+        ldc = np.array(ldc, copy=False).reshape(-1)
+        mdc = np.array(mdc, copy=False).reshape(-1)
+        if ldc.shape != (3,) or mdc.shape != (3,):
+            raise ValueError(
+                "Arguments to `VoxelSet.one` should be shape (3,) minmax coords of voxel."
+            )
+        return cls(np.array([ldc]), np.array([mdc - ldc]), np.array([data]))
+
+    @classmethod
+    def concatenate(cls, *sets):
+        # Short circuit "stupid" concat requests
+        if not sets:
+            return cls.empty()
+        elif len(sets) == 1:
+            return sets[0].copy()
+
+        primer = None
+        # Check which sets we are concatenating, maybe we can keep them in reduced data
+        # forms. If they don't line up, we expand and concatenate the expanded forms.
+        if any(
+            # `primer` is assigned the first non-empty set, all sizes must match sizes can
+            # still be 0D, 1D or 2D, but if they're allclose broadcasted it is fine! :)
+            not np.allclose(s.get_size(copy=False), primer.get_size(copy=False))
+            for s in sets
+            if s and (primer := primer or s)
+        ):
+            sizes = primer.get_size()
+            if len(sizes.shape) > 1:
+                # We happened to pick a VoxelSet that has a size matrix of equal sizes,
+                # so we take the opportunity to reduce it.
+                sizes = sizes[0]
+            if len(sizes.shape) > 0 and np.allclose(sizes, sizes[0]):
+                # Voxelset is actually even cubic regular!
+                sizes = sizes[0]
+            if all(s.regular for s in sets):
+                # Index coords with same sizes can simply be stacked
+                voxels = np.concatenate([s.get_raw(copy=False) for s in sets])
+                irregular = False
+            else:
+                voxels = np.concatenate([s.as_spatial_coords(copy=False) for s in sets])
+                irregular = True
+        else:
+            # We can't keep a single size, so expand into a matrix where needed and concat
+            sizes = np.concatenate([s.get_size_matrix(copy=False) for s in sets])
+            voxels = np.concatenate([s.as_spatial_coords(copy=False) for s in sets])
+            irregular = True
+
+        if any(s.has_data for s in sets):
+            fillers = [s.get_data(copy=False) for s in sets]
+            # Find all keys among data to concatenate
+            all_keys = set(itertools.chain(*(f.keys for f in fillers if f is not None)))
+            # Create an index for each key
+            keys = [*sorted(set(all_keys), key=str)]
+            # Allocate enough columns for all keys, or a data array with more unlabelled
+            # columns than that.
+            md = max(len(keys), *(f.shape[1] for f in fillers if f is not None))
+            if not keys:
+                keys = None
+            elif md > len(keys):
+                # Find and pad `keys` with labels for the extra numerical columns.
+                extra = md - len(keys)
+                new_nums = (s for c in itertools.count() if (s := str(c)) not in keys)
+                keys.extend(itertools.islice(new_nums, extra))
+                keys.extend(range(len(keys), md))
+                keys = sorted(keys)
+            ln = [len(s) for s in sets]
+            data = np.empty((sum(ln), md), dtype=object)
+            ptr = 0
+            for len_, fill in zip(ln, fillers):
+                if fill is not None:
+                    if not fill.keys:
+                        cols = slice(None, fill.shape[1])
+                    else:
+                        cols = [keys.index(key) for key in fill.keys]
+                    data[ptr : (ptr + len_), cols] = fill
+                    ptr += len_
+        else:
+            data = None
+            keys = None
+        return VoxelSet(voxels, sizes, data=data, data_keys=keys, irregular=irregular)
+
+    def copy(self):
+        if self.is_empty:
+            return VoxelSet.empty()
+        else:
+            return VoxelSet(
+                self.raw,
+                self.get_size(copy=True),
+                self.get_data(copy=True) if self.has_data else None,
+                irregular=not self.regular,
+            )
+
+    def get_raw(self, copy=True):
+        coords = self._indices if self.regular else self._coords
+        if copy:
+            coords = coords.copy()
+        return coords
+
+    def get_data(self, index=None, /, copy=True):
+        if self.has_data:
+            if index is not None:
+                return self._data[index]
+            else:
+                return self._data.copy()
+        else:
+            return None
+
+    def get_size(self, copy=True):
+        if self._single_size:
+            return np.array(self._size, copy=copy)
+        else:
+            return np.array(self._sizes, copy=copy)
+
+    def get_size_matrix(self, copy=True):
+        if self._single_size:
+            size = np.ones(3) * self._size
+            sizes = np.tile(size, (len(self.get_raw(copy=False)), 1))
+        else:
+            sizes = self._sizes
+            if copy:
+                sizes = sizes.copy()
+        return sizes
+
+    def as_spatial_coords(self, copy=True):
+        if self.regular:
+            coords = self._to_spatial_coords()
+        else:
+            coords = self._coords
+            if copy:
+                coords = coords.copy()
+        return coords
+
+    def as_boxes(self, cache=False):
+        if cache:
+            return self._boxes_cache()
+        else:
+            return self._boxes()
+
+    def as_boxtree(self, cache=False):
+        if cache:
+            return self._boxtree_cache()
+        else:
+            return self._boxtree()
+
+    def snap_to_grid(self, voxel_size, unique=False):
+        if self.regular:
+            grid = self._indices // _squash_zero(voxel_size / _squash_zero(self._size))
+        else:
+            grid = self._coords // _squash_zero(voxel_size)
+        data = self._data
+        if unique:
+            if self.has_data:
+                grid, id = np.unique(grid, return_index=True, axis=0)
+                data = data[id]
+            else:
+                grid = np.unique(grid, axis=0)
+        return VoxelSet(grid, voxel_size, data)
+
+    def resize(self, size):
+        val = np.array(size, copy=False)
+        if val.dtype.name == "object":
+            raise ValueError("Size must be number type")
+        if val.ndim > 1:
+            if len(val) != len(self):
+                raise ValueError("Individual voxel sizes must match amount of voxels.")
+            if self.regular:
+                self._coords = self.as_spatial_coords()
+                del self._indices
+                self._regular = False
+        self._size = size
+
+    def crop(self, ldc, mdc):
+        coords = self.as_spatial_coords(copy=False)
+        inside = np.all(np.logical_and(ldc <= coords, coords < mdc), axis=1)
+        return self[inside]
+
+    def crop_chunk(self, chunk):
+        return self.crop(chunk.ldc, chunk.mdc)
+
+    @classmethod
+    def fill(cls, positions, voxel_size, unique=True):
+        return cls(positions, 0, irregular=True).snap_to_grid(voxel_size, unique=unique)
+
+    def coordinates_of(self, positions):
+        if not self.regular:
+            raise ValueError("Cannot find a unique voxel index in irregular VoxelSet.")
+        return positions // self.get_size()
+
+    def index_of(self, positions):
+        coords = self.coordinates_of(positions)
+        map_ = {tuple(vox_coord): i for i, vox_coord in enumerate(self)}
+        return np.array([map_.get(tuple(coord), np.nan) for coord in coords])
+
+    def inside(self, positions):
+        mask = np.zeros(len(positions), dtype=bool)
+        ldc, mdc = self._box_bounds()
+        for voxel in zip(ldc, mdc):
+            mask |= np.all((positions >= ldc) & (positions < mdc), axis=1)
+        return mask
+
+    def unique(self):
+        raise NotImplementedError("and another one")
+
+    @property
+    def _single_size(self):
+        # One size fits all
+        return hasattr(self, "_size")
+
+    def _to_spatial_coords(self):
+        return self._indices * self._size
+
+    @functools.cache
+    def _boxtree_cache(self):
+        return self._boxtree()
+
+    def _boxtree(self):
+        return BoxTree(self.as_boxes())
+
+    @functools.cache
+    def _boxes_cache(self):
+        return self._boxes()
+
+    def _box_bounds(self):
+        base = self.as_spatial_coords(copy=False)
+        sizes = self.get_size(copy=False)
+        shifted = base + sizes
+        lt0 = sizes < 0
+        if np.any(lt0):
+            mdc = np.where(lt0, base, shifted)
+            ldc = np.where(lt0, shifted, base)
+        else:
+            ldc = base
+            mdc = shifted
+        return ldc, mdc
+
+    def _boxes(self):
+        return np.column_stack(self._box_bounds())
+
+    @classmethod
+    def from_morphology(cls, morphology, estimate_n, with_data=True):
+        meta = morphology.meta
+        if "mdc" in meta and "ldc" in meta:
+            ldc, mdc = meta["ldc"], meta["mdc"]
+        else:
+            ldc, mdc = morphology.bounds
+        # Find a good distribution of amount of voxels per side
+        size = mdc - ldc
+        per_side = _eq_sides(size, estimate_n)
+        voxel_size = size / per_side
+        _squash_temp = _squash_zero(voxel_size)
+        branch_vcs = [b.points // _squash_temp for b in morphology.branches]
+        if with_data:
+            voxel_reduce = {}
+            for branch, point_vcs in enumerate(branch_vcs):
+                for point, point_vc in enumerate(point_vcs):
+                    voxel_reduce.setdefault(tuple(point_vc), []).append((branch, point))
+            voxels = np.array(tuple(voxel_reduce.keys()))
+            # Transfer the voxel data into an object array
+            voxel_data_data = tuple(voxel_reduce.values())
+            # We need a bit of a workaround so that numpy doesn't make a regular from the
+            # `voxel_data_data` list of lists, when it has a matrix shape.
+            voxel_data = np.empty(len(voxel_data_data), dtype=object)
+            for i in range(len(voxel_data_data)):
+                voxel_data[i] = voxel_data_data[i]
+            return cls(voxels, voxel_size, data=voxel_data)
+        else:
+            voxels = np.unique(np.concatenate(branch_vcs), axis=0)
+            return cls(voxels, voxel_size)
+
+
+def _eq_sides(sides, n):
+    zeros = np.isclose(sides, 0)
+    if all(zeros):
+        # An empty or 1 point morphology should make only 1 (empty) voxel
+        return np.array([1])
+    elif any(zeros):
+        # Remove any zeros, by fixing their dimensions to 1 (zero-width) partition
+        solution = np.ones(len(sides))
+        solution[~zeros] = _eq_sides(sides[~zeros], n)
+        return solution
+    elif len(sides) == 1:
+        # Only 1 dimension, only 1 solution: all voxels along that dimension.
+        return np.array([n])
+
+    # Use the relative magnitudes of each side
+    norm = sides / max(sides)
+    # Find out how many divisions each side should to form a grid with `n` rhomboids.
+    per_side = norm * (n / np.prod(norm)) ** (1 / len(sides))
+    # Divisions should be integers, and minimum 1
+    solution = np.maximum(np.floor(per_side), 1)
+    order = np.argsort(sides)
+    smallest = order[0]
+    if len(sides) > 2:
+        # Because of the integer rounding the product isn't necesarily optimal, so we keep
+        # the safest (smallest) value, and solve the problem again in 1 less dimension.
+        solved = solution[smallest]
+        look_for = n / solved
+        others = sides[order[1:]]
+        solution[order[1:]] = _eq_sides(others, look_for)
+    else:
+        # In the final 2-dimensional case the remainder of the division is rounded off
+        # to the nearest integer, giving the smallest error on the product and final
+        # number of rhomboids in the grid.
+        largest = order[1]
+        solution[largest] = round(n / solution[smallest])
+    return solution
+
+
+# https://stackoverflow.com/a/24769712/1016004
+def _is_broadcastable(shape1, shape2):
+    for a, b in zip(shape1[::-1], shape2[::-1]):
+        if a == 1 or b == 1 or a == b:
+            pass
+        else:
+            return False
+    return True
+
+
+def _squash_zero(arr):
+    return np.where(np.isclose(arr, 0), np.finfo(float).max, arr)
+
+
+__all__ = ["VoxelData", "VoxelSet"]
```

### Comparing `bsb_core-4.0.1/pyproject.toml` & `bsb_core-4.1.0/pyproject.toml`

 * *Files 0% similar despite different names*

```diff
@@ -96,15 +96,15 @@
 [tool.black]
 line-length = 90
 
 [tool.isort]
 profile = "black"
 
 [tool.bumpversion]
-current_version = "4.0.1"
+current_version = "4.1.0"
 parse = "(?P<major>\\d+)\\.(?P<minor>\\d+)\\.(?P<patch>\\d+)"
 serialize = ["{major}.{minor}.{patch}"]
 search = "{current_version}"
 replace = "{new_version}"
 regex = false
 ignore_missing_version = false
 tag = true
```

### Comparing `bsb_core-4.0.1/PKG-INFO` & `bsb_core-4.1.0/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: bsb-core
-Version: 4.0.1
+Version: 4.1.0
 Summary: `bsb-core` is the backbone package contain the essential code of the BSB: A component
 Author-email: Robin De Schepper <robingilbert.deschepper@unipv.it>
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 Classifier: License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)
 Requires-Dist: numpy~=1.19
 Requires-Dist: scipy~=1.5
@@ -55,16 +55,16 @@
 Provides-Extra: dev
 Provides-Extra: docs
 Provides-Extra: parallel
 Provides-Extra: test
 
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
 [![Documentation Status](https://readthedocs.org/projects/bsb/badge/?version=latest)](https://bsb.readthedocs.io/en/latest/?badge=latest)
-[![Build Status](https://travis-ci.com/dbbs-lab/bsb.svg?branch=master)](https://travis-ci.com/dbbs-lab/bsb)
-[![codecov](https://codecov.io/gh/dbbs-lab/bsb/branch/master/graph/badge.svg)](https://codecov.io/gh/dbbs-lab/bsb)
+[![Build Status](https://travis-ci.com/dbbs-lab/bsb-core.svg?branch=main)](https://travis-ci.com/dbbs-lab/bsb-core)
+[![codecov](https://codecov.io/gh/dbbs-lab/bsb-core/branch/main/graph/badge.svg)](https://codecov.io/gh/dbbs-lab/bsb-core)
 
 <h3>:closed_book: Read the documentation on https://bsb.readthedocs.io/en/latest</h3>
 
 # BSB: A component framework for neural modelling
 
 Developed by the Department of Brain and Behavioral Sciences at the University of Pavia,
 the BSB is a component framework for neural modelling, which focusses on component
@@ -80,22 +80,22 @@
 
 ### pip
 
 Any package in the BSB ecosystem can be installed from PyPI through `pip`. Most users
 will want to install the main [bsb](https://pypi.org/project/bsb/) framework:
 
 ```
-pip install "bsb~=4.0"
+pip install "bsb~=4.1"
 ```
 
 Advanced users looking to control install an unconventional combination of plugins might
 be better of installing just this package, and the desired plugins:
 
 ```
-pip install "bsb-core~=4.0"
+pip install "bsb-core~=4.1"
 ```
 
 Note that installing `bsb-core` does not come with any plugins installed and the usually
 available storage engines, or configuration parsers will be missing.
 
 ### Developers
```

