# Comparing `tmp/alphafold3_pytorch-0.0.7.tar.gz` & `tmp/alphafold3_pytorch-0.0.8.tar.gz`

## Comparing `alphafold3_pytorch-0.0.7.tar` & `alphafold3_pytorch-0.0.8.tar`

### file list

```diff
@@ -1,16 +1,16 @@
--rw-r--r--   0        0        0       15 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/.env.sample
--rw-r--r--   0        0        0   248685 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/alphafold3.png
--rw-r--r--   0        0        0       59 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/contribute.sh
--rw-r--r--   0        0        0     1060 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/.github/workflows/publish.yml
--rw-r--r--   0        0        0      425 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/.github/workflows/test.yml
--rw-r--r--   0        0        0     1322 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/alphafold3_pytorch/__init__.py
--rw-r--r--   0        0        0    79684 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/alphafold3_pytorch/alphafold3.py
--rw-r--r--   0        0        0     9317 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/alphafold3_pytorch/attention.py
--rw-r--r--   0        0        0      951 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/alphafold3_pytorch/typing.py
--rw-r--r--   0        0        0   584890 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/docs/alphafold3-supplementary.pdf
--rw-r--r--   0        0        0    11454 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/tests/test_af3.py
--rw-r--r--   0        0        0     3084 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/.gitignore
--rw-r--r--   0        0        0     1066 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/LICENSE
--rw-r--r--   0        0        0     4417 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/README.md
--rw-r--r--   0        0        0     1304 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/pyproject.toml
--rw-r--r--   0        0        0     6624 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.7/PKG-INFO
+-rw-r--r--   0        0        0       15 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/.env.sample
+-rw-r--r--   0        0        0   248685 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/alphafold3.png
+-rw-r--r--   0        0        0       59 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/contribute.sh
+-rw-r--r--   0        0        0     1060 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/.github/workflows/publish.yml
+-rw-r--r--   0        0        0      425 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/.github/workflows/test.yml
+-rw-r--r--   0        0        0     1322 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/alphafold3_pytorch/__init__.py
+-rw-r--r--   0        0        0    80699 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/alphafold3_pytorch/alphafold3.py
+-rw-r--r--   0        0        0     9317 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/alphafold3_pytorch/attention.py
+-rw-r--r--   0        0        0      951 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/alphafold3_pytorch/typing.py
+-rw-r--r--   0        0        0   584890 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/docs/alphafold3-supplementary.pdf
+-rw-r--r--   0        0        0    11484 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/tests/test_af3.py
+-rw-r--r--   0        0        0     3084 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/.gitignore
+-rw-r--r--   0        0        0     1066 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/LICENSE
+-rw-r--r--   0        0        0     4417 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/README.md
+-rw-r--r--   0        0        0     1304 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/pyproject.toml
+-rw-r--r--   0        0        0     6624 2020-02-02 00:00:00.000000 alphafold3_pytorch-0.0.8/PKG-INFO
```

### Comparing `alphafold3_pytorch-0.0.7/alphafold3.png` & `alphafold3_pytorch-0.0.8/alphafold3.png`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/.github/workflows/publish.yml` & `alphafold3_pytorch-0.0.8/.github/workflows/publish.yml`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/alphafold3_pytorch/__init__.py` & `alphafold3_pytorch-0.0.8/alphafold3_pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/alphafold3_pytorch/alphafold3.py` & `alphafold3_pytorch-0.0.8/alphafold3_pytorch/alphafold3.py`

 * *Files 2% similar despite different names*

```diff
@@ -1165,15 +1165,16 @@
         *,
         depth,
         heads,
         dim = 384,
         dim_single_cond = None,
         dim_pairwise = 128,
         attn_window_size = None,
-        attn_pair_bias_kwargs: dict = dict()
+        attn_pair_bias_kwargs: dict = dict(),
+        serial = False
     ):
         super().__init__()
         dim_single_cond = default(dim_single_cond, dim)
 
         layers = ModuleList([])
 
         for _ in range(depth):
@@ -1205,40 +1206,48 @@
             layers.append(ModuleList([
                 conditionable_pair_bias,
                 conditionable_transition
             ]))
 
         self.layers = layers
 
+        self.serial = serial
+
     @typecheck
     def forward(
         self,
         noised_repr: Float['b n d'],
         *,
         single_repr: Float['b n ds'],
         pairwise_repr: Float['b n n dp'],
         mask: Bool['b n'] | None = None
     ):
+        serial = self.serial
+
         for attn, transition in self.layers:
 
             attn_out = attn(
                 noised_repr,
                 cond = single_repr,
                 pairwise_repr = pairwise_repr,
                 mask = mask
             )
 
+            if serial:
+                noised_repr = attn_out + noised_repr
+
             ff_out = transition(
                 noised_repr,
                 cond = single_repr
             )
 
-            # interesting, they use parallel attention and feedforward modules
+            if not serial:
+                ff_out = ff_out + attn_out
 
-            noised_repr = noised_repr + attn_out + ff_out
+            noised_repr = noised_repr + ff_out
 
         return noised_repr
 
 class AtomToTokenPooler(Module):
     def __init__(
         self,
         dim,
@@ -1307,15 +1316,16 @@
             num_transitions = 2
         ),
         atom_encoder_depth = 3,
         atom_encoder_heads = 4,
         token_transformer_depth = 24,
         token_transformer_heads = 16,
         atom_decoder_depth = 3,
-        atom_decoder_heads = 4
+        atom_decoder_heads = 4,
+        serial = False
     ):
         super().__init__()
 
         self.atoms_per_window = atoms_per_window
 
         # conditioning
 
@@ -1362,15 +1372,16 @@
 
         self.atom_encoder = DiffusionTransformer(
             dim = dim_atom,
             dim_single_cond = dim_atom,
             dim_pairwise = dim_atompair,
             attn_window_size = atoms_per_window,
             depth = atom_encoder_depth,
-            heads = atom_encoder_heads
+            heads = atom_encoder_heads,
+            serial = serial
         )
 
         self.atom_feats_to_pooled_token = AtomToTokenPooler(
             dim = dim_atom,
             dim_out = dim_token,
             atoms_per_window = atoms_per_window
         )
@@ -1383,30 +1394,32 @@
         )
 
         self.token_transformer = DiffusionTransformer(
             dim = dim_token,
             dim_single_cond = dim_single,
             dim_pairwise = dim_pairwise,
             depth = token_transformer_depth,
-            heads = token_transformer_heads
+            heads = token_transformer_heads,
+            serial = serial
         )
 
         self.attended_token_norm = nn.LayerNorm(dim_token)
 
         # atom attention decoding related modules
 
         self.tokens_to_atom_decoder_input_cond = LinearNoBias(dim_token, dim_atom)
 
         self.atom_decoder = DiffusionTransformer(
             dim = dim_atom,
             dim_single_cond = dim_atom,
             dim_pairwise = dim_atompair,
             attn_window_size = atoms_per_window,
             depth = atom_decoder_depth,
-            heads = atom_decoder_heads
+            heads = atom_decoder_heads,
+            serial = serial
         )
 
         self.atom_feat_to_atom_pos_update = nn.Sequential(
             nn.LayerNorm(dim_atom),
             LinearNoBias(dim_atom, 3)
         )
 
@@ -1698,61 +1711,76 @@
     def forward(
         self,
         normalized_atom_pos: Float['b m 3'],
         atom_mask: Bool['b m'],
         return_denoised_pos = False,
         additional_residue_feats: Float['b n rf'] | None = None,
         add_smooth_lddt_loss = False,
+        add_bond_loss = False,
         **network_condition_kwargs
     ) -> Float[''] | Tuple[Float[''], Float['b m 3']]:
 
         batch_size = normalized_atom_pos.shape[0]
 
         sigmas = self.noise_distribution(batch_size)
         padded_sigmas = rearrange(sigmas, 'b -> b 1 1')
 
         noise = torch.randn_like(normalized_atom_pos)
 
         noised_atom_pos = normalized_atom_pos + padded_sigmas * noise  # alphas are 1. in the paper
 
         network_condition_kwargs.update(atom_mask = atom_mask)
 
-        denoised = self.preconditioned_network_forward(
+        denoised_atom_pos = self.preconditioned_network_forward(
             noised_atom_pos,
             sigmas,
             network_condition_kwargs = network_condition_kwargs
         )
 
-        losses = F.mse_loss(denoised, normalized_atom_pos, reduction = 'none')
-        losses = reduce(losses, 'b ... -> b', 'mean')
+        # main diffusion mse loss
+
+        losses = F.mse_loss(denoised_atom_pos, normalized_atom_pos, reduction = 'none')
+
+        losses = losses * self.loss_weight(padded_sigmas)
+
+        loss = losses[atom_mask].mean()
+
+        # proposed extra bond loss during finetuning
 
-        losses = losses * self.loss_weight(sigmas)
+        if add_bond_loss:
+            denoised_cdist = torch.cdist(denoised_atom_pos, denoised_atom_pos, p = 2)
+            normalized_cdist = torch.cdist(normalized_atom_pos, normalized_atom_pos, p = 2)
 
-        loss = losses.mean()
+            bond_losses = F.mse_loss(denoised_cdist, normalized_cdist, reduction = 'none')
+            atompair_mask = einx.logical_and('b i, b j -> b i j', atom_mask, atom_mask)
+
+            loss = loss + bond_losses[atompair_mask].mean()
+
+        # proposed auxiliary smooth lddt loss
 
         if add_smooth_lddt_loss:
             assert exists(additional_residue_feats)
             w = self.net.atoms_per_window
 
             is_dna, is_rna = additional_residue_feats[..., 7], additional_residue_feats[..., 8]
             atom_is_dna, atom_is_rna = tuple(repeat(t, 'b n -> b (n w)', w = w) for t in (is_dna, is_rna))
 
             smooth_lddt_loss = calc_smooth_lddt_loss(
-                denoised,
+                denoised_atom_pos,
                 normalized_atom_pos,
                 atom_is_dna,
                 atom_is_rna
             ).mean()
 
             loss = loss + smooth_lddt_loss
 
         if not return_denoised_pos:
             return loss
 
-        return loss, denoised
+        return loss, denoised_atom_pos
 
 
 # modules todo
 
 class SmoothLDDTLoss(Module):
     """ Algorithm 27 """
 
@@ -1821,14 +1849,15 @@
         weights: Float['b n']
     ) -> Float['b n 3']:
         """
         pred_coords: predicted coordinates (b, n, 3)
         true_coords: true coordinates (b, n, 3)
         weights: weights for each atom (b, n)
         """
+
         # Compute weighted centroids
         pred_centroid = (pred_coords * weights.unsqueeze(-1)).sum(dim=1) / weights.sum(dim=1, keepdim=True)
         true_centroid = (true_coords * weights.unsqueeze(-1)).sum(dim=1) / weights.sum(dim=1, keepdim=True)
 
         # Center the coordinates
         pred_coords_centered = pred_coords - pred_centroid.unsqueeze(1)
         true_coords_centered = true_coords - true_centroid.unsqueeze(1)
@@ -2474,14 +2503,15 @@
         atom_mask: Bool['b m'],
         atompair_feats: Float['b m m dap'],
         additional_residue_feats: Float['b n rf'],
         msa: Float['b s n d'],
         templates: Float['b t n n dt'],
         template_mask: Bool['b t'],
         num_recycling_steps: int = 1,
+        diffusion_add_bond_loss: bool = False,
         residue_atom_indices: Int['b n'] | None = None,
         num_sample_steps: int | None = None,
         atom_pos: Float['b m 3'] | None = None,
         distance_labels: Int['b n n'] | None = None,
         pae_labels: Int['b n n'] | None = None,
         pde_labels: Int['b n n'] | None = None,
         plddt_labels: Int['b n'] | None = None,
@@ -2596,28 +2626,32 @@
             pairwise_trunk = pairwise,
             pairwise_rel_pos_feats = relative_position_encoding
         )
 
         # if neither atom positions or any labels are passed in, sample a structure and return
 
         if not return_loss:
-            return self.edm.sample(num_sample_steps = num_sample_steps, **diffusion_cond)
+            return self.edm.sample(
+                num_sample_steps = num_sample_steps,
+                **diffusion_cond
+            )
 
         # losses default to 0
 
         distogram_loss = diffusion_loss = confidence_loss = pae_loss = pde_loss = plddt_loss = resolved_loss = self.zero
 
         # otherwise, noise and make it learn to denoise
 
         if exists(atom_pos):
             diffusion_loss, denoised_atom_pos = self.edm(
                 atom_pos,
                 additional_residue_feats = additional_residue_feats,
                 add_smooth_lddt_loss = True,
                 return_denoised_pos = True,
+                add_bond_loss = diffusion_add_bond_loss,
                 **diffusion_cond
             )
 
         # calculate all logits and losses
 
         ignore = self.ignore_index
 
@@ -2629,15 +2663,14 @@
             distogram_loss = F.cross_entropy(distogram_logits, distance_labels, ignore_index = ignore)
 
         # confidence head
 
         should_call_confidence_head = any([*map(exists, confidence_head_labels)])
         return_pae_logits = exists(pae_labels)
 
-
         if should_call_confidence_head:
             assert exists(atom_pos), 'diffusion module needs to have been called'
 
             assert exists(residue_atom_indices)
 
             pred_atom_pos = einx.get_at('b (n [w]) c, b n -> b n c', denoised_atom_pos, residue_atom_indices)
```

### Comparing `alphafold3_pytorch-0.0.7/alphafold3_pytorch/attention.py` & `alphafold3_pytorch-0.0.8/alphafold3_pytorch/attention.py`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/alphafold3_pytorch/typing.py` & `alphafold3_pytorch-0.0.8/alphafold3_pytorch/typing.py`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/docs/alphafold3-supplementary.pdf` & `alphafold3_pytorch-0.0.8/docs/alphafold3-supplementary.pdf`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/tests/test_af3.py` & `alphafold3_pytorch-0.0.8/tests/test_af3.py`

 * *Files 1% similar despite different names*

```diff
@@ -230,15 +230,16 @@
         atom_feats = atom_feats,
         atompair_feats = atompair_feats,
         atom_mask = atom_mask,
         mask = mask,
         single_trunk_repr = single_trunk_repr,
         single_inputs_repr = single_inputs_repr,
         pairwise_trunk = pairwise_trunk,
-        pairwise_rel_pos_feats = pairwise_rel_pos_feats
+        pairwise_rel_pos_feats = pairwise_rel_pos_feats,
+        add_bond_loss = True
     )
 
     assert loss.numel() == 1
 
     sampled_atom_pos = edm.sample(
         atom_mask = atom_mask,
         atom_feats = atom_feats,
```

### Comparing `alphafold3_pytorch-0.0.7/.gitignore` & `alphafold3_pytorch-0.0.8/.gitignore`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/LICENSE` & `alphafold3_pytorch-0.0.8/LICENSE`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/README.md` & `alphafold3_pytorch-0.0.8/README.md`

 * *Files identical despite different names*

### Comparing `alphafold3_pytorch-0.0.7/pyproject.toml` & `alphafold3_pytorch-0.0.8/pyproject.toml`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [project]
 name = "alphafold3-pytorch"
-version = "0.0.7"
+version = "0.0.8"
 description = "Alphafold 3 - Pytorch"
 authors = [
     { name = "Phil Wang", email = "lucidrains@gmail.com" }
 ]
 readme = "README.md"
 requires-python = ">= 3.8"
 license = { file = "LICENSE" }
```

### Comparing `alphafold3_pytorch-0.0.7/PKG-INFO` & `alphafold3_pytorch-0.0.8/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.3
 Name: alphafold3-pytorch
-Version: 0.0.7
+Version: 0.0.8
 Summary: Alphafold 3 - Pytorch
 Project-URL: Homepage, https://pypi.org/project/alphafold3-pytorch/
 Project-URL: Repository, https://github.com/lucidrains/alphafold3-pytorch
 Author-email: Phil Wang <lucidrains@gmail.com>
 License: MIT License
         
         Copyright (c) 2024 Phil Wang
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.3 Name: alphafold3-pytorch Version: 0.0.7 Summary:
+Metadata-Version: 2.3 Name: alphafold3-pytorch Version: 0.0.8 Summary:
 Alphafold 3 - Pytorch Project-URL: Homepage, https://pypi.org/project/
 alphafold3-pytorch/ Project-URL: Repository, https://github.com/lucidrains/
 alphafold3-pytorch Author-email: Phil Wang
 gmail.com> License: MIT License Copyright (c) 2024 Phil Wang Permission is
 hereby granted, free of charge, to any person obtaining a copy of this software
 and associated documentation files (the "Software"), to deal in the Software
 without restriction, including without limitation the rights to use, copy,
```

