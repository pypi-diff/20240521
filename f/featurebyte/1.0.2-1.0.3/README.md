# Comparing `tmp/featurebyte-1.0.2.tar.gz` & `tmp/featurebyte-1.0.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "featurebyte-1.0.2.tar", max compression
+gzip compressed data, was "featurebyte-1.0.3.tar", max compression
```

## Comparing `featurebyte-1.0.2.tar` & `featurebyte-1.0.3.tar`

### file list

```diff
@@ -1,765 +1,782 @@
--rw-r--r--   0        0        0     3860 2024-03-15 07:44:05.322777 featurebyte-1.0.2/LICENSE
--rw-r--r--   0        0        0    19968 2024-03-15 07:44:05.322777 featurebyte-1.0.2/README.md
--rw-r--r--   0        0        0    18225 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/__init__.py
--rw-r--r--   0        0        0     2552 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/__main__.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/api/__init__.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/api/aggregator/__init__.py
--rw-r--r--   0        0        0     4917 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/api/aggregator/asat_aggregator.py
--rw-r--r--   0        0        0     6618 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/api/aggregator/base_aggregator.py
--rw-r--r--   0        0        0     5307 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/api/aggregator/forward_aggregator.py
--rw-r--r--   0        0        0     3130 2024-03-15 07:44:05.338777 featurebyte-1.0.2/featurebyte/api/aggregator/simple_aggregator.py
--rw-r--r--   0        0        0     1588 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/aggregator/vector_validator.py
--rw-r--r--   0        0        0     8593 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/aggregator/window_aggregator.py
--rw-r--r--   0        0        0     5372 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/base.py
--rw-r--r--   0        0        0      502 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/catalog.py
--rw-r--r--   0        0        0      531 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/feature.py
--rw-r--r--   0        0        0      744 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/feature_job_setting_analysis.py
--rw-r--r--   0        0        0      762 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/feature_list.py
--rw-r--r--   0        0        0      635 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/feature_namespace.py
--rw-r--r--   0        0        0      430 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/target_namespace.py
--rw-r--r--   0        0        0      469 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_handler/user_defined_function.py
--rw-r--r--   0        0        0    19433 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_object.py
--rw-r--r--   0        0        0     9637 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/api_object_util.py
--rw-r--r--   0        0        0    39963 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/base_table.py
--rw-r--r--   0        0        0     4869 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/batch_feature_table.py
--rw-r--r--   0        0        0     5333 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/batch_request_table.py
--rw-r--r--   0        0        0    45924 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/catalog.py
--rw-r--r--   0        0        0     2218 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/catalog_decorator.py
--rw-r--r--   0        0        0    14091 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/catalog_get_by_id_mixin.py
--rw-r--r--   0        0        0    11517 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/change_view.py
--rw-r--r--   0        0        0    11533 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/context.py
--rw-r--r--   0        0        0     6202 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/credential.py
--rw-r--r--   0        0        0     7615 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/data_source.py
--rw-r--r--   0        0        0    14221 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/deployment.py
--rw-r--r--   0        0        0     9699 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/dimension_table.py
--rw-r--r--   0        0        0     3107 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/dimension_view.py
--rw-r--r--   0        0        0    10727 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/entity.py
--rw-r--r--   0        0        0    22561 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/event_table.py
--rw-r--r--   0        0        0    15981 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/event_view.py
--rw-r--r--   0        0        0    46555 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature.py
--rw-r--r--   0        0        0    22524 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_group.py
--rw-r--r--   0        0        0    15395 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_job.py
--rw-r--r--   0        0        0     9065 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_job_setting_analysis.py
--rw-r--r--   0        0        0    58436 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_list.py
--rw-r--r--   0        0        0     3765 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_namespace.py
--rw-r--r--   0        0        0     6058 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_or_target_mixin.py
--rw-r--r--   0        0        0     1217 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_or_target_namespace_mixin.py
--rw-r--r--   0        0        0    10436 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_store.py
--rw-r--r--   0        0        0     2004 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_util.py
--rw-r--r--   0        0        0      557 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/feature_validation_util.py
--rw-r--r--   0        0        0    20606 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/groupby.py
--rw-r--r--   0        0        0     5622 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/historical_feature_table.py
--rw-r--r--   0        0        0    17141 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/item_table.py
--rw-r--r--   0        0        0    11923 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/item_view.py
--rw-r--r--   0        0        0     2864 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/lag.py
--rw-r--r--   0        0        0     6245 2024-03-15 07:44:05.342777 featurebyte-1.0.2/featurebyte/api/materialized_table.py
--rw-r--r--   0        0        0     6611 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/mixin.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/obs_table/__init__.py
--rw-r--r--   0        0        0     2641 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/obs_table/utils.py
--rw-r--r--   0        0        0    13093 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/observation_table.py
--rw-r--r--   0        0        0     7240 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/online_store.py
--rw-r--r--   0        0        0      721 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/periodic_task.py
--rw-r--r--   0        0        0     2132 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/primary_entity_mixin.py
--rw-r--r--   0        0        0     8485 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/relationship.py
--rw-r--r--   0        0        0     3474 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/request_column.py
--rw-r--r--   0        0        0     4989 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/savable_api_object.py
--rw-r--r--   0        0        0    21162 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/scd_table.py
--rw-r--r--   0        0        0     6029 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/scd_view.py
--rw-r--r--   0        0        0    52656 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/source_table.py
--rw-r--r--   0        0        0     5310 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/static_source_table.py
--rw-r--r--   0        0        0     5522 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/table.py
--rw-r--r--   0        0        0    16572 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/target.py
--rw-r--r--   0        0        0     4398 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/target_namespace.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/templates/__init__.py
--rw-r--r--   0        0        0     2054 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/templates/doc_util.py
--rw-r--r--   0        0        0      633 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/templates/entity_doc.py
--rw-r--r--   0        0        0     2352 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/templates/feature_or_target_doc.py
--rw-r--r--   0        0        0      427 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/templates/series_doc.py
--rw-r--r--   0        0        0    14196 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/use_case.py
--rw-r--r--   0        0        0     6018 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/use_case_or_context_mixin.py
--rw-r--r--   0        0        0    18284 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/user_defined_function.py
--rw-r--r--   0        0        0    13245 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/user_defined_function_injector.py
--rw-r--r--   0        0        0    70174 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/view.py
--rw-r--r--   0        0        0     1976 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/api/window_validator.py
--rw-r--r--   0        0        0     8258 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/app.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/__init__.py
--rw-r--r--   0        0        0     3116 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/date_util.py
--rw-r--r--   0        0        0      956 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/descriptor.py
--rw-r--r--   0        0        0      636 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/dict_util.py
--rw-r--r--   0        0        0     2029 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/doc_util.py
--rw-r--r--   0        0        0      754 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/allowed_classes.py
--rw-r--r--   0        0        0    15668 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/autodoc_processor.py
--rw-r--r--   0        0        0     1680 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/constants.py
--rw-r--r--   0        0        0     4597 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/custom_nav.py
--rw-r--r--   0        0        0     8560 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/doc_types.py
--rw-r--r--   0        0        0    48513 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/documentation_layout.py
--rw-r--r--   0        0        0     7430 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/extract_csv.py
--rw-r--r--   0        0        0     3479 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/formatters.py
--rw-r--r--   0        0        0    32133 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/gen_ref_pages_docs_builder.py
--rw-r--r--   0        0        0      877 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/markdown_extension/extension.py
--rw-r--r--   0        0        0     6290 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/pydantic_field_docs.py
--rw-r--r--   0        0        0    16430 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/resource_extractor.py
--rw-r--r--   0        0        0      519 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/resource_util.py
--rw-r--r--   0        0        0      200 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/documentation/util.py
--rw-r--r--   0        0        0     1276 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/env_util.py
--rw-r--r--   0        0        0     6740 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/formatting_util.py
--rw-r--r--   0        0        0     4847 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/join_utils.py
--rw-r--r--   0        0        0     5576 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/model_util.py
--rw-r--r--   0        0        0      815 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/path_util.py
--rw-r--r--   0        0        0     1192 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/progress.py
--rw-r--r--   0        0        0      451 2024-03-15 07:44:05.346777 featurebyte-1.0.2/featurebyte/common/singleton.py
--rw-r--r--   0        0        0      971 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/common/string.py
--rw-r--r--   0        0        0     2666 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/common/typing.py
--rw-r--r--   0        0        0    13712 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/common/utils.py
--rw-r--r--   0        0        0     5180 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/common/validator.py
--rw-r--r--   0        0        0    15650 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/config.py
--rw-r--r--   0        0        0     1366 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/conftest.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/__init__.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/__init__.py
--rw-r--r--   0        0        0    18208 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/count_dict.py
--rw-r--r--   0        0        0    25491 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/datetime.py
--rw-r--r--   0        0        0    10075 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/feature_datetime.py
--rw-r--r--   0        0        0     8510 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/feature_string.py
--rw-r--r--   0        0        0    15265 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/string.py
--rw-r--r--   0        0        0     9213 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/target_datetime.py
--rw-r--r--   0        0        0     8168 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/target_string.py
--rw-r--r--   0        0        0     2000 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/accessor/vector.py
--rw-r--r--   0        0        0     2994 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/distance.py
--rw-r--r--   0        0        0     8911 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/frame.py
--rw-r--r--   0        0        0    12835 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/generic.py
--rw-r--r--   0        0        0    16762 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/mixin.py
--rw-r--r--   0        0        0    43255 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/series.py
--rw-r--r--   0        0        0     1780 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/timedelta.py
--rw-r--r--   0        0        0     6233 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/core/util.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/__init__.py
--rw-r--r--   0        0        0      697 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/__main__.py
--rw-r--r--   0        0        0     5546 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/app.py
--rw-r--r--   0        0        0     3125 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/creditcard.sql
--rw-r--r--   0        0        0     1154 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/doctest_grocery.sql
--rw-r--r--   0        0        0     2200 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/grocery.sql
--rw-r--r--   0        0        0     5757 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/datasets/healthcare.sql
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/docker/__init__.py
--rw-r--r--   0        0        0     4650 2024-03-15 07:44:42.811272 featurebyte-1.0.2/featurebyte/docker/featurebyte.yml
--rw-r--r--   0        0        0    11425 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/docker/manager.py
--rw-r--r--   0        0        0    14239 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/enum.py
--rw-r--r--   0        0        0    12298 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/exception.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/__init__.py
--rw-r--r--   0        0        0     1276 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/enum.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/infra/__init__.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/__init__.py
--rw-r--r--   0        0        0     1447 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/databricks.py
--rw-r--r--   0        0        0    10872 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/spark_thrift.py
--rw-r--r--   0        0        0     5932 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/spark_thrift_source.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/model/__init__.py
--rw-r--r--   0        0        0    10088 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/model/feature_store.py
--rw-r--r--   0        0        0     2820 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/model/online_store.py
--rw-r--r--   0        0        0     1922 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/model/registry.py
--rw-r--r--   0        0        0     2354 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/online_store/mysql.py
--rw-r--r--   0        0        0     4208 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/patch.py
--rw-r--r--   0        0        0     1010 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/registry_store.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/schema/__init__.py
--rw-r--r--   0        0        0      734 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/schema/registry.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/service/__init__.py
--rw-r--r--   0        0        0     6392 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/service/feature_store.py
--rw-r--r--   0        0        0    13066 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/service/registry.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/utils/__init__.py
--rw-r--r--   0        0        0     4331 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/utils/materialize_helper.py
--rw-r--r--   0        0        0     5669 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/utils/on_demand_view.py
--rw-r--r--   0        0        0    28317 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feast/utils/registry_construction.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feature_manager/__init__.py
--rw-r--r--   0        0        0     2170 2024-03-15 07:44:05.350777 featurebyte-1.0.2/featurebyte/feature_manager/model.py
--rw-r--r--   0        0        0      785 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/feature_manager/sql_template.py
--rw-r--r--   0        0        0     5166 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/list_utility.py
--rw-r--r--   0        0        0     4231 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/logging.py
--rw-r--r--   0        0        0     6776 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/middleware.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/__init__.py
--rw-r--r--   0        0        0      604 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/migration_data_service.py
--rw-r--r--   0        0        0     1703 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/model.py
--rw-r--r--   0        0        0    13095 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/run.py
--rw-r--r--   0        0        0     1442 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/service/__init__.py
--rw-r--r--   0        0        0    10260 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/service/data_warehouse.py
--rw-r--r--   0        0        0     7239 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/service/feature.py
--rw-r--r--   0        0        0     8061 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/service/feature_list.py
--rw-r--r--   0        0        0    10501 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/migration/service/mixin.py
--rw-r--r--   0        0        0      694 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/__init__.py
--rw-r--r--   0        0        0    12174 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/base.py
--rw-r--r--   0        0        0      392 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/base_feature_or_target_table.py
--rw-r--r--   0        0        0      822 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/batch_feature_table.py
--rw-r--r--   0        0        0     1773 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/batch_request_table.py
--rw-r--r--   0        0        0     2886 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/catalog.py
--rw-r--r--   0        0        0     2206 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/context.py
--rw-r--r--   0        0        0    10405 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/credential.py
--rw-r--r--   0        0        0     1753 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/deployment.py
--rw-r--r--   0        0        0     2026 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/dimension_table.py
--rw-r--r--   0        0        0     3665 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/entity.py
--rw-r--r--   0        0        0     8165 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/entity_lookup_feature_table.py
--rw-r--r--   0        0        0    17184 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/entity_universe.py
--rw-r--r--   0        0        0     4394 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/entity_validation.py
--rw-r--r--   0        0        0     3551 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/event_table.py
--rw-r--r--   0        0        0    23775 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature.py
--rw-r--r--   0        0        0     4319 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_job_setting_analysis.py
--rw-r--r--   0        0        0    23430 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_list.py
--rw-r--r--   0        0        0     3817 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_list_namespace.py
--rw-r--r--   0        0        0    13520 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_list_store_info.py
--rw-r--r--   0        0        0     4755 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_namespace.py
--rw-r--r--   0        0        0      725 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_query_set.py
--rw-r--r--   0        0        0     7669 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_store.py
--rw-r--r--   0        0        0     2462 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/feature_table_cache_metadata.py
--rw-r--r--   0        0        0     1024 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/historical_feature_table.py
--rw-r--r--   0        0        0     2919 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/item_table.py
--rw-r--r--   0        0        0     2075 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/materialized_table.py
--rw-r--r--   0        0        0     1592 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/mixin.py
--rw-r--r--   0        0        0     6225 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/observation_table.py
--rw-r--r--   0        0        0    11897 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/offline_store_feature_table.py
--rw-r--r--   0        0        0    21940 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/offline_store_ingest_query.py
--rw-r--r--   0        0        0     4666 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/online_store.py
--rw-r--r--   0        0        0     1845 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/online_store_compute_query.py
--rw-r--r--   0        0        0     3958 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/online_store_spec.py
--rw-r--r--   0        0        0     1295 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/online_store_table_version.py
--rw-r--r--   0        0        0     8127 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/parent_serving.py
--rw-r--r--   0        0        0     3117 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/periodic_task.py
--rw-r--r--   0        0        0     1455 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/persistent.py
--rw-r--r--   0        0        0     1024 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/proxy_table.py
--rw-r--r--   0        0        0     4414 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/relationship.py
--rw-r--r--   0        0        0      944 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/relationship_analysis.py
--rw-r--r--   0        0        0     8916 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/request_input.py
--rw-r--r--   0        0        0     3554 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/scd_table.py
--rw-r--r--   0        0        0     1482 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/semantic.py
--rw-r--r--   0        0        0     1400 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/sqlglot_expression.py
--rw-r--r--   0        0        0     1475 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/static_source_table.py
--rw-r--r--   0        0        0     3274 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/target.py
--rw-r--r--   0        0        0     2107 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/target_namespace.py
--rw-r--r--   0        0        0      595 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/target_table.py
--rw-r--r--   0        0        0     1408 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/task.py
--rw-r--r--   0        0        0     5022 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/tile.py
--rw-r--r--   0        0        0     1315 2024-03-15 07:44:05.354777 featurebyte-1.0.2/featurebyte/models/tile_job_log.py
--rw-r--r--   0        0        0     2968 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/models/tile_registry.py
--rw-r--r--   0        0        0     2057 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/models/use_case.py
--rw-r--r--   0        0        0    10762 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/models/user_defined_function.py
--rw-r--r--   0        0        0      154 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/persistent/__init__.py
--rw-r--r--   0        0        0     9468 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/persistent/audit.py
--rw-r--r--   0        0        0    25817 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/persistent/base.py
--rw-r--r--   0        0        0    13458 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/persistent/mongo.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/__init__.py
--rw-r--r--   0        0        0     3592 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/algorithm.py
--rw-r--r--   0        0        0     3754 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/enum.py
--rw-r--r--   0        0        0    26801 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/graph.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/graph_node/__init__.py
--rw-r--r--   0        0        0     4592 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/graph_node/base.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/__init__.py
--rw-r--r--   0        0        0     2876 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/column_info.py
--rw-r--r--   0        0        0    12700 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/common_table.py
--rw-r--r--   0        0        0     1707 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/critical_data_info.py
--rw-r--r--   0        0        0    11859 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/entity_lookup_plan.py
--rw-r--r--   0        0        0     8606 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/entity_relationship_info.py
--rw-r--r--   0        0        0     8033 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/feature_job_setting.py
--rw-r--r--   0        0        0    22611 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/graph.py
--rw-r--r--   0        0        0    24668 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/model/table.py
--rw-r--r--   0        0        0     1076 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/__init__.py
--rw-r--r--   0        0        0     5434 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/agg_func.py
--rw-r--r--   0        0        0    46947 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/base.py
--rw-r--r--   0        0        0     8442 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/binary.py
--rw-r--r--   0        0        0    22092 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/cleaning_operation.py
--rw-r--r--   0        0        0    28725 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/count_dict.py
--rw-r--r--   0        0        0    15921 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/date.py
--rw-r--r--   0        0        0     4570 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/distance.py
--rw-r--r--   0        0        0    10138 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/function.py
--rw-r--r--   0        0        0    84645 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/generic.py
--rw-r--r--   0        0        0    17423 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/input.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/__init__.py
--rw-r--r--   0        0        0      873 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/column.py
--rw-r--r--   0        0        0     6152 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/config.py
--rw-r--r--   0        0        0    22630 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/operation.py
--rw-r--r--   0        0        0    20954 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/sdk_code.py
--rw-r--r--   0        0        0     1912 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/templates/databricks_feature_spec.tpl
--rw-r--r--   0        0        0      222 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/templates/on_demand_function.tpl
--rw-r--r--   0        0        0      215 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/templates/on_demand_function_sql.tpl
--rw-r--r--   0        0        0      261 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/templates/on_demand_view.tpl
--rw-r--r--   0        0        0       47 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/metadata/templates/sdk_code.tpl
--rw-r--r--   0        0        0     6405 2024-03-15 07:44:05.358777 featurebyte-1.0.2/featurebyte/query_graph/node/mixin.py
--rw-r--r--   0        0        0    30947 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/nested.py
--rw-r--r--   0        0        0     5660 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/request.py
--rw-r--r--   0        0        0     2054 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/scalar.py
--rw-r--r--   0        0        0    10047 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/schema.py
--rw-r--r--   0        0        0     9730 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/string.py
--rw-r--r--   0        0        0     9937 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/unary.py
--rw-r--r--   0        0        0     1246 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/utils.py
--rw-r--r--   0        0        0     1042 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/validator.py
--rw-r--r--   0        0        0     4569 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/node/vector.py
--rw-r--r--   0        0        0      979 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/pruning_util.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/__init__.py
--rw-r--r--   0        0        0     1091 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/__init__.py
--rw-r--r--   0        0        0    25257 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/base.py
--rw-r--r--   0        0        0     9649 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/databricks.py
--rw-r--r--   0        0        0    19904 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/snowflake.py
--rw-r--r--   0        0        0     1773 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/spark.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/__init__.py
--rw-r--r--   0        0        0     7882 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/asat.py
--rw-r--r--   0        0        0    23188 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/base.py
--rw-r--r--   0        0        0    11493 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/base_lookup.py
--rw-r--r--   0        0        0     6654 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/forward.py
--rw-r--r--   0        0        0     5939 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/item.py
--rw-r--r--   0        0        0     3843 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/latest.py
--rw-r--r--   0        0        0      916 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/lookup.py
--rw-r--r--   0        0        0     1232 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/lookup_target.py
--rw-r--r--   0        0        0     3811 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/request_table.py
--rw-r--r--   0        0        0    28905 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/window.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/__init__.py
--rw-r--r--   0        0        0     3810 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/aggregate.py
--rw-r--r--   0        0        0    13422 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/base.py
--rw-r--r--   0        0        0     2723 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/binary.py
--rw-r--r--   0        0        0     6094 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/count_dict.py
--rw-r--r--   0        0        0    10402 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/datetime.py
--rw-r--r--   0        0        0     1492 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/distance.py
--rw-r--r--   0        0        0     1907 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/function.py
--rw-r--r--   0        0        0     7251 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/generic.py
--rw-r--r--   0        0        0     2823 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/groupby.py
--rw-r--r--   0        0        0     4526 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/input.py
--rw-r--r--   0        0        0     1449 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/is_in.py
--rw-r--r--   0        0        0     6208 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/join.py
--rw-r--r--   0        0        0     7275 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/join_feature.py
--rw-r--r--   0        0        0     2291 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/literal.py
--rw-r--r--   0        0        0      878 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/request.py
--rw-r--r--   0        0        0     8251 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/string.py
--rw-r--r--   0        0        0    11707 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/tile.py
--rw-r--r--   0        0        0     5170 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/track_changes.py
--rw-r--r--   0        0        0     5786 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/unary.py
--rw-r--r--   0        0        0     2536 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/util.py
--rw-r--r--   0        0        0     1297 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/ast/vector.py
--rw-r--r--   0        0        0     6204 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/batch_helper.py
--rw-r--r--   0        0        0     7583 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/builder.py
--rw-r--r--   0        0        0     5725 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/common.py
--rw-r--r--   0        0        0     1704 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/dataframe.py
--rw-r--r--   0        0        0     2134 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/entity.py
--rw-r--r--   0        0        0     1865 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/expression.py
--rw-r--r--   0        0        0    29823 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/feature_compute.py
--rw-r--r--   0        0        0    14416 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/feature_historical.py
--rw-r--r--   0        0        0     4642 2024-03-15 07:44:05.362777 featurebyte-1.0.2/featurebyte/query_graph/sql/feature_preview.py
--rw-r--r--   0        0        0    12442 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/groupby_helper.py
--rw-r--r--   0        0        0      426 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/__init__.py
--rw-r--r--   0        0        0     3673 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/base.py
--rw-r--r--   0        0        0    34700 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/preview.py
--rw-r--r--   0        0        0     9266 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/tile.py
--rw-r--r--   0        0        0     4958 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/materialisation.py
--rw-r--r--   0        0        0     1351 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/offset.py
--rw-r--r--   0        0        0    21900 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/online_serving.py
--rw-r--r--   0        0        0     1305 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/online_serving_util.py
--rw-r--r--   0        0        0    11585 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/online_store_compute_query.py
--rw-r--r--   0        0        0     5951 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/parent_serving.py
--rw-r--r--   0        0        0      936 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/query_graph_util.py
--rw-r--r--   0        0        0    16325 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/scd_helper.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/__init__.py
--rw-r--r--   0        0        0     2907 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/base_lookup.py
--rw-r--r--   0        0        0     1908 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/lookup.py
--rw-r--r--   0        0        0     2037 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/lookup_target.py
--rw-r--r--   0        0        0    25794 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/specs.py
--rw-r--r--   0        0        0     2281 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/template.py
--rw-r--r--   0        0        0    12508 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/tile_compute.py
--rw-r--r--   0        0        0     8103 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/tile_util.py
--rw-r--r--   0        0        0    13069 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/tiling.py
--rw-r--r--   0        0        0      633 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/sql/vector_helper.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/__init__.py
--rw-r--r--   0        0        0     5620 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/base.py
--rw-r--r--   0        0        0    23367 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/decompose_point.py
--rw-r--r--   0        0        0     4997 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/definition.py
--rw-r--r--   0        0        0     4925 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/flattening.py
--rw-r--r--   0        0        0     6689 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/null_filling_value.py
--rw-r--r--   0        0        0    14618 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/offline_store_ingest.py
--rw-r--r--   0        0        0     9686 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/on_demand_function.py
--rw-r--r--   0        0        0     7854 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/on_demand_view.py
--rw-r--r--   0        0        0     6425 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/operation_structure.py
--rw-r--r--   0        0        0    18994 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/pruning.py
--rw-r--r--   0        0        0     3049 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/quick_pruning.py
--rw-r--r--   0        0        0    10873 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/reconstruction.py
--rw-r--r--   0        0        0    13999 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/transform/sdk_code.py
--rw-r--r--   0        0        0     7561 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/query_graph/util.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/__init__.py
--rw-r--r--   0        0        0    11010 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/app_container_config.py
--rw-r--r--   0        0        0     1370 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/base_materialized_table_router.py
--rw-r--r--   0        0        0     7394 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/base_router.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/batch_feature_table/__init__.py
--rw-r--r--   0        0        0     6241 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/batch_feature_table/api.py
--rw-r--r--   0        0        0     5053 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/batch_feature_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/batch_request_table/__init__.py
--rw-r--r--   0        0        0     6269 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/batch_request_table/api.py
--rw-r--r--   0        0        0     3824 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/batch_request_table/controller.py
--rw-r--r--   0        0        0      868 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/block_modification_handler.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/catalog/__init__.py
--rw-r--r--   0        0        0     7176 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/catalog/api.py
--rw-r--r--   0        0        0     1374 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/catalog/catalog_name_injector.py
--rw-r--r--   0        0        0     6282 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/catalog/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/common/__init__.py
--rw-r--r--   0        0        0    12212 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/common/base.py
--rw-r--r--   0        0        0     6837 2024-03-15 07:44:05.366777 featurebyte-1.0.2/featurebyte/routes/common/base_materialized_table.py
--rw-r--r--   0        0        0    14260 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/base_table.py
--rw-r--r--   0        0        0     2442 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/derive_primary_entity_helper.py
--rw-r--r--   0        0        0     6000 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/feature_metadata_extractor.py
--rw-r--r--   0        0        0     1861 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/feature_or_target_helper.py
--rw-r--r--   0        0        0     9042 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/feature_or_target_table.py
--rw-r--r--   0        0        0     2028 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/primary_entity_validator.py
--rw-r--r--   0        0        0      864 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/common/schema.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/context/__init__.py
--rw-r--r--   0        0        0     4802 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/context/api.py
--rw-r--r--   0        0        0     8926 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/context/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/credential/__init__.py
--rw-r--r--   0        0        0     4856 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/credential/api.py
--rw-r--r--   0        0        0     2924 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/credential/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/deployment/__init__.py
--rw-r--r--   0        0        0     8535 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/deployment/api.py
--rw-r--r--   0        0        0    16594 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/deployment/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/dimension_table/__init__.py
--rw-r--r--   0        0        0     7268 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/dimension_table/api.py
--rw-r--r--   0        0        0     3686 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/dimension_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/entity/__init__.py
--rw-r--r--   0        0        0     5323 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/entity/api.py
--rw-r--r--   0        0        0     6399 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/entity/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/event_table/__init__.py
--rw-r--r--   0        0        0     7969 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/event_table/api.py
--rw-r--r--   0        0        0     4824 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/event_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature/__init__.py
--rw-r--r--   0        0        0     7807 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature/api.py
--rw-r--r--   0        0        0    15929 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_job_setting_analysis/__init__.py
--rw-r--r--   0        0        0     7006 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_job_setting_analysis/api.py
--rw-r--r--   0        0        0     6631 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_job_setting_analysis/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_list/__init__.py
--rw-r--r--   0        0        0     9913 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_list/api.py
--rw-r--r--   0        0        0    21652 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_list/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_list_namespace/__init__.py
--rw-r--r--   0        0        0     5384 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_list_namespace/api.py
--rw-r--r--   0        0        0     9740 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_list_namespace/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_namespace/__init__.py
--rw-r--r--   0        0        0     4953 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_namespace/api.py
--rw-r--r--   0        0        0     8156 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_namespace/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_store/__init__.py
--rw-r--r--   0        0        0    11414 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_store/api.py
--rw-r--r--   0        0        0    13436 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/feature_store/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/historical_feature_table/__init__.py
--rw-r--r--   0        0        0     7509 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/historical_feature_table/api.py
--rw-r--r--   0        0        0     4859 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/historical_feature_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/item_table/__init__.py
--rw-r--r--   0        0        0     6819 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/item_table/api.py
--rw-r--r--   0        0        0     4282 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/item_table/controller.py
--rw-r--r--   0        0        0     8257 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/lazy_app_container.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/observation_table/__init__.py
--rw-r--r--   0        0        0     7435 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/observation_table/api.py
--rw-r--r--   0        0        0     8291 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/observation_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/online_store/__init__.py
--rw-r--r--   0        0        0     4258 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/online_store/api.py
--rw-r--r--   0        0        0     2958 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/online_store/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/periodic_tasks/__init__.py
--rw-r--r--   0        0        0     3059 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/periodic_tasks/api.py
--rw-r--r--   0        0        0      540 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/periodic_tasks/controller.py
--rw-r--r--   0        0        0    25926 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/registry.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/relationship_info/__init__.py
--rw-r--r--   0        0        0     4662 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/relationship_info/api.py
--rw-r--r--   0        0        0     5601 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/relationship_info/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/scd_table/__init__.py
--rw-r--r--   0        0        0     6741 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/scd_table/api.py
--rw-r--r--   0        0        0     4078 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/scd_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/semantic/__init__.py
--rw-r--r--   0        0        0     3680 2024-03-15 07:44:05.370777 featurebyte-1.0.2/featurebyte/routes/semantic/api.py
--rw-r--r--   0        0        0      943 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/semantic/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/static_source_table/__init__.py
--rw-r--r--   0        0        0     6243 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/static_source_table/api.py
--rw-r--r--   0        0        0     3807 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/static_source_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/table/__init__.py
--rw-r--r--   0        0        0     1735 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/table/api.py
--rw-r--r--   0        0        0      464 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target/__init__.py
--rw-r--r--   0        0        0     5261 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target/api.py
--rw-r--r--   0        0        0     8860 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target_namespace/__init__.py
--rw-r--r--   0        0        0     5667 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target_namespace/api.py
--rw-r--r--   0        0        0     2662 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target_namespace/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target_table/__init__.py
--rw-r--r--   0        0        0     7932 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target_table/api.py
--rw-r--r--   0        0        0     3966 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/target_table/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/task/__init__.py
--rw-r--r--   0        0        0     2124 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/task/api.py
--rw-r--r--   0        0        0     2331 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/task/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/temp_data/__init__.py
--rw-r--r--   0        0        0      932 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/temp_data/api.py
--rw-r--r--   0        0        0     1353 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/temp_data/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/use_case/__init__.py
--rw-r--r--   0        0        0     7180 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/use_case/api.py
--rw-r--r--   0        0        0    11508 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/use_case/controller.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/user_defined_function/__init__.py
--rw-r--r--   0        0        0     6071 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/user_defined_function/api.py
--rw-r--r--   0        0        0    10312 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/routes/user_defined_function/controller.py
--rw-r--r--   0        0        0      180 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/__init__.py
--rw-r--r--   0        0        0     1431 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/batch_feature_table.py
--rw-r--r--   0        0        0      842 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/batch_request_table.py
--rw-r--r--   0        0        0     1799 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/catalog.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/common/__init__.py
--rw-r--r--   0        0        0     1226 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/common/base.py
--rw-r--r--   0        0        0      643 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/common/feature_or_target.py
--rw-r--r--   0        0        0     3662 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/common/operation.py
--rw-r--r--   0        0        0     3223 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/context.py
--rw-r--r--   0        0        0     3437 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/credential.py
--rw-r--r--   0        0        0     1730 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/deployment.py
--rw-r--r--   0        0        0     1192 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/dimension_table.py
--rw-r--r--   0        0        0     1684 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/entity.py
--rw-r--r--   0        0        0     2688 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/event_table.py
--rw-r--r--   0        0        0     6281 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature.py
--rw-r--r--   0        0        0     5903 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature_job_setting_analysis.py
--rw-r--r--   0        0        0     8266 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature_list.py
--rw-r--r--   0        0        0     1473 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature_list_namespace.py
--rw-r--r--   0        0        0     2062 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature_namespace.py
--rw-r--r--   0        0        0     3685 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature_store.py
--rw-r--r--   0        0        0      420 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/feature_table_cache_metadata.py
--rw-r--r--   0        0        0     1634 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/historical_feature_table.py
--rw-r--r--   0        0        0    13415 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/info.py
--rw-r--r--   0        0        0     1216 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/item_table.py
--rw-r--r--   0        0        0      599 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/materialized_table.py
--rw-r--r--   0        0        0     2131 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/observation_table.py
--rw-r--r--   0        0        0     1724 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/online_store.py
--rw-r--r--   0        0        0      459 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/periodic_task.py
--rw-r--r--   0        0        0     1032 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/preview.py
--rw-r--r--   0        0        0     1544 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/relationship_info.py
--rw-r--r--   0        0        0     1190 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/request_table.py
--rw-r--r--   0        0        0     1736 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/scd_table.py
--rw-r--r--   0        0        0      914 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/semantic.py
--rw-r--r--   0        0        0     1152 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/static_source_table.py
--rw-r--r--   0        0        0     4568 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/table.py
--rw-r--r--   0        0        0     2619 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/target.py
--rw-r--r--   0        0        0     1852 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/target_namespace.py
--rw-r--r--   0        0        0     3190 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/target_table.py
--rw-r--r--   0        0        0     1772 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/task.py
--rw-r--r--   0        0        0     2624 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/use_case.py
--rw-r--r--   0        0        0     2682 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/user_defined_function.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/__init__.py
--rw-r--r--   0        0        0      405 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/progress.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/__init__.py
--rw-r--r--   0        0        0     3161 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/base.py
--rw-r--r--   0        0        0      739 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/batch_feature_create.py
--rw-r--r--   0        0        0      610 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/batch_feature_table.py
--rw-r--r--   0        0        0      602 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/batch_request_table.py
--rw-r--r--   0        0        0     1557 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/deployment_create_update.py
--rw-r--r--   0        0        0     1425 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/feature_job_setting_analysis.py
--rw-r--r--   0        0        0      802 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/feature_list_batch_feature_create.py
--rw-r--r--   0        0        0     1113 2024-03-15 07:44:05.374777 featurebyte-1.0.2/featurebyte/schema/worker/task/feature_list_create.py
--rw-r--r--   0        0        0      747 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/feature_list_make_production_ready.py
--rw-r--r--   0        0        0      793 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/historical_feature_table.py
--rw-r--r--   0        0        0     1459 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/materialized_table_delete.py
--rw-r--r--   0        0        0      589 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/observation_table.py
--rw-r--r--   0        0        0      801 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/observation_table_upload.py
--rw-r--r--   0        0        0      443 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/online_store_cleanup.py
--rw-r--r--   0        0        0      577 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/online_store_initialize.py
--rw-r--r--   0        0        0      540 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/scheduled_feature_materialize.py
--rw-r--r--   0        0        0      602 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/static_source_table.py
--rw-r--r--   0        0        0      591 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/target_table.py
--rw-r--r--   0        0        0      962 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/test.py
--rw-r--r--   0        0        0      459 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/schema/worker/task/tile.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/__init__.py
--rw-r--r--   0        0        0    40084 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/base_document.py
--rw-r--r--   0        0        0     3870 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/base_feature_service.py
--rw-r--r--   0        0        0     5950 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/base_table_document.py
--rw-r--r--   0        0        0     1929 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/batch_feature_table.py
--rw-r--r--   0        0        0     3553 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/batch_request_table.py
--rw-r--r--   0        0        0     3748 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/catalog.py
--rw-r--r--   0        0        0     5568 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/context.py
--rw-r--r--   0        0        0     6380 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/credential.py
--rw-r--r--   0        0        0    32196 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/deploy.py
--rw-r--r--   0        0        0      653 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/deployment.py
--rw-r--r--   0        0        0      674 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/dimension_table.py
--rw-r--r--   0        0        0     3401 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/entity.py
--rw-r--r--   0        0        0     4374 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/entity_lookup_feature_table.py
--rw-r--r--   0        0        0    11963 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/entity_relationship_extractor.py
--rw-r--r--   0        0        0     6106 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/entity_serving_names.py
--rw-r--r--   0        0        0     9675 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/entity_validation.py
--rw-r--r--   0        0        0      618 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/event_table.py
--rw-r--r--   0        0        0    13938 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature.py
--rw-r--r--   0        0        0    10948 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_facade.py
--rw-r--r--   0        0        0     5129 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_job_setting_analysis.py
--rw-r--r--   0        0        0    28124 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_list.py
--rw-r--r--   0        0        0     6538 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_list_facade.py
--rw-r--r--   0        0        0      582 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_list_namespace.py
--rw-r--r--   0        0        0     4261 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_list_status.py
--rw-r--r--   0        0        0    18951 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_manager.py
--rw-r--r--   0        0        0    33996 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_materialize.py
--rw-r--r--   0        0        0     4176 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_materialize_scheduler.py
--rw-r--r--   0        0        0      574 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_namespace.py
--rw-r--r--   0        0        0    12755 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_offline_store_info.py
--rw-r--r--   0        0        0    19167 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_preview.py
--rw-r--r--   0        0        0    15047 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_readiness.py
--rw-r--r--   0        0        0     1468 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_store.py
--rw-r--r--   0        0        0    13833 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_store_warehouse.py
--rw-r--r--   0        0        0    30052 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_table_cache.py
--rw-r--r--   0        0        0     4320 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/feature_table_cache_metadata.py
--rw-r--r--   0        0        0     4074 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/historical_feature_table.py
--rw-r--r--   0        0        0     8781 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/historical_features.py
--rw-r--r--   0        0        0    13266 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/historical_features_and_target.py
--rw-r--r--   0        0        0      604 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/item_table.py
--rw-r--r--   0        0        0     8319 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/materialized_table.py
--rw-r--r--   0        0        0     4343 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/mixin.py
--rw-r--r--   0        0        0     5517 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/namespace_handler.py
--rw-r--r--   0        0        0    25289 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/observation_table.py
--rw-r--r--   0        0        0     8689 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/offline_store_feature_table.py
--rw-r--r--   0        0        0     7268 2024-03-15 07:44:05.378778 featurebyte-1.0.2/featurebyte/service/offline_store_feature_table_comment.py
--rw-r--r--   0        0        0    10543 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/offline_store_feature_table_construction.py
--rw-r--r--   0        0        0    25725 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/offline_store_feature_table_manager.py
--rw-r--r--   0        0        0     9514 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_enable.py
--rw-r--r--   0        0        0    23693 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_serving.py
--rw-r--r--   0        0        0     5948 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_store.py
--rw-r--r--   0        0        0     4519 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_store_cleanup.py
--rw-r--r--   0        0        0     3797 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_store_cleanup_scheduler.py
--rw-r--r--   0        0        0     2695 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_store_compute_query_service.py
--rw-r--r--   0        0        0     3228 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/online_store_table_version.py
--rw-r--r--   0        0        0     5818 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/parent_serving.py
--rw-r--r--   0        0        0      437 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/periodic_task.py
--rw-r--r--   0        0        0    10058 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/preview.py
--rw-r--r--   0        0        0     9331 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/relationship.py
--rw-r--r--   0        0        0     2311 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/relationship_info.py
--rw-r--r--   0        0        0      991 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/sanitizer.py
--rw-r--r--   0        0        0      590 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/scd_table.py
--rw-r--r--   0        0        0      600 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/semantic.py
--rw-r--r--   0        0        0     3018 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/session_manager.py
--rw-r--r--   0        0        0     6699 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/session_validator.py
--rw-r--r--   0        0        0     6049 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/specialized_dtype.py
--rw-r--r--   0        0        0     2898 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/static_source_table.py
--rw-r--r--   0        0        0     1104 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/table.py
--rw-r--r--   0        0        0    17946 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/table_columns_info.py
--rw-r--r--   0        0        0     6940 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/table_facade.py
--rw-r--r--   0        0        0     3283 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/table_info.py
--rw-r--r--   0        0        0     2081 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/table_status.py
--rw-r--r--   0        0        0    10320 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/target.py
--rw-r--r--   0        0        0     6515 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/target_helper/base_feature_or_target_computer.py
--rw-r--r--   0        0        0     4723 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/target_helper/compute_target.py
--rw-r--r--   0        0        0      535 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/target_namespace.py
--rw-r--r--   0        0        0     3767 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/target_table.py
--rw-r--r--   0        0        0    12043 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/task_manager.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/templates/__init__.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/templates/online_serving/__init__.py
--rw-r--r--   0        0        0      707 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/templates/online_serving/python.tpl
--rw-r--r--   0        0        0      132 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/templates/online_serving/sh.tpl
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile/__init__.py
--rw-r--r--   0        0        0    11900 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile/tile_task_executor.py
--rw-r--r--   0        0        0     2434 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile_cache.py
--rw-r--r--   0        0        0     7045 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile_job_log.py
--rw-r--r--   0        0        0    13443 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile_manager.py
--rw-r--r--   0        0        0     2681 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile_registry_service.py
--rw-r--r--   0        0        0     2519 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/tile_scheduler.py
--rw-r--r--   0        0        0     2509 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/use_case.py
--rw-r--r--   0        0        0     2663 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/user_defined_function.py
--rw-r--r--   0        0        0     1142 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/user_service.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/validator/__init__.py
--rw-r--r--   0        0        0     5065 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/validator/entity_relationship_validator.py
--rw-r--r--   0        0        0     7014 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/validator/materialized_table_delete.py
--rw-r--r--   0        0        0    11726 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/validator/production_ready_validator.py
--rw-r--r--   0        0        0    16096 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/version.py
--rw-r--r--   0        0        0    13077 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/view_construction.py
--rw-r--r--   0        0        0     4591 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/service/working_schema.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/session/__init__.py
--rw-r--r--   0        0        0    33943 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/session/base.py
--rw-r--r--   0        0        0    20163 2024-03-15 07:44:05.382778 featurebyte-1.0.2/featurebyte/session/base_spark.py
--rw-r--r--   0        0        0     6348 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/databricks.py
--rw-r--r--   0        0        0     3901 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/databricks_unity.py
--rw-r--r--   0        0        0      476 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/enum.py
--rw-r--r--   0        0        0     5023 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/hive.py
--rw-r--r--   0        0        0     6941 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/manager.py
--rw-r--r--   0        0        0     2455 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/session_helper.py
--rw-r--r--   0        0        0     9179 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/simple_storage.py
--rw-r--r--   0        0        0    17798 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/snowflake.py
--rw-r--r--   0        0        0    14040 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/spark.py
--rw-r--r--   0        0        0     3882 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/sqlite.py
--rw-r--r--   0        0        0     6615 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/session/webhdfs.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/__init__.py
--rw-r--r--   0        0        0     2569 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/base.py
--rw-r--r--   0        0        0     3666 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/common.py
--rw-r--r--   0        0        0        6 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks/.gitignore
--rw-r--r--   0        0        0      786 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_COSINE_SIMILARITY.sql
--rw-r--r--   0        0        0      277 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_ENTROPY.sql
--rw-r--r--   0        0        0      212 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_LEAST_FREQUENT.sql
--rw-r--r--   0        0        0      212 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT.sql
--rw-r--r--   0        0        0      703 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql
--rw-r--r--   0        0        0      217 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT_VALUE.sql
--rw-r--r--   0        0        0      172 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_NUM_UNIQUE.sql
--rw-r--r--   0        0        0      828 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_GET_RANK.sql
--rw-r--r--   0        0        0      299 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_GET_RELATIVE_FREQUENCY.sql
--rw-r--r--   0        0        0      413 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_INDEX_TO_TIMESTAMP.sql
--rw-r--r--   0        0        0      203 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_OBJECT_DELETE.sql
--rw-r--r--   0        0        0      377 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_TIMESTAMP_TO_INDEX.sql
--rw-r--r--   0        0        0      579 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_TIMEZONE_OFFSET_TO_SECOND.sql
--rw-r--r--   0        0        0      957 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql
--rw-r--r--   0        0        0      528 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_ENTROPY.sql
--rw-r--r--   0        0        0      178 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_LEAST_FREQUENT.sql
--rw-r--r--   0        0        0      178 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT.sql
--rw-r--r--   0        0        0      777 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql
--rw-r--r--   0        0        0      184 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_VALUE.sql
--rw-r--r--   0        0        0      188 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_NUM_UNIQUE.sql
--rw-r--r--   0        0        0     1491 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_GET_RANK.sql
--rw-r--r--   0        0        0      449 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_GET_RELATIVE_FREQUENCY.sql
--rw-r--r--   0        0        0      559 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql
--rw-r--r--   0        0        0      559 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql
--rw-r--r--   0        0        0      653 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql
--rw-r--r--   0        0        0      863 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_AVG.sql
--rw-r--r--   0        0        0      614 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_MAX.sql
--rw-r--r--   0        0        0      841 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SIMPLE_AVERAGE.sql
--rw-r--r--   0        0        0      570 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SUM.sql
--rw-r--r--   0        0        0      852 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_COSINE_SIMILARITY.sql
--rw-r--r--   0        0        0      153 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/snowflake/T_TILE_MONITOR_SUMMARY.sql
--rw-r--r--   0        0        0        6 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/spark/.gitignore
--rw-r--r--   0        0        0      191 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/spark/T_TILE_MONITOR_SUMMARY.sql
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/spark/__init__.py
--rw-r--r--   0        0        0    48457 2024-03-15 07:45:41.763990 featurebyte-1.0.2/featurebyte/sql/spark/featurebyte-hive-udf-1.0.6-SNAPSHOT-all.jar
--rw-r--r--   0        0        0     1923 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/tile_common.py
--rw-r--r--   0        0        0     5635 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/tile_generate.py
--rw-r--r--   0        0        0     3857 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/tile_generate_entity_tracking.py
--rw-r--r--   0        0        0     8611 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/tile_monitor.py
--rw-r--r--   0        0        0     2833 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/tile_registry.py
--rw-r--r--   0        0        0     6805 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/sql/tile_schedule_online_store.py
--rw-r--r--   0        0        0      372 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/storage/__init__.py
--rw-r--r--   0        0        0     5122 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/storage/azure.py
--rw-r--r--   0        0        0     6518 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/storage/base.py
--rw-r--r--   0        0        0     3640 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/storage/local.py
--rw-r--r--   0        0        0      415 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/storage/local_temp.py
--rw-r--r--   0        0        0     5675 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/storage/s3.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/tile/__init__.py
--rw-r--r--   0        0        0      411 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/tile/sql_template.py
--rw-r--r--   0        0        0    29559 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/tile/tile_cache.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/__init__.py
--rw-r--r--   0        0        0     1921 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/credential.py
--rw-r--r--   0        0        0     1605 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/messaging.py
--rw-r--r--   0        0        0      518 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/persistent.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/snowflake/__init__.py
--rw-r--r--   0        0        0      462 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/snowflake/sql.py
--rw-r--r--   0        0        0     3748 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/utils/storage.py
--rw-r--r--   0        0        0     4175 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/__init__.py
--rw-r--r--   0        0        0      170 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/enum.py
--rw-r--r--   0        0        0     1159 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/progress.py
--rw-r--r--   0        0        0     3525 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/registry.py
--rw-r--r--   0        0        0     2502 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/schedulers.py
--rw-r--r--   0        0        0      230 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/start.py
--rw-r--r--   0        0        0      214 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/task/__init__.py
--rw-r--r--   0        0        0     4166 2024-03-15 07:44:05.386778 featurebyte-1.0.2/featurebyte/worker/task/base.py
--rw-r--r--   0        0        0     1212 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/batch_feature_create.py
--rw-r--r--   0        0        0     4554 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/batch_feature_table.py
--rw-r--r--   0        0        0     3020 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/batch_request_table.py
--rw-r--r--   0        0        0     4993 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/catalog_online_store_update.py
--rw-r--r--   0        0        0     4002 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/deployment_create_update.py
--rw-r--r--   0        0        0     6259 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/feature_job_setting_analysis.py
--rw-r--r--   0        0        0     5210 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/feature_job_setting_analysis_backtest.py
--rw-r--r--   0        0        0     4014 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/feature_list_batch_feature_create.py
--rw-r--r--   0        0        0     4066 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/feature_list_create.py
--rw-r--r--   0        0        0     1666 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/feature_list_make_production_ready.py
--rw-r--r--   0        0        0     4307 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/historical_feature_table.py
--rw-r--r--   0        0        0     8629 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/materialized_table_delete.py
--rw-r--r--   0        0        0     1980 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/mixin.py
--rw-r--r--   0        0        0     3646 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/observation_table.py
--rw-r--r--   0        0        0     4665 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/observation_table_upload.py
--rw-r--r--   0        0        0     1213 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/online_store_cleanup.py
--rw-r--r--   0        0        0     1669 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/scheduled_feature_materialize.py
--rw-r--r--   0        0        0     2921 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/static_source_table.py
--rw-r--r--   0        0        0     5951 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/target_table.py
--rw-r--r--   0        0        0     1141 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/test_task.py
--rw-r--r--   0        0        0     1757 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task/tile_task.py
--rw-r--r--   0        0        0     9194 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/task_executor.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/test_util/__init__.py
--rw-r--r--   0        0        0     3138 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/test_util/random_task.py
--rw-r--r--   0        0        0        0 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/util/__init__.py
--rw-r--r--   0        0        0    17303 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/util/batch_feature_creator.py
--rw-r--r--   0        0        0     1773 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/util/observation_set_helper.py
--rw-r--r--   0        0        0     1404 2024-03-15 07:44:05.390778 featurebyte-1.0.2/featurebyte/worker/util/task_progress_updater.py
--rw-r--r--   0        0        0     8304 2024-03-15 07:44:40.879247 featurebyte-1.0.2/pyproject.toml
--rw-r--r--   0        0        0    23491 1970-01-01 00:00:00.000000 featurebyte-1.0.2/PKG-INFO
+-rw-r--r--   0        0        0     3860 2024-05-21 11:11:12.724927 featurebyte-1.0.3/LICENSE
+-rw-r--r--   0        0        0    19968 2024-05-21 11:11:12.724927 featurebyte-1.0.3/README.md
+-rw-r--r--   0        0        0    18347 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/__init__.py
+-rw-r--r--   0        0        0     2553 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/__main__.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/_overrides/__init__.py
+-rw-r--r--   0        0        0     1469 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/_overrides/typechecked_override.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/accessor/__init__.py
+-rw-r--r--   0        0        0     7284 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/accessor/databricks.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/__init__.py
+-rw-r--r--   0        0        0     3937 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/asat_aggregator.py
+-rw-r--r--   0        0        0     6612 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/base_aggregator.py
+-rw-r--r--   0        0        0     1907 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/base_asat_aggregator.py
+-rw-r--r--   0        0        0     5301 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/forward_aggregator.py
+-rw-r--r--   0        0        0     3877 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/forward_asat_aggregator.py
+-rw-r--r--   0        0        0     3153 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/simple_aggregator.py
+-rw-r--r--   0        0        0     1589 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/vector_validator.py
+-rw-r--r--   0        0        0     8587 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/aggregator/window_aggregator.py
+-rw-r--r--   0        0        0     5373 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/base.py
+-rw-r--r--   0        0        0      503 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/catalog.py
+-rw-r--r--   0        0        0      531 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/feature.py
+-rw-r--r--   0        0        0      744 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0      762 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/feature_list.py
+-rw-r--r--   0        0        0      635 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/feature_namespace.py
+-rw-r--r--   0        0        0      430 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/target_namespace.py
+-rw-r--r--   0        0        0      470 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_handler/user_defined_function.py
+-rw-r--r--   0        0        0    19434 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_object.py
+-rw-r--r--   0        0        0     9638 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/api_object_util.py
+-rw-r--r--   0        0        0    39993 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/base_table.py
+-rw-r--r--   0        0        0     4870 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/batch_feature_table.py
+-rw-r--r--   0        0        0     5334 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/batch_request_table.py
+-rw-r--r--   0        0        0    46204 2024-05-21 11:11:12.740925 featurebyte-1.0.3/featurebyte/api/catalog.py
+-rw-r--r--   0        0        0     2219 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/catalog_decorator.py
+-rw-r--r--   0        0        0    14092 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/catalog_get_by_id_mixin.py
+-rw-r--r--   0        0        0    11518 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/change_view.py
+-rw-r--r--   0        0        0    11534 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/context.py
+-rw-r--r--   0        0        0     5919 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/credential.py
+-rw-r--r--   0        0        0     7616 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/data_source.py
+-rw-r--r--   0        0        0    16416 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/deployment.py
+-rw-r--r--   0        0        0     9729 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/dimension_table.py
+-rw-r--r--   0        0        0     3108 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/dimension_view.py
+-rw-r--r--   0        0        0    10728 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/entity.py
+-rw-r--r--   0        0        0    22591 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/event_table.py
+-rw-r--r--   0        0        0    16297 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/event_view.py
+-rw-r--r--   0        0        0    47541 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature.py
+-rw-r--r--   0        0        0    22517 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_group.py
+-rw-r--r--   0        0        0    15412 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_job.py
+-rw-r--r--   0        0        0     9066 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0    59014 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_list.py
+-rw-r--r--   0        0        0     3766 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_namespace.py
+-rw-r--r--   0        0        0     6059 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_or_target_mixin.py
+-rw-r--r--   0        0        0     1218 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_or_target_namespace_mixin.py
+-rw-r--r--   0        0        0    10437 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_store.py
+-rw-r--r--   0        0        0     2005 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_util.py
+-rw-r--r--   0        0        0      558 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/feature_validation_util.py
+-rw-r--r--   0        0        0    25564 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/groupby.py
+-rw-r--r--   0        0        0     9640 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/historical_feature_table.py
+-rw-r--r--   0        0        0    17171 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/item_table.py
+-rw-r--r--   0        0        0    11924 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/item_view.py
+-rw-r--r--   0        0        0     2865 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/lag.py
+-rw-r--r--   0        0        0     6649 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/materialized_table.py
+-rw-r--r--   0        0        0    17049 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/mixin.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/obs_table/__init__.py
+-rw-r--r--   0        0        0     2642 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/obs_table/utils.py
+-rw-r--r--   0        0        0    14682 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/observation_table.py
+-rw-r--r--   0        0        0     7241 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/online_store.py
+-rw-r--r--   0        0        0      722 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/periodic_task.py
+-rw-r--r--   0        0        0     2133 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/primary_entity_mixin.py
+-rw-r--r--   0        0        0     8515 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/relationship.py
+-rw-r--r--   0        0        0     3475 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/request_column.py
+-rw-r--r--   0        0        0     4990 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/savable_api_object.py
+-rw-r--r--   0        0        0    21192 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/scd_table.py
+-rw-r--r--   0        0        0     6030 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/scd_view.py
+-rw-r--r--   0        0        0    53110 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/source_table.py
+-rw-r--r--   0        0        0     5311 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/static_source_table.py
+-rw-r--r--   0        0        0     5523 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/table.py
+-rw-r--r--   0        0        0    16353 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/target.py
+-rw-r--r--   0        0        0     5008 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/target_namespace.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/templates/__init__.py
+-rw-r--r--   0        0        0     2048 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/templates/doc_util.py
+-rw-r--r--   0        0        0      633 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/templates/entity_doc.py
+-rw-r--r--   0        0        0     2352 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/templates/feature_or_target_doc.py
+-rw-r--r--   0        0        0      427 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/templates/series_doc.py
+-rw-r--r--   0        0        0    14197 2024-05-21 11:11:12.744925 featurebyte-1.0.3/featurebyte/api/use_case.py
+-rw-r--r--   0        0        0     6019 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/api/use_case_or_context_mixin.py
+-rw-r--r--   0        0        0    18314 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/api/user_defined_function.py
+-rw-r--r--   0        0        0    13841 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/api/user_defined_function_injector.py
+-rw-r--r--   0        0        0     2557 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/api/utils.py
+-rw-r--r--   0        0        0    70609 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/api/view.py
+-rw-r--r--   0        0        0     1977 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/api/window_validator.py
+-rw-r--r--   0        0        0     8363 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/app.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/__init__.py
+-rw-r--r--   0        0        0     3117 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/date_util.py
+-rw-r--r--   0        0        0      957 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/descriptor.py
+-rw-r--r--   0        0        0      637 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/dict_util.py
+-rw-r--r--   0        0        0     2030 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/doc_util.py
+-rw-r--r--   0        0        0      754 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/allowed_classes.py
+-rw-r--r--   0        0        0    15669 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/autodoc_processor.py
+-rw-r--r--   0        0        0     1681 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/constants.py
+-rw-r--r--   0        0        0     4598 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/custom_nav.py
+-rw-r--r--   0        0        0     8561 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/doc_types.py
+-rw-r--r--   0        0        0    48883 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/documentation_layout.py
+-rw-r--r--   0        0        0     7431 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/extract_csv.py
+-rw-r--r--   0        0        0     3480 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/formatters.py
+-rw-r--r--   0        0        0    32152 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/gen_ref_pages_docs_builder.py
+-rw-r--r--   0        0        0      878 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/markdown_extension/extension.py
+-rw-r--r--   0        0        0     6291 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/pydantic_field_docs.py
+-rw-r--r--   0        0        0    16431 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/resource_extractor.py
+-rw-r--r--   0        0        0      520 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/resource_util.py
+-rw-r--r--   0        0        0      201 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/documentation/util.py
+-rw-r--r--   0        0        0     1261 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/env_util.py
+-rw-r--r--   0        0        0     6741 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/formatting_util.py
+-rw-r--r--   0        0        0     4848 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/join_utils.py
+-rw-r--r--   0        0        0     5577 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/model_util.py
+-rw-r--r--   0        0        0      816 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/path_util.py
+-rw-r--r--   0        0        0     1985 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/progress.py
+-rw-r--r--   0        0        0      452 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/singleton.py
+-rw-r--r--   0        0        0      972 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/string.py
+-rw-r--r--   0        0        0    11326 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/utils.py
+-rw-r--r--   0        0        0     5181 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/common/validator.py
+-rw-r--r--   0        0        0    15669 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/config.py
+-rw-r--r--   0        0        0     1367 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/conftest.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/__init__.py
+-rw-r--r--   0        0        0    18300 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/count_dict.py
+-rw-r--r--   0        0        0    25492 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/datetime.py
+-rw-r--r--   0        0        0    10076 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/feature_datetime.py
+-rw-r--r--   0        0        0     8511 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/feature_string.py
+-rw-r--r--   0        0        0    15266 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/string.py
+-rw-r--r--   0        0        0     9214 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/target_datetime.py
+-rw-r--r--   0        0        0     8169 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/target_string.py
+-rw-r--r--   0        0        0     2001 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/accessor/vector.py
+-rw-r--r--   0        0        0     2995 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/distance.py
+-rw-r--r--   0        0        0     8886 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/frame.py
+-rw-r--r--   0        0        0    13575 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/generic.py
+-rw-r--r--   0        0        0     5625 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/mixin.py
+-rw-r--r--   0        0        0    43278 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/series.py
+-rw-r--r--   0        0        0     1774 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/timedelta.py
+-rw-r--r--   0        0        0     6227 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/core/util.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/datasets/__init__.py
+-rw-r--r--   0        0        0      698 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/datasets/__main__.py
+-rw-r--r--   0        0        0     5547 2024-05-21 11:11:12.748925 featurebyte-1.0.3/featurebyte/datasets/app.py
+-rw-r--r--   0        0        0     3125 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/datasets/creditcard.sql
+-rw-r--r--   0        0        0     1154 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/datasets/doctest_grocery.sql
+-rw-r--r--   0        0        0     2200 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/datasets/grocery.sql
+-rw-r--r--   0        0        0     5757 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/datasets/healthcare.sql
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/docker/__init__.py
+-rw-r--r--   0        0        0     5298 2024-05-21 11:11:42.120524 featurebyte-1.0.3/featurebyte/docker/featurebyte.yml
+-rw-r--r--   0        0        0    11425 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/docker/manager.py
+-rw-r--r--   0        0        0    14311 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/enum.py
+-rw-r--r--   0        0        0    13577 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/exception.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/__init__.py
+-rw-r--r--   0        0        0     1277 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/enum.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/infra/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/__init__.py
+-rw-r--r--   0        0        0     1448 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/databricks.py
+-rw-r--r--   0        0        0    10967 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/spark_thrift.py
+-rw-r--r--   0        0        0     5933 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/spark_thrift_source.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/model/__init__.py
+-rw-r--r--   0        0        0    10089 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/model/feature_store.py
+-rw-r--r--   0        0        0     2821 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/model/online_store.py
+-rw-r--r--   0        0        0     1902 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/model/registry.py
+-rw-r--r--   0        0        0     2357 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/online_store/mysql.py
+-rw-r--r--   0        0        0     9825 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/patch.py
+-rw-r--r--   0        0        0     1011 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/registry_store.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/schema/__init__.py
+-rw-r--r--   0        0        0      771 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/schema/registry.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/service/__init__.py
+-rw-r--r--   0        0        0    12953 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/service/feature_store.py
+-rw-r--r--   0        0        0    15284 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/service/registry.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/utils/__init__.py
+-rw-r--r--   0        0        0     4818 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/utils/materialize_helper.py
+-rw-r--r--   0        0        0     5694 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/utils/on_demand_view.py
+-rw-r--r--   0        0        0    37630 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feast/utils/registry_construction.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feature_manager/__init__.py
+-rw-r--r--   0        0        0     2209 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feature_manager/model.py
+-rw-r--r--   0        0        0      786 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/feature_manager/sql_template.py
+-rw-r--r--   0        0        0     5167 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/list_utility.py
+-rw-r--r--   0        0        0     4194 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/logging.py
+-rw-r--r--   0        0        0     6793 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/middleware.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/__init__.py
+-rw-r--r--   0        0        0      605 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/migration_data_service.py
+-rw-r--r--   0        0        0     1704 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/model.py
+-rw-r--r--   0        0        0    13096 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/run.py
+-rw-r--r--   0        0        0     1443 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/service/__init__.py
+-rw-r--r--   0        0        0    10261 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/service/data_warehouse.py
+-rw-r--r--   0        0        0     7240 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/service/feature.py
+-rw-r--r--   0        0        0     8062 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/service/feature_list.py
+-rw-r--r--   0        0        0    10502 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/service/mixin.py
+-rw-r--r--   0        0        0     2015 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/migration/service/offline_store_feature_table.py
+-rw-r--r--   0        0        0      695 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/__init__.py
+-rw-r--r--   0        0        0    12569 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/base.py
+-rw-r--r--   0        0        0      393 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/base_feature_or_target_table.py
+-rw-r--r--   0        0        0      823 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/batch_feature_table.py
+-rw-r--r--   0        0        0     1774 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/batch_request_table.py
+-rw-r--r--   0        0        0     2887 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/catalog.py
+-rw-r--r--   0        0        0     2429 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/context.py
+-rw-r--r--   0        0        0    10406 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/credential.py
+-rw-r--r--   0        0        0     2309 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/deployment.py
+-rw-r--r--   0        0        0     2246 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/dimension_table.py
+-rw-r--r--   0        0        0     3666 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/entity.py
+-rw-r--r--   0        0        0     4065 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/entity_lookup_feature_table.py
+-rw-r--r--   0        0        0    22311 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/entity_universe.py
+-rw-r--r--   0        0        0     4395 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/entity_validation.py
+-rw-r--r--   0        0        0     3865 2024-05-21 11:11:12.752925 featurebyte-1.0.3/featurebyte/models/event_table.py
+-rw-r--r--   0        0        0    25037 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature.py
+-rw-r--r--   0        0        0     4320 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0    23841 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_list.py
+-rw-r--r--   0        0        0     3805 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_list_namespace.py
+-rw-r--r--   0        0        0    17063 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_list_store_info.py
+-rw-r--r--   0        0        0     4748 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_namespace.py
+-rw-r--r--   0        0        0      817 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_query_set.py
+-rw-r--r--   0        0        0     7919 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_store.py
+-rw-r--r--   0        0        0     2463 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/feature_table_cache_metadata.py
+-rw-r--r--   0        0        0     1025 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/historical_feature_table.py
+-rw-r--r--   0        0        0     3330 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/item_table.py
+-rw-r--r--   0        0        0     2076 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/materialized_table.py
+-rw-r--r--   0        0        0     1593 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/mixin.py
+-rw-r--r--   0        0        0     6730 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/observation_table.py
+-rw-r--r--   0        0        0    16178 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/offline_store_feature_table.py
+-rw-r--r--   0        0        0    23208 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/offline_store_ingest_query.py
+-rw-r--r--   0        0        0     4667 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/online_store.py
+-rw-r--r--   0        0        0     1846 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/online_store_compute_query.py
+-rw-r--r--   0        0        0     3959 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/online_store_spec.py
+-rw-r--r--   0        0        0     1296 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/online_store_table_version.py
+-rw-r--r--   0        0        0     7974 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/parent_serving.py
+-rw-r--r--   0        0        0     3118 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/periodic_task.py
+-rw-r--r--   0        0        0     1456 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/persistent.py
+-rw-r--r--   0        0        0    12926 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/precomputed_lookup_feature_table.py
+-rw-r--r--   0        0        0     1025 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/proxy_table.py
+-rw-r--r--   0        0        0     4415 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/relationship.py
+-rw-r--r--   0        0        0      945 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/relationship_analysis.py
+-rw-r--r--   0        0        0     8657 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/request_input.py
+-rw-r--r--   0        0        0     3934 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/scd_table.py
+-rw-r--r--   0        0        0     1483 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/semantic.py
+-rw-r--r--   0        0        0     1401 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/sqlglot_expression.py
+-rw-r--r--   0        0        0     1476 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/static_source_table.py
+-rw-r--r--   0        0        0     3275 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/target.py
+-rw-r--r--   0        0        0     2100 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/target_namespace.py
+-rw-r--r--   0        0        0      352 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/target_table.py
+-rw-r--r--   0        0        0     1409 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/task.py
+-rw-r--r--   0        0        0     5056 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/tile.py
+-rw-r--r--   0        0        0     1316 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/tile_job_log.py
+-rw-r--r--   0        0        0     3245 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/tile_registry.py
+-rw-r--r--   0        0        0     2058 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/use_case.py
+-rw-r--r--   0        0        0    10773 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/models/user_defined_function.py
+-rw-r--r--   0        0        0      155 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/persistent/__init__.py
+-rw-r--r--   0        0        0     9469 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/persistent/audit.py
+-rw-r--r--   0        0        0    25802 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/persistent/base.py
+-rw-r--r--   0        0        0    13459 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/persistent/mongo.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/__init__.py
+-rw-r--r--   0        0        0     3593 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/algorithm.py
+-rw-r--r--   0        0        0     3811 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/enum.py
+-rw-r--r--   0        0        0    26827 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/graph.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/graph_node/__init__.py
+-rw-r--r--   0        0        0     4593 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/graph_node/base.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/__init__.py
+-rw-r--r--   0        0        0     2229 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/column_info.py
+-rw-r--r--   0        0        0    12714 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/common_table.py
+-rw-r--r--   0        0        0     1708 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/critical_data_info.py
+-rw-r--r--   0        0        0    11860 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/entity_lookup_plan.py
+-rw-r--r--   0        0        0     9729 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/entity_relationship_info.py
+-rw-r--r--   0        0        0     8034 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/feature_job_setting.py
+-rw-r--r--   0        0        0    23206 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/graph.py
+-rw-r--r--   0        0        0    24631 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/model/table.py
+-rw-r--r--   0        0        0     1077 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/node/__init__.py
+-rw-r--r--   0        0        0     5411 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/node/agg_func.py
+-rw-r--r--   0        0        0    47828 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/node/base.py
+-rw-r--r--   0        0        0     8601 2024-05-21 11:11:12.756924 featurebyte-1.0.3/featurebyte/query_graph/node/binary.py
+-rw-r--r--   0        0        0    22086 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/cleaning_operation.py
+-rw-r--r--   0        0        0    31326 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/count_dict.py
+-rw-r--r--   0        0        0    16775 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/date.py
+-rw-r--r--   0        0        0     4600 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/distance.py
+-rw-r--r--   0        0        0    10951 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/function.py
+-rw-r--r--   0        0        0    87307 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/generic.py
+-rw-r--r--   0        0        0    17424 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/input.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/__init__.py
+-rw-r--r--   0        0        0      874 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/column.py
+-rw-r--r--   0        0        0     6405 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/config.py
+-rw-r--r--   0        0        0    22731 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/operation.py
+-rw-r--r--   0        0        0    20955 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/sdk_code.py
+-rw-r--r--   0        0        0     2035 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/templates/databricks_feature_spec.tpl
+-rw-r--r--   0        0        0      222 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/templates/on_demand_function.tpl
+-rw-r--r--   0        0        0      262 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/templates/on_demand_function_sql.tpl
+-rw-r--r--   0        0        0      261 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/templates/on_demand_view.tpl
+-rw-r--r--   0        0        0       47 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/metadata/templates/sdk_code.tpl
+-rw-r--r--   0        0        0     6487 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/mixin.py
+-rw-r--r--   0        0        0    30980 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/nested.py
+-rw-r--r--   0        0        0     5784 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/request.py
+-rw-r--r--   0        0        0     2077 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/scalar.py
+-rw-r--r--   0        0        0    10049 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/schema.py
+-rw-r--r--   0        0        0    12047 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/string.py
+-rw-r--r--   0        0        0    10073 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/unary.py
+-rw-r--r--   0        0        0     1247 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/utils.py
+-rw-r--r--   0        0        0     1043 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/validator.py
+-rw-r--r--   0        0        0     4599 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/node/vector.py
+-rw-r--r--   0        0        0      980 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/pruning_util.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/__init__.py
+-rw-r--r--   0        0        0     1092 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/__init__.py
+-rw-r--r--   0        0        0    25590 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/base.py
+-rw-r--r--   0        0        0    10464 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/databricks.py
+-rw-r--r--   0        0        0    20210 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/snowflake.py
+-rw-r--r--   0        0        0     1774 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/spark.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/__init__.py
+-rw-r--r--   0        0        0      580 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/asat.py
+-rw-r--r--   0        0        0    23225 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/base.py
+-rw-r--r--   0        0        0     8303 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/base_asat.py
+-rw-r--r--   0        0        0    11655 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/base_lookup.py
+-rw-r--r--   0        0        0     6703 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/forward.py
+-rw-r--r--   0        0        0      643 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/forward_asat.py
+-rw-r--r--   0        0        0     5988 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/item.py
+-rw-r--r--   0        0        0     3844 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/latest.py
+-rw-r--r--   0        0        0      917 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/lookup.py
+-rw-r--r--   0        0        0     1233 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/lookup_target.py
+-rw-r--r--   0        0        0     3812 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/request_table.py
+-rw-r--r--   0        0        0    28906 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/window.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/__init__.py
+-rw-r--r--   0        0        0     3853 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/aggregate.py
+-rw-r--r--   0        0        0    13423 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/base.py
+-rw-r--r--   0        0        0     2724 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/binary.py
+-rw-r--r--   0        0        0     6106 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/count_dict.py
+-rw-r--r--   0        0        0    10396 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/datetime.py
+-rw-r--r--   0        0        0     1493 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/distance.py
+-rw-r--r--   0        0        0     1908 2024-05-21 11:11:12.760924 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/function.py
+-rw-r--r--   0        0        0     7252 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/generic.py
+-rw-r--r--   0        0        0     2832 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/groupby.py
+-rw-r--r--   0        0        0     4527 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/input.py
+-rw-r--r--   0        0        0     1450 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/is_in.py
+-rw-r--r--   0        0        0     6238 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/join.py
+-rw-r--r--   0        0        0     7276 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/join_feature.py
+-rw-r--r--   0        0        0     2285 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/literal.py
+-rw-r--r--   0        0        0      879 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/request.py
+-rw-r--r--   0        0        0     8281 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/string.py
+-rw-r--r--   0        0        0    11800 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/tile.py
+-rw-r--r--   0        0        0     5171 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/track_changes.py
+-rw-r--r--   0        0        0     5817 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/unary.py
+-rw-r--r--   0        0        0     2615 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/util.py
+-rw-r--r--   0        0        0     1298 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/ast/vector.py
+-rw-r--r--   0        0        0     6249 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/batch_helper.py
+-rw-r--r--   0        0        0     7584 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/builder.py
+-rw-r--r--   0        0        0     5726 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/common.py
+-rw-r--r--   0        0        0     1705 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/dataframe.py
+-rw-r--r--   0        0        0     1388 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/deduplication.py
+-rw-r--r--   0        0        0     2785 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/entity.py
+-rw-r--r--   0        0        0     1866 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/expression.py
+-rw-r--r--   0        0        0    32139 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/feature_compute.py
+-rw-r--r--   0        0        0    14652 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/feature_historical.py
+-rw-r--r--   0        0        0     4643 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/feature_preview.py
+-rw-r--r--   0        0        0    12443 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/groupby_helper.py
+-rw-r--r--   0        0        0      427 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/__init__.py
+-rw-r--r--   0        0        0     3576 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/base.py
+-rw-r--r--   0        0        0    36508 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/preview.py
+-rw-r--r--   0        0        0     9344 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/tile.py
+-rw-r--r--   0        0        0     4959 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/materialisation.py
+-rw-r--r--   0        0        0     1352 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/offset.py
+-rw-r--r--   0        0        0    22643 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/online_serving.py
+-rw-r--r--   0        0        0     1306 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/online_serving_util.py
+-rw-r--r--   0        0        0    11586 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/online_store_compute_query.py
+-rw-r--r--   0        0        0     5952 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/parent_serving.py
+-rw-r--r--   0        0        0      937 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/query_graph_util.py
+-rw-r--r--   0        0        0    18610 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/scd_helper.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/__init__.py
+-rw-r--r--   0        0        0      473 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/aggregate_asat.py
+-rw-r--r--   0        0        0     2994 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/base_aggregate_asat.py
+-rw-r--r--   0        0        0     2908 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/base_lookup.py
+-rw-r--r--   0        0        0      503 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/forward_aggregate_asat.py
+-rw-r--r--   0        0        0     1909 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/lookup.py
+-rw-r--r--   0        0        0     2038 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/lookup_target.py
+-rw-r--r--   0        0        0    23298 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/specs.py
+-rw-r--r--   0        0        0     2282 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/template.py
+-rw-r--r--   0        0        0    12509 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/tile_compute.py
+-rw-r--r--   0        0        0     8104 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/tile_util.py
+-rw-r--r--   0        0        0    13070 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/tiling.py
+-rw-r--r--   0        0        0      634 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/sql/vector_helper.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/__init__.py
+-rw-r--r--   0        0        0     5621 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/base.py
+-rw-r--r--   0        0        0    23368 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/decompose_point.py
+-rw-r--r--   0        0        0     4998 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/definition.py
+-rw-r--r--   0        0        0     4926 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/flattening.py
+-rw-r--r--   0        0        0     6735 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/null_filling_value.py
+-rw-r--r--   0        0        0    14619 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/offline_store_ingest.py
+-rw-r--r--   0        0        0     9687 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/on_demand_function.py
+-rw-r--r--   0        0        0     7848 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/on_demand_view.py
+-rw-r--r--   0        0        0     6426 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/operation_structure.py
+-rw-r--r--   0        0        0    18995 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/pruning.py
+-rw-r--r--   0        0        0     3050 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/quick_pruning.py
+-rw-r--r--   0        0        0    10874 2024-05-21 11:11:12.764923 featurebyte-1.0.3/featurebyte/query_graph/transform/reconstruction.py
+-rw-r--r--   0        0        0    14046 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/query_graph/transform/sdk_code.py
+-rw-r--r--   0        0        0     7562 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/query_graph/util.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/__init__.py
+-rw-r--r--   0        0        0    11011 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/app_container_config.py
+-rw-r--r--   0        0        0     1371 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/base_materialized_table_router.py
+-rw-r--r--   0        0        0     7395 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/base_router.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/batch_feature_table/__init__.py
+-rw-r--r--   0        0        0     6242 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/batch_feature_table/api.py
+-rw-r--r--   0        0        0     5054 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/batch_feature_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/batch_request_table/__init__.py
+-rw-r--r--   0        0        0     6270 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/batch_request_table/api.py
+-rw-r--r--   0        0        0     3825 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/batch_request_table/controller.py
+-rw-r--r--   0        0        0      869 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/block_modification_handler.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/catalog/__init__.py
+-rw-r--r--   0        0        0     7177 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/catalog/api.py
+-rw-r--r--   0        0        0     1375 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/catalog/catalog_name_injector.py
+-rw-r--r--   0        0        0     6283 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/catalog/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/__init__.py
+-rw-r--r--   0        0        0    12213 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/base.py
+-rw-r--r--   0        0        0     6838 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/base_materialized_table.py
+-rw-r--r--   0        0        0    14364 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/base_table.py
+-rw-r--r--   0        0        0     2443 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/derive_primary_entity_helper.py
+-rw-r--r--   0        0        0     6043 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/feature_metadata_extractor.py
+-rw-r--r--   0        0        0     1862 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/feature_or_target_helper.py
+-rw-r--r--   0        0        0     9053 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/feature_or_target_table.py
+-rw-r--r--   0        0        0     2029 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/primary_entity_validator.py
+-rw-r--r--   0        0        0      865 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/common/schema.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/context/__init__.py
+-rw-r--r--   0        0        0     4803 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/context/api.py
+-rw-r--r--   0        0        0     8927 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/context/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/credential/__init__.py
+-rw-r--r--   0        0        0     4857 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/credential/api.py
+-rw-r--r--   0        0        0     2941 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/credential/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/deployment/__init__.py
+-rw-r--r--   0        0        0     8536 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/deployment/api.py
+-rw-r--r--   0        0        0    17831 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/deployment/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/dimension_table/__init__.py
+-rw-r--r--   0        0        0     7253 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/dimension_table/api.py
+-rw-r--r--   0        0        0     3687 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/dimension_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/entity/__init__.py
+-rw-r--r--   0        0        0     5324 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/entity/api.py
+-rw-r--r--   0        0        0     6400 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/entity/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/event_table/__init__.py
+-rw-r--r--   0        0        0     7954 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/event_table/api.py
+-rw-r--r--   0        0        0     4825 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/event_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature/__init__.py
+-rw-r--r--   0        0        0     7808 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature/api.py
+-rw-r--r--   0        0        0    15930 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_job_setting_analysis/__init__.py
+-rw-r--r--   0        0        0     7007 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_job_setting_analysis/api.py
+-rw-r--r--   0        0        0     6632 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_job_setting_analysis/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_list/__init__.py
+-rw-r--r--   0        0        0     9914 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_list/api.py
+-rw-r--r--   0        0        0    22617 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_list/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_list_namespace/__init__.py
+-rw-r--r--   0        0        0     5385 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_list_namespace/api.py
+-rw-r--r--   0        0        0     9741 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_list_namespace/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_namespace/__init__.py
+-rw-r--r--   0        0        0     4954 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_namespace/api.py
+-rw-r--r--   0        0        0     8157 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_namespace/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_store/__init__.py
+-rw-r--r--   0        0        0    12415 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_store/api.py
+-rw-r--r--   0        0        0    14714 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/feature_store/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/historical_feature_table/__init__.py
+-rw-r--r--   0        0        0     7510 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/historical_feature_table/api.py
+-rw-r--r--   0        0        0     5017 2024-05-21 11:11:12.768923 featurebyte-1.0.3/featurebyte/routes/historical_feature_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/item_table/__init__.py
+-rw-r--r--   0        0        0     6804 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/item_table/api.py
+-rw-r--r--   0        0        0     4327 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/item_table/controller.py
+-rw-r--r--   0        0        0     8258 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/lazy_app_container.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/observation_table/__init__.py
+-rw-r--r--   0        0        0     7436 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/observation_table/api.py
+-rw-r--r--   0        0        0     8926 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/observation_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/online_store/__init__.py
+-rw-r--r--   0        0        0     4259 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/online_store/api.py
+-rw-r--r--   0        0        0     2959 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/online_store/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/periodic_tasks/__init__.py
+-rw-r--r--   0        0        0     3060 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/periodic_tasks/api.py
+-rw-r--r--   0        0        0      541 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/periodic_tasks/controller.py
+-rw-r--r--   0        0        0    26521 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/registry.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/relationship_info/__init__.py
+-rw-r--r--   0        0        0     4663 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/relationship_info/api.py
+-rw-r--r--   0        0        0     5602 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/relationship_info/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/scd_table/__init__.py
+-rw-r--r--   0        0        0     6726 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/scd_table/api.py
+-rw-r--r--   0        0        0     4079 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/scd_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/semantic/__init__.py
+-rw-r--r--   0        0        0     3879 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/semantic/api.py
+-rw-r--r--   0        0        0     1458 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/semantic/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/static_source_table/__init__.py
+-rw-r--r--   0        0        0     6244 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/static_source_table/api.py
+-rw-r--r--   0        0        0     3808 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/static_source_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/table/__init__.py
+-rw-r--r--   0        0        0     1736 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/table/api.py
+-rw-r--r--   0        0        0      465 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target/__init__.py
+-rw-r--r--   0        0        0     5262 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target/api.py
+-rw-r--r--   0        0        0     8861 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target_namespace/__init__.py
+-rw-r--r--   0        0        0     5668 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target_namespace/api.py
+-rw-r--r--   0        0        0     2948 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target_namespace/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target_table/__init__.py
+-rw-r--r--   0        0        0     7933 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target_table/api.py
+-rw-r--r--   0        0        0     4834 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/target_table/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/task/__init__.py
+-rw-r--r--   0        0        0     2125 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/task/api.py
+-rw-r--r--   0        0        0     2332 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/task/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/temp_data/__init__.py
+-rw-r--r--   0        0        0      933 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/temp_data/api.py
+-rw-r--r--   0        0        0     1354 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/temp_data/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/use_case/__init__.py
+-rw-r--r--   0        0        0     7288 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/use_case/api.py
+-rw-r--r--   0        0        0    13699 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/use_case/controller.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/user_defined_function/__init__.py
+-rw-r--r--   0        0        0     6072 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/user_defined_function/api.py
+-rw-r--r--   0        0        0    10313 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/routes/user_defined_function/controller.py
+-rw-r--r--   0        0        0      181 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/__init__.py
+-rw-r--r--   0        0        0     1428 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/batch_feature_table.py
+-rw-r--r--   0        0        0      843 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/batch_request_table.py
+-rw-r--r--   0        0        0     1796 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/catalog.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/common/__init__.py
+-rw-r--r--   0        0        0     1227 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/common/base.py
+-rw-r--r--   0        0        0      640 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/common/feature_or_target.py
+-rw-r--r--   0        0        0     3663 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/common/operation.py
+-rw-r--r--   0        0        0     2807 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/context.py
+-rw-r--r--   0        0        0     3438 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/credential.py
+-rw-r--r--   0        0        0     1902 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/deployment.py
+-rw-r--r--   0        0        0     1193 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/dimension_table.py
+-rw-r--r--   0        0        0     1685 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/entity.py
+-rw-r--r--   0        0        0     2689 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/event_table.py
+-rw-r--r--   0        0        0     6297 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature.py
+-rw-r--r--   0        0        0     5913 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0     8199 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature_list.py
+-rw-r--r--   0        0        0     1474 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature_list_namespace.py
+-rw-r--r--   0        0        0     2059 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature_namespace.py
+-rw-r--r--   0        0        0     3682 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature_store.py
+-rw-r--r--   0        0        0      421 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/feature_table_cache_metadata.py
+-rw-r--r--   0        0        0     1631 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/historical_feature_table.py
+-rw-r--r--   0        0        0    13468 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/info.py
+-rw-r--r--   0        0        0     1217 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/item_table.py
+-rw-r--r--   0        0        0      600 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/materialized_table.py
+-rw-r--r--   0        0        0     2259 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/observation_table.py
+-rw-r--r--   0        0        0     1732 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/online_store.py
+-rw-r--r--   0        0        0      460 2024-05-21 11:11:12.772923 featurebyte-1.0.3/featurebyte/schema/periodic_task.py
+-rw-r--r--   0        0        0     1033 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/preview.py
+-rw-r--r--   0        0        0     1541 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/relationship_info.py
+-rw-r--r--   0        0        0     1187 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/request_table.py
+-rw-r--r--   0        0        0     1737 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/scd_table.py
+-rw-r--r--   0        0        0      928 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/semantic.py
+-rw-r--r--   0        0        0     1149 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/static_source_table.py
+-rw-r--r--   0        0        0     4564 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/table.py
+-rw-r--r--   0        0        0     2635 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/target.py
+-rw-r--r--   0        0        0     1839 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/target_namespace.py
+-rw-r--r--   0        0        0     3342 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/target_table.py
+-rw-r--r--   0        0        0     1773 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/task.py
+-rw-r--r--   0        0        0     2630 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/use_case.py
+-rw-r--r--   0        0        0     2686 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/user_defined_function.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/__init__.py
+-rw-r--r--   0        0        0      406 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/progress.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/__init__.py
+-rw-r--r--   0        0        0     3285 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/base.py
+-rw-r--r--   0        0        0      740 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/batch_feature_create.py
+-rw-r--r--   0        0        0      662 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/batch_feature_table.py
+-rw-r--r--   0        0        0      654 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/batch_request_table.py
+-rw-r--r--   0        0        0      858 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/data_description.py
+-rw-r--r--   0        0        0     1571 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/deployment_create_update.py
+-rw-r--r--   0        0        0     1426 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0      803 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/feature_list_batch_feature_create.py
+-rw-r--r--   0        0        0     1114 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/feature_list_create.py
+-rw-r--r--   0        0        0      748 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/feature_list_make_production_ready.py
+-rw-r--r--   0        0        0      794 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/historical_feature_table.py
+-rw-r--r--   0        0        0     1351 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/materialized_table_delete.py
+-rw-r--r--   0        0        0      821 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/observation_table.py
+-rw-r--r--   0        0        0     1033 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/observation_table_upload.py
+-rw-r--r--   0        0        0      496 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/online_store_cleanup.py
+-rw-r--r--   0        0        0      622 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/online_store_initialize.py
+-rw-r--r--   0        0        0      593 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/scheduled_feature_materialize.py
+-rw-r--r--   0        0        0      603 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/static_source_table.py
+-rw-r--r--   0        0        0     1171 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/target_table.py
+-rw-r--r--   0        0        0      963 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/test.py
+-rw-r--r--   0        0        0      511 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/schema/worker/task/tile.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/__init__.py
+-rw-r--r--   0        0        0    40617 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/base_document.py
+-rw-r--r--   0        0        0     4167 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/base_feature_service.py
+-rw-r--r--   0        0        0     5951 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/base_table_document.py
+-rw-r--r--   0        0        0     1930 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/batch_feature_table.py
+-rw-r--r--   0        0        0     3554 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/batch_request_table.py
+-rw-r--r--   0        0        0     3749 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/catalog.py
+-rw-r--r--   0        0        0     5741 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/context.py
+-rw-r--r--   0        0        0     6652 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/credential.py
+-rw-r--r--   0        0        0    38984 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/deploy.py
+-rw-r--r--   0        0        0      681 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/deployment.py
+-rw-r--r--   0        0        0      675 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/dimension_table.py
+-rw-r--r--   0        0        0     3402 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/entity.py
+-rw-r--r--   0        0        0     5854 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/entity_lookup_feature_table.py
+-rw-r--r--   0        0        0    12732 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/entity_relationship_extractor.py
+-rw-r--r--   0        0        0     6440 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/entity_serving_names.py
+-rw-r--r--   0        0        0    15424 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/entity_validation.py
+-rw-r--r--   0        0        0      619 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/event_table.py
+-rw-r--r--   0        0        0      365 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/exception.py
+-rw-r--r--   0        0        0    16640 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature.py
+-rw-r--r--   0        0        0    10949 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_facade.py
+-rw-r--r--   0        0        0     5130 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0    28450 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_list.py
+-rw-r--r--   0        0        0     7269 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_list_facade.py
+-rw-r--r--   0        0        0      583 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_list_namespace.py
+-rw-r--r--   0        0        0     4261 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_list_status.py
+-rw-r--r--   0        0        0    25566 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_manager.py
+-rw-r--r--   0        0        0    58094 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_materialize.py
+-rw-r--r--   0        0        0     4177 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_materialize_scheduler.py
+-rw-r--r--   0        0        0      575 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_namespace.py
+-rw-r--r--   0        0        0    14426 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_offline_store_info.py
+-rw-r--r--   0        0        0    19168 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_preview.py
+-rw-r--r--   0        0        0    15048 2024-05-21 11:11:12.776923 featurebyte-1.0.3/featurebyte/service/feature_readiness.py
+-rw-r--r--   0        0        0     1469 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/feature_store.py
+-rw-r--r--   0        0        0    13834 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/feature_store_warehouse.py
+-rw-r--r--   0        0        0    29624 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/feature_table_cache.py
+-rw-r--r--   0        0        0     4321 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/feature_table_cache_metadata.py
+-rw-r--r--   0        0        0     4075 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/historical_feature_table.py
+-rw-r--r--   0        0        0     8854 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/historical_features.py
+-rw-r--r--   0        0        0    13380 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/historical_features_and_target.py
+-rw-r--r--   0        0        0     1605 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/item_table.py
+-rw-r--r--   0        0        0     8058 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/materialized_table.py
+-rw-r--r--   0        0        0     4344 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/mixin.py
+-rw-r--r--   0        0        0     5518 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/namespace_handler.py
+-rw-r--r--   0        0        0    33358 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/observation_table.py
+-rw-r--r--   0        0        0    13137 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/offline_store_feature_table.py
+-rw-r--r--   0        0        0     7272 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/offline_store_feature_table_comment.py
+-rw-r--r--   0        0        0    12152 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/offline_store_feature_table_construction.py
+-rw-r--r--   0        0        0    33532 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/offline_store_feature_table_manager.py
+-rw-r--r--   0        0        0     7847 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_enable.py
+-rw-r--r--   0        0        0    18574 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_serving.py
+-rw-r--r--   0        0        0     5949 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_store.py
+-rw-r--r--   0        0        0     4520 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_store_cleanup.py
+-rw-r--r--   0        0        0     3798 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_store_cleanup_scheduler.py
+-rw-r--r--   0        0        0     2696 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_store_compute_query_service.py
+-rw-r--r--   0        0        0     3229 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/online_store_table_version.py
+-rw-r--r--   0        0        0     6793 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/parent_serving.py
+-rw-r--r--   0        0        0      438 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/periodic_task.py
+-rw-r--r--   0        0        0    10566 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/preview.py
+-rw-r--r--   0        0        0     9332 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/relationship.py
+-rw-r--r--   0        0        0     2311 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/relationship_info.py
+-rw-r--r--   0        0        0      992 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/sanitizer.py
+-rw-r--r--   0        0        0      591 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/scd_table.py
+-rw-r--r--   0        0        0      601 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/semantic.py
+-rw-r--r--   0        0        0     3019 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/session_manager.py
+-rw-r--r--   0        0        0     6700 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/session_validator.py
+-rw-r--r--   0        0        0     6050 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/specialized_dtype.py
+-rw-r--r--   0        0        0     2899 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/static_source_table.py
+-rw-r--r--   0        0        0     1424 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/table.py
+-rw-r--r--   0        0        0    17947 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/table_columns_info.py
+-rw-r--r--   0        0        0     6941 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/table_facade.py
+-rw-r--r--   0        0        0     3284 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/table_info.py
+-rw-r--r--   0        0        0     2081 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/table_status.py
+-rw-r--r--   0        0        0    10321 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/target.py
+-rw-r--r--   0        0        0     6758 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/target_helper/base_feature_or_target_computer.py
+-rw-r--r--   0        0        0     4936 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/target_helper/compute_target.py
+-rw-r--r--   0        0        0      536 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/target_namespace.py
+-rw-r--r--   0        0        0     4716 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/target_table.py
+-rw-r--r--   0        0        0    12076 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/task_manager.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/templates/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/templates/online_serving/__init__.py
+-rw-r--r--   0        0        0      707 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/templates/online_serving/python.tpl
+-rw-r--r--   0        0        0      132 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/templates/online_serving/sh.tpl
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile/__init__.py
+-rw-r--r--   0        0        0    11901 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile/tile_task_executor.py
+-rw-r--r--   0        0        0     3568 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile_cache.py
+-rw-r--r--   0        0        0     7046 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile_job_log.py
+-rw-r--r--   0        0        0    14271 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile_manager.py
+-rw-r--r--   0        0        0     3759 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile_registry_service.py
+-rw-r--r--   0        0        0     2520 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/tile_scheduler.py
+-rw-r--r--   0        0        0     2510 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/use_case.py
+-rw-r--r--   0        0        0     2664 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/user_defined_function.py
+-rw-r--r--   0        0        0     1143 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/user_service.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/validator/__init__.py
+-rw-r--r--   0        0        0     5066 2024-05-21 11:11:12.780922 featurebyte-1.0.3/featurebyte/service/validator/entity_relationship_validator.py
+-rw-r--r--   0        0        0     7015 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/service/validator/materialized_table_delete.py
+-rw-r--r--   0        0        0    11727 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/service/validator/production_ready_validator.py
+-rw-r--r--   0        0        0    16097 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/service/version.py
+-rw-r--r--   0        0        0    13080 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/service/view_construction.py
+-rw-r--r--   0        0        0     4640 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/service/working_schema.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/__init__.py
+-rw-r--r--   0        0        0    41139 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/base.py
+-rw-r--r--   0        0        0    22306 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/base_spark.py
+-rw-r--r--   0        0        0     5686 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/databricks.py
+-rw-r--r--   0        0        0     7470 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/databricks_unity.py
+-rw-r--r--   0        0        0      477 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/enum.py
+-rw-r--r--   0        0        0     5024 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/hive.py
+-rw-r--r--   0        0        0     7234 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/manager.py
+-rw-r--r--   0        0        0     5935 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/session_helper.py
+-rw-r--r--   0        0        0     9180 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/simple_storage.py
+-rw-r--r--   0        0        0    18153 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/snowflake.py
+-rw-r--r--   0        0        0    10650 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/spark.py
+-rw-r--r--   0        0        0     4175 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/sqlite.py
+-rw-r--r--   0        0        0     6616 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/session/webhdfs.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/__init__.py
+-rw-r--r--   0        0        0     2390 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/base.py
+-rw-r--r--   0        0        0     1106 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/common.py
+-rw-r--r--   0        0        0        6 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks/.gitignore
+-rw-r--r--   0        0        0      825 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_COSINE_SIMILARITY.sql
+-rw-r--r--   0        0        0      371 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_ENTROPY.sql
+-rw-r--r--   0        0        0      212 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_LEAST_FREQUENT.sql
+-rw-r--r--   0        0        0      212 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT.sql
+-rw-r--r--   0        0        0      703 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql
+-rw-r--r--   0        0        0      217 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT_VALUE.sql
+-rw-r--r--   0        0        0      172 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_NUM_UNIQUE.sql
+-rw-r--r--   0        0        0      828 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_GET_RANK.sql
+-rw-r--r--   0        0        0      359 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_GET_RELATIVE_FREQUENCY.sql
+-rw-r--r--   0        0        0      413 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_INDEX_TO_TIMESTAMP.sql
+-rw-r--r--   0        0        0      203 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_OBJECT_DELETE.sql
+-rw-r--r--   0        0        0      377 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_TIMESTAMP_TO_INDEX.sql
+-rw-r--r--   0        0        0      579 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_TIMEZONE_OFFSET_TO_SECOND.sql
+-rw-r--r--   0        0        0     1004 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql
+-rw-r--r--   0        0        0      548 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_ENTROPY.sql
+-rw-r--r--   0        0        0      178 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_LEAST_FREQUENT.sql
+-rw-r--r--   0        0        0      178 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT.sql
+-rw-r--r--   0        0        0      777 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql
+-rw-r--r--   0        0        0      184 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_VALUE.sql
+-rw-r--r--   0        0        0      188 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_NUM_UNIQUE.sql
+-rw-r--r--   0        0        0     1491 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_GET_RANK.sql
+-rw-r--r--   0        0        0      449 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_GET_RELATIVE_FREQUENCY.sql
+-rw-r--r--   0        0        0      559 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql
+-rw-r--r--   0        0        0      559 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql
+-rw-r--r--   0        0        0      653 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql
+-rw-r--r--   0        0        0      863 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_AVG.sql
+-rw-r--r--   0        0        0      614 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_MAX.sql
+-rw-r--r--   0        0        0      841 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SIMPLE_AVERAGE.sql
+-rw-r--r--   0        0        0      570 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SUM.sql
+-rw-r--r--   0        0        0      852 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_COSINE_SIMILARITY.sql
+-rw-r--r--   0        0        0      153 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/snowflake/T_TILE_MONITOR_SUMMARY.sql
+-rw-r--r--   0        0        0        6 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/spark/.gitignore
+-rw-r--r--   0        0        0      191 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/spark/T_TILE_MONITOR_SUMMARY.sql
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/spark/__init__.py
+-rw-r--r--   0        0        0    48525 2024-05-21 11:12:39.867957 featurebyte-1.0.3/featurebyte/sql/spark/featurebyte-hive-udf-1.0.8-SNAPSHOT-all.jar
+-rw-r--r--   0        0        0     1924 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/tile_common.py
+-rw-r--r--   0        0        0     5569 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/tile_generate.py
+-rw-r--r--   0        0        0     3757 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/tile_generate_entity_tracking.py
+-rw-r--r--   0        0        0     8514 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/tile_monitor.py
+-rw-r--r--   0        0        0     2949 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/tile_registry.py
+-rw-r--r--   0        0        0     6698 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/sql/tile_schedule_online_store.py
+-rw-r--r--   0        0        0      373 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/storage/__init__.py
+-rw-r--r--   0        0        0     5124 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/storage/azure.py
+-rw-r--r--   0        0        0     9767 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/storage/base.py
+-rw-r--r--   0        0        0     3642 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/storage/local.py
+-rw-r--r--   0        0        0      416 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/storage/local_temp.py
+-rw-r--r--   0        0        0     5677 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/storage/s3.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/tile/__init__.py
+-rw-r--r--   0        0        0      412 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/tile/sql_template.py
+-rw-r--r--   0        0        0    34447 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/tile/tile_cache.py
+-rw-r--r--   0        0        0     2660 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/typing.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/__init__.py
+-rw-r--r--   0        0        0     1556 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/async_helper.py
+-rw-r--r--   0        0        0     1922 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/credential.py
+-rw-r--r--   0        0        0     1606 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/messaging.py
+-rw-r--r--   0        0        0      520 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/persistent.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/snowflake/__init__.py
+-rw-r--r--   0        0        0      463 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/snowflake/sql.py
+-rw-r--r--   0        0        0     3749 2024-05-21 11:11:12.784922 featurebyte-1.0.3/featurebyte/utils/storage.py
+-rw-r--r--   0        0        0     6562 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/__init__.py
+-rw-r--r--   0        0        0     3656 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/registry.py
+-rw-r--r--   0        0        0     2503 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/schedulers.py
+-rw-r--r--   0        0        0      294 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/start.py
+-rw-r--r--   0        0        0      215 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/__init__.py
+-rw-r--r--   0        0        0     4167 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/base.py
+-rw-r--r--   0        0        0     1213 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/batch_feature_create.py
+-rw-r--r--   0        0        0     4555 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/batch_feature_table.py
+-rw-r--r--   0        0        0     3021 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/batch_request_table.py
+-rw-r--r--   0        0        0     5288 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/catalog_online_store_update.py
+-rw-r--r--   0        0        0     1859 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/data_description.py
+-rw-r--r--   0        0        0     4003 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/deployment_create_update.py
+-rw-r--r--   0        0        0     6260 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/feature_job_setting_analysis.py
+-rw-r--r--   0        0        0     5211 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/feature_job_setting_analysis_backtest.py
+-rw-r--r--   0        0        0     4015 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/feature_list_batch_feature_create.py
+-rw-r--r--   0        0        0     4067 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/feature_list_create.py
+-rw-r--r--   0        0        0     1930 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/feature_list_make_production_ready.py
+-rw-r--r--   0        0        0     4084 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/historical_feature_table.py
+-rw-r--r--   0        0        0     8013 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/materialized_table_delete.py
+-rw-r--r--   0        0        0     1853 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/mixin.py
+-rw-r--r--   0        0        0     3781 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/observation_table.py
+-rw-r--r--   0        0        0     4800 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/observation_table_upload.py
+-rw-r--r--   0        0        0     1214 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/online_store_cleanup.py
+-rw-r--r--   0        0        0     1670 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/scheduled_feature_materialize.py
+-rw-r--r--   0        0        0     2922 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/static_source_table.py
+-rw-r--r--   0        0        0     7360 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/target_table.py
+-rw-r--r--   0        0        0     1142 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/test_task.py
+-rw-r--r--   0        0        0     1758 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task/tile_task.py
+-rw-r--r--   0        0        0    10148 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/task_executor.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/test_util/__init__.py
+-rw-r--r--   0        0        0     3139 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/test_util/random_task.py
+-rw-r--r--   0        0        0        0 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/util/__init__.py
+-rw-r--r--   0        0        0    17306 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/util/batch_feature_creator.py
+-rw-r--r--   0        0        0     1774 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/util/observation_set_helper.py
+-rw-r--r--   0        0        0     1405 2024-05-21 11:11:12.788922 featurebyte-1.0.3/featurebyte/worker/util/task_progress_updater.py
+-rw-r--r--   0        0        0     8362 2024-05-21 11:11:41.080536 featurebyte-1.0.3/pyproject.toml
+-rw-r--r--   0        0        0    23483 1970-01-01 00:00:00.000000 featurebyte-1.0.3/PKG-INFO
```

### Comparing `featurebyte-1.0.2/LICENSE` & `featurebyte-1.0.3/LICENSE`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/README.md` & `featurebyte-1.0.3/README.md`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/__init__.py` & `featurebyte-1.0.3/featurebyte/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Python Library for FeatureOps"""
+
 from typing import Any, List, Optional
 
 import os
 import shutil
 import sys
 
 import pandas as pd
@@ -131,25 +132,25 @@
         If no profile is found in configuration file
     """
     conf = Configurations()
 
     # check if we are in DataBricks environment and valid secrets are present create a profile automatically
     try:
         from databricks.sdk.runtime import dbutils  # pylint: disable=import-outside-toplevel
-        from pyspark.errors.exceptions.captured import (  # pylint: disable=import-outside-toplevel
-            IllegalArgumentException,
-        )
 
         if len(conf.profiles) == 1 and conf.profiles[0].name == "local":
             api_url = None
             api_token = None
             try:
                 api_url = dbutils.secrets.get(scope="featurebyte", key="api-url")
                 api_token = dbutils.secrets.get(scope="featurebyte", key="api-token")
-            except IllegalArgumentException:
+            except Exception as databricks_exc:  # pylint: disable=broad-exception-caught
+                # use a more generic exception to avoid dependency on databricks sdk blocking
+                # the rest of the code
+                logger.warning(f"Failed to get secrets from Databricks: {databricks_exc}")
                 logger.info(
                     "Add the secrets (featurebyte.api-url, featurebyte.api-token) for auto profile creation:\n"
                     "databricks secrets create-scope --scope featurebyte\n"
                     "databricks secrets put --scope featurebyte --key api-url --string-value <api-url>\n"
                     "databricks secrets put --scope featurebyte --key api-token --string-value <api-token>"
                 )
             if api_url and api_token:
```

### Comparing `featurebyte-1.0.2/featurebyte/__main__.py` & `featurebyte-1.0.3/featurebyte/__main__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Featurebyte CLI tools
 """
+
 import typer
 from rich.text import Text
 
 from featurebyte import version
 from featurebyte.datasets.app import app as datasets_app
 from featurebyte.docker.manager import (
     ApplicationName,
```

### Comparing `featurebyte-1.0.2/featurebyte/api/aggregator/asat_aggregator.py` & `featurebyte-1.0.3/featurebyte/api/aggregator/forward_aggregator.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,151 +1,172 @@
 """
-This module contains as at aggregator related class
+Forward aggregator module
 """
-from __future__ import annotations
 
-from typing import List, Optional, Type, cast
+from __future__ import annotations
 
-from typeguard import typechecked
+from typing import Any, List, Optional, Type, cast
 
 from featurebyte.api.aggregator.base_aggregator import BaseAggregator
-from featurebyte.api.feature import Feature
-from featurebyte.api.scd_view import SCDView
+from featurebyte.api.change_view import ChangeView
+from featurebyte.api.event_view import EventView
+from featurebyte.api.item_view import ItemView
+from featurebyte.api.target import Target
 from featurebyte.api.view import View
-from featurebyte.common.model_util import validate_offset_string
-from featurebyte.common.typing import OptionalScalar
+from featurebyte.common.model_util import parse_duration_string
 from featurebyte.enum import AggFunc
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.node.agg_func import construct_agg_func
+from featurebyte.typing import OptionalScalar
 
 
-class AsAtAggregator(BaseAggregator):
+class ForwardAggregator(BaseAggregator):
     """
-    AsAtAggregator implements the aggregate_asat method for GroupBy
+    ForwardAggregator implements the forward_aggregate method for GroupBy
     """
 
     @property
     def supported_views(self) -> List[Type[View]]:
-        return [SCDView]
+        return [EventView, ItemView, ChangeView]
 
     @property
     def aggregation_method_name(self) -> str:
-        return "aggregate_asat"
+        return "forward_aggregate"
 
-    @typechecked
-    def aggregate_asat(
+    def forward_aggregate(
         self,
-        value_column: Optional[str] = None,
-        method: Optional[str] = None,
-        feature_name: Optional[str] = None,
-        offset: Optional[str] = None,
-        backward: bool = True,
+        value_column: str,
+        method: str,
+        window: Optional[str] = None,
+        target_name: Optional[str] = None,
         fill_value: OptionalScalar = None,
         skip_fill_na: bool = False,
-    ) -> Feature:
+    ) -> Target:
         """
-        Aggregate a column in SlowlyChangingView as at a point in time
+        Aggregate given value_column for each group specified in keys over a time window.
 
         Parameters
         ----------
-        value_column: Optional[str]
+        value_column: str
             Column to be aggregated
         method: str
             Aggregation method
-        feature_name: str
-            Output feature name
-        offset: Optional[str]
-            Optional offset to apply to the point in time column in the feature request. The
-            aggregation result will be as at the point in time adjusted by this offset. Format of
-            offset is "{size}{unit}", where size is a positive integer and unit is one of the
-            following:
-
-            "ns": nanosecond
-            "us": microsecond
-            "ms": millisecond
-            "s": second
-            "m": minute
-            "h": hour
-            "d": day
-            "w": week
-
-        backward: bool
-            Whether the offset should be applied backward or forward
+        window: str
+            Window of the aggregation
+        target_name: str
+            Name of the target column
         fill_value: OptionalScalar
             Value to fill if the value in the column is empty
         skip_fill_na: bool
-            Whether to skip filling na values
+            Whether to skip filling NaN values
 
         Returns
         -------
-        Feature
+        Target
         """
+        # Validation
         self._validate_parameters(
+            value_column=value_column,
             method=method,
+            window=window,
+            target_name=target_name,
+        )
+        self._validate_fill_value_and_skip_fill_na(fill_value=fill_value, skip_fill_na=skip_fill_na)
+        # Create new node parameters
+        node_params = self._prepare_node_parameters(
             value_column=value_column,
-            feature_name=feature_name,
-            offset=offset,
-            fill_value=fill_value,
-            skip_fill_na=skip_fill_na,
+            method=method,
+            window=window,
+            target_name=target_name,
+            timestamp_col=self.view.timestamp_column,
         )
+        # Add forward aggregate node to graph.
+        forward_aggregate_node = self.view.graph.add_operation(
+            node_type=NodeType.FORWARD_AGGREGATE,
+            node_params=node_params,
+            node_output_type=NodeOutputType.FRAME,
+            input_nodes=[self.view.node],
+        )
+        agg_method = construct_agg_func(agg_func=cast(AggFunc, method))
+        output_var_type = self.get_output_var_type(agg_method, method, value_column)
+        # Project, build and return Target
+        assert target_name is not None
+        target = self.view.project_target_from_node(
+            forward_aggregate_node, target_name, output_var_type
+        )
+        if not skip_fill_na:
+            return self._fill_feature_or_target(target, method, target_name, fill_value)  # type: ignore[return-value]
+        return target
 
-        view = cast(SCDView, self.view)
-        node_params = {
+    def _prepare_node_parameters(
+        self,
+        value_column: Optional[str],
+        method: str,
+        window: Optional[str],
+        target_name: Optional[str],
+        timestamp_col: Optional[str],
+    ) -> dict[str, Any]:
+        """
+        Helper function to prepare node parameters.
+
+        Parameters
+        ----------
+        value_column: Optional[str]
+            Column to be aggregated
+        method: str
+            Aggregation method
+        window: str
+            Window of the aggregation
+        target_name: str
+            Name of the target column
+        timestamp_col: str
+            Timestamp column
+
+        Returns
+        -------
+        dict[str, Any]
+        """
+        return {
             "keys": self.keys,
             "parent": value_column,
             "agg_func": method,
-            "value_by": self.category,
-            "name": feature_name,
+            "window": window,
+            "name": target_name,
             "serving_names": self.serving_names,
+            "value_by": self.category,
             "entity_ids": self.entity_ids,
-            "offset": offset,
-            "backward": backward,
-            **view.get_common_scd_parameters().dict(),
+            "timestamp_col": timestamp_col,
         }
-        groupby_node = self.view.graph.add_operation(
-            node_type=NodeType.AGGREGATE_AS_AT,
-            node_params=node_params,
-            node_output_type=NodeOutputType.FRAME,
-            input_nodes=[self.view.node],
-        )
-
-        assert method is not None
-        assert feature_name is not None
-        agg_method = construct_agg_func(agg_func=cast(AggFunc, method))
-
-        return self._project_feature_from_groupby_node(
-            agg_method=agg_method,
-            feature_name=feature_name,
-            groupby_node=groupby_node,
-            method=method,
-            value_column=value_column,
-            fill_value=fill_value,
-            skip_fill_na=skip_fill_na,
-        )
 
     def _validate_parameters(
         self,
-        method: Optional[str],
-        feature_name: Optional[str],
-        value_column: Optional[str],
-        offset: Optional[str],
-        fill_value: OptionalScalar,
-        skip_fill_na: bool,
+        value_column: str,
+        method: Optional[str] = None,
+        window: Optional[str] = None,
+        target_name: Optional[str] = None,
     ) -> None:
-        self._validate_method_and_value_column(method=method, value_column=value_column)
-        self._validate_fill_value_and_skip_fill_na(fill_value=fill_value, skip_fill_na=skip_fill_na)
-
-        if method == AggFunc.LATEST:
-            raise ValueError("latest aggregation method is not supported for aggregated_asat")
+        """
+        Helper function to validate parameters.
 
-        if feature_name is None:
-            raise ValueError("feature_name is required")
+        Parameters
+        ----------
+        value_column: str
+            Column to be aggregated
+        method: str
+            Aggregation method
+        window: str
+            Window of the aggregation
+        target_name: str
+            Name of the target column
+
+        Raises
+        ------
+        ValueError
+            raised when target is not specified
+        """
+        self._validate_method_and_value_column(method=method, value_column=value_column)
 
-        view = cast(SCDView, self.view)
-        for key in self.keys:
-            if key == view.natural_key_column:
-                raise ValueError(
-                    "Natural key column cannot be used as a groupby key in aggregate_asat"
-                )
+        if not target_name:
+            raise ValueError("Target name must be specified")
 
-        if offset is not None:
-            validate_offset_string(offset)
+        if window:
+            parse_duration_string(window)
```

### Comparing `featurebyte-1.0.2/featurebyte/api/aggregator/base_aggregator.py` & `featurebyte-1.0.3/featurebyte/api/aggregator/base_aggregator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 """
 This module contains base aggregator related class
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, Type, Union
 
 from abc import ABC, abstractmethod
 
 from featurebyte.api.aggregator.vector_validator import validate_vector_aggregate_parameters
 from featurebyte.api.feature import Feature
 from featurebyte.api.target import Target
 from featurebyte.api.view import View
-from featurebyte.common.typing import OptionalScalar, get_or_default
 from featurebyte.enum import AggFunc, DBVarType
 from featurebyte.exception import AggregationNotSupportedForViewError
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.agg_func import AggFuncType
+from featurebyte.typing import OptionalScalar, get_or_default
 
 
 class BaseAggregator(ABC):
     """
     BaseAggregator is the base class for aggregators in groupby
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/api/aggregator/simple_aggregator.py` & `featurebyte-1.0.3/featurebyte/api/aggregator/simple_aggregator.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 """
 This module contains simple aggregator related class
 """
+
 from __future__ import annotations
 
-from typing import List, Literal, Optional, Type
+from typing import List, Optional, Type
+from typing_extensions import Literal
 
 from featurebyte.api.aggregator.base_aggregator import BaseAggregator
 from featurebyte.api.feature import Feature
 from featurebyte.api.item_view import ItemView
 from featurebyte.api.view import View
-from featurebyte.common.typing import OptionalScalar
 from featurebyte.enum import AggFunc
 from featurebyte.query_graph.node.agg_func import construct_agg_func
 from featurebyte.query_graph.transform.reconstruction import (
     ItemGroupbyNode,
     add_pruning_sensitive_operation,
 )
+from featurebyte.typing import OptionalScalar
 
 
 class SimpleAggregator(BaseAggregator):
     """
     SimpleAggregator implements the aggregate method for GroupBy
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/api/aggregator/vector_validator.py` & `featurebyte-1.0.3/featurebyte/api/aggregator/vector_validator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Vector aggregate validator
 """
+
 from typing import List, Optional
 
 from featurebyte.enum import AggFunc, DBVarType
 from featurebyte.query_graph.model.column_info import ColumnInfo
 
 VECTOR_AGGREGATE_SUPPORTED_FUNCTIONS = {AggFunc.MAX, AggFunc.AVG, AggFunc.SUM}
```

### Comparing `featurebyte-1.0.2/featurebyte/api/aggregator/window_aggregator.py` & `featurebyte-1.0.3/featurebyte/api/aggregator/window_aggregator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 """
 This module contains window aggregator related class
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Type, cast
 
 import os
 
 from featurebyte.api.aggregator.base_aggregator import BaseAggregator
 from featurebyte.api.change_view import ChangeView
 from featurebyte.api.event_view import EventView
 from featurebyte.api.feature_group import FeatureGroup
 from featurebyte.api.item_view import ItemView
 from featurebyte.api.view import View
 from featurebyte.api.window_validator import validate_window
-from featurebyte.common.typing import OptionalScalar
 from featurebyte.enum import AggFunc
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.query_graph.node.agg_func import construct_agg_func
 from featurebyte.query_graph.transform.reconstruction import (
     GroupByNode,
     add_pruning_sensitive_operation,
 )
+from featurebyte.typing import OptionalScalar
 
 
 class WindowAggregator(BaseAggregator):
     """
     WindowAggregator implements the aggregate_over method for GroupBy
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/api/api_handler/base.py` & `featurebyte-1.0.3/featurebyte/api/api_handler/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 List handler
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Type
 
 import json
 from functools import partial
 from itertools import groupby
```

### Comparing `featurebyte-1.0.2/featurebyte/api/api_handler/feature.py` & `featurebyte-1.0.3/featurebyte/api/api_handler/feature.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/api/api_handler/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/api/api_handler/feature_job_setting_analysis.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/api/api_handler/feature_list.py` & `featurebyte-1.0.3/featurebyte/api/api_handler/feature_list.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/api/api_handler/feature_namespace.py` & `featurebyte-1.0.3/featurebyte/api/api_handler/feature_namespace.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/api/api_object.py` & `featurebyte-1.0.3/featurebyte/api/api_object.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ApiObject class
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, ClassVar, Dict, List, Optional, Type, TypeVar, Union
 
 import operator
 from http import HTTPStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/api/api_object_util.py` & `featurebyte-1.0.3/featurebyte/api/api_object_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 API Object Util
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Iterator, List, Optional, Union
 
 import ctypes
 import threading
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/api/base_table.py` & `featurebyte-1.0.3/featurebyte/api/base_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """
 DataColumn class
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
-from typing import Any, ClassVar, List, Literal, Optional, Tuple, Type, TypeVar, Union, cast
+from typing import Any, ClassVar, List, Optional, Tuple, Type, TypeVar, Union, cast
+from typing_extensions import Literal
 
 from datetime import datetime
 from http import HTTPStatus
 
 import pandas as pd
 from bson.objectid import ObjectId
 from pandas import DataFrame
```

### Comparing `featurebyte-1.0.2/featurebyte/api/batch_feature_table.py` & `featurebyte-1.0.3/featurebyte/api/batch_feature_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchFeatureTable class
 """
+
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/api/batch_request_table.py` & `featurebyte-1.0.3/featurebyte/api/batch_request_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTable class
 """
+
 # pylint: disable=duplicate-code
 from __future__ import annotations
 
 from typing import Any, Optional, Union
 
 from pathlib import Path
```

### Comparing `featurebyte-1.0.2/featurebyte/api/catalog.py` & `featurebyte-1.0.3/featurebyte/api/catalog.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """
 Catalog module
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
-from typing import Any, Dict, List, Literal, Optional, Union
+from typing import Any, Dict, List, Optional, Union
+from typing_extensions import Literal
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
 
 from featurebyte.api.api_handler.base import ListHandler
@@ -676,15 +678,17 @@
         >>> contexts = catalog.list_contexts()
         """
 
         return Context.list(include_id=include_id)
 
     @update_and_reset_catalog
     def list_relationships(
-        self, include_id: Optional[bool] = True, relationship_type: Optional[Literal[tuple(RelationshipType)]] = None  # type: ignore
+        self,
+        include_id: Optional[bool] = True,
+        relationship_type: Optional[Literal[tuple(RelationshipType)]] = None,  # type: ignore[misc]
     ) -> pd.DataFrame:
         """
         List all relationships that exist in your FeatureByte instance, or filtered by relationship type.
 
         This provides a dataframe with:
 
         - the relationship id
@@ -956,35 +960,41 @@
         List saved static source tables.
 
         >>> static_source_tables = catalog.list_static_source_tables()
         """
         return StaticSourceTable.list(include_id=include_id)
 
     @update_and_reset_catalog
-    def list_deployments(self, include_id: Optional[bool] = True) -> pd.DataFrame:
+    def list_deployments(
+        self,
+        include_id: Optional[bool] = True,
+        feature_list_id: Optional[Union[ObjectId, str]] = None,
+    ) -> pd.DataFrame:
         """
         List saved deployments.
 
         Parameters
         ----------
         include_id: Optional[bool]
             Whether to include id in the list.
+        feature_list_id: Optional[Union[ObjectId, str]]
+            Filter deployments by feature list ID.
 
         Returns
         -------
         pd.DataFrame
             Table of deployments.
 
         Examples
         --------
         List saved deployments.
 
         >>> deployments = catalog.list_deployments()
         """
-        return Deployment.list(include_id=include_id)
+        return Deployment.list(include_id=include_id, feature_list_id=feature_list_id)
 
     @update_and_reset_catalog
     def list_user_defined_functions(self, include_id: Optional[bool] = True) -> pd.DataFrame:
         """
         List saved user defined functions.
 
         Parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/api/catalog_decorator.py` & `featurebyte-1.0.3/featurebyte/api/catalog_decorator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Catalog decorator.
 """
+
 from typing import Any
 
 from functools import wraps
 
 from featurebyte.logging import get_logger
 from featurebyte.models.base import activate_catalog, get_active_catalog_id
```

### Comparing `featurebyte-1.0.2/featurebyte/api/catalog_get_by_id_mixin.py` & `featurebyte-1.0.3/featurebyte/api/catalog_get_by_id_mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Catalog - get_by_*_id mixin.
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from bson import ObjectId
 
 from featurebyte.api.batch_feature_table import BatchFeatureTable
```

### Comparing `featurebyte-1.0.2/featurebyte/api/change_view.py` & `featurebyte-1.0.3/featurebyte/api/change_view.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ChangeView class
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, List, Optional, Tuple, Union
 
 from datetime import datetime
 
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/api/context.py` & `featurebyte-1.0.3/featurebyte/api/context.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Context module
 """
+
 from typing import Any, Dict, List, Optional
 
 import pandas as pd
 from bson import ObjectId
 from pandas import DataFrame
 from typeguard import typechecked
```

### Comparing `featurebyte-1.0.2/featurebyte/api/credential.py` & `featurebyte-1.0.3/featurebyte/api/credential.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Credential module
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import Field
 from typeguard import typechecked
 
@@ -17,23 +18,14 @@
 from featurebyte.schema.credential import CredentialCreate, CredentialRead, CredentialUpdate
 
 
 @typechecked
 class Credential(DeletableApiObject, SavableApiObject):
     """
     Credential class is the data model used to represent your credentials that are persisted.
-
-    Examples
-    --------
-    >>> credential = Credential(  # doctest: +SKIP
-    ...   name=feature_store.name,
-    ...   feature_store_id=feature_store.id,
-    ...   database_credential=database_credential,
-    ...   storage_credential=storage_credential,
-    ... )
     """
 
     # documentation metadata
     __fbautodoc__ = FBAutoDoc(
         proxy_class="featurebyte.Credential",
         skip_params_and_signature_in_class_docs=True,
     )
@@ -116,15 +108,15 @@
         Credential
 
         Examples
         --------
         Create a new credential.
 
         >>> credential = fb.Credential.create(  # doctest: +SKIP
-        ...     feature_store=fb.FeatureStore.get("playground"),
+        ...     feature_store_name="playground",
         ...     database_credential=UsernamePasswordCredential(
         ...         username="username",
         ...         password="password"
         ...     ),
         ...     storage_credential=S3StorageCredential(
         ...         s3_access_key_id="access_key_id",
         ...         s3_secret_access_key="s3_secret_access_key",
```

### Comparing `featurebyte-1.0.2/featurebyte/api/data_source.py` & `featurebyte-1.0.3/featurebyte/api/data_source.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureStore class
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, cast
 
 from http import HTTPStatus
 
 from typeguard import typechecked
```

### Comparing `featurebyte-1.0.2/featurebyte/api/deployment.py` & `featurebyte-1.0.3/featurebyte/api/deployment.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,30 +1,38 @@
 """
 Deployment module
 """
+
 from __future__ import annotations
 
-from typing import Any, Dict, Literal, Optional
+from typing import Any, Dict, Optional, Union
+from typing_extensions import Literal
 
 from http import HTTPStatus
 
 import pandas as pd
 from bson import ObjectId
 from typeguard import typechecked
 
+from featurebyte.api.accessor.databricks import DataBricksAccessor
 from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.batch_feature_table import BatchFeatureTable
 from featurebyte.api.batch_request_table import BatchRequestTable
 from featurebyte.api.feature_job import FeatureJobStatusResult
 from featurebyte.api.feature_list import FeatureList
 from featurebyte.api.savable_api_object import DeletableApiObject
+from featurebyte.api.use_case import UseCase
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.formatting_util import CodeStr
 from featurebyte.config import Configurations
-from featurebyte.exception import FeatureListNotOnlineEnabledError, RecordRetrievalException
+from featurebyte.exception import (
+    DeploymentDataBricksAccessorError,
+    FeatureListNotOnlineEnabledError,
+    RecordRetrievalException,
+)
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.deployment import DeploymentModel
 from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
 from featurebyte.schema.deployment import DeploymentUpdate
 
 
 class Deployment(DeletableApiObject):
@@ -59,14 +67,45 @@
     _list_foreign_keys = [
         ForeignKeyMapping("feature_list_id", FeatureList, "feature_list_name", "name", True),
         ForeignKeyMapping("feature_list_id", FeatureList, "feature_list_version", "version", True),
         ForeignKeyMapping("feature_list_id", FeatureList, "num_feature", "num_feature", True),
     ]
 
     @property
+    def databricks(self) -> DataBricksAccessor:
+        """
+        DataBricks accessor object
+
+        Returns
+        -------
+        DataBricksAccessor
+
+        Raises
+        ------
+        DeploymentDataBricksAccessorError
+            If deployment is not enabled or not using DataBricks Unity as the store
+        """
+        if not self.enabled:
+            raise DeploymentDataBricksAccessorError("Deployment is not enabled")
+
+        store_info = self.feature_list.cached_model.store_info  # type: ignore
+        if store_info.type != "databricks_unity":
+            raise DeploymentDataBricksAccessorError(
+                "Deployment is not using DataBricks Unity as the store"
+            )
+
+        target_name, target_dtype = None, None
+        if self.use_case and self.use_case.target:
+            target = self.use_case.target
+            target_name, target_dtype = target.name, target.dtype
+        return DataBricksAccessor(
+            store_info=store_info, target_name=target_name, target_dtype=target_dtype
+        )
+
+    @property
     def enabled(self) -> bool:
         """
         Deployment enabled status
 
         Returns
         -------
         bool
@@ -80,14 +119,39 @@
 
         Returns
         -------
         PydanticObjectId
         """
         return self.cached_model.feature_list_id
 
+    @property
+    def feature_list(self) -> FeatureList:
+        """
+        Feature list object associated with this deployment
+
+        Returns
+        -------
+        FeatureList
+        """
+        return FeatureList.get_by_id(self.feature_list_id)
+
+    @property
+    def use_case(self) -> Optional[UseCase]:
+        """
+        Use case object associated with this deployment
+
+        Returns
+        -------
+        Optional[UseCase]
+        """
+        use_case_id = self.cached_model.use_case_id  # type: ignore
+        if use_case_id is None:
+            return None
+        return UseCase.get_by_id(use_case_id)
+
     def info(self, verbose: bool = False) -> Dict[str, Any]:
         """
         Returns a dictionary that summarizes the essential information of the deployment represented by the
         Deployment object.
 
         Parameters
         ----------
@@ -353,36 +417,45 @@
         Get a Deployment object that is already saved.
 
         >>> fb.Deployment.get_by_id(<deployment_id>)  # doctest: +SKIP
         """
         return cls._get_by_id(id=id)
 
     @classmethod
-    def list(cls, include_id: Optional[bool] = True) -> pd.DataFrame:
+    def list(
+        cls,
+        include_id: Optional[bool] = True,
+        feature_list_id: Optional[Union[ObjectId, str]] = None,
+    ) -> pd.DataFrame:
         """
         Returns a DataFrame that lists the deployments by their names, feature list names, feature list versions,
         number of features, and whether the features are enabled.
 
         Parameters
         ----------
         include_id: Optional[bool]
             Whether to include id in the list.
+        feature_list_id: Optional[Union[ObjectId, str]]
+            Filter deployments by feature list ID.
 
         Returns
         -------
         DataFrame
             Table of objects.
 
         Examples
         --------
         List all deployments.
 
         >>> deployments = fb.Deployment.list()
         """
-        return super().list(include_id=include_id)
+        params = {}
+        if feature_list_id:
+            params["feature_list_id"] = str(feature_list_id)
+        return cls._list(include_id=include_id, params=params)
 
     def delete(self) -> None:
         """
         Delete the deployment from the persistent data store. The deployment can only be deleted if it is not
         enabled and not associated with any active batch feature table.
 
         Examples
```

### Comparing `featurebyte-1.0.2/featurebyte/api/dimension_table.py` & `featurebyte-1.0.3/featurebyte/api/dimension_table.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 DimensionTable class
 """
+
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, ClassVar, List, Literal, Optional, Type, cast
+from typing import TYPE_CHECKING, ClassVar, List, Optional, Type, cast
+from typing_extensions import Literal
 
 from bson import ObjectId
 from pydantic import Field, StrictStr, root_validator
 
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.validator import construct_data_model_root_validator
```

### Comparing `featurebyte-1.0.2/featurebyte/api/dimension_view.py` & `featurebyte-1.0.3/featurebyte/api/dimension_view.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 DimensionView class
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar
 
 from pydantic import Field
 
 from featurebyte.api.scd_view import SCDView
```

### Comparing `featurebyte-1.0.2/featurebyte/api/entity.py` & `featurebyte-1.0.3/featurebyte/api/entity.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Entity class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List
 
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
```

### Comparing `featurebyte-1.0.2/featurebyte/api/event_table.py` & `featurebyte-1.0.3/featurebyte/api/event_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 EventTable class
 """
+
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, ClassVar, List, Literal, Optional, Type, Union, cast
+from typing import TYPE_CHECKING, Any, ClassVar, List, Optional, Type, Union, cast
+from typing_extensions import Literal
 
 from datetime import datetime
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field, StrictStr, root_validator
 from typeguard import typechecked
```

### Comparing `featurebyte-1.0.2/featurebyte/api/event_view.py` & `featurebyte-1.0.3/featurebyte/api/event_view.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 """
 EventView class
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, ClassVar, Optional, cast
 
 import copy
 
 from bson import ObjectId
 from pydantic import Field
 
 from featurebyte.api.lag import LaggableViewColumn
 from featurebyte.api.view import GroupByMixin, RawMixin, View
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import validate_type_is_feature
 from featurebyte.enum import TableDataType
 from featurebyte.exception import EventViewMatchingEntityColumnNotFound
 from featurebyte.query_graph.enum import GraphNodeType, NodeOutputType, NodeType
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.query_graph.node.input import EventTableInputNodeParameters, InputNode
+from featurebyte.typing import validate_type_is_feature
 
 if TYPE_CHECKING:
     from featurebyte.api.feature import Feature
 
 
 class EventViewColumn(LaggableViewColumn):
     """
@@ -217,14 +218,15 @@
 
     def _validate_feature_addition(
         self, new_column_name: str, feature: Feature, entity_col_override: Optional[str]
     ) -> None:
         """
         Validates feature addition
         - Checks that the feature is non-time based
+        - Checks that the feature does not use request columns
         - Checks that entity is present in one of the columns
 
         Parameters
         ----------
         new_column_name: str
             the new column name we want to use
         feature: Feature
@@ -240,14 +242,20 @@
         # Validate whether the new column name is used
         self._validate_column_is_not_used(new_column_name)
 
         # Validate whether feature is time based
         if feature.is_time_based:
             raise ValueError("We currently only support the addition of non-time based features.")
 
+        # Validate whether request column is used in feature definition
+        if feature.used_request_column:
+            raise ValueError(
+                "We currently only support the addition of features that do not use request columns."
+            )
+
         # Validate entity_col_override
         if entity_col_override is not None:
             self._validate_entity_col_override(entity_col_override)
 
     @staticmethod
     def _get_feature_entity_col(feature: Feature) -> str:
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature.py` & `featurebyte-1.0.3/featurebyte/api/feature.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """
 Feature and FeatureList classes
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
-from typing import Any, ClassVar, Dict, List, Literal, Optional, Sequence, Tuple, Type, Union, cast
+from typing import Any, ClassVar, Dict, List, Optional, Sequence, Tuple, Type, Union, cast
+from typing_extensions import Literal
 
 from http import HTTPStatus
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field, root_validator
 from typeguard import typechecked
@@ -37,15 +39,14 @@
     PREVIEW_DOC,
     TABLE_IDS_DOC,
     VERSION_DOC,
 )
 from featurebyte.api.templates.series_doc import ISNULL_DOC, NOTNULL_DOC
 from featurebyte.common.descriptor import ClassInstanceMethodDescriptor
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import ScalarSequence
 from featurebyte.common.utils import enforce_observation_set_row_order, is_server_mode
 from featurebyte.config import Configurations
 from featurebyte.core.accessor.count_dict import CdAccessorMixin
 from featurebyte.core.accessor.feature_datetime import FeatureDtAccessorMixin
 from featurebyte.core.accessor.feature_string import FeatureStrAccessorMixin
 from featurebyte.core.series import FrozenSeries, FrozenSeriesT, Series
 from featurebyte.enum import ConflictResolution
@@ -53,26 +54,31 @@
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.logging import get_logger
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.tile import TileSpec
+from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.graph import GlobalQueryGraph
 from featurebyte.query_graph.model.common_table import TabularSource
-from featurebyte.query_graph.model.feature_job_setting import TableFeatureJobSetting
+from featurebyte.query_graph.model.feature_job_setting import (
+    TableFeatureJobSetting,
+    TableIdFeatureJobSetting,
+)
 from featurebyte.query_graph.node.cleaning_operation import TableCleaningOperation
 from featurebyte.schema.feature import (
     BatchFeatureCreatePayload,
     BatchFeatureItem,
     FeatureCreate,
     FeatureModelResponse,
     FeatureSQL,
     FeatureUpdate,
 )
+from featurebyte.typing import ScalarSequence
 
 logger = get_logger(__name__)
 
 DOCSTRING_FORMAT_PARAMS = {"class_name": "Feature"}
 
 
 # pylint: disable=too-many-ancestors
@@ -610,14 +616,41 @@
 
         Returns
         -------
         bool
         """
         return super().saved
 
+    @property
+    def used_request_column(self) -> bool:
+        """
+        Returns whether the Feature object uses request column(s) in the computation.
+
+        Returns
+        -------
+        bool
+        """
+        try:
+            return self.cached_model.used_request_column
+        except RecordRetrievalException:
+            return self.graph.has_node_type(
+                target_node=self.node, node_type=NodeType.REQUEST_COLUMN
+            )
+
+    @property
+    def table_id_feature_job_settings(self) -> List[TableIdFeatureJobSetting]:
+        """
+        Return table feature job settings of tables used by the feature
+
+        Returns
+        -------
+        List[TableFeatureJobSetting]
+        """
+        return self.graph.extract_table_id_feature_job_settings(target_node=self.node)
+
     @typechecked
     def save(
         self, conflict_resolution: ConflictResolution = "raise", _id: Optional[ObjectId] = None
     ) -> None:
         """
         Adds a Feature object to the catalog.
 
@@ -907,25 +940,27 @@
         - [FeatureList.create_new_version](/reference/featurebyte.api.feature_list.FeatureList.create_new_version/)
         """
         client = Configurations().get_client()
         response = client.post(
             url=self._route,
             json={
                 "source_feature_id": str(self.id),
-                "table_feature_job_settings": [
-                    table_feature_job_setting.dict()
-                    for table_feature_job_setting in table_feature_job_settings
-                ]
-                if table_feature_job_settings
-                else None,
-                "table_cleaning_operations": [
-                    clean_ops.dict() for clean_ops in table_cleaning_operations
-                ]
-                if table_cleaning_operations
-                else None,
+                "table_feature_job_settings": (
+                    [
+                        table_feature_job_setting.dict()
+                        for table_feature_job_setting in table_feature_job_settings
+                    ]
+                    if table_feature_job_settings
+                    else None
+                ),
+                "table_cleaning_operations": (
+                    [clean_ops.dict() for clean_ops in table_cleaning_operations]
+                    if table_cleaning_operations
+                    else None
+                ),
             },
         )
         if response.status_code != HTTPStatus.CREATED:
             raise RecordCreationException(response=response)
 
         object_dict = response.json()
         self._update_cache(object_dict)  # update object cache store
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_group.py` & `featurebyte-1.0.3/featurebyte/api/feature_group.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,15 +26,14 @@
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature import Feature
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.mixin import AsyncMixin
 from featurebyte.api.observation_table import ObservationTable
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.env_util import get_alive_bar_additional_params
-from featurebyte.common.typing import Scalar
 from featurebyte.common.utils import dataframe_from_json, enforce_observation_set_row_order
 from featurebyte.config import Configurations
 from featurebyte.core.mixin import ParentMixin
 from featurebyte.core.series import Series
 from featurebyte.enum import ConflictResolution
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.logging import get_logger
@@ -46,14 +45,15 @@
 from featurebyte.schema.feature import (
     MAX_BATCH_FEATURE_ITEM_COUNT,
     BatchFeatureCreatePayload,
     BatchFeatureItem,
 )
 from featurebyte.schema.feature_list import FeatureListCreateJob, FeatureListPreview, FeatureListSQL
 from featurebyte.schema.worker.task.feature_list_create import FeatureParameters
+from featurebyte.typing import Scalar
 
 logger = get_logger(__name__)
 
 
 Item = Union[Feature, "BaseFeatureGroup"]
 FeatureObjects = OrderedDict[str, Feature]
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_job.py` & `featurebyte-1.0.3/featurebyte/api/feature_job.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureJobMixin class
 """
+
 from typing import Any, Dict, List, Tuple
 
 import base64
 import datetime
 import textwrap
 from abc import abstractmethod
 from http import HTTPStatus
@@ -112,15 +113,15 @@
             bins = pd.date_range(
                 start=self.request_date - datetime.timedelta(hours=self.job_history_window),
                 end=self.request_date,
                 freq=f"{freq_min} min",
             ).to_list()
             plt.hist(_strip_nulls(self.job_session_logs.COMPLETED), bins=bins, rwidth=0.7)
             plt.title(f"Job distribution over time (bin size: {bin_size})")
-            plt.axvline(x=self.request_date, color="red")
+            plt.axvline(x=self.request_date, color="red")  # type: ignore
             buffer = BytesIO()
             fig.savefig(buffer, format="png", metadata={"Software": None})
             image_1 = base64.b64encode(buffer.getvalue()).decode("utf-8")
             plt.close()
 
             # plot job duration distributions
             completed_jobs = self.job_session_logs["COMPLETED"].count()
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/api/feature_job_setting_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureJobSettingAnalysis class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Union
 
 from io import BytesIO
 from pathlib import Path
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_list.py` & `featurebyte-1.0.3/featurebyte/api/feature_list.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 """
 FeatureListVersion class
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
 from typing import (
     TYPE_CHECKING,
     Any,
     ClassVar,
     Dict,
     List,
-    Literal,
     Optional,
     Sequence,
     Tuple,
     TypeVar,
     Union,
     cast,
 )
+from typing_extensions import Literal
 
 from http import HTTPStatus
 
 import pandas as pd
 from bson.objectid import ObjectId
 from pydantic import Field, root_validator
 from typeguard import typechecked
@@ -623,14 +624,33 @@
     def _get_create_payload(self) -> dict[str, Any]:
         feature_ids = [feature.id for feature in self.feature_objects.values()]
         data = FeatureListCreate(
             **{**self.dict(by_alias=True, exclude_none=True), "feature_ids": feature_ids}
         )
         return data.json_dict()
 
+    def list_deployments(self) -> pd.DataFrame:
+        """
+        List all deployments associated with the FeatureList object.
+
+        Returns
+        -------
+        pd.DataFrame
+            List of deployments
+
+        Examples
+        --------
+        >>> feature_list = catalog.get_feature_list("invoice_feature_list")
+        >>> feature_list.list_deployments()  # doctest: +SKIP
+        """
+        # pylint: disable=import-outside-toplevel
+        from featurebyte.api.deployment import Deployment
+
+        return Deployment.list(feature_list_id=self.id)
+
     def save(
         self, conflict_resolution: ConflictResolution = "raise", _id: Optional[ObjectId] = None
     ) -> None:
         """
         Adds a FeatureList object to the catalog.
 
         A conflict could be triggered when the object being saved has violated a uniqueness check at the catalog.
@@ -1352,17 +1372,15 @@
             },
         )
         if response.status_code != HTTPStatus.CREATED:
             raise RecordCreationException(response=response)
         return FeatureList(**response.json(), **self._get_init_params())
 
     @typechecked
-    def update_status(
-        self, status: Literal[tuple(FeatureListStatus)]  # type: ignore[misc]
-    ) -> None:
+    def update_status(self, status: Literal[tuple(FeatureListStatus)]) -> None:  # type: ignore[misc]
         """
         A FeatureList can have one of five statuses:
 
         "DEPLOYED": Assigned to FeatureLists with at least one active version online.
         "TEMPLATE": For FeatureLists serving as reference templates or safe starting points.
         "PUBLIC DRAFT": For FeatureLists shared for feedback purposes.
         "DRAFT": For FeatureLists in the prototype stage.
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_namespace.py` & `featurebyte-1.0.3/featurebyte/api/feature_namespace.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature Namespace module.
 """
+
 from typing import List, Optional, Union
 
 import pandas as pd
 
 from featurebyte.api.api_handler.base import ListHandler
 from featurebyte.api.api_handler.feature_namespace import FeatureNamespaceListHandler
 from featurebyte.api.feature_or_target_namespace_mixin import FeatureOrTargetNamespaceMixin
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_or_target_mixin.py` & `featurebyte-1.0.3/featurebyte/api/feature_or_target_mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Mixin class containing common methods for feature or target classes
 """
+
 from typing import Any, Sequence, Union, cast
 
 import time
 from abc import ABC
 from http import HTTPStatus
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_or_target_namespace_mixin.py` & `featurebyte-1.0.3/featurebyte/api/feature_or_target_namespace_mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base feature target namespace
 """
+
 from typing import List
 
 from pydantic import Field
 
 from featurebyte.api.api_object import ApiObject
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_store.py` & `featurebyte-1.0.3/featurebyte/api/feature_store.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureStore class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from bson import ObjectId
 from pandas import DataFrame
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_util.py` & `featurebyte-1.0.3/featurebyte/api/feature_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature util module.
 """
+
 from typing import List, Optional, Union
 
 import pandas as pd
 
 from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.api.entity import Entity
```

### Comparing `featurebyte-1.0.2/featurebyte/api/feature_validation_util.py` & `featurebyte-1.0.3/featurebyte/api/feature_validation_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature validation util
 """
+
 from typing import List
 
 from featurebyte.query_graph.enum import NodeType
 
 
 def assert_is_lookup_feature(node_types_lineage: List[NodeType]) -> None:
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/api/groupby.py` & `featurebyte-1.0.3/featurebyte/api/groupby.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,32 +1,37 @@
 """
 This module contains groupby related class
 """
+
 from __future__ import annotations
 
-from typing import List, Literal, Optional, Union
+from typing import List, Optional, Union
+from typing_extensions import Literal
+
+import warnings
 
 from typeguard import typechecked
 
 from featurebyte.api.aggregator.asat_aggregator import AsAtAggregator
 from featurebyte.api.aggregator.forward_aggregator import ForwardAggregator
+from featurebyte.api.aggregator.forward_asat_aggregator import ForwardAsAtAggregator
 from featurebyte.api.aggregator.simple_aggregator import SimpleAggregator
 from featurebyte.api.aggregator.window_aggregator import WindowAggregator
 from featurebyte.api.change_view import ChangeView
 from featurebyte.api.entity import Entity
 from featurebyte.api.event_view import EventView
 from featurebyte.api.feature import Feature
 from featurebyte.api.feature_group import FeatureGroup
 from featurebyte.api.item_view import ItemView
 from featurebyte.api.scd_view import SCDView
 from featurebyte.api.target import Target
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import OptionalScalar
 from featurebyte.enum import AggFunc
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
+from featurebyte.typing import OptionalScalar
 
 
 class GroupBy:
     """
     The groupby method of a view returns a GroupBy class that can be used to group data based on one or more columns
     representing entities (specified in the key parameter). Within each entity or group of entities, the GroupBy
     class applies aggregation function(s) to the data.
@@ -254,15 +259,15 @@
         feature_name: Optional[str] = None,
         offset: Optional[str] = None,
         backward: bool = True,
         fill_value: OptionalScalar = None,
         skip_fill_na: bool = False,
     ) -> Feature:
         """
-        The aggregate_as_at method of a GroupBy instance returns an Aggregate ""as at"" Feature object. The object
+        The aggregate_asat method of a GroupBy instance returns an Aggregate ""as at"" Feature object. The object
         aggregates data from the column specified by the value_column parameter using the aggregation method provided
         by the method parameter. By default, the aggrgegation is done on rows active at the point-in-time indicated in
         the feature request. The primary entity of the Feature is determined by the grouping key of the GroupBy
         instance,
 
         These aggregation operations are exclusively available for Slowly Changing Dimension (SCD) views, and the
         grouping key used in the GroupBy instance should not be the natural key of the SCD view.
@@ -303,15 +308,16 @@
             "s": second
             "m": minute
             "h": hour
             "d": day
             "w": week
 
         backward: bool
-            Whether the offset should be applied backward or forward
+            Whether the offset should be applied backward or forward. Note that this parameter is
+            deprecated. Please use `forward_aggregate_asat` to create a Target object instead.
         fill_value: OptionalScalar
             Value to fill if the value in the column is empty
         skip_fill_na: bool
             Whether to skip filling NaN values
 
         Returns
         -------
@@ -337,14 +343,19 @@
 
         >>> feature_12w_before = active_credit_card_by_cust.aggregate_asat(  # doctest: +SKIP
         ...   method=fb.AggFunc.COUNT,
         ...   feature_name="Number of Active Credit Cards 12 w before",
         ...   offset="12w"
         ... )
         """
+        if backward is False:
+            warnings.warn(
+                "The backward parameter has no effect. Please use forward_aggregate_asat to create "
+                "a Target object instead."
+            )
         return AsAtAggregator(
             self.view_obj, self.category, self.entity_ids, self.keys, self.serving_names
         ).aggregate_asat(
             value_column=value_column,
             method=method,
             feature_name=feature_name,
             offset=offset,
@@ -474,7 +485,111 @@
             value_column=value_column,
             method=method,
             window=window,
             target_name=target_name,
             fill_value=fill_value,
             skip_fill_na=skip_fill_na,
         )
+
+    @typechecked
+    def forward_aggregate_asat(
+        self,
+        value_column: Optional[str] = None,
+        method: Optional[Literal[tuple(AggFunc)]] = None,  # type: ignore[misc]
+        target_name: Optional[str] = None,
+        offset: Optional[str] = None,
+        fill_value: OptionalScalar = None,
+        skip_fill_na: bool = False,
+    ) -> Target:
+        """
+        The forward_aggregate_asat method of a GroupBy instance returns an Aggregate ""as at""
+        Target object. The object aggregates data from the column specified by the value_column
+        parameter using the aggregation method provided by the method parameter. By default, the
+        aggrgegation is done on rows active at the point-in-time indicated in the feature request.
+        The primary entity of the Feature is determined by the grouping key of the GroupBy instance,
+        These aggregation operations are exclusively available for Slowly Changing Dimension (SCD)
+        views, and the grouping key used in the GroupBy instance should not be the natural key of
+        the SCD view.
+
+        For instance, a possible example of an aggregate ‘as at’ target from a Credit Cards table
+        could be the count of credit cards held by a customer at the point-in-time indicated in the
+        target request.
+
+        If an offset is defined, the aggregation uses the active rows of the SCD view's data at the
+        point-in-time indicated in the feature request, plus the specified offset.
+
+        If the GroupBy instance involves computation across a categorical column, the returned
+        Target object is a Cross Aggregate "as at" Target. In this scenario, the target value
+        after materialization is a dictionary with keys representing the categories of the
+        categorical column and their corresponding values indicating the aggregated values for each
+        category. You may choose to fill the target value with a default value if the column to be
+        aggregated is empty.
+
+        It is possible to perform additional transformations on the Target object, and the Target
+        object is added to the catalog solely when explicitly saved.
+
+        Parameters
+        ----------
+        value_column: Optional[str]
+            Column to be aggregated
+        method: Optional[Literal[tuple(AggFunc)]]
+            Aggregation method
+        target_name: str
+            Output feature name
+        offset: Optional[str]
+            Optional offset to apply to the point in time column in the target request. The
+            aggregation result will be as at the point in time adjusted by this offset. Format of
+            offset is "{size}{unit}", where size is a positive integer and unit is one of the
+            following:
+
+            "ns": nanosecond
+            "us": microsecond
+            "ms": millisecond
+            "s": second
+            "m": minute
+            "h": hour
+            "d": day
+            "w": week
+
+        fill_value: OptionalScalar
+            Value to fill if the value in the column is empty
+        skip_fill_na: bool
+            Whether to skip filling NaN values
+
+        Returns
+        -------
+        Feature
+
+        Examples
+        --------
+        Count number of active cards per customer at a point-in-time.
+
+        >>> # Filter active cards
+        >>> cond = credit_card_accounts['status'] == "active"  # doctest: +SKIP
+        >>> # Group by customer
+        >>> active_credit_card_by_cust = credit_card_accounts[cond].groupby(  # doctest: +SKIP
+        ...   "CustomerID"
+        ... )
+        >>> target = active_credit_card_by_cust.forward_aggregate_asat(  # doctest: +SKIP
+        ...   method=fb.AggFunc.COUNT,
+        ...   feature_name="Number of Active Credit Cards",
+        ... )
+
+
+        Count number of active cards per customer 12 weeks after a point-in-time
+
+        >>> target_12w_after = active_credit_card_by_cust.forward_aggregate_asat(  # doctest: +SKIP
+        ...   method=fb.AggFunc.COUNT,
+        ...   feature_name="Number of Active Credit Cards 12 w after",
+        ...   offset="12w"
+        ... )
+        """
+        return ForwardAsAtAggregator(
+            self.view_obj, self.category, self.entity_ids, self.keys, self.serving_names
+        ).forward_aggregate_asat(
+            value_column=value_column,
+            method=method,
+            target_name=target_name,
+            offset=offset,
+            fill_value=fill_value,
+            skip_fill_na=skip_fill_na,
+        )
```

### Comparing `featurebyte-1.0.2/featurebyte/api/historical_feature_table.py` & `featurebyte-1.0.3/featurebyte/api/historical_feature_table.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,28 +1,40 @@
 """
 HistoricalFeatureTable class
 """
+
 from __future__ import annotations
 
-from typing import Optional, Union
+from typing import TYPE_CHECKING, Optional, TypeVar, Union
 
 from pathlib import Path
 
 import pandas as pd
 from typeguard import typechecked
 
 from featurebyte.api.api_object import ApiObject
 from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.materialized_table import MaterializedTableMixin
 from featurebyte.api.observation_table import ObservationTable
+from featurebyte.api.target import Target
 from featurebyte.common.doc_util import FBAutoDoc
+from featurebyte.logging import get_logger
+from featurebyte.models.base import get_active_catalog_id
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.schema.historical_feature_table import HistoricalFeatureTableListRecord
 
+if TYPE_CHECKING:
+    from featurebyte.api.feature_list import FeatureList
+else:
+    FeatureList = TypeVar("FeatureList")
+
+
+logger = get_logger(__name__)
+
 
 class HistoricalFeatureTable(HistoricalFeatureTableModel, ApiObject, MaterializedTableMixin):
     """
     HistoricalFeatureTable class
     """
 
     __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.HistoricalFeatureTable")
@@ -38,14 +50,128 @@
         "created_at",
     ]
     _list_foreign_keys = [
         ForeignKeyMapping("feature_store_id", FeatureStore, "feature_store_name"),
         ForeignKeyMapping("observation_table_id", ObservationTable, "observation_table_name"),
     ]
 
+    @property
+    def observation_table(self) -> Optional[ObservationTable]:
+        """
+        ObservationTable object associated with the historical feature table.
+
+        Returns
+        -------
+        Optional[ObservationTable]
+            ObservationTable object
+        """
+        observation_table_id = self.cached_model.observation_table_id
+        if observation_table_id is None:
+            return None
+        return ObservationTable.get_by_id(observation_table_id)
+
+    @property
+    def feature_list(self) -> FeatureList:
+        """
+        FeatureList object associated with the historical feature table.
+
+        Returns
+        -------
+        FeatureList
+            FeatureList object
+        """
+        # pylint: disable=import-outside-toplevel
+        from featurebyte.api.feature_list import FeatureList
+
+        feature_list_id = self.cached_model.feature_list_id
+        return FeatureList.get_by_id(feature_list_id)  # type: ignore
+
+    @property
+    def feature_names(self) -> list[str]:
+        """
+        List of feature names associated with the historical feature table.
+
+        Returns
+        -------
+        list[str]
+            List of feature names
+        """
+        return self.feature_list.feature_names
+
+    @property
+    def target_name(self) -> Optional[str]:
+        """
+        Target name associated with the historical feature table.
+
+        Returns
+        -------
+        Optional[str]
+            Target name
+        """
+        observation_table = self.observation_table
+        if observation_table is None:
+            return None
+
+        target_id = observation_table.cached_model.target_id  # type: ignore
+        if target_id is None:
+            return None
+
+        target = Target.get_by_id(target_id)
+        return target.name
+
+    @classmethod
+    def get(cls, name: str) -> HistoricalFeatureTable:
+        hist_feature_table = cls._get(name=name)
+
+        # Add mlflow tracking in get_historical_tables
+        try:
+            import mlflow  # pylint: disable=import-outside-toplevel
+        except ImportError:
+            mlflow = None
+
+        if mlflow and mlflow.active_run():
+            # log featurebyte training data information
+            feature_list = hist_feature_table.feature_list
+            try:
+                mlflow.log_param(
+                    "fb_training_data",
+                    {
+                        "catalog_id": get_active_catalog_id(),
+                        "feature_list_name": feature_list.name,
+                        "target_name": hist_feature_table.target_name,
+                        "dataset_name": hist_feature_table.name,
+                        "primary_entity": [entity.name for entity in feature_list.primary_entity],
+                    },
+                )
+            except Exception as exc:  # pylint: disable=broad-except
+                logger.warning(
+                    f"Failed to log featurebyte training data information to mlflow: {exc}"
+                )
+
+        return hist_feature_table
+
+    def list_deployments(self) -> pd.DataFrame:
+        """
+        List all deployments associated with the historical feature table.
+
+        Returns
+        -------
+        pd.DataFrame
+            List of deployments
+
+        Examples
+        --------
+        >>> historical_feature_table = catalog.get_historical_feature_table("historical_feature_table_name")  # doctest: +SKIP
+        >>> historical_feature_table.list_deployments()  # doctest: +SKIP
+        """
+        # pylint: disable=import-outside-toplevel
+        from featurebyte.api.deployment import Deployment
+
+        return Deployment.list(feature_list_id=self.cached_model.feature_list_id)
+
     def preview(self, limit: int = 10) -> pd.DataFrame:
         """
         Returns a DataFrame that contains a selection of rows of the historical feature table.
 
         Parameters
         ----------
         limit: int
```

### Comparing `featurebyte-1.0.2/featurebyte/api/item_table.py` & `featurebyte-1.0.3/featurebyte/api/item_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 ItemTable class
 """
+
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, ClassVar, List, Literal, Optional, Type, Union, cast
+from typing import TYPE_CHECKING, Any, ClassVar, List, Optional, Type, Union, cast
+from typing_extensions import Literal
 
 import operator
 
 from bson import ObjectId
 from cachetools import cachedmethod
 from pydantic import Field, StrictStr, root_validator
```

### Comparing `featurebyte-1.0.2/featurebyte/api/item_view.py` & `featurebyte-1.0.3/featurebyte/api/item_view.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ItemView class
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, List, Optional
 
 from pydantic import Field
 
 from featurebyte.api.event_view import EventView
```

### Comparing `featurebyte-1.0.2/featurebyte/api/lag.py` & `featurebyte-1.0.3/featurebyte/api/lag.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Lag module
 """
+
 from __future__ import annotations
 
 from typing import List, TypeVar, Union
 
 from typeguard import typechecked
 
 from featurebyte.api.view import ViewColumn
```

### Comparing `featurebyte-1.0.2/featurebyte/api/materialized_table.py` & `featurebyte-1.0.3/featurebyte/api/materialized_table.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 """
 Materialized Table Mixin
 """
+
 from typing import Any, Callable, ClassVar, Optional, Tuple, Union
 
-import os
-import tempfile
 from http import HTTPStatus
 from pathlib import Path
 
 import pandas as pd
 from typeguard import typechecked
 
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.source_table import SourceTable
-from featurebyte.common.utils import parquet_from_arrow_stream
+from featurebyte.api.utils import (
+    dataframe_from_arrow_stream_with_progress,
+    parquet_from_arrow_stream,
+)
+from featurebyte.common.utils import ResponseStream
 from featurebyte.config import Configurations
 from featurebyte.enum import SourceType
 from featurebyte.exception import RecordDeletionException, RecordRetrievalException
 from featurebyte.models.materialized_table import MaterializedTableModel
 from featurebyte.query_graph.sql.common import get_fully_qualified_table_name, sql_to_string
 
 
@@ -56,30 +59,40 @@
             raise FileExistsError(f"{output_path} already exists.")
 
         client = Configurations().get_client()
         response = client.get(f"{self._route}/pyarrow_table/{self.id}", stream=True)
         if response.status_code != HTTPStatus.OK:
             raise RecordRetrievalException(response)
         parquet_from_arrow_stream(
-            response=response, output_path=output_path, num_rows=self.num_rows
+            ResponseStream(response.iter_content(1024)),
+            output_path=output_path,
+            num_rows=self.num_rows,
         )
         return output_path
 
     def to_pandas(self) -> pd.DataFrame:
         """
         Converts the table to pandas dataframe.
 
         Returns
         -------
         pd.DataFrame
+
+        Raises
+        ------
+        RecordRetrievalException
+            Error retrieving record from API.
         """
-        with tempfile.TemporaryDirectory() as temp_dir:
-            output_path = os.path.join(temp_dir, "temp.parquet")
-            self.download(output_path=output_path)
-            return pd.read_parquet(output_path)
+        client = Configurations().get_client()
+        response = client.get(f"{self._route}/pyarrow_table/{self.id}", stream=True)
+        if response.status_code != HTTPStatus.OK:
+            raise RecordRetrievalException(response)
+        return dataframe_from_arrow_stream_with_progress(
+            ResponseStream(response.iter_content(1024)), num_rows=self.num_rows
+        )
 
     def to_spark_df(self) -> Any:
         """
         Get a spark dataframe from the table.
 
         Returns
         -------
```

### Comparing `featurebyte-1.0.2/featurebyte/api/obs_table/utils.py` & `featurebyte-1.0.3/featurebyte/api/obs_table/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Observation table utils
 """
+
 from typing import Any, Dict, List, Optional, Tuple
 
 from featurebyte.common.utils import get_version
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.metadata.sdk_code import (
     ObjectClass,
```

### Comparing `featurebyte-1.0.2/featurebyte/api/observation_table.py` & `featurebyte-1.0.3/featurebyte/api/observation_table.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,36 +1,44 @@
 """
 ObservationTable class
 """
+
 # pylint: disable=duplicate-code
 from __future__ import annotations
 
-from typing import Any, List, Literal, Optional, Sequence, Union
+from typing import Any, List, Optional, Sequence, Union
+from typing_extensions import Literal
 
 import os
 from pathlib import Path
 
 import pandas as pd
 from bson import ObjectId
 from typeguard import typechecked
 
 from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature_store import FeatureStore
 from featurebyte.api.materialized_table import MaterializedTableMixin
 from featurebyte.api.primary_entity_mixin import PrimaryEntityMixin
+from featurebyte.api.target_namespace import TargetNamespace
 from featurebyte.api.templates.doc_util import substitute_docstring
 from featurebyte.api.templates.entity_doc import (
     ENTITY_DOC,
     PRIMARY_ENTITY_DOC,
     PRIMARY_ENTITY_IDS_DOC,
 )
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.observation_table import ObservationInput, ObservationTableModel, Purpose
+from featurebyte.models.observation_table import (
+    ObservationInput,
+    ObservationTableModel,
+    Purpose,
+    TargetInput,
+)
 from featurebyte.schema.observation_table import (
     ObservationTableListRecord,
     ObservationTableUpdate,
     ObservationTableUpload,
 )
 
 DOCSTRING_FORMAT_PARAMS = {"class_name": "ObservationTable"}
@@ -92,14 +100,47 @@
 
     @property
     @substitute_docstring(doc_template=PRIMARY_ENTITY_DOC, format_kwargs=DOCSTRING_FORMAT_PARAMS)
     def primary_entity(self) -> List[Entity]:  # pylint: disable=missing-function-docstring
         return [Entity.get_by_id(entity_id) for entity_id in self.primary_entity_ids]
 
     @property
+    def target_namespace(self) -> Optional[TargetNamespace]:
+        """
+        Returns the target namespace associated to the observation table.
+
+        Returns
+        -------
+        Optional[TargetNamespace]
+            Target namespace of the observation table.
+        """
+        target_namespace_id = self.cached_model.target_namespace_id  # type: ignore
+        if not target_namespace_id:
+            return None
+        return TargetNamespace.get_by_id(target_namespace_id)
+
+    @property
+    def target(self) -> Optional[Any]:
+        """
+        Returns the target associated to the observation table.
+
+        Returns
+        -------
+        Optional[Any]
+            Target of the observation table.
+        """
+        from featurebyte.api.target import Target  # pylint: disable=import-outside-toplevel
+
+        if isinstance(self.cached_model.request_input, TargetInput):
+            target_id = self.cached_model.request_input.target_id
+            if target_id:
+                return Target.get_by_id(target_id)
+        return None
+
+    @property
     def context_id(self) -> ObjectId:
         """
         Returns the context id of the observation table.
 
         Returns
         -------
         ObjectId
@@ -375,28 +416,33 @@
     @classmethod
     def upload(
         cls,
         file_path: Union[str, Path],
         name: str,
         purpose: Optional[Purpose] = None,
         primary_entities: Optional[List[str]] = None,
+        target_column: Optional[str] = None,
     ) -> ObservationTable:
         """
         Upload a file to create an observation table. This file can either be a CSV or Parquet file.
 
         Parameters
         ----------
         file_path: Union[str, Path]
             Path to file to upload. The file path should end in the appropriate .csv or .parquet file extension.
         name: str
             Name of the observation table to create.
         purpose: Optional[Purpose]
             Purpose of the observation table.
         primary_entities: Optional[List[str]]
             List of primary entities for the observation table.
+        target_column: Optional[str]
+            Name of the column in the observation table that stores the target values.
+            The target column name must match an existing target namespace in the catalog.
+            The data type and primary entities must match the those in the target namespace.
 
         Returns
         -------
         ObservationTable
 
         Examples
         --------
@@ -412,14 +458,15 @@
             for entity_name in primary_entities:
                 primary_entity_ids.append(Entity.get(entity_name).id)
         payload = ObservationTableUpload(
             name=name,
             purpose=purpose,
             primary_entity_ids=primary_entity_ids,
             uploaded_file_name=os.path.basename(file_path),
+            target_column=target_column,
         )
         with open(file_path, "rb") as file_object:
             observation_table_doc = cls.post_async_task(
                 route=f"{cls._route}/upload",
                 payload={"payload": payload.json()},
                 is_payload_json=False,
                 files={"observation_set": file_object},
```

### Comparing `featurebyte-1.0.2/featurebyte/api/online_store.py` & `featurebyte-1.0.3/featurebyte/api/online_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStore class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from bson import ObjectId
 from pandas import DataFrame
```

### Comparing `featurebyte-1.0.2/featurebyte/api/periodic_task.py` & `featurebyte-1.0.3/featurebyte/api/periodic_task.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 PeriodicTask class
 """
+
 from __future__ import annotations
 
 from featurebyte.api.savable_api_object import SavableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.models.periodic_task import PeriodicTask as PeriodicTaskModel
```

### Comparing `featurebyte-1.0.2/featurebyte/api/primary_entity_mixin.py` & `featurebyte-1.0.3/featurebyte/api/primary_entity_mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Primary entity mixin
 """
+
 from typing import List, Sequence
 
 from abc import abstractmethod
 
 from bson import ObjectId
 
 from featurebyte.api.api_object import ApiObject
```

### Comparing `featurebyte-1.0.2/featurebyte/api/relationship.py` & `featurebyte-1.0.3/featurebyte/api/relationship.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """
 Relationships API object
 """
-from typing import Any, Dict, Literal, Optional
+
+from typing import Any, Dict, Optional
+from typing_extensions import Literal
 
 import pandas as pd
 from pydantic import Field
 from typeguard import typechecked
 
 from featurebyte.api.api_object import ApiObject
 from featurebyte.api.api_object_util import ForeignKeyMapping
```

### Comparing `featurebyte-1.0.2/featurebyte/api/request_column.py` & `featurebyte-1.0.3/featurebyte/api/request_column.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 RequestColumn related classes for on-demand features
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import Field
 
 from featurebyte.common.doc_util import FBAutoDoc
```

### Comparing `featurebyte-1.0.2/featurebyte/api/savable_api_object.py` & `featurebyte-1.0.3/featurebyte/api/savable_api_object.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SavableApiObject class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from http import HTTPStatus
 
 from bson import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/api/scd_table.py` & `featurebyte-1.0.3/featurebyte/api/scd_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 SCDTable class
 """
+
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, ClassVar, List, Literal, Optional, Tuple, Type, cast
+from typing import TYPE_CHECKING, ClassVar, List, Optional, Tuple, Type, cast
+from typing_extensions import Literal
 
 from bson import ObjectId
 from pydantic import Field, StrictStr, root_validator
 
 from featurebyte.api.base_table import TableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.validator import construct_data_model_root_validator
```

### Comparing `featurebyte-1.0.2/featurebyte/api/scd_view.py` & `featurebyte-1.0.3/featurebyte/api/scd_view.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SCDView class
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, Optional
 
 from pydantic import Field
 
 from featurebyte.api.view import GroupByMixin, RawMixin, View, ViewColumn
```

### Comparing `featurebyte-1.0.2/featurebyte/api/source_table.py` & `featurebyte-1.0.3/featurebyte/api/source_table.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SourceTable class
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, ClassVar, List, Optional, Tuple, Type, TypeVar, Union, cast
 
 from abc import ABC, abstractmethod
 from datetime import datetime
@@ -12,14 +13,15 @@
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
 
 from featurebyte.api.entity import Entity
+from featurebyte.api.mixin import SampleMixin
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.utils import dataframe_from_json
 from featurebyte.config import Configurations
 from featurebyte.core.frame import BaseFrame
 from featurebyte.core.mixin import perf_logging
 from featurebyte.enum import DBVarType
 from featurebyte.exception import RecordRetrievalException
@@ -54,15 +56,15 @@
     ItemTable = TypeVar("ItemTable")
     SCDTable = TypeVar("SCDTable")
 
 
 logger = get_logger(__name__)
 
 
-class TableDataFrame(BaseFrame):
+class TableDataFrame(BaseFrame, SampleMixin):
     """
     TableDataFrame class is a frame encapsulation of the table objects (like event table, item table).
     This class is used to construct the query graph for previewing/sampling underlying table stored at the
     data warehouse. The constructed query graph is stored locally (not loaded into the global query graph).
     """
 
     table_data: BaseTableData
@@ -1003,14 +1005,15 @@
         name: str,
         sample_rows: Optional[int] = None,
         columns: Optional[list[str]] = None,
         columns_rename_mapping: Optional[dict[str, str]] = None,
         context_name: Optional[str] = None,
         skip_entity_validation_checks: Optional[bool] = False,
         primary_entities: Optional[List[str]] = None,
+        target_column: Optional[str] = None,
     ) -> ObservationTable:
         """
         Creates an ObservationTable from the SourceTable.
 
         When you specify the columns and the columns_rename_mapping parameters, make sure that the table has:
 
         - column(s) containing entity values with an accepted serving name.
@@ -1031,14 +1034,18 @@
             column names when creating the observation table. If None, no columns are renamed.
         context_name: Optional[str]
             Context name for the observation table.
         skip_entity_validation_checks: Optional[bool]
             Skip entity validation checks when creating the observation table.
         primary_entities: Optional[List[str]]
             List of primary entities for the observation table.
+        target_column: Optional[str]
+            Name of the column in the observation table that stores the target values.
+            The target column name must match an existing target namespace in the catalog.
+            The data type and primary entities must match the those in the target namespace.
 
         Returns
         -------
         ObservationTable
 
         Examples
         --------
@@ -1079,14 +1086,15 @@
                 columns=columns,
                 columns_rename_mapping=columns_rename_mapping,
             ),
             sample_rows=sample_rows,
             context_id=context_id,
             skip_entity_validation_checks=skip_entity_validation_checks,
             primary_entity_ids=primary_entity_ids,
+            target_column=target_column,
         )
         observation_table_doc = ObservationTable.post_async_task(
             route="/observation_table", payload=payload.json_dict()
         )
         return ObservationTable.get_by_id(observation_table_doc["_id"])
 
     def create_batch_request_table(
```

### Comparing `featurebyte-1.0.2/featurebyte/api/static_source_table.py` & `featurebyte-1.0.3/featurebyte/api/static_source_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTable class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/api/table.py` & `featurebyte-1.0.3/featurebyte/api/table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Table class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict
 
 from bson import ObjectId
 
 from featurebyte.api.base_table import TableListMixin
```

### Comparing `featurebyte-1.0.2/featurebyte/api/target.py` & `featurebyte-1.0.3/featurebyte/api/target.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target API object
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Sequence, Union, cast
 
 import pandas as pd
 from bson import ObjectId
 from pydantic import Field, root_validator
@@ -34,16 +35,14 @@
 from featurebyte.api.templates.series_doc import ISNULL_DOC, NOTNULL_DOC
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.utils import dataframe_to_arrow_bytes, enforce_observation_set_row_order
 from featurebyte.core.accessor.target_datetime import TargetDtAccessorMixin
 from featurebyte.core.accessor.target_string import TargetStrAccessorMixin
 from featurebyte.core.series import Series
 from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.observation_table import TargetInput
-from featurebyte.models.request_input import RequestInputType
 from featurebyte.models.target import TargetModel
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.schema.target import TargetCreate
 from featurebyte.schema.target_table import TargetTableCreate
 
 DOCSTRING_FORMAT_PARAMS = {"class_name": "Target"}
 
@@ -348,33 +347,34 @@
         Examples
         --------
         >>> target = catalog.get_target("target")  # doctest: +SKIP
         >>> target.compute_target_table(observation_table, "target_table")  # doctest: +SKIP
         """
         is_input_observation_table = isinstance(observation_table, ObservationTable)
         observation_table_id = observation_table.id if is_input_observation_table else None
-        input_type = (
-            RequestInputType.OBSERVATION_TABLE
-            if is_input_observation_table
-            else RequestInputType.DATAFRAME
-        )
+
+        if self.saved:
+            target_id = self.id
+            graph = None
+            node_names = None
+        else:
+            target_id = None
+            graph = self.graph
+            node_names = [self.node.name]
+
         target_table_create_params = TargetTableCreate(
             name=observation_table_name,
             observation_table_id=observation_table_id,
             feature_store_id=self.feature_store.id,
             serving_names_mapping=serving_names_mapping,
-            graph=self.graph,
-            node_names=[self.node.name],
-            request_input=TargetInput(
-                target_id=self.id,
-                observation_table_id=observation_table_id,
-                type=input_type,
-            ),
+            graph=graph,
+            node_names=node_names,
             context_id=observation_table.context_id if is_input_observation_table else None,
             skip_entity_validation_checks=skip_entity_validation_checks,
+            target_id=target_id,
         )
         if is_input_observation_table:
             files = None
         else:
             assert isinstance(observation_table, pd.DataFrame)
             files = {"observation_set": dataframe_to_arrow_bytes(observation_table)}
         observation_table_doc = self.post_async_task(
```

### Comparing `featurebyte-1.0.2/featurebyte/api/target_namespace.py` & `featurebyte-1.0.3/featurebyte/api/target_namespace.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 """
 Feature Namespace module.
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import Field
 
 from featurebyte.api.api_handler.base import ListHandler
 from featurebyte.api.api_handler.target_namespace import TargetNamespaceListHandler
 from featurebyte.api.api_object_util import ForeignKeyMapping
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature_or_target_namespace_mixin import FeatureOrTargetNamespaceMixin
 from featurebyte.api.savable_api_object import DeletableApiObject, SavableApiObject
 from featurebyte.common.doc_util import FBAutoDoc
+from featurebyte.enum import DBVarType
 from featurebyte.exception import RecordRetrievalException
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.target_namespace import TargetNamespaceModel
 from featurebyte.schema.target_namespace import TargetNamespaceUpdate
 
 
 class TargetNamespace(FeatureOrTargetNamespaceMixin, DeletableApiObject, SavableApiObject):
@@ -25,14 +27,15 @@
     TargetNamespace represents a Target set, in which all the targets in the set have the same name. The different
     elements typically refer to different versions of a Target.
     """
 
     __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.TargetNamespace")
 
     internal_window: Optional[str] = Field(alias="window")
+    internal_dtype: DBVarType = Field(alias="dtype")
 
     # class variables
     _route = "/target_namespace"
     _update_schema_class = TargetNamespaceUpdate
     _list_schema = TargetNamespaceModel
     _get_schema = TargetNamespaceModel
     _list_fields = [
@@ -43,47 +46,66 @@
     ]
     _list_foreign_keys = [
         ForeignKeyMapping("entity_ids", Entity, "entities"),
     ]
 
     @classmethod
     def create(
-        cls, name: str, primary_entity: List[str], window: Optional[str] = None
+        cls, name: str, primary_entity: List[str], dtype: DBVarType, window: Optional[str] = None
     ) -> TargetNamespace:
         """
         Create a new TargetNamespace.
 
         Parameters
         ----------
         name: str
             Name of the TargetNamespace
         primary_entity: List[str]
             List of entities.
+        dtype: DBVarType
+            Data type of the TargetNamespace
         window: Optional[str]
             Window of the TargetNamespace
 
         Returns
         -------
         TargetNamespace
             The created TargetNamespace
 
         Examples
         --------
         >>> target_namespace = fb.TargetNamespace.create(  # doctest: +SKIP
         ...     name="amount_7d_target",
         ...     window="7d",
+        ...     dtype=DBVarType.FLOAT,
         ...     primary_entity=["customer"]
         ... )
         """
         entity_ids = [Entity.get(entity_name).id for entity_name in primary_entity]
-        target_namespace = TargetNamespace(name=name, entity_ids=entity_ids, window=window)
+        target_namespace = TargetNamespace(
+            name=name, entity_ids=entity_ids, dtype=dtype, window=window
+        )
         target_namespace.save()
         return target_namespace
 
     @property
+    def dtype(self) -> DBVarType:
+        """
+        Database variable type of the target namespace.
+
+        Returns
+        -------
+        DBVarType
+        """
+        try:
+            return self.cached_model.dtype
+        except RecordRetrievalException:
+            return self.internal_dtype
+
+    @property
     def window(self) -> Optional[str]:
         """
         Window of the target namespace.
 
         Returns
         -------
         str
@@ -133,12 +155,13 @@
         - the target namespace is not used in any target
 
         Examples
         --------
         >>> target_namespace = fb.TargetNamespace.create(  # doctest: +SKIP
         ...     name="amount_7d_target",
         ...     window="7d",
+        ...     dtype=DBVarType.FLOAT,
         ...     primary_entity=["customer"]
         ... )
         >>> target_namespace.delete()  # doctest: +SKIP
         """
         self._delete()
```

### Comparing `featurebyte-1.0.2/featurebyte/api/templates/doc_util.py` & `featurebyte-1.0.3/featurebyte/api/templates/doc_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 """
 Utility functions for docstring templating
 """
+
 from typing import Any, Dict, Optional
 
 import textwrap
 from functools import wraps
 
-from featurebyte.common.typing import Func
+from featurebyte.typing import Func
 
 
 def substitute_docstring(
     doc_template: str,
     description: Optional[str] = None,
     parameters: Optional[str] = None,
     returns: Optional[str] = None,
```

### Comparing `featurebyte-1.0.2/featurebyte/api/templates/entity_doc.py` & `featurebyte-1.0.3/featurebyte/api/templates/entity_doc.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/api/templates/feature_or_target_doc.py` & `featurebyte-1.0.3/featurebyte/api/templates/feature_or_target_doc.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/api/use_case.py` & `featurebyte-1.0.3/featurebyte/api/use_case.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 UseCase module
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 import pandas as pd
 from bson import ObjectId
 from pandas import DataFrame
```

### Comparing `featurebyte-1.0.2/featurebyte/api/use_case_or_context_mixin.py` & `featurebyte-1.0.3/featurebyte/api/use_case_or_context_mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base UseCase and Context mixin
 """
+
 from typing import Any, Dict, List, Optional
 
 import pandas as pd
 from typeguard import typechecked
 
 from featurebyte.api.api_object import ApiObject
 from featurebyte.api.api_object_util import iterate_api_object_using_paginated_routes
```

### Comparing `featurebyte-1.0.2/featurebyte/api/user_defined_function.py` & `featurebyte-1.0.3/featurebyte/api/user_defined_function.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 UserDefinedFunction API object
 """
+
 from __future__ import annotations
 
-from typing import Any, ClassVar, Dict, List, Literal, Optional, Union, cast
+from typing import Any, ClassVar, Dict, List, Optional, Union, cast
+from typing_extensions import Literal
 
 from http import HTTPStatus
 
 from bson import ObjectId
 from pydantic import Field
 from typeguard import typechecked
```

### Comparing `featurebyte-1.0.2/featurebyte/api/user_defined_function_injector.py` & `featurebyte-1.0.3/featurebyte/api/user_defined_function_injector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains generic function construction logic for user defined functions.
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, List, Tuple, Type, Union
 
 import inspect
 import textwrap
 from dataclasses import dataclass
@@ -104,15 +105,24 @@
                 else:
                     raise ValueError(f'Parameter "{func_param.name}" is not provided.')
             else:
                 value = kwargs.pop(func_param.name)
         return value, kwargs
 
     @staticmethod
-    def _validate_series_inputs(series_inputs: FuncInputSeriesList) -> None:
+    def _validate_feature_inputs(feature_inputs: List[Feature]) -> None:
+        for index, feat in enumerate(feature_inputs, start=1):
+            if feat.used_request_column:
+                raise ValueError(
+                    f'Error in feature #{index} ("{feat.name}"): This feature was created with a request column '
+                    "and cannot be used as an input to this function. Please change the feature and try again."
+                )
+
+    @classmethod
+    def _validate_series_inputs(cls, series_inputs: FuncInputSeriesList) -> None:
         expected_row_index_lineage = series_inputs[0].row_index_lineage
         check_row_index_lineage = not isinstance(series_inputs[0], Feature)
         expected_series_type = Feature if isinstance(series_inputs[0], Feature) else ViewColumn
         for series_input in series_inputs[1:]:
             if not isinstance(series_input, expected_series_type):
                 # check if all view columns have matching type
                 raise TypeError(
@@ -126,14 +136,17 @@
             ):
                 # check if all view columns have matching row index lineage
                 raise ValueError(
                     f'The row of the input ViewColumns "{series_inputs[0].name}" does not match '
                     f'the row of the input ViewColumns "{series_input.name}".'
                 )
 
+        if expected_series_type is Feature:
+            cls._validate_feature_inputs(series_inputs)  # type: ignore
+
     def _extract_node_parameters(
         self, *args: Any, **kwargs: Any
     ) -> Tuple[List[FunctionParameterInput], FuncInputSeriesList]:
         if len(args) > len(self.function_parameters):
             raise ValueError(
                 f"Too many arguments. Expected {len(self.function_parameters)} but got {len(args)}."
             )
```

### Comparing `featurebyte-1.0.2/featurebyte/api/view.py` & `featurebyte-1.0.3/featurebyte/api/view.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,57 +1,57 @@
 """
 View class
 """
+
 # pylint: disable=too-many-lines,too-many-public-methods
 from __future__ import annotations
 
 from typing import (
     TYPE_CHECKING,
     Any,
     ClassVar,
     Dict,
     List,
-    Literal,
     Optional,
     Tuple,
     Type,
     TypeVar,
     Union,
     cast,
 )
+from typing_extensions import Literal
 
 from abc import ABC, abstractmethod
 from datetime import datetime
 
 import pandas as pd
 from pydantic import PrivateAttr
 from typeguard import typechecked
 
 from featurebyte.api.batch_request_table import BatchRequestTable
 from featurebyte.api.entity import Entity
 from featurebyte.api.feature import Feature
 from featurebyte.api.feature_group import FeatureGroup
+from featurebyte.api.mixin import SampleMixin
 from featurebyte.api.obs_table.utils import get_definition_for_obs_table_creation_from_view
 from featurebyte.api.observation_table import ObservationTable
 from featurebyte.api.static_source_table import StaticSourceTable
 from featurebyte.api.target import Target
 from featurebyte.api.window_validator import validate_offset
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.join_utils import (
     apply_column_name_modifiers,
     apply_column_name_modifiers_columns_info,
     combine_column_info_of_views,
     filter_columns,
     filter_columns_info,
     is_column_name_in_columns,
 )
-from featurebyte.common.typing import ScalarSequence
 from featurebyte.core.frame import Frame, FrozenFrame
 from featurebyte.core.generic import ProtectedColumnsQueryObject, QueryObject
-from featurebyte.core.mixin import SampleMixin
 from featurebyte.core.series import FrozenSeries, FrozenSeriesT, Series
 from featurebyte.enum import DBVarType
 from featurebyte.exception import (
     ChangeViewNoJoinColumnError,
     NoJoinKeyFoundError,
     RepeatedColumnNamesError,
 )
@@ -68,14 +68,15 @@
 )
 from featurebyte.query_graph.node.generic import JoinMetadata, ProjectNode
 from featurebyte.query_graph.node.input import InputNode
 from featurebyte.query_graph.node.nested import BaseGraphNode
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate
 from featurebyte.schema.observation_table import ObservationTableCreate
 from featurebyte.schema.static_source_table import StaticSourceTableCreate
+from featurebyte.typing import ScalarSequence
 
 if TYPE_CHECKING:
     from featurebyte.api.groupby import GroupBy
 else:
     GroupBy = TypeVar("GroupBy")
 
 ViewT = TypeVar("ViewT", bound="View")
@@ -312,42 +313,42 @@
     def as_target(self, target_name: str, offset: Optional[str] = None) -> Target:
         """
         Create a lookup target directly from the column in the View.
 
         For SCD views, lookup targets are materialized through point-in-time joins, and the resulting value represents
         the active row for the natural key at the point-in-time indicated in the target request.
 
-        To obtain a target value at a specific time before the request's point-in-time, an offset can be specified.
+        To obtain a target value at a specific time after the request's point-in-time, an offset can be specified.
 
         Parameters
         ----------
         target_name: str
             Name of the target to create.
         offset: str
-            When specified, retrieve target value as of this offset prior to the point-in-time.
+            When specified, retrieve target value as of this offset after the point-in-time.
 
         Returns
         -------
         Target
 
         Examples
         --------
         >>> customer_view = catalog.get_view("GROCERYCUSTOMER")
         >>> # Extract operating system from BrowserUserAgent column
         >>> customer_view["OperatingSystemIsWindows"] = customer_view.BrowserUserAgent.str.contains("Windows")
         >>> # Create a target from the OperatingSystemIsWindows column
         >>> uses_windows = customer_view.OperatingSystemIsWindows.as_target("UsesWindows")
 
 
-        If the view is a Slowly Changing Dimension View, you may also consider to create a target that retrieves the
-        entity's attribute at a point-in-time prior to the point-in-time specified in the target request by specifying
+        If the view is a Slowly Changing Dimension View, you may also consider creating a target that retrieves the
+        entity's attribute at a specific time after the point-in-time specified in the target request by specifying
         an offset.
 
-        >>> uses_windows_12w_ago = customer_view.OperatingSystemIsWindows.as_target(
-        ...   "UsesWindows_12w_ago", offset="12w"
+        >>> uses_windows_next_12w = customer_view.OperatingSystemIsWindows.as_target(
+        ...   "UsesWindows_next_12w", offset="12w"
         ... )
         """
         view, input_column_name = self._get_view_and_input_col_for_lookup("as_target")
 
         # Perform validation
         validate_offset(offset)
 
@@ -687,15 +688,15 @@
             columns_info=[
                 ColumnInfo(name=col.name, dtype=col.dtype, entity_id=None, semantic_id=None)
                 for col in input_node.parameters.columns
             ],
         )
 
 
-class View(ProtectedColumnsQueryObject, Frame, ABC):
+class View(ProtectedColumnsQueryObject, Frame, SampleMixin, ABC):
     """
     Views are cleaned versions of Catalog tables and offer a flexible and efficient way to work with Catalog tables.
     They allow operations like creating new columns, filtering records, conditionally editing columns, extracting lags,
     capturing attribute changes, and joining views, similar to Pandas. Unlike Pandas DataFrames, which require loading
     all data into memory, views are materialized only when needed during previews or feature materialization.
 
     When a view is created, it inherits the metadata of the Catalog Table it originated from. There are currently five
@@ -1663,14 +1664,15 @@
         name: str,
         sample_rows: Optional[int] = None,
         columns: Optional[list[str]] = None,
         columns_rename_mapping: Optional[dict[str, str]] = None,
         context_name: Optional[str] = None,
         skip_entity_validation_checks: Optional[bool] = False,
         primary_entities: Optional[List[str]] = None,
+        target_column: Optional[str] = None,
     ) -> ObservationTable:
         """
         Creates an ObservationTable from the View.
 
         When you specify the columns and the columns_rename_mapping parameters, make sure that the table has:
 
         - a column containing entity values with an accepted serving name.
@@ -1692,14 +1694,18 @@
         context_name: Optional[str]
             Context name for the observation table.
         skip_entity_validation_checks: Optional[bool]
             Skip entity validation checks when creating the observation table.
         primary_entities: Optional[List[str]]
             List of primary entities for the observation table. If None, the primary entities are
             inferred from the view.
+        target_column: Optional[str]
+            Name of the column in the observation table that stores the target values.
+            The target column name must match an existing target namespace in the catalog.
+            The data type and primary entities must match the those in the target namespace.
 
         Returns
         -------
         ObservationTable
             ObservationTable object.
 
         Raises
@@ -1731,18 +1737,18 @@
                 if columns and column_info.name not in columns:
                     # for special columns that are inherited in the view (like event_id_column),
                     # exclude them from the primary entity check
                     continue
                 if column_info.entity_id:
                     primary_entity_ids.append(column_info.entity_id)
 
-        if not primary_entity_ids:
-            raise ValueError(
-                "No primary entities found. Please specify the primary entities when creating the observation table."
-            )
+            if not primary_entity_ids:
+                raise ValueError(
+                    "No primary entities found. Please specify the primary entities when creating the observation table."
+                )
 
         pruned_graph, mapped_node = self.extract_pruned_graph_and_node()
         definition = get_definition_for_obs_table_creation_from_view(
             pruned_graph,
             mapped_node,
             name,
             sample_rows,
@@ -1762,14 +1768,15 @@
                 columns_rename_mapping=columns_rename_mapping,
                 definition=definition,
             ),
             sample_rows=sample_rows,
             context_id=context_id,
             skip_entity_validation_checks=skip_entity_validation_checks,
             primary_entity_ids=primary_entity_ids,
+            target_column=target_column,
         )
         observation_table_doc = ObservationTable.post_async_task(
             route="/observation_table", payload=payload.json_dict()
         )
         return ObservationTable.get_by_id(observation_table_doc["_id"])
 
     def create_batch_request_table(
```

### Comparing `featurebyte-1.0.2/featurebyte/api/window_validator.py` & `featurebyte-1.0.3/featurebyte/api/window_validator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Validate window parameter input.
 """
+
 from typing import Optional
 
 from featurebyte.common.model_util import parse_duration_string, validate_offset_string
 
 
 def validate_offset(offset: Optional[str]) -> None:
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/app.py` & `featurebyte-1.0.3/featurebyte/app.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 """
 FastAPI Application
 """
+
 from typing import Any, Callable, Coroutine, List, Optional
 
-import aioredis
+import redis.asyncio as redis
 import uvicorn
 from fastapi import Depends, FastAPI, Header, Request
 from starlette.websockets import WebSocket
 
+from featurebyte._overrides.typechecked_override import custom_typechecked
 from featurebyte.common.utils import get_version
-from featurebyte.logging import get_logger
+from featurebyte.logging import configure_featurebyte_logger, get_logger
 from featurebyte.middleware import ExceptionMiddleware
 from featurebyte.models.base import PydanticObjectId, User
 from featurebyte.routes.base_router import BaseRouter
 from featurebyte.routes.batch_feature_table.api import BatchFeatureTableRouter
 from featurebyte.routes.batch_request_table.api import BatchRequestTableRouter
 from featurebyte.routes.catalog.api import CatalogRouter
 from featurebyte.routes.context.api import ContextRouter
@@ -50,16 +52,20 @@
 from featurebyte.schema import APIServiceStatus
 from featurebyte.schema.task import TaskId
 from featurebyte.utils.messaging import REDIS_URI
 from featurebyte.utils.persistent import MongoDBImpl
 from featurebyte.utils.storage import get_storage, get_temp_storage
 from featurebyte.worker import get_celery, get_redis
 
+configure_featurebyte_logger()
 logger = get_logger(__name__)
 
+# import to override typechecked decorator
+_ = custom_typechecked
+
 
 def _dep_injection_func(
     request: Request, active_catalog_id: Optional[PydanticObjectId] = None
 ) -> None:
     """
     Inject dependencies into the requests
 
@@ -212,31 +218,23 @@
         task_id: TaskId
             Task ID.
         """
         await websocket.accept()
         user = User()
         channel = f"task_{user.id}_{task_id}_progress"
 
-        logger.debug("Listening to channel", extra={"channel": channel})
-        redis = await aioredis.from_url(REDIS_URI)
-        sub = redis.pubsub()
-        await sub.subscribe(channel)
-
-        # listen for messages
-        async for message in sub.listen():
-            if message and isinstance(message, dict):
-                data = message.get("data")
-                if isinstance(data, bytes):
-                    await websocket.send_bytes(data)
-
-        # clean up
-        logger.debug("Unsubscribing from channel", extra={"channel": channel})
-        await sub.unsubscribe(channel)
-        await sub.close()
-        redis.close()
+        async with redis.from_url(REDIS_URI) as client:
+            async with client.pubsub() as pubsub:  # type: ignore
+                logger.debug("Listening to channel", extra={"channel": channel})
+                await pubsub.subscribe(channel)
+                async for message in pubsub.listen():
+                    if message and isinstance(message, dict):
+                        data = message.get("data")
+                        if isinstance(data, bytes):
+                            await websocket.send_bytes(data)
 
     # Add exception middleware
     _app.add_middleware(ExceptionMiddleware)
 
     return _app
```

### Comparing `featurebyte-1.0.2/featurebyte/common/date_util.py` & `featurebyte-1.0.3/featurebyte/common/date_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Date related common utility function
 """
+
 from __future__ import annotations
 
 from datetime import datetime, timedelta, timezone
 
 
 def timestamp_utc_to_tile_index(
     input_dt: datetime,
```

### Comparing `featurebyte-1.0.2/featurebyte/common/descriptor.py` & `featurebyte-1.0.3/featurebyte/common/descriptor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains descriptor class.
 """
+
 from typing import Any, Callable, Optional, Type, TypeVar
 
 from functools import partial
 
 MethodT = Callable[..., Any]
 T = TypeVar("T")
```

### Comparing `featurebyte-1.0.2/featurebyte/common/dict_util.py` & `featurebyte-1.0.3/featurebyte/common/dict_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Dictionary related common utility function
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 
 def get_field_path_value(doc_dict: dict[str, Any], field_path: list[str]) -> dict[str, Any] | Any:
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/common/doc_util.py` & `featurebyte-1.0.3/featurebyte/common/doc_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains utility functions related to documentation
 """
+
 from typing import List, Optional
 
 from pydantic import BaseModel, Field
 
 COMMON_SKIPPED_ATTRIBUTES = [
     "Config",
     "Settings",
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/allowed_classes.py` & `featurebyte-1.0.3/featurebyte/common/documentation/allowed_classes.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/autodoc_processor.py` & `featurebyte-1.0.3/featurebyte/common/documentation/autodoc_processor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Autodoc Processor
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 import re
 from xml.etree import ElementTree as etree
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/constants.py` & `featurebyte-1.0.3/featurebyte/common/documentation/constants.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Constants
 """
+
 import inspect
 
 EMPTY_VALUE = inspect._empty
 
 ACTIVATE = "Activate"
 ADD_METADATA = "Add Metadata"
 BATCH_FEATURE_TABLE = "BatchFeatureTable"
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/custom_nav.py` & `featurebyte-1.0.3/featurebyte/common/documentation/custom_nav.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Custom nav module
 """
+
 from typing import Any, Iterable, List, Mapping
 
 from mkdocs_gen_files import Nav  # type: ignore[attr-defined]
 
 from featurebyte.common.documentation.constants import (
     ADD_METADATA,
     BATCH_FEATURE_TABLE,
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/doc_types.py` & `featurebyte-1.0.3/featurebyte/common/documentation/doc_types.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Reused types
 """
+
 from typing import Any, Dict, List, Literal, Optional
 
 import os
 from dataclasses import dataclass
 
 from docstring_parser import DocstringMeta
 from docstring_parser.common import Docstring as BaseDocstring
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/documentation_layout.py` & `featurebyte-1.0.3/featurebyte/common/documentation/documentation_layout.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Layout for API documentation.
 """
+
 from typing import List, Optional
 
 from dataclasses import dataclass
 
 from featurebyte.common.documentation.constants import (
     ADD_METADATA,
     BATCH_FEATURE_TABLE,
@@ -403,14 +404,15 @@
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.saved"]),
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.status"]),
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.updated_at"]),
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.is_default"]),
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.production_ready_fraction"]),
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.default_feature_fraction"]),
         DocLayoutItem([FEATURE_LIST, INFO, "FeatureList.version"]),
+        DocLayoutItem([FEATURE_LIST, LIST, "FeatureList.list_deployments"]),
         DocLayoutItem([FEATURE_LIST, LINEAGE, "FeatureList.catalog_id"]),
         DocLayoutItem([FEATURE_LIST, LINEAGE, "FeatureList.feature_ids"]),
         DocLayoutItem([FEATURE_LIST, LINEAGE, "FeatureList.id"]),
         DocLayoutItem([FEATURE_LIST, LINEAGE, "FeatureList.sql"]),
         DocLayoutItem([FEATURE_LIST, LINEAGE, "FeatureList.primary_entity"]),
         DocLayoutItem([FEATURE_LIST, MANAGE, "FeatureList.get_feature_jobs_status"]),
         DocLayoutItem([FEATURE_LIST, MANAGE, "FeatureList.delete"]),
@@ -838,14 +840,18 @@
             [UTILITY_CLASSES, GROUPBY, "view.GroupBy.aggregate_over"],
             doc_path_override="api.groupby.GroupBy.aggregate_over.md",
         ),
         DocLayoutItem(
             [UTILITY_CLASSES, GROUPBY, "view.GroupBy.forward_aggregate"],
             doc_path_override="api.groupby.GroupBy.forward_aggregate.md",
         ),
+        DocLayoutItem(
+            [UTILITY_CLASSES, GROUPBY, "view.GroupBy.forward_aggregate_asat"],
+            doc_path_override="api.groupby.GroupBy.forward_aggregate_asat.md",
+        ),
         DocLayoutItem([UTILITY_CLASSES, FEATURE, "FeatureVersionInfo"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "DatabricksDetails"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "DatabricksUnityDetails"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "SnowflakeDetails"]),
         DocLayoutItem([UTILITY_CLASSES, WAREHOUSE, "SparkDetails"]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL]),
         DocLayoutItem([UTILITY_CLASSES, CREDENTIAL, "AccessTokenCredential"]),
@@ -1024,14 +1030,15 @@
     Returns
     -------
     List[DocLayoutItem]
         The layout for the HistoricalFeatureTable module.
     """
     return [
         *_get_materialized_table_layout(HISTORICAL_FEATURE_TABLE),
+        DocLayoutItem([HISTORICAL_FEATURE_TABLE, LIST, "HistoricalFeatureTable.list_deployments"]),
         DocLayoutItem(
             [HISTORICAL_FEATURE_TABLE, LINEAGE, "HistoricalFeatureTable.feature_list_id"]
         ),
         DocLayoutItem(
             [HISTORICAL_FEATURE_TABLE, LINEAGE, "HistoricalFeatureTable.observation_table_id"]
         ),
         DocLayoutItem(
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/extract_csv.py` & `featurebyte-1.0.3/featurebyte/common/documentation/extract_csv.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Extract documentation into a CSV file.
 """
+
 from typing import Dict, List, Literal
 
 import csv
 from dataclasses import dataclass
 
 from featurebyte.common.documentation.doc_types import DocItems
 from featurebyte.common.documentation.documentation_layout import get_overall_layout
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/formatters.py` & `featurebyte-1.0.3/featurebyte/common/documentation/formatters.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Formatters
 """
+
 from typing import Any, ForwardRef, Optional, TypeVar
 
 import inspect
 from enum import Enum
 
 from pydantic.fields import Undefined
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/gen_ref_pages_docs_builder.py` & `featurebyte-1.0.3/featurebyte/common/documentation/gen_ref_pages_docs_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 Code to run in mkdocs#gen_ref_pages.py
 
 This is placed in here so that it can be imported as part of the featurebyte package.
 """
+
 from typing import Any, Dict, Generator, List, Optional, Tuple, Union
 
 import importlib
 import inspect
 import json
 import os
 from dataclasses import dataclass
@@ -289,20 +290,20 @@
         member_proxy_path = None
         if autodoc_config.proxy_class:
             # proxy class name not specified, only proxy path used
             member_proxy_path = ".".join([proxy_path, class_obj.__name__])
             member_doc_group = class_doc_group + [attribute_name]
         else:
             member_doc_group = class_doc_group + [attribute_name]
-        doc_groups[
-            DocGroupKey(class_obj.__module__, class_obj.__name__, attribute_name)
-        ] = DocGroupValue(
-            member_doc_group,
-            attribute_type,
-            member_proxy_path,
+        doc_groups[DocGroupKey(class_obj.__module__, class_obj.__name__, attribute_name)] = (
+            DocGroupValue(
+                member_doc_group,
+                attribute_type,
+                member_proxy_path,
+            )
         )
     return doc_groups
 
 
 def should_skip_path(components: List[str]) -> bool:
     """
     Check whether to skip path.
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/markdown_extension/extension.py` & `featurebyte-1.0.3/featurebyte/common/documentation/markdown_extension/extension.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureByte Autodoc Extension for mkdocs
 """
+
 from __future__ import annotations
 
 from markdown import Markdown
 from markdown.extensions import Extension
 
 from featurebyte.common.documentation.autodoc_processor import FBAutoDocProcessor
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/pydantic_field_docs.py` & `featurebyte-1.0.3/featurebyte/common/documentation/pydantic_field_docs.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Docstrings for pydantic field overrides for a class.
 """
+
 from typing import Dict
 
 CATALOG_ID = "catalog_id"
 CREATED_AT = "created_at"
 DTYPE = "dtype"
 ENTITY_IDS = "entity_ids"
 FEATURE_STORE = "feature_store"
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/resource_extractor.py` & `featurebyte-1.0.3/featurebyte/common/documentation/resource_extractor.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Extract resource details given a path descriptor.
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, get_type_hints
 
 import inspect
 import re
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/common/documentation/resource_util.py` & `featurebyte-1.0.3/featurebyte/common/documentation/resource_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Resource util.
 """
+
 from typing import Any
 
 import inspect
 
 from mkautodoc.extension import import_from_string
```

### Comparing `featurebyte-1.0.2/featurebyte/common/env_util.py` & `featurebyte-1.0.3/featurebyte/common/env_util.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains utility functions related to execution environment
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 
 def is_notebook() -> bool:
     """
@@ -44,10 +45,10 @@
     ----------
     html_content: str
         HTML content to display
     """
 
     if is_notebook():
         # pylint: disable=import-outside-toplevel
-        from IPython.display import HTML, display  # type: ignore
+        from IPython.display import HTML, display
 
         display(HTML(html_content), metadata={"isolated": True})
```

### Comparing `featurebyte-1.0.2/featurebyte/common/formatting_util.py` & `featurebyte-1.0.3/featurebyte/common/formatting_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utility functions for formatting data in Jupyter notebooks
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Union
 
 import copy
 import re
 from datetime import datetime
```

### Comparing `featurebyte-1.0.2/featurebyte/common/join_utils.py` & `featurebyte-1.0.3/featurebyte/common/join_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Join utils class
 """
+
 from typing import List, Optional, Set
 
 import copy
 
 from featurebyte.query_graph.model.column_info import ColumnInfo
```

### Comparing `featurebyte-1.0.2/featurebyte/common/model_util.py` & `featurebyte-1.0.3/featurebyte/common/model_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains the implementation of feature job setting validation
 """
+
 from __future__ import annotations
 
 from typing import Any, Tuple
 
 import re
 from datetime import datetime
```

### Comparing `featurebyte-1.0.2/featurebyte/common/path_util.py` & `featurebyte-1.0.3/featurebyte/common/path_util.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utilities to retrieve paths related information
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 import importlib
 import os
 import pkgutil
```

### Comparing `featurebyte-1.0.2/featurebyte/common/string.py` & `featurebyte-1.0.3/featurebyte/common/string.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 String utilities.
 """
+
 import unicodedata
 
 
 def sanitize_identifier(string: str) -> str:
     """
     Sanitizes a string to make it a valid identifier for database tables and columns.
```

### Comparing `featurebyte-1.0.2/featurebyte/common/typing.py` & `featurebyte-1.0.3/featurebyte/typing.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 Common utilities related to typing
 """
-from __future__ import annotations
 
-from typing import Any, Callable, Literal, Optional, Sequence, Type, Union, cast
+from typing import Any, Callable, Optional, Sequence, Type, Union, cast
+from typing_extensions import Literal
 
 import pandas as pd
 from pandas.api.types import is_scalar
 from pydantic import StrictFloat, StrictInt, StrictStr
 
 DatetimeSupportedPropertyType = Literal[
     "year",
```

### Comparing `featurebyte-1.0.2/featurebyte/common/utils.py` & `featurebyte-1.0.3/featurebyte/common/utils.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,37 +1,35 @@
 """
-Utility functions for API Objects
+Common utility functions
 """
+
 from __future__ import annotations
 
 from typing import Any, Generator, Iterator, List, Optional, Union
 
 import ast
 import functools
+import json
 import logging
 import os
 import time
 from contextlib import contextmanager
 from datetime import datetime
 from decimal import Decimal
 from importlib import metadata as importlib_metadata
-from pathlib import Path
 
 import numpy as np
 import pandas as pd
 import pyarrow as pa
-import pyarrow.parquet as pq
-from alive_progress import alive_bar
 from dateutil import parser
-from pandas.core.dtypes.common import is_string_dtype
-from requests import Response
 
-from featurebyte.common.env_util import get_alive_bar_additional_params
 from featurebyte.enum import DBVarType, InternalName
 
+ARROW_METADATA_DB_VAR_TYPE = b"db_var_type"
+
 
 class ResponseStream:
     """
     Simulate a buffer-like object from a streamed response content iterator
     """
 
     def __init__(self, request_iterator: Iterator[bytes]) -> None:
@@ -115,14 +113,41 @@
     sink = pa.BufferOutputStream()
     create_new_arrow_stream_writer(sink, table.schema).write_table(table)
     data = sink.getvalue().to_pybytes()
     assert isinstance(data, bytes)
     return data
 
 
+def dataframe_from_arrow_table(arrow_table: pa.Table) -> pd.DataFrame:
+    """
+    Convert arrow table to pandas dataframe, handling list and map types
+
+    Parameters
+    ----------
+    arrow_table: pa.Table
+        Arrow table object
+
+    Returns
+    -------
+    pd.DataFrame
+        Pandas Dataframe object
+    """
+    # handle conversion of list and map types
+    dataframe = arrow_table.to_pandas()
+    encoded_types = DBVarType.dictionary_types().union(DBVarType.array_types())
+    for field in arrow_table.schema:
+        if field.metadata and ARROW_METADATA_DB_VAR_TYPE in field.metadata:
+            db_var_type = field.metadata[ARROW_METADATA_DB_VAR_TYPE].decode()
+            if db_var_type in encoded_types:
+                dataframe[field.name] = dataframe[field.name].apply(
+                    lambda x: json.loads(x) if x else None
+                )
+    return dataframe
+
+
 def dataframe_from_arrow_stream(buffer: Any) -> pd.DataFrame:
     """
     Read data from arrow byte stream to pandas dataframe
 
     Parameters
     ----------
     buffer: Any
@@ -130,15 +155,16 @@
 
     Returns
     -------
     pd.DataFrame
         Pandas Dataframe object
     """
     reader = pa.ipc.open_stream(buffer)
-    return reader.read_all().to_pandas()
+    arrow_table = reader.read_all()
+    return dataframe_from_arrow_table(arrow_table)
 
 
 def literal_eval(value: Any) -> Any:
     """
     Runs ast.literal_eval on value, if it fails, return value
 
     Parameters
@@ -151,69 +177,14 @@
     Any
     """
     if value is None:
         return value
     return ast.literal_eval(value)
 
 
-def _update_batches_for_types(
-    batches: List[pa.RecordBatch], col_name_to_db_var_type: dict[str, DBVarType]
-) -> List[pa.RecordBatch]:
-    # Currently, we only need to perform updates if we have an ARRAY type.
-    if all(
-        array_type not in col_name_to_db_var_type.values() for array_type in DBVarType.array_types()
-    ):
-        return batches
-
-    output_list = []
-    for batch in batches:
-        curr_df = batch.to_pandas()
-        for col_name, db_var_type in col_name_to_db_var_type.items():
-            if db_var_type in DBVarType.array_types():
-                # Check if column is of string dtype
-                if is_string_dtype(curr_df[col_name]):
-                    # Apply a transformation to the column if the type is an array of strings, to convert them to list
-                    # type.
-                    curr_df[col_name] = curr_df[col_name].apply(literal_eval)
-        output_list.append(pa.RecordBatch.from_pandas(curr_df))
-
-    return output_list
-
-
-def pa_table_to_record_batches(
-    table: pa.Table, col_name_to_db_var_type: Optional[dict[str, DBVarType]] = None
-) -> List[pa.RecordBatch]:
-    """
-    Convert pyarrow table to list of RecordBatch object, with special handling
-    include schema in output for empty table
-
-    Parameters
-    ----------
-    table: pa.Table
-        PyArrow Table object
-    col_name_to_db_var_type: Optional[dict[str, DBVarType]]
-        Dict mapping column name to DBVarType
-
-    Returns
-    -------
-    Any
-        List of RecordBatch objects
-    """
-    if table.shape[0]:
-        # No mapping means we don't have to perform any updates.
-        if not col_name_to_db_var_type:
-            return table.to_batches()  # type: ignore[no-any-return]
-
-        return _update_batches_for_types(table.to_batches(), col_name_to_db_var_type)
-
-    # convert to pandas in order to create empty record batch with schema
-    # there is no way to get empty record batch from pyarrow table directly
-    return [pa.RecordBatch.from_pandas(table.to_pandas())]
-
-
 def prepare_dataframe_for_json(dataframe: pd.DataFrame) -> None:
     """
     Process pandas dataframe in-place before converting to json
 
     Parameters
     ----------
     dataframe: pd.DataFrame
@@ -318,47 +289,14 @@
             if dtype == DBVarType.TIMESTAMP_TZ:
                 dataframe[col_name] = dataframe[col_name].apply(_to_datetime)
             else:
                 raise NotImplementedError()
     return dataframe
 
 
-def parquet_from_arrow_stream(response: Response, output_path: Path, num_rows: int) -> None:
-    """
-    Write parquet file from arrow byte stream
-
-    Parameters
-    ----------
-    response: Response
-        Streamed http response
-    output_path: Path
-        Output path
-    num_rows: int
-        Number of rows to write
-    """
-    reader = pa.ipc.open_stream(ResponseStream(response.iter_content(1024)))
-    batch = reader.read_next_batch()
-    with pq.ParquetWriter(output_path, batch.schema) as writer:
-        try:
-            with alive_bar(
-                total=num_rows,
-                title="Downloading table",
-                **get_alive_bar_additional_params(),
-            ) as progress_bar:
-                while True:
-                    table = pa.Table.from_batches([batch])
-                    if table.num_rows == 0:
-                        break
-                    writer.write_table(table)
-                    progress_bar(table.num_rows)  # pylint: disable=not-callable
-                    batch = reader.read_next_batch()
-        except StopIteration:
-            pass
-
-
 def validate_datetime_input(value: Union[datetime, str]) -> str:
     """
     Validate datetime input value
 
     Parameters
     ---------
     value: Union[datetime, str]
```

### Comparing `featurebyte-1.0.2/featurebyte/common/validator.py` & `featurebyte-1.0.3/featurebyte/common/validator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """This module contains validators used for model input validation"""
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Set, Tuple
 
 from featurebyte.common.model_util import convert_version_string_to_dict, parse_duration_string
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.model.column_info import ColumnInfo
```

### Comparing `featurebyte-1.0.2/featurebyte/config.py` & `featurebyte-1.0.3/featurebyte/config.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Read configurations from ini file
 """
+
 from typing import Any, Dict, Iterator, List, Optional, Union, cast
 
 import json
 import os
 import ssl
 import time
 from contextlib import contextmanager
@@ -522,18 +523,18 @@
         Returns
         -------
         APIClient
             API client
         """
         if not is_server_mode():
             # pylint: disable=import-outside-toplevel,cyclic-import
-            from featurebyte.logging import reconfigure_loggers
+            from featurebyte.logging import configure_featurebyte_logger
 
             # configure logger
-            reconfigure_loggers(self)
+            configure_featurebyte_logger(self)
 
         return APIClient(
             api_url=self.profile.api_url,
             api_token=self.profile.api_token,
             ssl_verify=self.profile.ssl_verify,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/conftest.py` & `featurebyte-1.0.3/featurebyte/conftest.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Pytest configuration file for doctest
 """
+
 import pandas
 import pytest
 from alive_progress import config_handler
 
 import featurebyte
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/count_dict.py` & `featurebyte-1.0.3/featurebyte/core/accessor/count_dict.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 """
 This module contains count_dict accessor class
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, TypeVar, Union
 
 from typeguard import typechecked
 
 from featurebyte.api.feature_validation_util import assert_is_lookup_feature
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import Scalar
 from featurebyte.core.series import DefaultSeriesBinaryOperator
 from featurebyte.core.util import SeriesBinaryOperator, series_unary_operation
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.count_dict import GetValueFromDictionaryNode
+from featurebyte.typing import Scalar
 
 if TYPE_CHECKING:
     from featurebyte.api.feature import Feature
 else:
     Feature = TypeVar("Feature")
 
 
@@ -119,15 +120,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-        '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+        {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         New feature:
 
         >>> df["CustomerProductGroupCountsEntropy_7d"].iloc[0]
         1.475076311054695
         """
@@ -158,15 +159,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-        '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+        {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         New feature:
 
         >>> df["CustomerProductGroupCountsMostFrequent_7d"].iloc[0]
         'Colas, Thés glacés et Sodas'
         """
@@ -199,15 +200,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupTotalCost_7d"].iloc[0]
-        '{"Chips et Tortillas":2.0,"Colas, Thés glacés et Sodas":10.0,"Crèmes et Chantilly":0.75,"Pains":1.09,"Œufs":1.19}'
+        {'Chips et Tortillas': 2.0, 'Colas, Thés glacés et Sodas': 10.0, 'Crèmes et Chantilly': 0.75, 'Pains': 1.09, 'Œufs': 1.19}
 
         New feature:
 
         >>> df["CustomerProductGroupWithHighestTotalCost_7d"].iloc[0]
         'Colas, Thés glacés et Sodas'
         """
         return self._make_operation("key_with_highest_value", DBVarType.VARCHAR)
@@ -237,15 +238,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupTotalCost_7d"].iloc[0]
-        '{"Chips et Tortillas":2.0,"Colas, Thés glacés et Sodas":10.0,"Crèmes et Chantilly":0.75,"Pains":1.09,"Œufs":1.19}'
+        {'Chips et Tortillas': 2.0, 'Colas, Thés glacés et Sodas': 10.0, 'Crèmes et Chantilly': 0.75, 'Pains': 1.09, 'Œufs': 1.19}
 
         New feature:
 
         >>> df["CustomerProductGroupWithLowestTotalCost_7d"].iloc[0]
         'Crèmes et Chantilly'
         """
         return self._make_operation("key_with_lowest_value", DBVarType.VARCHAR)
@@ -280,21 +281,21 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-        '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+        {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         New feature:
 
         >>> df["CustomerProductGroupCountsUniqueCount_7d"].iloc[0]
-        5
+        5.0
         """
         return self._make_operation(
             "unique_count",
             DBVarType.FLOAT,
             additional_params={"include_missing": include_missing},
         )
 
@@ -327,21 +328,21 @@
         >>> features = fb.FeatureGroup([feature_1, feature_2, similarity])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature 1:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-         '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+         {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         Dictionary feature 2:
 
         >>> df["CustomerProductGroupCounts_90d"].iloc[0]
-        '{"Biscuits apéritifs":1,"Biscuits":1,"Bonbons":1,"Chips et Tortillas":2,"Colas, Thés glacés et Sodas":12,"Confitures":1,"Crèmes et Chantilly":2,"Céréales":1,"Emballages et sacs":1,"Fromages":3,"Glaces et Sorbets":1,"Glaçons":1,"Laits":4,"Noix":1,"Pains":4,"Petit-déjeuner":2,"Viande Surgelée":1,"Œufs":1}'
+        {'Biscuits apéritifs': 1, 'Biscuits': 1, 'Bonbons': 1, 'Chips et Tortillas': 2, 'Colas, Thés glacés et Sodas': 12, 'Confitures': 1, 'Crèmes et Chantilly': 2, 'Céréales': 1, 'Emballages et sacs': 1, 'Fromages': 3, 'Glaces et Sorbets': 1, 'Glaçons': 1, 'Laits': 4, 'Noix': 1, 'Pains': 4, 'Petit-déjeuner': 2, 'Viande Surgelée': 1, 'Œufs': 1}
 
 
         Similarity feature:
 
         >>> df["CustomerProductGroupCounts_7d_90d_similarity"].iloc[0]
         0.8653846153846161
         """
@@ -380,15 +381,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-        '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+        {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         New feature:
 
         >>> df["Chips et Tortillas Value"].iloc[0]
         1
         """
@@ -443,15 +444,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-        '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+        {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         New feature:
 
         >>> df["Chips et Tortillas Rank"].iloc[0]
         1.0
         """
@@ -502,15 +503,15 @@
         >>> features = fb.FeatureGroup([counts, new_feature])
         >>> df = features.preview(pd.DataFrame([{"POINT_IN_TIME": "2022-04-15 10:00:00", "GROCERYCUSTOMERGUID": "2f4c1578-29d6-44b7-83da-7c5bfb981fa0"}]))
 
 
         Dictionary feature:
 
         >>> df["CustomerProductGroupCounts_7d"].iloc[0]
-        '{"Chips et Tortillas":1,"Colas, Thés glacés et Sodas":3,"Crèmes et Chantilly":1,"Pains":1,"Œufs":1}'
+        {'Chips et Tortillas': 1, 'Colas, Thés glacés et Sodas': 3, 'Crèmes et Chantilly': 1, 'Pains': 1, 'Œufs': 1}
 
 
         New feature:
 
         >>> df["Chips et Tortillas Relative Frequency"].iloc[0]
         0.14285714285714302
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/datetime.py` & `featurebyte-1.0.3/featurebyte/core/accessor/datetime.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains datetime accessor class
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Iterable, Optional, Union, cast
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.model_util import validate_timezone_offset_string
 from featurebyte.core.util import series_binary_operation, series_unary_operation
@@ -414,19 +415,19 @@
 
         Compute the second component of a timestamp column:
 
         >>> view = catalog.get_view("GROCERYINVOICE")
         >>> view["TimestampSecond"] = view["Timestamp"].dt.second
         >>> view.preview(5).filter(regex="Timestamp")
                     Timestamp  TimestampSecond
-        0 2022-01-03 12:28:58             58.0
-        1 2022-01-03 16:32:15             15.0
-        2 2022-01-07 16:20:04              4.0
-        3 2022-01-10 16:18:32             32.0
-        4 2022-01-12 17:36:23             23.0
+        0 2022-01-03 12:28:58               58
+        1 2022-01-03 16:32:15               15
+        2 2022-01-07 16:20:04                4
+        3 2022-01-10 16:18:32               32
+        4 2022-01-12 17:36:23               23
         """
         return self._make_operation("second")
 
     @property
     def millisecond(self) -> FrozenSeries:
         """
         Returns the millisecond component of each element.
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/feature_datetime.py` & `featurebyte-1.0.3/featurebyte/core/accessor/feature_datetime.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature datetime accessor module.
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, TypeVar, Union
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.core.accessor.datetime import DatetimeAccessor
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/feature_string.py` & `featurebyte-1.0.3/featurebyte/core/accessor/feature_string.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature string accessor module.
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Optional, TypeVar
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.core.accessor.string import StringAccessor
 from featurebyte.query_graph.node.string import Side
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/string.py` & `featurebyte-1.0.3/featurebyte/core/accessor/string.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains string accessor class
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Optional, TypeVar
 
 from typeguard import typechecked
 
 from featurebyte.common.doc_util import FBAutoDoc
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/target_datetime.py` & `featurebyte-1.0.3/featurebyte/core/accessor/target_datetime.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target datetime accessor module.
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, TypeVar, Union
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.core.accessor.datetime import DatetimeAccessor
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/target_string.py` & `featurebyte-1.0.3/featurebyte/core/accessor/target_string.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target string accessor module.
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Optional, TypeVar
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.core.accessor.string import StringAccessor
 from featurebyte.query_graph.node.string import Side
```

### Comparing `featurebyte-1.0.2/featurebyte/core/accessor/vector.py` & `featurebyte-1.0.3/featurebyte/core/accessor/vector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Vector accessor module
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, TypeVar
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.core.util import series_binary_operation
 from featurebyte.enum import DBVarType
```

### Comparing `featurebyte-1.0.2/featurebyte/core/distance.py` & `featurebyte-1.0.3/featurebyte/core/distance.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for distance related operations
 """
+
 from typing import Any, List, Optional
 
 from typeguard import typechecked
 
 from featurebyte import Feature, Target
 from featurebyte.api.view import ViewColumn
 from featurebyte.core.series import Series
```

### Comparing `featurebyte-1.0.2/featurebyte/core/frame.py` & `featurebyte-1.0.3/featurebyte/core/frame.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 """
 Frame class
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple, TypeVar, Union
 
 import pandas as pd
 from pydantic import Field, validator
 from typeguard import typechecked
 
 from featurebyte.core.generic import QueryObject
-from featurebyte.core.mixin import GetAttrMixin, OpsMixin, SampleMixin
+from featurebyte.core.mixin import GetAttrMixin, OpsMixin
 from featurebyte.core.series import FrozenSeries, Series
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.node.validator import construct_unique_name_validator
 
 
-class BaseFrame(QueryObject, SampleMixin):
+class BaseFrame(QueryObject):
     """
     BaseFrame class
 
     Parameters
     ----------
     columns_info: List[ColumnInfo]
         List of column specifications that are contained in this frame.
```

### Comparing `featurebyte-1.0.2/featurebyte/core/generic.py` & `featurebyte-1.0.3/featurebyte/core/generic.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 """
 This module generic query object classes
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Callable, Dict, Optional, Tuple, TypeVar, cast
 
 import json
 import operator
 from abc import abstractmethod
 
 from cachetools import LRUCache, cachedmethod
 from cachetools.keys import hashkey
 from pydantic import Field, root_validator
+from typeguard import typechecked
 
 from featurebyte.common.utils import get_version
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.query_graph.algorithm import dfs_traversal
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.graph import GlobalQueryGraph, QueryGraph
@@ -23,14 +25,15 @@
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.metadata.operation import (
     NodeOutputCategory,
     OperationStructure,
     OperationStructureInfo,
 )
+from featurebyte.query_graph.sql.interpreter import GraphInterpreter
 from featurebyte.query_graph.transform.flattening import GraphFlatteningTransformer
 from featurebyte.query_graph.transform.sdk_code import SDKCodeExtractor
 
 if TYPE_CHECKING:
     from pydantic.typing import AbstractSetIntStr, MappingIntStrAny
 
 
@@ -310,14 +313,35 @@
     def clear_operation_structure_cache(cls) -> None:
         """
         Clear the operation structure cache
         """
         del cls._operation_structure_cache
         cls._operation_structure_cache = _create_operation_structure_cache()
 
+    @typechecked
+    def preview_sql(self, limit: int = 10, **kwargs: Any) -> str:
+        """
+        Generate SQL query to preview the transformation output
+
+        Parameters
+        ----------
+        limit: int
+            maximum number of return rows
+        **kwargs: Any
+            Additional keyword parameters
+
+        Returns
+        -------
+        str
+        """
+        pruned_graph, mapped_node = self.extract_pruned_graph_and_node(**kwargs)
+        return GraphInterpreter(
+            pruned_graph, source_type=self.feature_store.type
+        ).construct_preview_sql(node_name=mapped_node.name, num_rows=limit)[0]
+
 
 class ProtectedColumnsQueryObject(QueryObject):
     """
     QueryObject contains at least one or more protected column(s). The protected column should not be overridden
     or remove from the parent node.
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/core/mixin.py` & `featurebyte-1.0.3/featurebyte/api/mixin.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,237 +1,220 @@
 """
-Mixin classes used by core objects
+This module contains the mixin class used by api objects.
 """
+
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, Dict, Iterable, Optional, Protocol, Tuple, Union
+from typing import Any, Optional, Tuple, Union
 
 import time
-from abc import abstractmethod
 from datetime import datetime
-from functools import wraps
 from http import HTTPStatus
 
 import pandas as pd
-from pydantic import BaseModel, PrivateAttr, StrictStr
+from alive_progress import alive_bar
+from requests import Response
 from typeguard import typechecked
 
+from featurebyte.api.api_object_util import ProgressThread
+from featurebyte.common.env_util import get_alive_bar_additional_params
 from featurebyte.common.utils import dataframe_from_json, validate_datetime_input
 from featurebyte.config import Configurations
-from featurebyte.enum import DBVarType
-from featurebyte.exception import RecordRetrievalException
+from featurebyte.core.mixin import HasExtractPrunedGraphAndNode, perf_logging
+from featurebyte.exception import (
+    RecordCreationException,
+    RecordRetrievalException,
+    RecordUpdateException,
+    ResponseException,
+)
 from featurebyte.logging import get_logger
-from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.query_graph.enum import NodeOutputType, NodeType
-from featurebyte.query_graph.graph import GlobalQueryGraph
-from featurebyte.query_graph.model.graph import QueryGraphModel
-from featurebyte.query_graph.node import Node
-from featurebyte.query_graph.sql.interpreter import GraphInterpreter
+from featurebyte.models.base import FeatureByteBaseModel, get_active_catalog_id
 from featurebyte.schema.feature_store import (
     FeatureStorePreview,
     FeatureStoreSample,
     FeatureStoreShape,
 )
+from featurebyte.schema.task import TaskStatus
 
-if TYPE_CHECKING:
-    from featurebyte.core.frame import FrozenFrame
-    from featurebyte.core.series import FrozenSeries
-
+POLLING_INTERVAL = 3
 
 logger = get_logger(__name__)
 
 
-def perf_logging(func: Any) -> Any:
+class AsyncMixin(FeatureByteBaseModel):
     """
-    Decorator to log function execution time.
-
-    Parameters
-    ----------
-    func: Any
-        Function to decorate
-
-    Returns
-    -------
-    Any
+    AsyncMixin class for async task
     """
 
-    @wraps(func)
-    def wrapper(*args: Any, **kwargs: Any) -> Any:
-        start = time.time()
-        result = func(*args, **kwargs)
-        elapsed = time.time() - start
-        logger.debug(f"Function {func.__name__} took {elapsed} seconds")
-        return result
-
-    return wrapper
-
+    @classmethod
+    def _poll_async_task(
+        cls,
+        task_response: Response,
+        delay: float = POLLING_INTERVAL,
+        retrieve_result: bool = True,
+        has_output_url: bool = True,
+        task_failure_exception_class: type[ResponseException] = RecordCreationException,
+    ) -> dict[str, Any]:
+        response_dict = task_response.json()
+        status = response_dict["status"]
+        task_id = response_dict["id"]
 
-class OpsMixin:
-    """
-    OpsMixin contains common properties & operations shared between Frame & Series
-    """
-
-    @property
-    def pytype_dbtype_map(self) -> dict[Any, DBVarType]:
-        """
-        Supported python builtin scalar type to database type mapping
+        # poll the task route (if the task is still running)
+        client = Configurations().get_client()
+        task_get_response = None
 
-        Returns
-        -------
-        dict
-            mapping from supported builtin type to DB type
+        with alive_bar(
+            manual=True,
+            title="Working...",
+            **get_alive_bar_additional_params(),
+        ) as progress_bar:
+            try:
+                # create progress update thread
+                thread = ProgressThread(task_id=task_id, progress_bar=progress_bar)
+                thread.daemon = True
+                thread.start()
+
+                while status in [
+                    TaskStatus.STARTED,
+                    TaskStatus.PENDING,
+                ]:  # retrieve task status
+                    task_get_response = client.get(url=f"/task/{task_id}")
+                    if task_get_response.status_code == HTTPStatus.OK:
+                        status = task_get_response.json()["status"]
+                        time.sleep(delay)
+                    else:
+                        raise RecordRetrievalException(task_get_response)
+
+                if status == TaskStatus.SUCCESS:
+                    progress_bar.title = "Done!"
+                    progress_bar(1)  # pylint: disable=not-callable
+            except KeyboardInterrupt:
+                # try to revoke task
+                client.patch(f"/task/{task_id}", json={"revoke": True})
+                raise
+            finally:
+                thread.raise_exception()
+                thread.join(timeout=0)
+
+        # check the task status
+        if status != TaskStatus.SUCCESS:
+            raise task_failure_exception_class(response=task_get_response or task_response)
+
+        # retrieve task result
+        output_url = response_dict.get("output_path")
+        if output_url is None and task_get_response:
+            output_url = task_get_response.json().get("output_path")
+        if output_url is None and has_output_url:
+            raise RecordRetrievalException(response=task_get_response or task_response)
+
+        if not retrieve_result:
+            return {"output_url": output_url}
+
+        logger.debug("Retrieving task result", extra={"output_url": output_url})
+        result_response = client.get(url=output_url)
+        if result_response.status_code == HTTPStatus.OK:
+            return dict(result_response.json())
+        raise RecordRetrievalException(response=result_response)
+
+    @classmethod
+    def post_async_task(
+        cls,
+        route: str,
+        payload: dict[str, Any],
+        delay: float = POLLING_INTERVAL,
+        retrieve_result: bool = True,
+        has_output_url: bool = True,
+        is_payload_json: bool = True,
+        files: Optional[dict[str, Any]] = None,
+    ) -> dict[str, Any]:
         """
-        return {
-            bool: DBVarType.BOOL,
-            int: DBVarType.INT,
-            float: DBVarType.FLOAT,
-            str: DBVarType.VARCHAR,
-            pd.Timestamp: DBVarType.TIMESTAMP,
-        }
-
-    @staticmethod
-    def _add_filter_operation(
-        item: FrozenFrame | FrozenSeries, mask: FrozenSeries, node_output_type: NodeOutputType
-    ) -> Node:
-        """
-        Add filter node into the graph & return the node
+        Post async task to the worker & retrieve the results (blocking)
 
         Parameters
         ----------
-        item: FrozenFrame | FrozenSeries
-            object to be filtered
-        mask: FrozenSeries
-            mask used to filter the item object
-        node_output_type: NodeOutputType
-            note output type
+        route: str
+            Async task route
+        payload: dict[str, Any]
+            Task payload
+        delay: float
+            Delay used in polling the task
+        retrieve_result: bool
+            Whether to retrieve result from output_url
+        has_output_url: bool
+            Whether the task response has output_url
+        is_payload_json: bool
+            Whether the payload should be passed via the json parameter. If False, the payload will
+            be passed via the data parameter. Set this to False for routes that expects
+            multipart/form-data encoding.
+        files: Optional[dict[str, Any]]
+            Optional files to be passed to the request
 
         Returns
         -------
-        Node
-            Filter node
+        dict[str, Any]
+            Response data
 
         Raises
         ------
-        TypeError
-            if mask Series is not boolean type
-        ValueError
-            if the row index between item object & mask are not aligned
-        """
-        if mask.dtype != DBVarType.BOOL:
-            raise TypeError("Only boolean Series filtering is supported!")
-        if item.row_index_lineage != mask.row_index_lineage:
-            raise ValueError(f"Row indices between '{item}' and '{mask}' are not aligned!")
-
-        node = GlobalQueryGraph().add_operation(
-            node_type=NodeType.FILTER,
-            node_params={},
-            node_output_type=node_output_type,
-            input_nodes=[item.node, mask.node],
-        )
-        return node
-
-
-class ParentMixin(BaseModel):
-    """
-    ParentMixin stores the parent object of the current object
-    """
-
-    _parent: Any = PrivateAttr(default=None)
-
-    @property
-    def parent(self) -> Any:
-        """
-        Parent Frame object of the current series
-
-        Returns
-        -------
-        Any
-        """
-        return self._parent
-
-    def set_parent(self, parent: Any) -> None:
-        """
-        Set parent of the current object
-
-        Parameters
-        ----------
-        parent: Any
-            Parent which current series belongs to
+        RecordCreationException
+            When unexpected creation failure
         """
-        self._parent = parent
-
-
-class HasColumnVarTypeMap(Protocol):
-    """
-    Class with column_var_type_map attribute / property
-    """
-
-    column_var_type_map: Dict[StrictStr, DBVarType]
-
-
-class GetAttrMixin:
-    """
-    GetAttrMixin contains some helper methods to access column in a frame like object
-    """
-
-    def __dir__(self: HasColumnVarTypeMap) -> Iterable[str]:
-        # provide column name lookup and completion for __getattr__
-        attrs = set(object.__dir__(self))
-        attrs.difference_update(dir(BaseModel))
-        return attrs.union(self.column_var_type_map)
-
-    def _ipython_key_completions_(self: HasColumnVarTypeMap) -> set[str]:
-        # provide column name lookup and completion for __getitem__
-        return set(self.column_var_type_map)
-
-    def __getattr__(self, item: str) -> Any:
-        try:
-            return object.__getattribute__(self, item)
-        except AttributeError as exc:
-            if item in self.column_var_type_map:
-                return self.__getitem__(item)
-            raise exc
-
-
-class HasExtractPrunedGraphAndNode(Protocol):
-    """
-    Class with extract_pruned_graph_and_node attribute / property
-    """
-
-    feature_store: FeatureStoreModel
+        client = Configurations().get_client()
+        post_kwargs = {"url": route, "files": files}
+        if is_payload_json:
+            post_kwargs["json"] = payload
+        else:
+            post_kwargs["data"] = payload
+        create_response = client.post(**post_kwargs)  # type: ignore[arg-type]
+        if create_response.status_code != HTTPStatus.CREATED:
+            raise RecordCreationException(response=create_response)
+        return cls._poll_async_task(
+            task_response=create_response,
+            delay=delay,
+            retrieve_result=retrieve_result,
+            has_output_url=has_output_url,
+        )
 
-    @abstractmethod
-    def extract_pruned_graph_and_node(self, **kwargs: Any) -> tuple[QueryGraphModel, Node]:
+    @classmethod
+    def patch_async_task(
+        cls, route: str, payload: dict[str, Any], delay: float = POLLING_INTERVAL
+    ) -> None:
         """
-        Extract pruned graph & node from the global query graph
+        Patch async task to the worker & wait for the task to finish (blocking)
 
         Parameters
         ----------
-        **kwargs: Any
-            Additional keyword parameters
+        route: str
+            Async task route
+        payload: dict[str, Any]
+            Task payload
+        delay: float
+            Delay used in polling the task
 
         Raises
         ------
-        NotImplementedError
-            Method not implemented
+        RecordUpdateException
+            When unexpected update failure
         """
-
-    @property
-    def timestamp_column(self) -> Optional[str]:
-        """
-        Timestamp column to be used for datetime filtering during sampling
-
-        Returns
-        -------
-        Optional[str]
-        """
-        return None
+        client = Configurations().get_client()
+        update_response = client.patch(url=route, json=payload)
+        if update_response.status_code == HTTPStatus.OK:
+            return
+        if update_response.status_code == HTTPStatus.ACCEPTED:
+            cls._poll_async_task(
+                task_response=update_response,
+                delay=delay,
+                retrieve_result=False,
+                task_failure_exception_class=RecordUpdateException,
+            )
+            return
+        raise RecordUpdateException(response=update_response)
 
 
-class SampleMixin:
+class SampleMixin(AsyncMixin):
     """
     Supports preview and sample functions
     """
 
     @perf_logging
     @typechecked
     def preview(self: HasExtractPrunedGraphAndNode, limit: int = 10, **kwargs: Any) -> pd.DataFrame:
@@ -457,19 +440,14 @@
             Additional keyword parameters.
 
         Returns
         -------
         pd.DataFrame
             Summary of the view.
 
-        Raises
-        ------
-        RecordRetrievalException
-            Describe request failed.
-
         Examples
         --------
         Get summary of a view.
         >>> catalog.get_view("GROCERYPRODUCT").describe()
                                     GroceryProductGuid        ProductGroup
         dtype                                  VARCHAR             VARCHAR
         unique                                   29099                  87
@@ -505,35 +483,13 @@
             graph=pruned_graph,
             node_name=mapped_node.name,
             from_timestamp=from_timestamp,
             to_timestamp=to_timestamp,
             timestamp_column=self.timestamp_column,
             feature_store_id=self.feature_store.id,
         )
-        client = Configurations().get_client()
-        response = client.post(
-            url=f"/feature_store/description?size={size}&seed={seed}", json=payload.json_dict()
+        catalog_id = get_active_catalog_id()
+        data_description = AsyncMixin.post_async_task(
+            route=f"/feature_store/data_description?size={size}&seed={seed}&catalog_id={catalog_id}",
+            payload=payload.json_dict(),
         )
-        if response.status_code != HTTPStatus.OK:
-            raise RecordRetrievalException(response)
-        return dataframe_from_json(response.json())
-
-    @typechecked
-    def preview_sql(self: HasExtractPrunedGraphAndNode, limit: int = 10, **kwargs: Any) -> str:
-        """
-        Generate SQL query to preview the transformation output
-
-        Parameters
-        ----------
-        limit: int
-            maximum number of return rows
-        **kwargs: Any
-            Additional keyword parameters
-
-        Returns
-        -------
-        str
-        """
-        pruned_graph, mapped_node = self.extract_pruned_graph_and_node(**kwargs)
-        return GraphInterpreter(
-            pruned_graph, source_type=self.feature_store.type
-        ).construct_preview_sql(node_name=mapped_node.name, num_rows=limit)[0]
+        return dataframe_from_json(data_description)
```

### Comparing `featurebyte-1.0.2/featurebyte/core/series.py` & `featurebyte-1.0.3/featurebyte/core/series.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 """
 Series class
 """
+
 # pylint: disable=too-many-lines,too-many-public-methods
 from __future__ import annotations
 
-from typing import Any, Callable, Literal, Optional, Sequence, Type, TypeVar, Union
+from typing import Any, Callable, Optional, Sequence, Type, TypeVar, Union
+from typing_extensions import Literal
 
 from functools import wraps
 
 import pandas as pd
 from pydantic import Field, StrictStr
 from typeguard import typechecked
 
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import Scalar, ScalarSequence, Timestamp, is_scalar_nan
 from featurebyte.core.accessor.datetime import DtAccessorMixin
 from featurebyte.core.accessor.string import StrAccessorMixin
 from featurebyte.core.accessor.vector import VectorAccessorMixin
 from featurebyte.core.generic import QueryObject
 from featurebyte.core.mixin import OpsMixin, ParentMixin
 from featurebyte.core.util import SeriesBinaryOperator, series_unary_operation
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
+from featurebyte.typing import Scalar, ScalarSequence, Timestamp, is_scalar_nan
 
 FrozenSeriesT = TypeVar("FrozenSeriesT", bound="FrozenSeries")
 FuncT = TypeVar("FuncT", bound=Callable[..., "FrozenSeriesT"])
 
 
 def numeric_only(func: FuncT) -> FuncT:
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/core/timedelta.py` & `featurebyte-1.0.3/featurebyte/core/timedelta.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 """
 Module for Timedelta related functions
 """
+
 from __future__ import annotations
 
 from typeguard import typechecked
 
-from featurebyte.common.typing import TimedeltaSupportedUnitType
 from featurebyte.core.series import Series
 from featurebyte.core.util import series_unary_operation
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
+from featurebyte.typing import TimedeltaSupportedUnitType
 
 
 @typechecked
 def to_timedelta(series: Series, unit: TimedeltaSupportedUnitType) -> Series:
     """
     Construct a timedelta Series that can be used to increment a datetime Series.
```

### Comparing `featurebyte-1.0.2/featurebyte/core/util.py` & `featurebyte-1.0.3/featurebyte/core/util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """
 util.py contains common functions used across different classes
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, cast
 
-from featurebyte.common.typing import AllSupportedValueTypes, Scalar, ScalarSequence
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.node.scalar import get_value_parameter
+from featurebyte.typing import AllSupportedValueTypes, Scalar, ScalarSequence
 
 if TYPE_CHECKING:
     from featurebyte.core.series import FrozenSeries, FrozenSeriesT
 
 
 def series_unary_operation(
     input_series: FrozenSeriesT,
```

### Comparing `featurebyte-1.0.2/featurebyte/datasets/__main__.py` & `featurebyte-1.0.3/featurebyte/datasets/__main__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 CLI tools to run in featurebyte-server container
 """
+
 import base64
 import sys
 
 from featurebyte.session.hive import HiveConnection
 
 if __name__ == "__main__":
     if len(sys.argv) != 2:
```

### Comparing `featurebyte-1.0.2/featurebyte/datasets/app.py` & `featurebyte-1.0.3/featurebyte/datasets/app.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 CLI tools for managing sample datasets
 """
+
 import base64
 import os
 import re
 import tarfile
 import tempfile
 from urllib import request
```

### Comparing `featurebyte-1.0.2/featurebyte/datasets/creditcard.sql` & `featurebyte-1.0.3/featurebyte/datasets/creditcard.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/datasets/doctest_grocery.sql` & `featurebyte-1.0.3/featurebyte/datasets/doctest_grocery.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/datasets/grocery.sql` & `featurebyte-1.0.3/featurebyte/datasets/grocery.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/datasets/healthcare.sql` & `featurebyte-1.0.3/featurebyte/datasets/healthcare.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/docker/featurebyte.yml` & `featurebyte-1.0.3/featurebyte/docker/featurebyte.yml`

 * *Files 11% similar despite different names*

```diff
@@ -1,24 +1,42 @@
-version: "3.8"
 services:
   mongo-rs:
     networks:
       - featurebyte
     hostname: mongo-rs
     restart: unless-stopped
     container_name: mongo-rs
-    image: "featurebyte/mongo:6"
-    environment:
-      - MONGO_HOSTNAME=mongo-rs
+    entrypoint: []
+    image: "mongo:6"
+    healthcheck:
+      test: mongosh 'mongodb://mongo-rs/admin?replicaSet=rs0' --quiet --eval "exit" 2>/dev/null
+    command:
+      - /bin/bash
+      - -c
+      - |
+        set -em
+
+        # Creating Database
+        /usr/bin/mongod --quiet --dbpath=/tmp --bind_ip_all --replSet rs0 &
+        while ! mongosh --quiet --eval "exit" 2>/dev/null; do sleep 1; done; echo "mongodb started"
+
+        # If not bootstrapped, bootstrap
+        if ! mongosh --quiet --eval "rs.status()" 1>/dev/null 2>&1; then
+          mongosh --quiet <<EOF
+            var config = { "_id": "rs0", "version": 1, "members": [{ "_id": 1, "host": "mongo-rs", "priority": 1 }]};
+            rs.initiate(config, { force: true });
+        EOF
+        fi
+
+        # Checking rs.status()
+        while ! mongosh --quiet --eval "rs.status()" 1>/dev/null 2>&1; do sleep 1; done
+
+        fg  # Reconnect to mongod
     volumes:
       - mongodb:/data/
-    healthcheck:
-      start_period: 10s
-      interval: 5s
-      test: ["CMD", "mongosh", "--port=27022", "--eval", "rs.status()"]
     logging:
       driver: local
   redis:
     networks:
       - featurebyte
     hostname: redis
     restart: unless-stopped
@@ -32,26 +50,26 @@
       driver: local
   featurebyte-server:
     networks:
       - featurebyte
     hostname: featurebyte-server
     restart: unless-stopped
     container_name: featurebyte-server
-    image: featurebyte/featurebyte-server:1.0.2
+    image: featurebyte/featurebyte-server:1.0.3
     depends_on:
       mongo-rs:
         condition: service_healthy
     ports:
       - "0.0.0.0:8088:8088"
     command: ["bash", "/docker-entrypoint.sh", "server"]
     environment:
       - "FEATUREBYTE_HOME=/app/.featurebyte"
       - "MPLCONFIGDIR=/app/matplotlib"
       - "REDIS_URI=redis://redis:6379"
-      - "MONGODB_URI=mongodb://mongo-rs:27021,mongo-rs:27022/?replicaSet=rs0"
+      - "MONGODB_URI=mongodb://mongo-rs/?replicaSet=rs0"
       - "API_HOST=0.0.0.0"
       - "API_PORT=8088"
       - "LOG_LEVEL=${LOG_LEVEL}"
       - "LOCAL_UID=${LOCAL_UID}"
       - "LOCAL_GID=${LOCAL_GID}"
       - "KRB5_REALM=${KRB5_REALM}"
       - "KRB5_KDC=${KRB5_KDC}"
@@ -67,25 +85,25 @@
       driver: local
   featurebyte-worker:
     networks:
       - featurebyte
     hostname: featurebyte-worker
     restart: unless-stopped
     container_name: featurebyte-worker
-    image: featurebyte/featurebyte-server:1.0.2
+    image: featurebyte/featurebyte-server:1.0.3
     depends_on:
       mongo-rs:
         condition: service_healthy
       redis:
         condition: service_healthy
     environment:
       - "FEATUREBYTE_HOME=/app/.featurebyte"
       - "MPLCONFIGDIR=/app/matplotlib"
       - "REDIS_URI=redis://redis:6379"
-      - "MONGODB_URI=mongodb://mongo-rs:27021,mongo-rs:27022/?replicaSet=rs0"
+      - "MONGODB_URI=mongodb://mongo-rs/?replicaSet=rs0"
       - "LOG_LEVEL=${LOG_LEVEL}"
       - "LOCAL_UID=${LOCAL_UID}"
       - "LOCAL_GID=${LOCAL_GID}"
       - "KRB5_REALM=${KRB5_REALM}"
       - "KRB5_KDC=${KRB5_KDC}"
     command: ["bash", "/docker-entrypoint.sh", "worker"]
     volumes:
```

### Comparing `featurebyte-1.0.2/featurebyte/docker/manager.py` & `featurebyte-1.0.3/featurebyte/docker/manager.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/enum.py` & `featurebyte-1.0.3/featurebyte/enum.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 This module contains all the enums used across different modules
 
 Note: do not include server only dependencies here
 """
+
 from __future__ import annotations
 
 from typing import Literal
 
 import functools
 from enum import Enum
 
@@ -112,15 +113,15 @@
 
     def __str__(self) -> str:
         return str(self.value)
 
 
 class DBVarType(StrEnum):
     """
-    The DBVarType enum class provides a way to represetn various Dababase variable types supported by FeatureByte.
+    The DBVarType enum class provides a way to represent various Database variable types supported by FeatureByte.
     """
 
     __fbautodoc__ = FBAutoDoc(proxy_class="featurebyte.enum.DBVarType")
 
     # primitive_types
     BOOL = "BOOL", "Boolean column"
     CHAR = "CHAR", "Fixed-length string column"
@@ -199,28 +200,28 @@
         """
         Types for specialized type detection
 
         Returns
         -------
         set[DBVarType]
         """
-        return {cls.ARRAY, cls.OBJECT, cls.STRUCT}
+        return {cls.ARRAY, cls.OBJECT, cls.STRUCT, cls.DICT}
 
     @classmethod
     def dictionary_types(cls) -> set[DBVarType]:
         """
         Types for dictionary
 
         Returns
         -------
         set[DBVarType]
         """
         # FIXME: remove this after we update to the dictionary type
         # Snowflake uses OBJECT for dictionary type & Spark uses STRUCT for dictionary type
-        return {cls.OBJECT, cls.STRUCT}
+        return {cls.OBJECT, cls.STRUCT, cls.DICT, cls.MAP}
 
     @classmethod
     def array_types(cls) -> set[DBVarType]:
         """
         Types for array
 
         Returns
@@ -403,14 +404,15 @@
     STATIC_SOURCE_TABLE_CREATE = "STATIC_SOURCE_TABLE_CREATE"
     TARGET_TABLE_CREATE = "TARGET_TABLE_CREATE"
     TEST = "TEST"
     TILE_COMPUTE = "TILE_COMPUTE"
     ONLINE_STORE_TABLE_CLEANUP = "ONLINE_STORE_TABLE_CLEANUP"
     CATALOG_ONLINE_STORE_UPDATE = "CATALOG_ONLINE_STORE_UPDATE"
     SCHEDULED_FEATURE_MATERIALIZE = "SCHEDULED_FEATURE_MATERIALIZE"
+    DATA_DESCRIPTION = "DATA_DESCRIPTION"
 
     # Tasks to be deprecated
     FEATURE_LIST_CREATE_WITH_BATCH_FEATURE_CREATE = "FEATURE_LIST_CREATE_WITH_BATCH_FEATURE_CREATE"
 
 
 class TableDataType(StrEnum):
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/exception.py` & `featurebyte-1.0.3/featurebyte/exception.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 List of Exceptions
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from asyncio.exceptions import CancelledError
 
 from requests.exceptions import JSONDecodeError
@@ -204,14 +205,26 @@
 
 class CredentialsError(BaseUnprocessableEntityError):
     """
     Raise when the credentials used to access the resource is missing or invalid
     """
 
 
+class DataWarehouseConnectionError(BaseUnprocessableEntityError):
+    """
+    Raise when connection to data warehouse cannot be established
+    """
+
+
+class DataWarehouseOperationError(BaseUnprocessableEntityError):
+    """
+    Raise when data warehouse operations failed
+    """
+
+
 class DocumentError(BaseUnprocessableEntityError):
     """
     General exception raised when there are some issue at persistent layer operations
     """
 
 
 class DocumentNotFoundError(DocumentError):
@@ -360,14 +373,20 @@
 
 class NoFeatureJobSettingInSourceError(FeatureByteException):
     """
     Raise when the input table does not have any feature job setting.
     """
 
 
+class DeploymentDataBricksAccessorError(FeatureByteException):
+    """
+    Raise when there is an error with deployment
+    """
+
+
 class NoChangesInFeatureVersionError(DocumentError):
     """
     Raise when we try to create a new feature version, but there are no differences.
     """
 
 
 class EntityRelationshipConflictError(DocumentError):
@@ -384,14 +403,20 @@
 
 class ColumnNotFoundError(FeatureByteException):
     """
     Raised when a specified column is not found in the view or table
     """
 
 
+class FeatureMaterializationError(FeatureByteException):
+    """
+    Raised when there is an error with feature materialization
+    """
+
+
 class DockerError(FeatureByteException):
     """
     Raised when there is an error with Docker
     """
 
 
 class DatabaseNotFoundError(BaseFailedDependencyError):
@@ -426,14 +451,23 @@
     Raise when the catalog is not specified in a catalog-specific request
     """
 
     def __str__(self) -> str:
         return "Catalog not specified. Please specify a catalog."
 
 
+class NotInDataBricksEnvironmentError(BaseFailedDependencyError):
+    """
+    Raise when the code is not running in a DataBricks environment
+    """
+
+    def __str__(self) -> str:
+        return "This method can only be called in a DataBricks environment."
+
+
 class UseCaseInvalidDataError(BaseUnprocessableEntityError):
     """
     Raise when invalid observation table default is specified
     """
 
 
 class ObservationTableInvalidContextError(BaseUnprocessableEntityError):
@@ -469,14 +503,26 @@
 
 class ObservationTableMissingColumnsError(BaseUnprocessableEntityError):
     """
     Raise when observation table is missing required columns
     """
 
 
+class ObservationTableInvalidTargetNameError(BaseUnprocessableEntityError):
+    """
+    Raise when observation table specifies a target name that does not exist
+    """
+
+
+class ObservationTableTargetDefinitionExistsError(BaseUnprocessableEntityError):
+    """
+    Raise when observation table specifies a target name that already has a definition
+    """
+
+
 class TaskNotRevocableError(BaseUnprocessableEntityError):
     """
     Raise when task is not revocable
     """
 
 
 class TaskNotFound(DocumentNotFoundError):
@@ -489,7 +535,13 @@
 TaskRevokeExceptions = (SystemExit, KeyboardInterrupt, RuntimeError, CancelledError)
 
 
 class TaskCanceledError(FeatureByteException):
     """
     Raise when task is canceled
     """
+
+
+class CursorSchemaError(FeatureByteException):
+    """
+    Raise when cursor schema is not as expected
+    """
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/enum.py` & `featurebyte-1.0.3/featurebyte/feast/enum.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Enum related to feast
 """
+
 from __future__ import annotations
 
 from feast.types import PrimitiveFeastType
 
 from featurebyte.enum import DBVarType
 
 
@@ -25,15 +26,15 @@
     ------
     ValueError
         If the DBVarType is not supported by Feast
     """
     mapping = {
         DBVarType.BOOL: PrimitiveFeastType.BOOL,
         DBVarType.VARCHAR: PrimitiveFeastType.STRING,
-        DBVarType.FLOAT: PrimitiveFeastType.FLOAT64,
+        DBVarType.FLOAT: PrimitiveFeastType.FLOAT32,
         DBVarType.INT: PrimitiveFeastType.INT64,
         DBVarType.TIMESTAMP_TZ: PrimitiveFeastType.UNIX_TIMESTAMP,
         DBVarType.TIMESTAMP: PrimitiveFeastType.UNIX_TIMESTAMP,
         DBVarType.OBJECT: PrimitiveFeastType.STRING,
         DBVarType.STRUCT: PrimitiveFeastType.STRING,
         DBVarType.FLAT_DICT: PrimitiveFeastType.STRING,
         DBVarType.ARRAY: PrimitiveFeastType.STRING,
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/databricks.py` & `featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/databricks.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 DataBricks offline store
 """
+
 from typing import Literal
 
 from featurebyte import AccessTokenCredential
 from featurebyte.feast.infra.offline_stores.spark_thrift import (
     BaseSparkThriftOfflineStoreConfig,
     SparkThriftOfflineStore,
 )
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/spark_thrift.py` & `featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/spark_thrift.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Spark Thrift Server Offline Store
 """
+
 from typing import Any, Callable, Iterable, List, Literal, Optional, Tuple, Union
 
 import json
 from abc import abstractmethod
 from datetime import datetime
 
 import numpy as np
@@ -273,14 +274,18 @@
         Returns
         -------
         pd.DataFrame
         """
         result = self.db_session.execute_query_blocking(self.query)
         assert isinstance(result, pd.DataFrame)
 
+        # skip if result is empty
+        if result.shape[0] == 0:
+            return result
+
         # convert arrays to string
         for column in result.columns:
             if result[column].dtype == "object":
                 if isinstance(result[column].iloc[0], np.ndarray):
                     result[column] = result[column].apply(
                         lambda x: json.dumps(x.tolist()) if x is not None else None
                     )
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/infra/offline_stores/spark_thrift_source.py` & `featurebyte-1.0.3/featurebyte/feast/infra/offline_stores/spark_thrift_source.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Spark Thrift Server Data Source
 """
+
 # pylint: disable=no-name-in-module
 from typing import Any, Callable, Dict, Iterable, Optional, Tuple, cast
 
 import json
 
 from feast.data_source import DataSource
 from feast.protos.feast.core.DataSource_pb2 import DataSource as DataSourceProto
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/model/feature_store.py` & `featurebyte-1.0.3/featurebyte/feast/model/feature_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains feature store details used to construct feast data source & offline store config
 """
+
 from typing import Any, Optional, Union, cast
 
 from abc import ABC, abstractmethod
 
 from feast import SnowflakeSource
 from feast.data_source import DataSource
 from feast.infra.offline_stores.snowflake import SnowflakeOfflineStoreConfig
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/model/online_store.py` & `featurebyte-1.0.3/featurebyte/feast/model/online_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Models to construct feast online store config from featurebyte BaseOnlineStoreDetails
 """
+
 from __future__ import annotations
 
 from typing import Union, cast
 from typing_extensions import Annotated
 
 from abc import abstractmethod  # pylint: disable=wrong-import-order
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/model/registry.py` & `featurebyte-1.0.3/featurebyte/feast/model/registry.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 Feast registry model
 """
-from typing import List, Optional
+
+from typing import Any, Dict, List, Optional
 
 from pathlib import Path
 
 import pymongo
 
 # pylint: disable=no-name-in-module
 from feast.protos.feast.core.Registry_pb2 import Registry as RegistryProto
@@ -21,20 +22,22 @@
 class FeastRegistryModel(FeatureByteCatalogBaseDocumentModel):
     """Feast registry model"""
 
     offline_table_name_prefix: str
     registry: bytes = Field(default_factory=bytes, exclude=True)
     feature_store_id: PydanticObjectId
     registry_path: Optional[str] = Field(default=None)
+    deployment_id: Optional[PydanticObjectId] = Field(default=None)
 
-    @property
-    def remote_attribute_paths(self) -> List[Path]:
+    @classmethod
+    def _get_remote_attribute_paths(cls, document_dict: Dict[str, Any]) -> List[Path]:
         paths = []
-        if self.registry_path:
-            paths.append(Path(self.registry_path))
+        registry_path = document_dict.get("registry_path")
+        if registry_path:
+            paths.append(Path(registry_path))
         return paths
 
     def registry_proto(self) -> RegistryProto:
         """
         Get feast registry proto
 
         Returns
@@ -54,17 +57,12 @@
         collection_name = "feast_registry"
         unique_constraints = [
             UniqueValuesConstraint(
                 fields=("_id",),
                 conflict_fields_signature={"id": ["_id"]},
                 resolution_signature=None,
             ),
-            UniqueValuesConstraint(
-                fields=("name",),
-                conflict_fields_signature={"id": ["_id"]},
-                resolution_signature=None,
-            ),
         ]
         indexes = FeatureByteCatalogBaseDocumentModel.Settings.indexes + [
             pymongo.operations.IndexModel("feature_store_id"),
         ]
         auditable = False
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/online_store/mysql.py` & `featurebyte-1.0.3/featurebyte/feast/online_store/mysql.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Custom MySQL online store implementation for Feast.
 """
+
 from typing import Literal, Sequence
 
 from feast import Entity, FeatureView, RepoConfig
 from feast.infra.online_stores.contrib.mysql_online_store.mysql import (
     MySQLOnlineStore as BaseMySQLOnlineStore,
 )
 from feast.infra.online_stores.contrib.mysql_online_store.mysql import (
@@ -15,17 +16,17 @@
     _table_id,
 )
 
 
 class FBMySQLOnlineStoreConfig(BaseMySQLOnlineStoreConfig):
     """Configuration for the MySQL online store"""
 
-    type: Literal[
-        "mysql", "featurebyte.feast.online_store.mysql.FBMySQLOnlineStore"
-    ] = "featurebyte.feast.online_store.mysql.FBMySQLOnlineStore"
+    type: Literal["mysql", "featurebyte.feast.online_store.mysql.FBMySQLOnlineStore"] = (
+        "featurebyte.feast.online_store.mysql.FBMySQLOnlineStore"
+    )
 
 
 class FBMySQLOnlineStore(BaseMySQLOnlineStore):
     """An online store implementation that uses MySQL."""
 
     def update(
         self,
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/registry_store.py` & `featurebyte-1.0.3/featurebyte/feast/registry_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Featurebyte registry store
 """
+
 from typing import cast
 
 from pathlib import Path
 
 # pylint: disable=no-name-in-module
 from feast.infra.registry.file import FileRegistryStore
 from feast.protos.feast.core.Registry_pb2 import Registry as RegistryProto
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/schema/registry.py` & `featurebyte-1.0.3/featurebyte/feast/schema/registry.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 """
 Feast registry related schemas
 """
+
 from typing import List, Optional
 
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
 
 
 class FeastRegistryCreate(FeatureByteBaseModel):
     """
     Feast registry create schema
     """
 
     feature_lists: List[FeatureListModel]
+    deployment_id: PydanticObjectId
 
 
 class FeastRegistryUpdate(BaseDocumentServiceUpdateSchema):
     """
     Feast registry update schema
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/service/registry.py` & `featurebyte-1.0.3/featurebyte/feast/service/registry.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,29 +1,32 @@
 """
 Feast registry service
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 import random
 from pathlib import Path
 
 from bson import ObjectId
 from redis import Redis
 from redis.lock import Lock
 
 from featurebyte.feast.model.registry import FeastRegistryModel
 from featurebyte.feast.schema.registry import FeastRegistryCreate, FeastRegistryUpdate
 from featurebyte.feast.utils.registry_construction import FeastRegistryBuilder
+from featurebyte.models.deployment import DeploymentModel
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.persistent import Persistent
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.service.base_document import BaseDocumentService
 from featurebyte.service.catalog import CatalogService
+from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.entity_lookup_feature_table import EntityLookupFeatureTableService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.online_store import OnlineStoreService
 from featurebyte.storage import Storage
@@ -47,14 +50,15 @@
         feature_list_service: FeatureListService,
         feature_service: FeatureService,
         entity_service: EntityService,
         feature_store_service: FeatureStoreService,
         online_store_service: OnlineStoreService,
         catalog_service: CatalogService,
         entity_lookup_feature_table_service: EntityLookupFeatureTableService,
+        deployment_service: DeploymentService,
         storage: Storage,
         redis: Redis[Any],
     ):
         super().__init__(
             user=user,
             persistent=persistent,
             catalog_id=catalog_id,
@@ -65,14 +69,15 @@
         self.feature_list_service = feature_list_service
         self.feature_service = feature_service
         self.entity_service = entity_service
         self.feature_store_service = feature_store_service
         self.online_store_service = online_store_service
         self.catalog_service = catalog_service
         self.entity_lookup_feature_table_service = entity_lookup_feature_table_service
+        self.deployment_service = deployment_service
 
     def get_registry_storage_lock(self, timeout: int) -> Lock:
         """
         Get registry storage lock
 
         Parameters
         ----------
@@ -81,19 +86,24 @@
 
         Returns
         -------
         Lock
         """
         return self.redis.lock(f"feast_registry_storage_update:{self.catalog_id}", timeout=timeout)
 
-    async def _create_project_name(
-        self, catalog_id: ObjectId, hex_digit_num: int = 7, max_try: int = 100
-    ) -> str:
+    async def _get_or_create_project_name(self, hex_digit_num: int = 7, max_try: int = 100) -> str:
+        # check if there exists a registry document with the same catalog ID,
+        # reuse the project name if it exists
+        async for registry_doc in self.list_documents_as_dict_iterator(
+            query_filter={"catalog_id": self.catalog_id}
+        ):
+            return str(registry_doc["name"])
+
         # generate 7 hex digits
-        project_name = str(catalog_id)[-hex_digit_num:]
+        project_name = str(self.catalog_id)[-hex_digit_num:]
         document_dict = await self.persistent.find_one(
             collection_name=self.collection_name,
             query_filter={"name": project_name},
             projection={"_id": 1},
         )
         if not document_dict:
             return project_name
@@ -114,63 +124,64 @@
                 raise RuntimeError("Unable to generate unique project name")
 
     async def _create_offline_table_name_prefix(self, feature_store_id: ObjectId) -> str:
         res, _ = await self.persistent.aggregate_find(
             collection_name=self.collection_name,
             pipeline=[
                 {"$match": {"feature_store_id": feature_store_id}},
-                {"$group": {"_id": None, "unique_names": {"$addToSet": "$name"}}},
+                {
+                    "$group": {
+                        "_id": "$catalog_id",
+                        "offline_table_name_prefix": {"$first": "$offline_table_name_prefix"},
+                    }
+                },
+                {
+                    "$project": {
+                        "_id": 0,
+                        "catalog_id": "$_id",
+                        "offline_table_name_prefix": 1,
+                    }
+                },
             ],
         )
-        results = list(res)
-        found_names = set(results[0]["unique_names"]) if results else set()
-        name_count = len(found_names)
-        return f"cat{name_count + 1}"
+        catalog_id_to_prefix = {
+            item["catalog_id"]: item["offline_table_name_prefix"] for item in res
+        }
+        if self.catalog_id in catalog_id_to_prefix:
+            return str(catalog_id_to_prefix[self.catalog_id])
+        return f"cat{len(catalog_id_to_prefix) + 1}"
 
-    async def get_or_create_feast_registry(
-        self,
-        catalog_id: ObjectId,
-        feature_store_id: Optional[ObjectId],
-    ) -> FeastRegistryModel:
+    async def get_or_create_feast_registry(self, deployment: DeploymentModel) -> FeastRegistryModel:
         """
         Get or create project name
 
         Parameters
         ----------
-        catalog_id: ObjectId
-            Catalog id
-        feature_store_id: Optional[ObjectId]
-            Feature store id
+        deployment: DeploymentModel
+            Deployment object
 
         Returns
         -------
         FeastRegistryModel
         """
-        query_filter = {"catalog_id": catalog_id}
-        if feature_store_id:
-            query_filter["feature_store_id"] = feature_store_id
-        else:
-            catalog = await self.catalog_service.get_document(document_id=catalog_id)
-            query_filter["feature_store_id"] = catalog.default_feature_store_ids[0]
-
-        query_result = await self.list_documents_as_dict(query_filter=query_filter, page_size=1)
-        if query_result["total"]:
-            registry = await self._populate_remote_attributes(
-                FeastRegistryModel(**query_result["data"][0])
-            )
+        if deployment.registry_info:
+            registry = await self.get_document(document_id=deployment.registry_info.registry_id)
             return registry
 
-        registry = await self.create_document(data=FeastRegistryCreate(feature_lists=[]))
+        registry = await self.create_document(
+            data=FeastRegistryCreate(feature_lists=[], deployment_id=deployment.id)
+        )
         return registry
 
     async def _construct_feast_registry_model(  # pylint: disable=too-many-locals
         self,
         project_name: Optional[str],
         offline_table_name_prefix: Optional[str],
         feature_lists: List[FeatureListModel],
+        deployment_id: Optional[ObjectId],
         document_id: Optional[ObjectId] = None,
     ) -> FeastRegistryModel:
         # retrieve latest feature lists
         feature_ids = set()
         recent_feature_lists = []
         for feature_list in feature_lists:
             recent_feature_list = await self.feature_list_service.get_document(
@@ -187,17 +198,15 @@
             query_filter={"_id": {"$in": list(feature_ids)}}
         ):
             features.append(feature)
             entity_ids.update(feature.entity_ids)
             feature_store_ids.add(feature.tabular_source.feature_store_id)
 
         entities = []
-        async for entity in self.entity_service.list_documents_iterator(
-            query_filter={"_id": {"$in": list(entity_ids)}}
-        ):
+        async for entity in self.entity_service.list_documents_iterator(query_filter={}):
             entities.append(entity)
 
         if len(feature_store_ids) > 1:
             raise ValueError("Feature store IDs must be the same for all features")
 
         assert self.catalog_id is not None
         catalog = await self.catalog_service.get_document(document_id=self.catalog_id)
@@ -217,40 +226,51 @@
         entity_lookup_steps_mapping = (
             await self.entity_lookup_feature_table_service.get_entity_lookup_steps_mapping(
                 feature_lists
             )
         )
 
         if not project_name:
-            project_name = await self._create_project_name(catalog_id=self.catalog_id)
+            project_name = await self._get_or_create_project_name()
         if not offline_table_name_prefix:
             offline_table_name_prefix = await self._create_offline_table_name_prefix(
                 feature_store_id=feature_store_id
             )
 
+        serving_entity_ids = None
+        if deployment_id is not None:
+            deployment = await self.deployment_service.get_document(deployment_id)
+            if deployment.serving_entity_ids is not None:
+                serving_entity_ids = deployment.serving_entity_ids
+
         feast_registry_proto = FeastRegistryBuilder.create(
             feature_store=feature_store,
             online_store=online_store,
             entities=entities,
             features=features,
             feature_lists=feature_lists,
             project_name=project_name,
             entity_lookup_steps_mapping=entity_lookup_steps_mapping,
+            serving_entity_ids=serving_entity_ids,
         )
         return FeastRegistryModel(
             _id=document_id,
             name=project_name,
             offline_table_name_prefix=offline_table_name_prefix,
             registry=feast_registry_proto.SerializeToString(),
             feature_store_id=feature_store_id,
+            deployment_id=deployment_id,
         )
 
     async def _populate_remote_attributes(self, document: FeastRegistryModel) -> FeastRegistryModel:
         if document.registry_path:
-            document.registry = await self.storage.get_bytes(Path(document.registry_path))
+            document.registry = await self.storage.get_bytes(
+                Path(document.registry_path),
+                cache_key=f"{document.registry_path}_{document.updated_at}",
+            )
         return document
 
     async def _move_registry_to_storage(self, document: FeastRegistryModel) -> FeastRegistryModel:
         feast_registry_path = self.get_full_remote_file_path(
             f"feast_registry/{document.id}/feast_registry.pb"
         )
         await self.storage.put_bytes(document.registry, feast_registry_path)
@@ -270,40 +290,47 @@
         Returns
         -------
         FeastRegistryModel
             Created document
         """
         with self.get_registry_storage_lock(FEAST_REGISTRY_REDIS_LOCK_TIMEOUT):
             document = await self._construct_feast_registry_model(
-                project_name=None, offline_table_name_prefix=None, feature_lists=data.feature_lists
+                project_name=None,
+                offline_table_name_prefix=None,
+                feature_lists=data.feature_lists,
+                deployment_id=data.deployment_id,
             )
             document = await self._move_registry_to_storage(document)
             return await super().create_document(data=document)  # type: ignore
 
     async def update_document(
         self,
         document_id: ObjectId,
         data: FeastRegistryUpdate,
         exclude_none: bool = True,
         document: Optional[FeastRegistryModel] = None,
         return_document: bool = True,
         skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
     ) -> Optional[FeastRegistryModel]:
         assert data.feature_store_id is None, "Not allowed to update feature store ID directly"
         if data.feature_lists is None:
-            return await self.get_document(document_id=document_id)
+            return await self.get_document(
+                document_id=document_id, populate_remote_attributes=populate_remote_attributes
+            )
 
         with self.get_registry_storage_lock(FEAST_REGISTRY_REDIS_LOCK_TIMEOUT):
             original_doc = await self.get_document(
                 document_id=document_id, populate_remote_attributes=False
             )
             recreated_model = await self._construct_feast_registry_model(
                 project_name=original_doc.name,
                 offline_table_name_prefix=original_doc.offline_table_name_prefix,
                 feature_lists=data.feature_lists,
+                deployment_id=original_doc.deployment_id,
                 document_id=document_id,
             )
             assert recreated_model.id == document_id
 
             if original_doc.registry_path:
                 # attempt to remove old registry file
                 await self.storage.try_delete_if_exists(Path(original_doc.registry_path))
@@ -315,22 +342,48 @@
                 update={
                     "$set": {"registry_path": document.registry_path},
                     "$unset": {"registry": ""},  # remove registry field from older document
                 },
                 user_id=self.user.id,
                 disable_audit=self.should_disable_audit,
             )
-            return await self.get_document(document_id=document_id)
+            return await self.get_document(
+                document_id=document_id, populate_remote_attributes=populate_remote_attributes
+            )
 
     async def get_feast_registry_for_catalog(self) -> Optional[FeastRegistryModel]:
         """
-        Get feast registry document for the catalog if it exists
+        Get feast registry document for the catalog if it exists (this is used to retrieve older feast registry
+        that is not deployment specific)
 
         Returns
         -------
         Optional[FeastRegistryModel]
         """
         async for feast_registry_model in self.list_documents_iterator(
-            query_filter={"catalog_id": self.catalog_id}
+            query_filter={
+                "catalog_id": self.catalog_id,
+                "$or": [{"deployment_id": {"$exists": False}}, {"deployment_id": None}],
+            }
         ):
             return await self._populate_remote_attributes(feast_registry_model)
         return None
+
+    async def get_feast_registry(self, deployment: DeploymentModel) -> Optional[FeastRegistryModel]:
+        """
+        Get feast registry document for the deployment if it exists
+
+        Parameters
+        ----------
+        deployment: DeploymentModel
+            Deployment object
+
+        Returns
+        -------
+        Optional[FeastRegistryModel]
+        """
+        if deployment.registry_info:
+            # if the registry is deployment specific, get the feast registry document
+            return await self.get_document(document_id=deployment.registry_info.registry_id)
+
+        # otherwise, attempt to get the older feast registry that is not deployment specific
+        return await self.get_feast_registry_for_catalog()
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/utils/materialize_helper.py` & `featurebyte-1.0.3/featurebyte/feast/utils/materialize_helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 """
 Helper function for feature materialization
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 import copy
 from datetime import datetime
 from unittest.mock import patch
 
 from cachetools import TTLCache
 from feast import FeatureStore, FeatureView, utils
 from tqdm import tqdm
 
 from featurebyte.enum import InternalName
+from featurebyte.exception import FeatureMaterializationError
 from featurebyte.session.base import LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS, to_thread
 
 DEFAULT_MATERIALIZE_START_DATE = datetime(1970, 1, 1)
 
 feast_snowflake_session_cache: TTLCache[Any, Any] = TTLCache(maxsize=1024, ttl=3600)
 
 
@@ -60,14 +62,19 @@
         List of column names to materialize
     end_date : datetime
         End date of materialization
     start_date : Optional[datetime]
         Start date of materialization
     with_feature_timestamp : bool
         Whether to include the feature timestamp in the materialization
+
+    Raises
+    ------
+    FeatureMaterializationError
+        If materialization fails for any reason
     """
     if start_date is None:
         start_date = DEFAULT_MATERIALIZE_START_DATE
 
     if with_feature_timestamp and InternalName.FEATURE_TIMESTAMP_COLUMN not in columns:
         columns = list(columns) + [InternalName.FEATURE_TIMESTAMP_COLUMN.value]
 
@@ -99,21 +106,28 @@
 
     # FIXME: This patch is related to an implementation detail of RedisOnlineStore: it checks
     # whether the feature table's stored feature timestamp is the same as current feature timestamp,
     # and if so skip the update. This doesn't work for partial materialization because a feature
     # table's stored feature timestamp is always up-to-date except on the first materialization run.
     # Patching this effectively skips the check, but a better solution might be to override the
     # implementation of RedisOnlineStore.online_write_batch().
-    with patch("google.protobuf.timestamp_pb2.Timestamp.ParseFromString"), patch(
-        "feast.infra.utils.snowflake.snowflake_utils._cache", snowflake_session_cache
+    with (
+        patch("google.protobuf.timestamp_pb2.Timestamp.ParseFromString"),
+        patch("feast.infra.utils.snowflake.snowflake_utils._cache", snowflake_session_cache),
     ):
-        await to_thread(
-            provider.materialize_single_feature_view,
-            LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS,
-            config=feature_store.config,
-            feature_view=partial_feature_view,
-            start_date=start_date,
-            end_date=end_date,
-            registry=feature_store._registry,  # pylint: disable=protected-access
-            project=feature_store.project,
-            tqdm_builder=silent_tqdm_builder,
-        )
+        try:
+            await to_thread(
+                provider.materialize_single_feature_view,
+                LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS,
+                config=feature_store.config,
+                feature_view=partial_feature_view,
+                start_date=start_date,
+                end_date=end_date,
+                registry=feature_store._registry,  # pylint: disable=protected-access
+                project=feature_store.project,
+                tqdm_builder=silent_tqdm_builder,
+            )
+        except Exception as exc:
+            # add more context to the exception
+            raise FeatureMaterializationError(
+                f"Failed to materialize {partial_feature_view.name}: {partial_feature_view.features}"
+            ) from exc
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/utils/on_demand_view.py` & `featurebyte-1.0.3/featurebyte/feast/utils/on_demand_view.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 On demand feature view related classes and functions.
 """
+
 from typing import Any, Dict, List, Optional, Union, cast
 
 from unittest.mock import patch
 
 from feast import FeatureView, Field, RequestSource
 from feast.feature_view_projection import FeatureViewProjection
 from feast.on_demand_feature_view import OnDemandFeatureView, on_demand_feature_view
@@ -92,17 +93,17 @@
             for (
                 ingest_query_graph
             ) in offline_store_info.extract_offline_store_ingest_query_graphs():
                 fv_source = name_to_feast_feature_view[ingest_query_graph.offline_store_table_name]
                 sources.append(fv_source)
                 if fv_source.ttl is not None:
                     if ttl_seconds is None:
-                        ttl_seconds = fv_source.ttl.seconds
-                    elif fv_source.ttl.seconds < ttl_seconds:
-                        ttl_seconds = fv_source.ttl.seconds
+                        ttl_seconds = fv_source.ttl.total_seconds()
+                    elif fv_source.ttl.total_seconds() < ttl_seconds:
+                        ttl_seconds = fv_source.ttl.total_seconds()
 
             for request_node in feature_model.extract_request_column_nodes():
                 req_source = name_to_feast_request_source[request_node.parameters.column_name]
                 sources.append(req_source)
                 if request_node.parameters.column_name == SpecialColumnName.POINT_IN_TIME.value:
                     has_point_in_time = True
         else:
```

### Comparing `featurebyte-1.0.2/featurebyte/feast/utils/registry_construction.py` & `featurebyte-1.0.3/featurebyte/feast/utils/registry_construction.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """
 This module contains classes for constructing feast registry
 """
-# pylint: disable=no-name-in-module
+
+# pylint: disable=no-name-in-module, too-many-lines
 from __future__ import annotations
 
-from typing import Any, Dict, List, Optional, Tuple, cast
+from typing import Any, Dict, List, Optional, Sequence, Set, Tuple, cast
 
 import tempfile
 from collections import defaultdict
 from datetime import timedelta
 from unittest.mock import patch
 
 from feast import Entity as FeastEntity
@@ -17,42 +18,48 @@
 from feast import FeatureView as FeastFeatureView
 from feast import Field as FeastField
 from feast import OnDemandFeatureView as FeastOnDemandFeatureView
 from feast import RequestSource as FeastRequestSource
 from feast.data_source import DataSource as FeastDataSource
 from feast.feature_view import DUMMY_ENTITY
 from feast.protos.feast.core.Registry_pb2 import Registry as RegistryProto
-from feast.repo_config import RegistryConfig, RepoConfig
+from feast.repo_config import FeastConfigBaseModel, RegistryConfig, RepoConfig
 from feast.repo_contents import RepoContents
 from feast.repo_operations import apply_total_with_repo_instance
 
 from featurebyte.enum import DBVarType, InternalName, SpecialColumnName
 from featurebyte.feast.enum import to_feast_primitive_type
 from featurebyte.feast.model.feature_store import (
     FeastDatabaseDetails,
     FeatureStoreDetailsWithFeastConfiguration,
 )
 from featurebyte.feast.model.online_store import get_feast_online_store_details
 from featurebyte.feast.utils.on_demand_view import OnDemandFeatureViewConstructor
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.entity import EntityModel
-from featurebyte.models.entity_lookup_feature_table import get_entity_lookup_feature_tables
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.offline_store_ingest_query import (
     OfflineStoreEntityInfo,
     OfflineStoreIngestQueryGraph,
     get_time_aggregate_ttl_in_secs,
 )
 from featurebyte.models.online_store import OnlineStoreModel
 from featurebyte.models.parent_serving import EntityLookupStep
+from featurebyte.models.precomputed_lookup_feature_table import (
+    _get_feature_lists_to_relationships_info,
+    get_precomputed_lookup_feature_table,
+)
+from featurebyte.query_graph.model.entity_relationship_info import EntityAncestorDescendantMapper
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.query_graph.sql.entity import get_combined_serving_names
 
+DEFAULT_REGISTRY_PROJECT_NAME = "featurebyte_project"
+
 
 class EntityFeatureChecker:
     """
     Class for checking the consistency and completeness of entities and features.
     """
 
     @staticmethod
@@ -118,14 +125,15 @@
 
     table_name: str
     feature_job_setting: Optional[FeatureJobSetting]
     has_ttl: bool
     output_column_names: List[str]
     output_dtypes: List[DBVarType]
     primary_entity_info: List[OfflineStoreEntityInfo]
+    source_feature_table_name: Optional[str]
 
     @property
     def primary_entity_ids(self) -> Tuple[PydanticObjectId, ...]:
         """
         Get primary entity ids
 
         Returns
@@ -185,22 +193,18 @@
         FeastEntity
             Feast entity
         """
         # FIXME: We likely need to set the value type based on the dtype of the primary entity
         value_type = to_feast_primitive_type(DBVarType.VARCHAR).to_value_type()
         assert len(self.primary_entity_info) > 0
         serving_names = [entity_info.name for entity_info in self.primary_entity_info]
-        if len(serving_names) == 1:
-            join_keys = serving_names
-        else:
-            # For now, an entity may only have a single join key in feast
-            join_keys = [get_combined_serving_names(serving_names)]
+        entity_name = get_combined_serving_names(serving_names)
         entity = FeastEntity(
-            name=" x ".join(serving_names),
-            join_keys=join_keys,
+            name=entity_name,
+            join_keys=[entity_name],
             value_type=value_type,
         )
         return entity  # type: ignore[no-any-return]
 
     def create_feast_data_source(
         self,
         database_details: FeastDatabaseDetails,
@@ -290,14 +294,15 @@
     @staticmethod
     def create_offline_store_tables(
         features: List[FeatureModel],
         feature_lists: List[FeatureListModel],
         entity_id_to_serving_name: Dict[PydanticObjectId, str],
         feature_store: FeatureStoreModel,
         entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep],
+        serving_entity_ids: Optional[List[PydanticObjectId]],
     ) -> List[OfflineStoreTable]:
         """
         Group each offline store ingest query graphs of features into list of offline store tables
 
         Parameters
         ----------
         features: List[FeatureModel]
@@ -306,101 +311,120 @@
             List of feature lists
         entity_id_to_serving_name: Dict[PydanticObjectId, str]
             Mapping from entity id to serving name
         feature_store: FeatureStoreModel
             Feature store model
         entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep]
             Entity lookup steps mapping derived from feature lists
+        serving_entity_ids: Optional[List[PydanticObjectId]]
+            Serving entity ids based on the deployment
 
         Returns
         -------
         List[OfflineStoreTable]
             List of offline store tables
         """
         offline_table_key_to_ingest_query_graphs = defaultdict(list)
+        offline_table_key_to_feature_ids = defaultdict(set)
         for feature in features:
             offline_ingest_query_graphs = (
                 feature.offline_store_info.extract_offline_store_ingest_query_graphs()
             )
             for ingest_query_graph in offline_ingest_query_graphs:
                 table_name = ingest_query_graph.offline_store_table_name
                 offline_table_key_to_ingest_query_graphs[table_name].append(ingest_query_graph)
+                offline_table_key_to_feature_ids[table_name].add(feature.id)
 
         offline_store_tables = []
         for table_name, ingest_query_graphs in offline_table_key_to_ingest_query_graphs.items():
             offline_store_table = OfflineStoreTable.create(
                 table_name=table_name,
                 ingest_query_graphs=ingest_query_graphs,
                 entity_id_to_serving_name=entity_id_to_serving_name,
             )
             offline_store_tables.append(offline_store_table)
+            if serving_entity_ids is not None:
+                assert len(feature_lists) == 1
+                relationships_info = _get_feature_lists_to_relationships_info(feature_lists)[
+                    feature_lists[0].id
+                ]
+                relationships_mapper = EntityAncestorDescendantMapper.create(relationships_info)
+                related_serving_entity_ids = relationships_mapper.keep_related_entity_ids(
+                    entity_ids_to_filter=serving_entity_ids,
+                    filter_by=offline_store_table.primary_entity_ids,
+                )
+                if sorted(offline_store_table.primary_entity_ids) != related_serving_entity_ids:
+                    precomputed_lookup_feature_table = (
+                        OfflineStoreTableBuilder._get_precomputed_lookup_feature_table(
+                            offline_store_table=offline_store_table,
+                            full_serving_entity_ids=serving_entity_ids,
+                            feature_list=feature_lists[0],
+                            entity_id_to_serving_name=entity_id_to_serving_name,
+                            entity_lookup_steps_mapping=entity_lookup_steps_mapping,
+                            feature_store=feature_store,
+                            offline_table_key_to_feature_ids=offline_table_key_to_feature_ids,
+                        )
+                    )
+                    assert precomputed_lookup_feature_table is not None
+                    offline_store_tables.append(precomputed_lookup_feature_table)
 
-        offline_store_tables_for_entity_lookup = (
-            OfflineStoreTableBuilder.create_offline_store_tables_for_entity_lookup(
-                feature_lists=feature_lists,
-                feature_store=feature_store,
-                entity_lookup_steps_mapping=entity_lookup_steps_mapping,
-            )
-        )
-
-        return offline_store_tables + offline_store_tables_for_entity_lookup
+        return offline_store_tables
 
     @staticmethod
-    def create_offline_store_tables_for_entity_lookup(
-        feature_lists: List[FeatureListModel],
-        feature_store: FeatureStoreModel,
+    def _get_precomputed_lookup_feature_table(
+        offline_store_table: OfflineStoreTable,
+        full_serving_entity_ids: List[PydanticObjectId],
+        feature_list: FeatureListModel,
+        entity_id_to_serving_name: Dict[PydanticObjectId, str],
         entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep],
-    ) -> List[OfflineStoreTable]:
+        feature_store: FeatureStoreModel,
+        offline_table_key_to_feature_ids: Dict[str, Set[PydanticObjectId]],
+    ) -> Optional[OfflineStoreTable]:
         """
-        Create offline store tables for entity lookup purpose
+        Get a precomputed lookup feature table corresponding to offline_store_table that can be
+        readily served using serving_entity_ids
 
-        Parameters
-        ----------
-        feature_lists: List[FeatureListModel]
-            List of feature lists
-        feature_store: FeatureStoreModel
-            Feature store model
-        entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep]
-            Entity lookup steps mapping derived from feature lists
+        # noqa: DAR101
 
         Returns
         -------
-        List[OfflineStoreTable]
+        Optional[OfflineStoreTable]
         """
-
-        lookup_tables = get_entity_lookup_feature_tables(
-            feature_lists=feature_lists,
-            feature_store=feature_store,
+        precomputed_lookup_feature_table = get_precomputed_lookup_feature_table(
+            primary_entity_ids=list(offline_store_table.primary_entity_ids),
+            feature_ids=list(offline_table_key_to_feature_ids[offline_store_table.table_name]),
+            feature_list=feature_list,
+            full_serving_entity_ids=full_serving_entity_ids,
+            feature_table_name=offline_store_table.table_name,
+            feature_table_has_ttl=offline_store_table.has_ttl,
+            entity_id_to_serving_name=entity_id_to_serving_name,
             entity_lookup_steps_mapping=entity_lookup_steps_mapping,
+            feature_store_model=feature_store,
         )
-        if lookup_tables is None:
-            return []
-
-        entity_lookup_feature_tables = []
-        for lookup_table in lookup_tables:
-            entity_lookup_feature_tables.append(
-                OfflineStoreTable(
-                    table_name=lookup_table.name,
-                    feature_job_setting=lookup_table.feature_job_setting,
-                    has_ttl=lookup_table.has_ttl,
-                    output_column_names=lookup_table.output_column_names,
-                    output_dtypes=lookup_table.output_dtypes,
-                    primary_entity_info=[
-                        OfflineStoreEntityInfo(
-                            id=entity_id,
-                            name=serving_name,
-                            dtype=DBVarType.VARCHAR,
-                        )
-                        for (entity_id, serving_name) in zip(
-                            lookup_table.primary_entity_ids, lookup_table.serving_names
-                        )
-                    ],
-                )
+        if precomputed_lookup_feature_table is not None:
+            return OfflineStoreTable(
+                table_name=precomputed_lookup_feature_table.name,
+                feature_job_setting=offline_store_table.feature_job_setting,
+                has_ttl=offline_store_table.has_ttl,
+                output_column_names=offline_store_table.output_column_names,
+                output_dtypes=offline_store_table.output_dtypes,
+                primary_entity_info=[
+                    OfflineStoreEntityInfo(
+                        id=entity_id,
+                        name=serving_name,
+                        dtype=DBVarType.VARCHAR,
+                    )
+                    for (entity_id, serving_name) in zip(
+                        precomputed_lookup_feature_table.primary_entity_ids,
+                        precomputed_lookup_feature_table.serving_names,
+                    )
+                ],
+                source_feature_table_name=offline_store_table.table_name,
             )
-        return entity_lookup_feature_tables
+        return None
 
 
 class FeastAssetCreator:
     """
     Class for creating various Feast assets like Data Source, Feature View, etc.
     """
 
@@ -435,24 +459,24 @@
                                     DBVarType(req_col_node.parameters.dtype)
                                 ),
                             )
                         ],
                     )
 
             if SpecialColumnName.POINT_IN_TIME.value not in name_to_feast_request_source:
-                name_to_feast_request_source[
-                    SpecialColumnName.POINT_IN_TIME.value
-                ] = FeastRequestSource(
-                    name=SpecialColumnName.POINT_IN_TIME.value,
-                    schema=[
-                        FeastField(
-                            name=SpecialColumnName.POINT_IN_TIME.value,
-                            dtype=to_feast_primitive_type(DBVarType.TIMESTAMP),
-                        )
-                    ],
+                name_to_feast_request_source[SpecialColumnName.POINT_IN_TIME.value] = (
+                    FeastRequestSource(
+                        name=SpecialColumnName.POINT_IN_TIME.value,
+                        schema=[
+                            FeastField(
+                                name=SpecialColumnName.POINT_IN_TIME.value,
+                                dtype=to_feast_primitive_type(DBVarType.TIMESTAMP),
+                            )
+                        ],
+                    )
                 )
 
         return name_to_feast_request_source
 
     @staticmethod
     def create_feast_on_demand_feature_views(
         features: List[FeatureModel],
@@ -549,102 +573,224 @@
 
 class FeastRegistryBuilder:
     """
     Class for constructing the Feast Registry.
     """
 
     @staticmethod
-    def _create_repo_config(
-        project_name: str, online_store: Optional[OnlineStoreModel], registry_file_path: str
+    def get_offline_store_config(
+        feature_store_model: FeatureStoreModel, offline_store_credentials: Any
+    ) -> Any:
+        """
+        Get the offline store configuration
+
+        Parameters
+        ----------
+        feature_store_model: FeatureStoreModel
+            Feature store model
+        offline_store_credentials: Any
+            Offline store credentials
+
+        Returns
+        -------
+        Any
+            Offline store configuration
+        """
+        feature_store_details = FeatureStoreDetailsWithFeastConfiguration(
+            **feature_store_model.get_feature_store_details().dict()
+        )
+        database_credential = None
+        storage_credential = None
+        if offline_store_credentials:
+            database_credential = offline_store_credentials.database_credential
+            storage_credential = offline_store_credentials.storage_credential
+        offline_store_config = feature_store_details.details.get_offline_store_config(
+            database_credential=database_credential,
+            storage_credential=storage_credential,
+        )
+        return offline_store_config
+
+    @staticmethod
+    def get_online_store_config(
+        online_store: Optional[OnlineStoreModel],
+    ) -> Optional[FeastConfigBaseModel]:
+        """
+        Get the online store configuration
+
+        Parameters
+        ----------
+        online_store: Optional[OnlineStoreModel]
+            Online store model
+
+        Returns
+        -------
+        Optional[FaestConfigBaseModel]
+            Online store configuration
+        """
+        if online_store is None:
+            return None
+
+        return get_feast_online_store_details(
+            online_store_details=online_store.details
+        ).to_feast_online_store_config()
+
+    @staticmethod
+    def create_repo_config(
+        project_name: str,
+        registry_file_path: str,
+        offline_store_config: Optional[Any] = None,
+        online_store_config: Optional[Any] = None,
+        registry_store_type: Optional[str] = None,
     ) -> RepoConfig:
-        online_store_config: Optional[Dict[str, Any]] = None
-        if online_store:
-            online_store_config = (
-                get_feast_online_store_details(
-                    online_store_details=online_store.details,
-                )
-                .to_feast_online_store_config()
-                .dict(by_alias=True)
-            )
+        """
+        Create a RepoConfig object for the feast registry construction
+
+        Parameters
+        ----------
+        project_name: str
+            Project name
+        registry_file_path: str
+            Registry file path
+        offline_store_config: Optional[Any]
+            Feast offline store configuration
+        online_store_config: Optional[Any]
+            Online store configuration
+        registry_store_type: Optional[str]
+            Registry store type used for the registry
+
+        Returns
+        -------
+        RepoConfig
+        """
+        repo_config_kwargs = {
+            "online_store": online_store_config,
+            "entity_key_serialization_version": 2,
+        }
+        if offline_store_config:
+            repo_config_kwargs["offline_store"] = offline_store_config
+
+        registry_config_kwargs = {}
+        if registry_store_type:
+            registry_config_kwargs["registry_store_type"] = registry_store_type
+
         return RepoConfig(
             project=project_name,
             provider="local",
             registry=RegistryConfig(
                 registry_type="file",
                 path=registry_file_path,
                 cache_ttl_seconds=0,
+                **registry_config_kwargs,
             ),
-            online_store=online_store_config,
+            **repo_config_kwargs,
         )
 
     @classmethod
-    def _create_feast_registry_proto(
+    def create_feast_registry_proto_from_repo_content(
         cls,
-        project_name: Optional[str],
+        project_name: str,
+        offline_store_config: Optional[Any],
         online_store: Optional[OnlineStoreModel],
-        feast_data_sources: List[FeastDataSource],
-        primary_entity_ids_to_feast_entity: Dict[Tuple[PydanticObjectId, ...], FeastEntity],
-        feast_request_sources: List[FeastRequestSource],
-        feast_feature_views: List[FeastFeatureView],
-        feast_on_demand_feature_views: List[FeastOnDemandFeatureView],
-        feast_feature_services: List[FeastFeatureService],
+        repo_content: RepoContents,
     ) -> RegistryProto:
-        project_name = project_name or "featurebyte_project"
+        """
+        Create a feast RegistryProto from a RepoContents object
+
+        Parameters
+        ----------
+        project_name: str
+            Project name
+        offline_store_config: Optional[Any]
+            Offline store configuration
+        online_store: Optional[OnlineStoreModel]
+            Online store model
+        repo_content: RepoContents
+            Repo contents containing the feast assets
+
+        Returns
+        -------
+        RegistryProto
+        """
         with tempfile.NamedTemporaryFile() as temp_file:
-            repo_config = cls._create_repo_config(
+            online_store_config = cls.get_online_store_config(online_store)
+            repo_config = cls.create_repo_config(
                 project_name=project_name,
-                online_store=online_store,
+                offline_store_config=offline_store_config,
+                online_store_config=online_store_config,
                 registry_file_path=temp_file.name,
             )
             feature_store = FeastFeatureStore(config=repo_config)
             registry = feature_store.registry
 
-            # prepare repo content by adding all feast assets
-            repo_content = RepoContents(
-                data_sources=[],
-                entities=[],
-                feature_views=[],
-                feature_services=[],
-                on_demand_feature_views=[],
-                stream_feature_views=[],
-                request_feature_views=[],
-            )
-            for data_source in feast_data_sources + feast_request_sources:
-                repo_content.data_sources.append(data_source)
-            for entity in primary_entity_ids_to_feast_entity.values():
-                repo_content.entities.append(entity)
-            for feature_view in feast_feature_views:
-                repo_content.feature_views.append(feature_view)
-            for on_demand_feature_view in feast_on_demand_feature_views:
-                repo_content.on_demand_feature_views.append(on_demand_feature_view)
-            for feature_service in feast_feature_services:
-                repo_content.feature_services.append(feature_service)
-
             with patch("feast.on_demand_feature_view.OnDemandFeatureView.infer_features"):
                 # FIXME: (DEV-2946) patch to avoid calling infer_features() which may cause error
                 #  when the input to the on-demand feature view contains types requiring json decoding
                 #  (COUNT_DICT or ARRAY types) this simulates feast apply command
                 apply_total_with_repo_instance(
                     store=feature_store,
                     project=project_name,
                     registry=registry,
                     repo=repo_content,
                     skip_source_validation=True,
                 )
                 return cast(RegistryProto, registry.proto())
 
     @classmethod
+    def _create_feast_registry_proto(
+        cls,
+        project_name: Optional[str],
+        online_store: Optional[OnlineStoreModel],
+        feast_data_sources: List[FeastDataSource],
+        primary_entity_ids_to_feast_entity: Dict[Tuple[PydanticObjectId, ...], FeastEntity],
+        feast_request_sources: List[FeastRequestSource],
+        feast_feature_views: List[FeastFeatureView],
+        feast_on_demand_feature_views: List[FeastOnDemandFeatureView],
+        feast_feature_services: List[FeastFeatureService],
+    ) -> RegistryProto:
+        project_name = project_name or DEFAULT_REGISTRY_PROJECT_NAME
+
+        # prepare repo content by adding all feast assets
+        repo_content = RepoContents(
+            data_sources=[],
+            entities=[],
+            feature_views=[],
+            feature_services=[],
+            on_demand_feature_views=[],
+            stream_feature_views=[],
+            request_feature_views=[],
+        )
+        for data_source in feast_data_sources + feast_request_sources:
+            repo_content.data_sources.append(data_source)
+        for entity in primary_entity_ids_to_feast_entity.values():
+            repo_content.entities.append(entity)
+        for feature_view in feast_feature_views:
+            repo_content.feature_views.append(feature_view)
+        for on_demand_feature_view in feast_on_demand_feature_views:
+            repo_content.on_demand_feature_views.append(on_demand_feature_view)
+        for feature_service in feast_feature_services:
+            repo_content.feature_services.append(feature_service)
+
+        registry_proto = cls.create_feast_registry_proto_from_repo_content(
+            project_name=project_name,
+            offline_store_config=None,
+            online_store=online_store,
+            repo_content=repo_content,
+        )
+        return registry_proto
+
+    @classmethod
     def create(  # pylint: disable=too-many-locals
         cls,
         feature_store: FeatureStoreModel,
         online_store: Optional[OnlineStoreModel],
         entities: List[EntityModel],
         features: List[FeatureModel],
         feature_lists: List[FeatureListModel],
         entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep],
+        serving_entity_ids: Optional[List[PydanticObjectId]],
         project_name: Optional[str] = None,
     ) -> RegistryProto:
         """
         Create a feast RegistryProto from featurebyte asset models
 
         Parameters
         ----------
@@ -656,14 +802,16 @@
             List of featurebyte entity models
         features: List[FeatureModel]
             List of featurebyte feature models
         feature_lists: List[FeatureListModel]
             List of featurebyte feature list models
         entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep]
             Mapping from relationships info id to EntityLookupStep objects
+        serving_entity_ids: Optional[List[PydanticObjectId]]
+            Serving entity ids of the deployment that the registry will be associated with
         project_name: Optional[str]
             Project name
 
         Returns
         -------
         RegistryProto
         """
@@ -671,18 +819,20 @@
         EntityFeatureChecker.check_missing_features(features, feature_lists)
         offline_store_tables = OfflineStoreTableBuilder.create_offline_store_tables(
             features=features,
             feature_lists=feature_lists,
             entity_id_to_serving_name={entity.id: entity.serving_names[0] for entity in entities},
             feature_store=feature_store,
             entity_lookup_steps_mapping=entity_lookup_steps_mapping,
+            serving_entity_ids=serving_entity_ids,
         )
         primary_entity_ids_to_feast_entity: Dict[Tuple[PydanticObjectId, ...], FeastEntity] = {}
         feast_data_sources = []
         name_to_feast_feature_view: Dict[str, FeastFeatureView] = {}
+        all_feast_feature_views = []
         feature_store_details = FeatureStoreDetailsWithFeastConfiguration(
             **feature_store.get_feature_store_details().dict()
         )
         for offline_store_table in offline_store_tables:
             entity_key = offline_store_table.primary_entity_ids
             if len(entity_key) > 0:
                 feast_entity = primary_entity_ids_to_feast_entity.get(
@@ -701,15 +851,20 @@
             feast_data_sources.append(feast_data_source)
 
             feast_feature_view = offline_store_table.create_feast_feature_view(
                 name=offline_store_table.table_name,
                 entity=feast_entity,
                 data_source=feast_data_source,
             )
-            name_to_feast_feature_view[offline_store_table.table_name] = feast_feature_view
+            if offline_store_table.source_feature_table_name is not None:
+                table_name = offline_store_table.source_feature_table_name
+            else:
+                table_name = offline_store_table.table_name
+            name_to_feast_feature_view[table_name] = feast_feature_view
+            all_feast_feature_views.append(feast_feature_view)
 
         name_to_feast_request_source = FeastAssetCreator.create_feast_name_to_request_source(
             features
         )
         on_demand_feature_views = FeastAssetCreator.create_feast_on_demand_feature_views(
             features=features,
             name_to_feast_feature_view=name_to_feast_feature_view,
@@ -724,11 +879,95 @@
 
         return cls._create_feast_registry_proto(
             project_name=project_name,
             online_store=online_store,
             feast_data_sources=feast_data_sources,
             primary_entity_ids_to_feast_entity=primary_entity_ids_to_feast_entity,
             feast_request_sources=list(name_to_feast_request_source.values()),
-            feast_feature_views=list(name_to_feast_feature_view.values()),
+            feast_feature_views=all_feast_feature_views,
             feast_on_demand_feature_views=on_demand_feature_views,
             feast_feature_services=feast_feature_services,
         )
+
+    @classmethod
+    def create_feast_registry_proto_for_feature_materialization(
+        cls,
+        project_name: Optional[str],
+        offline_store_config: Any,
+        online_store: Optional[OnlineStoreModel],
+        feature_table_name: str,
+        feast_stores: Sequence[FeastFeatureStore],
+    ) -> RegistryProto:
+        """
+        Create a feast RegistryProto for feature materialization task
+
+        Parameters
+        ----------
+        project_name: Optional[str]
+            Project name
+        offline_store_config: Any
+            Offline store configuration
+        online_store: Optional[OnlineStoreModel]
+            Online store model
+        feature_table_name: str
+            Feature table name
+        feast_stores: Sequence[FeastFeatureStore]
+            Sequence of feast feature stores to get the feature view from
+
+        Returns
+        -------
+        RegistryProto
+        """
+        repo_content = RepoContents(
+            data_sources=[],
+            entities=[],
+            feature_views=[],
+            feature_services=[],
+            on_demand_feature_views=[],
+            stream_feature_views=[],
+            request_feature_views=[],
+        )
+
+        first_store = feast_stores[0]
+        repo_content.data_sources.extend(first_store.list_data_sources())
+        repo_content.entities.extend(first_store.list_entities())
+
+        first_fv = first_store.get_feature_view(feature_table_name)
+        if not first_fv.entities:
+            existing_entities = {entity.name for entity in repo_content.entities}
+            if DUMMY_ENTITY.name not in existing_entities:
+                repo_content.entities.append(DUMMY_ENTITY)
+
+            fv_entities = [DUMMY_ENTITY]
+        else:
+            fv_entities = [
+                FeastEntity(
+                    name=entity.name,
+                    join_keys=[entity.name],
+                    value_type=entity.dtype.to_value_type(),
+                )
+                for entity in first_fv.entity_columns
+            ]
+
+        feature_view_params = {
+            "name": feature_table_name,
+            "entities": fv_entities,
+            "ttl": first_fv.ttl,
+            "online": True,
+            "source": first_fv.batch_source,
+        }
+        name_to_field_map = {}
+        for feast_store in feast_stores:
+            for feature in feast_store.get_feature_view(feature_table_name).features:
+                if feature.name not in name_to_field_map:
+                    name_to_field_map[feature.name] = feature
+
+        feature_view_params["schema"] = list(name_to_field_map.values())
+        repo_content.feature_views.append(FeastFeatureView(**feature_view_params))
+
+        registry_proto = FeastRegistryBuilder.create_feast_registry_proto_from_repo_content(
+            project_name=project_name or DEFAULT_REGISTRY_PROJECT_NAME,
+            offline_store_config=offline_store_config,
+            online_store=online_store,
+            repo_content=repo_content,
+        )
+        return registry_proto
```

### Comparing `featurebyte-1.0.2/featurebyte/feature_manager/model.py` & `featurebyte-1.0.3/featurebyte/feature_manager/model.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This modules contains feature manager specific models
 """
+
 from __future__ import annotations
 
 from featurebyte.enum import SourceType
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.tile import TileSpec
 from featurebyte.query_graph.sql.interpreter import GraphInterpreter
 
@@ -52,10 +53,11 @@
                 value_column_types=info.tile_value_types,
                 tile_id=info.tile_table_id,
                 aggregation_id=info.aggregation_id,
                 aggregation_function_name=info.agg_func,
                 parent_column_name=info.parent,
                 category_column_name=info.value_by_column,
                 feature_store_id=self.tabular_source.feature_store_id,
+                windows=info.windows,
             )
             out.append(tile_spec)
         return out
```

### Comparing `featurebyte-1.0.2/featurebyte/feature_manager/sql_template.py` & `featurebyte-1.0.3/featurebyte/feature_manager/sql_template.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature SQL template
 """
+
 from jinja2 import Template
 
 tm_feature_tile_monitor = Template(
     """
     SELECT
         f.FEATURE_NAME AS NAME,
         f.FEATURE_VERSION AS VERSION,
```

### Comparing `featurebyte-1.0.2/featurebyte/list_utility.py` & `featurebyte-1.0.3/featurebyte/list_utility.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utility module
 """
+
 from typing import Optional
 
 import inspect
 from http import HTTPStatus
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/logging.py` & `featurebyte-1.0.3/featurebyte/logging.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 """
 Logging formatting
 """
+
 from __future__ import annotations
 
 from typing import Any, Mapping
 
 import logging
 import os
 import sys
 
 from featurebyte.common.env_util import is_notebook
-from featurebyte.config import Configurations
+from featurebyte.config import Configurations, LogLevel
 
 
 class CustomLogger(logging.Logger):
     """
     Custom logger to capture extra field
     """
 
@@ -97,29 +98,29 @@
 )
 CONSOLE_LOG_FORMATTER = logging.Formatter(
     "%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s | %(extra)s",
     "%Y-%m-%d %H:%M:%S",
 )
 
 
-def set_logger_level(logger: logging.Logger, configurations: Configurations) -> None:
+def set_logger_level(logger: logging.Logger, logging_level: LogLevel) -> None:
     """
     Set logger level
 
     Parameters
     ----------
     logger: logging.Logger
         Logger to set level
-    configurations: Configurations
-        Optional configurations used to configure logger
+    logging_level: LogLevel
+        Logging level to set to if LOG_LEVEL env var is not available
     """
     if os.environ.get("LOG_LEVEL"):
         logger.setLevel(os.environ.get("LOG_LEVEL"))  # type: ignore[arg-type]
     else:
-        logger.setLevel(configurations.logging.level)
+        logger.setLevel(logging_level)
 
 
 def get_logger(logger_name: str, configurations: Configurations | None = None) -> logging.Logger:
     """
     Get logger
 
     Parameters
@@ -129,38 +130,38 @@
     configurations: Configurations
         Optional configurations used to configure logger
 
     Returns
     -------
     logging.Logger
     """
+    _ = configurations
+    return logging.getLogger(logger_name)
+
+
+def configure_featurebyte_logger(configurations: Configurations | None = None) -> None:
+    """
+    Configure featurebyte logger
+
+    Parameters
+    ----------
+    configurations: Optional[Configurations]
+        Optional configurations used to configure logger
+    """
     configurations = configurations or Configurations()
+
     is_notebook_env = is_notebook()
     formatter: logging.Formatter = CONSOLE_LOG_FORMATTER
     if is_notebook_env:
         formatter = NOTEBOOK_LOG_FORMATTER
 
     console_handler = logging.StreamHandler(stream=sys.stderr)
     console_handler.setFormatter(formatter)
-    logger = logging.getLogger(logger_name)
-    logger.propagate = False
+    logger = logging.getLogger("featurebyte")
     if logger.hasHandlers():
         logger.handlers.clear()
     logger.addHandler(console_handler)
-    set_logger_level(logger, configurations)
-    return logger
-
-
-def reconfigure_loggers(configurations: Configurations) -> None:
-    """
-    Reconfigure all loggers with configurations.
 
-    Parameters
-    ----------
-    configurations: Configurations
-        Configurations to use
-    """
-    for name in logging.root.manager.loggerDict:  # pylint: disable=no-member
-        set_logger_level(logging.getLogger(name), configurations)
+    set_logger_level(logger, configurations.logging.level)
 
 
-__all__ = ["get_logger"]
+__all__ = ["get_logger", "configure_featurebyte_logger"]
```

### Comparing `featurebyte-1.0.2/featurebyte/middleware.py` & `featurebyte-1.0.3/featurebyte/middleware.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Handles API requests middleware
 """
+
 from typing import Any, Awaitable, Callable, Dict, Optional, Type, Union
 
 import inspect
 from http import HTTPStatus
 
 from fastapi import FastAPI, Request, Response
 from pydantic import ValidationError
@@ -149,24 +150,24 @@
         """
         return False
 
 
 # UNPROCESSABLE_ENTITY errors
 ExecutionContext.register(
     DocumentNotFoundError,
-    handle_status_code=lambda req, exc: HTTPStatus.UNPROCESSABLE_ENTITY
-    if req.method == "POST"
-    else HTTPStatus.NOT_FOUND,
+    handle_status_code=lambda req, exc: (
+        HTTPStatus.UNPROCESSABLE_ENTITY if req.method == "POST" else HTTPStatus.NOT_FOUND
+    ),
 )
 
 ExecutionContext.register(
     ColumnNotFoundError,
-    handle_status_code=lambda req, exc: HTTPStatus.UNPROCESSABLE_ENTITY
-    if req.method == "POST"
-    else HTTPStatus.NOT_FOUND,
+    handle_status_code=lambda req, exc: (
+        HTTPStatus.UNPROCESSABLE_ENTITY if req.method == "POST" else HTTPStatus.NOT_FOUND
+    ),
 )
 
 ExecutionContext.register(ValidationError, handle_status_code=HTTPStatus.UNPROCESSABLE_ENTITY)
 
 ExecutionContext.register(
     BaseUnprocessableEntityError, handle_status_code=HTTPStatus.UNPROCESSABLE_ENTITY
 )
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/migration_data_service.py` & `featurebyte-1.0.3/featurebyte/migration/migration_data_service.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Migration related service class(es)
 """
+
 from featurebyte.migration.model import (
     SchemaMetadataCreate,
     SchemaMetadataModel,
     SchemaMetadataUpdate,
 )
 from featurebyte.service.base_document import BaseDocumentService
 from featurebyte.service.mixin import GetOrCreateMixin
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/model.py` & `featurebyte-1.0.3/featurebyte/migration/model.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains schema related model
 """
+
 from typing import List
 
 from pydantic import Field
 
 from featurebyte.enum import StrEnum
 from featurebyte.models.base import (
     FeatureByteBaseDocumentModel,
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/run.py` & `featurebyte-1.0.3/featurebyte/migration/run.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Migration script
 """
+
 from __future__ import annotations
 
 from typing import Any, AsyncGenerator, Callable, Set, cast
 
 import asyncio
 import importlib
 import inspect
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/service/__init__.py` & `featurebyte-1.0.3/featurebyte/migration/service/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Database Migration
 """
+
 from typing import Any, Awaitable, Callable, TypeVar, cast
 
 import functools
 
 from pydantic import BaseModel
 
 AwaitableMigrateFunction = TypeVar(
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/service/data_warehouse.py` & `featurebyte-1.0.3/featurebyte/migration/service/data_warehouse.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Migration service for data warehouse working schema
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 import textwrap
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/service/feature.py` & `featurebyte-1.0.3/featurebyte/migration/service/feature.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature migration service
 """
+
 from typing import Dict, List
 
 from bson import ObjectId
 
 from featurebyte.logging import get_logger
 from featurebyte.migration.service import migrate
 from featurebyte.migration.service.mixin import BaseMongoCollectionMigration
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/service/feature_list.py` & `featurebyte-1.0.3/featurebyte/migration/service/feature_list.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature list migration service
 """
+
 from typing import Dict, List
 
 from bson import ObjectId
 
 from featurebyte.logging import get_logger
 from featurebyte.migration.service import migrate
 from featurebyte.migration.service.mixin import BaseDocumentServiceT, BaseMongoCollectionMigration
```

### Comparing `featurebyte-1.0.2/featurebyte/migration/service/mixin.py` & `featurebyte-1.0.3/featurebyte/migration/service/mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 MigrationServiceMixin class
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, Optional
 
 from abc import ABC, abstractmethod
 
 from celery import Celery
```

### Comparing `featurebyte-1.0.2/featurebyte/models/__init__.py` & `featurebyte-1.0.3/featurebyte/models/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Document models for serialization to persistent storage
 """
+
 from featurebyte.models.dimension_table import DimensionTableModel
 from featurebyte.models.entity import EntityModel
 from featurebyte.models.event_table import EventTableModel
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.feature_namespace import FeatureNamespaceModel
 from featurebyte.models.feature_store import FeatureStoreModel
```

### Comparing `featurebyte-1.0.2/featurebyte/models/base.py` & `featurebyte-1.0.3/featurebyte/models/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 FeatureByte specific BaseModel
 """
+
 from __future__ import annotations
 
-from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type, TypeVar, Union
 
 import json
 import re
 from datetime import datetime
 from pathlib import Path
 
 import numpy as np
@@ -22,14 +23,27 @@
 Model = TypeVar("Model", bound="FeatureByteBaseModel")
 
 DEFAULT_CATALOG_ID = ObjectId("23eda344d0313fb925f7883a")
 ACTIVE_CATALOG_ID: Optional[ObjectId] = None
 CAMEL_CASE_TO_SNAKE_CASE_PATTERN = re.compile("((?!^)(?<!_)[A-Z][a-z]+|(?<=[a-z0-9])[A-Z])")
 
 
+if TYPE_CHECKING:
+    NameStr = str
+else:
+
+    class NameStr(StrictStr):
+        """
+        Name string type
+        """
+
+        min_length = 0
+        max_length = 255
+
+
 def get_active_catalog_id() -> Optional[ObjectId]:
     """
     Get active catalog id
 
     Returns
     -------
     Optional[ObjectId]
@@ -263,15 +277,15 @@
 
     id: PydanticObjectId = Field(
         default_factory=ObjectId, alias="_id", allow_mutation=False, description="Record identifier"
     )
     user_id: Optional[PydanticObjectId] = Field(
         default=None, allow_mutation=False, description="User identifier"
     )
-    name: Optional[StrictStr] = Field(description="Record name")
+    name: Optional[NameStr] = Field(description="Record name")
     created_at: Optional[datetime] = Field(
         default=None, allow_mutation=False, description="Record creation time"
     )
     updated_at: Optional[datetime] = Field(
         default=None, allow_mutation=False, description="Record last updated time"
     )
     block_modification_by: List[ReferenceInfo] = Field(
@@ -320,23 +334,29 @@
         Returns
         -------
         List[UniqueValuesConstraint]
             collection name
         """
         return cls.Settings.unique_constraints
 
-    @property
-    def remote_attribute_paths(self) -> List[Path]:
+    @classmethod
+    def _get_remote_attribute_paths(cls, document_dict: Dict[str, Any]) -> List[Path]:
         """
-        Remote attribute paths
+        Get remote attribute paths for a document
+
+        Parameters
+        ----------
+        document_dict: Dict[str, Any]
+            Dict representation of the document
 
         Returns
         -------
         List[Path]
         """
+        _ = document_dict
         return []
 
     class Settings:
         """
         MongoDB settings
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/models/batch_feature_table.py` & `featurebyte-1.0.3/featurebyte/models/batch_feature_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchFeatureTable related model(s)
 """
+
 from __future__ import annotations
 
 import pymongo
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.materialized_table import MaterializedTableModel
```

### Comparing `featurebyte-1.0.2/featurebyte/models/batch_request_table.py` & `featurebyte-1.0.3/featurebyte/models/batch_request_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTable models
 """
+
 from __future__ import annotations
 
 from typing import Optional, Union
 from typing_extensions import Annotated
 
 import pymongo
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/models/catalog.py` & `featurebyte-1.0.3/featurebyte/models/catalog.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Catalog related models
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from datetime import datetime
 
 import pymongo
```

### Comparing `featurebyte-1.0.2/featurebyte/models/context.py` & `featurebyte-1.0.3/featurebyte/models/context.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 """
 This module contains context related models.
 """
+
 from typing import Any, Dict, List, Optional
 
 import pymongo
-from pydantic import root_validator
+from pydantic import root_validator, validator
 
+from featurebyte.common.validator import construct_sort_validator
 from featurebyte.models.base import (
     FeatureByteCatalogBaseDocumentModel,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 from featurebyte.query_graph.graph import QueryGraph
@@ -30,14 +32,19 @@
     primary_entity_ids: List[PydanticObjectId]
     graph: Optional[QueryGraph]
     node_name: Optional[str]
 
     default_preview_table_id: Optional[PydanticObjectId]
     default_eda_table_id: Optional[PydanticObjectId]
 
+    # pydantic validators
+    _sort_ids_validator = validator("primary_entity_ids", allow_reuse=True)(
+        construct_sort_validator()
+    )
+
     @root_validator(pre=True)
     @classmethod
     def _set_primary_entity_ids(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         entity_ids = values.get("entity_ids", None)
         primary_entity_ids = values.get("primary_entity_ids", None)
         if entity_ids and not primary_entity_ids:
             values["primary_entity_ids"] = entity_ids
```

### Comparing `featurebyte-1.0.2/featurebyte/models/credential.py` & `featurebyte-1.0.3/featurebyte/models/credential.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Document model for stored credentials
 """
+
 from typing import Callable, Dict, List, Literal, Optional, Union
 from typing_extensions import Annotated
 
 import base64  # pylint: disable=wrong-import-order
 import os  # pylint: disable=wrong-import-order
 
 import pymongo
```

### Comparing `featurebyte-1.0.2/featurebyte/models/deployment.py` & `featurebyte-1.0.3/featurebyte/models/use_case.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,61 +1,65 @@
 """
-Deployment model
+Use Case model
 """
-from __future__ import annotations
 
 from typing import List, Optional
 
 import pymongo
-from pydantic import BaseSettings, Field, StrictStr
+from pydantic import Field
 
 from featurebyte.models.base import (
     FeatureByteCatalogBaseDocumentModel,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 
 
-class DeploymentModel(FeatureByteCatalogBaseDocumentModel):
-    """Model for a deployment"""
+class UseCaseModel(FeatureByteCatalogBaseDocumentModel):
+    """
+    UseCaseModel represents a use case within a feature store
+
+    context_id: PydanticObjectId
+        The context id of ContextModel
+    target_id: Optional[PydanticObjectId]
+        The target id of TargetModel
+    target_namespace_id: Optional[PydanticObjectId]
+        The target namespace id of TargetModel
+    default_preview_table_id: PydanticObjectId
+        The default preview observation table
+    default_eda_table_id: PydanticObjectId
+        The default eda observation table
+    """
 
-    name: Optional[StrictStr]
-    feature_list_id: PydanticObjectId
-    enabled: bool
-    context_id: Optional[PydanticObjectId] = Field(default=None)
-    use_case_id: Optional[PydanticObjectId] = Field(default=None)
+    context_id: PydanticObjectId
+    target_id: Optional[PydanticObjectId]
+    target_namespace_id: PydanticObjectId
+    default_preview_table_id: Optional[PydanticObjectId] = Field(default=None)
+    default_eda_table_id: Optional[PydanticObjectId] = Field(default=None)
 
     class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
 
-        collection_name: str = "deployment"
+        collection_name: str = "use_case"
         unique_constraints: List[UniqueValuesConstraint] = [
             UniqueValuesConstraint(
                 fields=("_id",),
                 conflict_fields_signature={"id": ["_id"]},
-                resolution_signature=UniqueConstraintResolutionSignature.GET_NAME,
+                resolution_signature=UniqueConstraintResolutionSignature.GET_BY_ID,
             ),
             UniqueValuesConstraint(
                 fields=("name",),
                 conflict_fields_signature={"name": ["name"]},
                 resolution_signature=UniqueConstraintResolutionSignature.GET_NAME,
             ),
         ]
 
         indexes = FeatureByteCatalogBaseDocumentModel.Settings.indexes + [
-            pymongo.operations.IndexModel("feature_list_id"),
+            pymongo.operations.IndexModel("context_id"),
             [
                 ("name", pymongo.TEXT),
                 ("description", pymongo.TEXT),
             ],
         ]
-
-
-class FeastIntegrationSettings(BaseSettings):
-    """
-    Feast integration settings
-    """
-
-    FEATUREBYTE_FEAST_INTEGRATION_ENABLED: bool = False
```

### Comparing `featurebyte-1.0.2/featurebyte/models/dimension_table.py` & `featurebyte-1.0.3/featurebyte/models/dimension_table.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains DimensionTable related models
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, List, Tuple, Type
 
 from pydantic import root_validator
 
 from featurebyte.common.validator import construct_data_model_root_validator
@@ -38,14 +39,22 @@
         )
     )
 
     @property
     def primary_key_columns(self) -> List[str]:
         return [self.dimension_id_column]
 
+    @property
+    def special_columns(self) -> List[str]:
+        cols = [
+            self.dimension_id_column,
+            self.record_creation_timestamp_column,
+        ]
+        return [col for col in cols if col]
+
     def create_view_graph_node(
         self, input_node: InputNode, metadata: ViewMetadata, **kwargs: Any
     ) -> Tuple[GraphNode, List[ColumnInfo]]:
         table_data = DimensionTableData(**self.dict(by_alias=True)).clone(
             column_cleaning_operations=metadata.column_cleaning_operations
         )
         return table_data.construct_dimension_view_graph_node(  # pylint: disable=no-member
```

### Comparing `featurebyte-1.0.2/featurebyte/models/entity.py` & `featurebyte-1.0.3/featurebyte/models/entity.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Entity related models
 """
+
 from __future__ import annotations
 
 from typing import List
 
 from datetime import datetime
 
 import pymongo
```

### Comparing `featurebyte-1.0.2/featurebyte/models/entity_lookup_feature_table.py` & `featurebyte-1.0.3/featurebyte/service/offline_store_feature_table_comment.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,212 +1,190 @@
 """
-Entity lookup feature table construction
+OfflineStoreFeatureTableCommentService
 """
+
 from __future__ import annotations
 
-from typing import Any, Dict, List, Optional, Set
+from typing import Any, Callable, Coroutine, Dict, List, Optional, Sequence, Tuple, Union
 
 from dataclasses import dataclass
 
-from bson import ObjectId
-
-from featurebyte.enum import DBVarType, TableDataType
-from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.entity_universe import (
-    EntityUniverseModel,
-    EntityUniverseParams,
-    get_combined_universe,
-)
-from featurebyte.models.event_table import EventTableModel
-from featurebyte.models.feature_list import FeatureCluster, FeatureListModel
+from featurebyte.logging import get_logger
+from featurebyte.models.entity import EntityModel
+from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.offline_store_feature_table import OfflineStoreFeatureTableModel
-from featurebyte.models.parent_serving import EntityLookupStep
-from featurebyte.models.scd_table import SCDTableModel
-from featurebyte.models.sqlglot_expression import SqlglotExpressionModel
-from featurebyte.query_graph.enum import NodeOutputType, NodeType
-from featurebyte.query_graph.graph import QueryGraph
-from featurebyte.query_graph.model.entity_lookup_plan import EntityLookupPlanner
-from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
-from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
-from featurebyte.query_graph.node import Node
-from featurebyte.query_graph.node.generic import SCDBaseParameters
-from featurebyte.query_graph.transform.decompose_point import FeatureJobSettingExtractor
+from featurebyte.service.entity import EntityService
+from featurebyte.service.feature_namespace import FeatureNamespaceService
+from featurebyte.service.session_manager import SessionManagerService
+
+logger = get_logger(__name__)
 
 
 @dataclass
-class EntityLookupGraphResult:
+class TableComment:
     """
-    Query graph constructed for parent entity lookup
+    Representation of a comment for a specific offline feature table
     """
 
-    graph: QueryGraph
-    lookup_node: Node
-    feature_node_name: str
-    feature_dtype: DBVarType
-    feature_job_setting: Optional[FeatureJobSetting]
-
-
-def get_lookup_feature_table_name(relationship_info_id: ObjectId) -> str:
-    """
-    Get the offline feature table name for parent entity lookup
-
-    Parameters
-    ----------
-    relationship_info_id: ObjectId
-        Id of the relationship info
-
-    Returns
-    -------
-    str
-    """
-    return f"fb_entity_lookup_{relationship_info_id}"
-
-
-def get_entity_lookup_feature_tables(
-    feature_lists: List[FeatureListModel],
-    feature_store: FeatureStoreModel,
-    entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep],
-) -> Optional[List[OfflineStoreFeatureTableModel]]:
-    """
-    Get list of internal offline store feature tables for parent entity lookup purpose
-
-    Parameters
-    ----------
-    feature_lists: List[FeatureListModel]
-        Currently online enabled feature lists
-    feature_store: FeatureStoreModel
-        Feature store model
-    entity_lookup_steps_mapping: Dict[PydanticObjectId, EntityLookupStep]
-        Mapping from relationship info id to EntityLookupStep objects
-
-    Returns
-    -------
-    Optional[List[OfflineStoreFeatureTableModel]]
-    """
-    required_lookup_relationships: Set[EntityRelationshipInfo] = set()
-    catalog_id = None
-    for feature_list in feature_lists:
-        if feature_list.features_entity_lookup_info is None:
-            continue
-        for lookup_info in feature_list.features_entity_lookup_info:
-            required_lookup_relationships.update(lookup_info.join_steps)
-        if catalog_id is None:
-            catalog_id = feature_list.catalog_id
-        entity_lookup_plan = EntityLookupPlanner.generate_plan(
-            feature_list.primary_entity_ids, feature_list.relationships_info or []
-        )
-        for serving_entity_ids in feature_list.enabled_serving_entity_ids:
-            lookup_relationships = entity_lookup_plan.get_entity_lookup_steps(serving_entity_ids)
-            if lookup_relationships is not None:
-                required_lookup_relationships.update(lookup_relationships)
-
-    out = []
-    for lookup_relationship in required_lookup_relationships:
-        assert catalog_id is not None, "Catalog id is not set"
-        lookup_step = entity_lookup_steps_mapping[lookup_relationship.id]
-        lookup_graph_result = _get_entity_lookup_graph(
-            lookup_step=lookup_step,
-            feature_store=feature_store,
-        )
-        feature_cluster = FeatureCluster(
-            feature_store_id=feature_store.id,
-            graph=lookup_graph_result.graph,
-            node_names=[lookup_graph_result.feature_node_name],
-        )
-        universe_expr = get_combined_universe(
-            entity_universe_params=[
-                EntityUniverseParams(
-                    graph=lookup_graph_result.graph,
-                    node=lookup_graph_result.lookup_node,
-                    join_steps=None,
-                )
-            ],
-            source_type=feature_store.type,
-        )
-        entity_universe = EntityUniverseModel(
-            query_template=SqlglotExpressionModel.create(universe_expr)
-        )
-        entity_lookup_feature_table_model = OfflineStoreFeatureTableModel(
-            name=get_lookup_feature_table_name(lookup_step.id),
-            feature_ids=[],
-            primary_entity_ids=[lookup_step.child.entity_id],
-            serving_names=[lookup_step.child.serving_name],
-            feature_cluster=feature_cluster,
-            output_column_names=[lookup_step.parent.serving_name],
-            output_dtypes=[lookup_graph_result.feature_dtype],
-            entity_universe=entity_universe,
-            has_ttl=False,
-            feature_job_setting=lookup_graph_result.feature_job_setting,
-            entity_lookup_info=lookup_relationship,
-            catalog_id=catalog_id,
-        )
-        out.append(entity_lookup_feature_table_model)
-    return out
+    table_name: str
+    comment: str
 
 
-def _get_entity_lookup_graph(
-    lookup_step: EntityLookupStep,
-    feature_store: FeatureStoreModel,
-) -> EntityLookupGraphResult:
-    relation_table = lookup_step.table
-    graph = QueryGraph()
-    input_node = graph.add_operation_node(
-        node=relation_table.construct_input_node(feature_store_details=feature_store),
-        input_nodes=[],
-    )
-
-    feature_dtype = None
-    for column_info in relation_table.columns_info:
-        if column_info.entity_id == lookup_step.parent.entity_id:
-            feature_dtype = column_info.dtype
-    assert feature_dtype is not None
-
-    additional_params: Dict[str, Any]
-    if relation_table.type == TableDataType.SCD_TABLE:
-        assert isinstance(relation_table, SCDTableModel)
-        additional_params = {
-            "scd_parameters": SCDBaseParameters(
-                effective_timestamp_column=relation_table.effective_timestamp_column,
-                natural_key_column=relation_table.natural_key_column,
-                current_flag_column=relation_table.current_flag_column,
-                end_timestamp_column=relation_table.end_timestamp_column,
+@dataclass
+class ColumnComment:
+    """
+    Representation of a comment for a specific offline feature table column
+    """
+
+    table_name: str
+    column_name: str
+    comment: str
+
+
+class OfflineStoreFeatureTableCommentService:
+    """
+    OfflineStoreFeatureTableCommentService is responsible for generating comments for offline store
+    feature tables and columns, and applying them in the data warehouse.
+    """
+
+    def __init__(
+        self,
+        entity_service: EntityService,
+        session_manager_service: SessionManagerService,
+        feature_namespace_service: FeatureNamespaceService,
+    ):
+        self.entity_service = entity_service
+        self.session_manager_service = session_manager_service
+        self.feature_namespace_service = feature_namespace_service
+
+    async def apply_comments(
+        self,
+        feature_store: FeatureStoreModel,
+        comments: Sequence[Union[TableComment, ColumnComment]],
+        update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
+    ) -> None:
+        """
+        Add the provided table or column comments in the data warehouse
+
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            Feature store model
+        comments:  Sequence[Union[TableComment, ColumnComment]]
+            List of comments to be added
+        update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]
+            Progress callback
+        """
+        session = await self.session_manager_service.get_feature_store_session(feature_store)
+        for idx, entry in enumerate(comments):
+            try:
+                if isinstance(entry, TableComment):
+                    await session.comment_table(entry.table_name, entry.comment)
+                else:
+                    await session.comment_column(entry.table_name, entry.column_name, entry.comment)
+            except Exception as exc:  # pylint: disable=broad-except
+                if isinstance(entry, TableComment):
+                    extra = {"table_name": entry.table_name}
+                else:
+                    extra = {"table_name": entry.table_name, "column_name": entry.column_name}
+                logger.error("Failed to add comment: %s", exc, extra=extra)
+
+            if update_progress:
+                percent = int((idx + 1) / len(comments) * 100)
+                if isinstance(entry, TableComment):
+                    message = f"Added comment for table {entry.table_name}"
+                else:
+                    message = (
+                        f"Added comment for column {entry.column_name} in table {entry.table_name}"
+                    )
+                await update_progress(percent, message)
+
+    async def generate_table_comment(
+        self, feature_table_model: OfflineStoreFeatureTableModel
+    ) -> TableComment:
+        """
+        Generate comment for an offline feature table
+
+        Parameters
+        ----------
+        feature_table_model: OfflineStoreFeatureTableModel
+            Offline store feature table model
+
+        Returns
+        -------
+        TableComment
+        """
+        primary_entities = await self.entity_service.get_entities(
+            set(feature_table_model.primary_entity_ids)
+        )
+
+        def _format_entity(entity_model: EntityModel) -> str:
+            return f"{entity_model.name} (serving name: {entity_model.serving_names[0]})"
+
+        primary_entities_info = ", ".join([_format_entity(entity) for entity in primary_entities])
+        if feature_table_model.primary_entity_ids:
+            sentences = [
+                f"This feature table consists of features for primary entity {primary_entities_info}"
+            ]
+        else:
+            sentences = ["This feature table consists of features without a primary entity"]
+        if feature_table_model.feature_job_setting:
+            job_setting = feature_table_model.feature_job_setting
+            sentences.append(
+                f"It is updated every {job_setting.frequency_seconds} second(s), with a blind spot"
+                f" of {job_setting.blind_spot_seconds} second(s) and a time modulo frequency of"
+                f" {job_setting.time_modulo_frequency_seconds} second(s)"
+            )
+        comment = ". ".join(sentences) + "."
+        return TableComment(table_name=feature_table_model.name, comment=comment)
+
+    async def generate_column_comments(
+        self, feature_models: List[FeatureModel]
+    ) -> List[ColumnComment]:
+        """
+        Generate comments for columns in offline feature tables corresponding to the features
+
+        Parameters
+        ----------
+        feature_models: List[FeatureModel]
+            Feature models
+
+        Returns
+        -------
+        List[ColumnComment]
+        """
+        # Mapping to from table name and column name to comments
+        comments: Dict[Tuple[str, str], str] = {}
+
+        for feature in feature_models:
+            feature_description = (
+                await self.feature_namespace_service.get_document(feature.feature_namespace_id)
+            ).description
+
+            offline_ingest_graphs = (
+                feature.offline_store_info.extract_offline_store_ingest_query_graphs()
+            )
+            for offline_ingest_graph in offline_ingest_graphs:
+                table_name = offline_ingest_graph.offline_store_table_name
+                if feature.offline_store_info.is_decomposed:
+                    comment = (
+                        f"This intermediate feature is used to compute the feature"
+                        f" {feature.name} (version: {feature.version.name})"
+                    )
+                    if feature_description is not None:
+                        comment += f". Description of {feature.name}: {feature_description}"
+                    comments[(table_name, offline_ingest_graph.output_column_name)] = comment
+                elif feature_description is not None:
+                    comments[(table_name, offline_ingest_graph.output_column_name)] = (
+                        feature_description
+                    )
+
+        out = [
+            ColumnComment(
+                table_name=table_name,
+                column_name=column_name,
+                comment=comment,
             )
-        }
-    elif relation_table.type == TableDataType.EVENT_TABLE:
-        assert isinstance(relation_table, EventTableModel)
-        additional_params = {
-            "event_parameters": {
-                "event_timestamp_column": relation_table.event_timestamp_column,
-            }
-        }
-    else:
-        # TODO: handle ITEM_TABLE which also needs event_parameters
-        additional_params = {}
-    lookup_node = graph.add_operation(
-        node_type=NodeType.LOOKUP,
-        node_params={
-            "input_column_names": [lookup_step.parent.key],
-            "feature_names": [lookup_step.parent.serving_name],
-            "entity_column": lookup_step.child.key,
-            "serving_name": lookup_step.child.serving_name,
-            "entity_id": lookup_step.child.entity_id,
-            **additional_params,
-        },
-        node_output_type=NodeOutputType.FRAME,
-        input_nodes=[input_node],
-    )
-    feature_node = graph.add_operation(
-        node_type=NodeType.PROJECT,
-        node_params={"columns": [lookup_step.parent.serving_name]},
-        node_output_type=NodeOutputType.SERIES,
-        input_nodes=[lookup_node],
-    )
-    return EntityLookupGraphResult(
-        graph=graph,
-        lookup_node=lookup_node,
-        feature_node_name=feature_node.name,
-        feature_dtype=feature_dtype,
-        feature_job_setting=FeatureJobSettingExtractor(graph=graph).extract_from_agg_node(
-            node=lookup_node
-        ),
-    )
+            for ((table_name, column_name), comment) in comments.items()
+        ]
+        return out
```

### Comparing `featurebyte-1.0.2/featurebyte/models/entity_universe.py` & `featurebyte-1.0.3/featurebyte/models/entity_universe.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,47 +1,100 @@
 """
 This module contains the logic to construct the entity universe for a given node
 """
+
 from __future__ import annotations
 
-from typing import List, Optional, cast
+from typing import List, Optional, Sequence, cast
 
 from abc import abstractmethod
 from dataclasses import dataclass
 from datetime import datetime
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Select, Subqueryable, select
 
-from featurebyte.enum import SourceType
+from featurebyte.enum import InternalName, SourceType
 from featurebyte.models.base import FeatureByteBaseModel
+from featurebyte.models.item_table import ItemTableModel
 from featurebyte.models.parent_serving import EntityLookupStep
+from featurebyte.models.proxy_table import TableModel
 from featurebyte.models.sqlglot_expression import SqlglotExpressionModel
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
-from featurebyte.query_graph.node.generic import AggregateAsAtNode, ItemGroupbyNode, LookupNode
+from featurebyte.query_graph.node.generic import (
+    AggregateAsAtNode,
+    GroupByNode,
+    ItemGroupbyNode,
+    LookupNode,
+)
 from featurebyte.query_graph.node.input import EventTableInputNodeParameters
 from featurebyte.query_graph.node.nested import ItemViewGraphNodeParameters
+from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.builder import SQLOperationGraph
 from featurebyte.query_graph.sql.common import (
     EventTableTimestampFilter,
     SQLType,
     get_fully_qualified_table_name,
     get_qualified_column_identifier,
     quoted_identifier,
 )
+from featurebyte.query_graph.sql.online_serving_util import get_online_store_table_name
+from featurebyte.query_graph.sql.specs import TileBasedAggregationSpec
 from featurebyte.query_graph.sql.template import SqlExpressionTemplate
 from featurebyte.query_graph.transform.flattening import GraphFlatteningTransformer
 
 CURRENT_FEATURE_TIMESTAMP_PLACEHOLDER = "__fb_current_feature_timestamp"
 LAST_MATERIALIZED_TIMESTAMP_PLACEHOLDER = "__fb_last_materialized_timestamp"
 
 
+def columns_not_null(columns: Sequence[str]) -> Expression:
+    """
+    Returns an expression for a boolean condition that evaluates to true if none of the columns are
+    null. To be used to filter out rows with missing entity values in the entity universe.
+
+    Parameters
+    ----------
+    columns: List[str]
+        List of column names to check
+
+    Returns
+    -------
+    Expression
+    """
+    return expressions.and_(
+        *[
+            expressions.Is(
+                this=quoted_identifier(column),
+                expression=expressions.Not(this=expressions.Null()),
+            )
+            for column in columns
+        ]
+    )
+
+
+def get_dummy_entity_universe() -> Select:
+    """
+    Returns a dummy entity universe (actual value not important since it doesn't affect features
+    calculation)
+
+    Returns
+    -------
+    Select
+    """
+    return expressions.select(
+        expressions.alias_(make_literal_value(1), "dummy_entity", quoted=True)
+    )
+
+
+DUMMY_ENTITY_UNIVERSE = get_dummy_entity_universe()
+
+
 @dataclass
 class EntityUniverseParams:
     """
     Parameters for each entity universe to be constructed
     """
 
     graph: QueryGraphModel
@@ -71,19 +124,20 @@
             event_table_timestamp_filter=self.get_event_table_timestamp_filter(
                 graph=graph,
                 node=node,
             ),
         )
         sql_node = sql_graph.build(self.node)
         self.aggregate_input_expr = sql_node.sql
+        self.adapter = get_sql_adapter(source_type)
 
     @abstractmethod
-    def get_entity_universe_template(self) -> Expression:
+    def get_entity_universe_template(self) -> List[Expression]:
         """
-        Returns a SQL expression for the universe of the entity with placeholders for current
+        Returns SQL expressions for the universe of the entity with placeholders for current
         feature timestamp and last materialization timestamp
         """
 
     @abstractmethod
     def get_serving_names(self) -> List[str]:
         """
         Return list of serving names
@@ -118,15 +172,15 @@
     Construct the entity universe expression for lookup node
     """
 
     def get_serving_names(self) -> List[str]:
         node = cast(LookupNode, self.node)
         return [node.parameters.serving_name]
 
-    def get_entity_universe_template(self) -> Expression:
+    def get_entity_universe_template(self) -> List[Expression]:
         node = cast(LookupNode, self.node)
 
         if node.parameters.scd_parameters is not None:
             ts_col = node.parameters.scd_parameters.effective_timestamp_column
         elif node.parameters.event_parameters is not None:
             ts_col = node.parameters.event_parameters.event_timestamp_column
         else:
@@ -154,32 +208,33 @@
                     quoted_identifier(node.parameters.entity_column),
                     alias=node.parameters.serving_name,
                     quoted=True,
                 )
             )
             .distinct()
             .from_(aggregate_input_expr.subquery())
+            .where(columns_not_null([node.parameters.entity_column]))
         )
-        return universe_expr
+        return [universe_expr]
 
 
 class AggregateAsAtNodeEntityUniverseConstructor(BaseEntityUniverseConstructor):
     """
     Construct the entity universe expression for aggregate as at node
     """
 
     def get_serving_names(self) -> List[str]:
         node = cast(AggregateAsAtNode, self.node)
         return node.parameters.serving_names
 
-    def get_entity_universe_template(self) -> Expression:
+    def get_entity_universe_template(self) -> List[Expression]:
         node = cast(AggregateAsAtNode, self.node)
 
         if not node.parameters.serving_names:
-            return get_dummy_entity_universe()
+            return [DUMMY_ENTITY_UNIVERSE]
 
         ts_col = node.parameters.effective_timestamp_column
         filtered_aggregate_input_expr = self.aggregate_input_expr.where(
             expressions.and_(
                 expressions.GTE(
                     this=quoted_identifier(ts_col),
                     expression=LAST_MATERIALIZED_TIMESTAMP_PLACEHOLDER,
@@ -196,16 +251,17 @@
                     for key, serving_name in zip(
                         node.parameters.keys, node.parameters.serving_names
                     )
                 ]
             )
             .distinct()
             .from_(filtered_aggregate_input_expr.subquery())
+            .where(columns_not_null(node.parameters.keys))
         )
-        return universe_expr
+        return [universe_expr]
 
 
 class ItemAggregateNodeEntityUniverseConstructor(BaseEntityUniverseConstructor):
     """
     Construct the entity universe expression for item aggregate node
     """
 
@@ -245,43 +301,80 @@
             event_table_id=event_table_id,
             start_timestamp_placeholder_name=LAST_MATERIALIZED_TIMESTAMP_PLACEHOLDER,
             end_timestamp_placeholder_name=CURRENT_FEATURE_TIMESTAMP_PLACEHOLDER,
             to_cast_placeholders=False,
         )
         return event_table_timestamp_filter
 
-    def get_entity_universe_template(self) -> Expression:
+    def get_entity_universe_template(self) -> List[Expression]:
         node = cast(ItemGroupbyNode, self.node)
         universe_expr = (
             select(
                 *[
                     expressions.alias_(quoted_identifier(key), alias=serving_name, quoted=True)
                     for key, serving_name in zip(
                         node.parameters.keys, node.parameters.serving_names
                     )
                 ]
             )
             .distinct()
             .from_(self.aggregate_input_expr.subquery())
+            .where(columns_not_null(node.parameters.keys))
         )
-        return universe_expr
+        return [universe_expr]
 
 
-def get_dummy_entity_universe() -> Select:
+class TileBasedAggregateNodeEntityUniverseConstructor(BaseEntityUniverseConstructor):
     """
-    Returns a dummy entity universe (actual value not important since it doesn't affect features
-    calculation)
-
-    Returns
-    -------
-    Select
+    Construct the entity universe expression for tile based aggregate node
     """
-    return expressions.select(
-        expressions.alias_(make_literal_value(1), "dummy_entity", quoted=True)
-    )
+
+    def get_serving_names(self) -> List[str]:
+        node = cast(GroupByNode, self.node)
+        return node.parameters.serving_names
+
+    def get_entity_universe_template(self) -> List[Expression]:
+        node = cast(GroupByNode, self.node)
+
+        if not node.parameters.serving_names:
+            return [DUMMY_ENTITY_UNIVERSE]
+
+        agg_specs = TileBasedAggregationSpec.from_groupby_query_node(
+            graph=self.graph,
+            groupby_node=node,
+            adapter=self.adapter,
+            agg_result_name_include_serving_names=True,
+        )
+        return [self._get_universe_expr_from_agg_spec(node, agg_spec) for agg_spec in agg_specs]
+
+    def _get_universe_expr_from_agg_spec(
+        self, node: GroupByNode, agg_spec: TileBasedAggregationSpec
+    ) -> Expression:
+        result_type = self.adapter.get_physical_type_from_dtype(agg_spec.dtype)
+        online_store_table_name = get_online_store_table_name(
+            set(agg_spec.entity_ids if agg_spec.entity_ids is not None else []),
+            result_type=result_type,
+        )
+        online_store_table_condition = expressions.EQ(
+            this=quoted_identifier(InternalName.ONLINE_STORE_RESULT_NAME_COLUMN),
+            expression=make_literal_value(agg_spec.agg_result_name),
+        )
+        universe_expr = (
+            select(
+                *[quoted_identifier(serving_name) for serving_name in node.parameters.serving_names]
+            )
+            .distinct()
+            .from_(expressions.Table(this=online_store_table_name))
+            .where(
+                expressions.and_(
+                    online_store_table_condition, columns_not_null(node.parameters.serving_names)
+                )
+            )
+        )
+        return universe_expr
 
 
 def get_entity_universe_constructor(
     graph: QueryGraphModel, node: Node, source_type: SourceType
 ) -> BaseEntityUniverseConstructor:
     """
     Returns the entity universe constructor for the given node
@@ -304,14 +397,15 @@
     NotImplementedError
         If the node type is not supported
     """
     node_type_to_constructor = {
         NodeType.LOOKUP: LookupNodeEntityUniverseConstructor,
         NodeType.AGGREGATE_AS_AT: AggregateAsAtNodeEntityUniverseConstructor,
         NodeType.ITEM_GROUPBY: ItemAggregateNodeEntityUniverseConstructor,
+        NodeType.GROUPBY: TileBasedAggregateNodeEntityUniverseConstructor,
     }
     if node.type in node_type_to_constructor:
         return node_type_to_constructor[node.type](graph, node, source_type)  # type: ignore
     raise NotImplementedError(f"Unsupported node type: {node.type}")
 
 
 def _apply_join_step(universe_expr: Expression, join_step: EntityLookupStep) -> Expression:
@@ -363,89 +457,124 @@
         universe_expr = _apply_join_step(universe_expr, join_step)
     return universe_expr
 
 
 def get_combined_universe(
     entity_universe_params: List[EntityUniverseParams],
     source_type: SourceType,
-) -> Expression:
+) -> Optional[Expression]:
     """
     Returns the combined entity universe expression
 
     Parameters
     ----------
     entity_universe_params: List[EntityUniverseParams]
         Parameters of the entity universe to be constructed
     source_type: SourceType
         Source type information
 
     Returns
     -------
-    Expression
+    Optional[Expression]
     """
     combined_universe_expr: Optional[Expression] = None
     processed_universe_exprs = set()
+    has_dummy_entity_universe = False
 
     for params in entity_universe_params:
         entity_universe_constructor = get_entity_universe_constructor(
             params.graph, params.node, source_type
         )
-        current_universe_expr = entity_universe_constructor.get_entity_universe_template()
-        if params.join_steps:
-            current_universe_expr = apply_join_steps(current_universe_expr, params.join_steps[::-1])
-        if combined_universe_expr is None:
-            combined_universe_expr = current_universe_expr
-        elif current_universe_expr not in processed_universe_exprs:
-            combined_universe_expr = expressions.Union(
-                this=current_universe_expr,
-                distinct=True,
-                expression=combined_universe_expr,
-            )
-        processed_universe_exprs.add(current_universe_expr)
+        for current_universe_expr in entity_universe_constructor.get_entity_universe_template():
+            if current_universe_expr == DUMMY_ENTITY_UNIVERSE:
+                # Add dummy entity universe later after going through all other universes
+                has_dummy_entity_universe = True
+                continue
+            if params.join_steps:
+                current_universe_expr = apply_join_steps(
+                    current_universe_expr, params.join_steps[::-1]
+                )
+            if combined_universe_expr is None:
+                combined_universe_expr = current_universe_expr
+            elif current_universe_expr not in processed_universe_exprs:
+                combined_universe_expr = expressions.Union(
+                    this=current_universe_expr,
+                    distinct=True,
+                    expression=combined_universe_expr,
+                )
+            processed_universe_exprs.add(current_universe_expr)
+
+    if has_dummy_entity_universe and combined_universe_expr is None:
+        # Construct dummy entity universe only when there is no other universes to union with. This
+        # is to handle the case when a feature is made up of window aggregates with and without
+        # entity (such ingest graph is not decomposed). When that happens, the dummy universe is
+        # ignored.
+        combined_universe_expr = DUMMY_ENTITY_UNIVERSE
 
-    assert combined_universe_expr is not None
     return combined_universe_expr
 
 
-def construct_window_aggregates_universe(
-    serving_names: List[str],
-    aggregate_result_table_names: List[str],
-) -> Expression:
+def get_item_relation_table_lookup_universe(item_table_model: TableModel) -> expressions.Select:
     """
-    Construct the entity universe expression for window aggregate
+    Get the entity universe for a relation table that is an ItemTable. This is used when looking up
+    a parent entity using a child entity (item id column) in an ItemTable.
 
     Parameters
     ----------
-    serving_names: List[str]
-        The serving names of the entities
-    aggregate_result_table_names: List[str]
-        The names of the aggregate result tables
+    item_table_model: TableModel
+        Item table model
 
     Returns
     -------
-    Expression
+    expressions.Select
     """
-    if not serving_names:
-        return get_dummy_entity_universe()
-
-    assert len(aggregate_result_table_names) > 0
-
-    # select distinct serving names across all aggregation result tables
-    distinct_serving_names_from_tables = [
-        select(*[quoted_identifier(serving_name) for serving_name in serving_names])
+    assert isinstance(item_table_model, ItemTableModel)
+    event_table_model = item_table_model.event_table_model
+    assert event_table_model is not None
+    filtered_event_table_expr = (
+        expressions.select(quoted_identifier(event_table_model.event_id_column))
+        .from_(
+            get_fully_qualified_table_name((event_table_model.tabular_source.table_details.dict()))
+        )
+        .where(
+            expressions.and_(
+                expressions.GTE(
+                    this=quoted_identifier(event_table_model.event_timestamp_column),
+                    expression=LAST_MATERIALIZED_TIMESTAMP_PLACEHOLDER,
+                ),
+                expressions.LT(
+                    this=quoted_identifier(event_table_model.event_timestamp_column),
+                    expression=CURRENT_FEATURE_TIMESTAMP_PLACEHOLDER,
+                ),
+            )
+        )
+    )
+    universe = (
+        expressions.select(quoted_identifier(item_table_model.item_id_column))
         .distinct()
-        .from_(table_name)
-        for table_name in aggregate_result_table_names
-    ]
-
-    union_expr = distinct_serving_names_from_tables[0]
-    for expr in distinct_serving_names_from_tables[1:]:
-        union_expr = expressions.Union(this=expr, distinct=True, expression=union_expr)  # type: ignore
-
-    return union_expr
+        .from_(
+            expressions.Table(
+                this=get_fully_qualified_table_name(
+                    item_table_model.tabular_source.table_details.dict()
+                ),
+                alias="ITEM",
+            ),
+        )
+        .join(
+            filtered_event_table_expr.subquery(alias="EVENT"),
+            on=expressions.EQ(
+                this=get_qualified_column_identifier(item_table_model.event_id_column, "ITEM"),
+                expression=get_qualified_column_identifier(
+                    event_table_model.event_id_column, "EVENT"
+                ),
+            ),
+            join_type="INNER",
+        )
+    )
+    return universe
 
 
 class EntityUniverseModel(FeatureByteBaseModel):
     """
     EntityUniverseModel class
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/models/entity_validation.py` & `featurebyte-1.0.3/featurebyte/models/entity_validation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Models related to entity validation
 """
+
 from __future__ import annotations
 
 from typing import Dict, List, Optional
 
 from bson import ObjectId
 from pydantic import validator
```

### Comparing `featurebyte-1.0.2/featurebyte/models/event_table.py` & `featurebyte-1.0.3/featurebyte/models/event_table.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains EventTable related models
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, List, Optional, Tuple, Type
 
 from datetime import datetime
 
 from pydantic import root_validator
@@ -79,14 +80,24 @@
 
     @property
     def primary_key_columns(self) -> List[str]:
         if self.event_id_column:
             return [self.event_id_column]
         return []  # DEV-556: event_id_column should not be empty
 
+    @property
+    def special_columns(self) -> List[str]:
+        cols = [
+            self.event_timestamp_column,
+            self.event_id_column,
+            self.record_creation_timestamp_column,
+            self.event_timestamp_timezone_offset_column,
+        ]
+        return [col for col in cols if col]
+
     def create_view_graph_node(
         self, input_node: InputNode, metadata: ViewMetadata, **kwargs: Any
     ) -> Tuple[GraphNode, List[ColumnInfo]]:
         table_data = EventTableData(**self.dict(by_alias=True)).clone(
             column_cleaning_operations=metadata.column_cleaning_operations
         )
         return table_data.construct_event_view_graph_node(  # pylint: disable=no-member
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature.py` & `featurebyte-1.0.3/featurebyte/models/feature.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """
 This module contains Feature related models
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
+import traceback
 from datetime import datetime
 
 import pymongo
 from bson import ObjectId
 from pydantic import Field, root_validator, validator
 
 from featurebyte.common.validator import construct_sort_validator, version_validator
@@ -105,14 +107,20 @@
     # transaction -> order -> customer -> city -> state
     # if the feature uses order & city entities, the relationship info will be (order -> customer -> city)
     # transaction and state will not be included as they are not used by the feature.
     relationships_info: Optional[List[EntityRelationshipInfo]] = Field(
         allow_mutation=False, default=None
     )
 
+    # entity join steps contains the steps required to join the entities used by the feature or target
+    # when it is None, it means that the attribute is not initialized (for backward compatibility)
+    entity_join_steps: Optional[List[EntityRelationshipInfo]] = Field(
+        allow_mutation=False, default=None
+    )
+
     # offline store info contains the information used to construct the offline store table(s) required
     # by the feature or target.
     internal_offline_store_info: Optional[Dict[str, Any]] = Field(
         alias="offline_store_info", default=None
     )
 
     # pydantic validators
@@ -445,15 +453,15 @@
         List[AggregationNodeInfo]
         """
         operation_structure = self.graph.extract_operation_structure(self.node)
         output = []
         for agg in operation_structure.iterate_aggregations():
             node = self.graph.get_node_by_name(agg.node_name)
             input_node_names = self.graph.backward_edges_map[node.name]
-            assert len(input_node_names) <= 1
+            assert len(input_node_names) <= 1, "Aggregation node should have at most one input node"
             output.append(
                 AggregationNodeInfo(
                     node_type=node.type,
                     input_node_name=input_node_names[0] if input_node_names else None,
                     node_name=node.name,
                 )
             )
@@ -577,14 +585,19 @@
 
         try:
             tile_infos = interpreter.construct_tile_gen_sql(node, is_on_demand=False)
         except StopIteration:
             # add a try except block here for the old features that may trigger StopIteration,
             # in this case, we will not add tile related attributes
             return values
+        except Exception:
+            # print a traceback for debugging purpose
+            # without this, the error message will be swallowed by the root_validator
+            print(traceback.format_exc())
+            raise
 
         aggregation_ids = []
         for info in tile_infos:
             aggregation_ids.append(info.aggregation_id)
 
         values["aggregation_ids"] = aggregation_ids
 
@@ -598,14 +611,36 @@
         ):
             values["aggregation_result_names"].append(query.result_name)
             online_store_table_names.add(query.table_name)
         values["online_store_table_names"] = sorted(online_store_table_names)
 
         return values
 
+    @property
+    def used_request_column(self) -> bool:
+        """
+        Returns whether the Feature object uses request column(s) in the computation.
+
+        Returns
+        -------
+        bool
+        """
+        return self.graph.has_node_type(target_node=self.node, node_type=NodeType.REQUEST_COLUMN)
+
+    @property
+    def used_user_defined_function(self) -> bool:
+        """
+        Returns whether the Feature object uses user defined function(s) in the computation.
+
+        Returns
+        -------
+        bool
+        """
+        return self.graph.has_node_type(target_node=self.node, node_type=NodeType.GENERIC_FUNCTION)
+
     class Settings(BaseFeatureModel.Settings):
         """
         MongoDB settings
         """
 
         collection_name: str = "feature"
         indexes = BaseFeatureModel.Settings.indexes + [
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/models/feature_job_setting_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains FeatureJobSettingAnalysis related models
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple
 
 import datetime
 
 import pymongo
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_list.py` & `featurebyte-1.0.3/featurebyte/models/feature_list.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Feature list related models
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Set
 
 import functools
 from collections import defaultdict
 from pathlib import Path
@@ -37,14 +38,15 @@
 from featurebyte.models.parent_serving import FeatureNodeRelationshipsInfo
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.entity_relationship_info import (
     EntityRelationshipInfo,
     FeatureEntityLookupInfo,
 )
 from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.node.schema import ColumnSpec
 from featurebyte.query_graph.pruning_util import get_combined_graph_and_nodes
 
 
 class FeatureTypeFeatureCount(FeatureByteBaseModel):
     """
     Feature count corresponding to the feature type within a feature list
 
@@ -301,17 +303,14 @@
         Feature readiness distribution of this feature list
     version: VersionIdentifier
         Feature list version
     relationships_info: Optional[List[EntityRelationshipInfo]]
         List of entity relationship info for the feature list
     supported_serving_entity_ids: List[ServingEntity]
         List of supported serving entity ids, serving entity id is a list of entity ids for serving
-    enabled_serving_entity_ids: List[ServingEntity]
-        Subset of supported_serving_entity_ids. List of serving entity ids that are used by
-        currently enabled deployments
     deployed: bool
         Whether to deploy this feature list version
     feature_list_namespace_id: PydanticObjectId
         Feature list namespace id of the object
     created_at: Optional[datetime]
         Datetime when the FeatureList was first saved or published
     internal_feature_clusters: Optional[List[Any]]
@@ -326,17 +325,14 @@
     )
     features_entity_lookup_info: Optional[List[FeatureEntityLookupInfo]] = Field(
         allow_mutation=False, default=None
     )
     supported_serving_entity_ids: List[ServingEntity] = Field(
         allow_mutation=False, default_factory=list
     )
-    enabled_serving_entity_ids: List[ServingEntity] = Field(
-        allow_mutation=False, default_factory=list
-    )
     readiness_distribution: FeatureReadinessDistribution = Field(
         allow_mutation=False, default_factory=list
     )
     dtype_distribution: List[FeatureTypeFeatureCount] = Field(
         allow_mutation=False, default_factory=list
     )
     deployed: bool = Field(allow_mutation=False, default=False)
@@ -372,15 +368,15 @@
         "primary_entity_ids",
         "entity_ids",
         "table_ids",
         allow_reuse=True,
     )(construct_sort_validator())
     _version_validator = validator("version", pre=True, allow_reuse=True)(version_validator)
 
-    @validator("supported_serving_entity_ids", "enabled_serving_entity_ids")
+    @validator("supported_serving_entity_ids")
     @classmethod
     def _validate_supported_serving_entity_ids(
         cls, value: List[ServingEntity]
     ) -> List[ServingEntity]:
         return [
             sorted(set(serving_entity))
             for serving_entity in sorted(value, key=lambda e: (len(e), e))
@@ -549,54 +545,69 @@
             return None
         if self._feature_clusters is None:
             self._feature_clusters = [
                 FeatureCluster(**cluster) for cluster in self.internal_feature_clusters
             ]
         return self._feature_clusters
 
-    @property
-    def remote_attribute_paths(self) -> List[Path]:
+    @classmethod
+    def _get_remote_attribute_paths(cls, document_dict: Dict[str, Any]) -> List[Path]:
         paths = []
-        if self.feature_clusters_path:
-            paths.append(Path(self.feature_clusters_path))
+        feature_clusters_path = document_dict.get("feature_clusters_path")
+        if feature_clusters_path:
+            paths.append(Path(feature_clusters_path))
         return paths
 
     @property
     def store_info(self) -> StoreInfo:
         """
         Store info for a feature list
 
         Returns
         -------
         StoreInfo
         """
         return parse_obj_as(StoreInfo, self.internal_store_info or {"type": "uninitialized"})  # type: ignore
 
     def initialize_store_info(
-        self, features: List[FeatureModel], feature_store: FeatureStoreModel
+        self,
+        features: List[FeatureModel],
+        feature_store: FeatureStoreModel,
+        feature_table_map: Dict[str, Any],
+        serving_entity_specs: Optional[List[ColumnSpec]],
     ) -> None:
         """
         Initialize store info for a feature list
 
         Parameters
         ----------
         features: List[FeatureModel]
             List of features
         feature_store: FeatureStoreModel
             Feature store model
+        feature_table_map: Dict[str, Any]
+            List of offline feature table map from source table name to actual feature table
+            (could be a source table or a precomputed lookup table)
+        serving_entity_specs: Optional[List[ColumnSpec]]
+            List of serving entity specs
         """
         store_type_to_store_info_class = {
             SourceType.SNOWFLAKE: SnowflakeStoreInfo,
             SourceType.DATABRICKS: DataBricksStoreInfo,
             SourceType.DATABRICKS_UNITY: DataBricksUnityStoreInfo,
             SourceType.SPARK: SparkStoreInfo,
         }
         if feature_store.type in store_type_to_store_info_class:
             store_info_class = store_type_to_store_info_class[feature_store.type]
-            store_info = store_info_class.create(features=features, feature_store=feature_store)
+            store_info = store_info_class.create(
+                features=features,
+                feature_store=feature_store,
+                feature_table_map=feature_table_map,
+                serving_entity_specs=serving_entity_specs,
+            )
             self.internal_store_info = store_info.dict(by_alias=True)
 
     class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_list_namespace.py` & `featurebyte-1.0.3/featurebyte/models/feature_list_namespace.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Feature list namespace related models
 """
+
 from __future__ import annotations
 
 from typing import Any, List
 
 import pymongo
 from pydantic import Field, root_validator, validator
 
@@ -56,15 +57,15 @@
     deployed_feature_list_ids: List[PydanticObjectId] = Field(
         allow_mutation=False, default_factory=list
     )
     default_feature_list_id: PydanticObjectId = Field(allow_mutation=False)
     status: FeatureListStatus = Field(allow_mutation=False, default=FeatureListStatus.DRAFT)
 
     # pydantic validators
-    _sort_feature_list_ids_validator = validator(
+    _sort_ids_validator = validator(
         "feature_list_ids", "feature_namespace_ids", "deployed_feature_list_ids", allow_reuse=True
     )(construct_sort_validator())
 
     @root_validator(pre=True)
     @classmethod
     def _derive_feature_related_attributes(cls, values: dict[str, Any]) -> dict[str, Any]:
         # "features" is not an attribute to the FeatureList model, when it appears in the input to
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_list_store_info.py` & `featurebyte-1.0.3/featurebyte/models/feature_list_store_info.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 """
 This module contains Feature list store info related models
 """
+
 from __future__ import annotations
 
-from typing import Dict, List, Literal, Optional, Tuple, Union
+from typing import Any, Dict, List, Literal, Optional, Set, Tuple, Union
 from typing_extensions import Annotated
 
 from pydantic import Field
 
 from featurebyte.enum import DBVarType, SpecialColumnName
-from featurebyte.models.base import FeatureByteBaseModel
+from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.metadata.sdk_code import (
     CodeGenerator,
     ExpressionStr,
     VariableNameStr,
@@ -36,44 +37,56 @@
     """
 
     type: StoreInfoType
     feast_enabled: bool = Field(default=False)
 
     @classmethod
     def create(
-        cls, features: List[FeatureModel], feature_store: FeatureStoreModel
+        cls,
+        features: List[FeatureModel],
+        feature_store: FeatureStoreModel,
+        feature_table_map: Dict[str, Any],
+        serving_entity_specs: Optional[List[ColumnSpec]],
     ) -> BaseStoreInfo:
         """
         Create store info for a feature list
 
         Parameters
         ----------
         features: List[FeatureModel]
             List of features
         feature_store: FeatureStoreModel
             Feature store model used by the features & feature list
+        feature_table_map: Dict[str, Any]
+            Feature table map
+        serving_entity_specs: Optional[List[ColumnSpec]]
+            Serving entity specs
 
         Returns
         -------
         StoreInfo
         """
-        _ = features, feature_store
+        _ = features, feature_store, feature_table_map, serving_entity_specs
         return cls(feast_enabled=True)
 
 
 class UninitializedStoreInfo(BaseStoreInfo):
     """
     Uninitialized store info
     """
 
     type: Literal["uninitialized"] = Field("uninitialized", const=True)
 
     @classmethod
     def create(
-        cls, features: List[FeatureModel], feature_store: FeatureStoreModel
+        cls,
+        features: List[FeatureModel],
+        feature_store: FeatureStoreModel,
+        feature_table_map: Dict[str, Any],
+        serving_entity_specs: Optional[List[ColumnSpec]],
     ) -> UninitializedStoreInfo:
         return cls(feast_enabled=False)
 
 
 class SnowflakeStoreInfo(BaseStoreInfo):
     """
     Snowflake store info
@@ -175,136 +188,90 @@
             DBVarType.BOOL: "BooleanType",
             DBVarType.TIMESTAMP: "TimestampType",
         }
         if dtype not in type_map:
             raise ValueError(f"Unsupported dtype: {dtype}")
         return type_map[dtype]
 
-    def _get_base_dataframe_schema_and_import_statement(self) -> Tuple[str, str]:
+    def _get_base_dataframe_schema_and_import_statement(
+        self, target_spec: ColumnSpec
+    ) -> Tuple[str, str]:
         schemas = []
         required_imports = {"StructType", "StructField"}
-        for column_spec in self.base_dataframe_specs:
+        base_dataframe_specs = [target_spec]
+        base_dataframe_specs.extend(self.base_dataframe_specs)
+        for column_spec in base_dataframe_specs:
             field_type = self.to_spark_dtype(column_spec.dtype)
+            column_name = column_spec.name
+            if column_spec.name == target_spec.name:
+                column_name = VariableNameStr("target_column")
             schemas.append(
                 get_object_class_from_function_call(
                     "StructField",
-                    column_spec.name,
+                    column_name,
                     ExpressionStr(f"{field_type}()"),
                 )
             )
             required_imports.add(field_type)
 
         schema_statement = f"StructType({repr(schemas)})"
         import_classes = ", ".join(sorted(required_imports))
         pyspark_import_statement = f"from pyspark.sql.types import {import_classes}"
         return schema_statement, pyspark_import_statement
 
-    @property
-    def feature_specs_definition(self) -> str:
+    def get_feature_specs_definition(
+        self, target_spec: Optional[ColumnSpec], include_log_model: bool = True
+    ) -> str:
         """
-        Feature specs definition
+        Get Feature specs definition for DataBricks
+
+        Parameters
+        ----------
+        target_spec: Optional[ColumnSpec]
+            Target column spec
+        include_log_model: bool
+            Whether to include log model statement in the generated code
 
         Returns
         -------
         str
         """
         code_gen = CodeGenerator(template="databricks_feature_spec.tpl")
         feature_specs = ExpressionStr(self.feature_specs)
+        target_spec = target_spec or ColumnSpec(name="[TARGET_COLUMN]", dtype=DBVarType.FLOAT)
         (
             base_dataframe_schema,
             pyspark_import_statement,
-        ) = self._get_base_dataframe_schema_and_import_statement()
+        ) = self._get_base_dataframe_schema_and_import_statement(target_spec=target_spec)
         codes = code_gen.generate(
             to_format=True,
             remove_unused_variables=False,
             databricks_sdk_version=self.databricks_sdk_version,
             pyspark_import_statement=pyspark_import_statement,
             features=feature_specs,
             exclude_columns=self.exclude_columns,
             require_timestamp_lookup_key=self.require_timestamp_lookup_key,
             schema=base_dataframe_schema,
+            target_column=target_spec.name,
+            include_log_model=include_log_model,
         )
         return codes
 
     @staticmethod
     def _get_fully_qualified_schema_name(feature_store: FeatureStoreModel) -> str:
         feature_store_details = feature_store.details
         assert isinstance(feature_store_details, (DatabricksDetails, DatabricksUnityDetails))
         catalog_name = feature_store_details.catalog_name
         schema_name = feature_store_details.schema_name
         return f"{catalog_name}.{schema_name}"
 
-    @staticmethod
-    def _create_feature_functions(
-        features: List[FeatureModel], schema_name: str
-    ) -> List[DataBricksFeatureFunction]:
-        feature_functions = []
-        for feature in features:
-            offline_store_info = feature.offline_store_info
-            if offline_store_info.udf_info:
-                input_bindings = {
-                    sql_input_info.sql_input_var_name: sql_input_info.column_name
-                    for sql_input_info in offline_store_info.udf_info.sql_inputs_info
-                }
-                feature_functions.append(
-                    DataBricksFeatureFunction(
-                        udf_name=f"{schema_name}.{offline_store_info.udf_info.sql_function_name}",
-                        input_bindings=input_bindings,
-                        output_name=feature.name,
-                    )
-                )
-        return feature_functions
-
-    @staticmethod
-    def _create_feature_lookups(
-        features: List[FeatureModel], schema_name: str
-    ) -> List[DataBricksFeatureLookup]:
-        feature_lookups = []
-        table_name_to_feature_lookup = {}
-        for feature in features:
-            offline_store_info = feature.offline_store_info
-            entity_id_to_serving_name = {
-                info.entity_id: info.serving_name for info in offline_store_info.serving_names_info
-            }
-            for ingest_query in offline_store_info.extract_offline_store_ingest_query_graphs():
-                table_name = ingest_query.offline_store_table_name
-                timestamp_lookup_key = VariableNameStr("timestamp_lookup_key")
-
-                if table_name not in table_name_to_feature_lookup:
-                    if len(ingest_query.primary_entity_ids) > 0:
-                        lookup_key = [
-                            entity_id_to_serving_name[entity_id]
-                            for entity_id in ingest_query.primary_entity_ids
-                        ]
-                    else:
-                        lookup_key = [DUMMY_ENTITY_COLUMN_NAME]
-                    table_name_to_feature_lookup[table_name] = DataBricksFeatureLookup(
-                        table_name=f"{schema_name}.{ingest_query.offline_store_table_name}",
-                        lookup_key=lookup_key,
-                        timestamp_lookup_key=timestamp_lookup_key,
-                        lookback_window=None,
-                        feature_names=[],
-                        rename_outputs={},
-                    )
-
-                column_name = ingest_query.output_column_name
-                feature_lookup = table_name_to_feature_lookup[table_name]
-                feature_lookup.feature_names.append(column_name)
-                if not offline_store_info.udf_info:
-                    assert feature.name is not None, "Feature does not have a name"
-                    feature_lookup.rename_outputs[column_name] = feature.name
-
-        for feature_lookup in table_name_to_feature_lookup.values():
-            feature_lookups.append(feature_lookup)
-        return feature_lookups
-
     @classmethod
-    def create(
-        cls, features: List[FeatureModel], feature_store: FeatureStoreModel
-    ) -> DataBricksUnityStoreInfo:
+    def _derive_entity_request_column_info_from_features(
+        cls, features: List[FeatureModel]
+    ) -> Tuple[Dict[PydanticObjectId, ColumnSpec], Dict[str, DBVarType], Set[str]]:
         exclude_columns = {SpecialColumnName.POINT_IN_TIME.value}
         entity_id_to_column_spec = {}
         request_column_name_to_dtype = {}
         for feature in features:
             offline_store_info = feature.offline_store_info
             entity_id_to_serving_name = {
                 info.entity_id: info.serving_name for info in offline_store_info.serving_names_info
@@ -327,48 +294,164 @@
                     ),
                     node_type=NodeType.REQUEST_COLUMN,
                 ):
                     assert isinstance(node, RequestColumnNode)
                     node_params = node.parameters
                     if node_params.column_name not in request_column_name_to_dtype:
                         request_column_name_to_dtype[node_params.column_name] = node_params.dtype
+        return entity_id_to_column_spec, request_column_name_to_dtype, exclude_columns
 
+    @classmethod
+    def _derive_feature_specs(
+        cls,
+        feature: FeatureModel,
+        feature_table_map: Dict[str, Any],
+        schema_name: str,
+        output_column_names: Set[str],
+    ) -> List[Union[DataBricksFeatureLookup, DataBricksFeatureFunction]]:
+        output: List[Union[DataBricksFeatureLookup, DataBricksFeatureFunction]] = []
+        table_name_to_feature_lookup = {}
+        offline_store_info = feature.offline_store_info
+        timestamp_lookup_key = VariableNameStr("timestamp_lookup_key")
+        for ingest_query in offline_store_info.extract_offline_store_ingest_query_graphs():
+            table = feature_table_map[ingest_query.offline_store_table_name]
+            if table.name not in table_name_to_feature_lookup:
+                lookup_key = table.serving_names
+                if not lookup_key:
+                    lookup_key = [DUMMY_ENTITY_COLUMN_NAME]
+                table_name_to_feature_lookup[table.name] = DataBricksFeatureLookup(
+                    table_name=f"{schema_name}.{table.name}",
+                    lookup_key=lookup_key,
+                    timestamp_lookup_key=timestamp_lookup_key,
+                    lookback_window=None,
+                    feature_names=[],
+                    rename_outputs={},
+                )
+
+            column_name = ingest_query.output_column_name
+            feature_lookup = table_name_to_feature_lookup[table.name]
+            output_column_name = column_name
+            if not offline_store_info.udf_info:
+                assert feature.name is not None, "Feature does not have a name"
+                output_column_name = feature.name
+
+            if output_column_name not in output_column_names:
+                output_column_names.add(output_column_name)
+                feature_lookup.feature_names.append(column_name)
+                if not offline_store_info.udf_info:
+                    feature_lookup.rename_outputs[column_name] = output_column_name
+
+        for feature_lookup in table_name_to_feature_lookup.values():
+            if feature_lookup.feature_names:
+                output.append(feature_lookup)
+
+        if offline_store_info.udf_info:
+            input_bindings = {
+                sql_input_info.sql_input_var_name: sql_input_info.column_name
+                for sql_input_info in offline_store_info.udf_info.sql_inputs_info
+            }
+            if feature.name not in output_column_names:
+                output.append(
+                    DataBricksFeatureFunction(
+                        udf_name=f"{schema_name}.{offline_store_info.udf_info.sql_function_name}",
+                        input_bindings=input_bindings,
+                        output_name=feature.name,
+                    )
+                )
+                assert feature.name is not None, "Feature does not have a name"
+                output_column_names.add(feature.name)
+        return output
+
+    @classmethod
+    def _derive_base_dataframe_and_feature_specs(
+        cls,
+        entity_id_to_column_spec: Dict[PydanticObjectId, ColumnSpec],
+        request_column_name_to_dtype: Dict[str, DBVarType],
+        features: List[FeatureModel],
+        feature_store: FeatureStoreModel,
+        feature_table_map: Dict[str, Any],
+        entity_serving_specs: Optional[List[ColumnSpec]],
+    ) -> Tuple[
+        List[ColumnSpec], List[Union[DataBricksFeatureLookup, DataBricksFeatureFunction]], Set[str]
+    ]:
+        exclude_columns: Set[str] = set()
+        output_column_names: Set[str] = set()
         fully_qualified_schema_name = cls._get_fully_qualified_schema_name(feature_store)
-        feature_specs: List[
-            Union[DataBricksFeatureLookup, DataBricksFeatureFunction]
-        ] = cls._create_feature_lookups(
-            features, fully_qualified_schema_name
-        ) + cls._create_feature_functions(
-            features, fully_qualified_schema_name
-        )
+        feature_specs = []
+        for feature in features:
+            feature_specs.extend(
+                cls._derive_feature_specs(
+                    feature=feature,
+                    feature_table_map=feature_table_map,
+                    schema_name=fully_qualified_schema_name,
+                    output_column_names=output_column_names,
+                )
+            )
+
+        base_dataframe_specs = []
+        for feature_spec in feature_specs:
+            if isinstance(feature_spec, DataBricksFeatureLookup) and feature_spec.lookup_key == [
+                DUMMY_ENTITY_COLUMN_NAME
+            ]:
+                base_dataframe_specs.append(
+                    ColumnSpec(name=DUMMY_ENTITY_COLUMN_NAME, dtype=DBVarType.VARCHAR)
+                )
+                exclude_columns.add(DUMMY_ENTITY_COLUMN_NAME)
+                break
+
+        if entity_serving_specs:
+            for column_spec in entity_serving_specs:
+                base_dataframe_specs.append(column_spec)
+                exclude_columns.add(column_spec.name)
+        else:
+            for entity_id in sorted(entity_id_to_column_spec.keys()):
+                base_dataframe_specs.append(entity_id_to_column_spec[entity_id])
+                exclude_columns.add(entity_id_to_column_spec[entity_id].name)
 
-        base_dataframe_specs = [
-            ColumnSpec(name="[TARGET_COLUMN]", dtype=DBVarType.FLOAT),
-        ]
         has_point_in_time = False
-        for entity_id in sorted(entity_id_to_column_spec.keys()):
-            base_dataframe_specs.append(entity_id_to_column_spec[entity_id])
-            exclude_columns.add(entity_id_to_column_spec[entity_id].name)
         for column_name in sorted(request_column_name_to_dtype.keys()):
             base_dataframe_specs.append(
                 ColumnSpec(name=column_name, dtype=request_column_name_to_dtype[column_name])
             )
             if column_name == SpecialColumnName.POINT_IN_TIME:
                 has_point_in_time = True
 
         if not has_point_in_time:
             base_dataframe_specs.append(
                 ColumnSpec(name=SpecialColumnName.POINT_IN_TIME.value, dtype=DBVarType.TIMESTAMP)
             )
 
+        return base_dataframe_specs, feature_specs, exclude_columns
+
+    @classmethod
+    def create(
+        cls,
+        features: List[FeatureModel],
+        feature_store: FeatureStoreModel,
+        feature_table_map: Dict[str, Any],
+        serving_entity_specs: Optional[List[ColumnSpec]],
+    ) -> DataBricksUnityStoreInfo:
+        entity_id_to_column_spec, request_column_name_to_dtype, exclude_cols = (
+            cls._derive_entity_request_column_info_from_features(features)
+        )
+        base_dataframe_specs, feature_specs, more_exclude_cols = (
+            cls._derive_base_dataframe_and_feature_specs(
+                entity_id_to_column_spec=entity_id_to_column_spec,
+                request_column_name_to_dtype=request_column_name_to_dtype,
+                features=features,
+                feature_store=feature_store,
+                feature_table_map=feature_table_map,
+                entity_serving_specs=serving_entity_specs,
+            )
+        )
         return cls(
             feast_enabled=True,
             feature_specs=feature_specs,
             base_dataframe_specs=base_dataframe_specs,
-            exclude_columns=sorted(exclude_columns),
+            exclude_columns=sorted(exclude_cols | more_exclude_cols),
             require_timestamp_lookup_key=True,
         )
 
 
 StoreInfo = Annotated[
     Union[
         UninitializedStoreInfo,
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_namespace.py` & `featurebyte-1.0.3/featurebyte/models/feature_namespace.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains FeatureNamespace related models
 """
+
 from __future__ import annotations
 
 from typing import List
 
 import pymongo
 from pydantic import Field, validator
 
@@ -117,17 +118,17 @@
     default_feature_id: PydanticObjectId = Field(allow_mutation=False)
     online_enabled_feature_ids: List[PydanticObjectId] = Field(
         allow_mutation=False, default_factory=list
     )
     table_ids: List[PydanticObjectId] = Field(allow_mutation=False)
 
     # pydantic validators
-    _sort_feature_ids_validator = validator(
-        "feature_ids", "entity_ids", "table_ids", allow_reuse=True
-    )(construct_sort_validator())
+    _sort_ids_validator = validator("feature_ids", "entity_ids", "table_ids", allow_reuse=True)(
+        construct_sort_validator()
+    )
 
     class Settings(BaseFeatureNamespaceModel.Settings):
         """
         MongoDB settings
         """
 
         collection_name: str = "feature_namespace"
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_query_set.py` & `featurebyte-1.0.3/featurebyte/models/feature_query_set.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 FeatureQuerySet related classes
 """
+
 from __future__ import annotations
 
-from typing import Union
+from typing import Optional, Union
 
 from dataclasses import dataclass
 
 from sqlglot.expressions import Expression
 
 
 @dataclass
@@ -26,8 +27,10 @@
     """
     HistoricalFeatureQuerySet is a collection of FeatureQuery that materializes intermediate feature
     tables and a final query that joins them into one.
     """
 
     feature_queries: list[FeatureQuery]
     output_query: Union[str, Expression]
+    output_table_name: Optional[str]
     progress_message: str
+    validate_output_row_index: bool = False
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_store.py` & `featurebyte-1.0.3/featurebyte/models/feature_store.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 """
 This module contains DatabaseSource related models
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, Dict, List, Optional, Tuple, Type
 
 from abc import ABC, abstractmethod
 
 import pymongo
 from pydantic import Field, StrictStr
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.enum import OrderedStrEnum
 from featurebyte.models.base import (
     FeatureByteBaseDocumentModel,
     FeatureByteCatalogBaseDocumentModel,
+    NameStr,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.graph_node.base import GraphNode
 from featurebyte.query_graph.model.column_info import ColumnInfo
@@ -27,15 +29,15 @@
 from featurebyte.query_graph.node.input import InputNode
 from featurebyte.query_graph.node.schema import FeatureStoreDetails
 
 
 class FeatureStoreModel(FeatureByteBaseDocumentModel, FeatureStoreDetails):
     """Model for a feature store"""
 
-    name: StrictStr
+    name: NameStr
 
     def get_feature_store_details(self) -> FeatureStoreDetails:
         """
         Get feature store details
 
         Returns
         -------
@@ -184,14 +186,25 @@
         Primary key column names
 
         Returns
         -------
         List[str]
         """
 
+    @property
+    @abstractmethod
+    def special_columns(self) -> List[str]:
+        """
+        Special columns is a list of columns that have special meaning in the table
+
+        Returns
+        -------
+        List[str]
+        """
+
     @abstractmethod
     def create_view_graph_node(
         self, input_node: InputNode, metadata: Any, **kwargs: Any
     ) -> Tuple[GraphNode, List[ColumnInfo]]:
         """
         Create view graph node
```

### Comparing `featurebyte-1.0.2/featurebyte/models/feature_table_cache_metadata.py` & `featurebyte-1.0.3/featurebyte/models/feature_table_cache_metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Feature Table Cache related models
 """
+
 from typing import Any, Dict, List, Optional
 
 import pymongo
 from pydantic import Field, root_validator
 
 from featurebyte.models.base import (
     FeatureByteBaseModel,
```

### Comparing `featurebyte-1.0.2/featurebyte/models/historical_feature_table.py` & `featurebyte-1.0.3/featurebyte/models/historical_feature_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 HistoricalFeatureTableModel
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 import pymongo
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/models/item_table.py` & `featurebyte-1.0.3/featurebyte/models/item_table.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 This module contains ItemTable related models
 """
+
 from __future__ import annotations
 
-from typing import Any, ClassVar, List, Tuple, Type
+from typing import Any, ClassVar, List, Optional, Tuple, Type
 
-from pydantic import root_validator
+from pydantic import Field, root_validator
 
 from featurebyte.common.validator import construct_data_model_root_validator
 from featurebyte.enum import DBVarType
+from featurebyte.models.event_table import EventTableModel
 from featurebyte.models.feature_store import TableModel
 from featurebyte.query_graph.graph_node.base import GraphNode
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.model.table import ItemTableData
 from featurebyte.query_graph.node.input import InputNode
 from featurebyte.query_graph.node.nested import ItemViewMetadata
 
@@ -53,18 +55,29 @@
                 ("record_creation_timestamp_column", DBVarType.supported_timestamp_types()),
                 ("event_id_column", DBVarType.supported_id_types()),
                 ("item_id_column", DBVarType.supported_id_types()),
             ],
         )
     )
 
+    event_table_model: Optional[EventTableModel] = Field(default=None, exclude=True)
+
     @property
     def primary_key_columns(self) -> List[str]:
         return [self.item_id_column]
 
+    @property
+    def special_columns(self) -> List[str]:
+        cols = [
+            self.item_id_column,
+            self.event_id_column,
+            self.record_creation_timestamp_column,
+        ]
+        return [col for col in cols if col]
+
     def create_view_graph_node(
         self,
         input_node: InputNode,
         metadata: ItemViewMetadata,
         **kwargs: Any,
     ) -> Tuple[GraphNode, List[ColumnInfo]]:
         table_data = ItemTableData(**self.dict(by_alias=True)).clone(
```

### Comparing `featurebyte-1.0.2/featurebyte/models/materialized_table.py` & `featurebyte-1.0.3/featurebyte/models/materialized_table.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 MaterializedTable model
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 import pymongo
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/models/mixin.py` & `featurebyte-1.0.3/featurebyte/models/mixin.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains mixin classes for models.
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from pydantic import Field, PrivateAttr
 
 from featurebyte.models.base import FeatureByteBaseModel
```

### Comparing `featurebyte-1.0.2/featurebyte/models/observation_table.py` & `featurebyte-1.0.3/featurebyte/models/observation_table.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 ObservationTableModel models
 """
+
 from __future__ import annotations
 
 from typing import Dict, List, Optional, Union
 from typing_extensions import Annotated, Literal
 
 from datetime import datetime  # pylint: disable=wrong-import-order
 
 import pymongo
+from bson import ObjectId
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.validator import construct_sort_validator
 from featurebyte.enum import StrEnum
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.materialized_table import MaterializedTableModel
@@ -40,15 +42,15 @@
 
 
 class TargetInput(FeatureByteBaseModel):
     """
     TargetInput is an input from a target that can be used to create an ObservationTableModel
     """
 
-    target_id: PydanticObjectId
+    target_id: Optional[PydanticObjectId]
     observation_table_id: Optional[PydanticObjectId]
     type: Literal[RequestInputType.OBSERVATION_TABLE, RequestInputType.DATAFRAME]
 
     async def materialize(
         self,
         session: BaseSession,
         destination: TableDetails,
@@ -141,19 +143,34 @@
     use_case_ids: List[PydanticObjectId] = Field(default_factory=list)
     purpose: Optional[Purpose] = Field(default=None)
     least_recent_point_in_time: Optional[StrictStr] = Field(default=None)
     entity_column_name_to_count: Optional[Dict[str, int]] = Field(default_factory=dict)
     min_interval_secs_between_entities: Optional[float] = Field(default_factory=None)
     primary_entity_ids: Optional[List[PydanticObjectId]] = Field(default_factory=list)
     has_row_index: Optional[bool] = Field(default=False)
+    target_namespace_id: Optional[PydanticObjectId] = Field(default=None)
 
     _sort_primary_entity_ids_validator = validator("primary_entity_ids", allow_reuse=True)(
         construct_sort_validator()
     )
 
+    @property
+    def target_id(self) -> Optional[ObjectId]:
+        """
+        The target id associated with the observation table
+
+        Returns
+        -------
+        Optional[ObjectId]
+            The target id associated with the observation table
+        """
+        if isinstance(self.request_input, TargetInput):
+            return self.request_input.target_id
+        return None
+
     @validator("most_recent_point_in_time", "least_recent_point_in_time")
     @classmethod
     def _validate_most_recent_point_in_time(cls, value: Optional[str]) -> Optional[str]:
         if value is None:
             return None
         # Check that most_recent_point_in_time is a valid ISO 8601 datetime
         _ = datetime.fromisoformat(value)
```

### Comparing `featurebyte-1.0.2/featurebyte/models/offline_store_feature_table.py` & `featurebyte-1.0.3/featurebyte/service/feature_store_warehouse.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,354 +1,432 @@
 """
-OfflineStoreFeatureTableModel class
+Service for interacting with the data warehouse for queries around the feature store.
+
+We split this into a separate service, as these typically require a session object that is created.
 """
+
 from __future__ import annotations
 
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, AsyncGenerator, List, Optional, Tuple
+
+import os
 
-from dataclasses import dataclass
-from datetime import datetime
-from pathlib import Path
-
-import pymongo
-from bson import ObjectId
-from pydantic import BaseModel, Field, root_validator
-
-from featurebyte.common.model_util import convert_seconds_to_time_format
-from featurebyte.common.string import sanitize_identifier
-from featurebyte.enum import DBVarType
-from featurebyte.models.base import (
-    FeatureByteCatalogBaseDocumentModel,
-    PydanticObjectId,
-    UniqueValuesConstraint,
+from featurebyte.common.utils import dataframe_to_json
+from featurebyte.enum import InternalName, MaterializedTableNamePrefix
+from featurebyte.exception import (
+    DatabaseNotFoundError,
+    LimitExceededError,
+    SchemaNotFoundError,
+    TableNotFoundError,
 )
-from featurebyte.models.entity import EntityModel
-from featurebyte.models.entity_universe import EntityUniverseModel
-from featurebyte.models.feature import FeatureModel
-from featurebyte.models.feature_list import FeatureCluster
-from featurebyte.models.offline_store_ingest_query import OfflineStoreIngestQueryGraph
-from featurebyte.query_graph.graph import QueryGraph
-from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
-from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
-from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
+from featurebyte.logging import get_logger
+from featurebyte.models.feature_store import FeatureStoreModel
+from featurebyte.models.user_defined_function import UserDefinedFunctionModel
+from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
+from featurebyte.query_graph.model.common_table import TabularSource
+from featurebyte.query_graph.model.table import TableDetails, TableSpec
+from featurebyte.query_graph.sql.common import quoted_identifier, sql_to_string
+from featurebyte.query_graph.sql.materialisation import (
+    get_feature_store_id_expr,
+    get_source_count_expr,
+    get_source_expr,
+)
+from featurebyte.schema.feature_store import FeatureStoreShape
+from featurebyte.service.feature_store import FeatureStoreService
+from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.session.base import INTERACTIVE_SESSION_TIMEOUT_SECONDS, BaseSession
 
+MAX_TABLE_CELLS = int(
+    os.environ.get("MAX_TABLE_CELLS", 10000000 * 300)
+)  # 10 million rows, 300 columns
 
-class OnlineStoreLastMaterializedAt(BaseModel):
-    """
-    Last materialized timestamp for an online store
-    """
 
-    online_store_id: PydanticObjectId
-    value: datetime
+logger = get_logger(__name__)
 
 
-class OfflineStoreFeatureTableModel(FeatureByteCatalogBaseDocumentModel):
+class FeatureStoreWarehouseService:
     """
-    OfflineStoreFeatureTable class
+    FeatureStoreWarehouseService is responsible for interacting with the data warehouse.
     """
 
-    name: str
-    name_prefix: Optional[str] = Field(default=None)
-    name_suffix: Optional[str] = Field(default=None)
-    feature_ids: List[PydanticObjectId]
-    primary_entity_ids: List[PydanticObjectId]
-    serving_names: List[str]
-    feature_job_setting: Optional[FeatureJobSetting]
-    has_ttl: bool
-    last_materialized_at: Optional[datetime]
-    online_stores_last_materialized_at: List[OnlineStoreLastMaterializedAt] = Field(
-        default_factory=list
-    )
-
-    feature_cluster_path: Optional[str] = Field(default=None)
-    feature_cluster: Optional[FeatureCluster]
-
-    output_column_names: List[str]
-    output_dtypes: List[DBVarType]
-    internal_entity_universe: Optional[Dict[str, Any]] = Field(alias="entity_universe")
-    entity_lookup_info: Optional[EntityRelationshipInfo]
-    feature_store_id: Optional[PydanticObjectId] = Field(default=None)
-
-    @root_validator
-    @classmethod
-    def _set_feature_store_id(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+    def __init__(
+        self,
+        session_manager_service: SessionManagerService,
+        feature_store_service: FeatureStoreService,
+    ):
+        self.session_manager_service = session_manager_service
+        self.feature_store_service = feature_store_service
+
+    async def check_user_defined_function_exists(
+        self,
+        user_defined_function: UserDefinedFunctionModel,
+        feature_store: FeatureStoreModel,
+    ) -> None:
         """
-        Set feature_store_id
+        Check whether user defined function in feature store
 
         Parameters
         ----------
-        values : Dict[str, Any]
-            Values
-
-        Returns
-        -------
-        Dict[str, Any]
+        user_defined_function: UserDefinedFunctionModel
+            User defined function model
+        feature_store: FeatureStoreModel
+            Feature store model
         """
-        if not values.get("feature_store_id", None) and values.get("feature_cluster"):
-            values["feature_store_id"] = values["feature_cluster"].feature_store_id
-        return values
-
-    @property
-    def remote_attribute_paths(self) -> List[Path]:
-        paths = []
-        if self.feature_cluster_path:
-            paths.append(Path(self.feature_cluster_path))
-        return paths
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
+        )
+        await db_session.check_user_defined_function(user_defined_function=user_defined_function)
 
-    @property
-    def table_signature(self) -> Dict[str, Any]:
+    async def list_databases(
+        self, feature_store: FeatureStoreModel, get_credential: Optional[Any]
+    ) -> List[str]:
         """
-        Get table signature
+        List databases in feature store
+
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        get_credential: Optional[Any]
+            Get credential handler function
 
         Returns
         -------
-        Dict[str, Any]
+        List[str]
+            List of database names
         """
-        entity_lookup_info = None
-        if self.entity_lookup_info:
-            entity_lookup_info = self.entity_lookup_info.dict(by_alias=True)
-        return {
-            "catalog_id": self.catalog_id,
-            "primary_entity_ids": self.primary_entity_ids,
-            "serving_names": self.serving_names,
-            "feature_job_setting": self.feature_job_setting.dict()
-            if self.feature_job_setting
-            else None,
-            "has_ttl": self.has_ttl,
-            "entity_lookup_info": entity_lookup_info,
-        }
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, get_credential=get_credential
+        )
+        return await db_session.list_databases()
 
-    @property
-    def entity_universe(self) -> EntityUniverseModel:
+    async def list_schemas(
+        self,
+        feature_store: FeatureStoreModel,
+        database_name: str,
+    ) -> List[str]:
         """
-        Get entity universe
+        List schemas in feature store
+
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+
+        Raises
+        ------
+        DatabaseNotFoundError
+            If database not found
 
         Returns
         -------
-        EntityUniverseModel
+        List[str]
+            List of schema names
+        """
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
+        )
+        try:
+            return await db_session.list_schemas(database_name=database_name)
+        except db_session.no_schema_error as exc:
+            raise DatabaseNotFoundError(f"Database {database_name} not found.") from exc
+
+    @staticmethod
+    def _is_visible_table(table_name: str, filter_featurebyte_tables: bool) -> bool:
+        if table_name.startswith("__"):
+            return False
+        if not filter_featurebyte_tables:
+            return True
+        # quick filter for materialized tables
+        if "TABLE" not in table_name:
+            return False
+        for prefix in MaterializedTableNamePrefix.visible():
+            if table_name.startswith(prefix):
+                return True
+        return False
+
+    @staticmethod
+    async def _is_featurebyte_schema(
+        db_session: BaseSession, database_name: str, schema_name: str
+    ) -> bool:
+        try:
+            sql_expr = get_feature_store_id_expr(
+                database_name=database_name, schema_name=schema_name
+            )
+            sql = sql_to_string(
+                sql_expr,
+                source_type=db_session.source_type,
+            )
+            _ = await db_session.execute_query(sql)
+            return True
+        except db_session.no_schema_error:
+            return False
+
+    async def list_tables(
+        self,
+        feature_store: FeatureStoreModel,
+        database_name: str,
+        schema_name: str,
+    ) -> List[TableSpec]:
+        """
+        List tables in feature store
+
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
 
         Raises
         ------
-        ValueError
-            If entity_universe is not set
+        SchemaNotFoundError
+            If schema not found
+
+        Returns
+        -------
+        List[TableSpec]
+            List of tables
         """
-        if self.internal_entity_universe is None:
-            raise ValueError("entity_universe is not set")
-        return EntityUniverseModel(**self.internal_entity_universe)
-
-    def _get_basename(self) -> str:
-        # max length of feature table name is 64
-        # reserving 8 characters for prefix (catalog name, `<project_name>_`, which is 7 hex digits)
-        # need to the same prefix for project name (same as catalog prefix)
-        # 3 characters for suffix (count, `_<num>`, num is 1-99)
-        max_len = 45  # 64 - 2 * 8 - 3
-        max_serv_name_len = 19
-        max_serv_num = 2
-        max_freq_len = (  # 45 - 39 - 1 = 5
-            max_len
-            - max_serv_name_len * max_serv_num  # serving names
-            - (max_serv_num - 1)  # underscores
-            - 1  # frequency separator
-        )
-
-        name = "_no_entity"
-        if self.serving_names:
-            # take first 3 serving names and join them with underscore
-            # if serving name is longer than 16 characters, truncate it
-            # max length of the name = 15 * 3 + 2 = 47
-            # strip leading and trailing underscores for all serving names & sanitized name
-            name = sanitize_identifier(
-                "_".join(
-                    serving_name[:max_serv_name_len].strip("_")
-                    for serving_name in self.serving_names[:max_serv_num]
-                )
-            ).strip("_")
-
-        if self.feature_job_setting:
-            # take the frequency part of the feature job setting
-            freq_part = ""
-            for component in reversed(range(1, 5)):
-                freq_part = convert_seconds_to_time_format(
-                    self.feature_job_setting.frequency_seconds, components=component
-                )
-                if len(freq_part) <= max_freq_len:
-                    break
-            keep = max_len - len(freq_part) - 1
-            name = f"{name[:keep]}_{freq_part}"
-        return name
 
-    def get_name(self) -> str:
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
+        )
+        is_featurebyte_schema = await self._is_featurebyte_schema(
+            db_session, database_name, schema_name
+        )
+        try:
+            tables = await db_session.list_tables(
+                database_name=database_name, schema_name=schema_name
+            )
+        except db_session.no_schema_error as exc:
+            raise SchemaNotFoundError(f"Schema {schema_name} not found.") from exc
+
+        return [
+            table for table in tables if self._is_visible_table(table.name, is_featurebyte_schema)
+        ]
+
+    async def list_columns(
+        self,
+        feature_store: FeatureStoreModel,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+    ) -> List[ColumnSpecWithDescription]:
         """
-        Get full name of the feature table
+        List columns in database table
+
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
+        table_name: str
+            Name of table to use
+
+        Raises
+        ------
+        TableNotFoundError
+            If table not found
 
         Returns
         -------
-        str
+        List[ColumnSpecWithDescription]
+            List of ColumnSpecWithDescription object
         """
-        full_name = ""
-        if self.name_prefix:
-            full_name += self.name_prefix + "_"
-
-        full_name += self._get_basename()
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
+        )
 
-        if self.name_suffix:
-            full_name += "_" + self.name_suffix
-        return full_name
+        try:
+            table_schema = await db_session.list_table_schema(
+                database_name=database_name, schema_name=schema_name, table_name=table_name
+            )
+        except db_session.no_schema_error as exc:
+            raise TableNotFoundError(f"Table {table_name} not found.") from exc
+
+        table_schema = {  # type: ignore[assignment]
+            col_name: v
+            for (col_name, v) in table_schema.items()
+            if col_name != InternalName.TABLE_ROW_INDEX
+        }
+        return list(table_schema.values())
 
-    def get_online_store_last_materialized_at(
-        self, online_store_id: ObjectId
-    ) -> Optional[datetime]:
+    async def get_table_details(
+        self,
+        feature_store: FeatureStoreModel,
+        database_name: str,
+        schema_name: str,
+        table_name: str,
+    ) -> TableDetails:
         """
-        Get the last materialized at timestamp for an online store. Returns None if this offline
-        store table has never been materialized to the online store.
+        Get table details
 
         Parameters
         ----------
-        online_store_id: ObjectId
-            Online store id
+        feature_store: FeatureStoreModel
+            FeatureStoreModel object
+        database_name: str
+            Name of database to use
+        schema_name: str
+            Name of schema to use
+        table_name: str
+            Name of table to use
+
+        Raises
+        ------
+        TableNotFoundError
+            If table not found
 
         Returns
         -------
-        Optional[datetime]
+        TableDetails
         """
-        if self.online_stores_last_materialized_at is not None:
-            for entry in self.online_stores_last_materialized_at:
-                if entry.online_store_id == online_store_id:
-                    return entry.value
-        return None
-
-    class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
-        """
-        MongoDB settings
-        """
-
-        collection_name: str = "offline_store_feature_table"
-        unique_constraints: List[UniqueValuesConstraint] = [
-            UniqueValuesConstraint(
-                fields=("_id",),
-                conflict_fields_signature={"id": ["_id"]},
-                resolution_signature=None,
-            ),
-        ]
-        indexes = FeatureByteCatalogBaseDocumentModel.Settings.indexes + [
-            pymongo.operations.IndexModel("feature_ids"),
-            pymongo.operations.IndexModel("primary_entity_ids"),
-            pymongo.operations.IndexModel("serving_names"),
-            pymongo.operations.IndexModel("feature_job_setting"),
-            pymongo.operations.IndexModel("has_ttl"),
-            pymongo.operations.IndexModel("entity_lookup_info"),
-        ]
-        auditable = False
 
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
+        )
+        try:
+            return await db_session.get_table_details(
+                database_name=database_name, schema_name=schema_name, table_name=table_name
+            )
+        except db_session.no_schema_error as exc:
+            raise TableNotFoundError(f"Table {table_name} not found.") from exc
+
+    async def _get_table_shape(
+        self, location: TabularSource, db_session: BaseSession
+    ) -> Tuple[Tuple[int, int], bool, list[str]]:
+        # check size of the table
+        sql_expr = get_source_count_expr(source=location.table_details)
+        sql = sql_to_string(
+            sql_expr,
+            source_type=db_session.source_type,
+        )
+        result = await db_session.execute_query(sql)
+        assert result is not None
+        columns_specs = await db_session.list_table_schema(**location.table_details.json_dict())
+        has_row_index = InternalName.TABLE_ROW_INDEX in columns_specs
+        columns = [
+            col_name
+            for col_name in columns_specs.keys()
+            if col_name != InternalName.TABLE_ROW_INDEX
+        ]
+        return (
+            (result["row_count"].iloc[0], len(columns)),
+            has_row_index,
+            columns,
+        )
 
-class FeaturesUpdate(BaseDocumentServiceUpdateSchema):
-    """
-    FeaturesUpdate class to be used when updating features related fields
-    """
-
-    feature_ids: List[PydanticObjectId]
-    feature_cluster: Optional[FeatureCluster]
-    feature_cluster_path: Optional[str]
-    output_column_names: List[str]
-    output_dtypes: List[DBVarType]
-    entity_universe: EntityUniverseModel
+    async def table_shape(self, location: TabularSource) -> FeatureStoreShape:
+        """
+        Get the shape table from location.
 
+        Parameters
+        ----------
+        location: TabularSource
+            Location to get shape from
 
-class OfflineLastMaterializedAtUpdate(BaseDocumentServiceUpdateSchema):
-    """
-    Schema to be used when updating last_materialized_at field
-    """
+        Returns
+        -------
+        FeatureStoreShape
+            Row and column counts
+        """
+        feature_store = await self.feature_store_service.get_document(
+            document_id=location.feature_store_id
+        )
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
+        )
+        shape, _, _ = await self._get_table_shape(location, db_session)
+        return FeatureStoreShape(num_rows=shape[0], num_cols=shape[1])
 
-    last_materialized_at: datetime
+    async def table_preview(self, location: TabularSource, limit: int) -> dict[str, Any]:
+        """
+        Preview table from location.
 
+        Parameters
+        ----------
+        location: TabularSource
+            Location to preview from
+        limit: int
+            Row limit on preview results
 
-class OnlineStoresLastMaterializedAtUpdate(BaseDocumentServiceUpdateSchema):
-    """
-    Schema to be used when updating online_stores_last_materialized_at field
-    """
+        Returns
+        -------
+        dict[str, Any]
+            Dataframe converted to json string
+        """
+        feature_store = await self.feature_store_service.get_document(
+            document_id=location.feature_store_id
+        )
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
+        )
+        sql_expr = get_source_expr(source=location.table_details).limit(limit)
+        sql = sql_to_string(
+            sql_expr,
+            source_type=db_session.source_type,
+        )
+        result = await db_session.execute_query(sql)
 
-    online_stores_last_materialized_at: List[OnlineStoreLastMaterializedAt]
+        # drop row index column if present
+        if result is not None and InternalName.TABLE_ROW_INDEX in result.columns:
+            result.drop(columns=[InternalName.TABLE_ROW_INDEX], inplace=True)
 
+        return dataframe_to_json(result)
 
-OfflineStoreFeatureTableUpdate = Union[
-    FeaturesUpdate, OfflineLastMaterializedAtUpdate, OnlineStoresLastMaterializedAtUpdate
-]
+    async def download_table(
+        self,
+        location: TabularSource,
+    ) -> Optional[AsyncGenerator[bytes, None]]:
+        """
+        Download table from location.
 
+        Parameters
+        ----------
+        location: TabularSource
+            Location to download from
 
-@dataclass
-class OfflineIngestGraphMetadata:
-    """
-    Information about offline ingest graph combined for all features
-    """
+        Returns
+        -------
+        AsyncGenerator[bytes, None]
+            Asynchronous bytes generator
 
-    feature_cluster: FeatureCluster
-    output_column_names: List[str]
-    output_dtypes: List[DBVarType]
-    offline_ingest_graphs: List[OfflineStoreIngestQueryGraph]
+        Raises
+        ------
+        LimitExceededError
+            Table size exceeds the limit.
+        """
+        feature_store = await self.feature_store_service.get_document(
+            document_id=location.feature_store_id
+        )
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
+        )
 
+        shape, has_row_index, columns = await self._get_table_shape(location, db_session)
+        logger.debug(
+            "Downloading table from feature store",
+            extra={
+                "location": location.json_dict(),
+                "shape": shape,
+            },
+        )
 
-def get_combined_ingest_graph(
-    features: List[FeatureModel],
-    primary_entities: List[EntityModel],
-    has_ttl: bool,
-    feature_job_setting: Optional[FeatureJobSetting],
-) -> OfflineIngestGraphMetadata:
-    """
-    Returns a combined ingest graph and related information for all features belonging to a
-    feature table
+        if shape[0] * shape[1] > MAX_TABLE_CELLS:
+            raise LimitExceededError(f"Table size {shape} exceeds download limit.")
 
-    Parameters
-    ----------
-    features : List[FeatureModel]
-        List of features
-    primary_entities : List[EntityModel]
-        List of primary entity models
-    has_ttl : bool
-        Whether the feature table has TTL
-    feature_job_setting : Optional[FeatureJobSetting]
-        Feature job setting
-
-    Returns
-    -------
-    OfflineIngestGraphMetadata
-    """
-    local_query_graph = QueryGraph()
-    output_nodes = []
-    output_column_names = []
-    output_dtypes = []
-    all_offline_ingest_graphs = []
-
-    primary_entity_ids = sorted([entity.id for entity in primary_entities])
-    for feature in features:
-        offline_ingest_graphs = (
-            feature.offline_store_info.extract_offline_store_ingest_query_graphs()
-        )
-        for offline_ingest_graph in offline_ingest_graphs:
-            if (
-                offline_ingest_graph.primary_entity_ids != primary_entity_ids
-                or offline_ingest_graph.has_ttl != has_ttl
-                or offline_ingest_graph.feature_job_setting != feature_job_setting
-            ):
-                # Feature of a primary entity can be decomposed into ingest graphs that should be
-                # published to different feature tables. Skip if it is not the same as the current
-                # feature table.
-                continue
-
-            graph, node = offline_ingest_graph.ingest_graph_and_node()
-            local_query_graph, local_name_map = local_query_graph.load(graph)
-            output_nodes.append(local_query_graph.get_node_by_name(local_name_map[node.name]))
-            output_column_names.append(offline_ingest_graph.output_column_name)
-            output_dtypes.append(offline_ingest_graph.output_dtype)
-            all_offline_ingest_graphs.append(offline_ingest_graph)
-
-    feature_cluster = FeatureCluster(
-        feature_store_id=features[0].tabular_source.feature_store_id,
-        graph=local_query_graph,
-        node_names=[node.name for node in output_nodes],
-    )
-
-    return OfflineIngestGraphMetadata(
-        feature_cluster=feature_cluster,
-        output_column_names=output_column_names,
-        output_dtypes=output_dtypes,
-        offline_ingest_graphs=all_offline_ingest_graphs,
-    )
+        sql_expr = get_source_expr(source=location.table_details, column_names=columns)
+        if has_row_index:
+            sql_expr = sql_expr.order_by(quoted_identifier(InternalName.TABLE_ROW_INDEX))
+        sql = sql_to_string(
+            sql_expr,
+            source_type=db_session.source_type,
+        )
+        return db_session.get_async_query_stream(sql)
```

### Comparing `featurebyte-1.0.2/featurebyte/models/offline_store_ingest_query.py` & `featurebyte-1.0.3/featurebyte/models/offline_store_ingest_query.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,44 +1,51 @@
 """
 OfflineStoreIngestQuery object stores the offline store ingest query for a feature.
 """
+
 from __future__ import annotations
 
 from typing import Dict, List, Optional, Tuple
 
 import datetime
 
 from bson import ObjectId
 from pydantic import Field, validator
 
 from featurebyte.common.string import sanitize_identifier
-from featurebyte.common.typing import Scalar
 from featurebyte.common.validator import construct_sort_validator
 from featurebyte.enum import DBVarType
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.mixin import QueryGraphMixin
 from featurebyte.query_graph.enum import GraphNodeType, NodeOutputType, NodeType
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
-from featurebyte.query_graph.node.metadata.sdk_code import CodeGenerator, VariableNameGenerator
+from featurebyte.query_graph.node.metadata.config import OnDemandFunctionCodeGenConfig
+from featurebyte.query_graph.node.metadata.sdk_code import (
+    CodeGenerator,
+    StatementStr,
+    ValueStr,
+    VariableNameGenerator,
+)
 from featurebyte.query_graph.node.nested import (
     AggregationNodeInfo,
     OfflineStoreIngestQueryGraphNodeParameters,
     OfflineStoreMetadata,
 )
 from featurebyte.query_graph.node.schema import ColumnSpec
 from featurebyte.query_graph.node.utils import subset_frame_column_expr
 from featurebyte.query_graph.transform.on_demand_function import (
     OnDemandFeatureFunctionExtractor,
     OnDemandFeatureFunctionGlobalState,
     SQLInputArgumentInfo,
 )
 from featurebyte.query_graph.transform.on_demand_view import OnDemandFeatureViewExtractor
 from featurebyte.query_graph.transform.quick_pruning import QuickGraphStructurePruningTransformer
+from featurebyte.typing import Scalar
 
 
 def get_time_aggregate_ttl_in_secs(feature_job_setting: FeatureJobSetting) -> int:
     """
     Get time-to-live (TTL) in seconds for the time aggregate operation
 
     Parameters
@@ -353,16 +360,15 @@
                 input_df_name=odfv_info.input_df_name,
                 output_df_name=odfv_info.output_df_name,
                 function_name=odfv_info.function_name,
                 ttl_seconds=self.time_to_live_in_secs,
             )
             self.odfv_info = odfv_info
 
-        if self.is_decomposed:
-            # FIXME: should handle the case for null_filling_value
+        if self.is_decomposed or self.null_filling_value is not None:
             # initialize the user defined function info
             udf_info = UserDefinedFunctionInfo(
                 sql_function_name=f"udf_{unique_func_name}",
                 sql_input_var_prefix="x",
                 sql_request_input_var_prefix="r",
                 function_name="user_defined_function",
                 input_var_prefix="col",
@@ -497,31 +503,52 @@
         sql_input_var_prefix: str = "x",
         sql_request_input_var_prefix: str = "r",
         sql_comment: str = "",
         function_name: str = "user_defined_function",
         input_var_prefix: str = "col",
         request_input_var_prefix: str = "request_col",
     ) -> OnDemandFeatureFunctionGlobalState:
-        if not self.is_decomposed:
-            raise ValueError(
-                "Cannot generate on demand feature function code for non-decomposed query graph"
+        codegen_kwargs = {
+            "sql_function_name": sql_function_name,
+            "sql_input_var_prefix": sql_input_var_prefix,
+            "sql_request_input_var_prefix": sql_request_input_var_prefix,
+            "sql_comment": sql_comment,
+            "function_name": function_name,
+            "input_var_prefix": input_var_prefix,
+            "request_input_var_prefix": request_input_var_prefix,
+            "output_dtype": output_dtype,
+        }
+        if self.is_decomposed:
+            node = self.graph.get_node_by_name(self.node_name)
+            codegen_state = OnDemandFeatureFunctionExtractor(graph=self.graph).extract(
+                node=node, **codegen_kwargs
+            )
+        else:
+            assert self.null_filling_value is not None
+            assert self.metadata is not None, "non-decomposed query graph must have metadata"
+            codegen_state = OnDemandFeatureFunctionGlobalState(
+                code_generation_config=OnDemandFunctionCodeGenConfig(**codegen_kwargs),
+                var_name_generator=VariableNameGenerator(one_based=True),
+            )
+            col = codegen_state.var_name_generator.convert_to_variable_name(
+                variable_name_prefix=codegen_state.code_generation_config.input_var_prefix,
+                node_name=None,
+            )
+            codegen_state.register_input_argument(
+                variable_name_prefix=codegen_state.code_generation_config.input_var_prefix,
+                py_type=codegen_state.code_generation_config.to_py_type(DBVarType(output_dtype)),
+                column_name=self.metadata.output_column_name,
+            )
+            fill_value_expr = ValueStr(self.null_filling_value).as_input()
+            codegen_state.code_generator.add_statements(
+                statements=[
+                    StatementStr(f"return {fill_value_expr} if pd.isnull({col}) else {col}")
+                ]
             )
 
-        node = self.graph.get_node_by_name(self.node_name)
-        codegen_state = OnDemandFeatureFunctionExtractor(graph=self.graph).extract(
-            node=node,
-            sql_function_name=sql_function_name,
-            sql_input_var_prefix=sql_input_var_prefix,
-            sql_request_input_var_prefix=sql_request_input_var_prefix,
-            sql_comment=sql_comment,
-            function_name=function_name,
-            input_var_prefix=input_var_prefix,
-            request_input_var_prefix=request_input_var_prefix,
-            output_dtype=output_dtype,
-        )
         return codegen_state
 
     def generate_databricks_user_defined_function_code(
         self,
         output_dtype: DBVarType,
         to_sql: bool = False,
         sql_function_name: str = "udf_func",
```

### Comparing `featurebyte-1.0.2/featurebyte/models/online_store.py` & `featurebyte-1.0.3/featurebyte/models/online_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Online Store related models
 """
+
 from typing import List, Literal, Optional, Union
 from typing_extensions import Annotated
 
 import pymongo
 from pydantic import Field, StrictStr
 
 from featurebyte.common.doc_util import FBAutoDoc
```

### Comparing `featurebyte-1.0.2/featurebyte/models/online_store_compute_query.py` & `featurebyte-1.0.3/featurebyte/models/online_store_compute_query.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 AggregationResult document model
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 import pymongo
 from pydantic import StrictStr
```

### Comparing `featurebyte-1.0.2/featurebyte/models/online_store_spec.py` & `featurebyte-1.0.3/featurebyte/models/online_store_spec.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Tile related models
 """
+
 from typing import Any, Dict, List, cast
 
 from pydantic import validator
 
 from featurebyte.enum import TableDataType
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.models.base import FeatureByteBaseModel
```

### Comparing `featurebyte-1.0.2/featurebyte/models/online_store_table_version.py` & `featurebyte-1.0.3/featurebyte/models/online_store_table_version.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStoreTableVersion model
 """
+
 from __future__ import annotations
 
 from typing import List
 
 import pymongo
 from pydantic import StrictStr
```

### Comparing `featurebyte-1.0.2/featurebyte/models/parent_serving.py` & `featurebyte-1.0.3/featurebyte/models/parent_serving.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 """
 Models related to serving parent features
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
 from pydantic import root_validator
 
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.entity import EntityModel
-from featurebyte.models.proxy_table import ProxyTableModel
+from featurebyte.models.proxy_table import TableModel
 from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.query_graph.node.schema import FeatureStoreDetails
 
 
 class EntityLookupInfo(FeatureByteBaseModel):
     """
     Information about an entity such as keys, serving names, that are relevant in the context of an
@@ -46,46 +47,46 @@
     parent: EntityLookupInfo
         Information about the parent entity
     child: EntityLookupInfo
         Information about the child entity
     """
 
     id: PydanticObjectId
-    table: ProxyTableModel
+    table: TableModel
     parent: EntityLookupInfo
     child: EntityLookupInfo
 
 
 class EntityLookupStepCreator(FeatureByteBaseModel):
     """
     Helper class containing concrete instances of EntityModel and TableModel to help with creating
     EntityLookupStep
     """
 
     entity_relationships_info: List[EntityRelationshipInfo]
     entities_by_id: Dict[PydanticObjectId, EntityModel]
-    tables_by_id: Dict[PydanticObjectId, ProxyTableModel]
+    tables_by_id: Dict[PydanticObjectId, TableModel]
     default_entity_lookup_steps: Dict[PydanticObjectId, EntityLookupStep]
 
     @root_validator(pre=True)
     @classmethod
     def _generate_default_entity_lookup_steps(cls, values: Dict[str, Any]) -> Dict[str, Any]:
-        entity_relationships_info = values["entity_relationships_info"]
+        entity_relationships_info: List[EntityRelationshipInfo] = values[
+            "entity_relationships_info"
+        ]
         entities_by_id = values["entities_by_id"]
         tables_by_id = values["tables_by_id"]
         default_entity_lookup_steps = {}
 
         for info in entity_relationships_info:
             relation_table = tables_by_id[info.relation_table_id]
             parent_entity = entities_by_id[info.related_entity_id]
             child_entity = entities_by_id[info.entity_id]
 
             if info.entity_column_name is None or info.related_entity_column_name is None:
-                # Backward compatibility for relationships without the column names; these are not
-                # truly frozen since the table columns_info is dynamic.
                 child_column_name = None
                 parent_column_name = None
                 for column_info in relation_table.columns_info:
                     if column_info.entity_id == child_entity.id:
                         child_column_name = column_info.name
                     elif column_info.entity_id == parent_entity.id:
                         parent_column_name = column_info.name
@@ -93,15 +94,15 @@
                 assert parent_column_name is not None
             else:
                 child_column_name = info.entity_column_name
                 parent_column_name = info.related_entity_column_name
 
             default_entity_lookup_steps[info.id] = EntityLookupStep(
                 id=info.id,
-                table=relation_table.dict(by_alias=True),
+                table=relation_table,
                 parent=EntityLookupInfo(
                     key=parent_column_name,
                     serving_name=parent_entity.serving_names[0],
                     entity_id=parent_entity.id,
                 ),
                 child=EntityLookupInfo(
                     key=child_column_name,
```

### Comparing `featurebyte-1.0.2/featurebyte/models/periodic_task.py` & `featurebyte-1.0.3/featurebyte/models/periodic_task.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Periodic Task document model
 """
+
 from typing import Any, Dict, List, Literal, Optional, Union
 
 from datetime import datetime
 
 import pymongo
 from pydantic import BaseModel, Field
 from pymongo.operations import IndexModel
```

### Comparing `featurebyte-1.0.2/featurebyte/models/persistent.py` & `featurebyte-1.0.3/featurebyte/models/persistent.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Pydantic Model for persistent storage
 """
+
 from typing import Any, Dict, List, Mapping, Optional
 
 from datetime import datetime
 
 from bson import ObjectId
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/models/proxy_table.py` & `featurebyte-1.0.3/featurebyte/models/proxy_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains ProxyTable pseudo models.
 """
+
 from __future__ import annotations
 
 from typing import Any, Union
 from typing_extensions import Annotated
 
 from pydantic import Field, parse_obj_as
```

### Comparing `featurebyte-1.0.2/featurebyte/models/relationship.py` & `featurebyte-1.0.3/featurebyte/models/relationship.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Relation mixin model
 """
+
 from typing import Any, Dict, List, Optional
 
 import pymongo
 from bson import ObjectId
 from pydantic import Field, root_validator, validator
 
 from featurebyte.common.validator import construct_sort_validator
```

### Comparing `featurebyte-1.0.2/featurebyte/models/relationship_analysis.py` & `featurebyte-1.0.3/featurebyte/models/relationship_analysis.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 RelationshipAnalysisService class
 """
+
 from __future__ import annotations
 
 from typing import List, TypeVar
 
 from featurebyte.models.entity import EntityModel
 
 EntityModelT = TypeVar("EntityModelT", bound=EntityModel)
```

### Comparing `featurebyte-1.0.2/featurebyte/models/request_input.py` & `featurebyte-1.0.3/featurebyte/models/request_input.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 RequestInput is the base class for all request input types.
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Literal, Optional, cast
 
 from abc import abstractmethod
 
 from pydantic import Field, PrivateAttr, StrictStr
@@ -14,15 +15,14 @@
 from featurebyte.enum import SourceType, StrEnum
 from featurebyte.exception import ColumnNotFoundError
 from featurebyte.models.base import FeatureByteBaseModel
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
-from featurebyte.query_graph.sql.common import sql_to_string
 from featurebyte.query_graph.sql.materialisation import (
     get_row_count_sql,
     get_source_expr,
     get_view_expr,
     select_and_rename_columns,
 )
 from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
@@ -77,15 +77,15 @@
             The query expression to get the row count for
 
         Returns
         -------
         int
         """
         query = get_row_count_sql(table_expr=query_expr, source_type=session.source_type)
-        result = await session.execute_query(query)
+        result = await session.execute_query_long_running(query)
         return int(result.iloc[0]["row_count"])  # type: ignore[union-attr]
 
     @abstractmethod
     async def get_column_names(self, session: BaseSession) -> list[str]:
         """
         Get the column names of the table query
 
@@ -155,23 +155,15 @@
                 adapter = get_sql_adapter(source_type=session.source_type)
                 query_expr = (
                     adapter.tablesample(query_expr, num_percent)
                     .order_by(expressions.Anonymous(this="RANDOM"))
                     .limit(sample_rows)
                 )
 
-        expression = get_sql_adapter(session.source_type).create_table_as(
-            table_details=destination, select_expr=query_expr
-        )
-        query = sql_to_string(
-            expression,
-            source_type=session.source_type,
-        )
-
-        await session.execute_query(query)
+        await session.create_table_as(table_details=destination, select_expr=query_expr)
 
     def _validate_columns_and_rename_mapping(self, available_columns: list[str]) -> None:
         referenced_columns = list(self.columns or [])
         referenced_columns += (
             list(self.columns_rename_mapping.keys()) if self.columns_rename_mapping else []
         )
         missing_columns = set(referenced_columns) - set(available_columns)
```

### Comparing `featurebyte-1.0.2/featurebyte/models/scd_table.py` & `featurebyte-1.0.3/featurebyte/models/scd_table.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains SCD table related models
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, List, Tuple, Type, Union
 
 from pydantic import root_validator
 
 from featurebyte.common.validator import construct_data_model_root_validator
@@ -59,14 +60,26 @@
             values["current_flag_column"] = values["current_flag"]
         return values
 
     @property
     def primary_key_columns(self) -> List[str]:
         return [self.natural_key_column]
 
+    @property
+    def special_columns(self) -> List[str]:
+        cols = [
+            self.natural_key_column,
+            self.surrogate_key_column,
+            self.effective_timestamp_column,
+            self.end_timestamp_column,
+            self.current_flag_column,
+            self.record_creation_timestamp_column,
+        ]
+        return [col for col in cols if col]
+
     def create_view_graph_node(
         self,
         input_node: InputNode,
         metadata: Union[ViewMetadata, ChangeViewMetadata],
         **kwargs: Any,
     ) -> Tuple[GraphNode, List[ColumnInfo]]:
         table_data = SCDTableData(**self.dict(by_alias=True)).clone(
```

### Comparing `featurebyte-1.0.2/featurebyte/models/semantic.py` & `featurebyte-1.0.3/featurebyte/models/semantic.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Semantic related models
 """
+
 from typing import List
 
 import pymongo
 
 from featurebyte.models.base import UniqueValuesConstraint
 from featurebyte.models.relationship import Relationship
```

### Comparing `featurebyte-1.0.2/featurebyte/models/sqlglot_expression.py` & `featurebyte-1.0.3/featurebyte/models/sqlglot_expression.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SqlglotExpressionModel class
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 import sqlglot
 from sqlglot.expressions import Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/models/static_source_table.py` & `featurebyte-1.0.3/featurebyte/models/static_source_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTableModel models
 """
+
 from __future__ import annotations
 
 from typing import Union
 from typing_extensions import Annotated
 
 import pymongo
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/models/target.py` & `featurebyte-1.0.3/featurebyte/models/target.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Target related models
 """
+
 from typing import Optional
 
 import pymongo
 from bson import ObjectId
 from pydantic import Field
 
 from featurebyte.common.model_util import parse_duration_string
```

### Comparing `featurebyte-1.0.2/featurebyte/models/target_namespace.py` & `featurebyte-1.0.3/featurebyte/models/target_namespace.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target namespace module
 """
+
 from typing import List, Optional
 
 import pymongo
 from pydantic import Field, validator
 
 from featurebyte.common.validator import construct_sort_validator, duration_string_validator
 from featurebyte.enum import DBVarType
@@ -42,15 +43,15 @@
     window: Optional[str]
 
     # list of IDs attached to this feature namespace or target namespace
     target_ids: List[PydanticObjectId] = Field(allow_mutation=False)
     default_target_id: Optional[PydanticObjectId] = Field(allow_mutation=False)
 
     # pydantic validators
-    _sort_feature_ids_validator = validator("target_ids", "entity_ids", allow_reuse=True)(
+    _sort_ids_validator = validator("target_ids", "entity_ids", allow_reuse=True)(
         construct_sort_validator()
     )
     _duration_validator = validator("window", pre=True, allow_reuse=True)(duration_string_validator)
 
     class Settings(BaseFeatureNamespaceModel.Settings):
         """
         MongoDB settings
```

### Comparing `featurebyte-1.0.2/featurebyte/models/task.py` & `featurebyte-1.0.3/featurebyte/models/task.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Task document model
 """
+
 from typing import Any, Dict, List, Optional
 
 from datetime import datetime
 from uuid import UUID, uuid4
 
 import pymongo
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/models/tile.py` & `featurebyte-1.0.3/featurebyte/models/tile.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains Tile related models
 """
+
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
 from pydantic import Field, root_validator, validator
 
 from featurebyte.enum import InternalName, StrEnum
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
@@ -59,14 +60,15 @@
     tile_id: str
     aggregation_id: str
     aggregation_function_name: Optional[str]
     parent_column_name: Optional[str]
     category_column_name: Optional[str]
     feature_store_id: Optional[ObjectId]
     entity_tracker_table_name: str
+    windows: List[Optional[str]]
 
     class Config:
         """
         Config for pydantic model
         """
 
         arbitrary_types_allowed: bool = True
```

### Comparing `featurebyte-1.0.2/featurebyte/models/tile_job_log.py` & `featurebyte-1.0.3/featurebyte/models/tile_job_log.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TileJobStatus model
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 import pymongo
 from pydantic import StrictStr
```

### Comparing `featurebyte-1.0.2/featurebyte/models/tile_registry.py` & `featurebyte-1.0.3/featurebyte/models/tile_registry.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TileModel document model
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from datetime import datetime
 
 import pymongo
@@ -39,14 +40,24 @@
             values["tile_end_date"] = values["start_date"]
         return values
 
     tile_end_date: datetime
     index: int
 
 
+class BackfillMetadata(BaseModel):
+    """
+    BackfillMetadata class
+
+    Metadata of the tile backfill process when enabling a deployment
+    """
+
+    start_date: datetime
+
+
 class TileModel(FeatureByteCatalogBaseDocumentModel):
     """
     TileModel document
     """
 
     feature_store_id: PydanticObjectId
     tile_id: StrictStr
@@ -59,14 +70,15 @@
 
     frequency_minute: int = Field(gt=0)
     time_modulo_frequency_second: int = Field(ge=0)
     blind_spot_second: int = Field(ge=0)
 
     last_run_metadata_online: Optional[LastRunMetadata]
     last_run_metadata_offline: Optional[LastRunMetadata]
+    backfill_metadata: Optional[BackfillMetadata]
 
     class Settings(FeatureByteCatalogBaseDocumentModel.Settings):
         """
         MongoDB settings
         """
 
         collection_name: str = "tile"
@@ -97,7 +109,8 @@
 class TileUpdate(BaseDocumentServiceUpdateSchema):
     """
     Schema for TileUpdate
     """
 
     last_run_metadata_online: Optional[LastRunMetadata]
     last_run_metadata_offline: Optional[LastRunMetadata]
+    backfill_metadata: Optional[BackfillMetadata]
```

### Comparing `featurebyte-1.0.2/featurebyte/models/user_defined_function.py` & `featurebyte-1.0.3/featurebyte/models/user_defined_function.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,39 +1,41 @@
 """
 This module contains UserDefinedFunction related models
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Union
 
 import pandas as pd
 import pymongo
 from pydantic import Field, validator
 from sqlglot.expressions import select
 from typeguard import check_type, typechecked
 
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import Scalar, Timestamp
 from featurebyte.enum import DBVarType, SourceType
 from featurebyte.models.base import (
     FeatureByteBaseDocumentModel,
     FeatureByteBaseModel,
+    NameStr,
     PydanticObjectId,
     UniqueValuesConstraint,
 )
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node.function import (
     GenericFunctionNode,
     GenericFunctionNodeParameters,
     ValueFunctionParameterInput,
 )
 from featurebyte.query_graph.node.scalar import TimestampValue
 from featurebyte.query_graph.sql.ast.base import SQLNodeContext
 from featurebyte.query_graph.sql.ast.function import GenericFunctionNode as GenericFunctionSQLNode
 from featurebyte.query_graph.sql.common import SQLType
+from featurebyte.typing import Scalar, Timestamp
 
 # supported function parameter input DBVarType to Python type mapping
 function_parameter_dtype_to_python_type = {
     DBVarType.BOOL: bool,
     DBVarType.VARCHAR: str,
     DBVarType.FLOAT: float,
     DBVarType.INT: int,
@@ -208,15 +210,15 @@
         Name of the function used to call in the SQL query
     function_parameters: List[FunctionParameter]
         List of function parameter specification
     catalog_id: Optional[PydanticObjectId]
         Catalog id of the function (if any), if not provided, it can be used across all catalogs
     """
 
-    name: str
+    name: NameStr
     sql_function_name: str
     function_parameters: List[FunctionParameter]
     output_dtype: DBVarType
     signature: str = Field(default_factory=str)
     catalog_id: Optional[PydanticObjectId]
     feature_store_id: PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/persistent/audit.py` & `featurebyte-1.0.3/featurebyte/persistent/audit.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Audit logging for persistent operations
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Tuple
 
 from functools import wraps
 
 import numpy as np
```

### Comparing `featurebyte-1.0.2/featurebyte/persistent/base.py` & `featurebyte-1.0.3/featurebyte/persistent/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,24 +1,15 @@
 """
 Persistent base class
 """
+
 from __future__ import annotations
 
-from typing import (
-    Any,
-    AsyncIterator,
-    Callable,
-    Dict,
-    Iterable,
-    List,
-    Literal,
-    Optional,
-    Tuple,
-    cast,
-)
+from typing import Any, AsyncIterator, Callable, Dict, Iterable, List, Optional, Tuple, cast
+from typing_extensions import Literal
 
 import copy
 from abc import ABC, abstractmethod
 from contextlib import asynccontextmanager
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/persistent/mongo.py` & `featurebyte-1.0.3/featurebyte/persistent/mongo.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Persistent storage using MongoDB
 """
+
 from __future__ import annotations
 
 from typing import Any, AsyncIterator, Dict, Iterable, List, Optional, Tuple, cast
 
 import asyncio
 import copy
 from asyncio import iscoroutine
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/algorithm.py` & `featurebyte-1.0.3/featurebyte/query_graph/algorithm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Generic graph related algorithms
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Set
 
 from featurebyte.query_graph.enum import NodeType
 
 if TYPE_CHECKING:
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/enum.py` & `featurebyte-1.0.3/featurebyte/query_graph/enum.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains all the enums used for query graph.
 """
+
 from typing import Set
 
 from featurebyte.enum import StrEnum
 
 
 class NodeType(StrEnum):
     """
@@ -54,14 +55,15 @@
     AGGREGATE_AS_AT = "aggregate_as_at"
     LOOKUP = "lookup"
     LOOKUP_TARGET = "lookup_target"
     JOIN = "join"
     JOIN_FEATURE = "join_feature"
     TRACK_CHANGES = "track_changes"
     FORWARD_AGGREGATE = "forward_aggregate"
+    FORWARD_AGGREGATE_AS_AT = "forward_aggregate_as_at"
 
     # other operations
     ASSIGN = "assign"
     CONDITIONAL = "conditional"
     ALIAS = "alias"
     IS_NULL = "is_null"
     CAST = "cast"
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/graph.py` & `featurebyte-1.0.3/featurebyte/query_graph/graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 """
 Implement graph data structure for query graph
 """
+
 from typing import (
     Any,
     Callable,
     DefaultDict,
     Dict,
     Iterator,
     List,
-    Literal,
     Optional,
     Set,
     Tuple,
     TypedDict,
     cast,
 )
+from typing_extensions import Literal
 
 from collections import OrderedDict, defaultdict
 
 from bson import ObjectId
 from pydantic import Field
 
 from featurebyte.common.singleton import SingletonMeta
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/graph_node/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/graph_node/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains graph node class.
 """
+
 from typing import Any, Dict, List, Optional, Tuple, cast
 
 from pydantic import BaseModel, parse_obj_as
 
 from featurebyte.query_graph.enum import GraphNodeType, NodeOutputType, NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/column_info.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/column_info.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains column info related models.
 """
+
 from typing import Any, Dict, Optional
 
 from pydantic import Field, root_validator
 
 from featurebyte.enum import DBVarType
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.model.critical_data_info import CriticalDataInfo
@@ -15,36 +16,15 @@
     """
     ColumnInfo for storing column information wth description
     """
 
     description: Optional[str] = Field(default=None)
 
 
-class ColumnInfoWithoutSemanticId(ColumnSpecWithDescription):
-    """
-    ColumnInfo for storing column information
-
-    name: str
-        Column name
-    dtype: DBVarType
-        Variable type of the column
-    entity_id: Optional[PydanticObjectId]
-        Entity id associated with the column
-    critical_data_info: Optional[CriticalDataInfo]
-        Critical data info of the column
-    description: Optional[str]
-        Column description
-    """
-
-    entity_id: Optional[PydanticObjectId] = Field(default=None)
-    critical_data_info: Optional[CriticalDataInfo] = Field(default=None)
-    description: Optional[str] = Field(default=None)
-
-
-class ColumnInfo(ColumnInfoWithoutSemanticId):
+class ColumnInfo(ColumnSpecWithDescription):
     """
     ColumnInfo for storing column information
 
     name: str
         Column name
     dtype: DBVarType
         Variable type of the column
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/common_table.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/common_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """
 This module contains common table related models.
 """
-from typing import Any, Dict, Iterable, List, Literal, Optional, Tuple, TypeVar, cast
+
+from typing import Any, Dict, Iterable, List, Optional, Tuple, TypeVar, cast
+from typing_extensions import Literal
 
 from abc import abstractmethod
 
 from pydantic import Field, validator
 
 from featurebyte.common.validator import columns_info_validator
 from featurebyte.enum import DBVarType, TableDataType
@@ -56,15 +58,15 @@
 
     # pydantic validators
     _validator = validator("columns_info", allow_reuse=True)(columns_info_validator)
 
     def __init_subclass__(cls, **kwargs: Any):
         # add table into DATA_TABLES & SPECIFIC_DATA_TABLES (if not generic type)
         table_type = cls.__fields__["type"]
-        if repr(table_type.type_).startswith("typing.Literal"):
+        if "Literal" in repr(table_type.type_):
             DATA_TABLES.append(cls)
         if table_type.default != TableDataType.SOURCE_TABLE:
             SPECIFIC_DATA_TABLES.append(cls)
 
     @property
     def column_cleaning_operations(self) -> List[ColumnCleaningOperation]:
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/critical_data_info.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/critical_data_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains critical data info related models.
 """
+
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import validator
 
 from featurebyte.exception import InvalidImputationsError
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/entity_lookup_plan.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/entity_lookup_plan.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Parent / child entity lookup related models
 """
+
 from __future__ import annotations
 
 from typing import Dict, Iterator, List, Optional, Sequence, Set, Tuple
 
 import copy
 from collections import defaultdict
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/entity_relationship_info.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/entity_relationship_info.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains entity relationship info related classes.
 """
+
 from typing import Dict, List, Optional, Sequence, Set
 
 from collections import defaultdict
 from dataclasses import dataclass
 
 from bson import ObjectId
 from pydantic import Field
@@ -246,7 +247,38 @@
             all_ancestors_ids.update(self.entity_id_to_ancestor_ids[entity_id])
 
         reduced_entity_ids = set()
         for entity_id in entity_ids:
             if entity_id not in all_ancestors_ids:
                 reduced_entity_ids.add(entity_id)
         return sorted(reduced_entity_ids)
+
+    def keep_related_entity_ids(
+        self,
+        entity_ids_to_filter: Sequence[ObjectId],
+        filter_by: Sequence[ObjectId],
+    ) -> List[ObjectId]:
+        """
+        Filter a list of entity ids to include only entity ids that are related to filter_by
+
+        Parameters
+        ----------
+        entity_ids_to_filter: Sequence[ObjectId]
+            List of entity ids to be filtered, e.g. a serving entity ids of a feature list
+        filter_by: Sequence[ObjectId]
+            Entity ids to filter by, e.g. the primary entity ids of an offline store feature table
+
+        Returns
+        -------
+        List[ObjectId]
+        """
+        related_entity_ids = set(filter_by)
+        for entity_id in filter_by:
+            related_entity_ids.update(self.entity_id_to_ancestor_ids[entity_id])
+            related_entity_ids.update(self.entity_id_to_descendant_ids[entity_id])
+
+        filtered_entity_ids = set()
+        for entity_id in entity_ids_to_filter:
+            if entity_id in related_entity_ids:
+                filtered_entity_ids.add(entity_id)
+
+        return sorted(filtered_entity_ids)
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/feature_job_setting.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/feature_job_setting.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature Job Setting Model
 """
+
 from typing import Any, Dict
 
 from pydantic import Field, root_validator
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.model_util import parse_duration_string, validate_job_setting_parameters
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/graph.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This model contains query graph internal model structures
 """
+
 from typing import Any, DefaultDict, Dict, Iterator, List, Optional, Set, Tuple, cast
 
 from collections import defaultdict
 
 from pydantic import Field, PrivateAttr, root_validator, validator
 
 from featurebyte.exception import GraphInconsistencyError
@@ -444,14 +445,34 @@
         output_column_name = None
         if isinstance(node, AliasNode):
             output_column_name = cast(str, node.parameters.name)
         elif isinstance(node, ProjectNode):
             output_column_name = cast(str, node.parameters.columns[0])
         return output_column_name
 
+    def has_node_type(self, target_node: Node, node_type: NodeType) -> bool:
+        """
+        Check if the query sub-graph has a specific node type
+
+        Parameters
+        ----------
+        target_node: Node
+            Target node used to start the search
+        node_type: NodeType
+            Node type to check
+
+        Returns
+        -------
+        bool
+            True if the query sub-graph has a request column node, False otherwise
+        """
+        for _ in self.iterate_nodes(target_node=target_node, node_type=node_type):
+            return True
+        return False
+
     def iterate_nodes(
         self,
         target_node: Node,
         node_type: Optional[NodeType],
         skip_node_type: Optional[NodeType] = None,
         skip_node_names: Optional[Set[str]] = None,
     ) -> Iterator[Node]:
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/model/table.py` & `featurebyte-1.0.3/featurebyte/query_graph/model/table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 This module contains specialized table related models.
 """
-from typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional, Tuple, Union
-from typing_extensions import Annotated  # pylint: disable=wrong-import-order
+
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
+from typing_extensions import Annotated, Literal
 
 from dataclasses import dataclass
 
 from bson import ObjectId
 from pydantic import Field, StrictStr, parse_obj_as
 
 from featurebyte.common.join_utils import (
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/__init__.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Query graph node related classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
 from typing import TYPE_CHECKING, Any, Union
 from typing_extensions import Annotated
 
 from pydantic import Field, parse_obj_as
 
 from featurebyte.common.path_util import import_submodules
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/agg_func.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/agg_func.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 Aggregation method model
 """
-from typing import TYPE_CHECKING, Any, ClassVar, Dict, Literal, Optional, Union
-from typing_extensions import Annotated
+
+from typing import TYPE_CHECKING, Any, ClassVar, Dict, Optional, Union
+from typing_extensions import Annotated, Literal
 
 from abc import abstractmethod  # pylint: disable=wrong-import-order
 
 from pydantic import BaseModel, Field, parse_obj_as
 
 from featurebyte.enum import AggFunc, DBVarType
 
@@ -15,15 +16,15 @@
 
 class BaseAggFunc(BaseModel):
     """BaseAggMethod class"""
 
     type: AggFunc
 
     def __init_subclass__(cls, **kwargs: Any):
-        if repr(cls.__fields__["type"].type_).startswith("typing.Literal"):
+        if "Literal" in repr(cls.__fields__["type"].type_):
             # only add agg method class to AGG_FUNCS if the type variable is a literal (to filter out base classes)
             AGG_FUNCS.append(cls)
 
     def derive_output_var_type(
         self, input_var_type: DBVarType, category: Optional[str] = None
     ) -> DBVarType:
         """
@@ -46,16 +47,15 @@
         if category:
             return DBVarType.OBJECT
         return self._derive_output_var_type(input_var_type=input_var_type, category=category)
 
     @abstractmethod
     def _derive_output_var_type(
         self, input_var_type: DBVarType, category: Optional[str] = None
-    ) -> DBVarType:
-        ...
+    ) -> DBVarType: ...
 
     @abstractmethod
     def is_var_type_supported(self, input_var_type: DBVarType) -> bool:
         """
         Check whether the input var type is supported
 
         Parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base classes required for constructing query graph nodes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
 # pylint: disable=too-many-lines
 from typing import (
     Any,
     Callable,
     ClassVar,
     Dict,
@@ -104,19 +105,19 @@
         extra = "forbid"
 
     def __init__(self, **kwargs: Any):
         super().__init__(**kwargs)
 
         # make sure subclass set certain properties correctly
         assert self.__fields__["type"].field_info.const is True
-        assert repr(self.__fields__["type"].type_).startswith("typing.Literal")
+        assert "Literal" in repr(self.__fields__["type"].type_)
         assert self.__fields__["output_type"].type_ is NodeOutputType
 
     def __init_subclass__(cls, **kwargs: Any):
-        if repr(cls.__fields__["type"].type_).startswith("typing.Literal"):
+        if "Literal" in repr(cls.__fields__["type"].type_):
             # only add node type class to NODE_TYPES if the type variable is a literal (to filter out base classes)
             NODE_TYPES.append(cls)
 
     @property
     def transform_info(self) -> str:
         """
         Construct from node transform object from this node
@@ -834,14 +835,41 @@
             ):
                 node_params[param_name] = self._normalize_nested_parameters(
                     nested_parameters=value, input_node_column_mappings=input_node_column_mappings
                 )
 
         return self.clone(parameters=node_params), output_column_remap
 
+    @staticmethod
+    def _to_datetime_expr(
+        var_name_expr: VarNameExpressionStr, to_handle_none: bool = True, **to_datetime_kwargs: Any
+    ) -> ExpressionStr:
+        """
+        Convert input variable name expression to datetime expression
+
+        Parameters
+        ----------
+        var_name_expr: VarNameExpressionStr
+            Variable name expression
+        to_handle_none: bool
+            Whether to handle null values
+        to_datetime_kwargs: Any
+            Additional keyword arguments for pd.to_datetime function
+
+        Returns
+        -------
+        ExpressionStr
+        """
+        to_dt_expr = get_object_class_from_function_call(
+            "pd.to_datetime", var_name_expr, utc=True, **to_datetime_kwargs
+        )
+        if to_handle_none:
+            return ExpressionStr(f"pd.NaT if {var_name_expr} is None else {to_dt_expr}")
+        return ExpressionStr(to_dt_expr)
+
 
 class SeriesOutputNodeOpStructMixin:
     """SeriesOutputNodeOpStructMixin class"""
 
     name: str
     transform_info: str
     output_type: NodeOutputType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/binary.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/binary.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """
 This module contains binary operation node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import List, Literal, Sequence, Tuple
+from typing import List, Sequence, Tuple
+from typing_extensions import Literal
 
 from pydantic import Field
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.base import (
     BaseSeriesOutputWithAScalarParamNode,
@@ -144,14 +146,18 @@
 
 
 class DivideNode(BinaryArithmeticOpNode):
     """DivideNode class"""
 
     type: Literal[NodeType.DIV] = Field(NodeType.DIV, const=True)
 
+    def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
+        _ = inputs
+        return DBVarType.FLOAT
+
     def generate_expression(self, left_operand: str, right_operand: str) -> str:
         return f"{left_operand} / {right_operand}"
 
     def generate_odfv_expression(self, left_operand: str, right_operand: str) -> str:
         return f"np.divide({left_operand}, {right_operand})"
 
     def generate_udf_expression(self, left_operand: str, right_operand: str) -> str:
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/cleaning_operation.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/cleaning_operation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 """
 This module contains cleaning operation related classes.
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import TYPE_CHECKING, Any, ClassVar, List, Literal, Optional, Sequence, Set, Union
-from typing_extensions import Annotated
+from typing import TYPE_CHECKING, Any, ClassVar, List, Optional, Sequence, Set, Union
+from typing_extensions import Annotated, Literal
 
 from abc import abstractmethod  # pylint: disable=wrong-import-order
 
 import pandas as pd
 from pydantic import Field, validator
 
 from featurebyte.common.doc_util import FBAutoDoc
-from featurebyte.common.typing import OptionalScalar, Scalar
 from featurebyte.enum import DBVarType, StrEnum
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.node.metadata.sdk_code import ClassEnum, ObjectClass
 from featurebyte.query_graph.node.validator import construct_unique_name_validator
+from featurebyte.typing import OptionalScalar, Scalar
 
 if TYPE_CHECKING:
     from featurebyte.query_graph.graph_node.base import GraphNode
     from featurebyte.query_graph.node import Node
 
 
 class ConditionOperationField(StrEnum):
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/count_dict.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/count_dict.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 """
 This module contains datetime operation related node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import Callable, ClassVar, Dict, List, Literal, Optional, Sequence, Set, Tuple, Union
-from typing_extensions import Annotated
+from typing import Callable, ClassVar, Dict, List, Optional, Sequence, Set, Tuple, Union
+from typing_extensions import Annotated, Literal
 
 import textwrap  # pylint: disable=wrong-import-order
 from abc import ABC, abstractmethod  # pylint: disable=wrong-import-order
 
+import numpy as np
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import Scalar
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.agg_func import construct_agg_func
 from featurebyte.query_graph.node.base import BaseSeriesOutputNode
 from featurebyte.query_graph.node.metadata.config import (
     OnDemandFunctionCodeGenConfig,
     OnDemandViewCodeGenConfig,
@@ -30,14 +31,15 @@
     ValueStr,
     VariableNameGenerator,
     VariableNameStr,
     VarNameExpressionInfo,
     get_object_class_from_function_call,
 )
 from featurebyte.query_graph.sql.common import MISSING_VALUE_REPLACEMENT
+from featurebyte.typing import Scalar
 
 
 class BaseCountDictOpNode(BaseSeriesOutputNode, ABC):
     """BaseCountDictOpNode class"""
 
     @property
     def max_input_count(self) -> int:
@@ -439,25 +441,77 @@
         config: OnDemandFunctionCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         input_var_name_expressions = self._assert_no_info_dict(node_inputs)
         var_name: str = input_var_name_expressions[0].as_input()
         return [], ExpressionStr(f"np.nan if pd.isna({var_name}) else list({var_name}.keys())")
 
 
-class GetValueFromDictionaryNode(BaseCountDictOpNode):
-    """Get value from dictionary node class"""
+class BaseCountDictWithKeyOpNode(BaseCountDictOpNode, ABC):
+    """Base class for count dictionary operation with key"""
 
     class Parameters(BaseModel):
         """Parameters"""
 
         value: Optional[Scalar]
 
-    type: Literal[NodeType.GET_VALUE] = Field(NodeType.GET_VALUE, const=True)
     parameters: Parameters
 
+    def get_key_value(self) -> ValueStr:
+        """
+        Get key value as ValueStr from node parameters
+
+        Returns
+        -------
+        ValueStr
+        """
+        # Note that the key of the dictionary/map is always a string for all supported data warehouses.
+        if isinstance(self.parameters.value, (int, float, np.integer, np.floating)):
+            param = ValueStr.create(str(int(self.parameters.value)))
+        else:
+            # If it is a string, it is already quoted.
+            # If it is other types, the dictionary lookup will return null.
+            param = ValueStr.create(self.parameters.value)
+        return param
+
+    @staticmethod
+    def get_key_value_func_name(
+        var_name_generator: VariableNameGenerator,
+    ) -> Tuple[List[StatementT], str]:
+        """
+        Create a function statement & get the function name for getting key value
+
+        Parameters
+        ----------
+        var_name_generator: VariableNameGenerator
+            Variable name generator
+
+        Returns
+        -------
+        Tuple[List[StatementT], str]
+        """
+        statements: List[StatementT] = []
+        func_name = "get_key_value"
+        if var_name_generator.should_insert_function(function_name=func_name):
+            func_string = f"""
+            def {func_name}(key):
+                if pd.isna(key):
+                    return key
+                if isinstance(key, (int, float, np.integer, np.floating)):
+                    return str(int(key))
+                return key
+            """
+            statements.append(StatementStr(textwrap.dedent(func_string)))
+        return statements, func_name
+
+
+class GetValueFromDictionaryNode(BaseCountDictWithKeyOpNode):
+    """Get value from dictionary node class"""
+
+    type: Literal[NodeType.GET_VALUE] = Field(NodeType.GET_VALUE, const=True)
+
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         aggregations = inputs[0].aggregations
         agg_column = aggregations[0]
         # This assumes that dictionary features are never post-processed.
         assert isinstance(agg_column, AggregationColumn)
         method = agg_column.method
         assert method is not None
@@ -466,93 +520,100 @@
         # as count method doesn't have any parent column, take the first input column as parent column
         parent_column = agg_column.column
         if parent_column is None:
             parent_column = inputs[0].columns[0]
         return agg_func.derive_output_var_type(parent_column.dtype, category=None)
 
     def generate_expression(self, operand: str, other_operands: List[str]) -> str:
-        param = other_operands[0] if other_operands else ValueStr.create(self.parameters.value)
+        param = other_operands[0] if other_operands else self.get_key_value()
         return f"{operand}.cd.get_value(key={param})"
 
     def _derive_on_demand_view_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandViewCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         input_var_name_expressions = self._assert_no_info_dict(node_inputs)
         var_name: str = input_var_name_expressions[0].as_input()
         value_expr: Union[str, ObjectClass]
+        statements: List[StatementT] = []
         if len(node_inputs) == 1:
-            param = ValueStr.create(self.parameters.value)
             value_expr = get_object_class_from_function_call(
                 f"{var_name}.apply",
-                ExpressionStr(f"lambda x: np.nan if pd.isna(x) else x.get({param})"),
+                ExpressionStr(f"lambda x: np.nan if pd.isna(x) else x.get({self.get_key_value()})"),
             )
         else:
+            func_statements, key_func_name = self.get_key_value_func_name(var_name_generator)
+            statements.extend(func_statements)
             operand: str = input_var_name_expressions[1].as_input()
             value_expr = (
-                f"{var_name}.combine({operand}, lambda x, y: np.nan if pd.isna(x) else x.get(y))"
+                f"{var_name}.combine({operand}, "
+                f"lambda x, y: np.nan if pd.isna(x) or pd.isna(y) else x.get({key_func_name}(y)))"
             )
 
-        return [], ExpressionStr(value_expr)
+        return statements, ExpressionStr(value_expr)
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandFunctionCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         input_var_name_expressions = self._assert_no_info_dict(node_inputs)
         var_name: str = input_var_name_expressions[0].as_input()
+        statements: List[StatementT] = []
         if len(node_inputs) == 1:
-            param = ValueStr.create(self.parameters.value)
             value_expr = ExpressionStr(
-                f"np.nan if pd.isna({var_name}) else {var_name}.get({param})"
+                f"np.nan if pd.isna({var_name}) else {var_name}.get({self.get_key_value()})"
             )
         else:
+            func_statements, key_func_name = self.get_key_value_func_name(var_name_generator)
+            statements.extend(func_statements)
             operand: str = input_var_name_expressions[1].as_input()
             value_expr = ExpressionStr(
-                f"np.nan if pd.isna({var_name}) else {var_name}.get({operand})"
+                f"np.nan if pd.isna({var_name}) else {var_name}.get({key_func_name}({operand}))"
             )
 
-        return [], value_expr
+        return statements, value_expr
 
 
-class GetRankFromDictionaryNode(BaseCountDictOpNode):
+class GetRankFromDictionaryNode(BaseCountDictWithKeyOpNode):
     """Get rank from dictionary node class"""
 
-    class Parameters(BaseModel):
+    class Parameters(BaseCountDictWithKeyOpNode.Parameters):
         """Parameters"""
 
-        value: Optional[Scalar]
         descending: bool = False
 
     type: Literal[NodeType.GET_RANK] = Field(NodeType.GET_RANK, const=True)
     parameters: Parameters
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.FLOAT
 
     def generate_expression(self, operand: str, other_operands: List[str]) -> str:
-        key = other_operands[0] if other_operands else self.parameters.value
+        key = other_operands[0] if other_operands else self.get_key_value()
         descending = ValueStr.create(self.parameters.descending)
         params = f"key={key}, descending={descending}"
         return f"{operand}.cd.get_value(key={params})"
 
     @staticmethod
     def _get_rank_func_name(
         var_name_generator: VariableNameGenerator,
     ) -> Tuple[List[StatementT], str]:
         statements: List[StatementT] = []
         func_name = "get_rank"
         if var_name_generator.should_insert_function(function_name=func_name):
             func_string = f"""
             def {func_name}(input_dict, key, is_descending):
-                if pd.isna(input_dict) or key not in input_dict:
+                if pd.isna(input_dict) or pd.isna(key):
+                    return np.nan
+                key = str(int(key)) if isinstance(key, (int, float, np.integer, np.floating)) else key
+                if key not in input_dict:
                     return np.nan
                 sorted_values = sorted(input_dict.values(), reverse=is_descending)
                 return sorted_values.index(input_dict[key]) + 1
             """
             statements.append(StatementStr(textwrap.dedent(func_string)))
         return statements, func_name
 
@@ -563,27 +624,29 @@
         config: OnDemandViewCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         input_var_name_expressions = self._assert_no_info_dict(node_inputs)
         var_name: str = input_var_name_expressions[0].as_input()
         descending = ValueStr.create(self.parameters.descending)
         statements, func_name = self._get_rank_func_name(var_name_generator)
         if len(node_inputs) == 1:
-            param = ValueStr.create(self.parameters.value)
             rank_expr = ExpressionStr(
                 get_object_class_from_function_call(
                     f"{var_name}.apply",
                     ExpressionStr(
-                        f"lambda dct: {func_name}(dct, key={param}, is_descending={descending})"
+                        f"lambda dct: {func_name}(dct, key={self.get_key_value()}, is_descending={descending})"
                     ),
                 )
             )
         else:
+            func_statements, key_func_name = self.get_key_value_func_name(var_name_generator)
+            statements.extend(func_statements)
             operand: str = input_var_name_expressions[1].as_input()
             rank_expr = ExpressionStr(
-                f"{var_name}.combine({operand}, lambda dct, key: {func_name}(dct, key=key, is_descending={descending}))"
+                f"{var_name}.combine({operand}, "
+                f"lambda dct, key: {func_name}(dct, key={key_func_name}(key), is_descending={descending}))"
             )
 
         return statements, rank_expr
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
@@ -591,57 +654,55 @@
         config: OnDemandFunctionCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         input_var_name_expressions = self._assert_no_info_dict(node_inputs)
         var_name: str = input_var_name_expressions[0].as_input()
         descending = ValueStr.create(self.parameters.descending)
         statements, func_name = self._get_rank_func_name(var_name_generator)
         if len(node_inputs) == 1:
-            param = ValueStr.create(self.parameters.value)
             rank_expr = ExpressionStr(
-                f"{func_name}({var_name}, key={param}, is_descending={descending})"
+                f"{func_name}({var_name}, key={self.get_key_value()}, is_descending={descending})"
             )
         else:
+            func_statements, key_func_name = self.get_key_value_func_name(var_name_generator)
+            statements.extend(func_statements)
             operand: str = input_var_name_expressions[1].as_input()
             rank_expr = ExpressionStr(
-                f"{func_name}({var_name}, key={operand}, is_descending={descending})"
+                f"{func_name}({var_name}, key={key_func_name}({operand}), is_descending={descending})"
             )
 
         return statements, rank_expr
 
 
-class GetRelativeFrequencyFromDictionaryNode(BaseCountDictOpNode):
+class GetRelativeFrequencyFromDictionaryNode(BaseCountDictWithKeyOpNode):
     """Get relative frequency from dictionary node class"""
 
-    class Parameters(BaseModel):
-        """Parameters"""
-
-        value: Optional[Scalar]
-
     type: Literal[NodeType.GET_RELATIVE_FREQUENCY] = Field(
         NodeType.GET_RELATIVE_FREQUENCY, const=True
     )
-    parameters: Parameters
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.FLOAT
 
     def generate_expression(self, operand: str, other_operands: List[str]) -> str:
-        param = other_operands[0] if other_operands else ValueStr.create(self.parameters.value)
+        param = other_operands[0] if other_operands else self.get_key_value()
         return f"{operand}.cd.get_relative_frequency(key={param})"
 
     @staticmethod
     def _get_relative_frequency_func_name(
         var_name_generator: VariableNameGenerator,
     ) -> Tuple[List[StatementT], str]:
         statements: List[StatementT] = []
         func_name = "get_relative_frequency"
         if var_name_generator.should_insert_function(function_name=func_name):
             func_string = f"""
             def {func_name}(input_dict, key):
-                if pd.isna(input_dict) or key not in input_dict:
+                if pd.isna(input_dict) or pd.isna(key):
+                    return np.nan
+                key = str(int(key)) if isinstance(key, (int, float, np.integer, np.floating)) else key
+                if key not in input_dict:
                     return np.nan
                 total_count = sum(input_dict.values())
                 if total_count == 0:
                     return 0
                 key_frequency = input_dict.get(key, 0)
                 return key_frequency / total_count
             """
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/date.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/date.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 """
 This module contains datetime operation related node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import Callable, Dict, List, Literal, Optional, Sequence, Tuple, Union
+from typing import Callable, Dict, List, Optional, Sequence, Tuple, Union
+from typing_extensions import Literal
 
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import DatetimeSupportedPropertyType, TimedeltaSupportedUnitType
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.node.base import (
     BaseSeriesOutputNode,
     BaseSeriesOutputWithSingleOperandNode,
 )
 from featurebyte.query_graph.node.metadata.config import (
@@ -26,14 +27,15 @@
     StatementT,
     ValueStr,
     VariableNameGenerator,
     VariableNameStr,
     VarNameExpressionInfo,
     get_object_class_from_function_call,
 )
+from featurebyte.typing import DatetimeSupportedPropertyType, TimedeltaSupportedUnitType
 
 
 class DatetimeExtractNode(BaseSeriesOutputNode):
     """DatetimeExtractNode class"""
 
     class Parameters(BaseModel):
         """Parameters"""
@@ -103,24 +105,24 @@
             delta_val = f"{self.parameters.timezone_offset}:00"
             delta = get_object_class_from_function_call("pd.to_timedelta", delta_val)
             offset_operand = var_name_generator.convert_to_variable_name(
                 variable_name_prefix="tz_offset", node_name=None
             )
             statements.append((offset_operand, delta))
         elif len(var_name_expressions) == 2:
-            offset_operand = var_name_expressions[1].as_input()
+            offset_operand = f"pd.to_timedelta({var_name_expressions[1].as_input()})"
         else:
             offset_operand = None
 
         dt_var_name: Union[str, VariableNameStr]
         if offset_operand:
             dt_var_name = var_name_generator.convert_to_variable_name(
                 variable_name_prefix=offset_adj_var_name_prefix, node_name=None
             )
-            expr = ExpressionStr(f"{ts_operand} + {offset_operand}")
+            expr = ExpressionStr(f"pd.to_datetime({ts_operand}) + {offset_operand}")
             statements.append((dt_var_name, expr))
         else:
             dt_var_name = ts_operand
 
         output = ExpressionStr(expr_func(dt_var_name, self.parameters.property))
         return statements, output
 
@@ -130,28 +132,28 @@
         var_name_generator: VariableNameGenerator,
         config: OnDemandViewCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         return self._derive_on_demand_view_or_user_defined_function_helper(
             node_inputs,
             var_name_generator,
             offset_adj_var_name_prefix="feat_dt",
-            expr_func=lambda dt_var_name, prop: f"{dt_var_name}.dt.{prop}",
+            expr_func=lambda dt_var_name, prop: f"pd.to_datetime({dt_var_name}).dt.{prop}",
         )
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandFunctionCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         return self._derive_on_demand_view_or_user_defined_function_helper(
             node_inputs,
             var_name_generator,
             offset_adj_var_name_prefix="feat",
-            expr_func=lambda dt_var_name, prop: f"{dt_var_name}.{prop}",
+            expr_func=lambda dt_var_name, prop: f"pd.to_datetime({dt_var_name}).{prop}",
         )
 
 
 class TimeDeltaExtractNode(BaseSeriesOutputWithSingleOperandNode):
     """TimeDeltaExtractNode class"""
 
     class Parameters(BaseModel):
@@ -182,31 +184,31 @@
         return DBVarType.FLOAT
 
     def generate_expression(self, operand: str) -> str:
         return f"{operand}.dt.{self.parameters.property}"
 
     def generate_odfv_expression(self, operand: str) -> str:
         if self.parameters.property == "millisecond":
-            return f"1e3 * {operand}.dt.total_seconds()"
+            return f"1e3 * pd.to_timedelta({operand}).dt.total_seconds()"
         if self.parameters.property == "microsecond":
-            return f"1e6 * {operand}.dt.total_seconds()"
+            return f"1e6 * pd.to_timedelta({operand}).dt.total_seconds()"
         if self.parameters.property == "second":
-            return f"{operand}.dt.total_seconds()"
+            return f"pd.to_timedelta({operand}).dt.total_seconds()"
 
-        return f"{operand}.dt.total_seconds() // {self.unit_to_seconds[self.parameters.property]}"
+        return f"pd.to_timedelta({operand}).dt.total_seconds() // {self.unit_to_seconds[self.parameters.property]}"
 
     def generate_udf_expression(self, operand: str) -> str:
         if self.parameters.property == "millisecond":
-            return f"1e3 * {operand}.total_seconds()"
+            return f"1e3 * pd.to_timedelta({operand}).total_seconds()"
         if self.parameters.property == "microsecond":
-            return f"1e6 * {operand}.total_seconds()"
+            return f"1e6 * pd.to_timedelta({operand}).total_seconds()"
         if self.parameters.property == "second":
-            return f"{operand}.total_seconds()"
+            return f"pd.to_timedelta({operand}).total_seconds()"
 
-        return f"{operand}.total_seconds() // {self.unit_to_seconds[self.parameters.property]}"
+        return f"pd.to_timedelta({operand}).total_seconds() // {self.unit_to_seconds[self.parameters.property]}"
 
 
 class DateDifferenceNode(BaseSeriesOutputNode):
     """DateDifferenceNode class"""
 
     type: Literal[NodeType.DATE_DIFF] = Field(NodeType.DATE_DIFF, const=True)
 
@@ -221,52 +223,60 @@
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.TIMEDELTA
 
     def _derive_python_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
+        sdk_code: bool,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         if len(node_inputs) == 1:
             # we don't allow subtracting timestamp with a scalar timedelta through SDK
             raise RuntimeError("DateAddNode with only one input is not supported")
 
         var_name_expressions = self._assert_no_info_dict(node_inputs)
         left_operand = var_name_expressions[0].as_input()
         right_operand = var_name_expressions[1].as_input()
-        return [], ExpressionStr(f"{left_operand} - {right_operand}")
+        statements: List[StatementT] = []
+        if sdk_code:
+            expr = ExpressionStr(f"{left_operand} - {right_operand}")
+        else:
+            expr = ExpressionStr(
+                f"pd.to_datetime({left_operand}) - pd.to_datetime({right_operand})"
+            )
+        return statements, expr
 
     def _derive_sdk_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         operation_structure: OperationStructure,
         config: SDKCodeGenConfig,
         context: CodeGenerationContext,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         _ = var_name_generator, operation_structure, config, context
-        return self._derive_python_code(node_inputs)
+        return self._derive_python_code(node_inputs, sdk_code=True)
 
     def _derive_on_demand_view_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandViewCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         _ = var_name_generator, config
-        return self._derive_python_code(node_inputs)
+        return self._derive_python_code(node_inputs, sdk_code=False)
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandFunctionCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         _ = var_name_generator, config
-        return self._derive_python_code(node_inputs)
+        return self._derive_python_code(node_inputs, sdk_code=False)
 
 
 class TimeDeltaNode(BaseSeriesOutputNode):
     """TimeDeltaNode class"""
 
     class Parameters(BaseModel):
         """Parameters"""
@@ -387,45 +397,53 @@
             # in this case, we should derive the var type from inputs[0].aggregations
             return inputs[0].aggregations[0].dtype
         return inputs[0].columns[0].dtype
 
     def _derive_python_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
+        sdk_code: bool,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         if len(node_inputs) == 1:
             # we don't allow adding timestamp with a scalar timedelta through SDK
             raise RuntimeError("DateAddNode with only one input is not supported")
 
         var_name_expressions = self._assert_no_info_dict(node_inputs)
         left_operand: str = var_name_expressions[0].as_input()
         right_operand = var_name_expressions[1].as_input()
-        return [], ExpressionStr(f"{left_operand} + {right_operand}")
+        statements: List[StatementT] = []
+        if sdk_code:
+            expr = ExpressionStr(f"{left_operand} + {right_operand}")
+        else:
+            expr = ExpressionStr(
+                f"pd.to_datetime({left_operand}) + pd.to_timedelta({right_operand})"
+            )
+        return statements, expr
 
     def _derive_sdk_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         operation_structure: OperationStructure,
         config: SDKCodeGenConfig,
         context: CodeGenerationContext,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         _ = var_name_generator, operation_structure, config, context
-        return self._derive_python_code(node_inputs)
+        return self._derive_python_code(node_inputs, sdk_code=True)
 
     def _derive_on_demand_view_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandViewCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         _ = var_name_generator, config
-        return self._derive_python_code(node_inputs)
+        return self._derive_python_code(node_inputs, sdk_code=False)
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandFunctionCodeGenConfig,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         _ = var_name_generator, config
-        return self._derive_python_code(node_inputs)
+        return self._derive_python_code(node_inputs, sdk_code=False)
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/distance.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/distance.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """
 Distance node module
 """
-from typing import List, Literal, Sequence, Tuple
+
+from typing import List, Sequence, Tuple
+from typing_extensions import Literal
 
 import textwrap
 
 from pydantic import BaseModel, Field
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/function.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/function.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,43 +1,48 @@
 """
 This module contains generic function related node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple, Union, cast
-from typing_extensions import Annotated
+from typing import Any, Dict, List, Optional, Sequence, Tuple, Union, cast
+from typing_extensions import Annotated, Literal
 
 from abc import abstractmethod  # pylint: disable=wrong-import-order
 
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import Scalar
 from featurebyte.enum import DBVarType, FunctionParameterInputForm
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.node.base import BaseSeriesOutputNode
-from featurebyte.query_graph.node.metadata.config import SDKCodeGenConfig
+from featurebyte.query_graph.node.metadata.config import (
+    OnDemandFunctionCodeGenConfig,
+    SDKCodeGenConfig,
+)
 from featurebyte.query_graph.node.metadata.operation import (
     DerivedDataColumn,
     NodeOutputCategory,
     OperationStructure,
     OperationStructureInfo,
     PostAggregationColumn,
 )
 from featurebyte.query_graph.node.metadata.sdk_code import (
     ClassEnum,
     CodeGenerationContext,
     CommentStr,
+    ExpressionStr,
     InfoDict,
     ObjectClass,
     StatementT,
     VariableNameGenerator,
     VarNameExpressionInfo,
     get_object_class_from_function_call,
 )
 from featurebyte.query_graph.node.scalar import TimestampValue, ValueParameterType
+from featurebyte.typing import Scalar
 
 SDKFunctionArgument = Union[VarNameExpressionInfo, Scalar, ObjectClass]
 
 
 class BaseFunctionParameterInput(BaseModel):
     """BaseFunctionParameterInput class"""
 
@@ -257,7 +262,21 @@
             node_output_category=operation_structure.output_category,
             node_output_type=operation_structure.output_type,
             node_name=self.name,
         )
         expression = get_object_class_from_function_call(udf_var_name, *function_parameters)
         statements.append((out_var_name, expression))
         return statements, out_var_name
+
+    def _derive_user_defined_function_code(
+        self,
+        node_inputs: List[VarNameExpressionInfo],
+        var_name_generator: VariableNameGenerator,
+        config: OnDemandFunctionCodeGenConfig,
+    ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
+        if config.to_generate_null_filling_function:
+            # NOTE: We make an assumption that all UDFs does not have null filling capability,
+            # any null input(s) will result in null output.
+            if self.parameters.output_dtype in DBVarType.supported_timestamp_types():
+                return [], ExpressionStr("pd.NaT")
+            return [], ExpressionStr("np.nan")
+        raise NotImplementedError("User defined function is not supported for GenericFunctionNode")
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/generic.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/generic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 This module contains SQL operation related node classes
 """
+
 # pylint: disable=too-many-lines
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import Any, ClassVar, Dict, List, Literal, Optional, Sequence, Set, Tuple, Union
+from typing import Any, ClassVar, Dict, List, Optional, Sequence, Set, Tuple, Union
+from typing_extensions import Literal
 
 from pydantic import BaseModel, Field, root_validator, validator
 
 from featurebyte.common.model_util import parse_duration_string
 from featurebyte.enum import DBVarType
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
@@ -617,14 +619,15 @@
         return [
             AggregationColumn(
                 name=self.parameters.name,
                 method=self.parameters.agg_func,
                 keys=self.parameters.keys,
                 window=self.parameters.window,
                 category=self.parameters.value_by,
+                offset=None,
                 column=col_name_map.get(self.parameters.parent),
                 filter=any(col.filter for col in columns),
                 aggregation_type=self.type,
                 node_names={node_name}.union(other_node_names),
                 node_name=node_name,
                 dtype=output_var_type,
             )
@@ -728,14 +731,15 @@
         return [
             AggregationColumn(
                 name=name,
                 method=self.parameters.agg_func,
                 keys=self.parameters.keys,
                 window=window,
                 category=self.parameters.value_by,
+                offset=None,
                 column=col_name_map.get(self.parameters.parent),
                 filter=any(col.filter for col in columns),
                 aggregation_type=self.type,
                 node_names={node_name}.union(other_node_names),
                 node_name=node_name,
                 dtype=output_var_type,
             )
@@ -887,14 +891,15 @@
         return [
             AggregationColumn(
                 name=self.parameters.name,
                 method=self.parameters.agg_func,
                 keys=self.parameters.keys,
                 window=None,
                 category=self.parameters.value_by,
+                offset=None,
                 column=col_name_map.get(self.parameters.parent),
                 filter=any(col.filter for col in columns),
                 aggregation_type=self.type,
                 node_names={node_name}.union(other_node_names),
                 node_name=node_name,
                 dtype=output_var_type,
             )
@@ -1024,21 +1029,25 @@
         self,
         columns: List[ViewDataColumn],
         node_name: str,
         other_node_names: Set[str],
         output_var_type: DBVarType,
     ) -> List[AggregationColumn]:
         name_to_column = {col.name: col for col in columns}
+        offset = None
+        if self.parameters.scd_parameters:
+            offset = self.parameters.scd_parameters.offset
         return [
             AggregationColumn(
                 name=feature_name,
                 method=None,
                 keys=[self.parameters.entity_column],
                 window=None,
                 category=None,
+                offset=offset,
                 column=name_to_column[input_column_name],
                 aggregation_type=self.type,  # type: ignore[arg-type]
                 node_names={node_name}.union(other_node_names),
                 node_name=node_name,
                 filter=any(col.filter for col in columns),
                 dtype=name_to_column[input_column_name].dtype,
             )
@@ -1150,17 +1159,17 @@
             var_name_generator=var_name_generator,
             node_output_type=NodeOutputType.FRAME,
             node_output_category=NodeOutputCategory.TARGET,
             to_associate_with_node_name=False,
         )
         feature_names = self.parameters.feature_names
         offset = self.parameters.offset
-        input_column_names = self.parameters.input_column_names
+        input_column_name = ValueStr.create(self.parameters.input_column_names[0])
         lookup_target_str = (
-            f"{var_name}.{input_column_names[0]}.as_target(target_name={ValueStr.create(feature_names[0])}, "
+            f"{var_name}[{input_column_name}].as_target(target_name={ValueStr.create(feature_names[0])}, "
             f"offset={ValueStr.create(offset)})"
         )
         return statements, ExpressionStr(lookup_target_str)
 
 
 class JoinMetadata(BaseModel):
     """Metadata to track general `view.join(...)` operation"""
@@ -1314,17 +1323,19 @@
         right_col_map = dict(zip(params.right_input_columns, params.right_output_columns))
 
         # construct input column name to output column mapping for left & right columns
         left_columns = {
             col.name: col.clone(
                 name=left_col_map[col.name],  # type: ignore
                 # if the join type is left, current node is not a compulsory node for the column
-                node_names=col.node_names.union([self.name])
-                if params.join_type != "left"
-                else col.node_names,
+                node_names=(
+                    col.node_names.union([self.name])
+                    if params.join_type != "left"
+                    else col.node_names
+                ),
                 node_name=self.name,
             )
             for col in inputs[0].columns
             if col.name in left_col_map
         }
         left_on_col = next(col for col in inputs[0].columns if col.name == self.parameters.left_on)
         right_columns = {}
@@ -1536,18 +1547,19 @@
                 view_required_columns.append(self.parameters.view_point_in_time_column)
             return view_required_columns
         return [self.parameters.feature_entity_column]
 
     @staticmethod
     def _validate_feature(feature_op_structure: OperationStructure) -> None:
         columns = feature_op_structure.aggregations
-        assert len(columns) == 1
+        assert len(columns) == 1, "Feature should have exactly one aggregation"
         # For now, the supported feature should have an item_groupby node in its lineage
-        assert any(node_name.startswith("item_groupby") for node_name in columns[0].node_names)
-        assert feature_op_structure.output_type == NodeOutputType.SERIES
+        assert (
+            feature_op_structure.output_type == NodeOutputType.SERIES
+        ), "Output should be a series"
 
     def _derive_node_operation_info(
         self,
         inputs: List[OperationStructure],
         global_state: OperationStructureInfo,
     ) -> OperationStructure:
         # First input is a View
@@ -1729,30 +1741,27 @@
 
 
 class AggregateAsAtParameters(BaseGroupbyParameters, SCDBaseParameters):
     """Parameters for AggregateAsAtNode"""
 
     name: OutColumnStr
     offset: Optional[str]
+    # Note: This is kept for backward compatibility and not used by SQL generation
     backward: Optional[bool]
 
 
-class AggregateAsAtNode(AggregationOpStructMixin, BaseNode):
-    """AggregateAsAt class"""
+class BaseAggregateAsAtNode(AggregationOpStructMixin, BaseNode):
+    """BaseAggregateAsAtNode class"""
 
-    type: Literal[NodeType.AGGREGATE_AS_AT] = Field(NodeType.AGGREGATE_AS_AT, const=True)
     output_type: NodeOutputType = Field(NodeOutputType.FRAME, const=True)
     parameters: AggregateAsAtParameters
 
     # class variable
     _auto_convert_expression_to_variable: ClassVar[bool] = False
 
-    # feature definition hash generation configuration
-    _normalized_output_prefix = "feat_"
-
     @property
     def max_input_count(self) -> int:
         return 1
 
     def _get_required_input_columns(
         self, input_index: int, available_column_names: List[str]
     ) -> Sequence[str]:
@@ -1774,24 +1783,34 @@
         col_name_map = {col.name: col for col in columns}
         return [
             AggregationColumn(
                 name=self.parameters.name,
                 method=self.parameters.agg_func,
                 keys=self.parameters.keys,
                 window=None,
-                category=None,
+                category=self.parameters.value_by,
+                offset=self.parameters.offset,
                 column=col_name_map.get(self.parameters.parent),
                 filter=any(col.filter for col in columns),
-                aggregation_type=self.type,
+                aggregation_type=self.type,  # type: ignore[arg-type]
                 node_names={node_name}.union(other_node_names),
                 node_name=node_name,
                 dtype=output_var_type,
             )
         ]
 
+
+class AggregateAsAtNode(BaseAggregateAsAtNode):
+    """AggregateAsAtNode class"""
+
+    type: Literal[NodeType.AGGREGATE_AS_AT] = Field(NodeType.AGGREGATE_AS_AT, const=True)
+
+    # feature definition hash generation configuration
+    _normalized_output_prefix = "feat_"
+
     def _derive_sdk_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         operation_structure: OperationStructure,
         config: SDKCodeGenConfig,
         context: CodeGenerationContext,
@@ -1810,22 +1829,67 @@
         )
         keys = ValueStr.create(self.parameters.keys)
         category = ValueStr.create(self.parameters.value_by)
         value_column = ValueStr.create(self.parameters.parent)
         method = ValueStr.create(self.parameters.agg_func)
         feature_name = ValueStr.create(self.parameters.name)
         offset = ValueStr.create(self.parameters.offset)
-        backward = ValueStr.create(self.parameters.backward)
         grouped = f"{var_name}.groupby(by_keys={keys}, category={category})"
         agg = (
             f"aggregate_asat(value_column={value_column}, "
             f"method={method}, "
             f"feature_name={feature_name}, "
             f"offset={offset}, "
-            f"backward={backward}, "
+            f"skip_fill_na=True)"
+        )
+        return statements, ExpressionStr(f"{grouped}.{agg}")
+
+
+class ForwardAggregateAsAtNode(BaseAggregateAsAtNode):
+    """ForwardAggregateAsAtNode class"""
+
+    type: Literal[NodeType.FORWARD_AGGREGATE_AS_AT] = Field(
+        NodeType.FORWARD_AGGREGATE_AS_AT, const=True
+    )
+
+    # feature definition hash generation configuration
+    _normalized_output_prefix = "target_"
+
+    def _derive_sdk_code(
+        self,
+        node_inputs: List[VarNameExpressionInfo],
+        var_name_generator: VariableNameGenerator,
+        operation_structure: OperationStructure,
+        config: SDKCodeGenConfig,
+        context: CodeGenerationContext,
+    ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
+        # Note: this node is a special case as the output of this node is not a complete SDK code.
+        # Currently, `scd_view.groupby(...).forward_aggregate_asat()` will generate
+        # ForwardAggregateAsAtNode + ProjectNode. Output of ForwardAggregateAsAtNode is just an
+        # expression, the actual variable assignment will be done at the ProjectNode.
+        var_name_expressions = self._assert_no_info_dict(node_inputs)
+        statements, var_name = self._convert_expression_to_variable(
+            var_name_expression=var_name_expressions[0],
+            var_name_generator=var_name_generator,
+            node_output_type=NodeOutputType.FRAME,
+            node_output_category=NodeOutputCategory.TARGET,
+            to_associate_with_node_name=False,
+        )
+        keys = ValueStr.create(self.parameters.keys)
+        category = ValueStr.create(self.parameters.value_by)
+        value_column = ValueStr.create(self.parameters.parent)
+        method = ValueStr.create(self.parameters.agg_func)
+        target_name = ValueStr.create(self.parameters.name)
+        offset = ValueStr.create(self.parameters.offset)
+        grouped = f"{var_name}.groupby(by_keys={keys}, category={category})"
+        agg = (
+            f"forward_aggregate_asat(value_column={value_column}, "
+            f"method={method}, "
+            f"target_name={target_name}, "
+            f"offset={offset}, "
             f"skip_fill_na=True)"
         )
         return statements, ExpressionStr(f"{grouped}.{agg}")
 
 
 class AliasNode(BaseNode):
     """AliasNode class"""
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/input.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/input.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 This module contains SQL operation related to input node
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import Any, ClassVar, Dict, List, Literal, Optional, Sequence, Tuple, Union
-from typing_extensions import Annotated
+from typing import Any, ClassVar, Dict, List, Optional, Sequence, Tuple, Union
+from typing_extensions import Annotated, Literal
 
 from abc import abstractmethod  # pylint: disable=wrong-import-order
 
 from bson import ObjectId
 from pydantic import BaseModel, Field, root_validator
 
 from featurebyte.enum import DBVarType, SourceType, TableDataType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/metadata/column.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/metadata/column.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contain columns related metadata used in node definition.
 """
+
 from typing import TYPE_CHECKING, Any
 
 if TYPE_CHECKING:
     from pydantic.typing import CallableGenerator
 
 
 class ColumnStr(str):
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/metadata/config.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/metadata/config.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Code generation config is used to control the code generating style.
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from bson import ObjectId
 from pydantic import BaseModel, Field
 
@@ -54,14 +55,15 @@
     sql_input_var_prefix: str = Field(default="x")
     sql_request_input_var_prefix: str = Field(default="r")
     sql_comment: str = Field(default="")
     function_name: str = Field(default="on_demand_feature_function")
     input_var_prefix: str = Field(default="col")
     request_input_var_prefix: str = Field(default="request_col")
     output_dtype: DBVarType
+    to_generate_null_filling_function: bool = Field(default=False)
 
     @classmethod
     def to_py_type(cls, dtype: DBVarType) -> str:
         """
         Convert DBVarType to Python type
 
         Parameters
@@ -82,16 +84,18 @@
         output = dtype.to_type_str()
         if output:
             return output
         if dtype in DBVarType.supported_timestamp_types():
             return "pd.Timestamp"
         if dtype == DBVarType.DATE:
             return "datetime.date"
-        if dtype in DBVarType.json_conversion_types():
-            return "str"
+        if dtype in DBVarType.dictionary_types():
+            return "dict[str, float]"
+        if dtype in DBVarType.array_types():
+            return "list[float]"
         raise ValueError(f"Unsupported dtype: {dtype}")
 
     @classmethod
     def to_sql_type(cls, py_type: str) -> str:
         """
         Convert DBVarType to SQL type
 
@@ -113,14 +117,16 @@
         mapping = {
             "bool": "BOOLEAN",
             "str": "STRING",
             "float": "DOUBLE",
             "int": "BIGINT",
             "pd.Timestamp": "TIMESTAMP",
             "datetime.date": "DATE",
+            "dict[str, float]": "MAP<STRING, DOUBLE>",
+            "list[float]": "ARRAY<DOUBLE>",
         }
         output = mapping.get(py_type)
         if output:
             return output
         raise ValueError(f"Unsupported py_type: {py_type}")
 
     @property
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/metadata/operation.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/metadata/operation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains models used to store node output operation info
 """
+
 from typing import (
     Any,
     DefaultDict,
     Dict,
     Iterator,
     List,
     Literal,
@@ -198,17 +199,19 @@
         key = (column.name, column.node_name)
         if key not in column_map:
             column_map[key] = column
         else:
             cur_col = column_map[key]
             column_map[key] = column.clone(
                 node_names=cur_col.node_names.union(column.node_names),
-                node_name=cur_col.node_name
-                if len(cur_col.node_names) > len(column.node_names)
-                else column.node_name,
+                node_name=(
+                    cur_col.node_name
+                    if len(cur_col.node_names) > len(column.node_names)
+                    else column.node_name
+                ),
                 filter=cur_col.filter or column.filter,
             )
         return column_map
 
     @classmethod
     def _flatten_columns(
         cls, columns: Sequence[Union[BaseDataColumn, "BaseDerivedColumn"]]
@@ -341,23 +344,25 @@
 class AggregationColumn(BaseDataColumn):
     """Aggregation column"""
 
     method: Optional[AggFunc]
     keys: Sequence[str]
     window: Optional[str]
     category: Optional[str]
+    offset: Optional[str]
     column: Optional[ViewDataColumn]
     aggregation_type: Literal[
         NodeType.GROUPBY,
         NodeType.ITEM_GROUPBY,
         NodeType.LOOKUP,
         NodeType.AGGREGATE_AS_AT,
         NodeType.REQUEST_COLUMN,
         NodeType.FORWARD_AGGREGATE,
         NodeType.LOOKUP_TARGET,
+        NodeType.FORWARD_AGGREGATE_AS_AT,
     ]
     type: Literal[FeatureDataColumnType.AGGREGATION] = FeatureDataColumnType.AGGREGATION
 
     def __hash__(self) -> int:
         key = (
             *self._get_hash_key(),
             self.type,
@@ -537,22 +542,20 @@
         if self.output_category == NodeOutputCategory.VIEW:
             return [col.name for col in self.columns if col.name]
         return [agg.name for agg in self.aggregations if agg.name]
 
     @overload
     def _split_column_by_type(
         self, columns: List[Union[SourceDataColumn, DerivedDataColumn]]
-    ) -> Tuple[List[SourceDataColumn], List[DerivedDataColumn]]:
-        ...
+    ) -> Tuple[List[SourceDataColumn], List[DerivedDataColumn]]: ...
 
     @overload
     def _split_column_by_type(
         self, columns: List[Union[AggregationColumn, PostAggregationColumn]]
-    ) -> Tuple[List[AggregationColumn], List[PostAggregationColumn]]:
-        ...
+    ) -> Tuple[List[AggregationColumn], List[PostAggregationColumn]]: ...
 
     def _split_column_by_type(
         self,
         columns: Union[
             List[Union[SourceDataColumn, DerivedDataColumn]],
             List[Union[AggregationColumn, PostAggregationColumn]],
         ],
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/metadata/sdk_code.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/metadata/sdk_code.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains models used for sdk code extractor.
 """
+
 from __future__ import annotations
 
 from typing import Any, DefaultDict, Dict, List, Optional, Set, Tuple, Union
 
 import ast
 import json
 import os
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/metadata/templates/databricks_feature_spec.tpl` & `featurebyte-1.0.3/featurebyte/query_graph/node/metadata/templates/databricks_feature_spec.tpl`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 # auto-generated by FeatureByte (based-on databricks-feature-store {{databricks_sdk_version}})
 # Import necessary modules for feature engineering and machine learning
 from databricks.feature_engineering import FeatureEngineeringClient
 from databricks.feature_engineering import FeatureFunction, FeatureLookup
+from pyspark.sql import SparkSession
 {{pyspark_import_statement}}
 import mlflow
 
 # Initialize the Feature Engineering client to interact with Databricks Feature Store
 fe = FeatureEngineeringClient()
 
 {% if require_timestamp_lookup_key -%}
@@ -22,24 +23,27 @@
 # Users should consider including request columns and primary entity columns here
 # This is important if these columns are not features but are only needed for lookup purposes
 exclude_columns = {{exclude_columns}}
 
 # Prepare the dataset for log model
 # 'features' is a list of feature lookups to be included in the training set
 # 'exclude_columns' is a list of columns to be excluded from the training set
-target_column = "[TARGET_COLUMN]"
+target_column = "{{target_column}}"
 schema = {{schema}}
+spark = SparkSession.builder.getOrCreate()
 log_model_dataset = fe.create_training_set(
     df=spark.createDataFrame([], schema),
     feature_lookups=features,
     label=target_column,
     exclude_columns=exclude_columns,
 )
 
+{% if include_log_model -%}
 # Log the model and register it to the unity catalog
 fe.log_model(
     model=model,  # model is the trained model
     artifact_path="[ARTIFACT_PATH]",  # artifact_path is the path to the model
     flavor=mlflow.sklearn,
     training_set=log_model_dataset,
     registered_model_name="[REGISTERED_MODEL_NAME]",  # registered model name in the unity catalog
 )
+{% endif -%}
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/mixin.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/mixin.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains mixins used in node classes
 """
+
 from typing import List, Optional, Set
 
 from abc import ABC, abstractmethod
 
 from pydantic import BaseModel
 
 from featurebyte.enum import AggFunc, DBVarType
@@ -163,15 +164,19 @@
         columns = [col for col in input_operation_info.columns if col.name in wanted_columns]
         if not wanted_columns and agg_func == AggFunc.COUNT:
             # for groupby aggregation, the wanted columns is empty
             # in this case, take the first column from input operation info
             columns = input_operation_info.columns[:1]
 
         output_category = NodeOutputCategory.FEATURE
-        if self.type in {NodeType.FORWARD_AGGREGATE, NodeType.LOOKUP_TARGET}:
+        if self.type in {
+            NodeType.FORWARD_AGGREGATE,
+            NodeType.LOOKUP_TARGET,
+            NodeType.FORWARD_AGGREGATE_AS_AT,
+        }:
             output_category = NodeOutputCategory.TARGET
 
         # prepare output variable type
         if agg_func:
             assert isinstance(self.parameters, BaseGroupbyParameters)
             aggregation_func_obj = construct_agg_func(agg_func)
             input_var_type = parent_columns[0].dtype if parent_columns else columns[0].dtype
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/nested.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/nested.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 """
 This module contains nested graph related node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     List,
-    Literal,
     Optional,
     Sequence,
     Tuple,
     TypeVar,
     Union,
     cast,
 )
-from typing_extensions import Annotated
+from typing_extensions import Annotated, Literal
 
 from abc import ABC, abstractmethod  # pylint: disable=wrong-import-order
 
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import Scalar
 from featurebyte.enum import DBVarType, SpecialColumnName, ViewMode
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.query_graph.enum import (
     FEAST_TIMESTAMP_POSTFIX,
     GraphNodeType,
     NodeOutputType,
     NodeType,
@@ -53,14 +52,15 @@
     ValueStr,
     VariableNameGenerator,
     VariableNameStr,
     VarNameExpressionInfo,
     get_object_class_from_function_call,
 )
 from featurebyte.query_graph.node.utils import subset_frame_column_expr
+from featurebyte.typing import Scalar
 
 
 class ProxyInputNode(BaseNode):
     """Proxy input node used by nested graph"""
 
     class ProxyInputNodeParameters(BaseModel):
         """Proxy input node parameters"""
@@ -391,17 +391,17 @@
         metadata = super()._prune_metadata(target_columns=target_columns, input_nodes=input_nodes)
         # for item view graph node, we need to use the event view graph node's metadata
         # to generate the event column cleaning operations
         assert len(input_nodes) == 2
         event_view_node = input_nodes[1]
         assert isinstance(event_view_node.parameters, EventViewGraphNodeParameters)
         event_view_metadata = event_view_node.parameters.metadata
-        metadata[
-            "event_column_cleaning_operations"
-        ] = event_view_metadata.column_cleaning_operations
+        metadata["event_column_cleaning_operations"] = (
+            event_view_metadata.column_cleaning_operations
+        )
         return metadata
 
 
 class DimensionViewGraphNodeParameters(BaseViewGraphNodeParameters):
     """GraphNode (type:dimension_view) parameters"""
 
     type: Literal[GraphNodeType.DIMENSION_VIEW] = Field(GraphNodeType.DIMENSION_VIEW, const=True)
@@ -634,17 +634,18 @@
             config=config,
             node_name=self.name,
         )
 
     def _derive_on_demand_view_or_user_defined_function_helper(
         self,
         var_name_generator: VariableNameGenerator,
-        input_var_name_expr: VarNameExpressionInfo,
+        input_var_name_expr: VariableNameStr,
         json_conversion_func: Callable[[VarNameExpressionInfo], ExpressionStr],
         null_filling_func: Callable[[VarNameExpressionInfo, ValueStr], ExpressionStr],
+        is_databricks_udf: bool,
         config_for_ttl: Optional[OnDemandViewCodeGenConfig] = None,
         ttl_handling_column: Optional[str] = None,
         ttl_seconds: Optional[int] = None,
     ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
         if self.parameters.type != GraphNodeType.OFFLINE_STORE_INGEST_QUERY:
             raise RuntimeError("BaseGroupNode._derive_on_demand_view_code should not be called!")
 
@@ -667,47 +668,46 @@
             # need to apply TTL handling on the input column
             var_name_map = {}
             for var_name in ["request_time", "cutoff", "feat_ts", "mask"]:
                 var_name_map[var_name] = var_name_generator.convert_to_variable_name(
                     variable_name_prefix=var_name, node_name=None
                 )
 
-            statements.append(  # request_time = pd.to_datetime(input_df_name["POINT_IN_TIME"])
+            # request_time = pd.to_datetime(input_df_name["POINT_IN_TIME"])
+            statements.append(
                 (
                     var_name_map["request_time"],
-                    get_object_class_from_function_call(
-                        "pd.to_datetime",
+                    self._to_datetime_expr(
                         ExpressionStr(
                             subset_frame_column_expr(
                                 VariableNameStr(input_df_name),
                                 SpecialColumnName.POINT_IN_TIME.value,
                             )
                         ),
-                        utc=True,
+                        to_handle_none=is_databricks_udf,
                     ),
                 )
             )
             statements.append(  # cutoff = request_time - pd.Timedelta(seconds=ttl_seconds)
                 (
                     var_name_map["cutoff"],
                     ExpressionStr(
                         f"{var_name_map['request_time']} - pd.Timedelta(seconds={ttl_seconds})"
                     ),
                 )
             )
             statements.append(  # feature_ts = pd.to_datetime(input_df_name[ttl_handling_column], unit="s", utc=True)
                 (
                     var_name_map["feat_ts"],
-                    get_object_class_from_function_call(
-                        "pd.to_datetime",
+                    self._to_datetime_expr(
                         ExpressionStr(
                             subset_frame_column_expr(VariableNameStr(input_df_name), feat_ts_col)
                         ),
+                        to_handle_none=False,
                         unit="s",
-                        utc=True,
                     ),
                 )
             )
             statements.append(  # mask = (feature_ts >= cutoff) & (feature_ts <= request_time)
                 (
                     var_name_map["mask"],
                     ExpressionStr(
@@ -720,18 +720,20 @@
                 StatementStr(
                     f"{input_df_name}.loc[~{var_name_map['mask']}, {repr(ttl_handling_column)}] = np.nan"
                 )
             )
 
         if node_params.output_dtype in DBVarType.supported_timestamp_types():
             var_name = var_name_generator.convert_to_variable_name("feat", node_name=self.name)
-            to_dt_expr = get_object_class_from_function_call(
-                "pd.to_datetime", input_var_name_expr, utc=True
+            statements.append(
+                (
+                    var_name,
+                    self._to_datetime_expr(input_var_name_expr, to_handle_none=is_databricks_udf),
+                )
             )
-            statements.append((var_name, to_dt_expr))
             return statements, var_name
 
         if node_params.output_dtype in DBVarType.json_conversion_types():
             var_name = var_name_generator.convert_to_variable_name("feat", node_name=self.name)
             json_conv_expr = json_conversion_func(input_var_name_expr)
             statements.append((var_name, json_conv_expr))
             return statements, var_name
@@ -771,14 +773,15 @@
             var_name_generator=var_name_generator,
             input_var_name_expr=input_var_name_expr,
             json_conversion_func=_json_conversion_func,
             null_filling_func=lambda expr, val: ExpressionStr(f"{expr}.fillna({val.as_input()})"),
             config_for_ttl=config_for_ttl,
             ttl_handling_column=ttl_handling_column,
             ttl_seconds=ttl_seconds,
+            is_databricks_udf=False,
         )
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandFunctionCodeGenConfig,
@@ -797,13 +800,14 @@
         input_var_name = var_name_generator.convert_to_variable_name(
             variable_name_prefix=config.input_var_prefix, node_name=associated_node_name
         )
         return self._derive_on_demand_view_or_user_defined_function_helper(
             var_name_generator=var_name_generator,
             input_var_name_expr=input_var_name,
             json_conversion_func=lambda expr: ExpressionStr(
-                f"np.nan if pd.isna({expr}) else json.loads({expr})"
+                f"np.nan if pd.isna({expr}) else {expr}"
             ),
             null_filling_func=lambda expr, val: ExpressionStr(
                 f"{val.as_input()} if pd.isna({expr}) else {expr}"
             ),
+            is_databricks_udf=True,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/request.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/request.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """
 Request data related node classes
 """
-from typing import List, Literal, Sequence, Tuple
+
+from typing import List, Sequence, Tuple
+from typing_extensions import Literal
 
 from pydantic import BaseModel, Field, StrictStr
 
 from featurebyte.enum import DBVarType, SpecialColumnName
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.node.base import BaseNode
 from featurebyte.query_graph.node.metadata.config import (
@@ -23,15 +25,14 @@
 from featurebyte.query_graph.node.metadata.sdk_code import (
     ClassEnum,
     CodeGenerationContext,
     StatementT,
     VariableNameGenerator,
     VariableNameStr,
     VarNameExpressionInfo,
-    get_object_class_from_function_call,
 )
 from featurebyte.query_graph.node.utils import subset_frame_column_expr
 
 
 class RequestColumnNode(BaseNode):
     """Request column node used by on-demand features"""
 
@@ -68,14 +69,15 @@
                     filter=False,
                     node_names={self.name},
                     node_name=self.name,
                     method=None,
                     keys=[],
                     window=None,
                     category=None,
+                    offset=None,
                     type=FeatureDataColumnType.AGGREGATION,
                     column=None,
                     aggregation_type=NodeType.REQUEST_COLUMN,
                 ),
             ],
             output_type=NodeOutputType.SERIES,
             output_category=NodeOutputCategory.FEATURE,
@@ -100,25 +102,27 @@
             raise NotImplementedError("Currently only POINT_IN_TIME column is supported")
         statements.append((var_name, obj))
         return statements, var_name
 
     def _derive_on_demand_view_or_user_defined_function_helper(
         self,
         var_name_generator: VariableNameGenerator,
-        input_var_name_expr: VarNameExpressionInfo,
+        input_var_name_expr: VariableNameStr,
         var_name_prefix: str,
-    ) -> Tuple[List[StatementT], VarNameExpressionInfo]:
+        is_databricks_udf: bool,
+    ) -> Tuple[List[StatementT], VariableNameStr]:
         if self.parameters.dtype in DBVarType.supported_timestamp_types():
             var_name = var_name_generator.convert_to_variable_name(
                 variable_name_prefix=var_name_prefix, node_name=self.name
             )
-            expression = get_object_class_from_function_call(
-                "pd.to_datetime", input_var_name_expr, utc=True
+            statement = (
+                var_name,
+                self._to_datetime_expr(input_var_name_expr, to_handle_none=is_databricks_udf),
             )
-            return [(var_name, expression)], var_name
+            return [statement], var_name
         return [], input_var_name_expr
 
     def _derive_on_demand_view_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandViewCodeGenConfig,
@@ -126,14 +130,15 @@
         input_df_name = config.input_df_name
         column_name = self.parameters.column_name
         expr = VariableNameStr(subset_frame_column_expr(input_df_name, column_name))
         return self._derive_on_demand_view_or_user_defined_function_helper(
             var_name_generator=var_name_generator,
             input_var_name_expr=expr,
             var_name_prefix="request_col",
+            is_databricks_udf=False,
         )
 
     def _derive_user_defined_function_code(
         self,
         node_inputs: List[VarNameExpressionInfo],
         var_name_generator: VariableNameGenerator,
         config: OnDemandFunctionCodeGenConfig,
@@ -145,8 +150,9 @@
         request_input_var_name = var_name_generator.convert_to_variable_name(
             variable_name_prefix=config.request_input_var_prefix, node_name=associated_node_name
         )
         return self._derive_on_demand_view_or_user_defined_function_helper(
             var_name_generator=var_name_generator,
             input_var_name_expr=request_input_var_name,
             var_name_prefix="feat",
+            is_databricks_udf=True,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/scalar.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/scalar.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 """
 Models for the value parameter used by SingleValueNodeParameters
 """
+
 from __future__ import annotations
 
-from typing import Literal, Union, cast
+from typing import Union, cast
+from typing_extensions import Literal
 
 import pandas as pd
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import AllSupportedValueTypes, Scalar, ScalarSequence
 from featurebyte.enum import StrEnum
+from featurebyte.typing import AllSupportedValueTypes, Scalar, ScalarSequence
 
 
 class NonNativeValueType(StrEnum):
     """
     Scalar value types enum
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/schema.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/schema.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 """
 This module contains feature store & table schemas that are used in node parameters.
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, Dict, Optional, Union
 
 from pydantic import Field, StrictStr, root_validator
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.enum import DBVarType, SourceType, StorageType
-from featurebyte.models.base import FeatureByteBaseModel
+from featurebyte.models.base import FeatureByteBaseModel, NameStr
 
 
 class BaseDatabaseDetails(FeatureByteBaseModel):
     """Model for data source information"""
 
     is_local_source: ClassVar[bool] = False
 
@@ -268,19 +269,19 @@
     type: SourceType
     details: Optional[DatabaseDetails]
 
 
 class TableDetails(FeatureByteBaseModel):
     """Table details"""
 
-    database_name: Optional[StrictStr]
-    schema_name: Optional[StrictStr]
-    table_name: StrictStr
+    database_name: Optional[NameStr]
+    schema_name: Optional[NameStr]
+    table_name: NameStr
 
 
 class ColumnSpec(FeatureByteBaseModel):
     """
     Schema for columns retrieval
     """
 
-    name: StrictStr
+    name: NameStr
     dtype: DBVarType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/string.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/string.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """
 This module contains string operation related node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import List, Literal, Optional, Tuple
+from typing import List, Optional, Tuple
+from typing_extensions import Literal
 
 import textwrap
 from abc import ABC
 
 from pydantic import BaseModel, Field
 
 from featurebyte.enum import DBVarType
@@ -49,14 +51,17 @@
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.INT
 
     def generate_expression(self, operand: str) -> str:
         return f"{operand}.str.len()"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        return f"{operand}.astype(object).str.len().astype(float)"
+
     def generate_udf_expression(self, operand: str) -> str:
         return f"len({operand})"
 
 
 class TrimNode(BaseStringAccessorOpNode):
     """TrimNode class"""
 
@@ -78,14 +83,24 @@
             value = ValueStr.create(self.parameters.character)
         if self.parameters.side == "both":
             return f"{operand}.str.strip(to_strip={value})"
         if self.parameters.side == "left":
             return f"{operand}.str.lstrip(to_strip={value})"
         return f"{operand}.str.rstrip(to_strip={value})"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        value = None
+        if self.parameters.character:
+            value = ValueStr.create(self.parameters.character)
+        if self.parameters.side == "both":
+            return f"{operand}.astype(object).str.strip(to_strip={value})"
+        if self.parameters.side == "left":
+            return f"{operand}.astype(object).str.lstrip(to_strip={value})"
+        return f"{operand}.astype(object).str.rstrip(to_strip={value})"
+
     def generate_udf_expression(self, operand: str) -> str:
         value = None
         if self.parameters.character:
             value = ValueStr.create(self.parameters.character)
         if self.parameters.side == "both":
             return f"{operand}.strip({value})"
         if self.parameters.side == "left":
@@ -109,14 +124,19 @@
         return DBVarType.VARCHAR
 
     def generate_expression(self, operand: str) -> str:
         pattern = ValueStr.create(self.parameters.pattern)
         replacement = ValueStr.create(self.parameters.replacement)
         return f"{operand}.str.replace(pat={pattern}, repl={replacement})"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        pattern = ValueStr.create(self.parameters.pattern)
+        replacement = ValueStr.create(self.parameters.replacement)
+        return f"{operand}.astype(object).str.replace(pat={pattern}, repl={replacement})"
+
     def generate_udf_expression(self, operand: str) -> str:
         pattern = ValueStr.create(self.parameters.pattern)
         replacement = ValueStr.create(self.parameters.replacement)
         return f"{operand}.replace({pattern}, {replacement})"
 
 
 class PadNode(BaseStringAccessorOpNode):
@@ -137,14 +157,20 @@
 
     def generate_expression(self, operand: str) -> str:
         width = self.parameters.length
         side = ValueStr.create(self.parameters.side)
         fill_char = ValueStr.create(self.parameters.pad)
         return f"{operand}.str.pad(width={width}, side={side}, fillchar={fill_char})"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        width = self.parameters.length
+        side = ValueStr.create(self.parameters.side)
+        fill_char = ValueStr.create(self.parameters.pad)
+        return f"{operand}.astype(object).str.pad(width={width}, side={side}, fillchar={fill_char})"
+
     @staticmethod
     def _get_pad_string_function_name(
         var_name_generator: VariableNameGenerator,
     ) -> Tuple[List[StatementT], str]:
         statements: List[StatementT] = []
         func_name = "pad_string"
         if var_name_generator.should_insert_function(function_name=func_name):
@@ -196,14 +222,17 @@
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.VARCHAR
 
     def generate_expression(self, operand: str) -> str:
         return f"{operand}.str.{self.parameters.case}()"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        return f"{operand}.astype(object).str.{self.parameters.case}()"
+
     def generate_udf_expression(self, operand: str) -> str:
         if self.parameters.case == "upper":
             return f"{operand}.upper()"
         return f"{operand}.lower()"
 
 
 class StringContainsNode(BaseStringAccessorOpNode):
@@ -221,14 +250,18 @@
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.BOOL
 
     def generate_expression(self, operand: str) -> str:
         pattern = ValueStr.create(self.parameters.pattern)
         return f"{operand}.str.contains(pat={pattern}, case={self.parameters.case})"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        pattern = ValueStr.create(self.parameters.pattern)
+        return f"{operand}.astype(object).str.contains(pat={pattern}, case={self.parameters.case})"
+
     def generate_udf_expression(self, operand: str) -> str:
         pattern = ValueStr.create(self.parameters.pattern)
         if self.parameters.case:
             return f"{pattern} in {operand}"
         return f"{pattern}.lower() in {operand}.lower()"
 
 
@@ -249,14 +282,20 @@
 
     def generate_expression(self, operand: str) -> str:
         stop = None
         if self.parameters.length is not None:
             stop = self.parameters.length + (self.parameters.start or 0)
         return f"{operand}.str.slice(start={self.parameters.start}, stop={stop})"
 
+    def generate_odfv_expression(self, operand: str) -> str:
+        stop = None
+        if self.parameters.length is not None:
+            stop = self.parameters.length + (self.parameters.start or 0)
+        return f"{operand}.astype(object).str.slice(start={self.parameters.start}, stop={stop})"
+
     def generate_udf_expression(self, operand: str) -> str:
         stop = None
         if self.parameters.length is not None:
             stop = self.parameters.length + (self.parameters.start or 0)
         return f"{operand}[{self.parameters.start}:{stop}]"
 
 
@@ -267,7 +306,15 @@
     parameters: ValueWithRightOpNodeParameters
 
     def derive_var_type(self, inputs: List[OperationStructure]) -> DBVarType:
         return DBVarType.VARCHAR
 
     def generate_expression(self, left_operand: str, right_operand: str) -> str:
         return f"{left_operand} + {right_operand}"
+
+    def generate_odfv_expression(self, left_operand: str, right_operand: str) -> str:
+        if self.parameters.value is not None:
+            return f"{left_operand}.astype(object) + {right_operand}"
+        return f"{left_operand}.astype(object) + {right_operand}.astype(object)"
+
+    def generate_udf_expression(self, left_operand: str, right_operand: str) -> str:
+        return f"{left_operand} + {right_operand}"
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/unary.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/unary.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """
 This module contains unary operation node classes
 """
+
 # DO NOT include "from __future__ import annotations" as it will trigger issue for pydantic model nested definition
-from typing import ClassVar, List, Literal, Type, Union
+from typing import ClassVar, List, Type, Union
+from typing_extensions import Literal
 
 from pydantic import BaseModel, Field
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node.base import BaseSeriesOutputWithSingleOperandNode
 from featurebyte.query_graph.node.metadata.operation import OperationStructure
@@ -288,15 +290,18 @@
             return DBVarType.VARCHAR
         return DBVarType.UNKNOWN  # type: ignore[unreachable]
 
     def generate_expression(self, operand: str) -> str:
         return f"{operand}.astype({self.parameters.type})"
 
     def generate_odfv_expression(self, operand: str) -> str:
-        return f"{operand}.map(lambda x: {self.parameters.type}(x) if pd.notnull(x) else x)"
+        expr = f"{operand}.map(lambda x: {self.parameters.type}(x) if pd.notnull(x) else x)"
+        if self.parameters.type == "str":
+            return f"{expr}.astype(object)"
+        return expr
 
     def generate_udf_expression(self, operand: str) -> str:
         return f"{self.parameters.type}({operand})"
 
 
 class IsStringNode(BaseSeriesOutputWithSingleOperandNode):
     """IsStringNode class"""
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/utils.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utility functions for query graph node module
 """
+
 from typing import Sequence
 
 
 def subset_frame_column_expr(frame_name: str, column_name: str) -> str:
     """
     Subset frame column expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/validator.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/validator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """This module constains validator used for model input validation by classes in query_graph directory."""
+
 from typing import Any, List, Optional
 
 
 def construct_unique_name_validator(field: str) -> Any:
     """
     Construct a unique name validator function which will check the uniqueness of the input list
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/node/vector.py` & `featurebyte-1.0.3/featurebyte/query_graph/node/vector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """
 Vector node module
 """
-from typing import List, Literal, Sequence, Tuple
+
+from typing import List, Sequence, Tuple
+from typing_extensions import Literal
 
 import textwrap
 
 from pydantic import Field
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/pruning_util.py` & `featurebyte-1.0.3/featurebyte/query_graph/pruning_util.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for Query Graph pruning related utilities
 """
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from featurebyte.query_graph.graph import QueryGraph
 
 if TYPE_CHECKING:
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/__init__.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Package for adapter classes to generate engine specific SQL expressions
 """
+
 from __future__ import annotations
 
 from featurebyte.enum import SourceType
 from featurebyte.query_graph.sql.adapter.base import BaseAdapter
 from featurebyte.query_graph.sql.adapter.databricks import DatabricksAdapter
 from featurebyte.query_graph.sql.adapter.snowflake import SnowflakeAdapter
 from featurebyte.query_graph.sql.adapter.spark import SparkAdapter
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 Base class for SQL adapters
 """
+
 from __future__ import annotations
 
-from typing import List, Literal, Optional
+from typing import List, Optional
+from typing_extensions import Literal
 
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 
 from numpy import format_float_positional
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Select, alias_, select
@@ -493,25 +495,34 @@
         nested_select_expr.args["from"].set("expressions", [tablesample_expr])
 
         return nested_select_expr
 
     @classmethod
     @abstractmethod
     def create_table_as(
-        cls, table_details: TableDetails, select_expr: Select, replace: bool = False
+        cls,
+        table_details: TableDetails,
+        select_expr: Select | str,
+        kind: Literal["TABLE", "VIEW"] = "TABLE",
+        partition_keys: list[str] | None = None,
+        replace: bool = False,
     ) -> Expression:
         """
         Construct query to create a table using a select statement
 
         Parameters
         ----------
         table_details: TableDetails
             TableDetails of the table to be created
-        select_expr: Select
+        select_expr: Select | str
             Select expression
+        kind: Literal["TABLE", "VIEW"]
+            Kind of table to create
+        partition_keys: list[str] | None
+            Partition keys
         replace: bool
             Whether to replace the table if exists
 
         Returns
         -------
         Expression
         """
@@ -675,15 +686,15 @@
             Whether to quote the vector aggregate aliases.
 
         Returns
         -------
         Select
         """
         _ = vector_aggregate_columns, quote_vector_agg_aliases
-        return input_expr.select(*select_keys, *agg_exprs).group_by(*keys)
+        return input_expr.select(*select_keys, *agg_exprs, copy=False).group_by(*keys, copy=False)
 
     @classmethod
     @abstractmethod
     def get_percentile_expr(cls, input_expr: Expression, quantile: float) -> Expression:
         """
         Get the percentile expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/databricks.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/databricks.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 """
 DatabricksAdapter class for generating Databricks specific SQL expressions
 """
+
 from __future__ import annotations
 
-from typing import Literal, Optional
+from typing import Optional
+from typing_extensions import Literal
 
 import re
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Select
 
 from featurebyte.enum import DBVarType, SourceType, StrEnum
 from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.adapter.base import BaseAdapter
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
-from featurebyte.query_graph.sql.common import get_fully_qualified_table_name
+from featurebyte.query_graph.sql.common import get_fully_qualified_table_name, quoted_identifier
 
 
 class DatabricksAdapter(BaseAdapter):
     """
     Helper class to generate Databricks specific SQL expressions
     """
 
@@ -204,51 +206,73 @@
     @classmethod
     def escape_quote_char(cls, query: str) -> str:
         # Databricks sql escapes ' with \'. Use regex to make it safe to call this more than once.
         return re.sub(r"(?<!\\)'", "\\'", query)
 
     @classmethod
     def create_table_as(
-        cls, table_details: TableDetails, select_expr: Select, replace: bool = False
+        cls,
+        table_details: TableDetails,
+        select_expr: Select | str,
+        kind: Literal["TABLE", "VIEW"] = "TABLE",
+        partition_keys: list[str] | None = None,
+        replace: bool = False,
     ) -> Expression:
         """
         Construct query to create a table using a select statement
 
         Parameters
         ----------
         table_details: TableDetails
             TableDetails of the table to be created
-        select_expr: Select
+        select_expr: Select | str
             Select expression
+        kind: Literal["TABLE", "VIEW"]
+            Kind of table to create
+        partition_keys: list[str] | None
+            Partition keys
         replace: bool
             Whether to replace the table if exists
 
         Returns
         -------
         Expression
         """
         destination_expr = get_fully_qualified_table_name(table_details.dict())
-        table_properties = [
-            expressions.TableFormatProperty(this=expressions.Var(this="DELTA")),
-            expressions.Property(
-                this=expressions.Literal(this="delta.columnMapping.mode"), value="'name'"
-            ),
-            expressions.Property(
-                this=expressions.Literal(this="delta.minReaderVersion"), value="'2'"
-            ),
-            expressions.Property(
-                this=expressions.Literal(this="delta.minWriterVersion"), value="'5'"
-            ),
-        ]
+
+        if kind == "TABLE":
+            table_properties = [
+                expressions.TableFormatProperty(this=expressions.Var(this="DELTA")),
+                expressions.Property(
+                    this=expressions.Literal(this="delta.columnMapping.mode"), value="'name'"
+                ),
+                expressions.Property(
+                    this=expressions.Literal(this="delta.minReaderVersion"), value="'2'"
+                ),
+                expressions.Property(
+                    this=expressions.Literal(this="delta.minWriterVersion"), value="'5'"
+                ),
+            ]
+            if partition_keys:
+                table_properties.append(
+                    expressions.PartitionedByProperty(
+                        this=expressions.Schema(
+                            expressions=[quoted_identifier(key) for key in partition_keys]
+                        )
+                    )
+                )
+            properties = expressions.Properties(expressions=table_properties)
+        else:
+            properties = None
 
         return expressions.Create(
             this=expressions.Table(this=destination_expr),
-            kind="TABLE",
+            kind=kind,
             expression=select_expr,
-            properties=expressions.Properties(expressions=table_properties),
+            properties=properties,
             replace=replace,
         )
 
     @classmethod
     def any_value(cls, expr: Expression) -> Expression:
         return expressions.Anonymous(this="first", expressions=[expr])
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/snowflake.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/snowflake.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 SnowflakeAdapter class for generating Snowflake specific SQL expressions
 """
+
 from __future__ import annotations
 
-from typing import List, Literal, Optional, cast
+from typing import List, Optional, cast
+from typing_extensions import Literal
 
 import re
 import string
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Identifier, Select, alias_, select
 
@@ -161,36 +163,45 @@
     @classmethod
     def escape_quote_char(cls, query: str) -> str:
         # Snowflake sql escapes ' with ''. Use regex to make it safe to call this more than once.
         return re.sub("(?<!')'(?!')", "''", query)
 
     @classmethod
     def create_table_as(
-        cls, table_details: TableDetails, select_expr: Select, replace: bool = False
+        cls,
+        table_details: TableDetails,
+        select_expr: Select | str,
+        kind: Literal["TABLE", "VIEW"] = "TABLE",
+        partition_keys: list[str] | None = None,
+        replace: bool = False,
     ) -> Expression:
         """
         Construct query to create a table using a select statement
 
         Parameters
         ----------
         table_details: TableDetails
             TableDetails of the table to be created
-        select_expr: Select
+        select_expr: Select | str
             Select expression
+        kind: Literal["TABLE", "VIEW"]
+            Kind of table to create
+        partition_keys: list[str] | None
+            Partition keys
         replace: bool
             Whether to replace the table if exists
 
         Returns
         -------
         Expression
         """
         destination_expr = get_fully_qualified_table_name(table_details.dict())
         return expressions.Create(
             this=expressions.Table(this=destination_expr),
-            kind="TABLE",
+            kind=kind,
             expression=select_expr,
             replace=replace,
         )
 
     @classmethod
     def online_store_pivot_prepare_value_column(
         cls,
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/adapter/spark.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/adapter/spark.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SparkAdapter class for generating Spark specific SQL expressions
 """
+
 from __future__ import annotations
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
 from featurebyte.enum import SourceType
 from featurebyte.query_graph.sql.adapter.databricks import DatabricksAdapter
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/asat.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/base_asat.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 """
 SQL generation for as-at aggregation
 """
+
 from __future__ import annotations
 
-from typing import Any, cast
+from typing import Any, TypeVar, cast
+
+from abc import abstractmethod
 
 from sqlglot import expressions
 from sqlglot.expressions import Select, select
 
 from featurebyte.enum import SpecialColumnName
 from featurebyte.query_graph.sql.aggregator.base import (
     AggregationResult,
@@ -17,35 +20,44 @@
 from featurebyte.query_graph.sql.aggregator.request_table import RequestTablePlan
 from featurebyte.query_graph.sql.common import (
     CteStatements,
     get_qualified_column_identifier,
     quoted_identifier,
 )
 from featurebyte.query_graph.sql.groupby_helper import GroupbyColumn, GroupbyKey, get_groupby_expr
-from featurebyte.query_graph.sql.offset import add_offset_to_timestamp
+from featurebyte.query_graph.sql.offset import OffsetDirection, add_offset_to_timestamp
 from featurebyte.query_graph.sql.scd_helper import END_TS, augment_scd_table_with_end_timestamp
-from featurebyte.query_graph.sql.specs import AggregateAsAtSpec
+from featurebyte.query_graph.sql.specifications.base_aggregate_asat import BaseAggregateAsAtSpec
 
+AsAtSpecT = TypeVar("AsAtSpecT", bound=BaseAggregateAsAtSpec)
 
-class AsAtAggregator(NonTileBasedAggregator[AggregateAsAtSpec]):
+
+class BaseAsAtAggregator(NonTileBasedAggregator[AsAtSpecT]):
     """
     AsAtAggregation is responsible for generating SQL for as at aggregation
     """
 
     def __init__(self, *args: Any, **kwargs: Any) -> None:
         super().__init__(*args, **kwargs)
         self.request_table_plan = RequestTablePlan(is_time_aware=True)
 
-    def additional_update(self, aggregation_spec: AggregateAsAtSpec) -> None:
+    @classmethod
+    @abstractmethod
+    def get_offset_direction(cls) -> OffsetDirection:
+        """
+        Get the direction when applying the offset to point-in-time
+        """
+
+    def additional_update(self, aggregation_spec: AsAtSpecT) -> None:
         """
         Update internal states to account for aggregation spec
 
         Parameters
         ----------
-        aggregation_spec: AggregateAsAtSpec
+        aggregation_spec: AsAtSpecT
             Aggregation spec
         """
         self.request_table_plan.add_aggregation_spec(aggregation_spec)
 
     def update_aggregation_table_expr(
         self,
         table_expr: Select,
@@ -58,15 +70,15 @@
             query = self._get_aggregation_subquery(specs)
             queries.append(query)
 
         return self._update_with_left_joins(
             table_expr=table_expr, current_query_index=current_query_index, queries=queries
         )
 
-    def _get_aggregation_subquery(self, specs: list[AggregateAsAtSpec]) -> LeftJoinableSubquery:
+    def _get_aggregation_subquery(self, specs: list[AsAtSpecT]) -> LeftJoinableSubquery:
         """
         Get aggregation subquery that performs the asat aggregation. The list of aggregation
         specifications provided can be done in a single groupby operation.
 
         Parameters
         ----------
         specs: list[AggregationAsAtSpec]
@@ -109,14 +121,15 @@
         else:
             point_in_time_expr = add_offset_to_timestamp(
                 adapter=self.adapter,
                 timestamp_expr=get_qualified_column_identifier(
                     SpecialColumnName.POINT_IN_TIME, "REQ"
                 ),
                 offset=spec.parameters.offset,
+                offset_direction=self.get_offset_direction(),
             )
 
         # Only join records from the SCD table that are valid as at point in time
         record_validity_condition = expressions.and_(
             # SCD.effective_timestamp_column <= REQ.POINT_IN_TIME; i.e. record became effective
             # at or before point in time
             expressions.LTE(
@@ -167,17 +180,19 @@
                 parent_expr=(
                     get_qualified_column_identifier(s.parameters.parent, "SCD")
                     if s.parameters.parent
                     else None
                 ),
                 result_name=s.agg_result_name,
                 parent_dtype=s.parent_dtype,
-                parent_cols=[get_qualified_column_identifier(s.parameters.parent, "SCD")]
-                if s.parameters.parent
-                else [],
+                parent_cols=(
+                    [get_qualified_column_identifier(s.parameters.parent, "SCD")]
+                    if s.parameters.parent
+                    else []
+                ),
             )
             for s in specs
         ]
         groupby_input_expr = (
             select()
             .from_(
                 expressions.Table(
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base class for aggregation SQL generators
 """
+
 from __future__ import annotations
 
 from typing import Any, Generic, Sequence, Tuple, TypeVar
 
 from abc import ABC, abstractmethod
 from dataclasses import dataclass, field
 
@@ -252,15 +253,16 @@
             for key in left_joinable_subquery.join_keys
         ]
         updated_table_expr = table_expr.join(
             left_joinable_subquery.expr.subquery(),
             join_type="left",
             join_alias=agg_table_alias,
             on=expressions.and_(*join_conditions) if join_conditions else expressions.true(),
-        ).select(*agg_result_name_aliases)
+            copy=False,
+        ).select(*agg_result_name_aliases, copy=False)
         return updated_table_expr
 
     @staticmethod
     def _wrap_in_nested_query(table_expr: Select, columns: list[str]) -> Select:
         """
         Wrap table_expr in a nested query with a REQ alias
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/base_lookup.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/base_lookup.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for lookup features
 """
+
 from __future__ import annotations
 
 from typing import Any, Iterable, Optional, Sequence, Tuple, TypeVar
 
 from abc import abstractmethod
 from dataclasses import dataclass
 
@@ -22,14 +23,15 @@
 )
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import (
     CteStatements,
     get_qualified_column_identifier,
     quoted_identifier,
 )
+from featurebyte.query_graph.sql.deduplication import get_deduplicated_expr
 from featurebyte.query_graph.sql.scd_helper import Table
 from featurebyte.query_graph.sql.specifications.base_lookup import BaseLookupSpec
 
 
 @dataclass
 class SubqueryWithPointInTimeCutoff(LeftJoinableSubquery):
     """
@@ -193,14 +195,15 @@
 
             if specs[0].event_parameters is not None:
                 event_timestamp_column = specs[0].event_parameters.event_timestamp_column
                 agg_expr = agg_expr.select(quoted_identifier(event_timestamp_column))
             else:
                 event_timestamp_column = None
 
+            agg_expr = get_deduplicated_expr(self.adapter, agg_expr, [serving_name])
             result = SubqueryWithPointInTimeCutoff(
                 expr=agg_expr,
                 column_names=[spec.agg_result_name for spec in specs],
                 join_keys=[serving_name],
                 event_timestamp_column=event_timestamp_column,
                 forward_point_in_time_offset=self.get_forward_point_in_time_offset(specs[0]),
                 adapter=self.adapter,
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/forward.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/forward.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target aggregator module
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 import pandas as pd
 from sqlglot import expressions
 from sqlglot.expressions import Select, select
@@ -113,17 +114,19 @@
                 parent_expr=(
                     get_qualified_column_identifier(s.parameters.parent, "SOURCE_TABLE")
                     if s.parameters.parent
                     else None
                 ),
                 result_name=s.agg_result_name,
                 parent_dtype=s.parent_dtype,
-                parent_cols=[get_qualified_column_identifier(s.parameters.parent, "SOURCE_TABLE")]
-                if s.parameters.parent
-                else [],
+                parent_cols=(
+                    [get_qualified_column_identifier(s.parameters.parent, "SOURCE_TABLE")]
+                    if s.parameters.parent
+                    else []
+                ),
             )
             for s in specs
         ]
         join_keys = []
         assert len(spec.serving_names) == len(spec.parameters.keys)
         for serving_name, key in zip(spec.serving_names, spec.parameters.keys):
             serving_name_sql = quoted_identifier(serving_name).sql()
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/item.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/item.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for aggregation without time windows from ItemView
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from sqlglot import expressions
 from sqlglot.expressions import Select, select
 
@@ -114,17 +115,19 @@
                 parent_expr=(
                     get_qualified_column_identifier(s.parameters.parent, "ITEM")
                     if s.parameters.parent
                     else None
                 ),
                 result_name=s.agg_result_name,
                 parent_dtype=s.parent_dtype,
-                parent_cols=[get_qualified_column_identifier(s.parameters.parent, "ITEM")]
-                if s.parameters.parent
-                else [],
+                parent_cols=(
+                    [get_qualified_column_identifier(s.parameters.parent, "ITEM")]
+                    if s.parameters.parent
+                    else []
+                ),
             )
             for s in agg_specs
         ]
         value_by = (
             GroupbyKey(
                 expr=get_qualified_column_identifier(spec.parameters.value_by, "ITEM"),
                 name=spec.parameters.value_by,
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/latest.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/latest.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for latest feature without a window
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 import sys
 
 from sqlglot.expressions import Select
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/lookup.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/lookup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for lookup features
 """
+
 from __future__ import annotations
 
 from sqlglot.expressions import Select
 
 from featurebyte.query_graph.node.generic import SCDLookupParameters
 from featurebyte.query_graph.sql.aggregator.base_lookup import BaseLookupAggregator
 from featurebyte.query_graph.sql.scd_helper import Table, get_scd_join_expr
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/lookup_target.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/lookup_target.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for lookup targets
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from sqlglot.expressions import Select
 
 from featurebyte.query_graph.node.generic import SCDLookupParameters
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/request_table.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/request_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Request table processing used by aggregators
 """
+
 from __future__ import annotations
 
 from typing import Tuple
 
 from sqlglot import expressions
 from sqlglot.expressions import select
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/aggregator/window.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/aggregator/window.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for aggregation with time windows
 """
+
 from __future__ import annotations
 
 from typing import Any, Iterable, Optional, Tuple, cast
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Select, alias_, select
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/aggregate.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/aggregate.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for aggregation related sql generation
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from dataclasses import dataclass
 
 from sqlglot.expressions import Expression, Select
@@ -34,14 +35,15 @@
     query_node_type = [
         NodeType.GROUPBY,
         NodeType.LOOKUP,
         NodeType.LOOKUP_TARGET,
         NodeType.AGGREGATE_AS_AT,
         NodeType.ITEM_GROUPBY,
         NodeType.FORWARD_AGGREGATE,
+        NodeType.FORWARD_AGGREGATE_AS_AT,
     ]
 
     @property
     def sql(self) -> Expression:
         return self.source_node.sql
 
     @classmethod
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module containing base classes and functions for building syntax tree
 """
+
 from __future__ import annotations
 
 from typing import Optional, Type, TypeVar, cast
 
 from abc import ABC, abstractmethod
 from copy import copy
 from dataclasses import dataclass, field
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/binary.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/binary.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for binary operations sql generation
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions, parse_one
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/count_dict.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/count_dict.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 Module for count dict sql generation
 """
+
 from __future__ import annotations
 
-from typing import Literal
+from typing_extensions import Literal
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
 from featurebyte.query_graph.enum import NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/datetime.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/datetime.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 """
 Module for datetime operations related sql generation
 """
+
 from __future__ import annotations
 
 from typing import Union, cast
 
 from dataclasses import dataclass
 
 import pandas as pd
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
-from featurebyte.common.typing import DatetimeSupportedPropertyType, TimedeltaSupportedUnitType
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.sql.ast.base import ExpressionNode, SQLNodeContext
 from featurebyte.query_graph.sql.ast.generic import ParsedExpressionNode, resolve_project_node
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.ast.util import prepare_binary_op_input_nodes
+from featurebyte.typing import DatetimeSupportedPropertyType, TimedeltaSupportedUnitType
 
 
 @dataclass
 class DatetimeExtractNode(ExpressionNode):
     """Node for extract datetime properties operation"""
 
     expr: Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/distance.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/distance.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Distance module
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 from dataclasses import dataclass
 
 from sqlglot.expressions import Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/function.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/function.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Generic function related SQLNode
 """
+
 from __future__ import annotations
 
 from typing import Any, List, cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/generic.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/generic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for generic operations sql generation
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions, parse_one
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/groupby.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/groupby.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for groupby operation (non-time aware) sql generation
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 from dataclasses import dataclass
 
 from sqlglot.expressions import Expression, select
@@ -61,17 +62,17 @@
             GroupbyColumn(
                 agg_func=parameters["agg_func"],
                 parent_expr=(
                     quoted_identifier(parameters["parent"]) if parameters["parent"] else None
                 ),
                 result_name=output_name,
                 parent_dtype=parent_dtype,
-                parent_cols=[quoted_identifier(parameters["parent"])]
-                if parameters["parent"]
-                else [],
+                parent_cols=(
+                    [quoted_identifier(parameters["parent"])] if parameters["parent"] else []
+                ),
             )
         ]
         node = ItemGroupby(
             context=context,
             columns_map=columns_map,
             input_node=cast(TableNode, context.input_sql_nodes[0]),
             keys=parameters["keys"],
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/input.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/input.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for input data sql generation
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/is_in.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/is_in.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Is in SQL node
 """
+
 from __future__ import annotations
 
 from dataclasses import dataclass
 
 from sqlglot import expressions, parse_one
 from sqlglot.expressions import Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/join.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/join.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 Module for join operation sql generation
 """
+
 from __future__ import annotations
 
-from typing import Literal, Optional, cast
+from typing import Optional, cast
+from typing_extensions import Literal
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
 from sqlglot.expressions import Select
 
 from featurebyte.query_graph.enum import NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/join_feature.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/join_feature.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for JOIN_FEATURE query node type
 """
+
 from __future__ import annotations
 
 from typing import Optional, Tuple, cast
 
 from collections import defaultdict
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/literal.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/literal.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """
 Module for literal value handling
 """
+
 from __future__ import annotations
 
 from typing import Any, cast
 
 from sqlglot import expressions, parse_one
 
-from featurebyte.common.typing import is_scalar_nan
 from featurebyte.query_graph.node.scalar import NonNativeValueType, TimestampValue
+from featurebyte.typing import is_scalar_nan
 
 
 def make_literal_value_from_non_native_types(value: dict[str, Any]) -> expressions.Expression:
     """Create a sqlglot literal value from non-native types
 
     Parameters
     ----------
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/request.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/request.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Request data related SQLNode
 """
+
 from __future__ import annotations
 
 from dataclasses import dataclass
 
 from sqlglot.expressions import Expression
 
 from featurebyte.query_graph.enum import NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/string.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/string.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 Module containing string operations related sql generation
 """
+
 from __future__ import annotations
 
-from typing import Literal, Optional
+from typing import Optional
+from typing_extensions import Literal
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
 from featurebyte.query_graph.enum import NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/tile.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/tile.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for tile related sql generation
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
@@ -67,15 +68,15 @@
                     make_literal_value(self.time_modulo_frequency),
                     make_literal_value(self.blind_spot),
                     make_literal_value(self.frequency_minute),
                 ],
             ),
             alias="index",
         )
-        input_tiled = select("*", tile_index_expr).from_(input_filtered.subquery())
+        input_tiled = select("*", tile_index_expr).from_(input_filtered.subquery(copy=False))
         keys = [quoted_identifier(k) for k in self.keys]
         if self.value_by is not None:
             keys.append(quoted_identifier(self.value_by))
 
         if self.is_order_dependent:
             return self._get_tile_sql_order_dependent(keys, input_tiled)
 
@@ -163,14 +164,15 @@
             .select("R.*")
             .from_(entity_table)
             .join(
                 self.input_node.sql,
                 join_alias="R",
                 join_type="inner",
                 on=join_conditions_expr,
+                copy=False,
             )
         )
         return cast(Select, select_expr)
 
     @staticmethod
     def _get_db_var_type_from_col_type(col_type: str) -> DBVarType:
         mapping: dict[str, DBVarType] = {"ARRAY": DBVarType.ARRAY, "EMBEDDING": DBVarType.EMBEDDING}
@@ -179,15 +181,15 @@
     def _get_tile_sql_order_independent(
         self,
         keys: list[Expression],
         input_tiled: expressions.Select,
     ) -> Expression:
         inner_groupby_keys = [expressions.Identifier(this="index"), *keys]
         groupby_keys = [GroupbyKey(expr=key, name=key.name) for key in inner_groupby_keys]
-        original_query = select().from_(input_tiled.subquery())
+        original_query = select().from_(input_tiled.subquery(copy=False))
 
         groupby_columns = []
         for spec in self.tile_specs:
             groupby_columns.append(
                 GroupbyColumn(
                     parent_dtype=self._get_db_var_type_from_col_type(spec.tile_column_type),
                     agg_func=spec.tile_aggregation_type,
@@ -239,28 +241,28 @@
                 for spec in self.tile_specs
             ],
         ]
         inner_expr = select(
             "index",
             *keys,
             *window_exprs,
-        ).from_(input_tiled.subquery())
+        ).from_(input_tiled.subquery(copy=False), copy=False)
 
         outer_condition = expressions.EQ(
             this=quoted_identifier(self.ROW_NUMBER),
             expression=make_literal_value(1),
         )
         tile_expr = (
             select(
                 "index",
                 *keys,
                 *[spec.tile_column_name for spec in self.tile_specs],
             )
-            .from_(inner_expr.subquery())
-            .where(outer_condition)
+            .from_(inner_expr.subquery(copy=False))
+            .where(outer_condition, copy=False)
         )
 
         return tile_expr
 
     @classmethod
     def build(cls, context: SQLNodeContext) -> BuildTileNode | None:
         sql_node = None
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/track_changes.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/track_changes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for TRACK_CHANGES node sql generation
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/unary.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/unary.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """
 Module for unary operations sql generation
 """
+
 from __future__ import annotations
 
-from typing import Literal, Union, cast
+from typing import Union, cast
+from typing_extensions import Literal
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
 from featurebyte.enum import DBVarType
@@ -107,15 +109,15 @@
         elif self.from_dtype == DBVarType.BOOL and self.new_type == "float":
             # Casting to FLOAT from BOOL directly is not allowed
             expr = expressions.Cast(this=self.expr.sql, to=expressions.DataType.build("BIGINT"))
         else:
             expr = self.expr.sql
         type_expr = {
             "int": expressions.DataType.build("BIGINT"),
-            "float": expressions.DataType.build("FLOAT"),
+            "float": expressions.DataType.build("DOUBLE"),
             "str": expressions.DataType.build("VARCHAR"),
         }[self.new_type]
         output_expr = expressions.Cast(this=expr, to=type_expr)
         return output_expr
 
     @classmethod
     def build(cls, context: SQLNodeContext) -> CastNode:
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/util.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/util.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utilities for building SQLNode
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.query_graph.sql.ast.base import ExpressionNode, SQLNodeContext, TableNode
 from featurebyte.query_graph.sql.ast.generic import ParsedExpressionNode
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
@@ -45,15 +46,15 @@
     if isinstance(right_node, ExpressionNode) and parameters.get("right_op"):
         # Swap left & right objects if the operation from the right object
         left_node, right_node = right_node, left_node
 
     if table_node is None:
         # In this case, the left node is a column from the request data. The right node must be from
         # a feature and has a valid table node.
-        assert right_node.table_node is not None
+        assert right_node.table_node is not None, "Right node must have a table node"
         table_node = right_node.table_node
 
     return table_node, left_node, right_node
 
 
 def prepare_unary_input_nodes(
     context: SQLNodeContext,
@@ -66,11 +67,11 @@
         Information related to SQL node building
 
     Returns
     -------
     tuple[TableNode, ExpressionNode, dict[str, Any]]
     """
     input_expr_node = context.input_sql_nodes[0]
-    assert isinstance(input_expr_node, ExpressionNode)
+    assert isinstance(input_expr_node, ExpressionNode), "Input node must be an expression node"
     table_node = input_expr_node.table_node
     assert table_node is not None
     return table_node, input_expr_node, context.parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/ast/vector.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/ast/vector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Vector SQL node module
 """
+
 from __future__ import annotations
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/batch_helper.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/batch_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 """
 Helpers to split a QueryGraph and nodes into smaller batches
 """
+
 from __future__ import annotations
 
 from sqlglot import expressions
 
 from featurebyte.enum import InternalName
 from featurebyte.models.feature_query_set import FeatureQuery
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.sql.common import get_qualified_column_identifier, quoted_identifier
 from featurebyte.query_graph.sql.feature_compute import FeatureExecutionPlanner
 from featurebyte.query_graph.sql.specs import NonTileBasedAggregationSpec, TileBasedAggregationSpec
 
-NUM_FEATURES_PER_QUERY = 50
+NUM_FEATURES_PER_QUERY = 20
 
 
 def split_nodes(
     graph: QueryGraph,
     nodes: list[Node],
     num_features_per_query: int,
     is_tile_cache: bool = False,
@@ -108,15 +109,15 @@
     expressions.Select
     """
     expr = expressions.select(
         *(
             get_qualified_column_identifier(col, "REQ")
             for col in maybe_add_row_index_column(request_table_columns, output_include_row_index)
         )
-    ).from_(f"{request_table_name} AS REQ")
+    ).from_(expressions.Table(this=quoted_identifier(request_table_name), alias="REQ"))
 
     table_alias_by_feature = {}
     for i, feature_set in enumerate(feature_queries):
         table_alias = f"T{i}"
         expr = expr.join(
             expressions.Table(
                 this=quoted_identifier(feature_set.table_name),
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/builder.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for SQL syntax tree builder
 """
+
 from __future__ import annotations
 
 from typing import Any, Iterable, Optional, Type
 
 from collections import defaultdict
 
 from featurebyte.common.path_util import import_submodules
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/common.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/common.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Common helpers and data structures for feature SQL generation
 """
+
 from __future__ import annotations
 
 from typing import Dict, Optional, Sequence, Tuple, Union
 
 from dataclasses import dataclass
 from enum import Enum
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/dataframe.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/dataframe.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for sql generation given a DataFrame (typically request data)
 """
+
 from __future__ import annotations
 
 from typing import cast
 
 import pandas as pd
 from sqlglot import expressions
 from sqlglot.expressions import Expression, select
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/entity.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/entity.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 """
 Entity related helpers
 """
+
 from __future__ import annotations
 
-from typing import Any, List
+from typing import Any, List, Optional, Sequence, Union
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
 
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
-from featurebyte.query_graph.sql.common import quoted_identifier
+from featurebyte.query_graph.sql.common import get_qualified_column_identifier, quoted_identifier
 
 DUMMY_ENTITY_COLUMN_NAME = "__featurebyte_dummy_entity"
 DUMMY_ENTITY_VALUE = "0"
 
 
 def get_combined_serving_names(serving_names: List[str]) -> str:
     """
@@ -28,33 +29,46 @@
     -------
     str
     """
     combined_serving_names = " x ".join(serving_names)
     return combined_serving_names
 
 
-def get_combined_serving_names_expr(serving_names: List[str]) -> Expression:
+def get_combined_serving_names_expr(
+    serving_names: Sequence[Union[str, Expression]],
+    serving_names_table_alias: Optional[str] = None,
+) -> Expression:
     """
     Get an expression for the concatenated serving names
 
     Parameters
     ----------
     serving_names: List[str]
         Column names of the serving names to be concatenated
+    serving_names_table_alias: Optional[str]
+        Table alias for the serving names. Serving names will not be table qualified if not
+        provided.
 
     Returns
     -------
     Expression
     """
     assert len(serving_names) > 0
     parts: List[Expression] = []
     for serving_name in serving_names:
-        expr = expressions.Cast(
-            this=quoted_identifier(serving_name), to=expressions.DataType.build("VARCHAR")
-        )
+        if isinstance(serving_name, str):
+            if serving_names_table_alias is not None:
+                serving_name_expr = get_qualified_column_identifier(
+                    serving_name, serving_names_table_alias
+                )
+            else:
+                serving_name_expr = quoted_identifier(serving_name)
+        else:
+            serving_name_expr = serving_name
+        expr = expressions.Cast(this=serving_name_expr, to=expressions.DataType.build("VARCHAR"))
         parts.append(expr)
         parts.append(make_literal_value("::"))
     combined_serving_names_expr = expressions.Coalesce(
         this=expressions.Concat(expressions=parts[:-1]),
         expressions=[make_literal_value("")],
     )
     return combined_serving_names_expr
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/expression.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/expression.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Additional expression used to construct SQL query
 """
+
 from typing import Optional
 
 from sqlglot.expressions import Anonymous, Expression, Func
 
 
 def make_trim_expression(this: Expression, character: Optional[Expression] = None) -> Expression:
     """Helper to create a Trim expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/feature_compute.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/feature_compute.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 """
 Module with logic related to feature SQL generation
 """
+
 from __future__ import annotations
 
 from typing import Iterable, Optional, Sequence, Set, Type, Union
 
 import sys
 from collections import defaultdict
 
 from bson import ObjectId
 from sqlglot import expressions
 from sqlglot.expressions import select
 
-from featurebyte.enum import SourceType
+from featurebyte.enum import DBVarType, SourceType
 from featurebyte.models.parent_serving import (
     EntityLookupStep,
     EntityRelationshipsContext,
     FeatureNodeRelationshipsInfo,
     ParentServingPreparation,
 )
 from featurebyte.query_graph.enum import NodeType
@@ -24,14 +25,15 @@
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.schema import FeatureStoreDetails
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.aggregator.asat import AsAtAggregator
 from featurebyte.query_graph.sql.aggregator.base import TileBasedAggregator
 from featurebyte.query_graph.sql.aggregator.forward import ForwardAggregator
+from featurebyte.query_graph.sql.aggregator.forward_asat import ForwardAsAtAggregator
 from featurebyte.query_graph.sql.aggregator.item import ItemAggregator
 from featurebyte.query_graph.sql.aggregator.latest import LatestAggregator
 from featurebyte.query_graph.sql.aggregator.lookup import LookupAggregator
 from featurebyte.query_graph.sql.aggregator.lookup_target import LookupTargetAggregator
 from featurebyte.query_graph.sql.aggregator.window import WindowAggregator
 from featurebyte.query_graph.sql.ast.base import TableNode
 from featurebyte.query_graph.sql.ast.generic import AliasNode, Project
@@ -40,36 +42,41 @@
     CteStatement,
     CteStatements,
     SQLType,
     construct_cte_sql,
     quoted_identifier,
 )
 from featurebyte.query_graph.sql.parent_serving import construct_request_table_with_parent_entities
+from featurebyte.query_graph.sql.specifications.aggregate_asat import AggregateAsAtSpec
+from featurebyte.query_graph.sql.specifications.forward_aggregate_asat import (
+    ForwardAggregateAsAtSpec,
+)
 from featurebyte.query_graph.sql.specifications.lookup import LookupSpec
 from featurebyte.query_graph.sql.specifications.lookup_target import LookupTargetSpec
 from featurebyte.query_graph.sql.specs import (
-    AggregateAsAtSpec,
     AggregationSpec,
     AggregationType,
     FeatureSpec,
     ForwardAggregateSpec,
     ItemAggregationSpec,
     NonTileBasedAggregationSpec,
     TileBasedAggregationSpec,
 )
 from featurebyte.query_graph.transform.flattening import GraphFlatteningTransformer
+from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
 
 AggregatorType = Union[
     LatestAggregator,
     LookupAggregator,
     LookupTargetAggregator,
     WindowAggregator,
     ItemAggregator,
     AsAtAggregator,
     ForwardAggregator,
+    ForwardAsAtAggregator,
 ]
 AggregationSpecType = Union[TileBasedAggregationSpec, NonTileBasedAggregationSpec]
 
 sys.setrecursionlimit(10000)
 
 
 class FeatureExecutionPlan:
@@ -89,20 +96,22 @@
             AggregationType.LATEST: LatestAggregator(**aggregator_kwargs),
             AggregationType.LOOKUP: LookupAggregator(**aggregator_kwargs),
             AggregationType.LOOKUP_TARGET: LookupTargetAggregator(**aggregator_kwargs),
             AggregationType.WINDOW: WindowAggregator(**aggregator_kwargs),
             AggregationType.ITEM: ItemAggregator(**aggregator_kwargs),
             AggregationType.AS_AT: AsAtAggregator(**aggregator_kwargs),
             AggregationType.FORWARD: ForwardAggregator(**aggregator_kwargs),
+            AggregationType.FORWARD_AS_AT: ForwardAsAtAggregator(**aggregator_kwargs),
         }
         self.feature_specs: dict[str, FeatureSpec] = {}
         self.feature_entity_lookup_steps: dict[str, EntityLookupStep] = {}
         self.adapter = get_sql_adapter(source_type)
         self.source_type = source_type
         self.parent_serving_preparation = parent_serving_preparation
+        self.feature_name_dtype_mapping: dict[str, DBVarType] = {}
         self.feature_store_details = feature_store_details
 
     @property
     def required_serving_names(self) -> set[str]:
         """Returns the list of required serving names
 
         Returns
@@ -284,14 +293,16 @@
         ValueError
             If there are duplicated feature names
         """
         key = feature_spec.feature_name
         if key in self.feature_specs:
             raise ValueError(f"Duplicated feature name: {key}")
         self.feature_specs[key] = feature_spec
+        if feature_spec.feature_dtype is not None:
+            self.feature_name_dtype_mapping[key] = feature_spec.feature_dtype
 
     def add_feature_entity_lookup_step(self, entity_lookup_step: EntityLookupStep) -> None:
         """
         Add an EntityLookupStep to the plan. This looks up a required parent entity column based on
         the relationships defined at feature level.
 
         Parameters
@@ -422,16 +433,20 @@
         """
         columns: list[expressions.Expression | str] = []
         if exclude_post_aggregation:
             for agg_result_name in agg_result_names:
                 columns.append(quoted_identifier(agg_result_name))
         else:
             for feature_spec in self.feature_specs.values():
+                feature_expr = feature_spec.feature_expr
+                feature_dtype = self.feature_name_dtype_mapping.get(feature_spec.feature_name)
+                if feature_dtype is not None:
+                    feature_expr = self._cast_output_column_by_dtype(feature_expr, feature_dtype)
                 feature_alias = expressions.alias_(
-                    feature_spec.feature_expr, alias=feature_spec.feature_name, quoted=True
+                    feature_expr, alias=feature_spec.feature_name, quoted=True
                 )
                 columns.append(feature_alias)
 
         if request_table_columns:
             request_table_column_names = [
                 f"AGG.{quoted_identifier(col).sql()}"
                 for col in request_table_columns
@@ -441,14 +456,30 @@
             request_table_column_names = []
 
         table_expr = cte_context.select(*request_table_column_names, *columns).from_(
             f"{self.AGGREGATION_TABLE_NAME} AS AGG"
         )
         return table_expr
 
+    @staticmethod
+    def _cast_output_column_by_dtype(
+        feature_expr: expressions.Expression, dtype: DBVarType
+    ) -> expressions.Expression:
+        if dtype == DBVarType.FLOAT:
+            return expressions.Cast(
+                this=feature_expr,
+                to=expressions.DataType.build("DOUBLE"),
+            )
+        if dtype == DBVarType.INT:
+            return expressions.Cast(
+                this=feature_expr,
+                to=expressions.DataType.build("BIGINT"),
+            )
+        return feature_expr
+
     def get_overall_entity_lookup_steps(self) -> list[EntityLookupStep]:
         """
         Get all the entity lookup steps required
 
         Returns
         -------
         list[EntityLookupStep]
@@ -532,15 +563,15 @@
             exclude_post_aggregation=exclude_post_aggregation,
             agg_result_names=agg_result_names,
             exclude_columns=exclude_columns,
         )
         return post_aggregation_sql
 
 
-class FeatureExecutionPlanner:
+class FeatureExecutionPlanner:  # pylint: disable=too-many-instance-attributes
     """Responsible for constructing a FeatureExecutionPlan given QueryGraphModel and Node
 
     Parameters
     ----------
     graph: QueryGraphModel
         Query graph
     source_type: SourceType
@@ -559,14 +590,15 @@
         serving_names_mapping: dict[str, str] | None = None,
         source_type: SourceType | None = None,
         parent_serving_preparation: ParentServingPreparation | None = None,
     ):
         if source_type is None:
             source_type = SourceType.SNOWFLAKE
         self.graph, self.node_name_map = GraphFlatteningTransformer(graph=graph).transform()
+        self.op_struct_extractor = OperationStructureExtractor(graph=self.graph)
         self.plan = FeatureExecutionPlan(
             source_type,
             is_online_serving,
             parent_serving_preparation=parent_serving_preparation,
         )
         self.source_type = source_type
         self.serving_names_mapping = serving_names_mapping
@@ -636,14 +668,15 @@
                     node, NodeType.ITEM_GROUPBY, skip_node_type=NodeType.GROUPBY
                 )
             )
         lookup_nodes = list(self.graph.iterate_nodes(node, NodeType.LOOKUP))
         lookup_target_nodes = list(self.graph.iterate_nodes(node, NodeType.LOOKUP_TARGET))
         asat_nodes = list(self.graph.iterate_nodes(node, NodeType.AGGREGATE_AS_AT))
         forward_aggregate_nodes = list(self.graph.iterate_nodes(node, NodeType.FORWARD_AGGREGATE))
+        forward_asat_nodes = list(self.graph.iterate_nodes(node, NodeType.FORWARD_AGGREGATE_AS_AT))
 
         out: list[AggregationSpecType] = []
         if groupby_nodes:
             for groupby_node in groupby_nodes:
                 out.extend(self.get_specs_from_groupby(groupby_node))
 
         if item_groupby_nodes:
@@ -663,14 +696,18 @@
             for asat_node in asat_nodes:
                 out.extend(self.get_non_tiling_specs(AggregateAsAtSpec, asat_node))
 
         if forward_aggregate_nodes:
             for forward_aggregate_node in forward_aggregate_nodes:
                 out.extend(self.get_non_tiling_specs(ForwardAggregateSpec, forward_aggregate_node))
 
+        if forward_asat_nodes:
+            for forward_asat_node in forward_asat_nodes:
+                out.extend(self.get_non_tiling_specs(ForwardAggregateAsAtSpec, forward_asat_node))
+
         return out
 
     def get_specs_from_groupby(self, groupby_node: Node) -> Sequence[TileBasedAggregationSpec]:
         """Update FeatureExecutionPlan with a groupby query node
 
         Parameters
         ----------
@@ -730,29 +767,38 @@
         sql_graph = SQLOperationGraph(
             self.graph,
             SQLType.POST_AGGREGATION,
             source_type=self.source_type,
             aggregation_specs=aggregation_specs,
         )
         sql_node = sql_graph.build(node)
+        op_struct = self.op_struct_extractor.extract(node=node).operation_structure_map[node.name]
+        name_to_dtype = {
+            aggregation.name: aggregation.dtype for aggregation in op_struct.aggregations
+        }
 
         if isinstance(sql_node, TableNode):
             # sql_node corresponds to a FeatureGroup that results from point-in-time groupby or item
             # groupby (e.g. AggregatedTilesNode, AggregatedItemGroupby nodes)
             for feature_name, feature_expr in sql_node.columns_map.items():
                 feature_spec = FeatureSpec(
                     feature_name=feature_name,
                     feature_expr=feature_expr,
+                    feature_dtype=name_to_dtype[feature_name],
                 )
                 self.plan.add_feature_spec(feature_spec)
         else:
             if isinstance(sql_node, Project):
                 feature_name = sql_node.column_name
             elif isinstance(sql_node, AliasNode):
                 feature_name = sql_node.name
             else:
                 # Otherwise, there is no way to know about the feature name. Technically speaking
                 # this could still be previewed as an "unnamed" feature since the expression is
                 # available, but it cannot be published.
                 feature_name = "Unnamed"
-            feature_spec = FeatureSpec(feature_name=feature_name, feature_expr=sql_node.sql)
+            feature_spec = FeatureSpec(
+                feature_name=feature_name,
+                feature_expr=sql_node.sql,
+                feature_dtype=name_to_dtype.get(feature_name),
+            )
             self.plan.add_feature_spec(feature_spec)
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/feature_historical.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/feature_historical.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Historical features SQL generation
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, Tuple, cast
 
 import datetime
 from abc import ABC, abstractmethod
 
@@ -106,15 +107,15 @@
         return self.dataframe[SpecialColumnName.POINT_IN_TIME].max()
 
     async def register_as_request_table(
         self, session: BaseSession, request_table_name: str, add_row_index: bool
     ) -> None:
         if add_row_index:
             self.dataframe[InternalName.TABLE_ROW_INDEX] = np.arange(self.dataframe.shape[0])
-        await session.register_table(request_table_name, self.dataframe)
+        await session.register_table(request_table_name, self.dataframe, temporary=False)
 
 
 class MaterializedTableObservationSet(ObservationSet):
     """
     Observation set based on a materialized table in data warehouse
     """
 
@@ -138,21 +139,21 @@
             row_number = expressions.Window(
                 this=expressions.Anonymous(this="ROW_NUMBER"),
                 order=expressions.Order(expressions=[expressions.Literal.number(1)]),
             )
             columns.append(
                 expressions.alias_(row_number, alias=InternalName.TABLE_ROW_INDEX, quoted=True),
             )
-        query = sql_to_string(
-            expressions.select(*columns).from_(
+
+        await session.create_table_as(
+            table_details=TableDetails(table_name=request_table_name),
+            select_expr=expressions.select(*columns).from_(
                 get_fully_qualified_table_name(self.observation_table.location.table_details.dict())
             ),
-            source_type=session.source_type,
         )
-        await session.register_table_with_query(request_table_name, query)
 
 
 def get_internal_observation_set(
     observation_set: pd.DataFrame | ObservationTableModel,
 ) -> ObservationSet:
     """
     Get the internal observation set representation
@@ -362,15 +363,17 @@
                 select_expr=sql_expr,
             ),
             source_type=source_type,
         )
         return FeatureQuerySet(
             feature_queries=[],
             output_query=output_query,
+            output_table_name=output_table_details.table_name,
             progress_message=progress_message,
+            validate_output_row_index=output_include_row_index,
         )
 
     feature_queries = []
     feature_set_table_name_prefix = f"__TEMP_{ObjectId()}"
 
     for i, nodes_group in enumerate(node_groups):
         feature_set_expr, feature_names = get_historical_features_expr(
@@ -410,9 +413,11 @@
             select_expr=output_expr,
         ),
         source_type=source_type,
     )
     return FeatureQuerySet(
         feature_queries=feature_queries,
         output_query=output_query,
+        output_table_name=output_table_details.table_name,
         progress_message=progress_message,
+        validate_output_row_index=output_include_row_index,
     )
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/feature_preview.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/feature_preview.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature or Target preview SQL generation
 """
+
 # pylint: disable=too-many-locals
 from __future__ import annotations
 
 from typing import Any, List, Optional, cast
 
 import time
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/groupby_helper.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/groupby_helper.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utilities related to SQL generation for groupby operations
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions, parse_one
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/base.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base class for SQL interpreter.
 """
+
 from __future__ import annotations
 
 from typing import Tuple, cast
 
 from sqlglot import expressions
 
 from featurebyte.enum import SourceType
@@ -26,54 +27,53 @@
     query_graph : QueryGraphModel
         Query graph
     source_type : SourceType
         Data source type information
     """
 
     def __init__(self, query_graph: QueryGraphModel, source_type: SourceType):
-        self.query_graph = query_graph
+        self.query_graph, self.node_name_map = GraphFlatteningTransformer(
+            graph=query_graph
+        ).transform()
         self.source_type = source_type
         self.adapter = get_sql_adapter(source_type)
 
-    def flatten_graph(self, node_name: str) -> Tuple[QueryGraphModel, Node]:
-        """
-        Flatten the query graph (replace those graph node with flattened nodes)
+    def get_flattened_node(self, node_name: str) -> Node:
+        """Get node in the flattened graph
 
         Parameters
         ----------
-        node_name: str
-            Target node name
+        node_name : str
+            Query graph node name (before flattening)
 
         Returns
         -------
-        Tuple[QueryGraphModel, str]
+        Node
         """
-        graph, node_name_map = GraphFlatteningTransformer(graph=self.query_graph).transform()
-        node = graph.get_node_by_name(node_name_map[node_name])
-        return graph, node
+        return self.query_graph.get_node_by_name(self.node_name_map[node_name])
 
     def construct_shape_sql(self, node_name: str) -> Tuple[str, int]:
         """Construct SQL to get row count from a given node
 
         Parameters
         ----------
         node_name : str
             Query graph node name
 
         Returns
         -------
         Tuple[str, int]
             SQL code to execute, and column count
         """
-        flat_graph, flat_node = self.flatten_graph(node_name=node_name)
-        operation_structure = QueryGraph(**flat_graph.dict()).extract_operation_structure(
+        flat_node = self.get_flattened_node(node_name)
+        operation_structure = QueryGraph(**self.query_graph.dict()).extract_operation_structure(
             flat_node, keep_all_source_columns=True
         )
         sql_node = SQLOperationGraph(
-            flat_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
+            self.query_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
         ).build(flat_node)
         assert isinstance(sql_node, (TableNode, ExpressionNode))
         if isinstance(sql_node, TableNode):
             sql_tree = sql_node.sql
         else:
             sql_tree = sql_node.sql_standalone
 
@@ -96,14 +96,14 @@
         node_name : str
             Query graph node name
 
         Returns
         -------
         Select
         """
-        flat_graph, flat_node = self.flatten_graph(node_name=node_name)
+        flat_node = self.get_flattened_node(node_name)
         sql_graph = SQLOperationGraph(
-            flat_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
+            self.query_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
         )
         sql_node = sql_graph.build(flat_node)
         assert isinstance(sql_node, TableNode)
         return cast(expressions.Select, sql_node.sql)
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/preview.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/preview.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Preview mixin for Graph Interpreter
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, List, Optional
 from typing import OrderedDict as OrderedDictT
 from typing import Set, Tuple, cast
 
 from collections import OrderedDict
@@ -136,31 +137,31 @@
             Whether to skip data conversion
 
         Returns
         -------
         Tuple[expressions.Select, dict[Optional[str], DBVarType]]
             SQL expression for data sample, column to apply conversion on resulting dataframe
         """
-        flat_graph, flat_node = self.flatten_graph(node_name=node_name)
+        flat_node = self.get_flattened_node(node_name)
         sql_graph = SQLOperationGraph(
-            flat_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
+            self.query_graph, sql_type=SQLType.MATERIALIZE, source_type=self.source_type
         )
         sql_node = sql_graph.build(flat_node)
 
         assert isinstance(sql_node, (TableNode, ExpressionNode))
         if isinstance(sql_node, TableNode):
             sql_tree = sql_node.sql
         else:
             sql_tree = sql_node.sql_standalone
 
         assert isinstance(sql_tree, expressions.Select)
 
         # apply type conversions
         operation_structure = QueryGraph(**self.query_graph.dict()).extract_operation_structure(
-            self.query_graph.get_node_by_name(node_name), keep_all_source_columns=True
+            self.query_graph.get_node_by_name(flat_node.name), keep_all_source_columns=True
         )
         if skip_conversion:
             type_conversions: dict[Optional[str], DBVarType] = {}
         else:
             sql_tree, type_conversions = self._apply_type_conversions(
                 sql_tree=sql_tree, columns=operation_structure.columns
             )
@@ -648,37 +649,58 @@
                 )
             )
         return expressions.select(*selections).from_(
             expressions.Subquery(this=cat_count_dict, alias="count_dict")
         )
 
     @staticmethod
-    def _get_ctes_with_casted_data(
-        sql_tree: expressions.Expression,
-    ) -> Tuple[expressions.Select, List[CteStatement]]:
-        cte_statements: List[CteStatement] = [("data", sql_tree)]
-
+    def _get_cte_with_casted_data(sql_tree: expressions.Expression) -> CteStatement:
         # get subquery with columns casted to string to compute value counts
         casted_columns = []
         for col_expr in sql_tree.expressions:
             col_name = col_expr.alias_or_name
             # add casted columns
             casted_columns.append(
                 expressions.alias_(
                     expressions.Cast(this=quoted_identifier(col_name), to=parse_one("STRING")),
                     col_name,
                     quoted=True,
                 )
             )
         sql_tree = expressions.select(*casted_columns).from_("data")
-        cte_statements.append((CASTED_DATA_TABLE_NAME, sql_tree))
+        return CASTED_DATA_TABLE_NAME, sql_tree
 
-        return sql_tree, cte_statements
+    @staticmethod
+    def _clip_column_before_stats_func(
+        col_expr: expressions.Expression,
+        col_dtype: DBVarType,
+        stats_name: str,
+    ) -> expressions.Expression:
+        if col_dtype not in DBVarType.supported_timestamp_types():
+            return col_expr
+
+        if stats_name not in {"min", "max"}:
+            return col_expr
+
+        invalid_mask = expressions.or_(
+            expressions.LT(
+                this=col_expr,
+                expression=make_literal_value("1900-01-01", cast_as_timestamp=True),
+            ),
+            expressions.GT(
+                this=col_expr,
+                expression=make_literal_value("2200-01-01", cast_as_timestamp=True),
+            ),
+        )
+        clipped_col_expr = expressions.If(
+            this=invalid_mask, true=expressions.Null(), false=col_expr
+        )
+        return clipped_col_expr
 
-    def _construct_stats_sql(  # pylint: disable=too-many-locals,too-many-branches
+    def _construct_stats_sql(  # pylint: disable=too-many-locals,too-many-branches,too-many-statements
         self,
         sql_tree: expressions.Select,
         columns: List[ViewDataColumn],
         stats_names: Optional[List[str]] = None,
     ) -> Tuple[expressions.Select, List[str], List[ViewDataColumn]]:
         """
         Construct sql to retrieve statistics for an SQL view
@@ -696,15 +718,20 @@
         -------
         Tuple[expressions.Select, List[str], List[ViewDataColumn]]
             Select expression, row indices, columns
         """
         columns_info = {
             column.name: column for column in columns if column.name or len(columns) == 1
         }
-        sql_tree, cte_statements = self._get_ctes_with_casted_data(sql_tree)
+
+        # original data
+        cte_statements: List[CteStatement] = [("data", sql_tree)]
+
+        # data casted to string
+        cte_casted_data = self._get_cte_with_casted_data(sql_tree)
 
         # only extract requested stats if specified
         required_stats_expressions = {}
         for stats_name, stats_values in self.stats_expressions.items():
             if stats_names is None or stats_name in stats_names:
                 required_stats_expressions[stats_name] = stats_values
 
@@ -732,26 +759,33 @@
             )
             top_required = "top" in required_stats_expressions and self._is_dtype_supported(
                 column.dtype, required_stats_expressions["top"][1]
             )
             if entropy_required or top_required:
                 table_name = f"counts__{column_idx}"
                 count_stats_sql = self._construct_count_stats_sql(
-                    col_expr=col_expr, column_idx=column_idx, col_dtype=column.dtype
+                    col_expr=quoted_identifier(col_name),
+                    column_idx=column_idx,
+                    col_dtype=column.dtype,
                 )
                 cte_statements.append((table_name, count_stats_sql))
                 count_tables.append(table_name)
 
             # stats
             for stats_name, (stats_func, supported_dtypes) in required_stats_expressions.items():
                 if stats_func:
                     if self._is_dtype_supported(column.dtype, supported_dtypes):
+                        stats_func_col_expr = self._clip_column_before_stats_func(
+                            col_expr=col_expr,
+                            col_dtype=column.dtype,
+                            stats_name=stats_name,
+                        )
                         stats_selections.append(
                             expressions.alias_(
-                                stats_func(col_expr, column_idx),
+                                stats_func(stats_func_col_expr, column_idx),
                                 f"{stats_name}__{column_idx}",
                                 quoted=True,
                             ),
                         )
                     else:
                         stats_selections.append(
                             self._empty_value_expr(f"{stats_name}__{column_idx}")
@@ -789,14 +823,25 @@
         all_tables = []
         if stats_selections:
             sql_tree = expressions.select(*stats_selections).from_("data")
             cte_statements.append(("stats", sql_tree))
             all_tables.append("stats")
         all_tables.extend(count_tables)
 
+        # check if data casted to string is used and if so add it to cte_statements
+        used_casted_data = False
+        for _, cte_expr in cte_statements[1:]:
+            for cur_expr, _, _ in cte_expr.walk():
+                if isinstance(cur_expr, expressions.Identifier):
+                    if cur_expr.alias_or_name == CASTED_DATA_TABLE_NAME:
+                        used_casted_data = True
+                        break
+        if used_casted_data:
+            cte_statements.insert(1, cte_casted_data)
+
         sql_tree = self._join_all_tables(cte_statements, all_tables).select(*final_selections)
 
         return sql_tree, ["dtype"] + list(required_stats_expressions.keys()), output_columns
 
     @staticmethod
     def _join_all_tables(
         cte_statements: List[CteStatement],
@@ -860,16 +905,17 @@
             single query.
 
         Returns
         -------
         DescribeQueries
             SQL code, type conversions to apply on result, row indices, columns
         """
+        flat_node = self.get_flattened_node(node_name)
         operation_structure = QueryGraph(**self.query_graph.dict()).extract_operation_structure(
-            self.query_graph.get_node_by_name(node_name), keep_all_source_columns=True
+            self.query_graph.get_node_by_name(flat_node.name), keep_all_source_columns=True
         )
 
         sample_sql_tree, type_conversions = self._construct_sample_sql(
             node_name=node_name,
             num_rows=num_rows,
             seed=seed,
             from_timestamp=from_timestamp,
@@ -924,15 +970,18 @@
         str
         """
         sql_tree, _ = self._construct_sample_sql(
             node_name=node_name,
             num_rows=num_rows,
             seed=seed,
         )
-        sql_tree, cte_statements = self._get_ctes_with_casted_data(sql_tree)
+        cte_statements: List[CteStatement] = [
+            ("data", sql_tree),
+            self._get_cte_with_casted_data(sql_tree),
+        ]
         # It's expected that this function is called on a node that is associated with a column and
         # not a frame, so here we simply take the first column.
         col_expr = sql_tree.expressions[0]
         col_name = col_expr.alias_or_name
         cat_counts = self._get_cat_counts(
             quoted_identifier(col_name), num_categories_limit=num_categories_limit
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/interpreter/tile.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/interpreter/tile.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains the Query Graph Interpreter
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from dataclasses import dataclass
 
 from featurebyte.enum import SourceType
@@ -144,15 +145,15 @@
         op_struct = op_struct_info.operation_structure_map[groupby_input_node.name]
         for column in op_struct.columns:
             if (
                 column.name == groupby_node.parameters.timestamp
                 and isinstance(column, SourceDataColumn)
                 and column.table_id is not None
             ):
-                assert isinstance(column, SourceDataColumn)
+                assert isinstance(column, SourceDataColumn), "SourceDataColumn expected"
                 return EventTableTimestampFilter(
                     timestamp_column_name=column.name,
                     event_table_id=column.table_id,
                 )
 
         return None
 
@@ -186,16 +187,16 @@
         ).build(groupby_node)
         sql = groupby_sql_node.sql
         tile_table_id = groupby_node.parameters.tile_id
         aggregation_id = groupby_node.parameters.aggregation_id
         entity_columns = groupby_sql_node.keys
         tile_value_columns = [spec.tile_column_name for spec in groupby_sql_node.tile_specs]
         tile_value_types = [spec.tile_column_type for spec in groupby_sql_node.tile_specs]
-        assert tile_table_id is not None
-        assert aggregation_id is not None
+        assert tile_table_id is not None, "Tile table ID is required"
+        assert aggregation_id is not None, "Aggregation ID is required"
         sql_template = SqlExpressionTemplate(sql_expr=sql, source_type=self.source_type)
         info = TileGenSql(
             tile_table_id=tile_table_id,
             tile_id_version=groupby_node.parameters.tile_id_version,
             aggregation_id=aggregation_id,
             sql_template=sql_template,
             columns=groupby_sql_node.columns,
@@ -235,12 +236,12 @@
         is_on_demand : bool
             Whether the SQL is for on-demand tile building for historical features
 
         Returns
         -------
         List[TileGenSql]
         """
-        flat_graph, flat_starting_node = self.flatten_graph(node_name=starting_node.name)
+        flat_starting_node = self.get_flattened_node(starting_node.name)
         generator = TileSQLGenerator(
-            flat_graph, is_on_demand=is_on_demand, source_type=self.source_type
+            self.query_graph, is_on_demand=is_on_demand, source_type=self.source_type
         )
         return generator.construct_tile_gen_sql(flat_starting_node)
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/materialisation.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/materialisation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation related to materialising tables such as ObservationTable
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Select, alias_, select
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/offset.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/offset.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Sql helper related to offset
 """
+
 from __future__ import annotations
 
 import pandas as pd
 from sqlglot.expressions import Expression
 
 from featurebyte.enum import StrEnum
 from featurebyte.query_graph.sql.adapter import BaseAdapter
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/online_serving.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/online_serving.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for online serving
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple, Union, cast
 
 import time
 from datetime import datetime
 
@@ -33,15 +34,14 @@
     maybe_add_row_index_column,
     split_nodes,
 )
 from featurebyte.query_graph.sql.common import (
     REQUEST_TABLE_NAME,
     get_fully_qualified_table_name,
     get_qualified_column_identifier,
-    sql_to_string,
 )
 from featurebyte.query_graph.sql.dataframe import construct_dataframe_sql_expr
 from featurebyte.query_graph.sql.entity import (
     DUMMY_ENTITY_COLUMN_NAME,
     DUMMY_ENTITY_VALUE,
     get_combined_serving_names,
     get_combined_serving_names_expr,
@@ -285,38 +285,44 @@
     return current_timestamp_expr
 
 
 def add_concatenated_serving_names(
     select_expr: expressions.Select,
     concatenate_serving_names: Optional[list[str]],
     source_type: SourceType,
+    serving_names_table_alias: Optional[str] = None,
 ) -> expressions.Select:
     """
     Add concatenated serving name column to the provided Select statement which is assumed to
     contain all the serving names.
 
     Parameters
     ----------
     select_expr: expressions.Select
         Select statement
     concatenate_serving_names: Optional[list[str]]
         List of serving names to concatenate
     source_type: SourceType
         Source type information
+    serving_names_table_alias: Optional[str]
+        Table alias for the serving names. Serving names will not be table qualified if not
+        provided.
 
     Returns
     -------
     expressions.Select
     """
     if concatenate_serving_names is None:
         return select_expr
     if len(concatenate_serving_names) > 1:
         updated_select_expr = select_expr.select(
             expressions.alias_(
-                get_combined_serving_names_expr(concatenate_serving_names),
+                get_combined_serving_names_expr(
+                    concatenate_serving_names, serving_names_table_alias=serving_names_table_alias
+                ),
                 alias=get_combined_serving_names(concatenate_serving_names),
                 quoted=True,
             )
         )
     elif source_type == SourceType.DATABRICKS_UNITY and len(concatenate_serving_names) == 0:
         updated_select_expr = select_expr.select(
             expressions.alias_(
@@ -404,14 +410,17 @@
                 select_expr=sql_expr,
             )
         else:
             output_query = sql_expr
         return FeatureQuerySet(
             feature_queries=[],
             output_query=output_query,
+            output_table_name=(
+                output_table_details.table_name if output_table_details is not None else None
+            ),
             progress_message=PROGRESS_MESSAGE_COMPUTING_ONLINE_FEATURES,
         )
 
     feature_queries = []
     feature_set_table_name_prefix = f"__TEMP_{ObjectId()}"
 
     for i, nodes_group in enumerate(node_groups):
@@ -445,23 +454,27 @@
         request_table_columns=request_table_columns,
         output_include_row_index=output_include_row_index,
     )
     output_expr = add_concatenated_serving_names(
         output_expr,
         concatenate_serving_names,
         source_type,
+        serving_names_table_alias="REQ",
     )
     if output_table_details is not None:
         output_expr = get_sql_adapter(source_type).create_table_as(  # type: ignore[assignment]
             table_details=output_table_details,
             select_expr=output_expr,
         )
     return FeatureQuerySet(
         feature_queries=feature_queries,
         output_query=output_expr,
+        output_table_name=(
+            output_table_details.table_name if output_table_details is not None else None
+        ),
         progress_message=PROGRESS_MESSAGE_COMPUTING_ONLINE_FEATURES,
     )
 
 
 class TemporaryBatchRequestTable(FeatureByteBaseModel):
     """
     Temporary batch request table created manually without going through the standard table
@@ -469,15 +482,15 @@
     materialization.
     """
 
     column_names: List[str]
     table_details: TableDetails
 
 
-async def get_online_features(  # pylint: disable=too-many-locals
+async def get_online_features(  # pylint: disable=too-many-locals,too-many-branches
     session: BaseSession,
     graph: QueryGraph,
     nodes: list[Node],
     request_data: Union[pd.DataFrame, BatchRequestTableModel, TemporaryBatchRequestTable],
     source_type: SourceType,
     online_store_table_version_service: OnlineStoreTableVersionService,
     parent_serving_preparation: Optional[ParentServingPreparation] = None,
@@ -534,51 +547,54 @@
         else:
             request_table_details = request_data.table_details
             request_table_columns = request_data.column_names[:]
 
     if len(node_groups) > 1:
         # If using multiple queries, FeatureQuerySet requires request table to be registered as a
         # table beforehand.
-        request_table_name = f"{REQUEST_TABLE_NAME}_{session.generate_session_unique_id()}"
         if isinstance(request_data, pd.DataFrame):
-            await session.register_table(request_table_name, request_data)
+            request_table_name = f"{REQUEST_TABLE_NAME}_{session.generate_session_unique_id()}"
+            await session.register_table(request_table_name, request_data, temporary=False)
         else:
             assert request_table_details is not None
-            query = sql_to_string(
-                expressions.select(expressions.Star()).from_(
-                    get_fully_qualified_table_name(request_table_details.dict())
-                ),
-                source_type=session.source_type,
-            )
-            await session.register_table_with_query(request_table_name, query)
+            request_table_name = request_table_details.table_name
     else:
         request_table_name = None
 
-    aggregation_result_names = get_aggregation_result_names(graph, nodes, source_type)
-    versions = await online_store_table_version_service.get_versions(aggregation_result_names)
-    query_set = get_online_features_query_set(
-        graph,
-        node_groups,
-        source_type=source_type,
-        request_table_columns=request_table_columns,
-        output_feature_names=get_feature_names(graph, nodes),
-        request_table_name=request_table_name,
-        request_table_expr=request_table_expr,
-        request_table_details=request_table_details,
-        parent_serving_preparation=parent_serving_preparation,
-        request_timestamp=request_timestamp,
-        output_table_details=output_table_details,
-        output_include_row_index=request_table_details is None,
-        concatenate_serving_names=concatenate_serving_names,
-    )
-    fill_version_placeholders_for_query_set(query_set, versions)
-    logger.debug(f"OnlineServingService sql prep elapsed: {time.time() - tic:.6f}s")
+    try:
+        aggregation_result_names = get_aggregation_result_names(graph, nodes, source_type)
+        versions = await online_store_table_version_service.get_versions(aggregation_result_names)
+        query_set = get_online_features_query_set(
+            graph,
+            node_groups,
+            source_type=source_type,
+            request_table_columns=request_table_columns,
+            output_feature_names=get_feature_names(graph, nodes),
+            request_table_name=request_table_name,
+            request_table_expr=request_table_expr,
+            request_table_details=request_table_details,
+            parent_serving_preparation=parent_serving_preparation,
+            request_timestamp=request_timestamp,
+            output_table_details=output_table_details,
+            output_include_row_index=request_table_details is None,
+            concatenate_serving_names=concatenate_serving_names,
+        )
+        fill_version_placeholders_for_query_set(query_set, versions)
+        logger.debug(f"OnlineServingService sql prep elapsed: {time.time() - tic:.6f}s")
+
+        tic = time.time()
+        df_features = await execute_feature_query_set(session, query_set)
+    finally:
+        if request_table_name is not None and request_table_details is None:
+            await session.drop_table(
+                table_name=request_table_name,
+                schema_name=session.schema_name,
+                database_name=session.database_name,
+            )
 
-    tic = time.time()
-    df_features = await execute_feature_query_set(session, query_set)
     if output_table_details is None:
         assert df_features is not None
         assert isinstance(request_data, pd.DataFrame)
         df_features = df_features.sort_values(InternalName.TABLE_ROW_INDEX).drop(
             InternalName.TABLE_ROW_INDEX, axis=1
         )
         df_features.index = request_data.index
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/online_serving_util.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/online_serving_util.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utilities related to online serving
 """
+
 from __future__ import annotations
 
 import hashlib
 import json
 
 from bson import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/online_store_compute_query.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/online_store_compute_query.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for online store compute queries
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, Tuple, cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions, parse_one
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/parent_serving.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/parent_serving.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SQL generation for looking up parent entities
 """
+
 from __future__ import annotations
 
 from typing import List
 
 from dataclasses import dataclass
 
 from sqlglot import expressions
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/query_graph_util.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/query_graph_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Query graph util module
 """
+
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
 
 
 def get_parent_dtype(
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/scd_helper.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/scd_helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 """
 Utilities for SCD join / lookup
 """
+
 from __future__ import annotations
 
 from typing import Literal, Optional, cast
 
 from dataclasses import dataclass
 
 from sqlglot import expressions, parse_one
 from sqlglot.expressions import Expression, Select, alias_, select
 
 from featurebyte.query_graph.sql.adapter import BaseAdapter
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import get_qualified_column_identifier, quoted_identifier
 
 # Internally used identifiers when constructing SQL
+from featurebyte.query_graph.sql.deduplication import get_deduplicated_expr
 from featurebyte.query_graph.sql.offset import OffsetDirection, add_offset_to_timestamp
 
 TS_COL = "__FB_TS_COL"
 EFFECTIVE_TS_COL = "__FB_EFFECTIVE_TS_COL"
 KEY_COL = "__FB_KEY_COL"
 LAST_TS = "__FB_LAST_TS"
 END_TS = "__FB_END_TS"
@@ -52,31 +54,62 @@
         -------
         Expression
         """
         if isinstance(self.timestamp_column, str):
             return quoted_identifier(self.timestamp_column)
         return self.timestamp_column
 
-    def as_subquery(self, alias: Optional[str] = None) -> Expression:
+    def as_subquery(
+        self,
+        alias: Optional[str] = None,
+        remove_missing_timestamp_values: bool = False,
+    ) -> Expression:
         """
         Returns an expression that can be selected from (converted to subquery if required)
 
         Parameters
         ----------
         alias: Optional[str]
             Table alias, if specified.
+        remove_missing_timestamp_values: bool
+            Whether to filter out missing values in timestamp column in the returned subquery
 
         Returns
         -------
         Expression
         """
         if isinstance(self.expr, str):
+            # This is when joining with tile tables. We can assume that tile index (the timestamp
+            # column) will not have any missing values, so remove_missing_timestamp_values is
+            # ignored.
             return expressions.Table(this=expressions.Identifier(this=self.expr), alias=alias)
         assert isinstance(self.expr, Select)
-        return cast(Expression, self.expr.subquery(alias=alias))
+        if remove_missing_timestamp_values:
+            expr = self.expr_with_non_missing_timestamp_values
+        else:
+            expr = self.expr
+        return cast(Expression, expr.subquery(alias=alias, copy=False))
+
+    @property
+    def expr_with_non_missing_timestamp_values(self) -> Select:
+        """
+        Get an expression for the table with missing timestamp values removed
+
+        Returns
+        -------
+        Select
+        """
+        assert isinstance(self.expr, Select)
+        assert isinstance(self.timestamp_column, str)
+        return self.expr.where(
+            expressions.Is(
+                this=quoted_identifier(self.timestamp_column),
+                expression=expressions.Not(this=expressions.Null()),
+            )
+        )
 
 
 def get_scd_join_expr(
     left_table: Table,
     right_table: Table,
     join_type: Literal["inner", "left"],
     adapter: BaseAdapter,
@@ -154,27 +187,47 @@
         offset=offset,
         offset_direction=offset_direction,
         allow_exact_match=allow_exact_match,
         convert_timestamps_to_utc=convert_timestamps_to_utc,
     )
 
     left_subquery = left_view_with_last_ts_expr.subquery(alias="L")
+
+    # Ensure right table (scd side) is unique in the join columns so that the join preserve the
+    # number of rows in the left table
+    assert isinstance(right_table.timestamp_column, str)
+    if isinstance(right_table.expr, Select):
+        # right_table.expr is a Select instance if it is a user provided SCD table.
+        deduplicated_expr = get_deduplicated_expr(
+            adapter=adapter,
+            table_expr=right_table.expr_with_non_missing_timestamp_values,
+            expected_primary_keys=[right_table.timestamp_column] + right_table.join_keys,
+        )
+        right_table = Table(
+            expr=deduplicated_expr,
+            timestamp_column=right_table.timestamp_column,
+            join_keys=right_table.join_keys,
+            input_columns=right_table.input_columns,
+            output_columns=right_table.output_columns,
+        )
     right_subquery = right_table.as_subquery(alias="R")
+
     assert isinstance(right_table.timestamp_column, str)
     join_conditions = [
         expressions.EQ(
             this=get_qualified_column_identifier(LAST_TS, "L"),
             expression=get_qualified_column_identifier(right_table.timestamp_column, "R"),
         ),
     ] + _key_cols_equality_conditions(right_table.join_keys)
 
-    select_expr = select_expr.from_(left_subquery).join(
+    select_expr = select_expr.from_(left_subquery, copy=False).join(
         right_subquery,
         join_type=join_type,
         on=expressions.and_(*join_conditions),
+        copy=False,
     )
     return select_expr
 
 
 def _convert_to_utc_ntz(
     col_expr: expressions.Expression, adapter: BaseAdapter
 ) -> expressions.Expression:
@@ -296,15 +349,15 @@
             quoted=True,
         ),
     ).from_(left_table.as_subquery())
 
     # Include all columns specified for the left table
     for input_col, output_col in zip(left_table.input_columns, left_table.output_columns):
         left_view_with_ts_and_key = left_view_with_ts_and_key.select(
-            alias_(quoted_identifier(input_col), alias=output_col, quoted=True)
+            alias_(quoted_identifier(input_col), alias=output_col, quoted=True), copy=False
         )
 
     # Right table. Set up the same special columns. The ordering of the columns in the SELECT
     # statement matters (must match the left table's).
     right_ts_and_key = select(
         alias_(right_ts_col, alias=TS_COL, quoted=True),
         *_alias_join_keys_as_key_cols(right_table.join_keys),
@@ -314,20 +367,20 @@
             quoted=True,
         ),
         alias_(
             make_literal_value(TS_TIE_BREAKER_VALUE_SCD_TABLE),
             alias=TS_TIE_BREAKER_COL,
             quoted=True,
         ),
-    ).from_(right_table.as_subquery())
+    ).from_(right_table.as_subquery(remove_missing_timestamp_values=True), copy=False)
 
     # Include all columns specified for the right table, but simply set them as NULL.
     for column in left_table.output_columns:
         right_ts_and_key = right_ts_and_key.select(
-            alias_(expressions.NULL, alias=column, quoted=True)
+            alias_(expressions.NULL, alias=column, quoted=True), copy=False
         )
 
     # Merge the above two temporary tables into one
     all_ts_and_key = expressions.Union(
         this=left_view_with_ts_and_key,
         expression=right_ts_and_key,
         distinct=False,
@@ -337,15 +390,17 @@
     #
     # When allow_exact_match=True and TS_COL ties, after the sorting in LAG, rows from the left
     # table will come after SCD rows in the right table since left rows have a larger value for
     # TS_TILE_BREAKER_VALUE (2) than right rows (always 1). This way, LAG yields the exact matching
     # date, instead of a previous effective date. Vice versa for allow_exact_match=False.
     order = expressions.Order(
         expressions=[
-            expressions.Ordered(this=quoted_identifier(TS_COL)),
+            expressions.Ordered(
+                this=expressions.Column(this=quoted_identifier(TS_COL)), nulls_first=True
+            ),
             expressions.Ordered(this=quoted_identifier(TS_TIE_BREAKER_COL)),
         ]
     )
     num_join_keys = len(left_table.join_keys)
     matched_effective_timestamp_expr = expressions.Window(
         this=expressions.IgnoreNulls(
             this=expressions.Anonymous(
@@ -371,18 +426,18 @@
         .from_(
             select(
                 *_key_cols_as_quoted_identifiers(num_join_keys),
                 alias_(matched_effective_timestamp_expr, alias=LAST_TS, quoted=True),
                 *[quoted_identifier(col) for col in left_table.output_columns],
                 quoted_identifier(EFFECTIVE_TS_COL),
             )
-            .from_(all_ts_and_key.subquery())
-            .subquery()
+            .from_(all_ts_and_key.subquery(copy=False))
+            .subquery(copy=False)
         )
-        .where(filter_original_left_view_rows)
+        .where(filter_original_left_view_rows, copy=False)
     )
 
     return left_view_with_effective_timestamp_expr
 
 
 def _alias_join_keys_as_key_cols(join_keys: list[str]) -> list[Expression]:
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/base_lookup.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/base_lookup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base lookup spec
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from abc import ABC
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/lookup.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/lookup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Lookup spec
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from dataclasses import dataclass
 
 from featurebyte.query_graph.model.graph import QueryGraphModel
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/specifications/lookup_target.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/specifications/lookup_target.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Lookup target spec
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from dataclasses import dataclass
 
 from featurebyte.query_graph.model.graph import QueryGraphModel
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/specs.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/specs.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for data structures that describe different types of aggregations that form features
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Tuple, Type, TypeVar, cast
 
 import hashlib
 import json
 from abc import ABC, abstractmethod
@@ -15,16 +16,14 @@
 from sqlglot.expressions import Expression, Select
 
 from featurebyte.enum import AggFunc, DBVarType, SourceType, StrEnum
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.generic import (
-    AggregateAsAtNode,
-    AggregateAsAtParameters,
     ForwardAggregateNode,
     ForwardAggregateParameters,
     GroupByNode,
     ItemGroupbyNode,
     ItemGroupbyParameters,
 )
 from featurebyte.query_graph.node.mixin import BaseGroupbyParameters
@@ -54,14 +53,15 @@
     LATEST = "latest"
     LOOKUP = "lookup"
     LOOKUP_TARGET = "lookup_target"
     WINDOW = "window"
     ITEM = "item"
     AS_AT = "as_at"
     FORWARD = "forward"
+    FORWARD_AS_AT = "forward_as_at"
 
 
 @dataclass
 class AggregationSpec(ABC):
     """
     Base class of all aggregation specifications
     """
@@ -624,84 +624,14 @@
                 parent_dtype=cls.get_parent_dtype_from_graph(graph, node.parameters.parent, node),
                 agg_result_name_include_serving_names=agg_result_name_include_serving_names,
             )
         ]
 
 
 @dataclass
-class AggregateAsAtSpec(NonTileBasedAggregationSpec):
-    """
-    As-at aggregation specification
-    """
-
-    parameters: AggregateAsAtParameters
-    parent_dtype: Optional[DBVarType]
-
-    @property
-    def agg_result_name(self) -> str:
-        """Column name of the aggregated result
-
-        Returns
-        -------
-        str
-            Column name of the aggregated result
-        """
-        args = self._get_additional_agg_result_name_params()
-        return self.get_agg_result_name_from_groupby_parameters(self.parameters, *args)
-
-    def _get_additional_agg_result_name_params(self) -> list[Any]:
-        args = []
-        if self.parameters is not None and self.parameters.offset is not None:
-            args.append(self.parameters.offset)
-        return args
-
-    @property
-    def aggregation_type(self) -> AggregationType:
-        return AggregationType.AS_AT
-
-    def get_source_hash_parameters(self) -> dict[str, Any]:
-        # Input to be aggregated
-        params: dict[str, Any] = {"source_expr": self.source_expr.sql()}
-
-        # Parameters that affect whether aggregation can be done together (e.g. same groupby keys)
-        parameters_dict = self.parameters.dict(exclude={"parent", "agg_func", "name"})
-        if parameters_dict.get("entity_ids") is not None:
-            parameters_dict["entity_ids"] = [
-                str(entity_id) for entity_id in parameters_dict["entity_ids"]
-            ]
-        params["parameters"] = parameters_dict
-
-        return params
-
-    @classmethod
-    def construct_specs(
-        cls,
-        node: Node,
-        aggregation_source: AggregationSource,
-        serving_names_mapping: Optional[dict[str, str]],
-        graph: Optional[QueryGraphModel],
-        agg_result_name_include_serving_names: bool,
-    ) -> list[AggregateAsAtSpec]:
-        assert isinstance(node, AggregateAsAtNode)
-        return [
-            AggregateAsAtSpec(
-                node_name=node.name,
-                feature_name=node.parameters.name,
-                parameters=node.parameters,
-                parent_dtype=cls.get_parent_dtype_from_graph(graph, node.parameters.parent, node),
-                aggregation_source=aggregation_source,
-                entity_ids=cast(List[ObjectId], node.parameters.entity_ids),
-                serving_names=node.parameters.serving_names,
-                serving_names_mapping=serving_names_mapping,
-                agg_result_name_include_serving_names=agg_result_name_include_serving_names,
-            )
-        ]
-
-
-@dataclass
 class ForwardAggregateSpec(NonTileBasedAggregationSpec):
     """
     ForwardAggregateSpec contains all information required to generate sql for a forward aggregate target.
     """
 
     parameters: ForwardAggregateParameters
     parent_dtype: Optional[DBVarType]
@@ -753,7 +683,8 @@
 class FeatureSpec:
     """
     Feature specification
     """
 
     feature_name: str
     feature_expr: Expression
+    feature_dtype: Optional[DBVarType]
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/template.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/template.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for constructs to work with template SQL code with placeholders
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from sqlglot import expressions
 from sqlglot.expressions import Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/tile_compute.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/tile_compute.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 On-demand tile computation for feature preview
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 import pandas as pd
 from sqlglot import expressions
 from sqlglot.expressions import Expression, Select, select
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/tile_util.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/tile_util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module for tiles related utilities
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, Tuple, cast
 
 from sqlglot import expressions, parse_one
 from sqlglot.expressions import Expression
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/tiling.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/tiling.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains helpers related to tiling-based aggregation functions
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/sql/vector_helper.py` & `featurebyte-1.0.3/featurebyte/query_graph/sql/vector_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Helper functions for vector aggregation
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from featurebyte.enum import AggFunc, DBVarType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/base.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains base class used in query graph transform directory.
 """
+
 from typing import Any, Dict, Generic, List, Tuple, TypeVar
 
 from abc import abstractmethod
 
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/decompose_point.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/decompose_point.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains
 """
+
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 from dataclasses import dataclass
 
 from pydantic import BaseModel
 
 from featurebyte.enum import DBVarType, TableDataType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/definition.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Definition extractor used to extract the definition hash of a query graph.
 """
+
 from typing import Any, Dict, List, Tuple
 
 from dataclasses import dataclass
 
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node import Node
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/flattening.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/flattening.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains graph flattening related classes.
 """
+
 from typing import Dict, List, Optional, Set
 
 from pydantic import BaseModel, Field
 
 from featurebyte.query_graph.enum import GraphNodeType
 from featurebyte.query_graph.model.graph import GraphNodeNameMap, QueryGraphModel
 from featurebyte.query_graph.node import Node
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/null_filling_value.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/null_filling_value.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 """
 This graph extractor is responsible for extracting null filling values from the graph.
 """
+
 from typing import Any, Dict, List, Optional, Tuple
 
 import pandas as pd
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import Scalar
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
 from featurebyte.query_graph.model.graph import NodeNameMap, QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.metadata.operation import OperationStructure
 from featurebyte.query_graph.node.mixin import AggregationOpStructMixin
 from featurebyte.query_graph.node.request import RequestColumnNode
 from featurebyte.query_graph.transform.base import BaseGraphExtractor
 from featurebyte.query_graph.transform.on_demand_function import OnDemandFeatureFunctionExtractor
 from featurebyte.query_graph.transform.operation_structure import OperationStructureExtractor
+from featurebyte.typing import Scalar
 
 
 class NullFillingValueGlobalState(BaseModel):
     """
     NullFillingValueGlobalState encapsulates the global state for null filling value extraction.
     """
 
@@ -128,14 +129,15 @@
             sql_input_var_prefix="",
             sql_request_input_var_prefix="",
             sql_comment="",
             function_name="extract_null_filling_value",
             input_var_prefix="",
             request_input_var_prefix="agg_col",
             output_dtype=DBVarType.FLOAT,
+            to_generate_null_filling_function=True,
         )
         input_values = []
         for sql_input_info in code_state.sql_inputs_info:
             val = "np.nan"
             if sql_input_info.py_type == "pd.Timestamp":
                 val = "pd.NaT"
             input_values.append(f"{sql_input_info.py_input_var_name}={val}")
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/offline_store_ingest.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/offline_store_ingest.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains offline store ingest query extraction related classes.
 """
+
 from typing import Any, Dict, List, Optional, Set
 
 from dataclasses import dataclass
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.graph_node.base import GraphNode
 from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/on_demand_function.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/on_demand_function.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 On demand function (for DataBricks) related classes and functions.
 """
+
 from typing import Any, Dict, List, Tuple
 
 from pydantic import BaseModel, Field
 
 from featurebyte.enum import DBVarType
 from featurebyte.query_graph.enum import GraphNodeType, NodeType
 from featurebyte.query_graph.node import Node
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/on_demand_view.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/on_demand_view.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 """
 On demand feature view (for Feast) related classes and functions.
 """
+
 from typing import Any, Dict, List, Tuple
 
 import textwrap
 
 from pydantic import BaseModel, Field
 
-from featurebyte.common.typing import Scalar
 from featurebyte.enum import SpecialColumnName
 from featurebyte.query_graph.enum import FEAST_TIMESTAMP_POSTFIX
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.metadata.config import OnDemandViewCodeGenConfig
 from featurebyte.query_graph.node.metadata.sdk_code import (
     CodeGenerator,
     StatementStr,
     ValueStr,
     VariableNameGenerator,
     VariableNameStr,
     VarNameExpressionInfo,
 )
 from featurebyte.query_graph.node.utils import subset_frame_column_expr
 from featurebyte.query_graph.transform.base import BaseGraphExtractor
+from featurebyte.typing import Scalar
 
 
 class OnDemandFeatureViewGlobalState(BaseModel):
     """
     On demand feature view global state
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/operation_structure.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/operation_structure.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains operation structure extraction related classes.
 """
+
 from typing import Any, Dict, List, Optional, Tuple
 
 from pydantic import BaseModel
 
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.metadata.operation import (
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/pruning.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/pruning.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains graph pruning related classes.
 """
+
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 from pydantic import BaseModel
 
 from featurebyte.query_graph.enum import GraphNodeType, NodeOutputType
 from featurebyte.query_graph.model.graph import GraphNodeNameMap, NodeNameMap, QueryGraphModel
 from featurebyte.query_graph.node import Node
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/quick_pruning.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/quick_pruning.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains graph quick pruning related classes.
 """
+
 from typing import Any, Dict, List, Set
 
 from pydantic import BaseModel, Field
 
 from featurebyte.query_graph.model.graph import GraphNodeNameMap, QueryGraphModel
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.transform.base import BaseGraphTransformer
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/reconstruction.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/reconstruction.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains graph reconstruction (by replacing certain nodes) related classes.
 """
+
 from typing import Any, Dict, Optional, Type, TypeVar, cast
 
 from abc import abstractmethod
 
 from pydantic import BaseModel, Field
 
 from featurebyte.query_graph.enum import NodeOutputType, NodeType
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/transform/sdk_code.py` & `featurebyte-1.0.3/featurebyte/query_graph/transform/sdk_code.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains an extractor class to generate SDK codes from a query graph.
 """
+
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 from bson import ObjectId
 from pydantic import BaseModel, Field
 
 from featurebyte.query_graph.enum import GraphNodeType, NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
@@ -113,14 +114,15 @@
             Backward nodes of the node
         """
         if node.type == NodeType.PROJECT and backward_nodes[0].type in (
             NodeType.FORWARD_AGGREGATE,
             NodeType.LOOKUP_TARGET,
             NodeType.ITEM_GROUPBY,
             NodeType.AGGREGATE_AS_AT,
+            NodeType.FORWARD_AGGREGATE_AS_AT,
         ):
             # ForwardAggregateNode's, LookupTargetNode's, ItemGroupByNode's & AggregateAsAtNode's SDK code like
             # * `view.groupby(...).forward_aggregate(...)`
             # * `view.as_target(...)`
             # * `view.groupby(...).aggregate(...)`
             # * `view.groupby(...).aggregate_as_at(...)`
             # already return a series, there is no operation for project node to generate any SDK code.
```

### Comparing `featurebyte-1.0.2/featurebyte/query_graph/util.py` & `featurebyte-1.0.3/featurebyte/query_graph/util.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utility functions
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List
 
 import hashlib
 import json
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/app_container_config.py` & `featurebyte-1.0.3/featurebyte/routes/app_container_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 App container config module.
 
 This contains all our registrations for dependency injection.
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union, get_type_hints
 
 import inspect
 from dataclasses import dataclass
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/base_materialized_table_router.py` & `featurebyte-1.0.3/featurebyte/routes/base_materialized_table_router.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base materialized table router
 """
+
 from typing import Generic, Type, TypeVar
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.materialized_table import MaterializedTableModel
 from featurebyte.routes.base_router import BaseRouter
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/base_router.py` & `featurebyte-1.0.3/featurebyte/routes/base_router.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base router
 """
+
 from typing import Dict, Generic, List, Optional, Type, TypeVar, Union, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
 from fastapi.routing import APIRoute
 from starlette.routing import BaseRoute
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/batch_feature_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/batch_feature_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchFeatureTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/batch_feature_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/batch_feature_table/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchFeatureTable API route controller
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.routes.common.base_materialized_table import BaseMaterializedTableController
 from featurebyte.routes.task.controller import TaskController
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/batch_request_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/batch_request_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/batch_request_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/batch_request_table/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTable API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple
 
 from bson import ObjectId
 
 from featurebyte.models.batch_request_table import BatchRequestTableModel
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/block_modification_handler.py` & `featurebyte-1.0.3/featurebyte/routes/block_modification_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Block modification handler
 """
+
 from typing import Iterator
 
 from contextlib import contextmanager
 
 
 class BlockModificationHandler:
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/catalog/api.py` & `featurebyte-1.0.3/featurebyte/routes/catalog/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Catalog API routes
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from http import HTTPStatus
 
 from fastapi import Query, Request, Response
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/catalog/catalog_name_injector.py` & `featurebyte-1.0.3/featurebyte/routes/catalog/catalog_name_injector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Catalog name injector
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.service.catalog import CatalogService
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/catalog/controller.py` & `featurebyte-1.0.3/featurebyte/routes/catalog/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Catalog API route controller
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentDeletionError
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/base.py` & `featurebyte-1.0.3/featurebyte/routes/common/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BaseController for API routes
 """
+
 from __future__ import annotations
 
 from typing import Any, Generic, List, Optional, Tuple, Type, TypeVar, Union, cast
 
 from bson.objectid import ObjectId
 
 from featurebyte.exception import DocumentDeletionError, DocumentUpdateError
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/base_materialized_table.py` & `featurebyte-1.0.3/featurebyte/routes/common/base_materialized_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base class for materialized table routes
 """
+
 from typing import Any, AsyncGenerator, Optional, TypeVar
 
 from io import BytesIO
 
 import pyarrow as pa
 from bson import ObjectId
 from pyarrow import parquet as pq
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/base_table.py` & `featurebyte-1.0.3/featurebyte/routes/common/base_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BaseDataController for API routes
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Tuple, Type, TypeVar, cast
 
 from bson.objectid import ObjectId
 
 from featurebyte.enum import SemanticType
@@ -14,14 +15,15 @@
 from featurebyte.models.item_table import ItemTableModel
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.models.scd_table import SCDTableModel
 from featurebyte.query_graph.model.column_info import ColumnInfo
 from featurebyte.query_graph.model.critical_data_info import CriticalDataInfo
 from featurebyte.routes.common.base import BaseDocumentController, PaginatedDocument
 from featurebyte.schema.table import TableServiceUpdate, TableUpdate
+from featurebyte.service.base_table_document import DocumentCreate
 from featurebyte.service.dimension_table import DimensionTableService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.event_table import EventTableService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
@@ -92,15 +94,17 @@
 
         Returns
         -------
         dict[str, Any]
         """
         column_semantic_map = {}
         for field, semantic_type in self.semantic_tag_rules.items():
-            semantic_id = await self.semantic_service.get_or_create_document(name=semantic_type)
+            semantic_id = await self.semantic_service.get_or_create_document(
+                name=semantic_type.value
+            )
             special_column_name = getattr(document, field)
             if special_column_name:
                 column_semantic_map[special_column_name] = semantic_id
         return column_semantic_map
 
     async def _add_semantic_tags(self, document: TableDocumentT) -> TableDocumentT:
         """
@@ -160,15 +164,15 @@
         )
         if table_details.description:
             document = await self.update_description(
                 document_id=document.id, description=table_details.description
             )
         return document
 
-    async def create_table(self, data: TableDocumentT) -> TableDocumentT:
+    async def create_table(self, data: DocumentCreate) -> TableDocumentT:
         """
         Create Table record at persistent
 
         Parameters
         ----------
         data: TableDocumentT
             EventTable/ItemTable/SCDTable/DimensionTable creation payload
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/derive_primary_entity_helper.py` & `featurebyte-1.0.3/featurebyte/routes/common/derive_primary_entity_helper.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Derive primary entity helper
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, Sequence
 
 from bson import ObjectId
 
 from featurebyte.models.entity import EntityModel
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/feature_metadata_extractor.py` & `featurebyte-1.0.3/featurebyte/routes/common/feature_metadata_extractor.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature metadata extractor
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, Type, TypeVar
 
 from bson import ObjectId
 
 from featurebyte.models.base import PydanticObjectId
@@ -142,14 +143,15 @@
                 "name": agg_col.name,
                 "column": reference_map.get(
                     agg_col.column, None
                 ),  # for count aggregation, column is None
                 "function": agg_col.method,
                 "keys": agg_col.keys,
                 "window": agg_col.window,
+                "offset": agg_col.offset,
                 "category": agg_col.category,
                 "filter": agg_col.filter,
                 "aggregation_type": agg_col.aggregation_type,
             }
 
         post_aggregation = None
         if op_struct.post_aggregation:
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/feature_or_target_helper.py` & `featurebyte-1.0.3/featurebyte/routes/common/feature_or_target_helper.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature or target helper
 """
+
 from typing import Any, Dict, List
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.routes.catalog.catalog_name_injector import CatalogNameInjector
 from featurebyte.service.table import TableService
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/feature_or_target_table.py` & `featurebyte-1.0.3/featurebyte/routes/common/feature_or_target_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature or target table controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Generic, List, Optional, TypeVar
 
 from abc import abstractmethod
 from dataclasses import dataclass
 from http import HTTPStatus
@@ -52,16 +53,16 @@
 MaterializedTableDocumentServiceT = TypeVar(
     "MaterializedTableDocumentServiceT",
     HistoricalFeatureTableService,
     TargetTableService,
 )
 PayloadT = TypeVar(
     "PayloadT",
-    TargetTableTaskPayload,
     HistoricalFeatureTableTaskPayload,
+    TargetTableTaskPayload,
 )
 TableCreateT = TypeVar(
     "TableCreateT",
     bound=FeatureOrTargetTableCreate,
 )
 
 
@@ -95,37 +96,37 @@
     Feature or target table controller
     """
 
     info_class: type[InfoTypeT]
 
     def __init__(
         self,
-        service: Any,
+        service: MaterializedTableDocumentServiceT,
         feature_store_warehouse_service: FeatureStoreWarehouseService,
         observation_table_service: ObservationTableService,
         entity_validation_service: EntityValidationService,
         task_controller: TaskController,
     ):
         super().__init__(
             service=service, feature_store_warehouse_service=feature_store_warehouse_service
         )
         self.observation_table_service = observation_table_service
         self.entity_validation_service = entity_validation_service
         self.task_controller = task_controller
 
     @abstractmethod
     async def get_additional_info_params(
-        self, document: MaterializedTableDocumentT
+        self, document: BaseFeatureOrTargetTableModel
     ) -> dict[str, Any]:
         """
         Get additional info params
 
         Parameters
         ----------
-        document: MaterializedTableDocumentT
+        document: BaseFeatureOrTargetTableModel
             document
 
         Returns
         -------
         dict[str, Any]
         """
 
@@ -267,12 +268,12 @@
         Returns
         -------
         InfoTypeT
         """
         _ = verbose
         document = await self.service.get_document(document_id)
         basic_info = await self.get_basic_info(document)
-        additional_params = await self.get_additional_info_params(document)  # type: ignore[arg-type]
+        additional_params = await self.get_additional_info_params(document)
         return self.info_class(
             **additional_params,
             **basic_info.dict(),
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/primary_entity_validator.py` & `featurebyte-1.0.3/featurebyte/routes/common/primary_entity_validator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Primary entity validator
 """
+
 from typing import List
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.routes.common.derive_primary_entity_helper import DerivePrimaryEntityHelper
 from featurebyte.service.entity import EntityService
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/common/schema.py` & `featurebyte-1.0.3/featurebyte/routes/common/schema.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Common classes mixin for API payload schema
 """
+
 from fastapi import Query
 
 # route query parameters
 COLUMN_STR_MAX_LENGTH = 255
 COLUMN_STR_MIN_LENGTH = 1
 PageQuery = Query(default=1, gt=0)
 PageSizeQuery = Query(default=10, gt=0, le=500)
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/context/api.py` & `featurebyte-1.0.3/featurebyte/routes/context/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Context API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from http import HTTPStatus
 
 from fastapi import Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/context/controller.py` & `featurebyte-1.0.3/featurebyte/routes/context/controller.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 """
 Context API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentCreationError, ObservationTableInvalidContextError
 from featurebyte.models.context import ContextModel
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.routes.common.primary_entity_validator import PrimaryEntityValidator
 from featurebyte.schema.context import ContextCreate, ContextList, ContextUpdate
 from featurebyte.schema.info import ContextInfo, EntityBriefInfo, EntityBriefInfoList
 from featurebyte.schema.observation_table import ObservationTableServiceUpdate
-from featurebyte.service.batch_feature_table import BatchFeatureTableService
+from featurebyte.service.batch_request_table import BatchRequestTableService
 from featurebyte.service.catalog import CatalogService
 from featurebyte.service.context import ContextService
 from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.use_case import UseCaseService
 from featurebyte.service.user_service import UserService
@@ -34,26 +35,26 @@
 
     paginated_document_class = ContextList
     document_update_schema_class = ContextUpdate
 
     def __init__(
         self,
         observation_table_service: ObservationTableService,
-        batch_feature_table_service: BatchFeatureTableService,
+        batch_request_table_service: BatchRequestTableService,
         context_service: ContextService,
         deployment_service: DeploymentService,
         user_service: UserService,
         entity_service: EntityService,
         use_case_service: UseCaseService,
         catalog_service: CatalogService,
         primary_entity_validator: PrimaryEntityValidator,
     ):
         super().__init__(service=context_service)
         self.observation_table_service = observation_table_service
-        self.batch_feature_table_service = batch_feature_table_service
+        self.batch_request_table_service = batch_request_table_service
         self.context_service = context_service
         self.deployment_service = deployment_service
         self.user_service = user_service
         self.entity_service = entity_service
         self.use_case_service = use_case_service
         self.catalog_service = catalog_service
         self.primary_entity_validator = primary_entity_validator
@@ -168,15 +169,15 @@
 
     async def service_and_query_pairs_for_checking_reference(
         self, document_id: ObjectId
     ) -> List[Tuple[Any, QueryFilter]]:
         return [
             (self.use_case_service, {"context_id": document_id}),
             (self.observation_table_service, {"context_id": document_id}),
-            (self.batch_feature_table_service, {"context_id": document_id}),
+            (self.batch_request_table_service, {"context_id": document_id}),
             (self.deployment_service, {"context_id": document_id}),
         ]
 
     async def get_info(self, context_id: ObjectId) -> ContextInfo:
         """
         Get detailed information about a Context
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/credential/api.py` & `featurebyte-1.0.3/featurebyte/routes/credential/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Credential API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from http import HTTPStatus
 
 from fastapi import Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/credential/controller.py` & `featurebyte-1.0.3/featurebyte/routes/credential/controller.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Credential API routes
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.credential import CredentialModel
 from featurebyte.routes.common.base import BaseDocumentController
@@ -83,17 +84,17 @@
         _ = verbose
         credential = await self.service.get_document(document_id=credential_id)
         return CredentialInfo(
             name=credential.name,
             feature_store_info=await self.feature_store_service.get_feature_store_info(
                 document_id=credential.feature_store_id, verbose=verbose
             ),
-            database_credential_type=credential.database_credential.type
-            if credential.database_credential
-            else None,
-            storage_credential_type=credential.storage_credential.type
-            if credential.storage_credential
-            else None,
+            database_credential_type=(
+                credential.database_credential.type if credential.database_credential else None
+            ),
+            storage_credential_type=(
+                credential.storage_credential.type if credential.storage_credential else None
+            ),
             created_at=credential.created_at,
             updated_at=credential.updated_at,
             description=credential.description,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/deployment/api.py` & `featurebyte-1.0.3/featurebyte/routes/deployment/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Deployment API routes
 """
+
 from typing import Literal, Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Query, Request, Response
 from fastapi.responses import ORJSONResponse
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/deployment/controller.py` & `featurebyte-1.0.3/featurebyte/routes/deployment/controller.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,27 @@
 """
 Deployment API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Literal, Optional, Tuple
 
 from http import HTTPStatus
 
 from bson import ObjectId
 from fastapi import HTTPException
 
-from featurebyte.exception import DocumentDeletionError, FeatureListNotOnlineEnabledError
+from featurebyte.exception import (
+    DocumentCreationError,
+    DocumentDeletionError,
+    FeatureListNotOnlineEnabledError,
+)
 from featurebyte.feast.service.feature_store import FeastFeatureStoreService
-from featurebyte.models.deployment import DeploymentModel, FeastIntegrationSettings
+from featurebyte.models.deployment import DeploymentModel
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.persistent.base import SortDir
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.deployment import (
     AllDeploymentList,
@@ -35,21 +40,22 @@
     DeploymentCreateUpdateTaskPayload,
     UpdateDeploymentPayload,
 )
 from featurebyte.service.batch_feature_table import BatchFeatureTableService
 from featurebyte.service.catalog import AllCatalogService, CatalogService
 from featurebyte.service.context import ContextService
 from featurebyte.service.deployment import AllDeploymentService, DeploymentService
+from featurebyte.service.entity_serving_names import EntityServingNamesService
 from featurebyte.service.feature_list import AllFeatureListService, FeatureListService
 from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
 from featurebyte.service.online_serving import OnlineServingService
 from featurebyte.service.use_case import UseCaseService
 
 
-class DeploymentController(
+class DeploymentController(  # pylint: disable=too-many-instance-attributes, too-many-arguments
     BaseDocumentController[DeploymentModel, DeploymentService, DeploymentList]
 ):
     """
     Deployment Controller
     """
 
     paginated_document_class = DeploymentList
@@ -61,46 +67,63 @@
         context_service: ContextService,
         feature_list_service: FeatureListService,
         online_serving_service: OnlineServingService,
         task_controller: TaskController,
         use_case_service: UseCaseService,
         batch_feature_table_service: BatchFeatureTableService,
         feast_feature_store_service: FeastFeatureStoreService,
+        entity_serving_names_service: EntityServingNamesService,
     ):
         super().__init__(deployment_service)
         self.catalog_service = catalog_service
         self.context_service = context_service
         self.feature_list_service = feature_list_service
         self.online_serving_service = online_serving_service
         self.task_controller = task_controller
         self.use_case_service = use_case_service
         self.batch_feature_table_service = batch_feature_table_service
         self.feast_feature_store_service = feast_feature_store_service
+        self.entity_serving_names_service = entity_serving_names_service
 
     async def create_deployment(self, data: DeploymentCreate) -> Task:
         """
         Create deployment.
 
         Parameters
         ----------
         data : DeploymentCreate
             Deployment data to create.
 
         Returns
         -------
         Task
             Task to create deployment.
+
+        Raises
+        ------
+        DocumentCreationError
+            Primary entity of the use case is not in the feature list's supported serving entities.
         """
         # check if feature list exists
-        _ = await self.feature_list_service.get_document(document_id=data.feature_list_id)
+        feature_list_doc = await self.feature_list_service.get_document_as_dict(
+            document_id=data.feature_list_id
+        )
 
         context_id = None
         if data.use_case_id:
             use_case = await self.use_case_service.get_document(document_id=data.use_case_id)
             context_id = use_case.context_id
+            context = await self.context_service.get_document(document_id=context_id)
+
+            # check whether the context primary entity is in the feature list supported serving entities
+            supported_serving_ids = feature_list_doc["supported_serving_entity_ids"]
+            if supported_serving_ids and context.primary_entity_ids not in supported_serving_ids:
+                raise DocumentCreationError(
+                    "Primary entity of the use case is not in the feature list's supported serving entities."
+                )
 
         payload = DeploymentCreateUpdateTaskPayload(
             deployment_payload=CreateDeploymentPayload(
                 name=data.name,
                 feature_list_id=data.feature_list_id,
                 enabled=False,
                 use_case_id=data.use_case_id,
@@ -216,38 +239,37 @@
 
         Raises
         ------
         HTTPException
             Invalid request payload
         """
         document = await self.service.get_document(deployment_id)
-
         feature_list = await self.feature_list_service.get_document(document.feature_list_id)
         catalog = await self.catalog_service.get_document(feature_list.catalog_id)
         try:
             result: Optional[OnlineFeaturesResponseModel]
-            if (
-                FeastIntegrationSettings().FEATUREBYTE_FEAST_INTEGRATION_ENABLED
-                and catalog.online_store_id is not None
-            ):
+            if feature_list.store_info.feast_enabled and catalog.online_store_id is not None:
                 feast_store = (
-                    await self.feast_feature_store_service.get_feast_feature_store_for_catalog()
+                    await self.feast_feature_store_service.get_feast_feature_store_for_deployment(
+                        deployment=document
+                    )
                 )
             else:
                 feast_store = None
 
             feast_feature_services = set()
             if feast_store is not None:
                 feast_feature_services.update(
                     [fs.name for fs in feast_store.list_feature_services()]
                 )
 
             if feast_store and feature_list.versioned_name in feast_feature_services:
                 result = await self.online_serving_service.get_online_features_by_feast(
                     feature_list=feature_list,
+                    deployment=document,
                     feast_store=feast_store,
                     request_data=data.entity_serving_names,
                 )
             else:
                 result = await self.online_serving_service.get_online_features_from_feature_list(
                     feature_list=feature_list,
                     request_data=data.entity_serving_names,
@@ -311,17 +333,21 @@
 
         Returns
         -------
         SampleEntityServingNames
             Sample entity serving names
         """
         deployment: DeploymentModel = await self.service.get_document(deployment_id)
-        entity_serving_names = await self.feature_list_service.get_sample_entity_serving_names(
-            feature_list_id=deployment.feature_list_id, count=count
-        )
+        entity_serving_names = []
+        if deployment.serving_entity_ids:
+            entity_serving_names = (
+                await self.entity_serving_names_service.get_sample_entity_serving_names(
+                    entity_ids=deployment.serving_entity_ids, table_ids=None, count=count
+                )
+            )
         return SampleEntityServingNames(entity_serving_names=entity_serving_names)
 
 
 class AllDeploymentController(
     BaseDocumentController[DeploymentModel, AllDeploymentService, DeploymentList]
 ):
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/dimension_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/dimension_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 DimensionTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
@@ -132,15 +133,15 @@
     ) -> DimensionTableModel:
         return await super().update_description(request, dimension_table_id, data)
 
     async def create_object(
         self, request: Request, data: DimensionTableCreate
     ) -> DimensionTableModel:
         controller = self.get_controller_for_request(request)
-        return await controller.create_table(data=data)  # type: ignore
+        return await controller.create_table(data=data)
 
     async def get_dimension_table_info(
         self, request: Request, dimension_table_id: PydanticObjectId, verbose: bool = VerboseQuery
     ) -> DimensionTableInfo:
         """
         Retrieve dimension table info
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/dimension_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/dimension_table/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 DimensionTable API route controller
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.enum import SemanticType
 from featurebyte.models.dimension_table import DimensionTableModel
 from featurebyte.routes.common.base_table import BaseTableDocumentController
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/entity/api.py` & `featurebyte-1.0.3/featurebyte/routes/entity/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Entity API routes
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/entity/controller.py` & `featurebyte-1.0.3/featurebyte/routes/entity/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Entity API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple
 
 from bson.objectid import ObjectId
 
 from featurebyte.models.entity import EntityModel
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/event_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/event_table/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 EventTable API routes
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
@@ -132,15 +133,15 @@
     async def update_description(
         self, request: Request, event_table_id: PydanticObjectId, data: DescriptionUpdate
     ) -> EventTableModel:
         return await super().update_description(request, event_table_id, data)
 
     async def create_object(self, request: Request, data: EventTableCreate) -> EventTableModel:
         controller = self.get_controller_for_request(request)
-        return await controller.create_table(data=data)  # type: ignore
+        return await controller.create_table(data=data)
 
     async def get_event_table_info(
         self, request: Request, event_table_id: PydanticObjectId, verbose: bool = VerboseQuery
     ) -> EventTableInfo:
         """
         Retrieve event table info
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/event_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/event_table/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 EventTable API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple
 
 from bson.objectid import ObjectId
 
 from featurebyte.enum import SemanticType
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature/api.py` & `featurebyte-1.0.3/featurebyte/routes/feature/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature API routes
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Union, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Query, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature/controller.py` & `featurebyte-1.0.3/featurebyte/routes/feature/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Union
 
 from http import HTTPStatus
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_job_setting_analysis/api.py` & `featurebyte-1.0.3/featurebyte/routes/feature_job_setting_analysis/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureJobSettingAnalysis API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_job_setting_analysis/controller.py` & `featurebyte-1.0.3/featurebyte/routes/feature_job_setting_analysis/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureJobSettingAnalysis API route controller
 """
+
 from __future__ import annotations
 
 import tempfile
 from io import BytesIO
 
 import pdfkit
 from bson import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_list/api.py` & `featurebyte-1.0.3/featurebyte/routes/feature_list/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureList API routes
 """
+
 # pylint: disable=duplicate-code
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Union, cast
 
 import json
 from http import HTTPStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_list/controller.py` & `featurebyte-1.0.3/featurebyte/routes/feature_list/controller.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,34 @@
 """
 FeatureList API route controller
 """
+
 from __future__ import annotations
 
-from typing import Any, Callable, Coroutine, Dict, List, Optional, Set, Union
+from typing import Any, Callable, Coroutine, Dict, List, Optional, Set, Tuple, Union
 
 import copy
 from http import HTTPStatus
 
 from bson import json_util
 from bson.objectid import ObjectId
 from fastapi import UploadFile
 from fastapi.exceptions import HTTPException
 
 from featurebyte.common.utils import dataframe_from_arrow_stream
 from featurebyte.exception import (
+    DocumentDeletionError,
     MissingPointInTimeColumnError,
     RequiredEntityNotProvidedError,
     TooRecentPointInTimeError,
 )
 from featurebyte.feature_manager.model import ExtendedFeatureModel
 from featurebyte.models.base import VersionIdentifier
 from featurebyte.models.feature_list import FeatureListModel, FeatureReadinessDistribution
+from featurebyte.models.persistent import QueryFilter
 from featurebyte.persistent.base import SortDir
 from featurebyte.routes.catalog.catalog_name_injector import CatalogNameInjector
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.feature_list import (
     FeatureListCreate,
     FeatureListCreateJob,
@@ -54,20 +57,22 @@
 from featurebyte.schema.worker.task.feature_list_create import (
     FeatureListCreateTaskPayload,
     FeaturesParameters,
 )
 from featurebyte.schema.worker.task.feature_list_make_production_ready import (
     FeatureListMakeProductionReadyTaskPayload,
 )
+from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_facade import FeatureListFacadeService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_preview import FeaturePreviewService
+from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
 from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
 from featurebyte.service.table import TableService
 from featurebyte.service.task_manager import TaskManager
 from featurebyte.service.tile_job_log import TileJobLogService
 from featurebyte.storage import Storage
 
 
@@ -86,27 +91,31 @@
         self,
         feature_list_service: FeatureListService,
         feature_list_facade_service: FeatureListFacadeService,
         feature_list_namespace_service: FeatureListNamespaceService,
         feature_service: FeatureService,
         entity_service: EntityService,
         table_service: TableService,
+        deployment_service: DeploymentService,
+        historical_feature_table_service: HistoricalFeatureTableService,
         catalog_name_injector: CatalogNameInjector,
         feature_preview_service: FeaturePreviewService,
         tile_job_log_service: TileJobLogService,
         task_controller: TaskController,
         task_manager: TaskManager,
         storage: Storage,
     ):
         super().__init__(feature_list_service)
         self.feature_list_facade_service = feature_list_facade_service
         self.feature_list_namespace_service = feature_list_namespace_service
         self.feature_service = feature_service
         self.entity_service = entity_service
         self.table_service = table_service
+        self.deployment_service = deployment_service
+        self.historical_feature_table_service = historical_feature_table_service
         self.catalog_name_injector = catalog_name_injector
         self.feature_preview_service = feature_preview_service
         self.tile_job_log_service = tile_job_log_service
         self.task_controller = task_controller
         self.task_manager = task_manager
         self.storage = storage
 
@@ -256,14 +265,17 @@
         Delete FeatureList at persistent
 
         Parameters
         ----------
         feature_list_id: ObjectId
             FeatureList ID
         """
+        await self.verify_operation_by_checking_reference(
+            document_id=feature_list_id, exception_class=DocumentDeletionError
+        )
         await self.feature_list_facade_service.delete_feature_list(feature_list_id=feature_list_id)
 
     async def list_feature_lists(
         self,
         page: int = 1,
         page_size: int = DEFAULT_PAGE_SIZE,
         sort_by: list[tuple[str, SortDir]] | None = None,
@@ -591,7 +603,15 @@
             Sample entity serving names
         """
 
         entity_serving_names = await self.service.get_sample_entity_serving_names(
             feature_list_id=feature_list_id, count=count
         )
         return SampleEntityServingNames(entity_serving_names=entity_serving_names)
+
+    async def service_and_query_pairs_for_checking_reference(
+        self, document_id: ObjectId
+    ) -> List[Tuple[Any, QueryFilter]]:
+        return [
+            (self.deployment_service, {"feature_list_id": document_id}),
+            (self.historical_feature_table_service, {"feature_list_id": document_id}),
+        ]
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_list_namespace/api.py` & `featurebyte-1.0.3/featurebyte/routes/feature_list_namespace/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureListNamespace API routes
 """
+
 # pylint: disable=duplicate-code
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_list_namespace/controller.py` & `featurebyte-1.0.3/featurebyte/routes/feature_list_namespace/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureListNamespace API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, cast
 
 import copy
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_namespace/api.py` & `featurebyte-1.0.3/featurebyte/routes/feature_namespace/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureNamespace API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_namespace/controller.py` & `featurebyte-1.0.3/featurebyte/routes/feature_namespace/controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureNamespace API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, cast
 
 import copy
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_store/api.py` & `featurebyte-1.0.3/featurebyte/routes/feature_store/api.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 """
 FeatureStore API routes
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
+from http import HTTPStatus
+
 from fastapi import Query, Request
 
 from featurebyte.exception import DocumentNotFoundError
-from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.base import DEFAULT_CATALOG_ID, PydanticObjectId
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.persistent import AuditDocumentList
 from featurebyte.persistent.base import SortDir
 from featurebyte.query_graph.model.column_info import ColumnInfo, ColumnSpecWithDescription
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.routes.base_router import BaseApiRouter
 from featurebyte.routes.common.schema import (
@@ -29,14 +32,15 @@
     FeatureStoreCreate,
     FeatureStoreList,
     FeatureStorePreview,
     FeatureStoreSample,
     FeatureStoreShape,
 )
 from featurebyte.schema.info import FeatureStoreInfo
+from featurebyte.schema.task import Task
 
 
 class FeatureStoreRouter(
     BaseApiRouter[FeatureStoreModel, FeatureStoreList, FeatureStoreCreate, FeatureStoreController]
 ):
     """
     Feature Store API router
@@ -113,14 +117,21 @@
         )
         self.router.add_api_route(
             "/description",
             self.get_data_description,
             methods=["POST"],
             response_model=Dict[str, Any],
         )
+        self.router.add_api_route(
+            "/data_description",
+            self.submit_data_description_task,
+            methods=["POST"],
+            response_model=Task,
+            status_code=HTTPStatus.CREATED,
+        )
 
     async def create_object(
         self,
         request: Request,
         data: FeatureStoreCreate,
     ) -> FeatureStoreModel:
         """
@@ -334,13 +345,30 @@
     ) -> Dict[str, Any]:
         """
         Retrieve data description for query graph node
         """
         controller: FeatureStoreController = request.state.app_container.feature_store_controller
         return await controller.describe(sample=sample, size=size, seed=seed)
 
+    @staticmethod
+    async def submit_data_description_task(
+        request: Request,
+        sample: FeatureStoreSample,
+        size: int = Query(default=0, gte=0, le=1000000),
+        seed: int = Query(default=1234),
+        catalog_id: PydanticObjectId = Query(default=None),
+    ) -> Task:
+        """
+        Submit data description task for query graph node
+        """
+        controller: FeatureStoreController = request.state.app_container.feature_store_controller
+        task_submit: Task = await controller.create_data_description(
+            sample=sample, size=size, seed=seed, catalog_id=catalog_id or DEFAULT_CATALOG_ID
+        )
+        return task_submit
+
     async def delete_object(
         self, request: Request, feature_store_id: PydanticObjectId
     ) -> DeleteResponse:
         controller: FeatureStoreController = self.get_controller_for_request(request)
         await controller.delete(document_id=feature_store_id)
         return DeleteResponse()
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/feature_store/controller.py` & `featurebyte-1.0.3/featurebyte/routes/feature_store/controller.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 """
 FeatureStore API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional, Tuple
 
 from bson.objectid import ObjectId
 
 from featurebyte.logging import get_logger
 from featurebyte.models.credential import CredentialModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.routes.common.base import BaseDocumentController
+from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.credential import CredentialCreate
 from featurebyte.schema.feature_store import (
     FeatureStoreCreate,
     FeatureStoreList,
     FeatureStorePreview,
     FeatureStoreSample,
     FeatureStoreShape,
 )
 from featurebyte.schema.info import FeatureStoreInfo
+from featurebyte.schema.task import Task
+from featurebyte.schema.worker.task.data_description import DataDescriptionTaskPayload
 from featurebyte.service.catalog import AllCatalogService
 from featurebyte.service.credential import CredentialService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
 from featurebyte.service.preview import PreviewService
 from featurebyte.service.session_manager import SessionManagerService
 from featurebyte.service.session_validator import SessionValidatorService
@@ -49,22 +53,24 @@
         feature_store_service: FeatureStoreService,
         preview_service: PreviewService,
         session_manager_service: SessionManagerService,
         session_validator_service: SessionValidatorService,
         feature_store_warehouse_service: FeatureStoreWarehouseService,
         credential_service: CredentialService,
         all_catalog_service: AllCatalogService,
+        task_controller: TaskController,
     ):
         super().__init__(feature_store_service)
         self.preview_service = preview_service
         self.session_manager_service = session_manager_service
         self.session_validator_service = session_validator_service
         self.feature_store_warehouse_service = feature_store_warehouse_service
         self.credential_service = credential_service
         self.all_catalog_service = all_catalog_service
+        self.task_controller = task_controller
 
     async def create_feature_store(
         self,
         data: FeatureStoreCreate,
     ) -> FeatureStoreModel:
         """
         Create Feature Store at persistent
@@ -379,14 +385,47 @@
         Returns
         -------
         dict[str, Any]
             Dataframe converted to json string
         """
         return await self.preview_service.describe(sample=sample, size=size, seed=seed)
 
+    async def create_data_description(
+        self, sample: FeatureStoreSample, size: int, seed: int, catalog_id: ObjectId
+    ) -> Task:
+        """
+        Retrieve data description for query graph node
+
+        Parameters
+        ----------
+        sample: FeatureStoreSample
+            FeatureStoreSample object
+        size: int
+            Maximum rows to sample
+        seed: int
+            Random seed to use for sampling
+        catalog_id: ObjectId
+            Catalog ID used for task submission
+
+        Returns
+        -------
+        Task
+        """
+
+        # prepare task payload and submit task
+        payload = DataDescriptionTaskPayload(
+            sample=sample,
+            size=size,
+            seed=seed,
+            user_id=self.task_controller.task_manager.user.id,
+            catalog_id=catalog_id,
+        )
+        task_id = await self.task_controller.task_manager.submit(payload=payload)
+        return await self.task_controller.get_task(task_id=str(task_id))
+
     async def service_and_query_pairs_for_checking_reference(
         self, document_id: ObjectId
     ) -> List[Tuple[Any, QueryFilter]]:
         return [(self.all_catalog_service, {"default_feature_store_ids": document_id})]
 
     async def get_info(
         self,
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/historical_feature_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/historical_feature_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 HistoricalFeatureTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 import json
 from http import HTTPStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/historical_feature_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/historical_feature_table/controller.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 """
 HistoricalTable API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, cast
 
 import pandas as pd
 from bson import ObjectId
 
+from featurebyte.models.base_feature_or_target_table import BaseFeatureOrTargetTableModel
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.routes.common.feature_or_target_table import (
     FeatureOrTargetTableController,
     ValidationParameters,
 )
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.historical_feature_table import (
@@ -88,16 +90,17 @@
         return (
             await self.historical_features_validation_parameters_service.get_validation_parameters(
                 table_create.featurelist_get_historical_features
             )
         )
 
     async def get_additional_info_params(
-        self, document: HistoricalFeatureTableModel
+        self, document: BaseFeatureOrTargetTableModel
     ) -> dict[str, Any]:
+        assert isinstance(document, HistoricalFeatureTableModel)
         if document.feature_list_id is None:
             return {}
         feature_list = await self.feature_list_service.get_document(
             document_id=document.feature_list_id
         )
         return {
             "feature_list_name": feature_list.name,
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/item_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/item_table/api.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ItemTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
@@ -122,15 +123,15 @@
     async def update_description(
         self, request: Request, item_table_id: PydanticObjectId, data: DescriptionUpdate
     ) -> ItemTableModel:
         return await super().update_description(request, item_table_id, data)
 
     async def create_object(self, request: Request, data: ItemTableCreate) -> ItemTableModel:
         controller = self.get_controller_for_request(request)
-        return await controller.create_table(data=data)  # type: ignore
+        return await controller.create_table(data=data)
 
     async def get_item_table_info(
         self, request: Request, item_table_id: PydanticObjectId, verbose: bool = VerboseQuery
     ) -> ItemTableInfo:
         """
         Retrieve item table info
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/item_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/item_table/controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 """
 ItemTable API route controller
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.enum import SemanticType
 from featurebyte.models.item_table import ItemTableModel
 from featurebyte.routes.common.base_table import BaseTableDocumentController
 from featurebyte.schema.info import ItemTableInfo
-from featurebyte.schema.item_table import ItemTableList, ItemTableServiceUpdate
+from featurebyte.schema.item_table import ItemTableCreate, ItemTableList, ItemTableServiceUpdate
 from featurebyte.service.entity import EntityService
 from featurebyte.service.event_table import EventTableService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
 from featurebyte.service.item_table import ItemTableService
@@ -66,15 +67,15 @@
             specialized_dtype_detection_service=specialized_dtype_detection_service,
             feature_store_service=feature_store_service,
             feature_store_warehouse_service=feature_store_warehouse_service,
         )
         self.table_info_service = table_info_service
         self.event_table_service = event_table_service
 
-    async def create_table(self, data: ItemTableModel) -> ItemTableModel:
+    async def create_table(self, data: ItemTableCreate) -> ItemTableModel:  # type: ignore[override]
         # check existence of event table first before creating item table
         _ = await self.event_table_service.get_document(document_id=data.event_table_id)
         return await super().create_table(data=data)
 
     async def get_info(self, document_id: ObjectId, verbose: bool) -> ItemTableInfo:
         """
         Get document info given document ID
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/lazy_app_container.py` & `featurebyte-1.0.3/featurebyte/routes/lazy_app_container.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Lazy app container functions the same as the app_container, but only initializes dependencies when needed.
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Type, Union
 
 from bson import ObjectId
 from celery import Celery
 from redis.client import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/observation_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/observation_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ObservationTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 import json
 from http import HTTPStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/observation_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/observation_table/controller.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ObservationTable API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, List, Optional, Tuple
 
 import pandas as pd
 from bson import ObjectId
 from fastapi import UploadFile
@@ -26,56 +27,59 @@
 )
 from featurebyte.schema.task import Task
 from featurebyte.service.context import ContextService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
 from featurebyte.service.observation_table import ObservationTableService
+from featurebyte.service.target_namespace import TargetNamespaceService
 from featurebyte.service.target_table import TargetTableService
 from featurebyte.service.task_manager import TaskManager
 from featurebyte.service.use_case import UseCaseService
 
 logger = get_logger(__name__)
 
 
-class ObservationTableController(
+class ObservationTableController(  # pylint: disable=too-many-instance-attributes
     BaseMaterializedTableController[
         ObservationTableModel, ObservationTableService, ObservationTableList
     ],
 ):
     """
     ObservationTable Controller
     """
 
     paginated_document_class = ObservationTableList
 
-    def __init__(
+    def __init__(  # pylint: disable=too-many-arguments
         self,
         observation_table_service: ObservationTableService,
         feature_store_warehouse_service: FeatureStoreWarehouseService,
         task_controller: TaskController,
         feature_store_service: FeatureStoreService,
         historical_feature_table_service: HistoricalFeatureTableService,
         target_table_service: TargetTableService,
         context_service: ContextService,
         task_manager: TaskManager,
         use_case_service: UseCaseService,
+        target_namespace_service: TargetNamespaceService,
     ):
         super().__init__(
             service=observation_table_service,
             feature_store_warehouse_service=feature_store_warehouse_service,
         )
         self.observation_table_service = observation_table_service
         self.task_controller = task_controller
         self.feature_store_service = feature_store_service
         self.historical_feature_table_service = historical_feature_table_service
         self.target_table_service = target_table_service
         self.context_service = context_service
         self.task_manager = task_manager
         self.use_case_service = use_case_service
+        self.target_namespace_service = target_namespace_service
 
     async def create_observation_table(
         self,
         data: ObservationTableCreate,
     ) -> Task:
         """
         Create ObservationTable by submitting a materialization task
@@ -184,19 +188,27 @@
         ObservationTableInfo
         """
         _ = verbose
         observation_table = await self.service.get_document(document_id=document_id)
         feature_store = await self.feature_store_service.get_document(
             document_id=observation_table.location.feature_store_id
         )
+        if observation_table.target_namespace_id is not None:
+            target_namespace = await self.target_namespace_service.get_document(
+                document_id=observation_table.target_namespace_id
+            )
+            target_name = target_namespace.name
+        else:
+            target_name = None
         return ObservationTableInfo(
             name=observation_table.name,
             type=observation_table.request_input.type,
             feature_store_name=feature_store.name,
             table_details=observation_table.location.table_details,
+            target_name=target_name,
             created_at=observation_table.created_at,
             updated_at=observation_table.updated_at,
             description=observation_table.description,
         )
 
     async def update_observation_table(
         self, observation_table_id: ObjectId, data: ObservationTableUpdate
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/online_store/api.py` & `featurebyte-1.0.3/featurebyte/routes/online_store/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStore API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from fastapi import Request
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/online_store/controller.py` & `featurebyte-1.0.3/featurebyte/routes/online_store/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStore API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple, cast
 
 from bson.objectid import ObjectId
 
 from featurebyte.logging import get_logger
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/periodic_tasks/api.py` & `featurebyte-1.0.3/featurebyte/routes/periodic_tasks/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Periodic Task API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/periodic_tasks/controller.py` & `featurebyte-1.0.3/featurebyte/routes/periodic_tasks/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Periodic Task API route controller
 """
+
 from __future__ import annotations
 
 from featurebyte.models.periodic_task import PeriodicTask
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.schema.periodic_task import PeriodicTaskList
 from featurebyte.service.periodic_task import PeriodicTaskService
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/registry.py` & `featurebyte-1.0.3/featurebyte/routes/registry.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 Registrations module.
 
 This contains all the dependencies that we want to register in order to get our fast API app up and running.
 """
+
 from featurebyte.feast.service.feature_store import FeastFeatureStoreService
 from featurebyte.feast.service.registry import FeastRegistryService
 from featurebyte.migration.migration_data_service import SchemaMetadataService
 from featurebyte.migration.service.data_warehouse import (
     DataWarehouseMigrationServiceV1,
     DataWarehouseMigrationServiceV3,
     TileColumnTypeExtractor,
@@ -17,14 +18,17 @@
 )
 from featurebyte.migration.service.feature_list import (
     FeatureListMigrationServiceV5,
     FeatureListMigrationServiceV6,
     FeatureListMigrationServiceV7,
 )
 from featurebyte.migration.service.mixin import DataWarehouseMigrationMixin
+from featurebyte.migration.service.offline_store_feature_table import (
+    OfflineStoreFeatureTableMigrationServiceV9,
+)
 from featurebyte.models.base import User
 from featurebyte.routes.app_container_config import AppContainerConfig
 from featurebyte.routes.batch_feature_table.controller import BatchFeatureTableController
 from featurebyte.routes.batch_request_table.controller import BatchRequestTableController
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.routes.catalog.catalog_name_injector import CatalogNameInjector
 from featurebyte.routes.catalog.controller import CatalogController
@@ -67,14 +71,15 @@
 from featurebyte.service.batch_request_table import BatchRequestTableService
 from featurebyte.service.catalog import AllCatalogService, CatalogService
 from featurebyte.service.context import ContextService
 from featurebyte.service.credential import CredentialService
 from featurebyte.service.deploy import (
     DeployFeatureListManagementService,
     DeployFeatureManagementService,
+    DeploymentServingEntityService,
     DeployService,
     FeastIntegrationService,
 )
 from featurebyte.service.deployment import AllDeploymentService, DeploymentService
 from featurebyte.service.dimension_table import DimensionTableService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.entity_lookup_feature_table import EntityLookupFeatureTableService
@@ -102,15 +107,15 @@
 from featurebyte.service.feature_table_cache_metadata import FeatureTableCacheMetadataService
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
 from featurebyte.service.historical_features import (
     HistoricalFeatureExecutor,
     HistoricalFeaturesService,
     HistoricalFeaturesValidationParametersService,
 )
-from featurebyte.service.item_table import ItemTableService
+from featurebyte.service.item_table import ExtendedItemTableService, ItemTableService
 from featurebyte.service.namespace_handler import NamespaceHandler
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.offline_store_feature_table import OfflineStoreFeatureTableService
 from featurebyte.service.offline_store_feature_table_comment import (
     OfflineStoreFeatureTableCommentService,
 )
 from featurebyte.service.offline_store_feature_table_construction import (
@@ -133,15 +138,15 @@
 from featurebyte.service.relationship_info import RelationshipInfoService
 from featurebyte.service.scd_table import SCDTableService
 from featurebyte.service.semantic import SemanticService
 from featurebyte.service.session_manager import SessionManagerService
 from featurebyte.service.session_validator import SessionValidatorService
 from featurebyte.service.specialized_dtype import SpecializedDtypeDetectionService
 from featurebyte.service.static_source_table import StaticSourceTableService
-from featurebyte.service.table import TableService
+from featurebyte.service.table import AllTableService, TableService
 from featurebyte.service.table_columns_info import TableColumnsInfoService
 from featurebyte.service.table_facade import TableFacadeService
 from featurebyte.service.table_info import TableInfoService
 from featurebyte.service.table_status import TableStatusService
 from featurebyte.service.target import TargetService
 from featurebyte.service.target_helper.compute_target import TargetComputer, TargetExecutor
 from featurebyte.service.target_namespace import TargetNamespaceService
@@ -169,14 +174,15 @@
 from featurebyte.utils.persistent import MongoDBImpl
 from featurebyte.utils.storage import get_storage, get_temp_storage
 from featurebyte.worker import get_celery, get_redis
 from featurebyte.worker.task.batch_feature_create import BatchFeatureCreateTask
 from featurebyte.worker.task.batch_feature_table import BatchFeatureTableTask
 from featurebyte.worker.task.batch_request_table import BatchRequestTableTask
 from featurebyte.worker.task.catalog_online_store_update import CatalogOnlineStoreUpdateTask
+from featurebyte.worker.task.data_description import DataDescriptionTask
 from featurebyte.worker.task.deployment_create_update import DeploymentCreateUpdateTask
 from featurebyte.worker.task.feature_job_setting_analysis import FeatureJobSettingAnalysisTask
 from featurebyte.worker.task.feature_job_setting_analysis_backtest import (
     FeatureJobSettingAnalysisBacktestTask,
 )
 from featurebyte.worker.task.feature_list_batch_feature_create import (
     FeatureListCreateWithBatchFeatureCreationTask,
@@ -203,14 +209,15 @@
 app_container_config = AppContainerConfig()
 
 # Register classes - please keep sorted by alphabetical order.
 app_container_config.register_class(AllCatalogService)
 app_container_config.register_class(AllDeploymentController)
 app_container_config.register_class(AllDeploymentService)
 app_container_config.register_class(AllFeatureListService)
+app_container_config.register_class(AllTableService)
 app_container_config.register_class(BatchFeatureTableController)
 app_container_config.register_class(BatchFeatureTableService)
 app_container_config.register_class(BatchRequestTableController)
 app_container_config.register_class(BatchRequestTableService)
 app_container_config.register_class(
     CatalogController, dependency_override={"service": "catalog_service"}
 )
@@ -227,26 +234,28 @@
 app_container_config.register_class(DataWarehouseMigrationMixin)
 app_container_config.register_class(DeployFeatureManagementService)
 app_container_config.register_class(DeployFeatureListManagementService)
 app_container_config.register_class(FeastIntegrationService)
 app_container_config.register_class(DeployService)
 app_container_config.register_class(DeploymentController)
 app_container_config.register_class(DeploymentService)
+app_container_config.register_class(DeploymentServingEntityService)
 app_container_config.register_class(DerivePrimaryEntityHelper)
 app_container_config.register_class(DimensionTableController)
 app_container_config.register_class(DimensionTableService)
 app_container_config.register_class(EntityController)
 app_container_config.register_class(EntityRelationshipService)
 app_container_config.register_class(EntityService)
 app_container_config.register_class(EntityServingNamesService)
 app_container_config.register_class(EntityValidationService)
 app_container_config.register_class(EntityRelationshipExtractorService)
 app_container_config.register_class(EntityLookupFeatureTableService)
 app_container_config.register_class(EventTableController)
 app_container_config.register_class(EventTableService)
+app_container_config.register_class(ExtendedItemTableService)
 app_container_config.register_class(FeatureController)
 app_container_config.register_class(FeatureService)
 app_container_config.register_class(FeatureFacadeService)
 app_container_config.register_class(FeatureJobSettingAnalysisController)
 app_container_config.register_class(FeatureJobSettingAnalysisService)
 app_container_config.register_class(FeatureListController)
 app_container_config.register_class(FeatureListEntityRelationshipValidator)
@@ -380,28 +389,30 @@
 app_container_config.register_class(FeatureListCreateTask)
 app_container_config.register_class(FeatureListCreateWithBatchFeatureCreationTask)
 app_container_config.register_class(StaticSourceTableTask)
 app_container_config.register_class(TileTask)
 app_container_config.register_class(OnlineStoreCleanupTask)
 app_container_config.register_class(LongRunningTask)
 app_container_config.register_class(TestTask)
+app_container_config.register_class(DataDescriptionTask)
 app_container_config.register_class(FeatureListMakeProductionReadyTask)
 app_container_config.register_class(TaskProgressUpdater)
 app_container_config.register_class(BatchFeatureCreator)
 app_container_config.register_class(BlockModificationHandler)
 app_container_config.register_class(MongoDBImpl, name_override="persistent")
 
 # register migration services
 app_container_config.register_class(DataWarehouseMigrationServiceV1)
 app_container_config.register_class(DataWarehouseMigrationServiceV3)
 app_container_config.register_class(FeatureMigrationServiceV4)
 app_container_config.register_class(FeatureListMigrationServiceV5)
 app_container_config.register_class(FeatureListMigrationServiceV6)
 app_container_config.register_class(FeatureListMigrationServiceV7)
 app_container_config.register_class(FeatureMigrationServiceV8)
+app_container_config.register_class(OfflineStoreFeatureTableMigrationServiceV9)
 
 app_container_config.register_factory_method(get_storage)
 app_container_config.register_factory_method(get_redis, name_override="redis")
 app_container_config.register_factory_method(get_temp_storage, name_override="temp_storage")
 app_container_config.register_factory_method(get_celery)
 
 # registry feast related services
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/relationship_info/api.py` & `featurebyte-1.0.3/featurebyte/routes/relationship_info/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 RelationshipInfo API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/relationship_info/controller.py` & `featurebyte-1.0.3/featurebyte/routes/relationship_info/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 RelationshipInfo controller
 """
+
 from typing import Any, Dict, List, Optional, Tuple
 
 from bson import ObjectId
 
 from featurebyte.models.relationship import RelationshipInfoModel
 from featurebyte.persistent.base import SortDir
 from featurebyte.routes.common.base import BaseDocumentController
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/scd_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/scd_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SCDTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
@@ -122,15 +123,15 @@
     async def update_description(
         self, request: Request, scd_table_id: PydanticObjectId, data: DescriptionUpdate
     ) -> SCDTableModel:
         return await super().update_description(request, scd_table_id, data)
 
     async def create_object(self, request: Request, data: SCDTableCreate) -> SCDTableModel:
         controller = self.get_controller_for_request(request)
-        return await controller.create_table(data=data)  # type: ignore
+        return await controller.create_table(data=data)
 
     async def get_scd_table_info(
         self, request: Request, scd_table_id: PydanticObjectId, verbose: bool = VerboseQuery
     ) -> SCDTableInfo:
         """
         Retrieve scd table info
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/scd_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/scd_table/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SCDTable API route controller
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.enum import SemanticType
 from featurebyte.models.scd_table import SCDTableModel
 from featurebyte.routes.common.base_table import BaseTableDocumentController
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/semantic/api.py` & `featurebyte-1.0.3/featurebyte/routes/semantic/api.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Semantic API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from http import HTTPStatus
 
 from fastapi import Request
@@ -19,15 +20,15 @@
     AuditLogSortByQuery,
     PageQuery,
     PageSizeQuery,
     SearchQuery,
     SortDirQuery,
 )
 from featurebyte.routes.semantic.controller import SemanticController
-from featurebyte.schema.common.base import DescriptionUpdate
+from featurebyte.schema.common.base import DeleteResponse, DescriptionUpdate
 from featurebyte.schema.semantic import SemanticCreate, SemanticList
 
 
 class SemanticRouter(
     BaseApiRouter[SemanticModel, SemanticList, SemanticCreate, SemanticController]
 ):
     """
@@ -109,7 +110,12 @@
             search,
         )
 
     async def update_description(
         self, request: Request, semantic_id: PydanticObjectId, data: DescriptionUpdate
     ) -> SemanticModel:
         return await super().update_description(request, semantic_id, data)
+
+    async def delete_object(
+        self, request: Request, semantic_id: PydanticObjectId
+    ) -> DeleteResponse:
+        return await super().delete_object(request, semantic_id)
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/static_source_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/static_source_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/static_source_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/static_source_table/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTable API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Tuple
 
 from bson import ObjectId
 
 from featurebyte.models.persistent import QueryFilter
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/table/api.py` & `featurebyte-1.0.3/featurebyte/routes/table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Table API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/target/api.py` & `featurebyte-1.0.3/featurebyte/routes/target/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target API routes
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Query, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/target/controller.py` & `featurebyte-1.0.3/featurebyte/routes/target/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple
 
 from http import HTTPStatus
 
 from bson import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/target_namespace/api.py` & `featurebyte-1.0.3/featurebyte/routes/target_namespace/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TargetNamespace API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/target_namespace/controller.py` & `featurebyte-1.0.3/featurebyte/routes/target_namespace/controller.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 """
 Target namespace controller
 """
+
 from typing import Any, List, Tuple
 
 from bson import ObjectId
 
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.models.target_namespace import TargetNamespaceModel
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.schema.target_namespace import (
     TargetNamespaceCreate,
     TargetNamespaceInfo,
     TargetNamespaceList,
 )
+from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.target import TargetService
 from featurebyte.service.target_namespace import TargetNamespaceService
 from featurebyte.service.use_case import UseCaseService
 
 
 class TargetNamespaceController(
     BaseDocumentController[TargetNamespaceModel, TargetNamespaceService, TargetNamespaceList],
@@ -28,18 +30,20 @@
     paginated_document_class = TargetNamespaceList
 
     def __init__(
         self,
         target_namespace_service: TargetNamespaceService,
         target_service: TargetService,
         use_case_service: UseCaseService,
+        observation_table_service: ObservationTableService,
     ):
         super().__init__(target_namespace_service)
         self.target_service = target_service
         self.use_case_service = use_case_service
+        self.observation_table_service = observation_table_service
 
     async def create_target_namespace(
         self,
         data: TargetNamespaceCreate,
     ) -> TargetNamespaceModel:
         """
         Create TargetNamespace at persistent
@@ -58,14 +62,15 @@
 
     async def service_and_query_pairs_for_checking_reference(
         self, document_id: ObjectId
     ) -> List[Tuple[Any, QueryFilter]]:
         return [
             (self.target_service, {"target_namespace_id": document_id}),
             (self.use_case_service, {"target_namespace_id": document_id}),
+            (self.observation_table_service, {"target_namespace_id": document_id}),
         ]
 
     async def get_info(self, document_id: ObjectId, verbose: bool) -> TargetNamespaceInfo:
         """
         Get target namespace info given document_id
 
         Parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/target_table/api.py` & `featurebyte-1.0.3/featurebyte/routes/target_table/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TargetTable API routes
 """
+
 from __future__ import annotations
 
 from typing import Optional, cast
 
 import json
 from http import HTTPStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/target_table/controller.py` & `featurebyte-1.0.3/featurebyte/routes/target_table/controller.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 """
 Target API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 import pandas as pd
 from fastapi import UploadFile
 
+from featurebyte.exception import DocumentNotFoundError
+from featurebyte.models.base_feature_or_target_table import BaseFeatureOrTargetTableModel
+from featurebyte.models.observation_table import TargetInput
 from featurebyte.models.target_table import TargetTableModel
 from featurebyte.routes.common.feature_or_target_table import (
     FeatureOrTargetTableController,
     ValidationParameters,
 )
 from featurebyte.routes.task.controller import TaskController
 from featurebyte.schema.info import TargetTableInfo
@@ -19,14 +23,15 @@
 from featurebyte.schema.task import Task
 from featurebyte.schema.worker.task.target_table import TargetTableTaskPayload
 from featurebyte.service.entity_validation import EntityValidationService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_store_warehouse import FeatureStoreWarehouseService
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.target import TargetService
+from featurebyte.service.target_namespace import TargetNamespaceService
 from featurebyte.service.target_table import TargetTableService
 
 
 class TargetTableController(
     FeatureOrTargetTableController[
         TargetTableModel,
         TargetTableService,
@@ -48,57 +53,75 @@
         target_table_service: TargetTableService,
         feature_store_warehouse_service: FeatureStoreWarehouseService,
         feature_store_service: FeatureStoreService,
         observation_table_service: ObservationTableService,
         entity_validation_service: EntityValidationService,
         task_controller: TaskController,
         target_service: TargetService,
+        target_namespace_service: TargetNamespaceService,
     ):
         super().__init__(
             service=target_table_service,
             feature_store_warehouse_service=feature_store_warehouse_service,
             observation_table_service=observation_table_service,
             entity_validation_service=entity_validation_service,
             task_controller=task_controller,
         )
         self.feature_store_service = feature_store_service
         self.target_service = target_service
+        self.target_namespace_service = target_namespace_service
 
     async def get_payload(
         self, table_create: TargetTableCreate, observation_set_dataframe: Optional[pd.DataFrame]
     ) -> TargetTableTaskPayload:
         return await self.service.get_target_table_task_payload(
             data=table_create, observation_set_dataframe=observation_set_dataframe
         )
 
     async def get_validation_parameters(
         self, table_create: TargetTableCreate
     ) -> ValidationParameters:
         feature_store = await self.feature_store_service.get_document(
             document_id=table_create.feature_store_id
         )
-        graph = table_create.graph
+        # Handle backward compatibility for requests from older SDK
+        if isinstance(table_create.request_input, TargetInput):
+            target_id = table_create.request_input.target_id
+        else:
+            target_id = table_create.target_id
+
+        try:
+            if target_id is None:
+                raise DocumentNotFoundError
+            target = await self.target_service.get_document(target_id)
+            graph = target.graph
+            nodes = [graph.get_node_by_name(target.node.name)]
+        except DocumentNotFoundError:
+            assert table_create.graph is not None
+            graph = table_create.graph
+            nodes = table_create.nodes
+
         assert graph is not None
         return ValidationParameters(
             graph=graph,
-            nodes=table_create.nodes,
+            nodes=nodes,
             feature_store=feature_store,
             serving_names_mapping=table_create.serving_names_mapping,
         )
 
-    async def get_additional_info_params(self, document: TargetTableModel) -> dict[str, Any]:
-        target = await self.target_service.get_document(document.target_id)
-        return {"target_name": target.name}
+    async def get_additional_info_params(
+        self, document: BaseFeatureOrTargetTableModel
+    ) -> dict[str, Any]:
+        assert isinstance(document, TargetTableModel)
+        if document.target_namespace_id is None:
+            return {}
+        target_namespace = await self.target_namespace_service.get_document(
+            document.target_namespace_id
+        )
+        return {"target_name": target_namespace.name}
 
     async def create_table(
         self,
         data: TargetTableCreate,
         observation_set: Optional[UploadFile],
     ) -> Task:
-        if data.graph is None and data.target_id is not None:
-            data_dict = data.dict()
-            target_doc = await self.target_service.get_document(data.target_id)
-            data_dict["target_id"] = None
-            data_dict["graph"] = target_doc.graph
-            data_dict["node_names"] = [target_doc.node_name]
-            data = TargetTableCreate(**data_dict)
         return await super().create_table(data=data, observation_set=observation_set)
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/task/api.py` & `featurebyte-1.0.3/featurebyte/routes/task/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Job status route
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from fastapi import APIRouter, Request
 
 from featurebyte.persistent.base import SortDir
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/task/controller.py` & `featurebyte-1.0.3/featurebyte/routes/task/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 JobStatus API route controller
 """
+
 from __future__ import annotations
 
 from http import HTTPStatus
 
 from fastapi import HTTPException
 
 from featurebyte.persistent.base import SortDir
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/temp_data/api.py` & `featurebyte-1.0.3/featurebyte/routes/temp_data/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Temp data route
 """
+
 from __future__ import annotations
 
 from http import HTTPStatus
 from pathlib import Path
 
 from fastapi import APIRouter, Query, Request
 from fastapi.responses import StreamingResponse
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/temp_data/controller.py` & `featurebyte-1.0.3/featurebyte/routes/temp_data/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Temp Data API route controller
 """
+
 from __future__ import annotations
 
 import mimetypes
 from http import HTTPStatus
 from pathlib import Path
 
 from fastapi import HTTPException
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/use_case/api.py` & `featurebyte-1.0.3/featurebyte/routes/use_case/api.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 UseCase API routes
 """
+
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
@@ -165,25 +166,27 @@
     request: Request,
     page: int = PageQuery,
     page_size: int = PageSizeQuery,
     sort_by: Optional[str] = SortByQuery,
     sort_dir: Optional[SortDir] = SortDirQuery,
     search: Optional[str] = SearchQuery,
     name: Optional[str] = NameQuery,
+    feature_list_id: Optional[PydanticObjectId] = None,
 ) -> UseCaseList:
     """
     List Use Case
     """
     controller = request.state.app_container.use_case_controller
-    doc_list: UseCaseList = await controller.list(
+    doc_list: UseCaseList = await controller.list_use_cases(
         page=page,
         page_size=page_size,
         sort_by=[(sort_by, sort_dir)] if sort_by and sort_dir else None,
         search=search,
         name=name,
+        feature_list_id=feature_list_id,
     )
     return doc_list
 
 
 @router.get("/audit/{use_case_id}", response_model=AuditDocumentList)
 async def list_use_case_audit_logs(
     request: Request,
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/use_case/controller.py` & `featurebyte-1.0.3/featurebyte/routes/use_case/controller.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,30 +1,35 @@
 """
 UseCase API route controller
 """
-from typing import Any, Dict, List, Tuple
+
+from typing import Any, Dict, List, Optional, Tuple
 
 from bson import ObjectId
 
 from featurebyte.exception import (
     DocumentCreationError,
     DocumentDeletionError,
     ObservationTableInvalidUseCaseError,
 )
+from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.models.use_case import UseCaseModel
+from featurebyte.persistent.base import SortDir
 from featurebyte.routes.common.base import BaseDocumentController
 from featurebyte.schema.info import EntityBriefInfo, EntityBriefInfoList, UseCaseInfo
 from featurebyte.schema.observation_table import ObservationTableServiceUpdate
 from featurebyte.schema.use_case import UseCaseCreate, UseCaseList, UseCaseUpdate
 from featurebyte.service.catalog import CatalogService
 from featurebyte.service.context import ContextService
 from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.entity import EntityService
+from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
+from featurebyte.service.mixin import DEFAULT_PAGE_SIZE
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.target import TargetService
 from featurebyte.service.target_namespace import TargetNamespaceService
 from featurebyte.service.use_case import UseCaseService
 from featurebyte.service.user_service import UserService
 
 
@@ -46,25 +51,27 @@
         context_service: ContextService,
         deployment_service: DeploymentService,
         entity_service: EntityService,
         observation_table_service: ObservationTableService,
         historical_feature_table_service: HistoricalFeatureTableService,
         catalog_service: CatalogService,
         target_namespace_service: TargetNamespaceService,
+        feature_list_service: FeatureListService,
     ):
         super().__init__(use_case_service)
         self.user_service = user_service
         self.target_service = target_service
         self.context_service = context_service
         self.deployment_service = deployment_service
         self.entity_service = entity_service
         self.observation_table_service = observation_table_service
         self.historical_feature_table_service = historical_feature_table_service
         self.catalog_service = catalog_service
         self.target_namespace_service = target_namespace_service
+        self.feature_list_service = feature_list_service
 
     async def create_use_case(self, data: UseCaseCreate) -> UseCaseModel:
         """
         Create a UseCase
 
         Parameters
         ----------
@@ -258,14 +265,70 @@
             primary_entities=EntityBriefInfoList(__root__=entity_briefs),
             context_name=context.name,
             target_name=target_name,
             default_preview_table=default_preview_table_name,
             default_eda_table=default_eda_table_name,
         )
 
+    async def list_use_cases(
+        self,
+        page: int = 1,
+        page_size: int = DEFAULT_PAGE_SIZE,
+        sort_by: Optional[List[Tuple[str, SortDir]]] = None,
+        search: Optional[str] = None,
+        name: Optional[str] = None,
+        feature_list_id: Optional[PydanticObjectId] = None,
+    ) -> UseCaseList:
+        """
+        List UseCases
+
+        Parameters
+        ----------
+        page: int
+            Page number
+        page_size: int
+            Number of items per page
+        sort_by: Optional[List[Tuple[str, SortDir]]]
+            Sort by fields
+        search: Optional[str]
+            Search string
+        name: Optional[str]
+            UseCase name
+        feature_list_id: Optional[PydanticObjectId]
+            FeatureList id
+
+        Returns
+        -------
+        UseCaseList
+        """
+        query_filter = {}
+        if feature_list_id:
+            feature_list_doc = await self.feature_list_service.get_document_as_dict(
+                document_id=feature_list_id, projection={"supported_serving_entity_ids": 1}
+            )
+            supported_serving_entity_ids = feature_list_doc.get("supported_serving_entity_ids")
+            if supported_serving_entity_ids:
+                context_ids = []
+                async for context_doc in self.context_service.list_documents_as_dict_iterator(
+                    query_filter={"primary_entity_ids": {"$in": supported_serving_entity_ids}},
+                    projection={"_id": 1},
+                ):
+                    context_ids.append(context_doc["_id"])
+
+                query_filter["context_id"] = {"$in": context_ids}
+
+        return await self.list(
+            page=page,
+            page_size=page_size,
+            sort_by=sort_by,
+            search=search,
+            name=name,
+            query_filter=query_filter,
+        )
+
     async def list_feature_tables(
         self, use_case_id: ObjectId, page: int, page_size: int
     ) -> Dict[str, Any]:
         """
         Delete UseCase from persistent
 
         Parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/user_defined_function/api.py` & `featurebyte-1.0.3/featurebyte/routes/user_defined_function/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 UserDefinedFunction API routes
 """
+
 from typing import Optional, cast
 
 from http import HTTPStatus
 
 from fastapi import APIRouter, Request
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/routes/user_defined_function/controller.py` & `featurebyte-1.0.3/featurebyte/routes/user_defined_function/controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 UserDefinedFunction API route controller
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Tuple, cast
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentCreationError, DocumentUpdateError
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/batch_feature_table.py` & `featurebyte-1.0.3/featurebyte/schema/batch_feature_table.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 """
 BatchFeatureTable API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr, root_validator
+from pydantic import Field, root_validator
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.schema.common.base import PaginationMixin
 from featurebyte.schema.materialized_table import BaseMaterializedTableListRecord
 
 
 class BatchFeatureTableCreate(FeatureByteBaseModel):
     """
     BatchFeatureTableCreate creation payload
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     feature_store_id: PydanticObjectId
     batch_request_table_id: PydanticObjectId
     deployment_id: PydanticObjectId
 
 
 class BatchFeatureTableList(PaginationMixin):
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/batch_request_table.py` & `featurebyte-1.0.3/featurebyte/schema/batch_request_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTableModel API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List
 
 from featurebyte.models.batch_request_table import BatchRequestInput, BatchRequestTableModel
 from featurebyte.schema.common.base import PaginationMixin
 from featurebyte.schema.request_table import BaseRequestTableCreate, BaseRequestTableListRecord
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/catalog.py` & `featurebyte-1.0.3/featurebyte/schema/catalog.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,36 @@
 """
 Catalog API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
 from featurebyte.models.base import (
     FeatureByteBaseModel,
+    NameStr,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 from featurebyte.models.catalog import CatalogModel
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class CatalogCreate(FeatureByteBaseModel):
     """
     Catalog creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     default_feature_store_ids: List[PydanticObjectId]
     online_store_id: Optional[PydanticObjectId] = Field(default=None)
 
 
 class CatalogList(PaginationMixin):
     """
     Paginated list of Catalog
@@ -38,15 +40,15 @@
 
 
 class CatalogUpdate(FeatureByteBaseModel):
     """
     Catalog update schema
     """
 
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
 
 
 class CatalogOnlineStoreUpdate(BaseDocumentServiceUpdateSchema):
     """
     Catalog update online store schema
     """
 
@@ -54,15 +56,15 @@
 
 
 class CatalogServiceUpdate(BaseDocumentServiceUpdateSchema):
     """
     Catalog service update schema
     """
 
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
     is_deleted: Optional[bool]
 
     class Settings(BaseDocumentServiceUpdateSchema.Settings):
         """
         Unique contraints checking
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/common/base.py` & `featurebyte-1.0.3/featurebyte/schema/common/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base info related schema
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from datetime import datetime
 
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/common/operation.py` & `featurebyte-1.0.3/featurebyte/schema/common/operation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains class used to transform or extract dictionary like object
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict
 
 from pydantic import BaseModel, Field
 from typeguard import typechecked
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/context.py` & `featurebyte-1.0.3/featurebyte/schema/context.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,48 +1,32 @@
 """
 Context API payload schema
 """
+
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr, root_validator, validator
+from pydantic import Field, StrictStr, root_validator
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.context import ContextModel
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class ContextCreate(FeatureByteBaseModel):
     """
     Context creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     primary_entity_ids: List[PydanticObjectId]
     description: Optional[StrictStr]
 
-    @validator("primary_entity_ids")
-    @classmethod
-    def _sort_primary_entity_ids(cls, value: List[PydanticObjectId]) -> List[PydanticObjectId]:
-        """
-        Sort primary_entity_ids
-
-        Parameters
-        ----------
-        value: List[PydanticObjectId]
-            value to be validated
-
-        Returns
-        -------
-        List[PydanticObjectId]
-        """
-        return sorted(value)
-
 
 class ContextList(PaginationMixin):
     """
     Paginated list of context
     """
 
     data: List[ContextModel]
@@ -59,15 +43,15 @@
     default_preview_table_id: Optional[PydanticObjectId]
     default_eda_table_id: Optional[PydanticObjectId]
     observation_table_id_to_remove: Optional[PydanticObjectId]
 
     remove_default_eda_table: Optional[bool]
     remove_default_preview_table: Optional[bool]
 
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
 
     @root_validator(pre=True)
     @classmethod
     def _validate_parameters(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         # check xor between graph & node_name
         graph = values.get("graph")
         node_name = values.get("node_name")
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/credential.py` & `featurebyte-1.0.3/featurebyte/schema/credential.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 Pydantic schemas for handling API payloads for credential routes
 """
+
 from typing import List, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr, validator
+from pydantic import Field, validator
 
 from featurebyte.models.base import (
     FeatureByteBaseDocumentModel,
     FeatureByteBaseModel,
+    NameStr,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 
 # pylint: disable=too-many-ancestors
 from featurebyte.models.credential import (
@@ -26,15 +28,15 @@
 
 class CredentialCreate(FeatureByteBaseModel):
     """
     Schema for credential creation
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
     feature_store_id: PydanticObjectId
     database_credential: Optional[DatabaseCredential]
     storage_credential: Optional[StorageCredential]
 
 
 class CredentialRead(FeatureByteBaseDocumentModel):
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/deployment.py` & `featurebyte-1.0.3/featurebyte/schema/deployment.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 """
 Pydantic schemas for handling API payloads for deployment routes
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
-from featurebyte.models.deployment import DeploymentModel
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
+from featurebyte.models.deployment import DeploymentModel, FeastRegistryInfo
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class DeploymentCreate(FeatureByteBaseModel):
     """
     Schema for deployment creation
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
     feature_list_id: PydanticObjectId
     use_case_id: Optional[PydanticObjectId]
 
 
 class DeploymentList(PaginationMixin):
     """
     Paginated list of Deployment
@@ -36,14 +37,22 @@
     """
     Schema for deployment update
     """
 
     enabled: Optional[bool]
 
 
+class DeploymentServiceUpdate(DeploymentUpdate):
+    """
+    Schema for deployment service update
+    """
+
+    registry_info: Optional[FeastRegistryInfo]
+
+
 class DeploymentSummary(FeatureByteBaseModel):
     """
     Schema for deployment summary
     """
 
     num_feature_list: int
     num_feature: int
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/dimension_table.py` & `featurebyte-1.0.3/featurebyte/schema/dimension_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 DimensionTable API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Literal
 
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.enum import TableDataType
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/entity.py` & `featurebyte-1.0.3/featurebyte/schema/entity.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,37 @@
 """
 Entity API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
 from featurebyte.models.base import (
     FeatureByteBaseModel,
+    NameStr,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 from featurebyte.models.entity import EntityModel, ParentEntity
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class EntityCreate(FeatureByteBaseModel):
     """
     Entity creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
-    serving_name: StrictStr
+    name: NameStr
+    serving_name: NameStr
 
 
 class EntityList(PaginationMixin):
     """
     Paginated list of Entity
     """
 
@@ -37,23 +39,23 @@
 
 
 class EntityUpdate(FeatureByteBaseModel):
     """
     Entity update schema
     """
 
-    name: StrictStr
+    name: NameStr
 
 
 class EntityServiceUpdate(BaseDocumentServiceUpdateSchema):
     """
     Entity service update schema
     """
 
-    name: Optional[str]
+    name: Optional[NameStr]
     ancestor_ids: Optional[List[PydanticObjectId]]
     parents: Optional[List[ParentEntity]]
     table_ids: Optional[List[PydanticObjectId]]
     primary_table_ids: Optional[List[PydanticObjectId]]
 
     class Settings(BaseDocumentServiceUpdateSchema.Settings):
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/event_table.py` & `featurebyte-1.0.3/featurebyte/schema/event_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 EventTable API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Literal, Optional
 
 from pydantic import Field, StrictStr, root_validator, validator
 
 from featurebyte.common.model_util import validate_timezone_offset_string
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/feature.py` & `featurebyte-1.0.3/featurebyte/schema/feature.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,27 @@
 """
 Feature API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from datetime import datetime
 
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr, validator
+from pydantic import Field, validator
 
 from featurebyte.enum import ConflictResolution
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
+from featurebyte.models.base import (
+    FeatureByteBaseModel,
+    NameStr,
+    PydanticObjectId,
+    VersionIdentifier,
+)
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.feature_job_setting import TableFeatureJobSetting
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node.cleaning_operation import TableCleaningOperation
@@ -28,15 +34,15 @@
 
 class FeatureCreate(FeatureByteBaseModel):
     """
     Feature Creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     graph: QueryGraph
     node_name: str
     tabular_source: TabularSource
 
 
 class FeatureServiceCreate(FeatureCreate):
     """
@@ -48,15 +54,15 @@
 
 class BatchFeatureItem(FeatureByteBaseModel):
     """
     Batch Feature Item schema
     """
 
     id: PydanticObjectId
-    name: StrictStr
+    name: NameStr
     node_name: str
     tabular_source: TabularSource
 
 
 class BatchFeatureCreatePayload(FeatureByteBaseModel):
     """
     Batch Feature Creation schema (used by the client to prepare the payload)
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/schema/feature_job_setting_analysis.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,45 +1,47 @@
 """
 FeatureJobSettingAnalysis API payload schema
 """
+
 from typing import Any, Dict, List, Literal, Optional, Union
 
 from datetime import datetime
 
 from bson.objectid import ObjectId
 from pandas import Timestamp
 from pydantic import Field, StrictStr, root_validator
 
 from featurebyte.models.base import (
     FeatureByteBaseDocumentModel,
     FeatureByteBaseModel,
+    NameStr,
     PydanticObjectId,
 )
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.schema.common.base import PaginationMixin
 
 
 class EventTableCandidate(FeatureByteBaseModel):
     """
     Event Table Candidate Schema
     """
 
-    name: StrictStr
+    name: NameStr
     tabular_source: TabularSource
     event_timestamp_column: StrictStr
     record_creation_timestamp_column: StrictStr
 
 
 class FeatureJobSettingAnalysisCreate(FeatureByteBaseModel):
     """
     Feature Job Setting Analysis Creation Schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
     event_table_id: Optional[PydanticObjectId] = Field(default=None)
     event_table_candidate: Optional[EventTableCandidate] = Field(default=None)
     analysis_date: Optional[datetime] = Field(default=None)
     analysis_length: int = Field(ge=3600, le=3600 * 24 * 28 * 6, default=3600 * 24 * 28)
     min_featurejob_period: int = Field(ge=60, le=3600 * 24 * 28, default=60)
     exclude_late_job: bool = Field(default=False)
     blind_spot_buffer_setting: int = Field(ge=5, le=3600 * 24 * 28, default=5)
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/feature_list.py` & `featurebyte-1.0.3/featurebyte/schema/feature_list.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,32 @@
 """
 FeatureList API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Union
 
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr, root_validator, validator
+from pydantic import Field, root_validator, validator
 
 from featurebyte.common.doc_util import FBAutoDoc
 from featurebyte.common.validator import version_validator
 from featurebyte.config import FEATURE_PREVIEW_ROW_LIMIT, ONLINE_FEATURE_REQUEST_ROW_LIMIT
 from featurebyte.enum import ConflictResolution
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
+from featurebyte.models.base import (
+    FeatureByteBaseModel,
+    NameStr,
+    PydanticObjectId,
+    VersionIdentifier,
+)
 from featurebyte.models.feature_list import (
     FeatureCluster,
     FeatureListModel,
     FeatureReadinessDistribution,
-    ServingEntity,
 )
 from featurebyte.query_graph.node.validator import construct_unique_name_validator
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 from featurebyte.schema.common.feature_or_target import ComputeRequest
 from featurebyte.schema.feature import (
     MAX_BATCH_FEATURE_ITEM_COUNT,
     BatchFeatureCreate,
@@ -33,25 +38,25 @@
 
 class FeatureListCreate(FeatureByteBaseModel):
     """
     Feature List Creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     feature_ids: List[PydanticObjectId] = Field(min_items=1)
 
 
 class FeatureListCreateJob(FeatureByteBaseModel):
     """
     Feature List Job Creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     features: Union[List[FeatureParameters], List[PydanticObjectId]] = Field(min_items=1)
     features_conflict_resolution: ConflictResolution
 
 
 class FeatureListServiceCreate(FeatureListCreate):
     """
     Feature List Service Creation schema
@@ -60,15 +65,15 @@
     feature_list_namespace_id: Optional[PydanticObjectId] = Field(default_factory=ObjectId)
 
 
 class FeatureListCreateWithBatchFeatureCreationMixin(FeatureByteBaseModel):
     """Feature List Creation with Batch Feature Creation mixin"""
 
     id: PydanticObjectId = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     conflict_resolution: ConflictResolution
     features: List[BatchFeatureItem]
     skip_batch_feature_creation: bool = Field(default=False)
 
     @root_validator(pre=True)
     @classmethod
     def _validate_payload(cls, values: Dict[str, Any]) -> Dict[str, Any]:
@@ -178,15 +183,14 @@
     """
     FeatureList service update schema
     """
 
     deployed: Optional[bool]
     online_enabled_feature_ids: Optional[List[PydanticObjectId]]
     readiness_distribution: Optional[FeatureReadinessDistribution]
-    enabled_serving_entity_ids: Optional[List[ServingEntity]]
 
 
 class ProductionReadyFractionComparison(FeatureByteBaseModel):
     """
     Production ready fraction comparison
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/feature_list_namespace.py` & `featurebyte-1.0.3/featurebyte/schema/feature_list_namespace.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureListNamespace API payload scheme
 """
+
 from typing import List, Optional
 
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 from featurebyte.models.feature_list import FeatureReadinessDistribution, FeatureTypeFeatureCount
 from featurebyte.models.feature_list_namespace import FeatureListNamespaceModel, FeatureListStatus
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/feature_namespace.py` & `featurebyte-1.0.3/featurebyte/schema/feature_namespace.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 """
 FeatureNamespace API pyaload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
 from featurebyte.enum import DBVarType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.feature_namespace import (
     DefaultVersionMode,
     FeatureNamespaceModel,
     FeatureReadiness,
 )
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class FeatureNamespaceCreate(FeatureByteBaseModel):
     """
     Feature Namespace Creation Schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     dtype: DBVarType
     feature_ids: List[PydanticObjectId] = Field(default_factory=list)
     readiness: FeatureReadiness
     default_feature_id: PydanticObjectId
     default_version_mode: DefaultVersionMode = Field(default=DefaultVersionMode.AUTO)
     entity_ids: List[PydanticObjectId]
     table_ids: List[PydanticObjectId]
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/feature_store.py` & `featurebyte-1.0.3/featurebyte/schema/feature_store.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 """
 FeatureStore API payload schema
 """
+
 from typing import Any, Dict, List, Optional
 
 from datetime import datetime
 
 from bson.objectid import ObjectId
-from pydantic import Field, StrictStr, root_validator
+from pydantic import Field, root_validator
 
 from featurebyte.enum import SourceType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.credential import DatabaseCredential, StorageCredential
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node.schema import DatabaseDetails
 from featurebyte.schema.common.base import PaginationMixin
 
 
 class FeatureStoreCreate(FeatureByteBaseModel):
     """
     Feature Store Creation Schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     type: SourceType
     details: DatabaseDetails
     database_credential: Optional[DatabaseCredential]
     storage_credential: Optional[StorageCredential]
 
 
 class FeatureStoreList(PaginationMixin):
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/historical_feature_table.py` & `featurebyte-1.0.3/featurebyte/schema/historical_feature_table.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """
 HistoricalFeatureTable API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
-from pydantic import StrictStr, root_validator
+from pydantic import root_validator
 
-from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.base import NameStr, PydanticObjectId
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 from featurebyte.schema.common.feature_or_target import FeatureOrTargetTableCreate
 from featurebyte.schema.feature_list import FeatureListGetHistoricalFeatures
 from featurebyte.schema.materialized_table import BaseMaterializedTableListRecord
 
 
@@ -47,8 +48,8 @@
 
 
 class HistoricalFeatureTableUpdate(BaseDocumentServiceUpdateSchema):
     """
     HistoricalFeatureTable update payload
     """
 
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/info.py` & `featurebyte-1.0.3/featurebyte/schema/info.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,26 @@
 """
 Info related schema
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from datetime import datetime
 
-from pydantic import Field, StrictStr, root_validator, validator
+from pydantic import Field, root_validator, validator
 
 from featurebyte.enum import DBVarType, SourceType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId, VersionIdentifier
+from featurebyte.models.base import (
+    FeatureByteBaseModel,
+    NameStr,
+    PydanticObjectId,
+    VersionIdentifier,
+)
 from featurebyte.models.credential import DatabaseCredentialType, StorageCredentialType
 from featurebyte.models.feature_list import FeatureReadinessDistribution, FeatureTypeFeatureCount
 from featurebyte.models.feature_list_namespace import FeatureListStatus
 from featurebyte.models.feature_namespace import DefaultVersionMode
 from featurebyte.models.feature_store import TableStatus
 from featurebyte.models.online_store import OnlineStoreDetails
 from featurebyte.models.request_input import RequestInputType
@@ -143,29 +149,29 @@
         return EventTableBriefInfoList(__root__=event_table_project.project(paginated_data))
 
 
 class TableColumnInfo(FeatureByteBaseModel):
     """
     TableColumnInfo for storing column information
 
-    name: str
+    name: NameStr
         Column name
     dtype: DBVarType
         Variable type of the column
     entity: str
         Entity name associated with the column
     semantic: str
         Semantic name associated with the column
     critical_data_info: CriticalDataInfo
         Critical data information associated with the column
     description: str
         Description of the column
     """
 
-    name: StrictStr
+    name: NameStr
     dtype: DBVarType
     entity: Optional[str] = Field(default=None)
     semantic: Optional[str] = Field(default=None)
     critical_data_info: Optional[CriticalDataInfo] = Field(default=None)
     description: Optional[str] = Field(default=None)
 
 
@@ -397,14 +403,15 @@
     """
     ObservationTable info schema
     """
 
     type: RequestInputType
     feature_store_name: str
     table_details: TableDetails
+    target_name: Optional[str]
 
 
 class BaseFeatureOrTargetTableInfo(BaseInfo):
     """
     BaseFeatureOrTargetTable info schema
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/item_table.py` & `featurebyte-1.0.3/featurebyte/schema/item_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ItemTable API payload schema
 """
+
 from typing import List, Literal
 
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.enum import TableDataType
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.item_table import ItemTableModel
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/materialized_table.py` & `featurebyte-1.0.3/featurebyte/schema/materialized_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Schemas common to materialized table
 """
+
 from typing import Any, Dict, Tuple
 
 from pydantic import root_validator
 
 from featurebyte.models.base import FeatureByteBaseDocumentModel
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/observation_table.py` & `featurebyte-1.0.3/featurebyte/schema/observation_table.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """
 ObservationTableModel API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from bson import ObjectId
 from pydantic import Field, StrictStr
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.observation_table import ObservationInput, ObservationTableModel, Purpose
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 from featurebyte.schema.request_table import BaseRequestTableCreate, BaseRequestTableListRecord
 
 
 class ObservationTableCreate(BaseRequestTableCreate):
     """
@@ -20,25 +21,27 @@
     """
 
     sample_rows: Optional[int] = Field(ge=0)
     request_input: ObservationInput
     skip_entity_validation_checks: bool = Field(default=False)
     purpose: Optional[Purpose] = Field(default=None)
     primary_entity_ids: Optional[List[PydanticObjectId]]
+    target_column: Optional[StrictStr] = Field(default=None)
 
 
 class ObservationTableUpload(FeatureByteBaseModel):
     """
     ObservationTableUpload creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     purpose: Optional[Purpose] = Field(default=None)
     primary_entity_ids: List[PydanticObjectId]
+    target_column: Optional[StrictStr] = Field(default=None)
 
 
 class ObservationTableList(PaginationMixin):
     """
     Schema for listing observation tables
     """
 
@@ -57,15 +60,15 @@
     """
 
     context_id: Optional[PydanticObjectId]
     context_id_to_remove: Optional[PydanticObjectId]
     use_case_id_to_add: Optional[PydanticObjectId]
     use_case_id_to_remove: Optional[PydanticObjectId]
     purpose: Optional[Purpose]
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
 
 
 class ObservationTableServiceUpdate(BaseDocumentServiceUpdateSchema, ObservationTableUpdate):
     """
     ObservationTable Update schema for service
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/online_store.py` & `featurebyte-1.0.3/featurebyte/schema/online_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 """
 OnlineStore API payload schema
 """
+
 from typing import List, Optional
 
 from datetime import datetime
 
 from bson.objectid import ObjectId
 from pydantic import Field, StrictStr, validator
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.online_store import OnlineStoreDetails
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class OnlineStoreCreate(FeatureByteBaseModel):
     """
     Online Store Creation Schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     details: OnlineStoreDetails
 
 
 class OnlineStoreUpdate(BaseDocumentServiceUpdateSchema):
     """
     Online Store Creation Schema
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/preview.py` & `featurebyte-1.0.3/featurebyte/schema/preview.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Preview schema
 """
+
 from typing import Optional
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.schema.common.feature_or_target import ComputeRequest
 from featurebyte.schema.feature_list import PreviewObservationSet
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/relationship_info.py` & `featurebyte-1.0.3/featurebyte/schema/relationship_info.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 """
 Relationship Info payload schema
 """
+
 from typing import List, Optional
 
 from datetime import datetime
 
 from bson import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.relationship import RelationshipInfoModel, RelationshipType
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class RelationshipInfoCreate(FeatureByteBaseModel):
     """
     Relationship Info Creation Schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     relationship_type: RelationshipType
     entity_id: PydanticObjectId
     related_entity_id: PydanticObjectId
     relation_table_id: PydanticObjectId
     entity_column_name: Optional[str]
     related_entity_column_name: Optional[str]
     enabled: bool
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/request_table.py` & `featurebyte-1.0.3/featurebyte/schema/request_table.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 """
 Base class for all request tables.
 """
+
 from typing import Any, Dict, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr, root_validator
+from pydantic import Field, root_validator
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.request_input import RequestInputType
 from featurebyte.schema.materialized_table import BaseMaterializedTableListRecord
 
 
 class BaseRequestTableCreate(FeatureByteBaseModel):
     """
     Base creation schema for all request tables
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     feature_store_id: PydanticObjectId
     context_id: Optional[PydanticObjectId]
 
 
 class BaseRequestTableListRecord(BaseMaterializedTableListRecord):
     """
     This model determines the schema when listing tables.
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/scd_table.py` & `featurebyte-1.0.3/featurebyte/schema/scd_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SCDTable API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Literal, Optional
 
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.enum import TableDataType
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/semantic.py` & `featurebyte-1.0.3/featurebyte/schema/semantic.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 """
 Semantic API payload schema
 """
+
 from typing import List, Optional
 
 from bson.objectid import ObjectId
 from pydantic import Field
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.relationship import Parent
 from featurebyte.models.semantic import SemanticModel
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class SemanticCreate(FeatureByteBaseModel):
     """
     Semantic creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: str
+    name: NameStr
 
 
 class SemanticList(PaginationMixin):
     """
     Paginated list of Semantic
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/static_source_table.py` & `featurebyte-1.0.3/featurebyte/schema/static_source_table.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 """
 StaticSourceTableModel API payload schema
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.static_source_table import StaticSourceInput, StaticSourceTableModel
 from featurebyte.schema.common.base import PaginationMixin
 from featurebyte.schema.request_table import BaseRequestTableListRecord
 
 
 class StaticSourceTableCreate(FeatureByteBaseModel):
     """
     StaticSourceTableModel creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     feature_store_id: PydanticObjectId
     sample_rows: Optional[int] = Field(ge=0)
     request_input: StaticSourceInput
 
 
 class StaticSourceTableList(PaginationMixin):
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/table.py` & `featurebyte-1.0.3/featurebyte/schema/table.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,37 @@
 """
 Table model's attribute payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from bson.objectid import ObjectId
 from pydantic import Field, StrictStr, validator
 
 from featurebyte.common.validator import columns_info_validator
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.feature_store import TableStatus
 from featurebyte.models.proxy_table import ProxyTableModel
-from featurebyte.query_graph.model.column_info import ColumnInfo, ColumnInfoWithoutSemanticId
+from featurebyte.query_graph.model.column_info import ColumnInfo, ColumnSpecWithDescription
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.model.critical_data_info import CriticalDataInfo
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class TableCreate(FeatureByteBaseModel):
     """
     TableService create schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     tabular_source: TabularSource
-    columns_info: List[ColumnInfoWithoutSemanticId]
+    columns_info: List[ColumnSpecWithDescription]
     record_creation_timestamp_column: Optional[StrictStr]
     description: Optional[StrictStr]
 
     # pydantic validators
     _columns_info_validator = validator("columns_info", allow_reuse=True)(columns_info_validator)
 
     @classmethod
@@ -124,36 +125,36 @@
 
 
 class ColumnCriticalDataInfoUpdate(FeatureByteBaseModel):
     """
     Column critical data info update payload schema
     """
 
-    column_name: StrictStr
+    column_name: NameStr
     critical_data_info: Optional[CriticalDataInfo]
 
 
 class ColumnEntityUpdate(FeatureByteBaseModel):
     """
     Column entity update payload schema
     """
 
-    column_name: StrictStr
+    column_name: NameStr
     entity_id: Optional[PydanticObjectId]
 
 
 class ColumnDescriptionUpdate(FeatureByteBaseModel):
     """
     Column description update payload schema
     """
 
-    column_name: StrictStr
+    column_name: NameStr
     description: Optional[StrictStr]
 
 
 class ColumnSemanticUpdate(FeatureByteBaseModel):
     """
     Column semantic update payload schema
     """
 
-    column_name: StrictStr
+    column_name: NameStr
     semantic_id: Optional[PydanticObjectId]
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/target.py` & `featurebyte-1.0.3/featurebyte/schema/target.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 """
 Target API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from datetime import datetime
 
 from bson.objectid import ObjectId
 from pydantic import Field, StrictStr
 
 from featurebyte.models.base import (
     FeatureByteBaseModel,
+    NameStr,
     PydanticObjectId,
     UniqueConstraintResolutionSignature,
     UniqueValuesConstraint,
 )
 from featurebyte.models.target import TargetModel
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.common_table import TabularSource
@@ -27,15 +29,15 @@
 
 class TargetCreate(FeatureByteBaseModel):
     """
     Target creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     graph: QueryGraph
     node_name: str
     tabular_source: TabularSource
 
 
 class TargetList(PaginationMixin):
     """
@@ -86,15 +88,15 @@
 
 
 class TargetServiceUpdate(BaseDocumentServiceUpdateSchema):
     """
     Target service update schema
     """
 
-    name: Optional[str]
+    name: Optional[NameStr]
 
     class Settings(BaseDocumentServiceUpdateSchema.Settings):
         """
         Unique constraints checking
         """
 
         unique_constraints: List[UniqueValuesConstraint] = [
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/target_namespace.py` & `featurebyte-1.0.3/featurebyte/schema/target_namespace.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """
 Target namespace schema
 """
+
 from typing import List, Optional
 
 from bson import ObjectId
-from pydantic import Field, StrictStr
+from pydantic import Field
 
 from featurebyte.enum import DBVarType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.feature_namespace import DefaultVersionMode
 from featurebyte.models.target_namespace import TargetNamespaceModel
 from featurebyte.schema.common.base import (
     BaseDocumentServiceUpdateSchema,
     BaseInfo,
     PaginationMixin,
 )
@@ -19,16 +20,16 @@
 
 class TargetNamespaceCreate(FeatureByteBaseModel):
     """
     Target Namespace Creation Schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
-    dtype: Optional[DBVarType]
+    name: NameStr
+    dtype: DBVarType
     target_ids: List[PydanticObjectId] = Field(default_factory=list)
     default_target_id: Optional[PydanticObjectId]
     default_version_mode: DefaultVersionMode = Field(default=DefaultVersionMode.AUTO)
     entity_ids: List[PydanticObjectId] = Field(default_factory=list)
     window: Optional[str]
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/target_table.py` & `featurebyte-1.0.3/featurebyte/schema/target_table.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 """
 TargetTable API payload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from pydantic import Field, StrictStr, root_validator
 
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.observation_table import ObservationInput
-from featurebyte.models.target_table import TargetTableModel
+from featurebyte.models.observation_table import ObservationTableModel, TargetInput
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.schema.common.base import PaginationMixin
 from featurebyte.schema.common.feature_or_target import FeatureOrTargetTableCreate
 from featurebyte.schema.materialized_table import BaseMaterializedTableListRecord
 
 
@@ -24,15 +24,15 @@
 
     serving_names_mapping: Optional[Dict[str, str]]
     target_id: Optional[PydanticObjectId]
     graph: Optional[QueryGraph] = Field(default=None)
     # Note that even though node_names is a list, we typically only expect one node. We can't change this to a non-list
     # for backwards compatibility reasons.
     node_names: Optional[List[StrictStr]] = Field(default=None)
-    request_input: ObservationInput
+    request_input: Optional[TargetInput]
     context_id: Optional[PydanticObjectId]
     skip_entity_validation_checks: bool = Field(default=False)
 
     @root_validator(pre=True)
     @classmethod
     def _check_graph_and_node_names(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         graph = values.get("graph", None)
@@ -48,14 +48,18 @@
             raise ValueError(
                 "If target_id is provided, graph and node_names should not be provided."
             )
 
         # If target is not provided, graph and node_names should be provided.
         both_are_not_none = graph is not None and node_names is not None
         if both_are_not_none:
+            if target_id is not None:
+                raise ValueError(
+                    "If graph and node_names are provided, target_id should not be provided."
+                )
             return values
         raise ValueError(
             "Both graph and node_names should be provided, or neither should be provided."
         )
 
     @property
     def nodes(self) -> List[Node]:
@@ -72,15 +76,15 @@
 
 
 class TargetTableList(PaginationMixin):
     """
     Schema for listing targe tables
     """
 
-    data: List[TargetTableModel]
+    data: List[ObservationTableModel]
 
 
 class TargetTableListRecord(BaseMaterializedTableListRecord):
     """
     Schema for listing target tables as a DataFrame
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/task.py` & `featurebyte-1.0.3/featurebyte/schema/task.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TaskStatus API payload schema
 """
+
 from typing import Any, Dict, List, Optional, Set, Union
 
 import datetime
 from uuid import UUID
 
 from pydantic import Field
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/use_case.py` & `featurebyte-1.0.3/featurebyte/schema/use_case.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 """
 Use Case API payload schema
 """
+
 from typing import Any, Dict, List, Optional
 
 from pydantic import Field, StrictStr, root_validator
 
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.use_case import UseCaseModel
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class UseCaseCreate(FeatureByteBaseModel):
     """
     Use Case creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=PydanticObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     target_id: Optional[PydanticObjectId]
     target_namespace_id: Optional[PydanticObjectId]
     context_id: PydanticObjectId
     description: Optional[StrictStr]
 
     @root_validator(pre=True)
     @classmethod
@@ -40,15 +41,15 @@
     default_preview_table_id: Optional[PydanticObjectId]
     default_eda_table_id: Optional[PydanticObjectId]
     observation_table_id_to_remove: Optional[PydanticObjectId]
 
     remove_default_eda_table: Optional[bool]
     remove_default_preview_table: Optional[bool]
 
-    name: Optional[StrictStr]
+    name: Optional[NameStr]
 
     @root_validator(pre=True)
     @classmethod
     def _validate_input(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         default_preview_table_id = values.get("default_preview_table_id", None)
         default_eda_table_id = values.get("default_eda_table_id", None)
         observation_table_id_to_remove = values.get("observation_table_id_to_remove", None)
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/user_defined_function.py` & `featurebyte-1.0.3/featurebyte/schema/user_defined_function.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 """
 UserDefinedFunction API payload schema
 """
+
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
 from pydantic import Field, StrictStr, root_validator, validator
 
 from featurebyte.enum import DBVarType
-from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
+from featurebyte.models.base import FeatureByteBaseModel, NameStr, PydanticObjectId
 from featurebyte.models.user_defined_function import FunctionParameter, UserDefinedFunctionModel
 from featurebyte.query_graph.node.validator import construct_unique_name_validator
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema, PaginationMixin
 
 
 class UserDefinedFunctionCreateBase(FeatureByteBaseModel):
     """
     UserDefinedFunction creation schema
     """
 
     id: Optional[PydanticObjectId] = Field(default_factory=ObjectId, alias="_id")
-    name: StrictStr
+    name: NameStr
     description: Optional[StrictStr] = Field(default=None)
-    sql_function_name: StrictStr
+    sql_function_name: NameStr
     function_parameters: List[FunctionParameter]
     output_dtype: DBVarType
 
     # pydantic validator
     _validate_unique_function_parameter_name = validator("function_parameters", allow_reuse=True)(
         construct_unique_name_validator(field="name")
     )
@@ -49,15 +50,15 @@
 
 
 class UserDefinedFunctionUpdate(FeatureByteBaseModel):
     """
     UserDefinedFunction update schema
     """
 
-    sql_function_name: Optional[StrictStr]
+    sql_function_name: Optional[NameStr]
     function_parameters: Optional[List[FunctionParameter]]
     output_dtype: Optional[DBVarType]
 
     # pydanctic validator
     _validate_unique_function_parameter_name = validator("function_parameters", allow_reuse=True)(
         construct_unique_name_validator(field="name")
     )
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/base.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/base.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 """
 BaseTaskPayload schema
 """
+
 from __future__ import annotations
 
 from typing import Any, ClassVar, Optional
 
 import json
-from enum import Enum
+from enum import Enum, IntEnum
 
 from bson.objectid import ObjectId
 from pydantic import Field
 
 from featurebyte.enum import StrEnum
 from featurebyte.models.base import FeatureByteBaseModel, PydanticObjectId
 
@@ -20,26 +21,37 @@
     Task type enum
     """
 
     CPU_TASK = "cpu_task"
     IO_TASK = "io_task"
 
 
+class TaskPriority(IntEnum):
+    """
+    Task priority enum
+    """
+
+    CRITICAL = 0
+    HIGH = 1
+    MEDIUM = 2
+    LOW = 3
+
+
 class BaseTaskPayload(FeatureByteBaseModel):
     """
     Base class for Task payload
     """
 
     user_id: Optional[PydanticObjectId]
     catalog_id: PydanticObjectId
     output_document_id: PydanticObjectId = Field(default_factory=ObjectId)
     output_collection_name: ClassVar[Optional[str]] = None
     command: ClassVar[Optional[Enum]] = None
     task_type: TaskType = Field(default=TaskType.IO_TASK)
-    priority: int = Field(default=0, ge=0, le=3)  # 0 is the highest priority
+    priority: TaskPriority = Field(default=TaskPriority.MEDIUM)
     is_scheduled_task: Optional[bool] = Field(default=False)
     is_revocable: ClassVar[Optional[bool]] = False
 
     class Config:
         """
         Configurations for BaseTaskPayload
         """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/batch_feature_create.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/batch_feature_create.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Batch feature create task payload
 """
+
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import Field
 
 from featurebyte.enum import ConflictResolution, WorkerCommand
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/batch_feature_table.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/batch_feature_table.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 Online prediction table task payload schema
 """
+
 from __future__ import annotations
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.schema.batch_feature_table import BatchFeatureTableCreate
-from featurebyte.schema.worker.task.base import BaseTaskPayload
+from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskPriority
 
 
 class BatchFeatureTableTaskPayload(BaseTaskPayload, BatchFeatureTableCreate):
     """
     BatchFeatureTable creation task payload
     """
 
     output_collection_name = BatchFeatureTableModel.collection_name()
     command = WorkerCommand.BATCH_FEATURE_TABLE_CREATE
+    priority = TaskPriority.CRITICAL
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/batch_request_table.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/batch_request_table.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 BatchRequestTableTaskPayload schema
 """
+
 from __future__ import annotations
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.schema.batch_request_table import BatchRequestTableCreate
-from featurebyte.schema.worker.task.base import BaseTaskPayload
+from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskPriority
 
 
 class BatchRequestTableTaskPayload(BaseTaskPayload, BatchRequestTableCreate):
     """
     BatchRequestTable creation task payload
     """
 
     output_collection_name = BatchRequestTableModel.collection_name()
     command = WorkerCommand.BATCH_REQUEST_TABLE_CREATE
+    priority = TaskPriority.CRITICAL
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/deployment_create_update.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/deployment_create_update.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """
 FeatureList Deploy Task Payload schema
 """
+
 from typing import Optional, Union
 from typing_extensions import Annotated, Literal
 
 from pydantic import BaseModel, Field
 
 from featurebyte.enum import StrEnum, WorkerCommand
-from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.base import NameStr, PydanticObjectId
 from featurebyte.models.deployment import DeploymentModel
 from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskType
 
 
 class DeploymentPayloadType(StrEnum):
     """Deployment payload type"""
 
@@ -19,15 +20,15 @@
     UPDATE = "update"
 
 
 class CreateDeploymentPayload(BaseModel):
     """Create deployment"""
 
     type: Literal[DeploymentPayloadType.CREATE] = Field(DeploymentPayloadType.CREATE, const=True)
-    name: Optional[str] = Field(default=None)
+    name: Optional[NameStr] = Field(default=None)
     feature_list_id: PydanticObjectId
     enabled: bool
     use_case_id: Optional[PydanticObjectId] = Field(default=None)
     context_id: Optional[PydanticObjectId] = Field(default=None)
 
 
 class UpdateDeploymentPayload(BaseModel):
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/feature_job_setting_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureJobSettingAnalysisTaskPayload schema
 """
+
 from typing import Optional
 
 from pydantic import Field
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.feature_job_setting_analysis import FeatureJobSettingAnalysisModel
 from featurebyte.schema.feature_job_setting_analysis import (
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/feature_list_batch_feature_create.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/feature_list_batch_feature_create.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature list creation with batch feature creation schema
 """
+
 from __future__ import annotations
 
 from pydantic import Field
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.schema.feature_list import FeatureListCreateWithBatchFeatureCreation
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/feature_list_create.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/feature_list_create.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 """
 Feature list creation creation schema
 """
+
 from __future__ import annotations
 
 from typing import List, Union
 
-from pydantic import BaseModel, Field, StrictStr
+from pydantic import BaseModel, Field
 
 from featurebyte.enum import ConflictResolution, WorkerCommand
-from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.base import NameStr, PydanticObjectId
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskType
 
 
 class FeatureParameters(BaseModel):
     """Feature parameters"""
 
     id: PydanticObjectId
-    name: StrictStr
+    name: NameStr
 
 
 class FeaturesParameters(BaseModel):
     """Feature list feature parameters"""
 
     features: Union[List[FeatureParameters], List[PydanticObjectId]]
 
@@ -31,10 +32,10 @@
     Feature list creation task payload
     """
 
     command = WorkerCommand.FEATURE_LIST_CREATE
     task_type: TaskType = Field(default=TaskType.CPU_TASK)
     output_collection_name = FeatureListModel.collection_name()
     feature_list_id: PydanticObjectId
-    feature_list_name: str
+    feature_list_name: NameStr
     features_parameters_path: str
     features_conflict_resolution: ConflictResolution
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/feature_list_make_production_ready.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/feature_list_make_production_ready.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature list make production ready task payload
 """
+
 from __future__ import annotations
 
 from pydantic import Field
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.feature_list import FeatureListModel
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/historical_feature_table.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/historical_feature_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 HistoricalFeaturesTaskPayload schema
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.schema.historical_feature_table import HistoricalFeatureTableCreate
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/materialized_table_delete.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/materialized_table_delete.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,34 +1,33 @@
 """
 Materialized Table Delete Task Payload schema
 """
+
 from __future__ import annotations
 
 from featurebyte.enum import StrEnum, WorkerCommand
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.models.static_source_table import StaticSourceTableModel
-from featurebyte.models.target_table import TargetTableModel
 from featurebyte.schema.worker.task.base import BaseTaskPayload
 
 
 class MaterializedTableCollectionName(StrEnum):
     """
     Materialized table collection name
     """
 
     OBSERVATION = ObservationTableModel.collection_name()
     HISTORICAL_FEATURE = HistoricalFeatureTableModel.collection_name()
     BATCH_REQUEST = BatchRequestTableModel.collection_name()
     BATCH_FEATURE = BatchFeatureTableModel.collection_name()
     STATIC_SOURCE = StaticSourceTableModel.collection_name()
-    TARGET = TargetTableModel.collection_name()
 
 
 class MaterializedTableDeleteTaskPayload(BaseTaskPayload):
     """
     Materialized Table Delete Task Payload
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/observation_table.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/observation_table_upload.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,18 +1,30 @@
 """
-ObservationTableTaskPayload schema
+ObservationTableUploadTaskPayload schema
 """
+
 from __future__ import annotations
 
-from featurebyte.enum import WorkerCommand
+from typing import Optional
+
+from pydantic import Field
+
+from featurebyte.enum import UploadFileFormat, WorkerCommand
+from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.observation_table import ObservationTableModel
-from featurebyte.schema.observation_table import ObservationTableCreate
-from featurebyte.schema.worker.task.base import BaseTaskPayload
+from featurebyte.schema.observation_table import ObservationTableUpload
+from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskType
 
 
-class ObservationTableTaskPayload(BaseTaskPayload, ObservationTableCreate):
+class ObservationTableUploadTaskPayload(BaseTaskPayload, ObservationTableUpload):
     """
-    ObservationTable creation task payload
+    ObservationTableUploadTaskPayload creation task payload
     """
 
     output_collection_name = ObservationTableModel.collection_name()
-    command = WorkerCommand.OBSERVATION_TABLE_CREATE
+    command = WorkerCommand.OBSERVATION_TABLE_UPLOAD
+    observation_set_storage_path: str
+    file_format: UploadFileFormat
+    # This is the name of the file that was uploaded by the user
+    uploaded_file_name: str
+    target_namespace_id: Optional[PydanticObjectId]
+    task_type: TaskType = Field(default=TaskType.CPU_TASK)
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/observation_table_upload.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/observation_table.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 """
-ObservationTableUploadTaskPayload schema
+ObservationTableTaskPayload schema
 """
+
 from __future__ import annotations
 
-from featurebyte.enum import UploadFileFormat, WorkerCommand
+from typing import Optional
+
+from pydantic import Field
+
+from featurebyte.enum import WorkerCommand
+from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.observation_table import ObservationTableModel
-from featurebyte.schema.observation_table import ObservationTableUpload
-from featurebyte.schema.worker.task.base import BaseTaskPayload
+from featurebyte.schema.observation_table import ObservationTableCreate
+from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskType
 
 
-class ObservationTableUploadTaskPayload(BaseTaskPayload, ObservationTableUpload):
+class ObservationTableTaskPayload(BaseTaskPayload, ObservationTableCreate):
     """
-    ObservationTableUploadTaskPayload creation task payload
+    ObservationTable creation task payload
     """
 
     output_collection_name = ObservationTableModel.collection_name()
-    command = WorkerCommand.OBSERVATION_TABLE_UPLOAD
-    observation_set_storage_path: str
-    file_format: UploadFileFormat
-    # This is the name of the file that was uploaded by the user
-    uploaded_file_name: str
+    command = WorkerCommand.OBSERVATION_TABLE_CREATE
+    target_namespace_id: Optional[PydanticObjectId]
+    task_type: TaskType = Field(default=TaskType.CPU_TASK)
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/static_source_table.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/static_source_table.py`

 * *Files identical despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTableTaskPayload schema
 """
+
 from __future__ import annotations
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.models.static_source_table import StaticSourceTableModel
 from featurebyte.schema.static_source_table import StaticSourceTableCreate
 from featurebyte.schema.worker.task.base import BaseTaskPayload
```

### Comparing `featurebyte-1.0.2/featurebyte/schema/worker/task/test.py` & `featurebyte-1.0.3/featurebyte/schema/worker/task/test.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TestTaskPayload schema
 """
+
 from typing import Optional
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.schema.worker.task.base import BaseTaskPayload, TaskType
 
 
 class TestTaskPayload(BaseTaskPayload):
```

### Comparing `featurebyte-1.0.2/featurebyte/service/base_document.py` & `featurebyte-1.0.3/featurebyte/service/base_document.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 """
 BaseService class
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
 from typing import Any, AsyncIterator, Dict, Generic, Iterator, List, Optional, Type, TypeVar, Union
 
 import copy
 from contextlib import contextmanager
 from dataclasses import dataclass
 from pathlib import Path
 
 import numpy as np
 import pandas as pd
 from bson.objectid import ObjectId
+from cachetools import LRUCache
 from pymongo.errors import OperationFailure
 from redis import Redis
 from tenacity import retry, retry_if_exception_type, wait_chain, wait_random
 
 from featurebyte.common.dict_util import get_field_path_value
 from featurebyte.exception import (
     CatalogNotSpecifiedError,
@@ -103,14 +105,15 @@
     collection. It will perform model level validation before writing to persistent and after
     reading from the persistent.
     """
 
     # pylint: disable=too-many-public-methods
 
     document_class: Type[Document]
+    _remote_attribute_cache: Any = LRUCache(maxsize=1024)
 
     def __init__(
         self,
         user: Any,
         persistent: Persistent,
         catalog_id: Optional[ObjectId],
         block_modification_handler: BlockModificationHandler,
@@ -415,38 +418,42 @@
             Additional keyword arguments
 
         Returns
         -------
         int
             number of records deleted
         """
-        document = await self.get_document(
+        document_dict = await self.get_document_as_dict(
             document_id=document_id,
             exception_detail=exception_detail,
             use_raw_query_filter=use_raw_query_filter,
             disable_audit=self.should_disable_audit,
             populate_remote_attributes=False,
             **kwargs,
         )
 
         # check if document is modifiable
-        self._check_document_modifiable(document=document.dict(by_alias=True))
+        self._check_document_modifiable(document=document_dict)
 
         query_filter = self._construct_get_query_filter(
             document_id=document_id, use_raw_query_filter=use_raw_query_filter, **kwargs
         )
         num_of_records_deleted = await self.persistent.delete_one(
             collection_name=self.collection_name,
             query_filter=query_filter,
             user_id=self.user.id,
             disable_audit=self.should_disable_audit,
         )
 
         # remove remote attributes
-        for remote_path in document.remote_attribute_paths:
+        for (
+            remote_path
+        ) in self.document_class._get_remote_attribute_paths(  # pylint: disable=protected-access
+            document_dict
+        ):
             await self.storage.try_delete_if_exists(remote_path)
         return int(num_of_records_deleted)
 
     def construct_list_query_filter(
         self,
         query_filter: Optional[QueryFilter] = None,
         use_raw_query_filter: bool = False,
@@ -1014,14 +1021,15 @@
         self,
         document_id: ObjectId,
         data: DocumentUpdateSchema,
         exclude_none: bool = True,
         document: Optional[Document] = None,
         return_document: bool = True,
         skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
     ) -> Optional[Document]:
         """
         Update document at persistent
 
         Parameters
         ----------
         document_id: ObjectId
@@ -1032,35 +1040,41 @@
             Whether to exclude None value(s) from the table
         document: Optional[Document]
             Document to be updated (when provided, this method won't query persistent for retrieval)
         return_document: bool
             Whether to make additional query to retrieval updated document & return
         skip_block_modification_check: bool
             Whether to skip block modification check (use with caution, only use when updating document description)
+        populate_remote_attributes: bool
+            Whether to populate remote attributes (e.g. file paths) when returning document
 
         Returns
         -------
         Optional[Document]
         """
         if document is None:
-            document = await self.get_document(document_id=document_id)
+            document = await self.get_document(
+                document_id=document_id, populate_remote_attributes=False
+            )
 
         # perform validation first before actual update
         update_dict = data.dict(exclude_none=exclude_none)
 
         # update document to persistent
         await self._update_document(
             document=document,
             update_dict=update_dict,
             update_document_class=type(data),
             skip_block_modification_check=skip_block_modification_check,
         )
 
         if return_document:
-            return await self.get_document(document_id=document_id)
+            return await self.get_document(
+                document_id=document_id, populate_remote_attributes=populate_remote_attributes
+            )
         return None
 
     @retry(
         retry=retry_if_exception_type(OperationFailure),
         wait=wait_chain(
             *[wait_random(max=RETRY_MAX_WAIT_IN_SEC) for _ in range(RETRY_MAX_ATTEMPT_NUM)]
         ),
```

### Comparing `featurebyte-1.0.2/featurebyte/service/base_feature_service.py` & `featurebyte-1.0.3/featurebyte/service/base_feature_service.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 """
 Base namespace service
 """
+
 from __future__ import annotations
 
-from typing import Any, List, Optional
+from typing import Any, Dict, List, Optional
 
 from dataclasses import dataclass
 
 from bson import ObjectId
 from redis import Redis
 
 from featurebyte.common.model_util import get_version
 from featurebyte.models.base import VersionIdentifier
+from featurebyte.models.entity import EntityModel
 from featurebyte.persistent import Persistent
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.routes.common.derive_primary_entity_helper import DerivePrimaryEntityHelper
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
@@ -26,14 +28,15 @@
 
 
 @dataclass
 class FeatureOrTargetDerivedData:
     """Feature or Target data"""
 
     primary_entity_ids: List[ObjectId]
+    entity_id_to_entity: Dict[ObjectId, EntityModel]
     relationships_info: List[EntityRelationshipInfo]
 
 
 class BaseFeatureService(
     BaseDocumentService[Document, DocumentCreateSchema, BaseDocumentServiceUpdateSchema]
 ):
     """
@@ -97,18 +100,20 @@
 
         Returns
         -------
         FeatureOrTargetDerivedData
         """
         query_graph = QueryGraph(**graph.dict(by_alias=True))
         entity_ids = query_graph.get_entity_ids(node_name=node_name)
+        extractor = self.entity_relationship_extractor_service
+        entity_id_to_entity = await extractor.get_entity_id_to_entity(entity_ids=entity_ids)
         primary_entity_ids = await self.derive_primary_entity_helper.derive_primary_entity_ids(
-            entity_ids=entity_ids
+            entity_ids=entity_ids, entity_id_to_entity=entity_id_to_entity
         )
-        extractor = self.entity_relationship_extractor_service
         relationships_info = await extractor.extract_relationship_from_primary_entity(
             entity_ids=entity_ids, primary_entity_ids=primary_entity_ids
         )
         return FeatureOrTargetDerivedData(
             primary_entity_ids=primary_entity_ids,
+            entity_id_to_entity=entity_id_to_entity,
             relationships_info=relationships_info,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/base_table_document.py` & `featurebyte-1.0.3/featurebyte/service/base_table_document.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BaseTableDocumentService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, TypeVar
 
 from bson.objectid import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/batch_feature_table.py` & `featurebyte-1.0.3/featurebyte/service/batch_feature_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchFeatureTableService class
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.enum import MaterializedTableNamePrefix
 from featurebyte.models.base import FeatureByteBaseDocumentModel
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/batch_request_table.py` & `featurebyte-1.0.3/featurebyte/service/batch_request_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTableService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/catalog.py` & `featurebyte-1.0.3/featurebyte/service/catalog.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 CatalogService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson import ObjectId
 
 from featurebyte.models.catalog import CatalogModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/context.py` & `featurebyte-1.0.3/featurebyte/service/context.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ContextService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson import ObjectId
 from redis import Redis
 
@@ -115,25 +116,29 @@
         self,
         document_id: ObjectId,
         data: ContextUpdate,
         exclude_none: bool = True,
         document: Optional[ContextModel] = None,
         return_document: bool = True,
         skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
     ) -> Optional[ContextModel]:
-        document = await self.get_document(document_id=document_id)
+        document = await self.get_document(
+            document_id=document_id, populate_remote_attributes=False
+        )
         if data.graph and data.node_name:
             node = data.graph.get_node_by_name(data.node_name)
             operation_structure = data.graph.extract_operation_structure(
                 node=node, keep_all_source_columns=True
             )
             await self._validate_view(operation_structure=operation_structure, context=document)
 
         document = await super().update_document(
             document_id=document_id,
             document=document,
             data=data,
             exclude_none=exclude_none,
             return_document=return_document,
             skip_block_modification_check=skip_block_modification_check,
+            populate_remote_attributes=populate_remote_attributes,
         )
         return document
```

### Comparing `featurebyte-1.0.2/featurebyte/service/credential.py` & `featurebyte-1.0.3/featurebyte/service/credential.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 CredentialService
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson.objectid import ObjectId
 from cryptography.fernet import InvalidToken
 from redis import Redis
@@ -140,14 +141,15 @@
         self,
         document_id: ObjectId,
         data: CredentialServiceUpdate,
         exclude_none: bool = True,
         document: Optional[CredentialModel] = None,
         return_document: bool = True,
         skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
     ) -> Optional[CredentialModel]:
         """
         Update document at persistent
 
         Parameters
         ----------
         document_id: ObjectId
@@ -158,21 +160,25 @@
             Whether to exclude None value(s) from the table
         document: Optional[CredentialModel]
             Credential to be updated (when provided, this method won't query persistent for retrieval)
         return_document: bool
             Whether to make additional query to retrieval updated document & return
         skip_block_modification_check: bool
             Whether to skip block modification check (used only when updating description)
+        populate_remote_attributes: bool
+            Whether to populate remote attributes
 
         Returns
         -------
         Optional[Document]
         """
         if document is None:
-            document = await self.get_document(document_id=document_id)
+            document = await self.get_document(
+                document_id=document_id, populate_remote_attributes=False
+            )
 
         # ensure document is decrypted
         try:
             document.decrypt_credentials()
         except InvalidToken:
             logger.warning("Credential is already decrypted")
 
@@ -185,8 +191,9 @@
         return await super().update_document(
             document_id=document_id,
             data=data,
             exclude_none=exclude_none,
             document=document,
             return_document=return_document,
             skip_block_modification_check=skip_block_modification_check,
+            populate_remote_attributes=populate_remote_attributes,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/deploy.py` & `featurebyte-1.0.3/featurebyte/service/deploy.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,42 +1,54 @@
 """
 DeployService class
 """
+
+# pylint: disable=too-many-lines
+
 from typing import Any, AsyncIterator, Callable, Coroutine, Dict, List, Optional, Sequence, Set
 
 import traceback
 
 from bson import ObjectId
 
 from featurebyte.common.progress import get_ranged_progress_callback
+from featurebyte.enum import DBVarType
 from featurebyte.exception import DocumentCreationError, DocumentUpdateError
 from featurebyte.feast.model.registry import FeastRegistryModel
 from featurebyte.feast.service.registry import FeastRegistryService
 from featurebyte.logging import get_logger
-from featurebyte.models.deployment import DeploymentModel, FeastIntegrationSettings
+from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.deployment import (
+    DeploymentModel,
+    FeastIntegrationSettings,
+    FeastRegistryInfo,
+)
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureListModel, ServingEntity
 from featurebyte.models.feature_list_namespace import FeatureListStatus
 from featurebyte.models.feature_namespace import FeatureReadiness
-from featurebyte.schema.deployment import DeploymentUpdate
+from featurebyte.query_graph.node.schema import ColumnSpec
+from featurebyte.schema.deployment import DeploymentServiceUpdate
 from featurebyte.schema.feature import FeatureServiceUpdate
 from featurebyte.schema.feature_list import FeatureListServiceUpdate
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
 from featurebyte.service.context import ContextService
 from featurebyte.service.deployment import DeploymentService
-from featurebyte.service.entity_relationship_extractor import ServingEntityEnumeration
+from featurebyte.service.entity import EntityService
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_namespace import FeatureListNamespaceService
 from featurebyte.service.feature_list_status import FeatureListStatusService
 from featurebyte.service.feature_offline_store_info import OfflineStoreInfoInitializationService
+from featurebyte.service.offline_store_feature_table import OfflineStoreFeatureTableService
 from featurebyte.service.offline_store_feature_table_manager import (
     OfflineStoreFeatureTableManagerService,
 )
 from featurebyte.service.online_enable import OnlineEnableService
+from featurebyte.service.table import TableService
 from featurebyte.service.use_case import UseCaseService
 from featurebyte.worker.util.task_progress_updater import TaskProgressUpdater
 
 logger = get_logger(__name__)
 
 
 class DeployFeatureManagementService:
@@ -88,17 +100,37 @@
             )
         )
         await self.feature_service.update_offline_store_info(
             document_id=feature.id, store_info=store_info.dict(by_alias=True)
         )
         return await self.feature_service.get_document(document_id=feature.id)
 
-    async def _update_feature_online_enabled_status(
-        self, feature_id: ObjectId, feature_list_id: ObjectId, feature_list_to_deploy: bool
+    async def update_feature_online_enabled_status(
+        self,
+        feature_id: ObjectId,
+        feature_list_id: ObjectId,
+        feature_list_to_deploy: bool,
     ) -> FeatureModel:
+        """
+        Update feature online enabled status
+
+        Parameters
+        ----------
+        feature_id: ObjectId
+            Target feature ID
+        feature_list_id: ObjectId
+            Target feature list ID
+        feature_list_to_deploy: bool
+            Whether to deploy the feature list
+
+        Returns
+        -------
+        FeatureModel
+            Updated feature model
+        """
         document = await self.feature_service.get_document(document_id=feature_id)
         deployed_feature_list_ids: Set[ObjectId] = set(document.deployed_feature_list_ids)
         if feature_list_to_deploy:
             deployed_feature_list_ids.add(feature_list_id)
         else:
             deployed_feature_list_ids.discard(feature_list_id)
 
@@ -112,58 +144,74 @@
             document_id=feature_id,
             data=FeatureServiceUpdate(deployed_feature_list_ids=sorted(deployed_feature_list_ids)),
             document=document,
             return_document=False,
         )
         return await self.feature_service.get_document(document_id=feature_id)
 
+    async def update_offline_feature_table_deployment_reference(
+        self,
+        feature_ids: List[PydanticObjectId],
+        deployment_id: ObjectId,
+        to_enable: bool,
+    ) -> None:
+        """
+        Update offline feature table deployment reference
+
+        Parameters
+        ----------
+        feature_ids: List[PydanticObjectId]
+            Target feature IDs
+        deployment_id: ObjectId
+            Target deployment ID
+        to_enable: bool
+            Whether to enable the feature list
+        """
+        await self.offline_store_feature_table_manager_service.update_table_deployment_reference(
+            feature_ids=feature_ids,
+            deployment_id=deployment_id,
+            to_enable=to_enable,
+        )
+
     async def online_enable_feature(
         self,
         feature: FeatureModel,
-        feature_list: FeatureListModel,
         feast_registry: Optional[FeastRegistryModel],
     ) -> FeatureModel:
         """
         Enable feature online
 
         Parameters
         ----------
         feature: FeatureModel
             Target feature
-        feature_list: FeatureListModel
-            Target feature list
         feast_registry: Optional[FeastRegistryModel]
             Feast registry of current catalog
 
         Returns
         -------
         FeatureModel
         """
         # update feature mongo record
         if feast_registry:
             feature = await self._update_feature_offline_store_info(
                 feature=feature,
                 table_name_prefix=feast_registry.offline_table_name_prefix,
             )
 
-        updated_feature = await self._update_feature_online_enabled_status(
-            feature_id=feature.id, feature_list_id=feature_list.id, feature_list_to_deploy=True
-        )
+        if not feature.online_enabled:
+            # update data warehouse and backward-fill tiles if the feature is not already online
+            await self.online_enable_service.update_data_warehouse(
+                feature=feature, target_online_enabled=True
+            )
 
-        # update data warehouse and backward-fill tiles
-        await self.online_enable_service.update_data_warehouse(
-            updated_feature=updated_feature,
-            online_enabled_before_update=feature.online_enabled,
-        )
-        return updated_feature
+        return feature
 
     async def online_disable_feature(
-        self,
-        feature: FeatureModel,
-        feature_list: FeatureListModel,
+        self, feature: FeatureModel, feature_list: FeatureListModel
     ) -> FeatureModel:
         """
         Disable feature online
 
         Parameters
         ----------
         feature: FeatureModel
@@ -172,46 +220,148 @@
             Target feature list
 
         Returns
         -------
         FeatureModel
         """
         # update feature mongo record
-        updated_feature = await self._update_feature_online_enabled_status(
-            feature_id=feature.id, feature_list_id=feature_list.id, feature_list_to_deploy=False
+        updated_feature = await self.update_feature_online_enabled_status(
+            feature_id=feature.id,
+            feature_list_id=feature_list.id,
+            feature_list_to_deploy=False,
         )
 
-        # update data warehouse and backward-fill tiles
-        await self.online_enable_service.update_data_warehouse(
-            updated_feature=updated_feature,
-            online_enabled_before_update=feature.online_enabled,
-        )
+        if not updated_feature.online_enabled:
+            # update data warehouse and backward-fill tiles
+            await self.online_enable_service.update_data_warehouse(
+                feature=feature, target_online_enabled=False
+            )
+
         return updated_feature
 
 
+class DeploymentServingEntityService:
+    """DeploymentServingEntityService is responsible for determining the serving entity ids for a
+    deployment
+    """
+
+    def __init__(
+        self,
+        feature_list_service: FeatureListService,
+        entity_service: EntityService,
+        table_service: TableService,
+        use_case_service: UseCaseService,
+        context_service: ContextService,
+    ):
+        self.feature_list_service = feature_list_service
+        self.entity_service = entity_service
+        self.table_service = table_service
+        self.use_case_service = use_case_service
+        self.context_service = context_service
+
+    async def get_serving_entity_specs(
+        self, serving_entity_ids: List[PydanticObjectId]
+    ) -> List[ColumnSpec]:
+        """
+        Get serving entity name & dtype for the given serving entity ids
+
+        Parameters
+        ----------
+        serving_entity_ids: List[PydanticObjectId]
+            Serving entity ids
+
+        Returns
+        -------
+        List[ColumnSpec]
+        """
+        entity_id_to_entity = {}
+        entity_id_to_table_id = {}
+        async for entity in self.entity_service.list_documents_iterator(
+            query_filter={"_id": {"$in": serving_entity_ids}}
+        ):
+            entity_id_to_entity[entity.id] = entity
+            if entity.primary_table_ids:
+                entity_id_to_table_id[entity.id] = entity.primary_table_ids[0]
+            elif entity.table_ids:
+                entity_id_to_table_id[entity.id] = entity.table_ids[0]
+
+        table_id_to_table = {}
+        async for table in self.table_service.list_documents_iterator(
+            query_filter={"_id": {"$in": list(entity_id_to_table_id.values())}}
+        ):
+            table_id_to_table[table.id] = table
+
+        entity_specs = []
+        for entity_id in serving_entity_ids:
+            serving_name = entity_id_to_entity[entity_id].serving_names[0]
+            serving_dtype = DBVarType.VARCHAR
+            if entity_id in entity_id_to_table_id:
+                table = table_id_to_table[entity_id_to_table_id[entity_id]]
+                for column in table.columns_info:
+                    if column.entity_id == entity_id:
+                        serving_dtype = column.dtype
+                        break
+            entity_specs.append(ColumnSpec(name=serving_name, dtype=serving_dtype))
+        return entity_specs
+
+    async def get_deployment_serving_entity_ids(
+        self,
+        feature_list_id: ObjectId,
+        context_id: Optional[ObjectId],
+        use_case_id: Optional[ObjectId],
+    ) -> ServingEntity:
+        """
+        Get the serving entity ids for the deployment
+
+        Parameters
+        ----------
+        feature_list_id: ObjectId
+            Id of the feature list
+        context_id: Optional[ObjectId]
+            Id of the context associated with the deployment
+        use_case_id: Optional[ObjectId]
+            Id of the use case associated with the deployment
+
+        Returns
+        -------
+        ServingEntity
+        """
+        # Get context from use case if not directly available
+        feature_list_primary_entity_ids = (
+            await self.feature_list_service.get_document_as_dict(
+                feature_list_id, projection={"primary_entity_ids": 1}
+            )
+        )["primary_entity_ids"]
+        if context_id is None:
+            if use_case_id is None:
+                return feature_list_primary_entity_ids  # type: ignore[no-any-return]
+            use_case_model = await self.use_case_service.get_document(use_case_id)
+            context_id = use_case_model.context_id
+        context_model = await self.context_service.get_document(context_id)
+        return context_model.primary_entity_ids
+
+
 class DeployFeatureListManagementService:
     """
     DeployFeatureListManagementService class is responsible for feature list management during deployment.
     """
 
     def __init__(
         self,
         feature_list_service: FeatureListService,
         feature_list_namespace_service: FeatureListNamespaceService,
         feature_list_status_service: FeatureListStatusService,
         deployment_service: DeploymentService,
-        use_case_service: UseCaseService,
-        context_service: ContextService,
+        deployment_serving_entity_service: DeploymentServingEntityService,
     ):
         self.feature_list_service = feature_list_service
         self.feature_list_namespace_service = feature_list_namespace_service
         self.feature_list_status_service = feature_list_status_service
         self.deployment_service = deployment_service
-        self.use_case_service = use_case_service
-        self.context_service = context_service
+        self.deployment_serving_entity_service = deployment_serving_entity_service
 
     async def get_feature_list(self, feature_list_id: ObjectId) -> FeatureListModel:
         """
         Get feature list
 
         Parameters
         ----------
@@ -308,262 +458,267 @@
         async for doc in self.deployment_service.list_documents_as_dict_iterator(
             query_filter={"feature_list_id": feature_list_id, "enabled": True}
         ):
             yield doc
         if to_enable_deployment:
             yield await self.deployment_service.get_document_as_dict(deployment_id)
 
-    async def _get_enabled_serving_entity_ids(
-        self,
-        feature_list_model: FeatureListModel,
-        deployment_id: ObjectId,
-        to_enable_deployment: bool,
-    ) -> List[ServingEntity]:
-        # List of all possible serving entity ids for the feature list
-        supported_serving_entity_ids_set = {
-            tuple(serving_entity_ids)
-            for serving_entity_ids in feature_list_model.supported_serving_entity_ids
-        }
-
-        # List of enabled serving entity ids is a subset of supported_serving_entity_ids and is
-        # determined by existing deployments
-        enabled_serving_entity_ids = []
-
-        context_id_to_model = {}
-        use_case_id_to_model = {}
-        async for doc in self._iterate_enabled_deployments_as_dict(
-            feature_list_model.id, deployment_id, to_enable_deployment
-        ):
-            context_id = doc["context_id"]
-
-            # Get context from use case if not directly available
-            if context_id is None:
-                use_case_id = doc["use_case_id"]
-                if use_case_id is None:
-                    continue
-                use_case_model = await self.use_case_service.get_document(use_case_id)
-                use_case_id_to_model[use_case_id] = use_case_model
-                context_id = use_case_model.context_id
-
-            context_model = await self.context_service.get_document(context_id)
-            context_id_to_model[context_id] = context_model
-
-            serving_entity_enumeration = ServingEntityEnumeration.create(
-                feature_list_model.relationships_info or []
-            )
-            current_enabled = set()
-            for serving_entity_ids in supported_serving_entity_ids_set:
-                combined_entity_ids = list(serving_entity_ids) + list(
-                    context_model.primary_entity_ids
-                )
-                reduced_entity_ids = serving_entity_enumeration.reduce_entity_ids(
-                    combined_entity_ids
-                )
-                # Include if serving_entity_ids is the same or a parent of use case primary entity
-                if sorted(reduced_entity_ids) == sorted(context_model.primary_entity_ids):
-                    enabled_serving_entity_ids.append(serving_entity_ids)
-                    current_enabled.add(serving_entity_ids)
-
-            # Don't need to test again serving entity ids that were already enabled
-            supported_serving_entity_ids_set.difference_update(current_enabled)
-
-        return [list(serving_entity_ids) for serving_entity_ids in enabled_serving_entity_ids]
-
-    async def deploy_feature_list(
-        self, feature_list: FeatureListModel, deployment_id: ObjectId
-    ) -> None:
+    async def deploy_feature_list(self, feature_list: FeatureListModel) -> FeatureListModel:
         """
         Deploy feature list
 
         Parameters
         ----------
         feature_list: FeatureListModel
             Target feature list
-        deployment_id: ObjectId
-            Deployment ID
+
+        Returns
+        -------
+        FeatureListModel
         """
-        enabled_serving_entity_ids = await self._get_enabled_serving_entity_ids(
-            feature_list_model=feature_list,
-            deployment_id=deployment_id,
-            to_enable_deployment=True,
-        )
-        await self.feature_list_service.update_document(
+        updated_feature_list = await self.feature_list_service.update_document(
             document_id=feature_list.id,
             data=FeatureListServiceUpdate(
                 deployed=True,
-                enabled_serving_entity_ids=enabled_serving_entity_ids,
             ),
             document=feature_list,
-            return_document=False,
         )
 
-        if feature_list.deployed:
-            return
-
         await self._update_feature_list_namespace(
             feature_list_namespace_id=feature_list.feature_list_namespace_id,
             feature_list_id=feature_list.id,
             feature_list_to_deploy=True,
         )
+        assert updated_feature_list is not None
+        return updated_feature_list
 
-    async def undeploy_feature_list(
-        self, feature_list: FeatureListModel, deployment_id: ObjectId
-    ) -> None:
+    async def undeploy_feature_list(self, feature_list: FeatureListModel) -> FeatureListModel:
         """
         Undeploy feature list
 
         Parameters
         ----------
         feature_list: FeatureListModel
             Target feature list
-        deployment_id: ObjectId
-            Deployment ID
+
+        Returns
+        -------
+        FeatureListModel
         """
-        enabled_serving_entity_ids = await self._get_enabled_serving_entity_ids(
-            feature_list_model=feature_list,
-            deployment_id=deployment_id,
-            to_enable_deployment=False,
-        )
-        await self.feature_list_service.update_document(
+        updated_feature_list = await self.feature_list_service.update_document(
             document_id=feature_list.id,
             data=FeatureListServiceUpdate(
                 deployed=False,
-                enabled_serving_entity_ids=enabled_serving_entity_ids,
             ),
             document=feature_list,
-            return_document=False,
         )
+
         await self._update_feature_list_namespace(
             feature_list_namespace_id=feature_list.feature_list_namespace_id,
             feature_list_id=feature_list.id,
             feature_list_to_deploy=False,
         )
+        assert updated_feature_list is not None
+        return updated_feature_list
 
 
 class FeastIntegrationService:
     """
     FeastIntegrationService class is responsible for orchestrating Feast integration related operations.
     """
 
     def __init__(
         self,
         feast_registry_service: FeastRegistryService,
         feature_list_service: FeatureListService,
+        offline_store_feature_table_service: OfflineStoreFeatureTableService,
         offline_store_feature_table_manager_service: OfflineStoreFeatureTableManagerService,
+        deployment_service: DeploymentService,
+        deployment_serving_entity_service: DeploymentServingEntityService,
     ):
         self.feast_registry_service = feast_registry_service
         self.feature_list_service = feature_list_service
+        self.offline_store_feature_table_service = offline_store_feature_table_service
         self.offline_store_feature_table_manager_service = (
             offline_store_feature_table_manager_service
         )
+        self.deployment_service = deployment_service
+        self.deployment_serving_entity_service = deployment_serving_entity_service
 
-    async def get_or_create_feast_registry(self, catalog_id: ObjectId) -> FeastRegistryModel:
+    async def get_or_create_feast_registry(self, deployment: DeploymentModel) -> FeastRegistryModel:
         """
         Get existing or create new Feast registry
 
         Parameters
         ----------
-        catalog_id: ObjectId
-            Target catalog ID
+        deployment: DeploymentModel
+            Target deployment
 
         Returns
         -------
         FeastRegistryModel
         """
-        feast_registry = await self.feast_registry_service.get_or_create_feast_registry(
-            catalog_id=catalog_id,
-            feature_store_id=None,
+        feast_registry = await self.feast_registry_service.get_or_create_feast_registry(deployment)
+        await self.deployment_service.update_document(
+            document_id=deployment.id,
+            data=DeploymentServiceUpdate(
+                registry_info=FeastRegistryInfo(
+                    registry_id=feast_registry.id,
+                    registry_path=feast_registry.registry_path,
+                )
+            ),
+            return_document=False,
         )
         return feast_registry
 
-    async def get_feast_registry(self) -> Optional[FeastRegistryModel]:
+    async def get_feast_registry(self, deployment: DeploymentModel) -> Optional[FeastRegistryModel]:
         """
         Get Feast registry
 
+        Parameters
+        ----------
+        deployment: DeploymentModel
+            Target deployment
+
         Returns
         -------
         Optional[FeastRegistryModel]
         """
-        feature_registry = await self.feast_registry_service.get_feast_registry_for_catalog()
+        feature_registry = await self.feast_registry_service.get_feast_registry(
+            deployment=deployment
+        )
         return feature_registry
 
     async def handle_online_enabled_features(
         self,
         features: List[FeatureModel],
+        feature_list_to_online_enable: FeatureListModel,
+        deployment: DeploymentModel,
         update_progress: Callable[[int, Optional[str]], Coroutine[Any, Any, None]],
     ) -> None:
         """
         Handle online enabled features
 
         Parameters
         ----------
         features: List[FeatureModel]
             List of features to be enabled online
+        feature_list_to_online_enable: FeatureListModel
+            Target feature list (online enable)
+        deployment: DeploymentModel
+            Target deployment
         update_progress: Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
             Optional progress update callback
         """
         await self.offline_store_feature_table_manager_service.handle_online_enabled_features(
-            features=features, update_progress=update_progress
+            features=features,
+            feature_list_to_online_enable=feature_list_to_online_enable,
+            deployment=deployment,
+            update_progress=update_progress,
         )
 
     async def handle_online_disabled_features(
         self,
-        features: List[FeatureModel],
+        feature_list_to_online_disable: FeatureListModel,
+        deployment: DeploymentModel,
         update_progress: Callable[[int, Optional[str]], Coroutine[Any, Any, None]],
     ) -> None:
         """
         Handle online disabled features
 
         Parameters
         ----------
-        features: List[FeatureModel]
-            List of features to be disabled online
+        feature_list_to_online_disable: FeatureListModel
+            Target feature list not to deploy (online disable)
+        deployment: DeploymentModel
+            Target deployment
         update_progress: Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
             Optional progress update callback
         """
         await self.offline_store_feature_table_manager_service.handle_online_disabled_features(
-            features=features, update_progress=update_progress
+            feature_list_to_online_disable=feature_list_to_online_disable,
+            deployment=deployment,
+            update_progress=update_progress,
         )
 
     async def handle_deployed_feature_list(
         self,
+        deployment: DeploymentModel,
         feature_list: FeatureListModel,
         online_enabled_features: List[FeatureModel],
     ) -> None:
         """
         Handle deployed feature list
 
         Parameters
         ----------
+        deployment: DeploymentModel
+            Target deployment
         feature_list: FeatureListModel
             Target feature list
         online_enabled_features: List[FeatureModel]
             List of online enabled features
         """
+        serving_entity_specs: Optional[List[ColumnSpec]] = None
+        serving_names = set()
+        if deployment.serving_entity_ids:
+            serving_entity_specs = (
+                await self.deployment_serving_entity_service.get_serving_entity_specs(
+                    serving_entity_ids=deployment.serving_entity_ids
+                )
+            )
+            serving_names = set(
+                entity_serving_spec.name for entity_serving_spec in serving_entity_specs
+            )
+
+        feature_table_map = {}
+        feat_table_service = self.offline_store_feature_table_service
+        async for source_table in feat_table_service.list_source_feature_tables_for_deployment(
+            deployment_id=deployment.id
+        ):
+            if serving_names:
+                if set(source_table.serving_names).issubset(serving_names):
+                    feature_table_map[source_table.name] = source_table
+                else:
+                    async for (
+                        precomputed_table
+                    ) in feat_table_service.list_precomputed_lookup_feature_tables_from_source(
+                        source_feature_table_id=source_table.id
+                    ):
+                        if set(precomputed_table.serving_names).issubset(serving_names):
+                            feature_table_map[source_table.name] = precomputed_table
+            else:
+                feature_table_map[source_table.name] = source_table
+
         await self.feature_list_service.update_store_info(
-            document_id=feature_list.id, features=online_enabled_features
+            document_id=feature_list.id,
+            features=online_enabled_features,
+            feature_table_map=feature_table_map,
+            serving_entity_specs=serving_entity_specs,
         )
 
 
 class DeployService:
     """DeployService class is responsible for orchestrating deployment."""
 
     def __init__(
         self,
         deployment_service: DeploymentService,
         deploy_feature_list_management_service: DeployFeatureListManagementService,
         deploy_feature_management_service: DeployFeatureManagementService,
+        deployment_serving_entity_service: DeploymentServingEntityService,
         feast_integration_service: FeastIntegrationService,
         task_progress_updater: TaskProgressUpdater,
     ):
         self.deployment_service = deployment_service
         self.feature_list_management_service = deploy_feature_list_management_service
         self.feature_management_service = deploy_feature_management_service
         self.feast_integration_service = feast_integration_service
         self.task_progress_updater = task_progress_updater
+        self.deployment_serving_entity_service = deployment_serving_entity_service
 
     async def _update_progress(self, percent: int, message: Optional[str] = None) -> None:
         await self.task_progress_updater.update_progress(percent=percent, message=message)
 
     async def _validate_deployment_enablement(self, feature_list: FeatureListModel) -> None:
         await self.feature_list_management_service.validate_feature_list_status(
             feature_list_namespace_id=feature_list.feature_list_namespace_id
@@ -580,64 +735,84 @@
         await self._update_progress(percent=1, message="Validating deployment enablement...")
         await self._validate_deployment_enablement(feature_list)
 
         # create or get feast registry
         feast_registry = None
         if FeastIntegrationSettings().FEATUREBYTE_FEAST_INTEGRATION_ENABLED:
             feast_registry = await self.feast_integration_service.get_or_create_feast_registry(
-                catalog_id=deployment.catalog_id
+                deployment=deployment
             )
+            deployment = await self.deployment_service.get_document(document_id=deployment.id)
 
         # enable features online
         feature_update_progress = get_ranged_progress_callback(
             self.task_progress_updater.update_progress, 2, 70
         )
-        features = []
         features_offline_feature_table_to_update = []
-        for feature_id in feature_list.feature_ids:
+        for i, feature_id in enumerate(feature_list.feature_ids):
             feature = await self.feature_management_service.feature_service.get_document(
                 document_id=feature_id
             )
-            updated_feature = await self.feature_management_service.online_enable_feature(
+            feature = await self.feature_management_service.online_enable_feature(
                 feature=feature,
-                feature_list=feature_list,
                 feast_registry=feast_registry,
             )
-            if updated_feature.online_enabled != feature.online_enabled:
-                features_offline_feature_table_to_update.append(updated_feature)
-            features.append(updated_feature)
+            if not feature.online_enabled:
+                features_offline_feature_table_to_update.append(feature)
+
             await feature_update_progress(
-                int(len(features) / len(feature_list.feature_ids) * 100),
+                int(i / len(feature_list.feature_ids) * 100),
                 f"Enabling feature online ({feature.name}) ...",
             )
 
-        # deploy feature list
-        await self._update_progress(71, f"Deploying feature list ({feature_list.name}) ...")
-        await self.feature_list_management_service.deploy_feature_list(
-            feature_list=feature_list, deployment_id=deployment.id
-        )
-
         if feast_registry:
             await self.feast_integration_service.handle_online_enabled_features(
                 features=features_offline_feature_table_to_update,
+                feature_list_to_online_enable=feature_list,
+                deployment=deployment,
                 update_progress=get_ranged_progress_callback(
-                    self.task_progress_updater.update_progress, 72, 98
+                    self.task_progress_updater.update_progress, 72, 95
                 ),
             )
+
+        features = []
+        for feature_id in feature_list.feature_ids:
+            # update online enabled flag of the feature(s)
+            updated_feature = (
+                await self.feature_management_service.update_feature_online_enabled_status(
+                    feature_id=feature_id,
+                    feature_list_id=feature_list.id,
+                    feature_list_to_deploy=True,
+                )
+            )
+            features.append(updated_feature)
+
+        # update offline feature table deployment reference
+        await self.feature_management_service.update_offline_feature_table_deployment_reference(
+            feature_ids=feature_list.feature_ids,
+            deployment_id=deployment.id,
+            to_enable=True,
+        )
+
+        if feast_registry:
             await self._update_progress(
-                99, f"Updating deployed feature list ({feature_list.name}) ..."
+                96, f"Updating deployed feature list ({feature_list.name}) ..."
             )
             await self.feast_integration_service.handle_deployed_feature_list(
-                feature_list=feature_list, online_enabled_features=features
+                deployment=deployment, feature_list=feature_list, online_enabled_features=features
             )
 
+        # deploy feature list
+        await self._update_progress(97, f"Deploying feature list ({feature_list.name}) ...")
+        await self.feature_list_management_service.deploy_feature_list(feature_list=feature_list)
+
         # update deployment status
         await self.deployment_service.update_document(
             document_id=deployment.id,
-            data=DeploymentUpdate(enabled=True),
+            data=DeploymentServiceUpdate(enabled=True),
             return_document=False,
         )
         await self._update_progress(100, "Deployment enabled successfully!")
 
     async def _check_feature_list_used_in_other_deployment(
         self, feature_list_id: ObjectId, exclude_deployment_id: ObjectId
     ) -> bool:
@@ -675,14 +850,18 @@
             await self._enable_deployment(deployment, feature_list)
         except Exception as exc:
             exc_traceback = traceback.format_exc()
             logger.error(
                 f"Failed to enable deployment {deployment.id}: {exc}. {exc_traceback}",
                 exc_info=True,
             )
+            # Get the possibly updated deployment document
+            deployment = await self.deployment_service.get_document(
+                document_id=deployment.id, populate_remote_attributes=True
+            )
             await self.disable_deployment(deployment, feature_list)
             raise DocumentUpdateError("Failed to enable deployment") from exc
 
     async def disable_deployment(
         self,
         deployment: DeploymentModel,
         feature_list: FeatureListModel,
@@ -702,52 +881,57 @@
             feature_list_id=feature_list.id, exclude_deployment_id=deployment.id
         ):
             # disable features online
             feature_update_progress = get_ranged_progress_callback(
                 self.task_progress_updater.update_progress, 0, 70
             )
             features = []
-            features_offline_feature_table_to_update = []
             for feature_id in feature_list.feature_ids:
                 feature = await self.feature_management_service.feature_service.get_document(
                     document_id=feature_id
                 )
                 updated_feature = await self.feature_management_service.online_disable_feature(
                     feature=feature,
                     feature_list=feature_list,
                 )
-                if feature.online_enabled != updated_feature.online_enabled:
-                    features_offline_feature_table_to_update.append(updated_feature)
                 features.append(updated_feature)
                 await feature_update_progress(
                     int(len(features) / len(feature_list.feature_ids) * 100),
                     f"Disabling features online ({feature.name}) ...",
                 )
 
+            # update offline feature table deployment reference
+            await self.feature_management_service.update_offline_feature_table_deployment_reference(
+                feature_ids=feature_list.feature_ids,
+                deployment_id=deployment.id,
+                to_enable=False,
+            )
+
             # undeploy feature list if not used in other deployment
             await self._update_progress(71, f"Undeploying feature list ({feature_list.name}) ...")
-            await self.feature_list_management_service.undeploy_feature_list(
-                feature_list=feature_list, deployment_id=deployment.id
+            feature_list = await self.feature_list_management_service.undeploy_feature_list(
+                feature_list=feature_list,
             )
 
-            # check if feast integration is enabled for the catalog
-            if FeastIntegrationSettings().FEATUREBYTE_FEAST_INTEGRATION_ENABLED:
-                feast_registry = await self.feast_integration_service.get_feast_registry()
-                if feast_registry and features_offline_feature_table_to_update:
-                    await self.feast_integration_service.handle_online_disabled_features(
-                        features=features_offline_feature_table_to_update,
-                        update_progress=get_ranged_progress_callback(
-                            self.task_progress_updater.update_progress, 72, 99
-                        ),
-                    )
+        # check if feast integration is enabled for the catalog
+        if FeastIntegrationSettings().FEATUREBYTE_FEAST_INTEGRATION_ENABLED:
+            feast_registry = await self.feast_integration_service.get_feast_registry(deployment)
+            if feast_registry:
+                await self.feast_integration_service.handle_online_disabled_features(
+                    feature_list_to_online_disable=feature_list,
+                    deployment=deployment,
+                    update_progress=get_ranged_progress_callback(
+                        self.task_progress_updater.update_progress, 72, 99
+                    ),
+                )
 
         # update deployment status
         await self.deployment_service.update_document(
             document_id=deployment.id,
-            data=DeploymentUpdate(enabled=False),
+            data=DeploymentServiceUpdate(enabled=False),
             return_document=False,
         )
         await self._update_progress(100, "Deployment disabled successfully!")
 
     async def create_deployment(
         self,
         feature_list_id: ObjectId,
@@ -784,29 +968,38 @@
             feature_list_id=feature_list_id
         )
         default_deployment_name = (
             f"Deployment with {feature_list.name}_{feature_list.version.to_str()}"
         )
         deployment = None
         try:
+            serving_entity_ids = (
+                await self.deployment_serving_entity_service.get_deployment_serving_entity_ids(
+                    feature_list_id=feature_list_id,
+                    use_case_id=use_case_id,
+                    context_id=context_id,
+                )
+            )
             deployment = await self.deployment_service.create_document(
                 data=DeploymentModel(
-                    _id=deployment_id,
+                    _id=deployment_id or ObjectId(),
                     name=deployment_name or default_deployment_name,
                     feature_list_id=feature_list_id,
+                    feature_list_namespace_id=feature_list.feature_list_namespace_id,
                     enabled=False,
                     use_case_id=use_case_id,
                     context_id=context_id,
+                    serving_entity_ids=serving_entity_ids,
                 )
             )
             if to_enable_deployment:
                 await self.enable_deployment(deployment, feature_list)
         except Exception as exc:
             if deployment:
-                await self.deployment_service.delete_document(document_id=deployment_id)
+                await self.deployment_service.delete_document(document_id=deployment.id)
             raise DocumentCreationError("Failed to create deployment") from exc
 
     async def update_deployment(
         self,
         deployment_id: ObjectId,
         to_enable_deployment: bool,
     ) -> None:
```

### Comparing `featurebyte-1.0.2/featurebyte/service/deployment.py` & `featurebyte-1.0.3/featurebyte/service/deployment.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 """
 DeploymentService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.deployment import DeploymentModel
-from featurebyte.schema.deployment import DeploymentUpdate
+from featurebyte.schema.deployment import DeploymentServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
 
 
-class DeploymentService(BaseDocumentService[DeploymentModel, DeploymentModel, DeploymentUpdate]):
+class DeploymentService(
+    BaseDocumentService[DeploymentModel, DeploymentModel, DeploymentServiceUpdate]
+):
     """
     DeploymentService class
     """
 
     document_class = DeploymentModel
-    document_update_class = DeploymentUpdate
+    document_update_class = DeploymentServiceUpdate
 
 
 class AllDeploymentService(DeploymentService):
     """
     AllDeploymentService class
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/service/dimension_table.py` & `featurebyte-1.0.3/featurebyte/service/dimension_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 DimensionTableService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.dimension_table import DimensionTableModel
 from featurebyte.schema.dimension_table import DimensionTableCreate, DimensionTableServiceUpdate
 from featurebyte.service.base_table_document import BaseTableDocumentService
```

### Comparing `featurebyte-1.0.2/featurebyte/service/entity.py` & `featurebyte-1.0.3/featurebyte/service/entity.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 EntityService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/entity_lookup_feature_table.py` & `featurebyte-1.0.3/featurebyte/service/entity_lookup_feature_table.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 """
 EntityLookupFeatureTableService class
 """
+
 from __future__ import annotations
 
 from typing import Dict, List, Optional
 
 from featurebyte.models.base import PydanticObjectId
-from featurebyte.models.entity_lookup_feature_table import get_entity_lookup_feature_tables
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.offline_store_feature_table import OfflineStoreFeatureTableModel
 from featurebyte.models.parent_serving import EntityLookupStep
+from featurebyte.models.precomputed_lookup_feature_table import get_precomputed_lookup_feature_table
 from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.parent_serving import ParentEntityLookupService
 
 
 class EntityLookupFeatureTableService:
@@ -81,32 +82,61 @@
                 for entity_lookup_info in feature_list.features_entity_lookup_info:
                     all_relationships_info.update(entity_lookup_info.join_steps)
         entity_lookup_steps_mapping = await self.get_entity_lookup_steps(
             list(all_relationships_info)
         )
         return entity_lookup_steps_mapping
 
-    async def get_entity_lookup_feature_tables(
+    async def get_precomputed_lookup_feature_table(
         self,
-        feature_lists: List[FeatureListModel],
+        primary_entity_ids: List[PydanticObjectId],
+        feature_ids: List[PydanticObjectId],
+        feature_list: FeatureListModel,
+        full_serving_entity_ids: List[PydanticObjectId],
+        feature_table_name: str,
+        feature_table_has_ttl: bool,
+        entity_id_to_serving_name: Dict[PydanticObjectId, str],
         feature_store_model: FeatureStoreModel,
-    ) -> Optional[List[OfflineStoreFeatureTableModel]]:
+        feature_table_id: Optional[PydanticObjectId],
+    ) -> Optional[OfflineStoreFeatureTableModel]:
         """
-        Get list of internal offline store feature tables for parent entity lookup purpose
+        Construct a precomputed lookup feature table for a given source feature table in order to
+        support a specific deployment with a predetermined serving entity ids
 
         Parameters
         ----------
-        feature_lists: List[FeatureListModel]
-            Currently online enabled feature lists
+        primary_entity_ids: List[PydanticObjectId]
+            Primary entity ids of the source feature table
+        feature_ids: List[PydanticObjectId]
+            List of features that references the source feature table
+        feature_list: FeatureListModel
+            Feature list associated with the deployment
+        full_serving_entity_ids: List[PydanticObjectId]
+            Serving entity ids of the deployment
+        feature_table_name: str
+            Name of the source feature table
+        feature_table_has_ttl: bool
+            Whether the source feature table has ttl
+        entity_id_to_serving_name: Dict[PydanticObjectId, str]
+            Mapping from entity id to serving name
         feature_store_model: FeatureStoreModel
-            Feature store document
+            Feature store
+        feature_table_id: PydanticObjectId
+            Id of the source feature table
 
         Returns
         -------
-        Optional[List[OfflineStoreFeatureTableModel]]
+        List[OfflineStoreFeatureTableModel]
         """
-        entity_lookup_steps_mapping = await self.get_entity_lookup_steps_mapping(feature_lists)
-        return get_entity_lookup_feature_tables(
-            feature_lists=feature_lists,
-            feature_store=feature_store_model,
+        entity_lookup_steps_mapping = await self.get_entity_lookup_steps_mapping([feature_list])
+        return get_precomputed_lookup_feature_table(
+            primary_entity_ids=primary_entity_ids,
+            feature_ids=feature_ids,
+            feature_list=feature_list,
+            full_serving_entity_ids=full_serving_entity_ids,
+            feature_table_name=feature_table_name,
+            feature_table_has_ttl=feature_table_has_ttl,
+            entity_id_to_serving_name=entity_id_to_serving_name,
             entity_lookup_steps_mapping=entity_lookup_steps_mapping,
+            feature_table_id=feature_table_id,
+            feature_store_model=feature_store_model,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/entity_relationship_extractor.py` & `featurebyte-1.0.3/featurebyte/service/entity_relationship_extractor.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 Entity Relationship Extractor Service
 """
+
 from typing import Dict, List, Optional, Sequence
 
 import itertools
 from collections import defaultdict
 from dataclasses import dataclass
 
 from bson import ObjectId
 
+from featurebyte.models.entity import EntityModel
 from featurebyte.query_graph.model.entity_relationship_info import (
     EntityAncestorDescendantMapper,
     EntityRelationshipInfo,
 )
 from featurebyte.routes.common.derive_primary_entity_helper import DerivePrimaryEntityHelper
 from featurebyte.service.entity import EntityService
 from featurebyte.service.relationship_info import RelationshipInfoService
@@ -241,14 +243,37 @@
         output = []
         async for relationship_info in self.relationship_info_service.list_documents_iterator(
             query_filter={"_id": {"$in": relationship_ids}},
         ):
             output.append(EntityRelationshipInfo(**relationship_info.dict(by_alias=True)))
         return output
 
+    async def get_entity_id_to_entity(
+        self, entity_ids: List[ObjectId]
+    ) -> Dict[ObjectId, EntityModel]:
+        """
+        Construct entity ID to entity dictionary mapping
+
+        Parameters
+        ----------
+        entity_ids: List[ObjectId]
+            List of entity IDs
+
+        Returns
+        -------
+        Dict[ObjectId, EntityModel]
+            Dictionary mapping entity ID to entity model
+        """
+        entity_id_to_entity: Dict[ObjectId, EntityModel] = {}
+        async for entity in self.entity_service.list_documents_iterator(
+            query_filter={"_id": {"$in": entity_ids}}
+        ):
+            entity_id_to_entity[entity.id] = entity
+        return entity_id_to_entity
+
     async def extract_relationship_from_primary_entity(
         self,
         entity_ids: List[ObjectId],
         primary_entity_ids: Optional[List[ObjectId]] = None,
     ) -> List[EntityRelationshipInfo]:
         """
         Extract relationships between entity ids and primary entity ids. This method is used to
```

### Comparing `featurebyte-1.0.2/featurebyte/service/entity_serving_names.py` & `featurebyte-1.0.3/featurebyte/service/entity_serving_names.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Entity serving names service
 """
+
 from typing import Any, Dict, List, Optional, Sequence
 
 from bson import ObjectId
 
 from featurebyte.models.feature_store import FeatureStoreModel, TableModel
 from featurebyte.models.relationship_analysis import derive_primary_entity
 from featurebyte.query_graph.sql.interpreter import GraphInterpreter
@@ -70,24 +71,24 @@
         )
         result = await db_session.execute_query(unique_values_sql)
         assert result is not None
         unique_values: List[Any] = result[column_name].to_list()
         return unique_values
 
     async def get_sample_entity_serving_names(  # pylint: disable=too-many-locals
-        self, entity_ids: Sequence[ObjectId], table_ids: Sequence[ObjectId], count: int
+        self, entity_ids: Sequence[ObjectId], table_ids: Optional[Sequence[ObjectId]], count: int
     ) -> List[Dict[str, str]]:
         """
         Get sample entity serving names for a list of entities and tables
 
         Parameters
         ----------
         entity_ids: Sequence[ObjectId]
             List of entity ids
-        table_ids: Sequence[ObjectId]
+        table_ids: Optional[Sequence[ObjectId]]
             List of table ids
         count: int
             Number of sample entity serving names to return
 
         Returns
         -------
         List[Dict[str, str]]
@@ -99,14 +100,22 @@
                     query_filter={"_id": {"$in": list(entity_ids)}}
                 )
             ]
         )
         entities: Dict[ObjectId, Dict[str, List[str]]] = {
             entity.id: {"serving_name": entity.serving_names} for entity in primary_entity
         }
+        if table_ids is None:
+            table_ids = []
+            for entity in primary_entity:
+                if entity.primary_table_ids:
+                    table_ids.append(entity.primary_table_ids[0])
+                else:
+                    table_ids.append(entity.table_ids[0])
+
         tables = self.table_service.list_documents_iterator(
             query_filter={"_id": {"$in": table_ids}}
         )
 
         feature_store: Optional[FeatureStoreModel] = None
         async for table in tables:
             assert isinstance(table, TableModel)
@@ -126,21 +135,21 @@
                 for column in entity_columns:
                     # skip if sample values already exists unless column is a primary key for the table
                     assert column.entity_id is not None, column
                     existing_sample_values = entities[column.entity_id].get("sample_value")
                     if existing_sample_values and column.name not in table.primary_key_columns:
                         continue
 
-                    entities[column.entity_id][
-                        "sample_value"
-                    ] = await self.get_table_column_unique_values(
-                        feature_store=feature_store,
-                        table=table,
-                        column_name=column.name,
-                        num_rows=count,
+                    entities[column.entity_id]["sample_value"] = (
+                        await self.get_table_column_unique_values(
+                            feature_store=feature_store,
+                            table=table,
+                            column_name=column.name,
+                            num_rows=count,
+                        )
                     )
 
         return [
             {
                 entity["serving_name"][0]: entity["sample_value"][
                     row_idx % len(entity["sample_value"])
                 ]
```

### Comparing `featurebyte-1.0.2/featurebyte/service/entity_validation.py` & `featurebyte-1.0.3/featurebyte/service/preview.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,224 +1,300 @@
 """
-Module to support serving using parent-child relationship
+PreviewService class
 """
+
 from __future__ import annotations
 
-from typing import Optional, Tuple
+from typing import Any, Optional, Tuple
+
+import pandas as pd
+from bson import ObjectId
 
-from featurebyte.exception import RequiredEntityNotProvidedError, UnexpectedServingNamesMappingError
-from featurebyte.models.entity_validation import EntityInfo
-from featurebyte.models.feature_list import FeatureListModel
+from featurebyte.common.utils import dataframe_to_json, timer
+from featurebyte.logging import get_logger
 from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.parent_serving import EntityRelationshipsContext, ParentServingPreparation
-from featurebyte.query_graph.model.graph import QueryGraphModel
-from featurebyte.query_graph.node import Node
-from featurebyte.query_graph.node.schema import FeatureStoreDetails
-from featurebyte.query_graph.sql.feature_compute import FeatureExecutionPlanner
-from featurebyte.service.entity import EntityService
-from featurebyte.service.parent_serving import ParentEntityLookupService
+from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.sql.interpreter import GraphInterpreter
+from featurebyte.schema.feature_store import (
+    FeatureStorePreview,
+    FeatureStoreSample,
+    FeatureStoreShape,
+)
+from featurebyte.service.feature_store import FeatureStoreService
+from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.session.base import INTERACTIVE_SESSION_TIMEOUT_SECONDS, BaseSession
+
+DEFAULT_COLUMNS_BATCH_SIZE = 50
+
 
+logger = get_logger(__name__)
 
-class EntityValidationService:
+
+class PreviewService:
     """
-    EntityValidationService class is responsible for validating that required entities are
-    provided in feature requests during preview, historical features, and online serving.
+    PreviewService class
     """
 
     def __init__(
         self,
-        entity_service: EntityService,
-        parent_entity_lookup_service: ParentEntityLookupService,
+        session_manager_service: SessionManagerService,
+        feature_store_service: FeatureStoreService,
     ):
-        self.entity_service = entity_service
-        self.parent_entity_lookup_service = parent_entity_lookup_service
+        self.feature_store_service = feature_store_service
+        self.session_manager_service = session_manager_service
 
-    async def get_entity_info_from_request(
-        self,
-        graph_nodes: Optional[Tuple[QueryGraphModel, list[Node]]],
-        feature_list_model: Optional[FeatureListModel],
-        request_column_names: set[str],
-        serving_names_mapping: dict[str, str] | None = None,
-    ) -> EntityInfo:
+    async def _get_feature_store_session(
+        self, graph: QueryGraph, node_name: str, feature_store_id: Optional[ObjectId]
+    ) -> Tuple[FeatureStoreModel, BaseSession]:
         """
-        Create an EntityInfo instance given graph and request
+        Get feature store and session from a graph
 
         Parameters
         ----------
-        graph_nodes : Optional[Tuple[QueryGraphModel, list[Node]]
-            Query graph and nodes
-        feature_list_model: Optional[FeatureListModel]
-            Feature list model
-        request_column_names: set[str]
-            Column names provided in the request
-        serving_names_mapping : dict[str, str] | None
-            Optional serving names mapping if the entities are provided under different serving
-            names in the request
+        graph: QueryGraph
+            Query graph to use
+        node_name: str
+            Name of node to use
+        feature_store_id: Optional[ObjectId]
+            Feature store id to use
 
         Returns
         -------
-        EntityInfo
+        Tuple[FeatureStoreModel, BaseSession]
         """
-
-        # serving_names_mapping: original serving names to overridden serving name
-        # inv_serving_names_mapping: overridden serving name to original serving name
-        if serving_names_mapping is not None:
-            inv_serving_names_mapping = {v: k for (k, v) in serving_names_mapping.items()}
+        # get feature store
+        if feature_store_id:
+            feature_store = await self.feature_store_service.get_document(
+                document_id=feature_store_id
+            )
+            assert feature_store
         else:
-            inv_serving_names_mapping = None
+            feature_store_dict = graph.get_input_node(
+                node_name
+            ).parameters.feature_store_details.dict()
+            feature_stores = self.feature_store_service.list_documents_iterator(
+                query_filter={
+                    "type": feature_store_dict["type"],
+                    "details": feature_store_dict["details"],
+                }
+            )
+            feature_store = (
+                await feature_stores.__anext__()  # pylint: disable=unnecessary-dunder-call
+            )
+            assert feature_store
 
-        def _get_original_serving_name(serving_name: str) -> str:
-            if inv_serving_names_mapping is not None and serving_name in inv_serving_names_mapping:
-                return inv_serving_names_mapping[serving_name]
-            return serving_name
-
-        # Infer provided entities from provided request columns
-        candidate_serving_names = {_get_original_serving_name(col) for col in request_column_names}
-        provided_entities = await self.entity_service.get_entities_with_serving_names(
-            candidate_serving_names
-        )
-
-        # Extract required entities from feature list (faster) or graph
-        if feature_list_model is not None:
-            if self._to_use_frozen_relationships(feature_list_model):
-                required_entity_ids = feature_list_model.primary_entity_ids
-            else:
-                required_entity_ids = feature_list_model.entity_ids
-            required_entities = await self.entity_service.get_entities(set(required_entity_ids))
-        else:
-            assert graph_nodes is not None
-            graph, nodes = graph_nodes
-            planner = FeatureExecutionPlanner(
-                graph=graph,
-                is_online_serving=False,
-                serving_names_mapping=serving_names_mapping,
+        session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
+        )
+        return feature_store, session
+
+    async def shape(self, preview: FeatureStorePreview) -> FeatureStoreShape:
+        """
+        Get the shape of a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+
+        Parameters
+        ----------
+        preview: FeatureStorePreview
+            FeatureStorePreview object
+
+        Returns
+        -------
+        FeatureStoreShape
+            Row and column counts
+        """
+        with timer("PreviewService.shape: Get feature store and session", logger):
+            feature_store, session = await self._get_feature_store_session(
+                graph=preview.graph,
+                node_name=preview.node_name,
+                feature_store_id=preview.feature_store_id,
             )
-            plan = planner.generate_plan(nodes)
-            required_entities = await self.entity_service.get_entities(plan.required_entity_ids)
 
-        return EntityInfo(
-            provided_entities=provided_entities,
-            required_entities=required_entities,
-            serving_names_mapping=serving_names_mapping,
+        node_num, edge_num = len(preview.graph.nodes), len(preview.graph.edges)
+        with timer(
+            "PreviewService.shape: Construct shape SQL",
+            logger,
+            extra={"node_num": node_num, "edge_num": edge_num},
+        ):
+            shape_sql, num_cols = GraphInterpreter(
+                preview.graph, source_type=feature_store.type
+            ).construct_shape_sql(node_name=preview.node_name)
+
+        with timer(
+            "PreviewService.shape: Execute shape SQL", logger, extra={"shape_sql": shape_sql}
+        ):
+            result = await session.execute_query(shape_sql)
+
+        assert result is not None
+        return FeatureStoreShape(
+            num_rows=result["count"].iloc[0],
+            num_cols=num_cols,
         )
 
-    async def validate_entities_or_prepare_for_parent_serving(
-        self,
-        request_column_names: set[str],
-        feature_store: FeatureStoreModel,
-        graph_nodes: Optional[Tuple[QueryGraphModel, list[Node]]] = None,
-        feature_list_model: Optional[FeatureListModel] = None,
-        serving_names_mapping: dict[str, str] | None = None,
-    ) -> ParentServingPreparation:
-        """
-        Validate that entities are provided correctly in feature requests
+    async def preview(self, preview: FeatureStorePreview, limit: int) -> dict[str, Any]:
+        """
+        Preview a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
 
         Parameters
         ----------
-        graph_nodes : Optional[Tuple[QueryGraphModel, list[Node]]]
-            Query graph and nodes
-        feature_list_model: Optional[FeatureListModel]
-            Feature list model
-        request_column_names: set[str]
-            Column names provided in the request
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object to be used to extract FeatureStoreDetails
-        serving_names_mapping : dict[str, str] | None
-            Optional serving names mapping if the entities are provided under different serving
-            names in the request
+        preview: FeatureStorePreview
+            FeatureStorePreview object
+        limit: int
+            Row limit on preview results
 
         Returns
         -------
-        ParentServingPreparation
-
-        Raises
-        ------
-        RequiredEntityNotProvidedError
-            When any of the required entities is not provided in the request and it is not possible
-            to retrieve the parent entities using existing relationships
-        UnexpectedServingNamesMappingError
-            When unexpected keys are provided in serving_names_mapping
-        """
-        assert graph_nodes is not None or feature_list_model is not None
-
-        entity_info = await self.get_entity_info_from_request(
-            graph_nodes=graph_nodes,
-            feature_list_model=feature_list_model,
-            request_column_names=request_column_names,
-            serving_names_mapping=serving_names_mapping,
-        )
-
-        if serving_names_mapping is not None:
-            provided_serving_names = {
-                entity.serving_names[0] for entity in entity_info.provided_entities
-            }
-            unexpected_keys = {
-                k for k in serving_names_mapping.keys() if k not in provided_serving_names
-            }
-            if unexpected_keys:
-                unexpected_keys_str = ", ".join(sorted(unexpected_keys))
-                raise UnexpectedServingNamesMappingError(
-                    f"Unexpected serving names provided in serving_names_mapping: {unexpected_keys_str}"
-                )
+        dict[str, Any]
+            Dataframe converted to json string
+        """
+        feature_store, session = await self._get_feature_store_session(
+            graph=preview.graph,
+            node_name=preview.node_name,
+            feature_store_id=preview.feature_store_id,
+        )
+        preview_sql, type_conversions = GraphInterpreter(
+            preview.graph, source_type=feature_store.type
+        ).construct_preview_sql(node_name=preview.node_name, num_rows=limit)
+        result = await session.execute_query(preview_sql)
+        return dataframe_to_json(result, type_conversions)
 
-        if entity_info.are_all_required_entities_provided():
-            join_steps = []
-        else:
-            # Try to see if missing entities can be obtained using the provided entities as children
-            try:
-                join_steps = await self.parent_entity_lookup_service.get_required_join_steps(
-                    entity_info
-                )
-            except RequiredEntityNotProvidedError:
-                raise RequiredEntityNotProvidedError(  # pylint: disable=raise-missing-from
-                    entity_info.format_missing_entities_error(
-                        [entity.id for entity in entity_info.missing_entities]
-                    )
-                )
-
-        feature_store_details = FeatureStoreDetails(**feature_store.dict())
-        entity_relationships_context = await self._get_entity_relationships_context(
-            entity_info,
-            feature_list_model,
-        )
-        return ParentServingPreparation(
-            join_steps=join_steps,
-            feature_store_details=feature_store_details,
-            entity_relationships_context=entity_relationships_context,
-        )
-
-    @staticmethod
-    def _to_use_frozen_relationships(feature_list_model: FeatureListModel) -> bool:
-        return (
-            feature_list_model.feature_clusters is not None
-            and feature_list_model.feature_clusters[0].feature_node_relationships_infos is not None
+    async def sample(self, sample: FeatureStoreSample, size: int, seed: int) -> dict[str, Any]:
+        """
+        Sample a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+
+        Parameters
+        ----------
+        sample: FeatureStoreSample
+            FeatureStoreSample object
+        size: int
+            Maximum rows to sample
+        seed: int
+            Random seed to use for sampling
+
+        Returns
+        -------
+        dict[str, Any]
+            Dataframe converted to json string
+        """
+        feature_store, session = await self._get_feature_store_session(
+            graph=sample.graph,
+            node_name=sample.node_name,
+            feature_store_id=sample.feature_store_id,
+        )
+        sample_sql, type_conversions = GraphInterpreter(
+            sample.graph, source_type=feature_store.type
+        ).construct_sample_sql(
+            node_name=sample.node_name,
+            num_rows=size,
+            seed=seed,
+            from_timestamp=sample.from_timestamp,
+            to_timestamp=sample.to_timestamp,
+            timestamp_column=sample.timestamp_column,
         )
+        result = await session.execute_query(sample_sql)
+        return dataframe_to_json(result, type_conversions)
 
-    async def _get_entity_relationships_context(
+    async def describe(
         self,
-        entity_info: EntityInfo,
-        feature_list_model: Optional[FeatureListModel],
-    ) -> Optional[EntityRelationshipsContext]:
-        if feature_list_model is None or feature_list_model.feature_clusters is None:
-            return None
-
-        feature_cluster = feature_list_model.feature_clusters[0]
-        if feature_cluster.feature_node_relationships_infos is None:
-            return None
-
-        all_relationships = set(feature_list_model.relationships_info or [])
-        all_relationships.update(feature_list_model.feature_clusters[0].combined_relationships_info)
-        entity_lookup_step_creator = (
-            await self.parent_entity_lookup_service.get_entity_lookup_step_creator(
-                list(all_relationships)
+        sample: FeatureStoreSample,
+        size: int,
+        seed: int,
+        columns_batch_size: Optional[int] = None,
+    ) -> dict[str, Any]:
+        """
+        Sample a QueryObject that is not a Feature (e.g. SourceTable, EventTable, EventView, etc)
+
+        Parameters
+        ----------
+        sample: FeatureStoreSample
+            FeatureStoreSample object
+        size: int
+            Maximum rows to sample
+        seed: int
+            Random seed to use for sampling
+        columns_batch_size: Optional[int]
+            Maximum number of columns to describe in a single query. More columns in the data will
+            be described in multiple queries. If None, a default value will be used. If 0, batching
+            will be disabled.
+
+        Returns
+        -------
+        dict[str, Any]
+            Dataframe converted to json string
+        """
+        if columns_batch_size is None:
+            columns_batch_size = DEFAULT_COLUMNS_BATCH_SIZE
+
+        feature_store, session = await self._get_feature_store_session(
+            graph=sample.graph,
+            node_name=sample.node_name,
+            feature_store_id=sample.feature_store_id,
+        )
+
+        describe_queries = GraphInterpreter(
+            sample.graph, source_type=feature_store.type
+        ).construct_describe_queries(
+            node_name=sample.node_name,
+            num_rows=size,
+            seed=seed,
+            from_timestamp=sample.from_timestamp,
+            to_timestamp=sample.to_timestamp,
+            timestamp_column=sample.timestamp_column,
+            stats_names=sample.stats_names,
+            columns_batch_size=columns_batch_size,
+        )
+        df_queries = []
+        for describe_query in describe_queries.queries:
+            logger.debug("Execute describe SQL", extra={"describe_sql": describe_query.sql})
+            result = await session.execute_query_long_running(describe_query.sql)
+            columns = describe_query.columns
+            assert result is not None
+            df_query = pd.DataFrame(
+                result.values.reshape(len(columns), -1).T,
+                index=describe_query.row_names,
+                columns=[str(column.name) for column in columns],
             )
+            df_queries.append(df_query)
+        results = pd.concat(df_queries, axis=1).dropna(axis=0, how="all")
+        return dataframe_to_json(results, describe_queries.type_conversions, skip_prepare=True)
+
+    async def value_counts(
+        self,
+        preview: FeatureStorePreview,
+        num_rows: int,
+        num_categories_limit: int,
+        seed: int = 1234,
+    ) -> dict[str, int]:
+        """
+        Get value counts for a column
+
+        Parameters
+        ----------
+        preview: FeatureStorePreview
+            FeatureStorePreview object
+        num_rows : int
+            Number of rows to include when calculating the counts
+        num_categories_limit : int
+            Maximum number of categories to include in the result. If there are more categories in
+            the data, the result will include the most frequent categories up to this number.
+        seed: int
+            Random seed to use for sampling
+
+        Returns
+        -------
+        dict[str, int]
+        """
+        feature_store, session = await self._get_feature_store_session(
+            graph=preview.graph,
+            node_name=preview.node_name,
+            feature_store_id=preview.feature_store_id,
         )
-        return EntityRelationshipsContext(
-            feature_list_primary_entity_ids=feature_list_model.primary_entity_ids,
-            feature_list_serving_names=[
-                entity_info.get_effective_serving_name(entity_info.get_entity(entity_id))
-                for entity_id in feature_list_model.primary_entity_ids
-            ],
-            feature_list_relationships_info=feature_list_model.relationships_info,
-            feature_node_relationships_infos=feature_cluster.feature_node_relationships_infos,
-            entity_lookup_step_creator=entity_lookup_step_creator,
+        value_counts_sql = GraphInterpreter(
+            preview.graph, source_type=feature_store.type
+        ).construct_value_counts_sql(
+            node_name=preview.node_name,
+            num_rows=num_rows,
+            num_categories_limit=num_categories_limit,
+            seed=seed,
         )
+        df_result = await session.execute_query(value_counts_sql)
+        assert df_result.columns.tolist() == ["key", "count"]  # type: ignore
+        return df_result.set_index("key")["count"].to_dict()  # type: ignore
```

### Comparing `featurebyte-1.0.2/featurebyte/service/event_table.py` & `featurebyte-1.0.3/featurebyte/service/event_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 EventTableService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.event_table import EventTableModel
 from featurebyte.schema.event_table import EventTableCreate, EventTableServiceUpdate
 from featurebyte.service.base_table_document import BaseTableDocumentService
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature.py` & `featurebyte-1.0.3/featurebyte/service/feature.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,25 +1,30 @@
 """
 FeatureService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from datetime import datetime
 
 from bson import ObjectId
 from redis import Redis
 
 from featurebyte.exception import DocumentCreationError, DocumentNotFoundError
 from featurebyte.models.base import VersionIdentifier
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_namespace import DefaultVersionMode, FeatureReadiness
 from featurebyte.persistent import Persistent
+from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.transform.offline_store_ingest import (
+    OfflineStoreIngestQueryGraphTransformer,
+)
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.routes.common.derive_primary_entity_helper import DerivePrimaryEntityHelper
 from featurebyte.schema.feature import FeatureServiceCreate
 from featurebyte.schema.feature_namespace import (
     FeatureNamespaceCreate,
     FeatureNamespaceServiceUpdate,
 )
@@ -124,14 +129,30 @@
         }
         if sanitize_for_definition:
             # since the feature model is created for definition, actual aggregation attributes are not
             # required. Add aggregation attributes to avoid triggering unnecessary aggregation attributes
             # derivation.
             feature_dict["aggregation_ids"] = ["dummy_aggregation_id"]
             feature_dict["aggregation_result_names"] = ["dummy_aggregation_result_name"]
+
+        feature = FeatureModel(**feature_dict)
+        if sanitize_for_definition:
+            return feature
+
+        # derive entity join steps
+        store_info_service = self.offline_store_info_initialization_service
+        entity_id_to_serving_name = {
+            entity_id: entity.serving_names[0]
+            for entity_id, entity in derived_data.entity_id_to_entity.items()
+        }
+        feature_dict["entity_join_steps"] = (
+            await store_info_service.get_entity_join_steps_for_feature_table(
+                feature=feature, entity_id_to_serving_name=entity_id_to_serving_name
+            )
+        )
         return FeatureModel(**feature_dict)
 
     @staticmethod
     def validate_feature(feature: FeatureModel) -> None:
         """
         Validate feature model before saving
 
@@ -141,15 +162,15 @@
             Feature model to validate
 
         Raises
         ------
         DocumentCreationError
             If the feature's feature job settings are not consistent
         """
-        # validate feature model
+        # validate feature job settings are consistent
         table_id_feature_job_settings = feature.extract_table_id_feature_job_settings()
         table_id_to_feature_job_setting = {}
         for table_id_feature_job_setting in table_id_feature_job_settings:
             table_id = table_id_feature_job_setting.table_id
             feature_job_setting = table_id_feature_job_setting.feature_job_setting
             if table_id not in table_id_to_feature_job_setting:
                 table_id_to_feature_job_setting[table_id] = feature_job_setting
@@ -160,14 +181,38 @@
                 ):
                     raise DocumentCreationError(
                         f"Feature job settings for table {table_id} are not consistent. "
                         f"Two different feature job settings are found: "
                         f"{table_id_to_feature_job_setting[table_id]} and {feature_job_setting}"
                     )
 
+        # validate feature with UDFs
+        if feature.used_user_defined_function:
+            transformer = OfflineStoreIngestQueryGraphTransformer(graph=feature.graph)
+            assert feature.name is not None
+            result = transformer.transform(
+                target_node=feature.node,
+                relationships_info=feature.relationships_info or [],
+                feature_name=feature.name,
+                feature_version=feature.version.to_str(),
+            )
+            if result.is_decomposed:
+                # if the graph is decomposed, it implies that on-demand-function is used when the
+                # feature is online-enabled. Check whether the UDF is used in the on-demand function.
+                decom_graph = result.graph
+                decom_node = result.graph.get_node_by_name(result.node_name_map[feature.node.name])
+                if decom_graph.has_node_type(
+                    target_node=decom_node, node_type=NodeType.GENERIC_FUNCTION
+                ):
+                    raise DocumentCreationError(
+                        "This feature requires a Python on-demand function during deployment. "
+                        "We cannot proceed with creating the feature because the on-demand function involves a UDF, "
+                        "and the Python version of the UDF is not supported at the moment."
+                    )
+
     async def create_document(self, data: FeatureServiceCreate) -> FeatureModel:
         document = await self.prepare_feature_model(data=data, sanitize_for_definition=False)
         self.validate_feature(feature=document)
 
         async with self.persistent.start_transaction() as session:
             # check any conflict with existing documents
             await self._check_document_unique_constraints(document=document)
@@ -351,7 +396,23 @@
 
         # get entities and tables used for the feature list
         return await self.entity_serving_names_service.get_sample_entity_serving_names(
             entity_ids=feature.entity_ids,
             table_ids=feature.table_ids,
             count=count,
         )
+
+    async def get_online_disabled_feature_ids(self) -> List[ObjectId]:
+        """
+        Get the ids of the features that are online disabled
+
+        Returns
+        -------
+        List[ObjectId]
+        """
+        disabled_feature_ids = []
+        async for doc in self.list_documents_as_dict_iterator(
+            query_filter={"online_enabled": False},
+            projection={"_id": 1},
+        ):
+            disabled_feature_ids.append(doc["_id"])
+        return disabled_feature_ids
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_facade.py` & `featurebyte-1.0.3/featurebyte/service/feature_facade.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature Facade Service which is responsible for handling high level feature operations
 """
+
 from typing import Optional
 
 from pprint import pformat
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentDeletionError, DocumentUpdateError
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/service/feature_job_setting_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureJobSettingAnalysisService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson.objectid import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_list.py` & `featurebyte-1.0.3/featurebyte/service/feature_list.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureListService class
 """
+
 from __future__ import annotations
 
 from typing import Any, AsyncIterator, Callable, Coroutine, Dict, List, Optional, Sequence, cast
 
 from dataclasses import dataclass
 from pathlib import Path
 
@@ -22,21 +23,23 @@
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import (
     FeatureCluster,
     FeatureListModel,
     FeatureReadinessDistribution,
 )
 from featurebyte.models.feature_list_namespace import FeatureListNamespaceModel
+from featurebyte.models.offline_store_feature_table import OfflineStoreFeatureTableModel
 from featurebyte.models.persistent import QueryFilter
 from featurebyte.persistent import Persistent
 from featurebyte.query_graph.model.entity_lookup_plan import EntityLookupPlanner
 from featurebyte.query_graph.model.entity_relationship_info import (
     EntityRelationshipInfo,
     FeatureEntityLookupInfo,
 )
+from featurebyte.query_graph.node.schema import ColumnSpec
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.schema.feature_list import FeatureListServiceCreate, FeatureListServiceUpdate
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.entity_relationship_extractor import (
     EntityRelationshipExtractorService,
@@ -164,15 +167,18 @@
         -------
         Lock
         """
         return self.redis.lock(f"feature_list_creation:{self.catalog_id}", timeout=timeout)
 
     async def _populate_remote_attributes(self, document: FeatureListModel) -> FeatureListModel:
         if document.feature_clusters_path:
-            feature_clusters = await self.storage.get_text(Path(document.feature_clusters_path))
+            feature_clusters = await self.storage.get_text(
+                Path(document.feature_clusters_path),
+                cache_key=document.feature_clusters_path,
+            )
             document.internal_feature_clusters = json_util.loads(feature_clusters)
         return document
 
     async def _move_feature_cluster_to_storage(
         self, document: FeatureListModel
     ) -> FeatureListModel:
         feature_cluster_path = self.get_full_remote_file_path(
@@ -274,37 +280,28 @@
         )
         features_entity_lookup_info = []
         entity_id_to_serving_name = (
             await self.entity_serving_names_service.get_entity_id_to_serving_name_for_offline_store(
                 entity_ids=list(set().union(*[feature.entity_ids for feature in features]))
             )
         )
+        store_info_service = self.offline_store_info_initialization_service
         for idx, feature in enumerate(features):
             feature_list_to_feature_primary_entity_join_steps = (
                 EntityLookupPlanner.generate_lookup_steps(
                     available_entity_ids=fl_primary_entity_ids,
                     required_entity_ids=feature.primary_entity_ids,
                     relationships_info=relationships_info,
                 )
             )
-            feature_internal_entity_join_steps = []
-            feature_tables_entity_ids = await self.offline_store_info_initialization_service.get_offline_store_feature_tables_entity_ids(
-                feature, entity_id_to_serving_name
-            )
-            for entity_ids in feature_tables_entity_ids:
-                if feature.relationships_info is None:
-                    continue
-                internal_steps = EntityLookupPlanner.generate_lookup_steps(
-                    available_entity_ids=feature.primary_entity_ids,
-                    required_entity_ids=entity_ids,
-                    relationships_info=feature.relationships_info,
-                )
-                for step in internal_steps:
-                    if step not in feature_internal_entity_join_steps:
-                        feature_internal_entity_join_steps.append(step)
+            feature_internal_entity_join_steps = (
+                await store_info_service.get_entity_join_steps_for_feature_table(
+                    feature=feature, entity_id_to_serving_name=entity_id_to_serving_name
+                )
+            )
             feature_entity_lookup_info = FeatureEntityLookupInfo(
                 feature_id=feature.id,
                 feature_list_to_feature_primary_entity_join_steps=feature_list_to_feature_primary_entity_join_steps,
                 feature_internal_entity_join_steps=feature_internal_entity_join_steps,
             )
             if feature_entity_lookup_info.join_steps:
                 features_entity_lookup_info.append(feature_entity_lookup_info)
@@ -465,17 +462,19 @@
             await progress_callback(10, "Extracting feature data")
         feature_data = await self._extract_feature_data(document)
 
         if progress_callback:
             await progress_callback(30, "Extracting entity relationship data")
         entity_relationship_data = await self.extract_entity_relationship_data(
             features=feature_data["features"],
-            progress_callback=get_ranged_progress_callback(progress_callback, 30, 60)
-            if progress_callback
-            else None,
+            progress_callback=(
+                get_ranged_progress_callback(progress_callback, 30, 60)
+                if progress_callback
+                else None
+            ),
         )
 
         # update document with derived output
         document = FeatureListModel(
             **{
                 **document.dict(by_alias=True),
                 "features": feature_data["features"],
@@ -545,33 +544,50 @@
             collection_name=self.collection_name,
             query_filter=self._construct_get_query_filter(document_id=document_id),
             update={"$set": {"readiness_distribution": readiness_distribution.dict()["__root__"]}},
             user_id=self.user.id,
             disable_audit=self.should_disable_audit,
         )
 
-    async def update_store_info(self, document_id: ObjectId, features: List[FeatureModel]) -> None:
+    async def update_store_info(
+        self,
+        document_id: ObjectId,
+        features: List[FeatureModel],
+        feature_table_map: Dict[str, OfflineStoreFeatureTableModel],
+        serving_entity_specs: Optional[List[ColumnSpec]],
+    ) -> None:
         """
         Update store info for a feature list
 
         Parameters
         ----------
         document_id: ObjectId
             Feature list id
         features: List[FeatureModel]
             List of features
+        feature_table_map: Dict[str, OfflineStoreFeatureTableModel]
+            Feature table map
+        serving_entity_specs: Optional[List[ColumnSpec]]
+            List of serving entity specs
         """
-        feature_list = await self.get_document(document_id=document_id)
+        feature_list = await self.get_document(
+            document_id=document_id, populate_remote_attributes=False
+        )
         assert set(feature_list.feature_ids) == set(feature.id for feature in features)
         self._check_document_modifiable(document=feature_list.dict(by_alias=True))
 
         feature_store = await self.feature_store_service.get_document(
             document_id=features[0].tabular_source.feature_store_id
         )
-        feature_list.initialize_store_info(features=features, feature_store=feature_store)
+        feature_list.initialize_store_info(
+            features=features,
+            feature_store=feature_store,
+            feature_table_map=feature_table_map,
+            serving_entity_specs=serving_entity_specs,
+        )
         if feature_list.internal_store_info:
             await self.persistent.update_one(
                 collection_name=self.collection_name,
                 query_filter=self._construct_get_query_filter(document_id=document_id),
                 update={"$set": {"store_info": feature_list.internal_store_info}},
                 user_id=self.user.id,
                 disable_audit=self.should_disable_audit,
@@ -628,15 +644,15 @@
         count: int
             Number of sample entity serving names to return
 
         Returns
         -------
         List[Dict[str, str]]
         """
-        feature_list = await self.get_document(feature_list_id)
+        feature_list = await self.get_document(feature_list_id, populate_remote_attributes=False)
 
         # get entities and tables used for the feature list
         return await self.entity_serving_names_service.get_sample_entity_serving_names(
             entity_ids=feature_list.entity_ids,
             table_ids=feature_list.table_ids,
             count=count,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_list_facade.py` & `featurebyte-1.0.3/featurebyte/service/feature_list_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature List Facade Service which is responsible for handling high level feature list operations
 """
+
 from typing import Any, Callable, Coroutine, Optional
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentDeletionError, DocumentNotFoundError
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.feature_list_namespace import FeatureListNamespaceModel, FeatureListStatus
@@ -82,34 +83,52 @@
         await self.feature_readiness_service.update_feature_list_namespace(
             feature_list_namespace_id=document.feature_list_namespace_id,
         )
         output = await self.feature_list_service.get_document(document_id=document.id)
         return output
 
     async def make_features_production_ready(
-        self, feature_list_id: ObjectId, ignore_guardrails: bool = False
+        self,
+        feature_list_id: ObjectId,
+        ignore_guardrails: bool = False,
+        progress_callback: Optional[Callable[..., Coroutine[Any, Any, None]]] = None,
     ) -> None:
         """
         Make feature(s) of the given feature list production ready
 
         Parameters
         ----------
         feature_list_id: ObjectId
             Feature list id
         ignore_guardrails: bool
             Ignore guardrails of feature readiness update
+        progress_callback: Optional[Callable[..., Coroutine[Any, Any, None]]]
+            Progress callback to update progress
         """
-        feature_list = await self.feature_list_service.get_document(document_id=feature_list_id)
-        for feature_id in feature_list.feature_ids:
+        feature_list = await self.feature_list_service.get_document(
+            document_id=feature_list_id, populate_remote_attributes=False
+        )
+        total_features = len(feature_list.feature_ids)
+        for i, feature_id in enumerate(feature_list.feature_ids):
             await self.feature_readiness_service.update_feature(
                 feature_id=feature_id,
                 readiness=FeatureReadiness.PRODUCTION_READY,
                 ignore_guardrails=ignore_guardrails,
             )
-        await self.feature_list_service.get_document(document_id=feature_list_id)
+            if progress_callback:
+                percent = int((i + 1) / total_features * 100)
+                await progress_callback(
+                    percent=percent,
+                    message=f"Updated feature readiness for feature {i + 1} / {total_features}",
+                )
+
+        if progress_callback:
+            await progress_callback(
+                percent=100, message="Completed making all features production ready"
+            )
 
     async def update_status(
         self, feature_list_namespace_id: ObjectId, status: FeatureListStatus
     ) -> FeatureListNamespaceModel:
         """
         Update status of a feature list namespace
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_list_namespace.py` & `featurebyte-1.0.3/featurebyte/service/feature_list_namespace.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureListNamespaceService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.feature_list_namespace import FeatureListNamespaceModel
 from featurebyte.schema.feature_list_namespace import FeatureListNamespaceServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_list_status.py` & `featurebyte-1.0.3/featurebyte/service/feature_list_status.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_manager.py` & `featurebyte-1.0.3/featurebyte/service/feature_manager.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureManagerService class
 """
+
 from __future__ import annotations
 
 from typing import List, Optional, Set
 
 from datetime import datetime, timedelta, timezone
 
 import pandas as pd
@@ -22,18 +23,48 @@
 from featurebyte.models.tile import TileSpec, TileType
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.online_store_cleanup_scheduler import OnlineStoreCleanupSchedulerService
 from featurebyte.service.online_store_compute_query_service import OnlineStoreComputeQueryService
 from featurebyte.service.tile_manager import TileManagerService
 from featurebyte.service.tile_registry_service import TileRegistryService
 from featurebyte.session.base import BaseSession
+from featurebyte.session.databricks_unity import DatabricksUnitySession
 
 logger = get_logger(__name__)
 
 
+def get_previous_job_datetime(
+    input_dt: datetime, frequency_minutes: int, time_modulo_frequency_seconds: int
+) -> datetime:
+    """
+    Calculate the expected current job datetime give input datetime, frequency_minutes and
+    time_modulo_frequency_seconds.
+
+    Parameters
+    ----------
+    input_dt: datetime
+        input datetime
+    frequency_minutes: int
+        frequency in minutes
+    time_modulo_frequency_seconds: int
+        time_modulo_frequency in seconds
+
+    Returns
+    -------
+    datetime
+    """
+    next_job_time = get_next_job_datetime(
+        input_dt=input_dt,
+        frequency_minutes=frequency_minutes,
+        time_modulo_frequency_seconds=time_modulo_frequency_seconds,
+    )
+    previous_job_time = next_job_time - timedelta(minutes=frequency_minutes)
+    return previous_job_time
+
+
 class FeatureManagerService:
     """
     FeatureManagerService is responsible for orchestrating the materialization of features and tiles
     when a feature is online enabled or disabled.
     """
 
     def __init__(
@@ -75,14 +106,15 @@
             by WorkingSchemaService.
         """
         if (
             FeastIntegrationSettings().FEATUREBYTE_FEAST_INTEGRATION_ENABLED
             and session.source_type == SourceType.DATABRICKS_UNITY
         ):
             # Register Databricks UDF for on-demand feature
+            assert isinstance(session, DatabricksUnitySession)
             await self.may_register_databricks_udf_for_on_demand_feature(session, feature_spec)
 
         if not feature_spec.is_online_store_eligible:
             logger.info(
                 "Skipping scheduling both online and offline tile jobs",
                 extra={"feature_name": feature_spec.feature.name},
             )
@@ -111,22 +143,18 @@
         # backfill historical tiles if required
         aggregation_id_to_tile_spec = {}
         tile_specs_to_be_scheduled = []
         for tile_spec in feature_spec.feature.tile_specs:
             aggregation_id_to_tile_spec[tile_spec.aggregation_id] = tile_spec
             tile_job_exists = await self.tile_manager_service.tile_job_exists(tile_spec=tile_spec)
             if not tile_job_exists:
-                # generate historical tiles
-                await self._generate_historical_tiles(session=session, tile_spec=tile_spec)
                 tile_specs_to_be_scheduled.append(tile_spec)
-
-            elif is_recreating_schema:
-                # if this is called when recreating the schema, we cannot assume that the historical
-                # tiles are available even if there is an active tile jobs.
-                await self._generate_historical_tiles(session=session, tile_spec=tile_spec)
+            await self._backfill_tiles(
+                session=session, tile_spec=tile_spec, schedule_time=schedule_time
+            )
 
         # populate feature store. if this is called when recreating the schema, we need to run all
         # the online store compute queries since the tables need to be regenerated.
         for query in self._filter_precompute_queries(
             feature_spec, None if is_recreating_schema else unscheduled_result_names
         ):
             await self._populate_feature_store(
@@ -195,76 +223,194 @@
     async def _populate_feature_store(
         self,
         session: BaseSession,
         tile_spec: TileSpec,
         schedule_time: datetime,
         aggregation_result_name: str,
     ) -> None:
-        next_job_time = get_next_job_datetime(
+        job_schedule_ts = get_previous_job_datetime(
             input_dt=schedule_time,
             frequency_minutes=tile_spec.frequency_minute,
             time_modulo_frequency_seconds=tile_spec.time_modulo_frequency_second,
         )
-        job_schedule_ts = next_job_time - timedelta(minutes=tile_spec.frequency_minute)
         job_schedule_ts_str = job_schedule_ts.strftime("%Y-%m-%d %H:%M:%S")
         await self.tile_manager_service.populate_feature_store(
             session, tile_spec, job_schedule_ts_str, aggregation_result_name
         )
 
-    async def _generate_historical_tiles(self, session: BaseSession, tile_spec: TileSpec) -> None:
-        # generate historical tile_values
-        date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
+    async def _backfill_tiles(
+        self, session: BaseSession, tile_spec: TileSpec, schedule_time: datetime
+    ) -> None:
+        """
+        Backfill tiles required to populate internal online store
 
-        # derive the latest tile_start_date
-        end_ind = date_util.timestamp_utc_to_tile_index(
-            datetime.utcnow(),
-            tile_spec.time_modulo_frequency_second,
-            tile_spec.blind_spot_second,
-            tile_spec.frequency_minute,
-        )
-        end_ts = date_util.tile_index_to_timestamp_utc(
-            end_ind,
-            tile_spec.time_modulo_frequency_second,
-            tile_spec.blind_spot_second,
-            tile_spec.frequency_minute,
+        This will determine the tiles that need to be backfilled using information stored in the
+        tile registry collection. Tiles between backfill_start_date and last_run_tile_end_date can
+        be assumed to be available because of previous backfills and on going scheduled tile tasks.
+        See an example below.
+
+        Feature: Amount Sum over 48 hours
+
+                 <-------------------------------- 48h ------------------------------->
+                                            (tiles available)
+                                      # # # # # # # # # # # # # # #
+        ---------|-------------------|----------------------------|-------------------|--> (time)
+                 ^                   ^                            ^                   ^
+             start_ts        backfill_start_date      last_run_tile_end_date       end_ts
+                 <------------------->                            <------------------->
+                    required compute                                 required compute
+           (start_ts_1)        (end_ts_1)                 (start_ts_2)           (end_ts_2)
+
+        Notes:
+
+        * start_ts: start timestamp of the tiles that need to be computed. This is determined by
+          end_ts and the largest feature derivation window of the feature.
+
+        * end_ts: end timestamp of the tiles that need to be computed. This is determined by the
+          feature job settings and the deployment time.
+
+        * backfill_start_date: start timestamp of the tile that was last backfilled. This is updated
+          by tile backfill process during deployment.
+
+        * last_run_tile_end_date: end timestamp of the tile that was last computed in the scheduled
+          tile task. This is updated by scheduled tile task regularly.
+
+        Parameters
+        ----------
+        session: BaseSession
+            Instance of BaseSession to interact with the data warehouse
+        tile_spec: TileSpec
+            Instance of TileSpec
+        schedule_time: datetime
+            The moment of enabling the feature
+        """
+        # Derive the tile end date based on expected previous job time
+        job_schedule_ts = get_previous_job_datetime(
+            input_dt=schedule_time,
+            frequency_minutes=tile_spec.frequency_minute,
+            time_modulo_frequency_seconds=tile_spec.time_modulo_frequency_second,
         )
-        end_ts_str = end_ts.strftime(date_format)
+        end_ts = job_schedule_ts - timedelta(seconds=tile_spec.blind_spot_second)
 
-        start_ts = datetime(1970, 1, 1, tzinfo=timezone.utc)
+        # Determine the earliest tiles required to compute the features for offline store. This is
+        # determined by the largest feature derivation window.
+        max_window_seconds: Optional[int] = None
+        for window in tile_spec.windows:
+            if window is None:
+                max_window_seconds = None
+                break
+            window_seconds = int(pd.Timedelta(window).total_seconds())
+            if max_window_seconds is None or window_seconds > max_window_seconds:
+                max_window_seconds = window_seconds
+
+        if max_window_seconds is None:
+            # Need to backfill all the way back for features without a bounded window
+            start_ts = datetime(1970, 1, 1, tzinfo=timezone.utc)
+        else:
+            # For features with a bounded window, backfill only the required tiles
+            start_ts = end_ts - timedelta(seconds=max_window_seconds)
+
+        start_ts_1: Optional[datetime] = None
+        end_ts_1: Optional[datetime] = None
+        start_ts_2: Optional[datetime] = None
+        end_ts_2: Optional[datetime] = None
 
         tile_model = await self.tile_registry_service.get_tile_model(
             tile_spec.tile_id, tile_spec.aggregation_id
         )
-        if tile_model is not None and tile_model.last_run_metadata_offline is not None:
-            start_ts = tile_model.last_run_metadata_offline.tile_end_date
+        if tile_model is not None and tile_model.backfill_metadata is not None:
+            if start_ts.replace(tzinfo=None) < tile_model.backfill_metadata.start_date:
+                # Need to compute tiles up to previous backfill start date
+                start_ts_1 = start_ts
+                end_ts_1 = tile_model.backfill_metadata.start_date
+            if tile_model.last_run_metadata_offline is None:
+                # Note: unlikely to happen as there should already be a tile job running, but we
+                # still need to handle this case in case of bad state / error
+                start_ts_2 = tile_model.backfill_metadata.start_date
+                end_ts_2 = end_ts
+            elif tile_model.last_run_metadata_offline.tile_end_date < end_ts.replace(tzinfo=None):
+                # Need to compute tiles from last run tile end date to end_ts
+                start_ts_2 = tile_model.last_run_metadata_offline.tile_end_date
+                end_ts_2 = end_ts
+            to_init_backfill_metadata = False
+        else:
+            # Need to compute tiles up to end_ts
+            start_ts_2 = start_ts
+            end_ts_2 = end_ts
+            to_init_backfill_metadata = True
+
+        logger.info(
+            "Determined tile start and end timestamps for online enabling",
+            extra={
+                "start_ts_1": str(start_ts_1),
+                "end_ts_1": str(end_ts_1),
+                "start_ts_2": str(start_ts_2),
+                "end_ts_2": str(end_ts_2),
+            },
+        )
 
-        logger.info(f"start_ts: {start_ts}")
+        if start_ts_1 is not None and end_ts_1 is not None:
+            await self._backfill_tiles_with_start_end(
+                session=session,
+                tile_spec=tile_spec,
+                start_ts=start_ts_1,
+                end_ts=end_ts_1,
+                update_last_run_metadata=False,
+                update_backfill_start_date=True,
+            )
 
+        if start_ts_2 is not None and end_ts_2 is not None:
+            await self._backfill_tiles_with_start_end(
+                session=session,
+                tile_spec=tile_spec,
+                start_ts=start_ts_2,
+                end_ts=end_ts_2,
+                update_last_run_metadata=True,
+                update_backfill_start_date=to_init_backfill_metadata,
+            )
+
+    async def _backfill_tiles_with_start_end(
+        self,
+        session: BaseSession,
+        tile_spec: TileSpec,
+        start_ts: datetime,
+        end_ts: datetime,
+        update_last_run_metadata: bool,
+        update_backfill_start_date: bool,
+    ) -> None:
+        # Align the start timestamp to the tile boundary
         start_ind = date_util.timestamp_utc_to_tile_index(
             start_ts,
             tile_spec.time_modulo_frequency_second,
             tile_spec.blind_spot_second,
             tile_spec.frequency_minute,
         )
         start_ts = date_util.tile_index_to_timestamp_utc(
             start_ind,
             tile_spec.time_modulo_frequency_second,
             tile_spec.blind_spot_second,
             tile_spec.frequency_minute,
         )
+        date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
         start_ts_str = start_ts.strftime(date_format)
-
+        end_ts_str = end_ts.strftime(date_format)
         await self.tile_manager_service.generate_tiles(
             session=session,
             tile_spec=tile_spec,
             tile_type=TileType.OFFLINE,
             end_ts_str=end_ts_str,
             start_ts_str=start_ts_str,
-            last_tile_start_ts_str=end_ts_str,
+            last_tile_start_ts_str=end_ts_str if update_last_run_metadata else None,
         )
+        if update_backfill_start_date:
+            await self.tile_registry_service.update_backfill_metadata(
+                tile_id=tile_spec.tile_id,
+                aggregation_id=tile_spec.aggregation_id,
+                backfill_start_date=start_ts,
+            )
 
     async def _update_tile_feature_mapping_table(
         self,
         feature_spec: OnlineFeatureSpec,
         unscheduled_result_names: Set[str],
     ) -> None:
         """
@@ -297,14 +443,15 @@
         """
         if (
             FeastIntegrationSettings().FEATUREBYTE_FEAST_INTEGRATION_ENABLED
             and session
             and session.source_type == SourceType.DATABRICKS_UNITY
         ):
             # Remove Databricks UDF for on-demand feature
+            assert isinstance(session, DatabricksUnitySession)
             await self.remove_databricks_udf_for_on_demand_feature_if_exists(session, feature_spec)
 
         if not feature_spec.is_online_store_eligible:
             return
 
         # cleaning online store compute queries
         await self.remove_online_store_compute_queries(feature_spec)
@@ -404,55 +551,60 @@
             query_start_ts=query_start_ts, query_end_ts=query_end_ts
         )
         result = await session.execute_query(sql)
         return result
 
     @staticmethod
     async def may_register_databricks_udf_for_on_demand_feature(
-        session: BaseSession, feature_spec: OnlineFeatureSpec
+        session: DatabricksUnitySession, feature_spec: OnlineFeatureSpec
     ) -> None:
         """
         Register Databricks UDF for on-demand feature.
 
         Parameters
         ----------
-        session: BaseSession
-            Instance of BaseSession to interact with the data warehouse
+        session: DatabricksUnitySession
+            Instance of DatabricksUnitySession to interact with the data warehouse
         feature_spec: OnlineFeatureSpec
             Instance of OnlineFeatureSpec
         """
         offline_store_info = feature_spec.feature.offline_store_info
-        if offline_store_info and offline_store_info.is_decomposed and offline_store_info.udf_info:
+        if offline_store_info and offline_store_info.udf_info and offline_store_info.udf_info.codes:
             udf_info = offline_store_info.udf_info
             logger.debug(
                 "Registering Databricks UDF for on-demand feature",
                 extra={
                     "feature_name": feature_spec.feature.name,
                     "function_name": udf_info.sql_function_name,
                 },
             )
             await session.execute_query(f"DROP FUNCTION IF EXISTS {udf_info.sql_function_name}")
             await session.execute_query(udf_info.codes)
+            await session.set_owner("FUNCTION", udf_info.sql_function_name)
 
     @staticmethod
     async def remove_databricks_udf_for_on_demand_feature_if_exists(
-        session: BaseSession, feature_spec: OnlineFeatureSpec
+        session: DatabricksUnitySession, feature_spec: OnlineFeatureSpec
     ) -> None:
         """
         Remove Databricks UDF for on-demand feature.
 
         Parameters
         ----------
-        session: BaseSession
-            Instance of BaseSession to interact with the data warehouse
+        session: DatabricksUnitySession
+            Instance of DatabricksUnitySession to interact with the data warehouse
         feature_spec: OnlineFeatureSpec
             Instance of OnlineFeatureSpec
         """
         offline_store_info = feature_spec.feature.offline_store_info
-        if offline_store_info and offline_store_info.is_decomposed and offline_store_info.udf_info:
+        if (
+            offline_store_info
+            and offline_store_info.udf_info
+            and offline_store_info.udf_info.sql_function_name
+        ):
             udf_info = offline_store_info.udf_info
             logger.debug(
                 "Removing Databricks UDF for on-demand feature",
                 extra={
                     "feature_name": feature_spec.feature.name,
                     "function_name": udf_info.sql_function_name,
                 },
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_materialize.py` & `featurebyte-1.0.3/featurebyte/service/feature_table_cache.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,824 +1,738 @@
 """
-FeatureMaterializeService class
+Module for managing physical feature table cache as well as metadata storage.
 """
-from __future__ import annotations
 
-from typing import Any, AsyncIterator, List, Optional, Tuple, cast
-
-import textwrap
-from contextlib import asynccontextmanager
-from dataclasses import dataclass
-from datetime import datetime
+from typing import Any, Callable, Coroutine, Dict, List, Optional, Tuple, cast
 
 import pandas as pd
 from bson import ObjectId
-from redis import Redis
-from redis.lock import Lock
 from sqlglot import expressions
 
-from featurebyte.enum import InternalName, SourceType
-from featurebyte.feast.service.feature_store import FeastFeatureStore, FeastFeatureStoreService
-from featurebyte.feast.service.registry import FeastRegistryService
-from featurebyte.feast.utils.materialize_helper import materialize_partial
-from featurebyte.models.offline_store_feature_table import (
-    OfflineLastMaterializedAtUpdate,
-    OfflineStoreFeatureTableModel,
+from featurebyte.common.progress import get_ranged_progress_callback
+from featurebyte.common.utils import timer
+from featurebyte.enum import InternalName, MaterializedTableNamePrefix
+from featurebyte.logging import get_logger
+from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.feature_store import FeatureStoreModel
+from featurebyte.models.feature_table_cache_metadata import (
+    CachedFeatureDefinition,
+    FeatureTableCacheMetadataModel,
 )
+from featurebyte.models.observation_table import ObservationTableModel
+from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.schema import TableDetails
-from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
-from featurebyte.query_graph.sql.ast.literal import make_literal_value
+from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.common import (
+    get_fully_qualified_table_name,
     get_qualified_column_identifier,
     quoted_identifier,
     sql_to_string,
 )
-from featurebyte.query_graph.sql.entity import DUMMY_ENTITY_COLUMN_NAME, get_combined_serving_names
-from featurebyte.query_graph.sql.online_serving import (
-    TemporaryBatchRequestTable,
-    get_online_features,
-)
+from featurebyte.query_graph.transform.definition import DefinitionHashExtractor
+from featurebyte.query_graph.transform.offline_store_ingest import extract_dtype_from_graph
 from featurebyte.service.entity_validation import EntityValidationService
-from featurebyte.service.feature import FeatureService
-from featurebyte.service.feature_store import FeatureStoreService
-from featurebyte.service.materialized_table import BaseMaterializedTableService
-from featurebyte.service.offline_store_feature_table import OfflineStoreFeatureTableService
-from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
+from featurebyte.service.feature_list import FeatureListService
+from featurebyte.service.feature_table_cache_metadata import FeatureTableCacheMetadataService
+from featurebyte.service.historical_features_and_target import get_historical_features, get_target
+from featurebyte.service.namespace_handler import NamespaceHandler
 from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.service.tile_cache import TileCacheService
 from featurebyte.session.base import BaseSession
 
-OFFLINE_STORE_TABLE_REDIS_LOCK_TIMEOUT_SECONDS = 3600
-
-
-@dataclass
-class MaterializedFeatures:
-    """
-    Information about materialised features ready to be published
-    """
+FEATURE_TABLE_CACHE_CHECK_PROGRESS_PERCENTAGE = 10
 
-    materialized_table_name: str
-    column_names: List[str]
-    data_types: List[str]
-    serving_names: List[str]
-    feature_timestamp: datetime
-    source_type: SourceType
+logger = get_logger(__name__)
 
-    @property
-    def serving_names_and_column_names(self) -> List[str]:
-        """
-        Returns serving names and column names in a combined list
 
-        Returns
-        -------
-        List[str]
-        """
-        result = self.serving_names[:]
-        if len(self.serving_names) > 1:
-            result.append(get_combined_serving_names(self.serving_names))
-        if len(self.serving_names) == 0 and self.source_type == SourceType.DATABRICKS_UNITY:
-            result.append(DUMMY_ENTITY_COLUMN_NAME)
-        result += self.column_names
-        return result
-
-
-class FeatureMaterializeService:  # pylint: disable=too-many-instance-attributes
+class FeatureTableCacheService:
     """
-    FeatureMaterializeService is responsible for materialising a set of currently online enabled
-    features so that they can be published to an external feature store. These features are
-    materialised into a new table in the data warehouse.
+    Service for managing physical feature table cache as well as metadata storage.
     """
 
-    def __init__(  # pylint: disable=too-many-arguments
+    def __init__(
         self,
-        feature_service: FeatureService,
-        online_store_table_version_service: OnlineStoreTableVersionService,
-        feature_store_service: FeatureStoreService,
+        feature_table_cache_metadata_service: FeatureTableCacheMetadataService,
+        namespace_handler: NamespaceHandler,
         session_manager_service: SessionManagerService,
-        feast_registry_service: FeastRegistryService,
-        feast_feature_store_service: FeastFeatureStoreService,
-        offline_store_feature_table_service: OfflineStoreFeatureTableService,
         entity_validation_service: EntityValidationService,
-        redis: Redis[Any],
+        tile_cache_service: TileCacheService,
+        feature_list_service: FeatureListService,
     ):
-        self.feature_service = feature_service
-        self.online_store_table_version_service = online_store_table_version_service
-        self.feature_store_service = feature_store_service
+        self.feature_table_cache_metadata_service = feature_table_cache_metadata_service
+        self.namespace_handler = namespace_handler
         self.session_manager_service = session_manager_service
-        self.feast_registry_service = feast_registry_service
-        self.feast_feature_store_service = feast_feature_store_service
-        self.offline_store_feature_table_service = offline_store_feature_table_service
         self.entity_validation_service = entity_validation_service
-        self.redis = redis
+        self.tile_cache_service = tile_cache_service
+        self.feature_list_service = feature_list_service
 
-    @asynccontextmanager
-    async def materialize_features(
+    async def definition_hashes_for_nodes(
         self,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        selected_columns: Optional[List[str]] = None,
-        session: Optional[BaseSession] = None,
-        use_last_materialized_timestamp: bool = True,
-    ) -> AsyncIterator[MaterializedFeatures]:
+        graph: QueryGraph,
+        nodes: List[Node],
+    ) -> List[str]:
         """
-        Materialise features for the provided offline store feature table.
+        Compute definition hashes for list of nodes
 
         Parameters
         ----------
-        feature_table_model: OfflineStoreFeatureTableModel
-            OfflineStoreFeatureTableModel object
-        selected_columns: Optional[List[str]]
-            Selected columns to materialize
-        session: Optional[BaseSession]
-            Session object
-        use_last_materialized_timestamp: bool
-            Whether to specify the last materialized timestamp of feature_table_model when creating
-            the entity universe. This is set to True on scheduled task, and False on initialization
-            of new columns.
-
-        Yields
-        ------
-        MaterializedFeatures
-            Metadata of the materialized features
-        """
-        assert feature_table_model.feature_cluster is not None, "Missing feature cluster"
-
-        # Create temporary batch request table with the universe of entities
-        if session is None:
-            session = await self._get_session(feature_table_model)
-
-        unique_id = ObjectId()
-        batch_request_table = TemporaryBatchRequestTable(
-            table_details=TableDetails(
-                database_name=session.database_name,
-                schema_name=session.schema_name,
-                table_name=f"TEMP_REQUEST_TABLE_{unique_id}".upper(),
-            ),
-            column_names=feature_table_model.serving_names,
-        )
-        adapter = get_sql_adapter(session.source_type)
-        feature_timestamp = datetime.utcnow()
-        create_batch_request_table_query = sql_to_string(
-            adapter.create_table_as(
-                table_details=batch_request_table.table_details,
-                select_expr=feature_table_model.entity_universe.get_entity_universe_expr(
-                    current_feature_timestamp=feature_timestamp,
-                    last_materialized_timestamp=feature_table_model.last_materialized_at
-                    if use_last_materialized_timestamp
-                    else None,
-                ),
-            ),
-            source_type=session.source_type,
-        )
-        await session.execute_query_long_running(create_batch_request_table_query)
-        await BaseMaterializedTableService.add_row_index_column(
-            session, batch_request_table.table_details
-        )
-
-        # Materialize features
-        output_table_details = TableDetails(
-            database_name=session.database_name,
-            schema_name=session.schema_name,
-            table_name=f"TEMP_FEATURE_TABLE_{unique_id}".upper(),
-        )
-        try:
-            if selected_columns is None:
-                nodes = feature_table_model.feature_cluster.nodes
-            else:
-                nodes = feature_table_model.feature_cluster.get_nodes_for_feature_names(
-                    selected_columns
-                )
-            feature_store = await self.feature_store_service.get_document(
-                document_id=feature_table_model.feature_cluster.feature_store_id
-            )
-            parent_serving_preparation = await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
-                graph_nodes=(feature_table_model.feature_cluster.graph, nodes),
-                feature_list_model=None,
-                request_column_names=set(feature_table_model.serving_names),
-                feature_store=feature_store,
-            )
-            await get_online_features(
-                session=session,
-                graph=feature_table_model.feature_cluster.graph,
-                nodes=nodes,
-                request_data=batch_request_table,
-                source_type=session.source_type,
-                online_store_table_version_service=self.online_store_table_version_service,
-                output_table_details=output_table_details,
-                request_timestamp=feature_timestamp,
-                parent_serving_preparation=parent_serving_preparation,
-                concatenate_serving_names=feature_table_model.serving_names,
-            )
+        graph: QueryGraph
+            Graph definition
+        nodes: List[Node]
+            Input node names
 
-            column_names = []
-            column_dtypes = []
-            for column_name, data_type in zip(
-                feature_table_model.output_column_names, feature_table_model.output_dtypes
-            ):
-                if selected_columns is None or column_name in selected_columns:
-                    column_names.append(column_name)
-                    column_dtypes.append(data_type)
-
-            yield MaterializedFeatures(
-                materialized_table_name=output_table_details.table_name,
-                column_names=column_names,
-                data_types=[adapter.get_physical_type_from_dtype(dtype) for dtype in column_dtypes],
-                serving_names=feature_table_model.serving_names,
-                feature_timestamp=feature_timestamp,
-                source_type=session.source_type,
-            )
+        Returns
+        -------
+        List[str]
+            List of definition hashes corresponding to nodes
+        """
+        node_names = [node.name for node in nodes]
+        pruned_graph, node_name_map = graph.quick_prune(target_node_names=node_names)
 
-        finally:
-            # Delete temporary batch request table and materialized feature table
-            for table_details in [batch_request_table.table_details, output_table_details]:
-                await session.drop_table(
-                    table_name=table_details.table_name,
-                    schema_name=table_details.schema_name,  # type: ignore
-                    database_name=table_details.database_name,  # type: ignore
-                    if_exists=True,
-                )
+        hashes = []
+        for node in nodes:
+            (
+                prepared_graph,
+                prepared_node_name,
+            ) = await self.namespace_handler.prepare_graph_to_store(
+                graph=pruned_graph,
+                node=pruned_graph.get_node_by_name(node_name_map[node.name]),
+                sanitize_for_definition=True,
+            )
+            definition_hash_extractor = DefinitionHashExtractor(graph=prepared_graph)
+            definition_hash = definition_hash_extractor.extract(
+                prepared_graph.get_node_by_name(prepared_node_name)
+            ).definition_hash
+            hashes.append(definition_hash)
+        return hashes
 
-    def get_table_update_lock(self, offline_store_table_name: str) -> Lock:
+    async def get_feature_definition_hashes(
+        self,
+        graph: QueryGraph,
+        nodes: List[Node],
+        feature_list_id: Optional[ObjectId],
+    ) -> List[str]:
         """
-        Get offline store table update lock.
-
-        This prevents concurrent writes to the same offline store table between scheduled task and
-        deployment task.
+        Get definition hashes for list of nodes. Retrieve the result from feature list if available.
 
         Parameters
         ----------
-        offline_store_table_name: str
-            Offline store table name
+        graph: QueryGraph
+            Query graph
+        nodes: List[Node]
+            Nodes
+        feature_list_id: ObjectId
+            Feature list id
 
         Returns
         -------
-        Lock
+        List[str]
         """
-        return self.redis.lock(
-            f"offline_store_table_update:{offline_store_table_name}",
-            timeout=OFFLINE_STORE_TABLE_REDIS_LOCK_TIMEOUT_SECONDS,
-        )
+        # Retrieve definition hashes if stored in feature list
+        definition_hashes_mapping = {}
+        if feature_list_id is not None:
+            feature_list = await self.feature_list_service.get_document(feature_list_id)
+            if feature_list.feature_clusters is not None:
+                stored_hashes = feature_list.feature_clusters[0].feature_node_definition_hashes
+                if stored_hashes is not None:
+                    for info in stored_hashes:
+                        if info.definition_hash is not None:
+                            definition_hashes_mapping[info.node_name] = info.definition_hash
+
+        # Fallback to deriving the hashes from scratch
+        missing_nodes = [node for node in nodes if node.name not in definition_hashes_mapping]
+        if missing_nodes:
+            missing_definition_hashes = await self.definition_hashes_for_nodes(graph, missing_nodes)
+            for node, definition_hash in zip(missing_nodes, missing_definition_hashes):
+                definition_hashes_mapping[node.name] = definition_hash
+
+        return [definition_hashes_mapping[node.name] for node in nodes]
 
-    async def scheduled_materialize_features(
+    def _get_column_exprs(
         self,
-        feature_table_model: OfflineStoreFeatureTableModel,
-    ) -> None:
-        """
-        Materialize features for the provided offline store feature table. This method is expected
-        to be called by a scheduler on regular interval, and the feature table is assumed to have
-        the correct schema already.
+        graph: QueryGraph,
+        nodes: List[Node],
+        hashes: List[str],
+        cached_features: Dict[str, str],
+    ) -> List[expressions.Alias]:
+        return [
+            expressions.alias_(
+                quoted_identifier(cached_features[definition_hash]),
+                alias=graph.get_node_output_column_name(node.name),
+                quoted=True,
+            )
+            for definition_hash, node in zip(hashes, nodes)
+        ]
+
+    async def get_non_cached_nodes(
+        self,
+        feature_table_cache_metadata: FeatureTableCacheMetadataModel,
+        nodes: List[Node],
+        hashes: List[str],
+    ) -> List[Tuple[Node, CachedFeatureDefinition]]:
+        """
+        Given an observation table, graph and set of nodes
+            - compute nodes definition hashes
+            - lookup existing Feature Table Cache metadata
+            - filter out nodes which are already added to the Feature Table Cache
+            - return cached and non-cached nodes and their definition hashes.
 
         Parameters
         ----------
-        feature_table_model: OfflineStoreFeatureTableModel
-            OfflineStoreFeatureTableModel object
-        """
-        session = await self._get_session(feature_table_model)
-        async with self.materialize_features(
-            feature_table_model, session=session, use_last_materialized_timestamp=True
-        ) as materialized_features:
-            with self.get_table_update_lock(offline_store_table_name=feature_table_model.name):
-                await self._insert_into_feature_table(
-                    session,
-                    feature_table_model,
-                    materialized_features,
-                )
+        feature_table_cache_metadata: FeatureTableCacheMetadataModel
+            Feature table cache metadata
+        nodes: List[Node]
+            Input node names
+        hashes: List[str]
+            Definition hashes corresponding to the list of nodes
 
-        # Feast online materialize
-        feature_store = await self._get_feast_feature_store()
-        if feature_store is not None and feature_store.config.online_store is not None:
-            assert feature_store.online_store_id is not None
-            online_store_last_materialized_at = (
-                feature_table_model.get_online_store_last_materialized_at(
-                    feature_store.online_store_id
+        Returns
+        -------
+        List[Tuple[Node, CachedFeatureDefinition]]
+            List of non cached nodes and respective newly-created cached feature definitions
+        """
+        cached_hashes = {
+            feat.definition_hash: feat for feat in feature_table_cache_metadata.feature_definitions
+        }
+        added_hashes = set()
+        non_cached_nodes = []
+        for definition_hash, node in zip(hashes, nodes):
+            if definition_hash in cached_hashes or definition_hash in added_hashes:
+                continue
+            added_hashes.add(definition_hash)
+            non_cached_nodes.append(
+                (
+                    node,
+                    CachedFeatureDefinition(definition_hash=definition_hash),
                 )
             )
-            await self._materialize_online(
+        return non_cached_nodes
+
+    async def _populate_intermediate_table(  # pylint: disable=too-many-arguments
+        self,
+        feature_store: FeatureStoreModel,
+        observation_table: ObservationTableModel,
+        db_session: BaseSession,
+        intermediate_table_name: str,
+        graph: QueryGraph,
+        nodes: List[Tuple[Node, CachedFeatureDefinition]],
+        is_target: bool = False,
+        serving_names_mapping: Optional[Dict[str, str]] = None,
+        progress_callback: Optional[
+            Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
+        ] = None,
+    ) -> None:
+        request_column_names = {col.name for col in observation_table.columns_info}
+        nodes_only = [node for node, _ in nodes]
+        parent_serving_preparation = (
+            await self.entity_validation_service.validate_entities_or_prepare_for_parent_serving(
+                graph_nodes=(graph, nodes_only),
+                request_column_names=request_column_names,
                 feature_store=feature_store,
-                feature_table_model=feature_table_model,
-                columns=feature_table_model.output_column_names,
-                start_date=online_store_last_materialized_at,
-                end_date=materialized_features.feature_timestamp,
+                serving_names_mapping=serving_names_mapping,
             )
-
-        # Update offline table last materialized timestamp
-        await self._update_offline_last_materialized_at(
-            feature_table_model, materialized_features.feature_timestamp
         )
+        output_table_details = TableDetails(
+            database_name=db_session.database_name,
+            schema_name=db_session.schema_name,
+            table_name=intermediate_table_name,
+        )
+        if is_target:
+            await get_target(
+                session=db_session,
+                graph=graph,
+                nodes=nodes_only,
+                observation_set=observation_table,
+                feature_store=feature_store,
+                output_table_details=output_table_details,
+                serving_names_mapping=serving_names_mapping,
+                parent_serving_preparation=parent_serving_preparation,
+                progress_callback=progress_callback,
+            )
+        else:
+            await get_historical_features(
+                session=db_session,
+                tile_cache_service=self.tile_cache_service,
+                graph=graph,
+                nodes=nodes_only,
+                observation_set=observation_table,
+                feature_store=feature_store,
+                output_table_details=output_table_details,
+                serving_names_mapping=serving_names_mapping,
+                parent_serving_preparation=parent_serving_preparation,
+                progress_callback=progress_callback,
+            )
 
-    async def initialize_new_columns(
+    async def _create_table(  # pylint: disable=too-many-arguments
         self,
-        feature_table_model: OfflineStoreFeatureTableModel,
+        feature_store: FeatureStoreModel,
+        observation_table: ObservationTableModel,
+        db_session: BaseSession,
+        final_table_name: str,
+        graph: QueryGraph,
+        nodes: List[Tuple[Node, CachedFeatureDefinition]],
+        is_target: bool = False,
+        serving_names_mapping: Optional[Dict[str, str]] = None,
+        progress_callback: Optional[
+            Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
+        ] = None,
     ) -> None:
-        """
-        Initialize new columns in the feature table. This is expected to be called when an
-        OfflineStoreFeatureTable is updated to include new columns.
-
-        Parameters
-        ----------
-        feature_table_model: OfflineStoreFeatureTableModel
-            OfflineStoreFeatureTableModel object
-        """
-        with self.get_table_update_lock(feature_table_model.name):
-            offline_info = await self._initialize_new_columns_offline(feature_table_model)
+        intermediate_table_name = (
+            f"__TEMP__{MaterializedTableNamePrefix.FEATURE_TABLE_CACHE}_{ObjectId()}"
+        )
+        try:
+            await self._populate_intermediate_table(
+                feature_store=feature_store,
+                observation_table=observation_table,
+                db_session=db_session,
+                intermediate_table_name=intermediate_table_name,
+                graph=graph,
+                nodes=nodes,
+                is_target=is_target,
+                serving_names_mapping=serving_names_mapping,
+                progress_callback=progress_callback,
+            )
 
-        if offline_info is not None:
-            column_names, materialize_end_date = offline_info
-            await self._initialize_new_columns_online(
-                feature_table_model=feature_table_model,
-                column_names=column_names,
-                end_date=materialize_end_date,
+            request_column_names = [col.name for col in observation_table.columns_info]
+            request_columns = [quoted_identifier(col) for col in request_column_names]
+            feature_names = [
+                expressions.alias_(
+                    quoted_identifier(cast(str, graph.get_node_output_column_name(node.name))),
+                    alias=feature_definition.feature_name,
+                    quoted=True,
+                )
+                for node, feature_definition in nodes
+            ]
+            await db_session.create_table_as(
+                table_details=TableDetails(
+                    database_name=db_session.database_name,
+                    schema_name=db_session.schema_name,
+                    table_name=final_table_name,
+                ),
+                select_expr=(
+                    expressions.select(quoted_identifier(InternalName.TABLE_ROW_INDEX))
+                    .select(*request_columns)
+                    .select(*feature_names)
+                    .from_(quoted_identifier(intermediate_table_name))
+                ),
+            )
+        finally:
+            await db_session.drop_table(
+                database_name=db_session.database_name,
+                schema_name=db_session.schema_name,
+                table_name=intermediate_table_name,
+                if_exists=True,
             )
 
-    async def _initialize_new_columns_offline(
+    async def _update_table(  # pylint: disable=too-many-arguments
         self,
-        feature_table_model: OfflineStoreFeatureTableModel,
-    ) -> Optional[Tuple[List[str], datetime]]:
-        session = await self._get_session(feature_table_model)
-        num_rows_in_feature_table = await self._num_rows_in_feature_table(
-            session, feature_table_model
+        feature_store: FeatureStoreModel,
+        observation_table: ObservationTableModel,
+        cache_metadata: FeatureTableCacheMetadataModel,
+        db_session: BaseSession,
+        graph: QueryGraph,
+        non_cached_nodes: List[Tuple[Node, CachedFeatureDefinition]],
+        is_target: bool = False,
+        serving_names_mapping: Optional[Dict[str, str]] = None,
+        progress_callback: Optional[
+            Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
+        ] = None,
+    ) -> None:
+        # create temporary table with features
+        intermediate_table_name = (
+            f"__TEMP__{MaterializedTableNamePrefix.FEATURE_TABLE_CACHE}_{ObjectId()}"
         )
 
-        if num_rows_in_feature_table is not None:
-            selected_columns = await self._ensure_compatible_schema(session, feature_table_model)
-            if not selected_columns:
-                return None
-        else:
-            selected_columns = None
+        merge_target_table_alias = "feature_table_cache"
+        merge_source_table_alias = "partial_features"
 
-        async with self.materialize_features(
-            session=session,
-            feature_table_model=feature_table_model,
-            selected_columns=selected_columns,
-            use_last_materialized_timestamp=False,
-        ) as materialized_features:
-            if num_rows_in_feature_table is None:
-                # Create feature table is it doesn't exist yet
-                await self._create_feature_table(
-                    session,
-                    feature_table_model,
-                    materialized_features,
+        try:
+            await self._populate_intermediate_table(
+                feature_store=feature_store,
+                observation_table=observation_table,
+                db_session=db_session,
+                intermediate_table_name=intermediate_table_name,
+                graph=graph,
+                nodes=non_cached_nodes,
+                is_target=is_target,
+                serving_names_mapping=serving_names_mapping,
+                progress_callback=progress_callback,
+            )
+
+            # alter cached tables adding columns for new features
+            adapter = get_sql_adapter(db_session.source_type)
+            table_exr = expressions.Table(
+                this=quoted_identifier(cache_metadata.table_name),
+                db=quoted_identifier(db_session.schema_name),
+                catalog=quoted_identifier(db_session.database_name),
+            )
+            columns_expr = [
+                expressions.ColumnDef(
+                    this=quoted_identifier(cast(str, definition.feature_name)),
+                    kind=adapter.get_physical_type_from_dtype(
+                        extract_dtype_from_graph(graph, node)
+                    ),
                 )
-                await self._update_offline_last_materialized_at(
-                    feature_table_model, materialized_features.feature_timestamp
+                for node, definition in non_cached_nodes
+            ]
+            await db_session.execute_query_long_running(
+                adapter.alter_table_add_columns(table_exr, columns_expr)
+            )
+
+            # merge temp table into cache table
+            merge_conditions = [
+                expressions.EQ(
+                    this=get_qualified_column_identifier(
+                        InternalName.TABLE_ROW_INDEX, merge_target_table_alias
+                    ),
+                    expression=get_qualified_column_identifier(
+                        InternalName.TABLE_ROW_INDEX, merge_source_table_alias
+                    ),
                 )
-                materialize_end_date = materialized_features.feature_timestamp
-            else:
-                if num_rows_in_feature_table == 0:
-                    feature_timestamp_value = materialized_features.feature_timestamp.isoformat()
-                else:
-                    feature_timestamp_value = await self._get_last_feature_timestamp(
-                        session, feature_table_model.name
+            ]
+            update_expr = expressions.Update(
+                expressions=[
+                    expressions.EQ(
+                        this=get_qualified_column_identifier(
+                            cast(str, definition.feature_name), merge_target_table_alias
+                        ),
+                        expression=get_qualified_column_identifier(
+                            cast(str, graph.get_node_output_column_name(node.name)),
+                            merge_source_table_alias,
+                        ),
                     )
-                # Merge into existing feature table. If the table exists but is empty, do not
-                # specify merge conditions so that the merge operation simply inserts rows with
-                # new columns.
-                await self._merge_into_feature_table(
-                    session=session,
-                    feature_table_model=feature_table_model,
-                    materialized_features=materialized_features,
-                    feature_timestamp_value=feature_timestamp_value,
-                    to_specify_merge_conditions=num_rows_in_feature_table > 0,
-                )
-                materialize_end_date = pd.Timestamp(feature_timestamp_value).to_pydatetime()
-
-            return materialized_features.column_names, materialize_end_date
+                    for node, definition in non_cached_nodes
+                ]
+            )
+            merge_expr = expressions.Merge(
+                this=expressions.Table(
+                    this=quoted_identifier(cache_metadata.table_name),
+                    db=quoted_identifier(db_session.schema_name),
+                    catalog=quoted_identifier(db_session.database_name),
+                    alias=expressions.TableAlias(this=merge_target_table_alias),
+                ),
+                using=expressions.Table(
+                    this=quoted_identifier(intermediate_table_name),
+                    db=quoted_identifier(db_session.schema_name),
+                    catalog=quoted_identifier(db_session.database_name),
+                    alias=expressions.TableAlias(this=merge_source_table_alias),
+                ),
+                on=expressions.and_(*merge_conditions),
+                expressions=[
+                    expressions.When(
+                        this=expressions.Column(this=expressions.Identifier(this="MATCHED")),
+                        then=update_expr,
+                    ),
+                ],
+            )
+            await db_session.execute_query_long_running(
+                sql_to_string(merge_expr, source_type=db_session.source_type)
+            )
+        finally:
+            await db_session.drop_table(
+                database_name=db_session.database_name,
+                schema_name=db_session.schema_name,
+                table_name=intermediate_table_name,
+                if_exists=True,
+            )
 
-    async def _initialize_new_columns_online(
-        self,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        column_names: List[str],
-        end_date: datetime,
-    ) -> None:
-        # Feast online materialize. Start date is not set because these are new columns.
-        feature_store = await self._get_feast_feature_store()
-        if feature_store is not None and feature_store.config.online_store is not None:
-            assert feature_store.online_store_id is not None
-            await self._materialize_online(
-                feature_store=feature_store,
-                feature_table_model=feature_table_model,
-                columns=column_names,
-                end_date=end_date,
+    @staticmethod
+    async def _feature_table_cache_exists(
+        cache_metadata: FeatureTableCacheMetadataModel, session: BaseSession
+    ) -> bool:
+        try:
+            query = sql_to_string(
+                expressions.select(expressions.Count(this=expressions.Star()))
+                .from_(quoted_identifier(cache_metadata.table_name))
+                .limit(1),
+                source_type=session.source_type,
             )
+            _ = await session.execute_query_long_running(query)
+            return True
+        except session._no_schema_error:  # pylint: disable=protected-access
+            return False
 
-    async def update_online_store(
+    async def create_or_update_feature_table_cache(
         self,
-        feature_store: FeastFeatureStore,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        session: BaseSession,
-    ) -> None:
+        feature_store: FeatureStoreModel,
+        observation_table: ObservationTableModel,
+        graph: QueryGraph,
+        nodes: List[Node],
+        is_target: bool = False,
+        feature_list_id: Optional[PydanticObjectId] = None,
+        serving_names_mapping: Optional[Dict[str, str]] = None,
+        progress_callback: Optional[
+            Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
+        ] = None,
+    ) -> Tuple[List[str], BaseSession]:
         """
-        Update an online store assuming the offline line feature table is already up-to-date. To be
-        called when a catalog switches to a new online store.
+        Create or update feature table cache
 
         Parameters
         ----------
-        feature_store: FeastFeatureStore
-            Feast feature store
-        feature_table_model: OfflineStoreFeatureTableModel
-            Offline store feature table model
-        session: BaseSession
-            Data warehouse session object
-        """
-        if feature_table_model.last_materialized_at is None:
-            # Skip if offline table is not computed yet as nothing to materialize
-            return
-        assert feature_store.online_store_id is not None
-        end_date = pd.Timestamp(
-            await self._get_last_feature_timestamp(session, feature_table_model.name)
-        ).to_pydatetime()
-        start_date = feature_table_model.get_online_store_last_materialized_at(
-            feature_store.online_store_id
-        )
-        await self._materialize_online(
-            feature_store=feature_store,
-            feature_table_model=feature_table_model,
-            columns=feature_table_model.output_column_names,
-            start_date=start_date,
-            end_date=end_date,
-        )
+        feature_store: FeatureStoreModel
+            Feature Store object
+        observation_table: ObservationTableModel
+            Observation table object
+        graph: QueryGraph
+            Graph definition
+        nodes: List[Node]
+            Input node names
+        is_target : bool
+            Whether it is a target computation call
+        feature_list_id: Optional[PydanticObjectId]
+            Optional feature list id
+        serving_names_mapping: Optional[Dict[str, str]]
+            Optional serving names mapping if the observations set has different serving name columns
+            than those defined in Entities
+        progress_callback: Optional[Callable[[int, Optional[str]], Coroutine[Any, Any, None]]]
+            Optional progress callback function
 
-    async def drop_columns(
-        self, feature_table_model: OfflineStoreFeatureTableModel, column_names: List[str]
-    ) -> None:
+        Returns
+        -------
+        Tuple[List[str], BaseSession]
+            Tuple of feature definitions corresponding to nodes, session object
         """
-        Remove columns from the feature table. This is expected to be called when some features in
-        the feature table model were just online disabled.
+        if progress_callback:
+            await progress_callback(1, "Checking feature table cache status")
 
-        Parameters
-        ----------
-        feature_table_model: OfflineStoreFeatureTableModel
-            OfflineStoreFeatureTableModel object
-        column_names: List[str]
-            List of column names to drop
-        """
-        session = await self._get_session(feature_table_model)
-        for column_name in column_names:
-            query = sql_to_string(
-                expressions.AlterTable(
-                    this=expressions.Table(this=quoted_identifier(feature_table_model.name)),
-                    actions=[
-                        expressions.Drop(
-                            this=quoted_identifier(column_name), kind="COLUMN", exists=True
-                        )
-                    ],
-                ),
-                source_type=session.source_type,
+        assert (
+            observation_table.has_row_index
+        ), "Observation Tables without row index are not supported"
+
+        cache_metadata = (
+            await self.feature_table_cache_metadata_service.get_or_create_feature_table_cache(
+                observation_table_id=observation_table.id,
             )
-            await session.execute_query(query)
+        )
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
+        )
+        feature_table_cache_exists = await self._feature_table_cache_exists(
+            cache_metadata, db_session
+        )
 
-    async def drop_table(self, feature_table_model: OfflineStoreFeatureTableModel) -> None:
-        """
-        Drop the feature table. This is expected to be called when the feature table is deleted.
+        hashes = await self.get_feature_definition_hashes(graph, nodes, feature_list_id)
+        non_cached_nodes = await self.get_non_cached_nodes(cache_metadata, nodes, hashes)
 
-        Parameters
-        ----------
-        feature_table_model: OfflineStoreFeatureTableModel
-            OfflineStoreFeatureTableModel object
-        """
-        session = await self._get_session(feature_table_model)
-        await session.drop_table(
-            feature_table_model.name,
-            schema_name=session.schema_name,
-            database_name=session.database_name,
-            if_exists=True,
-        )
-
-    async def _materialize_online(
-        self,
-        feature_store: FeastFeatureStore,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        columns: List[str],
-        end_date: datetime,
-        start_date: Optional[datetime] = None,
-    ) -> None:
-        """
-        Calls materialize_partial and also updates online store last materialized at
+        if progress_callback:
+            await progress_callback(
+                FEATURE_TABLE_CACHE_CHECK_PROGRESS_PERCENTAGE,
+                "Feature table cache status check completed",
+            )
+            remaining_progress_callback = get_ranged_progress_callback(
+                progress_callback, FEATURE_TABLE_CACHE_CHECK_PROGRESS_PERCENTAGE, 100
+            )
+        else:
+            remaining_progress_callback = None
 
-        # noqa: DAR101
-        """
-        if not columns:
-            return
-        await materialize_partial(
-            feature_store=feature_store,
-            feature_view=feature_store.get_feature_view(feature_table_model.name),
-            columns=columns,
-            end_date=end_date,
-            start_date=start_date,
-            with_feature_timestamp=feature_table_model.has_ttl,
-        )
-        assert feature_store.online_store_id is not None
-        await self.offline_store_feature_table_service.update_online_last_materialized_at(
-            document_id=feature_table_model.id,
-            online_store_id=feature_store.online_store_id,
-            last_materialized_at=end_date,
-        )
+        if non_cached_nodes:
+            if feature_table_cache_exists:
+                # if feature table cache exists - update existing table with new features
+                await self._update_table(
+                    feature_store=feature_store,
+                    observation_table=observation_table,
+                    cache_metadata=cache_metadata,
+                    db_session=db_session,
+                    graph=graph,
+                    non_cached_nodes=non_cached_nodes,
+                    is_target=is_target,
+                    serving_names_mapping=serving_names_mapping,
+                    progress_callback=remaining_progress_callback,
+                )
+            else:
+                # if feature table doesn't exist yet - create from scratch
+                await self._create_table(
+                    feature_store=feature_store,
+                    observation_table=observation_table,
+                    db_session=db_session,
+                    final_table_name=cache_metadata.table_name,
+                    graph=graph,
+                    nodes=non_cached_nodes,
+                    is_target=is_target,
+                    serving_names_mapping=serving_names_mapping,
+                    progress_callback=remaining_progress_callback,
+                )
+
+            await self.feature_table_cache_metadata_service.update_feature_table_cache(
+                observation_table_id=observation_table.id,
+                feature_definitions=[definition for _, definition in non_cached_nodes],
+            )
 
-    async def _update_offline_last_materialized_at(
-        self,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        feature_timestamp: datetime,
-    ) -> None:
-        update_schema = OfflineLastMaterializedAtUpdate(last_materialized_at=feature_timestamp)
-        await self.offline_store_feature_table_service.update_document(
-            document_id=feature_table_model.id, data=update_schema
-        )
+        return hashes, db_session
 
-    async def _get_feast_feature_store(self) -> Optional[FeastFeatureStore]:
+    async def read_from_cache(
+        self,
+        feature_store: FeatureStoreModel,
+        observation_table: ObservationTableModel,
+        graph: QueryGraph,
+        nodes: List[Node],
+        columns: Optional[List[str]] = None,
+    ) -> pd.DataFrame:
         """
-        Get the FeastFeatureStore object
+        Given the graph and set of nodes, read respective cached features from feature table cache.
+
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            Feature Store object
+        observation_table: ObservationTableModel
+            Observation table object
+        graph: QueryGraph
+            Graph definition
+        nodes: List[Node]
+            Node names
+        columns: Optional[List[str]]
+            Optional list of columns to read from the cache table
 
         Returns
         -------
-        Optional[FeastFeatureStore]
-            FeastFeatureStore object
+        pd.DataFrame
+            Result data
         """
-        feast_registry = await self.feast_registry_service.get_feast_registry_for_catalog()
-        if feast_registry is None:
-            return None
-        return await self.feast_feature_store_service.get_feast_feature_store(feast_registry.id)
-
-    async def _get_session(self, feature_table_model: OfflineStoreFeatureTableModel) -> BaseSession:
-        assert feature_table_model.feature_cluster is not None, "Missing feature cluster"
-        feature_store = await self.feature_store_service.get_document(
-            document_id=feature_table_model.feature_cluster.feature_store_id
-        )
-        session = await self.session_manager_service.get_feature_store_session(feature_store)
-        return session
-
-    @classmethod
-    async def _create_feature_table(
-        cls,
-        session: BaseSession,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        materialized_features: MaterializedFeatures,
-    ) -> None:
-        adapter = get_sql_adapter(session.source_type)
-        query = sql_to_string(
-            adapter.create_table_as(
-                table_details=TableDetails(
-                    database_name=session.database_name,
-                    schema_name=session.schema_name,
-                    table_name=feature_table_model.name,
-                ),
-                select_expr=expressions.select(
-                    expressions.alias_(
-                        make_literal_value(
-                            materialized_features.feature_timestamp.isoformat(),
-                            cast_as_timestamp=True,
-                        ),
-                        alias=InternalName.FEATURE_TIMESTAMP_COLUMN,
-                        quoted=True,
-                    )
-                )
-                .select(
-                    *[
-                        quoted_identifier(column)
-                        for column in materialized_features.serving_names_and_column_names
-                    ]
-                )
-                .from_(quoted_identifier(materialized_features.materialized_table_name)),
-            ),
-            source_type=session.source_type,
-        )
-        await session.execute_query(query)
-        await cls._add_primary_key_constraint_if_necessary(session, feature_table_model)
-
-    @classmethod
-    async def _add_primary_key_constraint_if_necessary(
-        cls, session: BaseSession, feature_table_model: OfflineStoreFeatureTableModel
-    ) -> None:
-        # Only needed for Databricks with Unity catalog for now
-        if session.source_type != SourceType.DATABRICKS_UNITY:
-            return
-
-        # Table constraint syntax is only supported in newer versions of sqlglot, so the queries are
-        # formatted manually here
-        if len(feature_table_model.serving_names) > 0:
-            primary_key_columns = feature_table_model.serving_names[:]
-        else:
-            primary_key_columns = [DUMMY_ENTITY_COLUMN_NAME]
-        quoted_timestamp_column = f"`{InternalName.FEATURE_TIMESTAMP_COLUMN}`"
-        quoted_primary_key_columns = [f"`{column_name}`" for column_name in primary_key_columns]
-        for quoted_col in [quoted_timestamp_column] + quoted_primary_key_columns:
-            await session.execute_query(
-                f"ALTER TABLE `{feature_table_model.name}` ALTER COLUMN {quoted_col} SET NOT NULL"
-            )
-        primary_key_args = ", ".join(
-            [f"{quoted_timestamp_column} TIMESERIES"] + quoted_primary_key_columns
-        )
-        await session.execute_query(
-            textwrap.dedent(
-                f"""
-                ALTER TABLE `{feature_table_model.name}` ADD CONSTRAINT `pk_{feature_table_model.name}`
-                PRIMARY KEY({primary_key_args})
-                """
-            ).strip()
+        hashes = await self.definition_hashes_for_nodes(graph, nodes)
+        cache_metadata = (
+            await self.feature_table_cache_metadata_service.get_or_create_feature_table_cache(
+                observation_table_id=observation_table.id,
+            )
         )
-
-    @staticmethod
-    async def _insert_into_feature_table(
-        session: BaseSession,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        materialized_features: MaterializedFeatures,
-    ) -> None:
-        query = sql_to_string(
-            expressions.Insert(
-                this=expressions.Schema(
-                    this=expressions.Table(this=quoted_identifier(feature_table_model.name)),
-                    expressions=[quoted_identifier(InternalName.FEATURE_TIMESTAMP_COLUMN)]
-                    + [
-                        quoted_identifier(column)
-                        for column in materialized_features.serving_names_and_column_names
-                    ],
-                ),
-                expression=expressions.select(
-                    expressions.alias_(
-                        make_literal_value(
-                            materialized_features.feature_timestamp.isoformat(),
-                            cast_as_timestamp=True,
-                        ),
-                        alias=InternalName.FEATURE_TIMESTAMP_COLUMN,
-                        quoted=True,
-                    )
-                )
-                .select(
-                    *[
-                        quoted_identifier(column)
-                        for column in materialized_features.serving_names_and_column_names
-                    ]
-                )
-                .from_(quoted_identifier(materialized_features.materialized_table_name)),
-            ),
-            source_type=session.source_type,
+        cached_hashes = set(
+            feature_def.definition_hash for feature_def in cache_metadata.feature_definitions
         )
-        await session.execute_query(query)
+        assert set(hashes) <= cached_hashes, "All nodes must be cached"
 
-    @staticmethod
-    async def _get_last_feature_timestamp(session: BaseSession, feature_table_name: str) -> str:
-        query = sql_to_string(
-            expressions.select(
-                expressions.alias_(
-                    expressions.Max(
-                        this=expressions.Column(
-                            this=quoted_identifier(InternalName.FEATURE_TIMESTAMP_COLUMN)
-                        )
-                    ),
-                    alias="RESULT",
-                )
-            ).from_(quoted_identifier(feature_table_name)),
-            source_type=session.source_type,
+        cached_features = {
+            feature.definition_hash: feature.feature_name
+            for feature in cache_metadata.feature_definitions
+        }
+        db_session = await self.session_manager_service.get_feature_store_session(
+            feature_store=feature_store
         )
-        result = await session.execute_query(query)
-        return str(result["RESULT"].iloc[0])  # type: ignore
-
-    @staticmethod
-    async def _merge_into_feature_table(
-        session: BaseSession,
-        feature_table_model: OfflineStoreFeatureTableModel,
-        materialized_features: MaterializedFeatures,
-        feature_timestamp_value: str,
-        to_specify_merge_conditions: bool,
-    ) -> None:
-        feature_timestamp_value_expr = expressions.Anonymous(
-            this="TO_TIMESTAMP", expressions=[make_literal_value(feature_timestamp_value)]
+        columns_expr = self._get_column_exprs(
+            graph, nodes, hashes, cast(Dict[str, str], cached_features)
         )
-        if to_specify_merge_conditions:
-            merge_conditions = [
-                expressions.EQ(
-                    this=get_qualified_column_identifier(serving_name, "offline_store_table"),
-                    expression=get_qualified_column_identifier(
-                        serving_name, "materialized_features"
-                    ),
-                )
-                for serving_name in feature_table_model.serving_names
-            ] + [
-                expressions.EQ(
-                    this=quoted_identifier(InternalName.FEATURE_TIMESTAMP_COLUMN),
-                    expression=feature_timestamp_value_expr,
-                ),
-            ]
-        else:
-            merge_conditions = []
-
-        update_expr = expressions.Update(
-            expressions=[
-                expressions.EQ(
-                    this=get_qualified_column_identifier(column, "offline_store_table"),
-                    expression=get_qualified_column_identifier(column, "materialized_features"),
+        additional_columns_expr = [quoted_identifier(col) for col in columns] if columns else []
+        select_expr = (
+            expressions.select(
+                quoted_identifier(InternalName.TABLE_ROW_INDEX), *additional_columns_expr
+            )
+            .select(*columns_expr)
+            .from_(
+                get_fully_qualified_table_name(
+                    {
+                        "database_name": db_session.database_name,
+                        "schema_name": db_session.schema_name,
+                        "table_name": cache_metadata.table_name,
+                    }
                 )
-                for column in materialized_features.column_names
-            ]
-        )
-        insert_expr = expressions.Insert(
-            this=expressions.Tuple(
-                expressions=[
-                    quoted_identifier(col)
-                    for col in [InternalName.FEATURE_TIMESTAMP_COLUMN.value]
-                    + materialized_features.serving_names_and_column_names
-                ]
-            ),
-            expression=expressions.Tuple(
-                expressions=[feature_timestamp_value_expr]
-                + [
-                    get_qualified_column_identifier(col, "materialized_features")
-                    for col in materialized_features.serving_names_and_column_names
-                ]
-            ),
-        )
-        merge_expr = expressions.Merge(
-            this=expressions.Table(
-                this=quoted_identifier(feature_table_model.name),
-                alias=expressions.TableAlias(this="offline_store_table"),
-            ),
-            using=expressions.select(
-                *[
-                    quoted_identifier(column)
-                    for column in materialized_features.serving_names_and_column_names
-                ]
             )
-            .from_(quoted_identifier(materialized_features.materialized_table_name))
-            .subquery(alias="materialized_features"),
-            on=expressions.and_(*merge_conditions) if merge_conditions else expressions.false(),
-            expressions=[
-                expressions.When(
-                    this=expressions.Column(this=expressions.Identifier(this="MATCHED")),
-                    then=update_expr,
-                ),
-                expressions.When(
-                    this=expressions.Not(
-                        this=expressions.Column(this=expressions.Identifier(this="MATCHED"))
-                    ),
-                    then=insert_expr,
-                ),
-            ],
         )
-        query = sql_to_string(merge_expr, source_type=session.source_type)
+        sql = sql_to_string(select_expr, source_type=db_session.source_type)
+        return await db_session.execute_query_long_running(sql)
 
-        await session.execute_query(query)
+    async def create_view_or_table_from_cache(
+        self,
+        feature_store: FeatureStoreModel,
+        observation_table: ObservationTableModel,
+        graph: QueryGraph,
+        nodes: List[Node],
+        output_view_details: TableDetails,
+        is_target: bool,
+        feature_list_id: Optional[PydanticObjectId] = None,
+        serving_names_mapping: Optional[Dict[str, str]] = None,
+        progress_callback: Optional[
+            Callable[[int, Optional[str]], Coroutine[Any, Any, None]]
+        ] = None,
+    ) -> bool:
+        """
+        Create or update cache table and create a new view which refers to the cached table
 
-    @classmethod
-    async def _num_rows_in_feature_table(
-        cls,
-        session: BaseSession,
-        feature_table_model: OfflineStoreFeatureTableModel,
-    ) -> Optional[int]:
-        try:
-            query = sql_to_string(
-                expressions.select(expressions.Count(this=expressions.Star())).from_(
-                    quoted_identifier(feature_table_model.name)
-                ),
-                source_type=session.source_type,
-            )
-            result = await session.execute_query(query)
-            return cast(int, result.iloc[0][0])  # type: ignore[union-attr]
-        except session._no_schema_error:  # pylint: disable=protected-access
-            return None
+        Parameters
+        ----------
+        feature_store: FeatureStoreModel
+            Feature Store object
+        observation_table: ObservationTableModel
+            Observation table object
+        graph: QueryGraph
+            Graph definition
+        nodes: List[Node]
+            Input node names
+        output_view_details: TableDetails
+            Output table details
+        is_target : bool
+            Whether it is a target computation call
+        feature_list_id: Optional[PydanticObjectId]
+            Optional feature list id
+        serving_names_mapping: Optional[Dict[str, str]]
+            Optional serving names mapping if the observations set has different serving name columns
+            than those defined in Entities
+        progress_callback: Optional[Callable[[int, Optional[str]], Coroutine[Any, Any, None]]]
+            Optional progress callback function
 
-    @classmethod
-    async def _ensure_compatible_schema(
-        cls,
-        session: BaseSession,
-        feature_table_model: OfflineStoreFeatureTableModel,
-    ) -> List[str]:
-        existing_columns = [
-            c.upper()
-            for c in (
-                await session.list_table_schema(
-                    feature_table_model.name, session.database_name, session.schema_name
+        Returns
+        -------
+        bool
+            Whether the output is a view
+        """
+        with timer(
+            "Update feature table cache",
+            logger,
+            extra={"catalog_id": str(observation_table.catalog_id)},
+        ):
+            hashes, db_session = await self.create_or_update_feature_table_cache(
+                feature_store=feature_store,
+                observation_table=observation_table,
+                graph=graph,
+                nodes=nodes,
+                is_target=is_target,
+                feature_list_id=feature_list_id,
+                serving_names_mapping=serving_names_mapping,
+                progress_callback=progress_callback,
+            )
+        cache_metadata = (
+            await self.feature_table_cache_metadata_service.get_or_create_feature_table_cache(
+                observation_table_id=observation_table.id,
+            )
+        )
+        cached_features = {
+            feature.definition_hash: feature.feature_name
+            for feature in cache_metadata.feature_definitions
+        }
+
+        request_column_names = [col.name for col in observation_table.columns_info]
+        request_columns = [quoted_identifier(col) for col in request_column_names]
+        columns_expr = self._get_column_exprs(
+            graph, nodes, hashes, cast(Dict[str, str], cached_features)
+        )
+        select_expr = (
+            expressions.select(quoted_identifier(InternalName.TABLE_ROW_INDEX))
+            .select(*request_columns)
+            .select(*columns_expr)
+            .from_(
+                get_fully_qualified_table_name(
+                    {
+                        "database_name": db_session.database_name,
+                        "schema_name": db_session.schema_name,
+                        "table_name": cache_metadata.table_name,
+                    }
                 )
-            ).keys()
-        ]
-        new_columns = [
-            col
-            for col in feature_table_model.output_column_names
-            if col.upper() not in existing_columns
-        ]
-
-        if new_columns:
-            adapter = get_sql_adapter(session.source_type)
-            query = adapter.alter_table_add_columns(
-                expressions.Table(this=quoted_identifier(feature_table_model.name)),
-                cls._get_column_defs(feature_table_model, new_columns, adapter),
             )
-            await session.execute_query(query)
-
-        return new_columns
-
-    @staticmethod
-    def _get_column_defs(
-        feature_table_model: OfflineStoreFeatureTableModel,
-        columns: List[str],
-        adapter: BaseAdapter,
-    ) -> List[expressions.ColumnDef]:
-        columns_and_types = {}
-        for column_name, column_data_type in zip(
-            feature_table_model.output_column_names, feature_table_model.output_dtypes
-        ):
-            if column_name in columns:
-                columns_and_types[column_name] = column_data_type
-
-        return [
-            expressions.ColumnDef(
-                this=quoted_identifier(column_name),
-                kind=adapter.get_physical_type_from_dtype(column_data_type),
+        )
+        try:
+            await db_session.create_table_as(
+                table_details=output_view_details,
+                select_expr=select_expr,
+                kind="VIEW",
+            )
+            return True
+        except:  # pylint: disable=bare-except
+            logger.info(
+                "Failed to create view. Trying to create a table instead",
+                extra={"observation_table_id": observation_table.id},
+                exc_info=True,
+            )
+            await db_session.create_table_as(
+                table_details=output_view_details,
+                select_expr=select_expr,
             )
-            for column_name, column_data_type in columns_and_types.items()
-        ]
+            return False
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_materialize_scheduler.py` & `featurebyte-1.0.3/featurebyte/service/feature_materialize_scheduler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureMaterializeSchedulerService class
 """
+
 from typing import Optional
 
 from bson import ObjectId
 
 from featurebyte.logging import get_logger
 from featurebyte.models.base import User
 from featurebyte.models.offline_store_feature_table import OfflineStoreFeatureTableModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_namespace.py` & `featurebyte-1.0.3/featurebyte/service/feature_namespace.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureNamespaceService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.feature_namespace import FeatureNamespaceModel
 from featurebyte.schema.feature_namespace import (
     FeatureNamespaceCreate,
     FeatureNamespaceServiceUpdate,
 )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_offline_store_info.py` & `featurebyte-1.0.3/featurebyte/service/feature_offline_store_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 """
 Feature Offline Store Info Initialization Service
 """
+
 from typing import Dict, List, Optional, Sequence, Tuple
 
 from bson import ObjectId
 
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.offline_store_ingest_query import (
     OfflineStoreInfo,
     OfflineStoreInfoMetadata,
     ServingNameInfo,
 )
 from featurebyte.query_graph.enum import GraphNodeType, NodeType
 from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.model.entity_lookup_plan import EntityLookupPlanner
+from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.query_graph.model.graph import QueryGraphModel
 from featurebyte.query_graph.node.base import BaseNode
 from featurebyte.query_graph.node.nested import OfflineStoreIngestQueryGraphNodeParameters
 from featurebyte.query_graph.transform.decompose_point import FeatureJobSettingExtractor
 from featurebyte.query_graph.transform.null_filling_value import NullFillingValueExtractor
 from featurebyte.query_graph.transform.offline_store_ingest import (
@@ -233,16 +236,19 @@
         result = transformer.transform(
             target_node=feature.node,
             relationships_info=feature.relationships_info or [],
             feature_name=feature.name,
             feature_version=feature.version.to_str(),
         )
 
-        null_filling_value_extractor = NullFillingValueExtractor(graph=feature.graph)
-        null_filling_value_state = null_filling_value_extractor.extract(node=feature.node)
+        null_filling_value = None
+        if not dry_run:
+            null_filling_value = (
+                NullFillingValueExtractor(graph=feature.graph).extract(node=feature.node).fill_value
+            )
 
         if result.is_decomposed:
             decomposed_graph, output_node_name = await self.reconstruct_decomposed_graph(
                 graph=result.graph,
                 node_name=result.node_name_map[feature.node.name],
                 catalog_id=feature.catalog_id,
                 table_name_prefix=table_name_prefix,
@@ -285,32 +291,74 @@
                 offline_store_table_name=table_name,
                 output_column_name=feature.versioned_name,
                 output_dtype=feature.dtype,
                 primary_entity_ids=feature.primary_entity_ids,
                 primary_entity_dtypes=[
                     entity_id_to_dtype[entity_id] for entity_id in feature.primary_entity_ids
                 ],
-                null_filling_value=null_filling_value_state.fill_value,
+                null_filling_value=null_filling_value,
             )
 
         # populate offline store info
         offline_store_info = OfflineStoreInfo(
             graph=decomposed_graph,
             node_name=output_node_name,
             node_name_map=result.node_name_map,
             is_decomposed=result.is_decomposed,
             metadata=metadata,
             serving_names_info=[
                 ServingNameInfo(serving_name=serving_name, entity_id=entity_id)
                 for entity_id, serving_name in entity_id_to_serving_name.items()
             ],
         )
-        offline_store_info.initialize(
-            feature_versioned_name=feature.versioned_name,
-            feature_dtype=feature.dtype,
-            feature_job_settings=[
-                setting.feature_job_setting for setting in feature.table_id_feature_job_settings
-            ],
-            feature_id=feature.id,
-            null_filling_value=null_filling_value_state.fill_value,
-        )
+        if not dry_run:
+            offline_store_info.initialize(
+                feature_versioned_name=feature.versioned_name,
+                feature_dtype=feature.dtype,
+                feature_job_settings=[
+                    setting.feature_job_setting for setting in feature.table_id_feature_job_settings
+                ],
+                feature_id=feature.id,
+                null_filling_value=null_filling_value,
+            )
+
         return offline_store_info
+
+    async def get_entity_join_steps_for_feature_table(
+        self, feature: FeatureModel, entity_id_to_serving_name: Dict[ObjectId, str]
+    ) -> List[EntityRelationshipInfo]:
+        """
+        Get entity join steps for feature table
+
+        Parameters
+        ----------
+        feature: FeatureModel
+            Feature
+        entity_id_to_serving_name: Dict[ObjectId, str]
+            Entity id to serving name mapping
+
+        Returns
+        -------
+        List[EntityRelationshipInfo]
+        """
+        if feature.entity_join_steps is not None:
+            return feature.entity_join_steps
+
+        # derive entity join steps for old features which don't have entity_join_steps
+        feature_tables_entity_ids = await self.get_offline_store_feature_tables_entity_ids(
+            feature, entity_id_to_serving_name
+        )
+
+        entity_join_steps = []
+        for entity_ids in feature_tables_entity_ids:
+            if feature.relationships_info is None:
+                continue
+            internal_steps = EntityLookupPlanner.generate_lookup_steps(
+                available_entity_ids=feature.primary_entity_ids,
+                required_entity_ids=entity_ids,
+                relationships_info=feature.relationships_info,
+            )
+            for step in internal_steps:
+                if step not in entity_join_steps:
+                    entity_join_steps.append(step)
+
+        return entity_join_steps
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_preview.py` & `featurebyte-1.0.3/featurebyte/service/feature_preview.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeaturePreviewService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Tuple
 
 import os
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_readiness.py` & `featurebyte-1.0.3/featurebyte/service/feature_readiness.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureReadinessService
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Sequence
 
 from bson.objectid import ObjectId
 from pymongo.errors import OperationFailure
 from tenacity import retry, retry_if_exception_type, wait_chain, wait_random
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_store.py` & `featurebyte-1.0.3/featurebyte/service/feature_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 FeatureStoreService class
 """
+
 from __future__ import annotations
 
 from typing import Type
 
 from bson import ObjectId
 
 from featurebyte.models.feature_store import FeatureStoreModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_store_warehouse.py` & `featurebyte-1.0.3/featurebyte/service/offline_store_feature_table.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,431 +1,380 @@
 """
-Service for interacting with the data warehouse for queries around the feature store.
-
-We split this into a separate service, as these typically require a session object that is created.
+OfflineStoreFeatureTableService class
 """
-from __future__ import annotations
 
-from typing import Any, AsyncGenerator, List, Optional, Tuple
+from __future__ import annotations
 
-import os
+from typing import Any, AsyncIterator, Dict, Optional
 
-from featurebyte.common.utils import dataframe_to_json
-from featurebyte.enum import InternalName, MaterializedTableNamePrefix
-from featurebyte.exception import (
-    DatabaseNotFoundError,
-    LimitExceededError,
-    SchemaNotFoundError,
-    TableNotFoundError,
-)
-from featurebyte.logging import get_logger
-from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.user_defined_function import UserDefinedFunctionModel
-from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
-from featurebyte.query_graph.model.common_table import TabularSource
-from featurebyte.query_graph.model.table import TableDetails, TableSpec
-from featurebyte.query_graph.sql.common import quoted_identifier, sql_to_string
-from featurebyte.query_graph.sql.materialisation import (
-    get_feature_store_id_expr,
-    get_source_count_expr,
-    get_source_expr,
-)
-from featurebyte.schema.feature_store import FeatureStoreShape
-from featurebyte.service.feature_store import FeatureStoreService
-from featurebyte.service.session_manager import SessionManagerService
-from featurebyte.session.base import INTERACTIVE_SESSION_TIMEOUT_SECONDS, BaseSession
+from datetime import datetime
+from pathlib import Path
 
-MAX_TABLE_CELLS = int(
-    os.environ.get("MAX_TABLE_CELLS", 10000000 * 300)
-)  # 10 million rows, 300 columns
+from bson import ObjectId, json_util
+from redis.lock import Lock
 
+from featurebyte.models.feature_list import FeatureCluster
+from featurebyte.models.offline_store_feature_table import (
+    FeaturesUpdate,
+    OfflineStoreFeatureTableModel,
+    OfflineStoreFeatureTableUpdate,
+    OnlineStoreLastMaterializedAt,
+    OnlineStoresLastMaterializedAtUpdate,
+)
+from featurebyte.service.base_document import BaseDocumentService
 
-logger = get_logger(__name__)
+OFFLINE_STORE_FEATURE_TABLE_REDIS_LOCK_TIMEOUT = 120  # a maximum life for the lock in seconds
 
 
-class FeatureStoreWarehouseService:
+class OfflineStoreFeatureTableService(
+    BaseDocumentService[
+        OfflineStoreFeatureTableModel, OfflineStoreFeatureTableModel, OfflineStoreFeatureTableUpdate
+    ]
+):
     """
-    FeatureStoreWarehouseService is responsible for interacting with the data warehouse.
+    OfflineStoreFeatureTableService class
     """
 
-    def __init__(
-        self,
-        session_manager_service: SessionManagerService,
-        feature_store_service: FeatureStoreService,
-    ):
-        self.session_manager_service = session_manager_service
-        self.feature_store_service = feature_store_service
+    document_class = OfflineStoreFeatureTableModel
+    document_update_class = OfflineStoreFeatureTableUpdate
 
-    async def check_user_defined_function_exists(
-        self,
-        user_defined_function: UserDefinedFunctionModel,
-        feature_store: FeatureStoreModel,
-    ) -> None:
+    def get_feature_cluster_storage_lock(self, document_id: ObjectId, timeout: int) -> Lock:
         """
-        Check whether user defined function in feature store
+        Get feature cluster storage lock
 
         Parameters
         ----------
-        user_defined_function: UserDefinedFunctionModel
-            User defined function model
-        feature_store: FeatureStoreModel
-            Feature store model
+        document_id: ObjectId
+            OfflineStoreFeatureTableModel id
+        timeout: int
+            Maximum life for the lock in seconds
+
+        Returns
+        -------
+        Lock
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store
-        )
-        await db_session.check_user_defined_function(user_defined_function=user_defined_function)
+        return self.redis.lock(f"offline_store_feature_table_update:{document_id}", timeout=timeout)
 
-    async def list_databases(
-        self, feature_store: FeatureStoreModel, get_credential: Optional[Any]
-    ) -> List[str]:
+    async def get_or_create_document(
+        self, data: OfflineStoreFeatureTableModel
+    ) -> OfflineStoreFeatureTableModel:
         """
-        List databases in feature store
+        Get or create an offline store feature table
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        get_credential: Optional[Any]
-            Get credential handler function
+        data: OfflineStoreFeatureTableModel
+            Offline store feature table model
 
         Returns
         -------
-        List[str]
-            List of database names
+        OfflineStoreFeatureTableModel
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, get_credential=get_credential
+        query_result = await self.list_documents_as_dict(
+            query_filter=data.table_signature, page_size=1
         )
-        return await db_session.list_databases()
+        if query_result["total"]:
+            return OfflineStoreFeatureTableModel(**query_result["data"][0])
+        return await self.create_document(data)
+
+    async def _populate_remote_attributes(
+        self, document: OfflineStoreFeatureTableModel
+    ) -> OfflineStoreFeatureTableModel:
+        if document.feature_cluster_path:
+            feature_cluster_json = await self.storage.get_text(Path(document.feature_cluster_path))
+            feature_cluster_dict = json_util.loads(feature_cluster_json)
+            document.feature_cluster = FeatureCluster(**feature_cluster_dict)
+        return document
+
+    async def _move_feature_cluster_to_storage(
+        self, document: OfflineStoreFeatureTableModel
+    ) -> OfflineStoreFeatureTableModel:
+        feature_cluster_path = self.get_full_remote_file_path(
+            f"offline_store_feature_table/{document.id}/feature_cluster.json"
+        )
+        assert isinstance(document.feature_cluster, FeatureCluster)
+        await self.storage.put_text(
+            json_util.dumps(document.feature_cluster.json_dict()), feature_cluster_path
+        )
+        document.feature_cluster_path = str(feature_cluster_path)
+        document.feature_cluster = None
+        return document
+
+    async def _create_document(
+        self, data: OfflineStoreFeatureTableModel
+    ) -> OfflineStoreFeatureTableModel:
+        if data.entity_lookup_info is None and data.precomputed_lookup_feature_table_info is None:
+            # check if name already exists
+            data.base_name = data.get_basename()
+            data.name = data.get_name()
+            query_filter = {
+                "name_prefix": data.name_prefix,
+                "base_name": data.base_name,
+                "precomputed_lookup_feature_table_info": None,
+            }
+            query_result = await self.list_documents_as_dict(query_filter=query_filter, page_size=1)
+
+            count = query_result["total"]
+            data.name_suffix = None
+            if count:
+                # if name already exists, append a suffix
+                data.name_suffix = str(count)
+                data.name = data.get_name()
 
-    async def list_schemas(
-        self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-    ) -> List[str]:
+        if data.feature_cluster is not None:
+            data = await self._move_feature_cluster_to_storage(data)
+
+        try:
+            output = await super().create_document(data)
+            assert output.catalog_id == data.catalog_id
+            return output
+        except Exception as exc:
+            if data.feature_cluster_path:
+                await self.storage.delete(Path(data.feature_cluster_path))
+            raise exc
+
+    async def create_document(
+        self, data: OfflineStoreFeatureTableModel
+    ) -> OfflineStoreFeatureTableModel:
         """
-        List schemas in feature store
+        Create a new document
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-
-        Raises
-        ------
-        DatabaseNotFoundError
-            If database not found
+        data : OfflineStoreFeatureTableModel
+            Document data
 
         Returns
         -------
-        List[str]
-            List of schema names
+        OfflineStoreFeatureTableModel
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store
-        )
-        try:
-            return await db_session.list_schemas(database_name=database_name)
-        except db_session.no_schema_error as exc:
-            raise DatabaseNotFoundError(f"Database {database_name} not found.") from exc
-
-    @staticmethod
-    def _is_visible_table(table_name: str, filter_featurebyte_tables: bool) -> bool:
-        if table_name.startswith("__"):
-            return False
-        if not filter_featurebyte_tables:
-            return True
-        # quick filter for materialized tables
-        if "TABLE" not in table_name:
-            return False
-        for prefix in MaterializedTableNamePrefix.visible():
-            if table_name.startswith(prefix):
-                return True
-        return False
-
-    @staticmethod
-    async def _is_featurebyte_schema(
-        db_session: BaseSession, database_name: str, schema_name: str
-    ) -> bool:
-        try:
-            sql_expr = get_feature_store_id_expr(
-                database_name=database_name, schema_name=schema_name
-            )
-            sql = sql_to_string(
-                sql_expr,
-                source_type=db_session.source_type,
+        with self.get_feature_cluster_storage_lock(
+            data.id, timeout=OFFLINE_STORE_FEATURE_TABLE_REDIS_LOCK_TIMEOUT
+        ):
+            return await self._create_document(data)
+
+    async def _update_offline_feature_table(
+        self,
+        document_id: ObjectId,
+        data: OfflineStoreFeatureTableUpdate,
+        exclude_none: bool = True,
+        skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
+    ) -> OfflineStoreFeatureTableModel:
+        original_doc = await self.get_document(
+            document_id=document_id, populate_remote_attributes=False
+        )
+        if isinstance(data, FeaturesUpdate):
+            assert (
+                data.feature_cluster_path is None
+            ), "feature_cluster_path should not be set in update"
+            if original_doc.feature_cluster_path:
+                # attempt to remove the old feature cluster
+                await self.storage.try_delete_if_exists(Path(original_doc.feature_cluster_path))
+
+            table = OfflineStoreFeatureTableModel(
+                **{**original_doc.dict(by_alias=True), **data.dict(by_alias=True)}
             )
-            _ = await db_session.execute_query(sql)
-            return True
-        except db_session.no_schema_error:
-            return False
+            table = await self._move_feature_cluster_to_storage(table)
+            data.feature_cluster = None
+            data.feature_cluster_path = table.feature_cluster_path
+
+        output = await super().update_document(
+            document_id,
+            data,
+            exclude_none=exclude_none,
+            document=original_doc,
+            skip_block_modification_check=skip_block_modification_check,
+            populate_remote_attributes=populate_remote_attributes,
+        )
+        assert output is not None
+        return output
+
+    async def update_document(
+        self,
+        document_id: ObjectId,
+        data: OfflineStoreFeatureTableUpdate,
+        exclude_none: bool = True,
+        document: Optional[OfflineStoreFeatureTableModel] = None,
+        return_document: bool = True,
+        skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
+    ) -> Optional[OfflineStoreFeatureTableModel]:
+        if isinstance(data, FeaturesUpdate):
+            with self.get_feature_cluster_storage_lock(
+                document_id, timeout=OFFLINE_STORE_FEATURE_TABLE_REDIS_LOCK_TIMEOUT
+            ):
+                return await self._update_offline_feature_table(
+                    document_id,
+                    data,
+                    exclude_none=exclude_none,
+                    skip_block_modification_check=skip_block_modification_check,
+                    populate_remote_attributes=populate_remote_attributes,
+                )
+
+        return await self._update_offline_feature_table(
+            document_id,
+            data,
+            exclude_none=exclude_none,
+            skip_block_modification_check=skip_block_modification_check,
+            populate_remote_attributes=populate_remote_attributes,
+        )
 
-    async def list_tables(
+    async def update_online_last_materialized_at(
         self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-        schema_name: str,
-    ) -> List[TableSpec]:
+        document_id: ObjectId,
+        online_store_id: ObjectId,
+        last_materialized_at: datetime,
+    ) -> None:
         """
-        List tables in feature store
+        Update the last materialized at timestamp for the given online_store_id
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-
-        Raises
-        ------
-        SchemaNotFoundError
-            If schema not found
-
-        Returns
-        -------
-        List[TableSpec]
-            List of tables
-        """
-
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store
+        document_id: ObjectId
+            OfflineStoreFeatureTableModel id
+        online_store_id: ObjectId
+            Online store id
+        last_materialized_at: datetime
+            Last materialized at timestamp to use
+        """
+        document = await self.get_document(
+            document_id=document_id, populate_remote_attributes=False
+        )
+        new_entry = OnlineStoreLastMaterializedAt(
+            online_store_id=online_store_id,
+            value=last_materialized_at,
+        )
+        updated_online_stores_last_materialized_at = [new_entry] + [
+            entry
+            for entry in document.online_stores_last_materialized_at
+            if entry.online_store_id != online_store_id
+        ]
+        update_schema = OnlineStoresLastMaterializedAtUpdate(
+            online_stores_last_materialized_at=updated_online_stores_last_materialized_at
         )
-        is_featurebyte_schema = await self._is_featurebyte_schema(
-            db_session, database_name, schema_name
+        await self.update_document(
+            document_id,
+            update_schema,
+            document=document,
         )
-        try:
-            tables = await db_session.list_tables(
-                database_name=database_name, schema_name=schema_name
-            )
-        except db_session.no_schema_error as exc:
-            raise SchemaNotFoundError(f"Schema {schema_name} not found.") from exc
-
-        return [
-            table for table in tables if self._is_visible_table(table.name, is_featurebyte_schema)
-        ]
 
-    async def list_columns(
-        self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-        schema_name: str,
-        table_name: str,
-    ) -> List[ColumnSpecWithDescription]:
+    async def add_deployment_id(self, document_id: ObjectId, deployment_id: ObjectId) -> None:
         """
-        List columns in database table
+        Add deployment id to the offline store feature table
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-        table_name: str
-            Name of table to use
-
-        Raises
-        ------
-        TableNotFoundError
-            If table not found
-
-        Returns
-        -------
-        List[ColumnSpecWithDescription]
-            List of ColumnSpecWithDescription object
+        document_id: ObjectId
+            Offline store feature table id
+        deployment_id: ObjectId
+            Deployment id
         """
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store
+        await self.update_documents(
+            query_filter={"_id": document_id},
+            update={"$addToSet": {"deployment_ids": deployment_id}},
         )
 
-        try:
-            table_schema = await db_session.list_table_schema(
-                database_name=database_name, schema_name=schema_name, table_name=table_name
-            )
-        except db_session.no_schema_error as exc:
-            raise TableNotFoundError(f"Table {table_name} not found.") from exc
-
-        table_schema = {  # type: ignore[assignment]
-            col_name: v
-            for (col_name, v) in table_schema.items()
-            if col_name != InternalName.TABLE_ROW_INDEX
-        }
-        return list(table_schema.values())
-
-    async def get_table_details(
-        self,
-        feature_store: FeatureStoreModel,
-        database_name: str,
-        schema_name: str,
-        table_name: str,
-    ) -> TableDetails:
+    async def remove_deployment_id(self, document_id: ObjectId, deployment_id: ObjectId) -> None:
         """
-        Get table details
+        Remove deployment id from the offline store feature table
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            FeatureStoreModel object
-        database_name: str
-            Name of database to use
-        schema_name: str
-            Name of schema to use
-        table_name: str
-            Name of table to use
-
-        Raises
-        ------
-        TableNotFoundError
-            If table not found
-
-        Returns
-        -------
-        TableDetails
+        document_id: ObjectId
+            Offline store feature table id
+        deployment_id: ObjectId
+            Deployment id
         """
-
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store
+        await self.update_documents(
+            query_filter={"_id": document_id},
+            update={"$pull": {"deployment_ids": deployment_id}},
         )
-        try:
-            return await db_session.get_table_details(
-                database_name=database_name, schema_name=schema_name, table_name=table_name
-            )
-        except db_session.no_schema_error as exc:
-            raise TableNotFoundError(f"Table {table_name} not found.") from exc
 
-    async def _get_table_shape(
-        self, location: TabularSource, db_session: BaseSession
-    ) -> Tuple[Tuple[int, int], bool, list[str]]:
-        # check size of the table
-        sql_expr = get_source_count_expr(source=location.table_details)
-        sql = sql_to_string(
-            sql_expr,
-            source_type=db_session.source_type,
-        )
-        result = await db_session.execute_query(sql)
-        assert result is not None
-        columns_specs = await db_session.list_table_schema(**location.table_details.json_dict())
-        has_row_index = InternalName.TABLE_ROW_INDEX in columns_specs
-        columns = [
-            col_name
-            for col_name in columns_specs.keys()
-            if col_name != InternalName.TABLE_ROW_INDEX
-        ]
-        return (
-            (result["row_count"].iloc[0], len(columns)),
-            has_row_index,
-            columns,
-        )
+    async def list_deprecated_entity_lookup_feature_tables_as_dict(
+        self,
+    ) -> AsyncIterator[Dict[str, Any]]:
+        """
+        Retrieve entity lookup feature tables that deprecated for clean up purpose
 
-    async def table_shape(self, location: TabularSource) -> FeatureStoreShape:
+        Yields
+        -------
+        AsyncIterator[OfflineStoreFeatureTableModel]
+            List query output
         """
-        Get the shape table from location.
+        async for doc in self.list_documents_as_dict_iterator(
+            query_filter={"entity_lookup_info": {"$ne": None}},
+            projection={"_id": 1, "name": 1},
+        ):
+            yield doc
+
+    async def list_source_feature_tables_for_deployment(
+        self, deployment_id: ObjectId
+    ) -> AsyncIterator[OfflineStoreFeatureTableModel]:
+        """
+        Retrieve list of source feature tables in the catalog
 
         Parameters
         ----------
-        location: TabularSource
-            Location to get shape from
+        deployment_id: ObjectId
+            Deployment id to search for
 
-        Returns
+        Yields
         -------
-        FeatureStoreShape
-            Row and column counts
+        AsyncIterator[OfflineStoreFeatureTableModel]
+            List query output
         """
-        feature_store = await self.feature_store_service.get_document(
-            document_id=location.feature_store_id
-        )
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
-        )
-        shape, _, _ = await self._get_table_shape(location, db_session)
-        return FeatureStoreShape(num_rows=shape[0], num_cols=shape[1])
+        async for doc in self.list_documents_iterator(
+            query_filter={
+                "precomputed_lookup_feature_table_info": {"$eq": None},
+                "deployment_ids": deployment_id,
+            }
+        ):
+            yield doc
 
-    async def table_preview(self, location: TabularSource, limit: int) -> dict[str, Any]:
+    async def list_precomputed_lookup_feature_tables_from_source(
+        self,
+        source_feature_table_id: ObjectId,
+    ) -> AsyncIterator[OfflineStoreFeatureTableModel]:
         """
-        Preview table from location.
+        Retrieve list of precomputed lookup feature tables associated with a source feature table
 
         Parameters
         ----------
-        location: TabularSource
-            Location to preview from
-        limit: int
-            Row limit on preview results
+        source_feature_table_id: ObjectId
+            Feature table id to search for
 
-        Returns
+        Yields
         -------
-        dict[str, Any]
-            Dataframe converted to json string
+        AsyncIterator[OfflineStoreFeatureTableModel]
+            List query output
         """
-        feature_store = await self.feature_store_service.get_document(
-            document_id=location.feature_store_id
-        )
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
-        )
-        sql_expr = get_source_expr(source=location.table_details).limit(limit)
-        sql = sql_to_string(
-            sql_expr,
-            source_type=db_session.source_type,
-        )
-        result = await db_session.execute_query(sql)
-
-        # drop row index column if present
-        if result is not None and InternalName.TABLE_ROW_INDEX in result.columns:
-            result.drop(columns=[InternalName.TABLE_ROW_INDEX], inplace=True)
-
-        return dataframe_to_json(result)
-
-    async def download_table(
-        self,
-        location: TabularSource,
-    ) -> Optional[AsyncGenerator[bytes, None]]:
+        async for doc in self.list_documents_iterator(
+            query_filter={
+                "precomputed_lookup_feature_table_info.source_feature_table_id": source_feature_table_id
+            }
+        ):
+            yield doc
+
+    async def list_precomputed_lookup_feature_tables_for_deployment(
+        self, deployment_id: ObjectId
+    ) -> AsyncIterator[OfflineStoreFeatureTableModel]:
         """
-        Download table from location.
+        Retrieve list of precomputed lookup feature tables in the catalog
 
         Parameters
         ----------
-        location: TabularSource
-            Location to download from
+        deployment_id: ObjectId
+            Deployment id to search for
 
-        Returns
+        Yields
         -------
-        AsyncGenerator[bytes, None]
-            Asynchronous bytes generator
-
-        Raises
-        ------
-        LimitExceededError
-            Table size exceeds the limit.
+        AsyncIterator[OfflineStoreFeatureTableModel]
+            List query output
         """
-        feature_store = await self.feature_store_service.get_document(
-            document_id=location.feature_store_id
-        )
-        db_session = await self.session_manager_service.get_feature_store_session(
-            feature_store=feature_store, timeout=INTERACTIVE_SESSION_TIMEOUT_SECONDS
-        )
-
-        shape, has_row_index, columns = await self._get_table_shape(location, db_session)
-        logger.debug(
-            "Downloading table from feature store",
-            extra={
-                "location": location.json_dict(),
-                "shape": shape,
-            },
-        )
-
-        if shape[0] * shape[1] > MAX_TABLE_CELLS:
-            raise LimitExceededError(f"Table size {shape} exceeds download limit.")
-
-        sql_expr = get_source_expr(source=location.table_details, column_names=columns)
-        if has_row_index:
-            sql_expr = sql_expr.order_by(quoted_identifier(InternalName.TABLE_ROW_INDEX))
-        sql = sql_to_string(
-            sql_expr,
-            source_type=db_session.source_type,
-        )
-        return db_session.get_async_query_stream(sql)
+        async for doc in self.list_documents_iterator(
+            query_filter={
+                "precomputed_lookup_feature_table_info": {"$ne": None},
+                "deployment_ids": deployment_id,
+            }
+        ):
+            yield doc
```

### Comparing `featurebyte-1.0.2/featurebyte/service/feature_table_cache_metadata.py` & `featurebyte-1.0.3/featurebyte/service/feature_table_cache_metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature Table Cache service
 """
+
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from bson import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/historical_feature_table.py` & `featurebyte-1.0.3/featurebyte/service/historical_feature_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 HistoricalFeatureTableService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from pathlib import Path
 
 import pandas as pd
```

### Comparing `featurebyte-1.0.2/featurebyte/service/historical_features.py` & `featurebyte-1.0.3/featurebyte/service/historical_features.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 HistoricalFeaturesService
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from dataclasses import dataclass
 
 from pydantic import Field
@@ -20,14 +21,15 @@
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_table_cache import FeatureTableCacheService
 from featurebyte.service.historical_features_and_target import get_historical_features
 from featurebyte.service.session_manager import SessionManagerService
 from featurebyte.service.target_helper.base_feature_or_target_computer import (
     BasicExecutorParams,
     Computer,
+    ExecutionResult,
     ExecutorParams,
     QueryExecutor,
 )
 from featurebyte.service.tile_cache import TileCacheService
 from featurebyte.worker.util.task_progress_updater import TaskProgressUpdater
 
 logger = get_logger(__name__)
@@ -55,20 +57,20 @@
         self,
         tile_cache_service: TileCacheService,
         feature_table_cache_service: FeatureTableCacheService,
     ):
         self.tile_cache_service = tile_cache_service
         self.feature_table_cache_service = feature_table_cache_service
 
-    async def execute(self, executor_params: HistoricalFeatureExecutorParams) -> None:
+    async def execute(self, executor_params: HistoricalFeatureExecutorParams) -> ExecutionResult:
         if (
             isinstance(executor_params.observation_set, ObservationTableModel)
             and executor_params.observation_set.has_row_index
         ):
-            await self.feature_table_cache_service.create_view_from_cache(
+            is_output_view = await self.feature_table_cache_service.create_view_or_table_from_cache(
                 feature_store=executor_params.feature_store,
                 observation_table=executor_params.observation_set,
                 graph=executor_params.graph,
                 nodes=executor_params.nodes,
                 output_view_details=executor_params.output_table_details,
                 is_target=False,
                 feature_list_id=executor_params.feature_list_id,
@@ -80,19 +82,20 @@
                 session=executor_params.session,
                 tile_cache_service=self.tile_cache_service,
                 graph=executor_params.graph,
                 nodes=executor_params.nodes,
                 observation_set=executor_params.observation_set,
                 serving_names_mapping=executor_params.serving_names_mapping,
                 feature_store=executor_params.feature_store,
-                is_feature_list_deployed=executor_params.is_feature_list_deployed,
                 parent_serving_preparation=executor_params.parent_serving_preparation,
                 output_table_details=executor_params.output_table_details,
                 progress_callback=executor_params.progress_callback,
             )
+            is_output_view = False
+        return ExecutionResult(is_output_view=is_output_view)
 
 
 class HistoricalFeaturesValidationParametersService:
     """
     Extracts ValidationParameters for historical features
 
     For some reason, HistoricalFeaturesService cannot be injected to a service, only tasks
```

### Comparing `featurebyte-1.0.2/featurebyte/service/historical_features_and_target.py` & `featurebyte-1.0.3/featurebyte/service/historical_features_and_target.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module with utility functions to compute historical features
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, Coroutine, Optional, Union
 
 import time
 
 import pandas as pd
@@ -14,20 +15,16 @@
 from featurebyte.logging import get_logger
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.models.parent_serving import ParentServingPreparation
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.schema import TableDetails
-from featurebyte.query_graph.sql.batch_helper import (
-    NUM_FEATURES_PER_QUERY,
-    get_feature_names,
-    split_nodes,
-)
-from featurebyte.query_graph.sql.common import REQUEST_TABLE_NAME, sql_to_string
+from featurebyte.query_graph.sql.batch_helper import NUM_FEATURES_PER_QUERY, get_feature_names
+from featurebyte.query_graph.sql.common import REQUEST_TABLE_NAME
 from featurebyte.query_graph.sql.feature_historical import (
     PROGRESS_MESSAGE_COMPUTING_FEATURES,
     PROGRESS_MESSAGE_COMPUTING_TARGET,
     TILE_COMPUTE_PROGRESS_MAX_PERCENT,
     get_historical_features_query_set,
     get_internal_observation_set,
     validate_historical_requests_point_in_time,
@@ -90,43 +87,49 @@
         # requires these entity columns to be present in the request table.
         parent_serving_result = construct_request_table_with_parent_entities(
             request_table_name=request_table_name,
             request_table_columns=request_table_columns,
             join_steps=parent_serving_preparation.join_steps,
             feature_store_details=parent_serving_preparation.feature_store_details,
         )
-        request_table_query = sql_to_string(parent_serving_result.table_expr, session.source_type)
         effective_request_table_name = parent_serving_result.new_request_table_name
-        await session.register_table_with_query(
-            effective_request_table_name,
-            request_table_query,
+        await session.create_table_as(
+            TableDetails(table_name=effective_request_table_name),
+            parent_serving_result.table_expr,
         )
 
-    await tile_cache_service.compute_tiles_on_demand(
-        session=session,
-        graph=graph,
-        nodes=nodes,
-        request_id=request_id,
-        request_table_name=effective_request_table_name,
-        feature_store_id=feature_store_id,
-        serving_names_mapping=serving_names_mapping,
-        progress_callback=progress_callback,
-    )
+    try:
+        await tile_cache_service.compute_tiles_on_demand(
+            session=session,
+            graph=graph,
+            nodes=nodes,
+            request_id=request_id,
+            request_table_name=effective_request_table_name,
+            feature_store_id=feature_store_id,
+            serving_names_mapping=serving_names_mapping,
+            progress_callback=progress_callback,
+        )
+    finally:
+        if parent_serving_preparation is not None:
+            await session.drop_table(
+                table_name=effective_request_table_name,
+                schema_name=session.schema_name,
+                database_name=session.database_name,
+            )
 
 
 async def get_historical_features(  # pylint: disable=too-many-locals, too-many-arguments
     session: BaseSession,
     tile_cache_service: TileCacheService,
     graph: QueryGraph,
     nodes: list[Node],
     observation_set: Union[pd.DataFrame, ObservationTableModel],
     feature_store: FeatureStoreModel,
     output_table_details: TableDetails,
     serving_names_mapping: dict[str, str] | None = None,
-    is_feature_list_deployed: bool = False,
     parent_serving_preparation: Optional[ParentServingPreparation] = None,
     progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
 ) -> None:
     """Get historical features
 
     Parameters
     ----------
@@ -141,18 +144,14 @@
     observation_set : Union[pd.DataFrame, ObservationTableModel]
         Observation set
     feature_store: FeatureStoreModel
         Feature store. We need the feature store id and source type information.
     serving_names_mapping : dict[str, str] | None
         Optional serving names mapping if the observations set has different serving name columns
         than those defined in Entities
-    is_feature_list_deployed : bool
-        Whether the feature list that triggered this historical request is deployed. If so, tile
-        tables would have already been back-filled and there is no need to check and calculate tiles
-        on demand.
     parent_serving_preparation: Optional[ParentServingPreparation]
         Preparation required for serving parent features
     output_table_details: TableDetails
         Output table details to write the results to
     progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]]
         Optional progress callback function
     """
@@ -173,86 +172,85 @@
     request_table_columns = observation_set.columns
 
     # Execute feature SQL code
     await observation_set.register_as_request_table(
         session, request_table_name, add_row_index=len(nodes) > NUM_FEATURES_PER_QUERY
     )
 
-    # Compute tiles on demand if required
-    if not is_feature_list_deployed:
+    try:
+        # Compute tiles on demand if required
         tile_cache_progress_callback = (
             get_ranged_progress_callback(
                 progress_callback,
                 0,
                 TILE_COMPUTE_PROGRESS_MAX_PERCENT,
             )
             if progress_callback
             else None
         )
         tic = time.time()
-        # Process nodes in batches
-        tile_cache_node_groups = split_nodes(
-            graph, nodes, NUM_FEATURES_PER_QUERY, is_tile_cache=True
-        )
-        for i, _nodes in enumerate(tile_cache_node_groups):
-            logger.debug("Checking and computing tiles on demand for %d nodes", len(_nodes))
-            await compute_tiles_on_demand(
-                session=session,
-                tile_cache_service=tile_cache_service,
-                graph=graph,
-                nodes=_nodes,
-                request_id=request_id,
-                request_table_name=request_table_name,
-                request_table_columns=request_table_columns,
-                feature_store_id=feature_store.id,
-                serving_names_mapping=serving_names_mapping,
-                parent_serving_preparation=parent_serving_preparation,
-                progress_callback=get_ranged_progress_callback(
-                    tile_cache_progress_callback,
-                    100 * i / len(tile_cache_node_groups),
-                    100 * (i + 1) / len(tile_cache_node_groups),
-                )
-                if tile_cache_progress_callback
-                else None,
-            )
+        await compute_tiles_on_demand(
+            session=session,
+            tile_cache_service=tile_cache_service,
+            graph=graph,
+            nodes=nodes,
+            request_id=request_id,
+            request_table_name=request_table_name,
+            request_table_columns=request_table_columns,
+            feature_store_id=feature_store.id,
+            serving_names_mapping=serving_names_mapping,
+            parent_serving_preparation=parent_serving_preparation,
+            progress_callback=(
+                tile_cache_progress_callback if tile_cache_progress_callback else None
+            ),
+        )
 
         elapsed = time.time() - tic
         logger.debug("Done checking and computing tiles on demand", extra={"duration": elapsed})
 
-    if progress_callback:
-        await progress_callback(
-            TILE_COMPUTE_PROGRESS_MAX_PERCENT, PROGRESS_MESSAGE_COMPUTING_FEATURES
-        )
-
-    # Generate SQL code that computes the features
-    historical_feature_query_set = get_historical_features_query_set(
-        graph=graph,
-        nodes=nodes,
-        request_table_columns=request_table_columns,
-        serving_names_mapping=serving_names_mapping,
-        source_type=feature_store.type,
-        output_table_details=output_table_details,
-        output_feature_names=get_feature_names(graph, nodes),
-        request_table_name=request_table_name,
-        parent_serving_preparation=parent_serving_preparation,
-        output_include_row_index=output_include_row_index,
-        progress_message=PROGRESS_MESSAGE_COMPUTING_FEATURES,
-    )
-    await execute_feature_query_set(
-        session,
-        feature_query_set=historical_feature_query_set,
-        progress_callback=get_ranged_progress_callback(
-            progress_callback,
-            TILE_COMPUTE_PROGRESS_MAX_PERCENT,
-            100,
+        if progress_callback:
+            await progress_callback(
+                TILE_COMPUTE_PROGRESS_MAX_PERCENT, PROGRESS_MESSAGE_COMPUTING_FEATURES
+            )
+
+        # Generate SQL code that computes the features
+        historical_feature_query_set = get_historical_features_query_set(
+            graph=graph,
+            nodes=nodes,
+            request_table_columns=request_table_columns,
+            serving_names_mapping=serving_names_mapping,
+            source_type=feature_store.type,
+            output_table_details=output_table_details,
+            output_feature_names=get_feature_names(graph, nodes),
+            request_table_name=request_table_name,
+            parent_serving_preparation=parent_serving_preparation,
+            output_include_row_index=output_include_row_index,
+            progress_message=PROGRESS_MESSAGE_COMPUTING_FEATURES,
+        )
+        await execute_feature_query_set(
+            session,
+            feature_query_set=historical_feature_query_set,
+            progress_callback=(
+                get_ranged_progress_callback(
+                    progress_callback,
+                    TILE_COMPUTE_PROGRESS_MAX_PERCENT,
+                    100,
+                )
+                if progress_callback
+                else None
+            ),
+        )
+        logger.debug(f"compute_historical_features in total took {time.time() - tic_:.2f}s")
+    finally:
+        await session.drop_table(
+            table_name=request_table_name,
+            schema_name=session.schema_name,
+            database_name=session.database_name,
+            if_exists=True,
         )
-        if progress_callback
-        else None,
-    )
-    logger.debug(f"compute_historical_features in total took {time.time() - tic_:.2f}s")
 
 
 async def get_target(
     session: BaseSession,
     graph: QueryGraph,
     nodes: list[Node],
     observation_set: Union[pd.DataFrame, ObservationTableModel],
@@ -306,33 +304,43 @@
     await observation_set.register_as_request_table(
         session,
         request_table_name,
         add_row_index=len(nodes) > NUM_FEATURES_PER_QUERY,
     )
 
     # Generate SQL code that computes the targets
-    historical_feature_query_set = get_historical_features_query_set(
-        graph=graph,
-        nodes=nodes,
-        request_table_columns=request_table_columns,
-        serving_names_mapping=serving_names_mapping,
-        source_type=feature_store.type,
-        output_table_details=output_table_details,
-        output_feature_names=get_feature_names(graph, nodes),
-        request_table_name=request_table_name,
-        parent_serving_preparation=parent_serving_preparation,
-        output_include_row_index=output_include_row_index,
-        progress_message=PROGRESS_MESSAGE_COMPUTING_TARGET,
-    )
+    try:
+        historical_feature_query_set = get_historical_features_query_set(
+            graph=graph,
+            nodes=nodes,
+            request_table_columns=request_table_columns,
+            serving_names_mapping=serving_names_mapping,
+            source_type=feature_store.type,
+            output_table_details=output_table_details,
+            output_feature_names=get_feature_names(graph, nodes),
+            request_table_name=request_table_name,
+            parent_serving_preparation=parent_serving_preparation,
+            output_include_row_index=output_include_row_index,
+            progress_message=PROGRESS_MESSAGE_COMPUTING_TARGET,
+        )
 
-    await execute_feature_query_set(
-        session=session,
-        feature_query_set=historical_feature_query_set,
-        progress_callback=get_ranged_progress_callback(
-            progress_callback,
-            TILE_COMPUTE_PROGRESS_MAX_PERCENT,
-            100,
+        await execute_feature_query_set(
+            session=session,
+            feature_query_set=historical_feature_query_set,
+            progress_callback=(
+                get_ranged_progress_callback(
+                    progress_callback,
+                    TILE_COMPUTE_PROGRESS_MAX_PERCENT,
+                    100,
+                )
+                if progress_callback
+                else None
+            ),
+        )
+        logger.debug(f"compute_targets in total took {time.time() - tic_:.2f}s")
+    finally:
+        await session.drop_table(
+            table_name=request_table_name,
+            schema_name=session.schema_name,
+            database_name=session.database_name,
+            if_exists=True,
         )
-        if progress_callback
-        else None,
-    )
-    logger.debug(f"compute_targets in total took {time.time() - tic_:.2f}s")
```

### Comparing `featurebyte-1.0.2/featurebyte/service/item_table.py` & `featurebyte-1.0.3/featurebyte/service/scd_table.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 """
-ItemTableService class
+SCDTableService class
 """
+
 from __future__ import annotations
 
-from featurebyte.models.item_table import ItemTableModel
-from featurebyte.schema.item_table import ItemTableCreate, ItemTableServiceUpdate
+from featurebyte.models.scd_table import SCDTableModel
+from featurebyte.schema.scd_table import SCDTableCreate, SCDTableServiceUpdate
 from featurebyte.service.base_table_document import BaseTableDocumentService
 
 
-class ItemTableService(
-    BaseTableDocumentService[ItemTableModel, ItemTableCreate, ItemTableServiceUpdate]
+class SCDTableService(
+    BaseTableDocumentService[SCDTableModel, SCDTableCreate, SCDTableServiceUpdate]
 ):
     """
-    ItemTableService class
+    SCDTableService class
     """
 
-    document_class = ItemTableModel
-    document_update_class = ItemTableServiceUpdate
+    document_class = SCDTableModel
+    document_update_class = SCDTableServiceUpdate
 
     @property
     def class_name(self) -> str:
-        return "ItemTable"
+        return "SCDTable"
```

### Comparing `featurebyte-1.0.2/featurebyte/service/materialized_table.py` & `featurebyte-1.0.3/featurebyte/service/materialized_table.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 """
 BaseMaterializedTableService contains common functionality for materialized tables
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional, Tuple
 
 from bson import ObjectId
 from redis import Redis
 from sqlglot import expressions
 
 from featurebyte.enum import InternalName
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.materialized_table import ColumnSpecWithEntityId
 from featurebyte.persistent import Persistent
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.node.schema import TableDetails
-from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.common import quoted_identifier, sql_to_string
 from featurebyte.query_graph.sql.materialisation import (
     get_row_index_column_expr,
     get_source_count_expr,
 )
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
@@ -167,15 +167,15 @@
             The columns info and number of rows
         """
         table_schema = await db_session.list_table_schema(
             table_name=table_details.table_name,
             database_name=table_details.database_name,
             schema_name=table_details.schema_name,
         )
-        df_row_count = await db_session.execute_query(
+        df_row_count = await db_session.execute_query_long_running(
             sql_to_string(
                 get_source_count_expr(table_details),
                 db_session.source_type,
             )
         )
         assert df_row_count is not None
         num_rows = df_row_count.iloc[0]["row_count"]
@@ -210,20 +210,15 @@
         ----------
         session: BaseSession
             Database session
         table_details: TableDetails
             Table details of the materialized table
         """
         row_number_expr = get_row_index_column_expr()
-        adapter = get_sql_adapter(session.source_type)
-        query = sql_to_string(
-            adapter.create_table_as(
-                table_details,
-                expressions.select(
-                    row_number_expr,
-                    expressions.Star(),
-                ).from_(quoted_identifier(table_details.table_name)),
-                replace=True,
-            ),
-            source_type=session.source_type,
+        await session.create_table_as(
+            table_details,
+            expressions.select(
+                row_number_expr,
+                expressions.Star(),
+            ).from_(quoted_identifier(table_details.table_name)),
+            replace=True,
         )
-        await session.execute_query(query)
```

### Comparing `featurebyte-1.0.2/featurebyte/service/mixin.py` & `featurebyte-1.0.3/featurebyte/service/mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains mixin class(es) used in the service directory.
 """
+
 from __future__ import annotations
 
 from typing import Any, Generic, Optional, Type, TypeVar
 
 from abc import abstractmethod
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/service/namespace_handler.py` & `featurebyte-1.0.3/featurebyte/service/namespace_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Namespace handler
 """
+
 from typing import Any, Dict, List, Tuple, Union
 
 from bson import ObjectId
 
 from featurebyte.common.utils import get_version
 from featurebyte.exception import DocumentInconsistencyError
 from featurebyte.models.base import FeatureByteCatalogBaseDocumentModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/observation_table.py` & `featurebyte-1.0.3/featurebyte/service/observation_table.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 ObservationTableService class
 """
+
 from __future__ import annotations
 
-from typing import Any, Dict, List, Optional, Tuple, cast
+from typing import Any, Dict, List, Optional, cast
 
 from dataclasses import dataclass
 from pathlib import Path
 
 import pandas as pd
 from bson import ObjectId
 from redis import Redis
@@ -22,22 +23,26 @@
     SourceType,
     SpecialColumnName,
     UploadFileFormat,
 )
 from featurebyte.exception import (
     MissingPointInTimeColumnError,
     ObservationTableInvalidContextError,
+    ObservationTableInvalidTargetNameError,
     ObservationTableInvalidUseCaseError,
     ObservationTableMissingColumnsError,
+    ObservationTableTargetDefinitionExistsError,
     UnsupportedPointInTimeColumnTypeError,
 )
 from featurebyte.models.base import FeatureByteBaseDocumentModel, PydanticObjectId
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.materialized_table import ColumnSpecWithEntityId
 from featurebyte.models.observation_table import ObservationTableModel, TargetInput
+from featurebyte.models.request_input import BaseRequestInput
+from featurebyte.models.target_namespace import TargetNamespaceModel
 from featurebyte.persistent import Persistent
 from featurebyte.query_graph.model.common_table import TabularSource
 from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.adapter import get_sql_adapter
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import (
     get_fully_qualified_table_name,
@@ -58,52 +63,121 @@
 )
 from featurebyte.service.context import ContextService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.materialized_table import BaseMaterializedTableService
 from featurebyte.service.preview import PreviewService
 from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.service.target import TargetService
+from featurebyte.service.target_namespace import TargetNamespaceService
 from featurebyte.service.use_case import UseCaseService
 from featurebyte.session.base import BaseSession
 from featurebyte.storage import Storage
 
 
 @dataclass
 class PointInTimeStats:
     """
     Point in time stats
     """
 
     least_recent: str
     most_recent: str
+    percentage_missing: float
+
+
+@dataclass
+class EntityColumnStats:
+    """
+    Entity column stats
+    """
+
+    entity_id: ObjectId
+    column_name: str
+    count: int
+    percentage_missing: float
+
+
+@dataclass
+class ObservationTableStats:
+    """
+    Statistics of special columns in observation table
+    """
+
+    point_in_time_stats: PointInTimeStats
+    entity_columns_stats: List[EntityColumnStats]
+
+    @property
+    def column_name_to_count(self) -> dict[str, int]:
+        """
+        Returns a mapping from entity column name to count
+
+        Returns
+        -------
+        dict[str, int]
+        """
+        return {
+            entity_stat.column_name: entity_stat.count for entity_stat in self.entity_columns_stats
+        }
+
+    def get_columns_with_missing_values(
+        self, entity_ids_to_check: Optional[list[PydanticObjectId]]
+    ) -> list[str]:
+        """
+        Return list of column names with missing values
+
+        Parameters
+        ----------
+        entity_ids_to_check: Optional[list[PydanticObjectId]]
+            If provided, only check for missing values in these entities
+
+        Returns
+        -------
+        list[str]
+        """
+        out = []
+        if self.point_in_time_stats.percentage_missing > 0:
+            out.append(SpecialColumnName.POINT_IN_TIME.value)
+        for entity_stats in self.entity_columns_stats:
+            if (
+                entity_ids_to_check is not None
+                and entity_stats.entity_id not in entity_ids_to_check
+            ):
+                continue
+            if entity_stats.percentage_missing > 0:
+                out.append(entity_stats.column_name)
+        return out
 
 
 def _convert_ts_to_str(timestamp_str: str) -> str:
     timestamp_obj = pd.Timestamp(timestamp_str)
     if timestamp_obj.tzinfo is not None:
         timestamp_obj = timestamp_obj.tz_convert("UTC").tz_localize(None)
     return cast(str, timestamp_obj.isoformat())
 
 
 def validate_columns_info(
     columns_info: List[ColumnSpecWithEntityId],
     primary_entity_ids: Optional[List[PydanticObjectId]] = None,
     skip_entity_validation_checks: bool = False,
+    target_namespace: Optional[TargetNamespaceModel] = None,
 ) -> None:
     """
     Validate column info.
 
     Parameters
     ----------
     columns_info: List[ColumnSpecWithEntityId]
         List of column specs with entity id
     primary_entity_ids: Optional[List[PydanticObjectId]]
         List of primary entity IDs
     skip_entity_validation_checks: bool
         Whether to skip entity validation checks
+    target_namespace: Optional[TargetNamespaceModel]
+        Target namespace document
 
     Raises
     ------
     MissingPointInTimeColumnError
         If the point in time column is missing.
     UnsupportedPointInTimeColumnTypeError
         If the point in time column is not of timestamp type.
@@ -138,14 +212,26 @@
         # Check that primary entity IDs passed in are present in the column info
         if primary_entity_ids is not None:
             if not entity_ids_from_cols.issuperset(set(primary_entity_ids)):
                 raise ValueError(
                     f"Primary entity IDs passed in are not present in the column info: {primary_entity_ids}"
                 )
 
+        # Check that target name is present in the column info
+        if target_namespace is not None:
+            if target_namespace.name not in columns_info_mapping:
+                raise ValueError(f'Target column "{target_namespace.name}" not found.')
+            if (
+                target_namespace.dtype is not None
+                and columns_info_mapping[target_namespace.name].dtype != target_namespace.dtype
+            ):
+                raise ValueError(
+                    f'Target column "{target_namespace.name}" should have dtype "{target_namespace.dtype}"'
+                )
+
 
 class ObservationTableService(
     BaseMaterializedTableService[ObservationTableModel, ObservationTableModel]
 ):
     """
     ObservationTableService class
     """
@@ -165,14 +251,16 @@
         preview_service: PreviewService,
         temp_storage: Storage,
         block_modification_handler: BlockModificationHandler,
         primary_entity_validator: PrimaryEntityValidator,
         use_case_service: UseCaseService,
         storage: Storage,
         redis: Redis[Any],
+        target_namespace_service: TargetNamespaceService,
+        target_service: TargetService,
     ):
         super().__init__(
             user,
             persistent,
             catalog_id,
             session_manager_service,
             feature_store_service,
@@ -182,19 +270,77 @@
             redis,
         )
         self.context_service = context_service
         self.preview_service = preview_service
         self.temp_storage = temp_storage
         self.primary_entity_validator = primary_entity_validator
         self.use_case_service = use_case_service
+        self.target_namespace_service = target_namespace_service
+        self.target_service = target_service
 
     @property
     def class_name(self) -> str:
         return "ObservationTable"
 
+    async def _validate_columns(
+        self,
+        available_columns: List[str],
+        primary_entity_ids: Optional[List[PydanticObjectId]],
+        target_column: Optional[str],
+    ) -> Optional[ObjectId]:
+        # Check if required column names are provided
+        missing_columns = []
+
+        # Check if point in time column is present
+        if SpecialColumnName.POINT_IN_TIME not in available_columns:
+            missing_columns.append(str(SpecialColumnName.POINT_IN_TIME))
+
+        # Check if entity columns are present
+        if primary_entity_ids:
+            for entity_id in primary_entity_ids:
+                entity = await self.entity_service.get_document(document_id=entity_id)
+                if not set(entity.serving_names).intersection(available_columns):
+                    missing_columns.append("/".join(entity.serving_names))
+
+        # Check if target column is present
+        if target_column:
+            if target_column not in available_columns:
+                missing_columns.append(target_column)
+            target_namespaces = await self.target_namespace_service.list_documents_as_dict(
+                query_filter={"name": target_column}
+            )
+            if target_namespaces["total"] == 0:
+                raise ObservationTableInvalidTargetNameError(
+                    f"Target name not found: {target_column}"
+                )
+
+            # validate target namespace has same primary entity ids
+            target_namespace = target_namespaces["data"][0]
+            if target_namespace["entity_ids"] != primary_entity_ids:
+                raise ObservationTableInvalidTargetNameError(
+                    f'Target "{target_column}" does not have matching primary entity ids.'
+                )
+            target_namespace_id = ObjectId(target_namespace["_id"])
+
+            # check if target namespace already has a definition
+            if target_namespace["target_ids"]:
+                raise ObservationTableTargetDefinitionExistsError(
+                    f'Target "{target_column}" already has a definition.'
+                )
+
+        else:
+            target_namespace_id = None
+
+        if missing_columns:
+            raise ObservationTableMissingColumnsError(
+                f"Required column(s) not found: {', '.join(missing_columns)}"
+            )
+
+        return target_namespace_id
+
     async def get_observation_table_task_payload(
         self, data: ObservationTableCreate
     ) -> ObservationTableTaskPayload:
         """
         Validate and convert a ObservationTableCreate schema to a ObservationTableTaskPayload schema
         which will be used to initiate the ObservationTable creation task.
 
@@ -216,19 +362,47 @@
 
         if data.context_id is not None:
             # Check if the context document exists when provided. This should perform additional
             # validation once additional information such as request schema are available in the
             # context.
             await self.context_service.get_document(document_id=data.context_id)
 
+        if isinstance(data.request_input, BaseRequestInput):
+            feature_store = await self.feature_store_service.get_document(
+                document_id=data.feature_store_id
+            )
+            db_session = await self.session_manager_service.get_feature_store_session(feature_store)
+            # validate columns
+            available_columns = await data.request_input.get_column_names(db_session)
+            columns_rename_mapping = data.request_input.columns_rename_mapping
+            if columns_rename_mapping:
+                available_columns = [
+                    columns_rename_mapping.get(col, col) for col in available_columns
+                ]
+            target_namespace_id = await self._validate_columns(
+                available_columns=available_columns,
+                primary_entity_ids=data.primary_entity_ids,
+                target_column=data.target_column,
+            )
+        elif (
+            isinstance(data.request_input, TargetInput) and data.request_input.target_id is not None
+        ):
+            target = await self.target_service.get_document(
+                document_id=data.request_input.target_id
+            )
+            target_namespace_id = target.target_namespace_id
+        else:
+            target_namespace_id = None
+
         return ObservationTableTaskPayload(
             **data.dict(by_alias=True),
             user_id=self.user.id,
             catalog_id=self.catalog_id,
             output_document_id=output_document_id,
+            target_namespace_id=target_namespace_id,
         )
 
     async def get_observation_table_upload_task_payload(
         self,
         data: ObservationTableUpload,
         observation_set_dataframe: pd.DataFrame,
         file_format: UploadFileFormat,
@@ -248,40 +422,28 @@
             File format of the uploaded file
         uploaded_file_name: str
             Name of the uploaded file
 
         Returns
         -------
         ObservationTableUploadTaskPayload
-
-        Raises
-        ------
-        ObservationTableMissingColumnsError
-            If the observation set dataframe is missing required columns.
         """
 
         # Check any conflict with existing documents
         output_document_id = data.id or ObjectId()
         await self._check_document_unique_constraints(
             document=FeatureByteBaseDocumentModel(_id=output_document_id, name=data.name),
         )
 
         # Check if required column names are provided
-        missing_columns = []
-        available_columns = set(observation_set_dataframe.columns.tolist())
-        if SpecialColumnName.POINT_IN_TIME not in observation_set_dataframe.columns:
-            missing_columns.append(str(SpecialColumnName.POINT_IN_TIME))
-        for entity_id in data.primary_entity_ids:
-            entity = await self.entity_service.get_document(document_id=entity_id)
-            if not set(entity.serving_names).intersection(available_columns):
-                missing_columns.append("/".join(entity.serving_names))
-        if missing_columns:
-            raise ObservationTableMissingColumnsError(
-                f"Required columns not found: {', '.join(missing_columns)}"
-            )
+        target_namespace_id = await self._validate_columns(
+            available_columns=observation_set_dataframe.columns.tolist(),
+            primary_entity_ids=data.primary_entity_ids,
+            target_column=data.target_column,
+        )
 
         # Persist dataframe to parquet file that can be read by the task later
         observation_set_storage_path = f"observation_table/{output_document_id}.parquet"
         await self.temp_storage.put_dataframe(
             observation_set_dataframe, Path(observation_set_storage_path)
         )
 
@@ -289,14 +451,15 @@
             **data.dict(by_alias=True),
             user_id=self.user.id,
             catalog_id=self.catalog_id,
             output_document_id=output_document_id,
             observation_set_storage_path=observation_set_storage_path,
             file_format=file_format,
             uploaded_file_name=uploaded_file_name,
+            target_namespace_id=target_namespace_id,
         )
 
     @staticmethod
     def get_minimum_iet_sql_expr(
         entity_column_names: List[str], table_details: TableDetails, source_type: SourceType
     ) -> Expression:
         """
@@ -387,43 +550,48 @@
         # Construct SQL
         sql_expr = self.get_minimum_iet_sql_expr(
             entity_col_names, table_details, db_session.source_type
         )
 
         # Execute SQL
         sql_string = sql_to_string(sql_expr, db_session.source_type)
-        min_interval_df = await db_session.execute_query(sql_string)
+        min_interval_df = await db_session.execute_query_long_running(sql_string)
         assert min_interval_df is not None
         value = min_interval_df.iloc[0]["MIN_INTERVAL"]
         if value is None or pd.isna(value):
             return None
         return float(value)
 
-    async def _get_column_name_to_entity_count(
+    async def _get_observation_table_stats(
         self,
         feature_store: FeatureStoreModel,
         table_details: TableDetails,
         columns_info: List[ColumnSpecWithEntityId],
-    ) -> Tuple[Dict[str, int], PointInTimeStats]:
+    ) -> ObservationTableStats:
         """
-        Get the entity column name to unique entity count mapping.
+        Get statistics of point in time and entity columns
+
+        Extract information such as entity column name to unique entity count mapping, point in time
+        stats, and missing value percentage for validation.
 
         Parameters
         ----------
         feature_store: FeatureStoreModel
             Feature store document
         table_details: TableDetails
             Table details of the materialized table
         columns_info: List[ColumnSpecWithEntityId]
             List of column specs with entity id
 
         Returns
         -------
-        Tuple[Dict[str, int], PointInTimeStats]
+        ObservationTableStats
         """
+        # pylint: disable=no-member
+
         # Get describe statistics
         source_table = SourceTable(
             feature_store=feature_store,
             tabular_source=TabularSource(
                 feature_store_id=feature_store.id,
                 table_details=TableDetails(
                     database_name=table_details.database_name,
@@ -433,48 +601,57 @@
             ),
             columns_info=columns_info,
         )
         graph, node = source_table.frame.extract_pruned_graph_and_node()
         sample = FeatureStoreSample(
             graph=graph,
             node_name=node.name,
-            stats_names=["unique", "max", "min"],
+            stats_names=["unique", "max", "min", "%missing"],
             feature_store_id=feature_store.id,
         )
         describe_stats_json = await self.preview_service.describe(sample, 0, 1234)
         describe_stats_dataframe = dataframe_from_json(describe_stats_json)
-        entity_cols = [col for col in columns_info if col.entity_id is not None]
-        column_name_to_count = {}
-        for col in entity_cols:
-            col_name = col.name
-            column_name_to_count[
-                col_name
-            ] = describe_stats_dataframe.loc[  # pylint: disable=no-member
-                "unique", col_name
-            ]
-        least_recent_time_str = describe_stats_dataframe.loc[  # pylint: disable=no-member
-            "min", SpecialColumnName.POINT_IN_TIME
-        ]
+        entity_columns_stats = []
+        for col in columns_info:
+            if col.entity_id is None:
+                continue
+            entity_columns_stats.append(
+                EntityColumnStats(
+                    entity_id=col.entity_id,
+                    column_name=col.name,
+                    count=describe_stats_dataframe.loc["unique", col.name],
+                    percentage_missing=describe_stats_dataframe.loc["%missing", col.name],
+                )
+            )
+        least_recent_time_str = describe_stats_dataframe.loc["min", SpecialColumnName.POINT_IN_TIME]
         least_recent_time_str = _convert_ts_to_str(least_recent_time_str)
-        most_recent_time_str = describe_stats_dataframe.loc[  # pylint: disable=no-member
-            "max", SpecialColumnName.POINT_IN_TIME
-        ]
+        most_recent_time_str = describe_stats_dataframe.loc["max", SpecialColumnName.POINT_IN_TIME]
         most_recent_time_str = _convert_ts_to_str(most_recent_time_str)
-        return column_name_to_count, PointInTimeStats(
-            least_recent=least_recent_time_str, most_recent=most_recent_time_str
+        percentage_missing = describe_stats_dataframe.loc[
+            "%missing", SpecialColumnName.POINT_IN_TIME
+        ]
+        point_in_time_stats = PointInTimeStats(
+            least_recent=least_recent_time_str,
+            most_recent=most_recent_time_str,
+            percentage_missing=percentage_missing,
+        )
+        return ObservationTableStats(
+            point_in_time_stats=point_in_time_stats,
+            entity_columns_stats=entity_columns_stats,
         )
 
     async def validate_materialized_table_and_get_metadata(
         self,
         db_session: BaseSession,
         table_details: TableDetails,
         feature_store: FeatureStoreModel,
         serving_names_remapping: Optional[Dict[str, str]] = None,
         skip_entity_validation_checks: bool = False,
         primary_entity_ids: Optional[List[PydanticObjectId]] = None,
+        target_namespace_id: Optional[PydanticObjectId] = None,
     ) -> Dict[str, Any]:
         """
         Validate and get additional metadata for the materialized observation table.
 
         Parameters
         ----------
         db_session: BaseSession
@@ -485,52 +662,75 @@
             Feature store document
         serving_names_remapping: Dict[str, str]
             Remapping of serving names
         skip_entity_validation_checks: bool
             Whether to skip entity validation checks
         primary_entity_ids: Optional[List[PydanticObjectId]]
             List of primary entity IDs
+        target_namespace_id: Optional[PydanticObjectId]
+            Target namespace ID
 
         Returns
         -------
         dict[str, Any]
+
+        Raises
+        ------
+        ValueError
+            If the observation table fails any of the validation
         """
         # Get column info and number of row metadata
         columns_info, num_rows = await self.get_columns_info_and_num_rows(
             db_session=db_session,
             table_details=table_details,
             serving_names_remapping=serving_names_remapping,
         )
         # Perform validation on primary entity IDs. We always perform this check if the primary entity IDs exist.
         if primary_entity_ids is not None:
             await self.primary_entity_validator.validate_entities_are_primary_entities(
                 primary_entity_ids
             )
 
         # Perform validation on column info
+        if target_namespace_id is not None:
+            target_namespace = await self.target_namespace_service.get_document(
+                document_id=target_namespace_id
+            )
+        else:
+            target_namespace = None
         validate_columns_info(
             columns_info,
             primary_entity_ids=primary_entity_ids,
             skip_entity_validation_checks=skip_entity_validation_checks,
+            target_namespace=target_namespace,
         )
         # Get entity column name to minimum interval mapping
         min_interval_secs_between_entities = await self._get_min_interval_secs_between_entities(
             db_session, columns_info, table_details
         )
         # Get entity statistics metadata
-        column_name_to_count, point_in_time_stats = await self._get_column_name_to_entity_count(
+        observation_table_stats = await self._get_observation_table_stats(
             feature_store, table_details, columns_info
         )
+        point_in_time_stats = observation_table_stats.point_in_time_stats
+        columns_with_missing_values = observation_table_stats.get_columns_with_missing_values(
+            primary_entity_ids
+        )
+        if columns_with_missing_values:
+            raise ValueError(
+                "These columns in the observation table must not contain any missing values: "
+                f'{", ".join(columns_with_missing_values)}'
+            )
 
         return {
             "columns_info": columns_info,
             "num_rows": num_rows,
             "most_recent_point_in_time": point_in_time_stats.most_recent,
             "least_recent_point_in_time": point_in_time_stats.least_recent,
-            "entity_column_name_to_count": column_name_to_count,
+            "entity_column_name_to_count": observation_table_stats.column_name_to_count,
             "min_interval_secs_between_entities": min_interval_secs_between_entities,
         }
 
     async def update_observation_table(  # pylint: disable=too-many-branches
         self, observation_table_id: ObjectId, data: ObservationTableServiceUpdate
     ) -> Optional[ObservationTableModel]:
         """
@@ -569,29 +769,39 @@
                         f"Cannot add UseCase {data.use_case_id_to_add} as it is already associated with the ObservationTable."
                     )
                 use_case = await self.use_case_service.get_document(
                     document_id=data.use_case_id_to_add
                 )
                 if use_case.context_id != observation_table.context_id:
                     raise ObservationTableInvalidUseCaseError(
-                        f"Cannot add UseCase {data.use_case_id_to_add} as its context_id is different from the existing context_id."
+                        f"Cannot add UseCase {data.use_case_id_to_add} due to mismatched contexts."
                     )
 
-                # validate request_input and target_id
-                if not isinstance(observation_table.request_input, TargetInput):
+                if (
+                    isinstance(observation_table.request_input, TargetInput)
+                    and observation_table.request_input.target_id != use_case.target_id
+                ):
                     raise ObservationTableInvalidUseCaseError(
-                        "observation table request_input is not TargetInput"
+                        f"Cannot add UseCase {data.use_case_id_to_add} due to mismatched targets."
                     )
 
-                if not use_case.target_id or (
-                    isinstance(observation_table.request_input, TargetInput)
-                    and observation_table.request_input.target_id != use_case.target_id
+                if (
+                    observation_table.target_namespace_id
+                    and use_case.target_namespace_id != observation_table.target_namespace_id
+                ):
+                    raise ObservationTableInvalidUseCaseError(
+                        f"Cannot add UseCase {data.use_case_id_to_add} due to mismatched targets."
+                    )
+
+                if (
+                    observation_table.target_namespace_id
+                    and use_case.target_namespace_id != observation_table.target_namespace_id
                 ):
                     raise ObservationTableInvalidUseCaseError(
-                        f"Cannot add UseCase {data.use_case_id_to_add} as its target_id is different from the existing target_id."
+                        f"Cannot add UseCase {data.use_case_id_to_add} as its target_namespace_id is different from the existing target_namespace_id."
                     )
 
                 use_case_ids.append(data.use_case_id_to_add)
 
             # validate use case id to remove
             if data.use_case_id_to_remove:
                 if data.use_case_id_to_remove not in use_case_ids:
```

### Comparing `featurebyte-1.0.2/featurebyte/service/offline_store_feature_table_comment.py` & `featurebyte-1.0.3/featurebyte/service/online_enable.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,188 +1,210 @@
 """
-OfflineStoreFeatureTableCommentService
+OnlineEnableService class
 """
+
 from __future__ import annotations
 
-from typing import Any, Callable, Coroutine, Dict, List, Optional, Sequence, Tuple, Union
+from typing import List, Optional
 
-from dataclasses import dataclass
+from bson.objectid import ObjectId
 
-from featurebyte.models.entity import EntityModel
+from featurebyte.exception import DataWarehouseConnectionError
+from featurebyte.feature_manager.model import ExtendedFeatureModel
+from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.feature import FeatureModel
-from featurebyte.models.feature_store import FeatureStoreModel
-from featurebyte.models.offline_store_feature_table import OfflineStoreFeatureTableModel
-from featurebyte.service.entity import EntityService
+from featurebyte.models.online_store_spec import OnlineFeatureSpec
+from featurebyte.persistent import Persistent
+from featurebyte.schema.feature import FeatureServiceUpdate
+from featurebyte.service.feature import FeatureService
+from featurebyte.service.feature_list import FeatureListService
+from featurebyte.service.feature_manager import FeatureManagerService
 from featurebyte.service.feature_namespace import FeatureNamespaceService
+from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.session.base import BaseSession
 
 
-@dataclass
-class TableComment:
-    """
-    Representation of a comment for a specific offline feature table
-    """
-
-    table_name: str
-    comment: str
-
-
-@dataclass
-class ColumnComment:
+class OnlineEnableService:
     """
-    Representation of a comment for a specific offline feature table column
-    """
-
-    table_name: str
-    column_name: str
-    comment: str
-
-
-class OfflineStoreFeatureTableCommentService:
-    """
-    OfflineStoreFeatureTableCommentService is responsible for generating comments for offline store
-    feature tables and columns, and applying them in the data warehouse.
+    OnlineEnableService class is responsible for maintaining the feature & feature list structure
+    of feature online enablement.
     """
 
     def __init__(
         self,
-        entity_service: EntityService,
+        persistent: Persistent,
         session_manager_service: SessionManagerService,
+        feature_service: FeatureService,
+        feature_store_service: FeatureStoreService,
         feature_namespace_service: FeatureNamespaceService,
+        feature_list_service: FeatureListService,
+        feature_manager_service: FeatureManagerService,
     ):
-        self.entity_service = entity_service
+        # pylint: disable=too-many-arguments
+        self.persistent = persistent
+        self.feature_service = feature_service
         self.session_manager_service = session_manager_service
+        self.feature_store_service = feature_store_service
         self.feature_namespace_service = feature_namespace_service
+        self.feature_list_service = feature_list_service
+        self.feature_manager_service = feature_manager_service
 
-    async def apply_comments(
+    async def _update_feature_lists(
         self,
-        feature_store: FeatureStoreModel,
-        comments: Sequence[Union[TableComment, ColumnComment]],
-        update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
+        feature_list_ids: List[PydanticObjectId],
+        feature: FeatureModel,
     ) -> None:
         """
-        Add the provided table or column comments in the data warehouse
+        Update online_enabled_feature_ids in feature list
 
         Parameters
         ----------
-        feature_store: FeatureStoreModel
-            Feature store model
-        comments:  Sequence[Union[TableComment, ColumnComment]]
-            List of comments to be added
-        update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]
-            Progress callback
-        """
-        session = await self.session_manager_service.get_feature_store_session(feature_store)
-        for idx, entry in enumerate(comments):
-            if isinstance(entry, TableComment):
-                await session.comment_table(entry.table_name, entry.comment)
-            else:
-                await session.comment_column(entry.table_name, entry.column_name, entry.comment)
-
-            if update_progress:
-                percent = int((idx + 1) / len(comments) * 100)
-                if isinstance(entry, TableComment):
-                    message = f"Added comment for table {entry.table_name}"
-                else:
-                    message = (
-                        f"Added comment for column {entry.column_name} in table {entry.table_name}"
-                    )
-                await update_progress(percent, message)
-
-    async def generate_table_comment(
-        self, feature_table_model: OfflineStoreFeatureTableModel
-    ) -> TableComment:
+        feature_list_ids: List[PydanticObjectId]
+            Feature list IDs to be updated
+        feature: FeatureModel
+            Updated Feature object
+        """
+        op_key = "$addToSet" if feature.online_enabled else "$pull"
+        await self.feature_list_service.update_documents(
+            query_filter={"_id": {"$in": feature_list_ids}},
+            update={op_key: {"online_enabled_feature_ids": feature.id}},
+        )
+
+    async def _update_feature_namespace(
+        self,
+        feature_namespace_id: ObjectId,
+        feature: FeatureModel,
+    ) -> None:
         """
-        Generate comment for an offline feature table
+        Update online_enabled_feature_ids in feature namespace
 
         Parameters
         ----------
-        feature_table_model: OfflineStoreFeatureTableModel
-            Offline store feature table model
-
-        Returns
-        -------
-        TableComment
-        """
-        primary_entities = await self.entity_service.get_entities(
-            set(feature_table_model.primary_entity_ids)
+        feature_namespace_id: ObjectId
+            FeatureNamespace ID
+        feature: FeatureModel
+            Updated Feature object
+        """
+        op_key = "$addToSet" if feature.online_enabled else "$pull"
+        await self.feature_namespace_service.update_documents(
+            query_filter={"_id": {"$in": [feature_namespace_id]}},
+            update={op_key: {"online_enabled_feature_ids": feature.id}},
         )
 
-        def _format_entity(entity_model: EntityModel) -> str:
-            return f"{entity_model.name} (serving name: {entity_model.serving_names[0]})"
+    @staticmethod
+    async def update_data_warehouse_with_session(
+        session: Optional[BaseSession],
+        feature_manager_service: FeatureManagerService,
+        feature: FeatureModel,
+        target_online_enabled: bool,
+        is_recreating_schema: bool = False,
+    ) -> None:
+        """
+        Update data warehouse registry upon changes to online enable status, such as enabling or
+        disabling scheduled tile and feature jobs
 
-        primary_entities_info = ", ".join([_format_entity(entity) for entity in primary_entities])
-        if feature_table_model.entity_lookup_info is not None:
-            parent_entity = await self.entity_service.get_document(
-                feature_table_model.entity_lookup_info.related_entity_id
+        Parameters
+        ----------
+        session: Optional[BaseSession]
+            Session object
+        feature_manager_service: FeatureManagerService
+            An instance of FeatureManagerService to handle materialization of features and tiles
+        feature: FeatureModel
+            Feature used to update the data warehouse
+        target_online_enabled: bool
+            Target online enabled status
+        is_recreating_schema: bool
+            Whether we are recreating the working schema from scratch. Only set as True when called
+            by WorkingSchemaService.
+        """
+        extended_feature_model = ExtendedFeatureModel(**feature.dict(by_alias=True))
+        online_feature_spec = OnlineFeatureSpec(feature=extended_feature_model)
+
+        if target_online_enabled:
+            assert session is not None
+            await feature_manager_service.online_enable(
+                session, online_feature_spec, is_recreating_schema=is_recreating_schema
             )
-            sentences = [
-                f"This feature table is used to lookup the entity {_format_entity(parent_entity)}"
-                f" using the entity {primary_entities_info} based on their parent-child"
-                f" relationship"
-            ]
-        elif feature_table_model.primary_entity_ids:
-            sentences = [
-                f"This feature table consists of features for primary entity {primary_entities_info}"
-            ]
         else:
-            sentences = ["This feature table consists of features without a primary entity"]
-        if feature_table_model.feature_job_setting:
-            job_setting = feature_table_model.feature_job_setting
-            sentences.append(
-                f"It is updated every {job_setting.frequency_seconds} second(s), with a blind spot"
-                f" of {job_setting.blind_spot_seconds} second(s) and a time modulo frequency of"
-                f" {job_setting.time_modulo_frequency_seconds} second(s)"
+            await feature_manager_service.online_disable(session, online_feature_spec)
+
+    async def update_data_warehouse(
+        self, feature: FeatureModel, target_online_enabled: bool
+    ) -> None:
+        """
+        Update data warehouse registry upon changes to online enable status, such as enabling or
+        disabling scheduled tile and feature jobs
+
+        Parameters
+        ----------
+        feature: FeatureModel
+            Feature used to update the data warehouse
+        target_online_enabled: bool
+            Target online enabled status
+
+        Raises
+        ------
+        DataWarehouseConnectionError
+            If data warehouse session cannot be created
+        """
+        feature_store_model = await self.feature_store_service.get_document(
+            document_id=feature.tabular_source.feature_store_id
+        )
+        try:
+            session = await self.session_manager_service.get_feature_store_session(
+                feature_store_model
             )
-        comment = ". ".join(sentences) + "."
-        return TableComment(table_name=feature_table_model.name, comment=comment)
+        except DataWarehouseConnectionError as exc:
+            if target_online_enabled:
+                raise exc
+            # This could happen if the data warehouse is defunct and no session can be established.
+            # In case of disabling a feature, we still want to be able to proceed and disable the
+            # associated periodic tasks.
+            session = None
+
+        await self.update_data_warehouse_with_session(
+            session=session,
+            feature_manager_service=self.feature_manager_service,
+            feature=feature,
+            target_online_enabled=target_online_enabled,
+        )
 
-    async def generate_column_comments(
-        self, feature_models: List[FeatureModel]
-    ) -> List[ColumnComment]:
+    async def update_feature(
+        self,
+        feature_id: ObjectId,
+        online_enabled: bool,
+    ) -> FeatureModel:
         """
-        Generate comments for columns in offline feature tables corresponding to the features
+        Update feature online enabled & trigger list of cascading updates
 
         Parameters
         ----------
-        feature_models: List[FeatureModel]
-            Feature models
+        feature_id: ObjectId
+            Target feature ID
+        online_enabled: bool
+            Value to update the feature online_enabled status
 
         Returns
         -------
-        List[ColumnComment]
+        FeatureModel
         """
-        # Mapping to from table name and column name to comments
-        comments: Dict[Tuple[str, str], str] = {}
-
-        for feature in feature_models:
-            feature_description = (
-                await self.feature_namespace_service.get_document(feature.feature_namespace_id)
-            ).description
-
-            offline_ingest_graphs = (
-                feature.offline_store_info.extract_offline_store_ingest_query_graphs()
+        document = await self.feature_service.get_document(document_id=feature_id)
+        async with self.persistent.start_transaction():
+            feature = await self.feature_service.update_document(
+                document_id=feature_id,
+                data=FeatureServiceUpdate(online_enabled=online_enabled),
+                document=document,
+                return_document=True,
+                populate_remote_attributes=False,
             )
-            for offline_ingest_graph in offline_ingest_graphs:
-                table_name = offline_ingest_graph.offline_store_table_name
-                if feature.offline_store_info.is_decomposed:
-                    comment = (
-                        f"This intermediate feature is used to compute the feature"
-                        f" {feature.name} (version: {feature.version.name})"
-                    )
-                    if feature_description is not None:
-                        comment += f". Description of {feature.name}: {feature_description}"
-                    comments[(table_name, offline_ingest_graph.output_column_name)] = comment
-                elif feature_description is not None:
-                    comments[
-                        (table_name, offline_ingest_graph.output_column_name)
-                    ] = feature_description
-
-        out = [
-            ColumnComment(
-                table_name=table_name,
-                column_name=column_name,
-                comment=comment,
+            assert isinstance(feature, FeatureModel)
+            await self._update_feature_namespace(
+                feature_namespace_id=feature.feature_namespace_id,
+                feature=feature,
             )
-            for ((table_name, column_name), comment) in comments.items()
-        ]
-        return out
+            await self._update_feature_lists(
+                feature_list_ids=feature.feature_list_ids,
+                feature=feature,
+            )
+
+        return await self.feature_service.get_document(document_id=feature_id)
```

### Comparing `featurebyte-1.0.2/featurebyte/service/offline_store_feature_table_construction.py` & `featurebyte-1.0.3/featurebyte/service/offline_store_feature_table_construction.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,57 +1,70 @@
 """
 OfflineStoreFeatureTableConstructionService class
 """
+
 from __future__ import annotations
 
-from typing import Dict, List, Optional, Sequence
+from typing import Dict, List, Optional, Sequence, Tuple
+
+import os
+import pickle
+from pathlib import Path
 
+import aiofiles
 from bson import ObjectId
 
 from featurebyte import FeatureJobSetting, SourceType
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.models.entity import EntityModel
 from featurebyte.models.entity_universe import (
     EntityUniverseModel,
     EntityUniverseParams,
-    construct_window_aggregates_universe,
     get_combined_universe,
 )
 from featurebyte.models.entity_validation import EntityInfo
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureCluster
 from featurebyte.models.offline_store_feature_table import (
     OfflineStoreFeatureTableModel,
     get_combined_ingest_graph,
 )
 from featurebyte.models.offline_store_ingest_query import OfflineStoreIngestQueryGraph
 from featurebyte.models.sqlglot_expression import SqlglotExpressionModel
 from featurebyte.query_graph.graph import QueryGraph
+from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.query_graph.node import Node
 from featurebyte.query_graph.node.generic import LookupNode
 from featurebyte.query_graph.node.mixin import BaseGroupbyParameters
 from featurebyte.service.entity import EntityService
 from featurebyte.service.entity_serving_names import EntityServingNamesService
+from featurebyte.service.exception import OfflineStoreFeatureTableBadStateError
+from featurebyte.service.offline_store_feature_table import OfflineStoreFeatureTableService
 from featurebyte.service.parent_serving import ParentEntityLookupService
+from featurebyte.storage import Storage
 
 
 class OfflineStoreFeatureTableConstructionService:
     """
     OfflineStoreFeatureTableConstructionService class
     """
 
     def __init__(
         self,
         entity_service: EntityService,
         parent_entity_lookup_service: ParentEntityLookupService,
         entity_serving_names_service: EntityServingNamesService,
+        offline_store_feature_table_service: OfflineStoreFeatureTableService,
+        storage: Storage,
     ):
         self.entity_service = entity_service
         self.parent_entity_lookup_service = parent_entity_lookup_service
         self.entity_serving_names_service = entity_serving_names_service
+        self.offline_store_feature_table_service = offline_store_feature_table_service
+        self.storage = storage
 
     async def get_dummy_offline_store_feature_table_model(
         self,
         primary_entity_ids: Sequence[ObjectId],
         feature_job_setting: Optional[FeatureJobSetting],
         has_ttl: bool,
         feature_store_id: ObjectId,
@@ -111,57 +124,83 @@
             catalog_id=catalog_id,
         )
 
     async def get_offline_store_feature_table_model(
         self,
         feature_table_name: str,
         features: List[FeatureModel],
-        aggregate_result_table_names: List[str],
         primary_entities: List[EntityModel],
         has_ttl: bool,
         feature_job_setting: Optional[FeatureJobSetting],
         source_type: SourceType,
     ) -> OfflineStoreFeatureTableModel:
         """
         Returns a OfflineStoreFeatureTableModel for a feature table
 
         Parameters
         ----------
         feature_table_name : str
             Feature table name
         features : List[FeatureModel]
             List of features
-        aggregate_result_table_names : List[str]
-            List of aggregate result table names
         primary_entities : List[EntityModel]
             List of primary entities
         has_ttl : bool
             Whether the feature table has TTL
         feature_job_setting : Optional[FeatureJobSetting]
             Feature job setting of the feature table
         source_type : SourceType
             Source type information
 
         Returns
         -------
         OfflineStoreFeatureTableModel
+
+        Raises
+        ------
+        OfflineStoreFeatureTableBadStateError
+            If the entity universe cannot be determined
         """
         ingest_graph_metadata = get_combined_ingest_graph(
             features=features,
             primary_entities=primary_entities,
             has_ttl=has_ttl,
             feature_job_setting=feature_job_setting,
         )
 
-        entity_universe = await self.get_entity_universe_model(
-            serving_names=[entity.serving_names[0] for entity in primary_entities],
-            aggregate_result_table_names=aggregate_result_table_names,
-            offline_ingest_graphs=ingest_graph_metadata.offline_ingest_graphs,
-            source_type=source_type,
-        )
+        try:
+            entity_universe = await self.get_entity_universe_model(
+                offline_ingest_graphs=ingest_graph_metadata.offline_ingest_graphs,
+                source_type=source_type,
+                feature_table_name=feature_table_name,
+            )
+        except OfflineStoreFeatureTableBadStateError:
+            # Temporarily save the offline ingest graphs and other information to a file for
+            # troubleshooting
+            debug_info = {
+                "feature_ids": [feature.id for feature in features],
+                "primary_entities": primary_entities,
+                "has_ttl": has_ttl,
+                "feature_job_setting": feature_job_setting,
+                "feature_table_name": feature_table_name,
+                "offline_ingest_graphs": ingest_graph_metadata.offline_ingest_graphs,
+            }
+            path = self.offline_store_feature_table_service.get_full_remote_file_path(
+                f"offline_store_feature_table/{feature_table_name}/entity_universe_debug_info.pickle"
+            )
+            try:
+                await self.storage.delete(Path(path))
+            except FileNotFoundError:
+                pass
+            async with aiofiles.tempfile.TemporaryDirectory() as tempdir_path:
+                file_path = os.path.join(tempdir_path, "data.pickle")
+                with open(file_path, "wb") as file_obj:
+                    pickle.dump(debug_info, file_obj, protocol=pickle.HIGHEST_PROTOCOL)
+                await self.storage.put(Path(file_path), Path(path))
+            raise
 
         return OfflineStoreFeatureTableModel(
             name=feature_table_name,
             feature_ids=[feature.id for feature in features],
             primary_entity_ids=[entity.id for entity in primary_entities],
             # FIXME: consolidate all the offline store serving names (get_entity_id_to_serving_name_for_offline_store)
             serving_names=[entity.serving_names[0] for entity in primary_entities],
@@ -171,81 +210,82 @@
             entity_universe=entity_universe,
             has_ttl=has_ttl,
             feature_job_setting=feature_job_setting.normalize() if feature_job_setting else None,
         )
 
     async def get_entity_universe_model(
         self,
-        serving_names: List[str],
-        aggregate_result_table_names: List[str],
-        offline_ingest_graphs: List[OfflineStoreIngestQueryGraph],
+        offline_ingest_graphs: List[
+            Tuple[OfflineStoreIngestQueryGraph, List[EntityRelationshipInfo]]
+        ],
         source_type: SourceType,
+        feature_table_name: str,
     ) -> EntityUniverseModel:
         """
         Create a new EntityUniverseModel object
 
         Parameters
         ----------
-        serving_names: List[str]
-            The serving names of the entities
-        aggregate_result_table_names: List[str]
-            The names of the aggregate result tables for window aggregates
         offline_ingest_graphs: List[OfflineStoreIngestQueryGraph]
             The offline ingest graphs that the entity universe is to be constructed from
         source_type: SourceType
             Source type information
+        feature_table_name: str
+            Name of the offline store feature table which the entity universe is for
 
         Returns
         -------
         EntityUniverse
+
+        Raises
+        ------
+        OfflineStoreFeatureTableBadStateError
+            If the entity universe cannot be determined
         """
-        if aggregate_result_table_names:
-            universe_expr = construct_window_aggregates_universe(
-                serving_names=serving_names,
-                aggregate_result_table_names=aggregate_result_table_names,
-            )
-        else:
-            params = []
-            for offline_ingest_graph in offline_ingest_graphs:
-                primary_entities = [
-                    (await self.entity_service.get_document(entity_id))
-                    for entity_id in offline_ingest_graph.primary_entity_ids
-                ]
-                for info in offline_ingest_graph.aggregation_nodes_info:
-                    node = offline_ingest_graph.graph.get_node_by_name(info.node_name)
-                    # The entity universe has to be described in terms of the primary entity. If the
-                    # aggregation is based on a parent entity, we need to map it back to the primary
-                    # entity (a child).
-                    non_primary_entity_ids = self._get_non_primary_entity_ids(
-                        node,
-                        offline_ingest_graph.primary_entity_ids,
+        params = []
+        for offline_ingest_graph, relationships_info in offline_ingest_graphs:
+            primary_entities = [
+                (await self.entity_service.get_document(entity_id))
+                for entity_id in offline_ingest_graph.primary_entity_ids
+            ]
+            for info in offline_ingest_graph.aggregation_nodes_info:
+                node = offline_ingest_graph.graph.get_node_by_name(info.node_name)
+                # The entity universe has to be described in terms of the primary entity. If the
+                # aggregation is based on a parent entity, we need to map it back to the primary
+                # entity (a child).
+                non_primary_entity_ids = self._get_non_primary_entity_ids(
+                    node,
+                    offline_ingest_graph.primary_entity_ids,
+                )
+                if non_primary_entity_ids:
+                    entity_info = EntityInfo(
+                        required_entities=[
+                            (await self.entity_service.get_document(entity_id))
+                            for entity_id in non_primary_entity_ids
+                        ],
+                        provided_entities=primary_entities,
                     )
-                    if non_primary_entity_ids:
-                        entity_info = EntityInfo(
-                            required_entities=[
-                                (await self.entity_service.get_document(entity_id))
-                                for entity_id in non_primary_entity_ids
-                            ],
-                            provided_entities=primary_entities,
-                        )
-                        join_steps = (
-                            await self.parent_entity_lookup_service.get_required_join_steps(
-                                entity_info
-                            )
-                        )
-                    else:
-                        join_steps = None
-                    params.append(
-                        EntityUniverseParams(
-                            graph=offline_ingest_graph.graph,
-                            node=node,
-                            join_steps=join_steps,
-                        )
+                    join_steps = await self.parent_entity_lookup_service.get_required_join_steps(
+                        entity_info, relationships_info
                     )
-            universe_expr = get_combined_universe(params, source_type)
+                else:
+                    join_steps = None
+                params.append(
+                    EntityUniverseParams(
+                        graph=offline_ingest_graph.graph,
+                        node=node,
+                        join_steps=join_steps,
+                    )
+                )
+        universe_expr = get_combined_universe(params, source_type)
+
+        if universe_expr is None:
+            raise OfflineStoreFeatureTableBadStateError(
+                f"Failed to create entity universe for offline store feature table {feature_table_name}"
+            )
 
         return EntityUniverseModel(query_template=SqlglotExpressionModel.create(universe_expr))
 
     @staticmethod
     def _get_non_primary_entity_ids(
         node: Node, offline_ingest_graph_primary_entity_ids: List[PydanticObjectId]
     ) -> List[PydanticObjectId]:
```

### Comparing `featurebyte-1.0.2/featurebyte/service/offline_store_feature_table_manager.py` & `featurebyte-1.0.3/featurebyte/service/offline_store_feature_table_manager.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,51 +1,59 @@
 """
 OfflineStoreFeatureTableUpdateService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, Coroutine, Dict, Iterable, List, Optional, Tuple, Union, cast
 
 from collections import defaultdict
 from dataclasses import dataclass
 
 from bson import ObjectId
 
 from featurebyte.common.progress import get_ranged_progress_callback
+from featurebyte.common.utils import timer
 from featurebyte.feast.model.registry import FeastRegistryModel
 from featurebyte.feast.schema.registry import FeastRegistryCreate, FeastRegistryUpdate
 from featurebyte.feast.service.registry import FeastRegistryService
+from featurebyte.logging import get_logger
 from featurebyte.models.base import PydanticObjectId
+from featurebyte.models.deployment import DeploymentModel
+from featurebyte.models.entity import EntityModel
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_list import FeatureListModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.models.offline_store_feature_table import (
     FeaturesUpdate,
     OfflineStoreFeatureTableModel,
 )
 from featurebyte.models.offline_store_ingest_query import OfflineStoreIngestQueryGraph
 from featurebyte.query_graph.model.feature_job_setting import FeatureJobSetting
 from featurebyte.service.catalog import CatalogService
+from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.entity import EntityService
 from featurebyte.service.entity_lookup_feature_table import EntityLookupFeatureTableService
+from featurebyte.service.exception import OfflineStoreFeatureTableBadStateError
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_materialize import FeatureMaterializeService
 from featurebyte.service.feature_materialize_scheduler import FeatureMaterializeSchedulerService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.offline_store_feature_table import OfflineStoreFeatureTableService
 from featurebyte.service.offline_store_feature_table_comment import (
     ColumnComment,
     OfflineStoreFeatureTableCommentService,
     TableComment,
 )
 from featurebyte.service.offline_store_feature_table_construction import (
     OfflineStoreFeatureTableConstructionService,
 )
-from featurebyte.service.online_store_compute_query_service import OnlineStoreComputeQueryService
+
+logger = get_logger(__name__)
 
 
 @dataclass
 class OfflineIngestGraphContainer:
     """
     OfflineIngestGraphContainer class
 
@@ -133,63 +141,86 @@
         catalog_id: ObjectId,
         catalog_service: CatalogService,
         feature_store_service: FeatureStoreService,
         offline_store_feature_table_service: OfflineStoreFeatureTableService,
         offline_store_feature_table_construction_service: OfflineStoreFeatureTableConstructionService,
         offline_store_feature_table_comment_service: OfflineStoreFeatureTableCommentService,
         feature_service: FeatureService,
-        online_store_compute_query_service: OnlineStoreComputeQueryService,
         entity_service: EntityService,
         feature_materialize_service: FeatureMaterializeService,
         feature_materialize_scheduler_service: FeatureMaterializeSchedulerService,
         feast_registry_service: FeastRegistryService,
         feature_list_service: FeatureListService,
         entity_lookup_feature_table_service: EntityLookupFeatureTableService,
+        deployment_service: DeploymentService,
     ):
         self.catalog_id = catalog_id
         self.catalog_service = catalog_service
         self.feature_store_service = feature_store_service
         self.offline_store_feature_table_service = offline_store_feature_table_service
         self.offline_store_feature_table_construction_service = (
             offline_store_feature_table_construction_service
         )
         self.offline_store_feature_table_comment_service = (
             offline_store_feature_table_comment_service
         )
         self.feature_service = feature_service
-        self.online_store_compute_query_service = online_store_compute_query_service
         self.entity_service = entity_service
         self.feature_materialize_service = feature_materialize_service
         self.feature_materialize_scheduler_service = feature_materialize_scheduler_service
         self.feast_registry_service = feast_registry_service
         self.feature_list_service = feature_list_service
         self.entity_lookup_feature_table_service = entity_lookup_feature_table_service
+        self.deployment_service = deployment_service
 
     async def handle_online_enabled_features(
         self,
         features: List[FeatureModel],
+        feature_list_to_online_enable: FeatureListModel,
+        deployment: DeploymentModel,
         update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
     ) -> None:
         """
         Handles the case where features are enabled for online serving by updating all affected
         feature tables.
 
         Parameters
         ----------
         features: List[FeatureModel]
             Features to be enabled for online serving
+        feature_list_to_online_enable: FeatureListModel
+            Feature list model to deploy (may not be enabled yet)
+        deployment: DeploymentModel
+            Deployment ID
         update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]
             Optional callback to update progress
         """
+        await self._delete_deprecated_entity_lookup_feature_tables()
+
         ingest_graph_container = await OfflineIngestGraphContainer.build(features)
-        feature_lists = await self._get_online_enabled_feature_lists()
+        feature_lists = await self._get_online_enabled_feature_lists(
+            feature_list_to_online_enable=feature_list_to_online_enable
+        )
         # Refresh feast registry since it's needed for when materializing the features from offline
         # store feature tables to online store
-        await self._create_or_update_feast_registry(feature_lists)
+        await self._create_or_update_feast_registry(
+            feature_lists=feature_lists,
+            active_feature_list=feature_list_to_online_enable,
+            deployment=deployment,
+            to_enable=True,
+        )
         offline_table_count = len(ingest_graph_container.offline_store_table_name_to_features)
+        feature_store_model = await self._get_feature_store_model()
+
+        # Update precomputed lookup feature tables for existing features
+        await self._update_precomputed_lookup_feature_tables_enable_deployment(
+            feature_list_to_online_enable,
+            feature_store_model,
+            deployment,
+        )
 
         new_tables = []
         for idx, (
             offline_store_table_name,
             offline_store_table_features,
         ) in enumerate(ingest_graph_container.iterate_features_by_table_name()):
             feature_table_dict = await self._get_compatible_existing_feature_table(
@@ -203,104 +234,172 @@
 
             # update existing table
             feature_ids = feature_table_dict["feature_ids"][:]
             feature_ids_set = set(feature_ids)
             for feature in offline_store_table_features:
                 if feature.id not in feature_ids_set:
                     feature_ids.append(feature.id)
+
+            # Initialize precomputed lookup feature tables using updated feature_ids
+            await self._update_precomputed_lookup_feature_table_enable_deployment(
+                feature_table_dict["_id"],
+                feature_ids,
+                feature_list_to_online_enable,
+                feature_store_model,
+                deployment,
+            )
+            await self.offline_store_feature_table_service.add_deployment_id(
+                document_id=feature_table_dict["_id"], deployment_id=deployment.id
+            )
+
             if feature_ids != feature_table_dict["feature_ids"]:
                 feature_table_model = await self._update_offline_store_feature_table(
                     feature_table_dict,
                     feature_ids,
                 )
             else:
                 # Note: don't set feature_table_model here since we don't want to run
                 # materialize below
                 feature_table_model = None
 
             if update_progress:
-                await update_progress(
-                    int((idx + 1) / offline_table_count * 60),
-                    f"Materializing features to online store for table {offline_store_table_name}",
+                message = (
+                    f"Materializing features to online store for table {offline_store_table_name} "
+                    f"({idx + 1} / {offline_table_count} tables)"
                 )
+                await update_progress(int((idx + 1) / offline_table_count * 90), message)
 
             if feature_table_model is not None:
                 await self.feature_materialize_service.initialize_new_columns(feature_table_model)
                 await self.feature_materialize_scheduler_service.start_job_if_not_exist(
                     feature_table_model
                 )
 
-        feature_store_model = await self._get_feature_store_model()
-        new_tables.extend(
-            await self._create_or_update_entity_lookup_feature_tables(
-                feature_lists,
-                feature_store_model,
-                get_ranged_progress_callback(update_progress, 60, 90) if update_progress else None,
-            )
-        )
-
         # Add comments to newly created tables and columns
         await self._update_table_and_column_comments(
             new_tables=new_tables,
             new_features=features,
             feature_store_model=feature_store_model,
-            update_progress=get_ranged_progress_callback(update_progress, 90, 100)
-            if update_progress
-            else None,
+            update_progress=(
+                get_ranged_progress_callback(update_progress, 90, 100) if update_progress else None
+            ),
         )
 
     async def handle_online_disabled_features(
         self,
-        features: List[FeatureModel],
+        feature_list_to_online_disable: FeatureListModel,
+        deployment: DeploymentModel,
         update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
     ) -> None:
         """
         Handles the case where a feature is disabled for online serving by updating all affected
         feature tables. In normal case there should only be one.
 
         Parameters
         ----------
-        features: FeatureModel
-            Model of the feature to be disabled for online serving
+        feature_list_to_online_disable: FeatureListModel
+            Feature list model to not deploy (may not be disabled yet)
+        deployment: DeploymentModel
+            Deployment
         update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]
             Optional callback to update progress
         """
-        feature_ids_to_remove = {feature.id for feature in features}
+        await self._delete_deprecated_entity_lookup_feature_tables()
+
+        feature_ids_to_remove = await self.feature_service.get_online_disabled_feature_ids()
         feature_table_data = await self.offline_store_feature_table_service.list_documents_as_dict(
-            query_filter={"feature_ids": {"$in": list(feature_ids_to_remove)}},
+            query_filter={
+                "$and": [
+                    {
+                        "$or": [
+                            {"feature_ids": {"$in": list(feature_ids_to_remove)}},
+                            {"feature_ids": {"$size": 0}},
+                        ]
+                    },
+                    {
+                        # Filter out precomputed lookup feature tables since those always have empty
+                        # feature_ids
+                        "$or": [
+                            {"precomputed_lookup_feature_table_info": None},
+                            {"precomputed_lookup_feature_table_info": {"$exists": False}},
+                        ],
+                    },
+                ]
+            },
+        )
+        feature_lists = await self._get_online_enabled_feature_lists(
+            feature_list_to_online_disable=feature_list_to_online_disable
         )
-        feature_lists = await self._get_online_enabled_feature_lists()
         offline_table_count = len(feature_table_data["data"])
 
+        # Update precomputed lookup feature tables based on the deployment that is being disabled
+        await self._update_precomputed_lookup_feature_tables_disable_deployment(
+            deployment.id,
+        )
+
         for idx, feature_table_dict in enumerate(feature_table_data["data"]):
             updated_feature_ids = [
                 feature_id
                 for feature_id in feature_table_dict["feature_ids"]
                 if feature_id not in feature_ids_to_remove
             ]
+            await self.offline_store_feature_table_service.remove_deployment_id(
+                document_id=feature_table_dict["_id"], deployment_id=deployment.id
+            )
+
             if updated_feature_ids:
-                updated_feature_table = await self._update_offline_store_feature_table(
-                    feature_table_dict,
-                    updated_feature_ids,
-                )
+                try:
+                    updated_feature_table = await self._update_offline_store_feature_table(
+                        feature_table_dict,
+                        updated_feature_ids,
+                    )
+                except OfflineStoreFeatureTableBadStateError:
+                    logger.error(
+                        "Failed to update offline store feature table when disabling a deployment",
+                        exc_info=True,
+                        extra={"table_name": feature_table_dict["name"]},
+                    )
+                    await self._delete_offline_store_feature_table(feature_table_dict["_id"])
+                    continue
+                removed_feature_ids = list(
+                    set(feature_table_dict["feature_ids"]) - set(updated_feature_ids)
+                )
+                removed_features = []
+                async for feature in self.feature_service.list_documents_iterator(
+                    {"_id": {"$in": removed_feature_ids}}
+                ):
+                    removed_features.append(feature)
+                columns_to_drop = self._get_offline_store_feature_table_columns(removed_features)
                 await self.feature_materialize_service.drop_columns(
-                    updated_feature_table, self._get_offline_store_feature_table_columns(features)
+                    updated_feature_table, columns_to_drop
                 )
+                async for (
+                    lookup_feature_table
+                ) in self.offline_store_feature_table_service.list_precomputed_lookup_feature_tables_from_source(
+                    feature_table_dict["_id"]
+                ):
+                    await self.feature_materialize_service.drop_columns(
+                        lookup_feature_table, columns_to_drop
+                    )
             else:
                 await self._delete_offline_store_feature_table(feature_table_dict["_id"])
 
             if update_progress:
-                await update_progress(
-                    int((idx + 1) / offline_table_count * 90),
-                    f"Updating offline store feature table {feature_table_dict['name']} for online disabling features",
-                )
-
-        await self._create_or_update_feast_registry(feature_lists)
-        feature_store_model = await self._get_feature_store_model()
-        await self._delete_entity_lookup_feature_tables(feature_lists, feature_store_model)
+                message = (
+                    f"Updating offline store feature table {feature_table_dict['name']} for online disabling features "
+                    f"({idx + 1} / {offline_table_count} tables)"
+                )
+                await update_progress(int((idx + 1) / offline_table_count * 90), message)
+
+        await self._create_or_update_feast_registry(
+            feature_lists=feature_lists,
+            active_feature_list=feature_list_to_online_disable,
+            deployment=deployment,
+            to_enable=False,
+        )
         if update_progress:
             await update_progress(100, "Updated entity lookup feature tables")
 
     @staticmethod
     def _get_offline_store_feature_table_columns(features: List[FeatureModel]) -> List[str]:
         for feature_model in features:
             info = feature_model.offline_store_info
@@ -340,215 +439,280 @@
         return cast(
             OfflineStoreFeatureTableModel,
             await self.offline_store_feature_table_service.update_document(
                 document_id=feature_table_dict["_id"], data=update_schema
             ),
         )
 
-    async def _construct_offline_store_feature_table_model(
+    async def _get_entities(
+        self, entity_ids: Optional[List[PydanticObjectId]]
+    ) -> List[EntityModel]:
+        entities_mapping = {}
+        query_filter = {} if entity_ids is None else {"_id": {"$in": entity_ids}}
+        async for entity_model in self.entity_service.list_documents_iterator(
+            query_filter=query_filter
+        ):
+            entities_mapping[entity_model.id] = entity_model
+        if entity_ids is not None:
+            # Preserve ordering of the provided entity_ids
+            entities = [entities_mapping[entity_id] for entity_id in entity_ids]
+        else:
+            entities = list(entities_mapping.values())
+        return entities
+
+    async def _get_feature_ids_to_model(
         self,
-        feature_table_name: str,
         feature_ids: List[ObjectId],
-        primary_entity_ids: List[PydanticObjectId],
-        has_ttl: bool,
-        feature_job_setting: Optional[FeatureJobSetting],
-    ) -> OfflineStoreFeatureTableModel:
+    ) -> Dict[ObjectId, FeatureModel]:
         feature_ids_to_model: Dict[ObjectId, FeatureModel] = {}
         async for feature_model in self.feature_service.list_documents_iterator(
             query_filter={"_id": {"$in": feature_ids}}
         ):
             feature_ids_to_model[feature_model.id] = feature_model
+        return feature_ids_to_model
 
-        primary_entities_mapping = {}
-        async for entity_model in self.entity_service.list_documents_iterator(
-            query_filter={"_id": {"$in": primary_entity_ids}}
-        ):
-            primary_entities_mapping[entity_model.id] = entity_model
-        primary_entities = [
-            primary_entities_mapping[primary_entity_id] for primary_entity_id in primary_entity_ids
-        ]
-
-        required_aggregate_result_tables = await self._get_required_aggregate_result_tables(
-            feature_id_to_models=feature_ids_to_model,
-            primary_entity_serving_names=[entity.serving_names[0] for entity in primary_entities],
-        )
+    async def _construct_offline_store_feature_table_model(
+        self,
+        feature_table_name: str,
+        feature_ids: List[ObjectId],
+        primary_entity_ids: List[PydanticObjectId],
+        has_ttl: bool,
+        feature_job_setting: Optional[FeatureJobSetting],
+    ) -> OfflineStoreFeatureTableModel:
+        feature_ids_to_model = await self._get_feature_ids_to_model(feature_ids)
+        primary_entities = await self._get_entities(primary_entity_ids)
 
         catalog_model = await self.catalog_service.get_document(self.catalog_id)
         feature_store_model = await self.feature_store_service.get_document(
             catalog_model.default_feature_store_ids[0]
         )
 
         return await self.offline_store_feature_table_construction_service.get_offline_store_feature_table_model(
             feature_table_name=feature_table_name,
             features=[feature_ids_to_model[feature_id] for feature_id in feature_ids],
-            aggregate_result_table_names=required_aggregate_result_tables,
             primary_entities=primary_entities,
             has_ttl=has_ttl,
             feature_job_setting=feature_job_setting,
             source_type=feature_store_model.type,
         )
 
-    async def _get_required_aggregate_result_tables(
+    async def _get_online_enabled_feature_lists(
         self,
-        feature_id_to_models: Dict[ObjectId, FeatureModel],
-        primary_entity_serving_names: List[str],
-    ) -> List[str]:
-        aggregate_result_table_names = set()
-        for feature_model in feature_id_to_models.values():
-            aggregate_result_table_names.update(feature_model.online_store_table_names)
-        aggregate_result_table_names = list(aggregate_result_table_names)  # type: ignore[assignment]
-
-        # Get aggregate result tables
-        filtered_table_names = []
-        if primary_entity_serving_names:
-            query_filter = {
-                "table_name": {"$in": aggregate_result_table_names},
-                "serving_names": {
-                    "$all": primary_entity_serving_names,
-                    "$size": len(primary_entity_serving_names),
-                },
-            }
-        else:
-            query_filter = {
-                "table_name": {"$in": aggregate_result_table_names},
-            }
-
-        async for online_store_compute_query_model in self.online_store_compute_query_service.list_documents_iterator(
-            query_filter=query_filter
-        ):
-            filtered_table_names.append(online_store_compute_query_model.table_name)
-
-        return sorted(set(filtered_table_names))
-
-    async def _get_online_enabled_feature_lists(self) -> List[FeatureListModel]:
+        feature_list_to_online_enable: Optional[FeatureListModel] = None,
+        feature_list_to_online_disable: Optional[FeatureListModel] = None,
+    ) -> List[FeatureListModel]:
         feature_lists = []
-        async for feature_list_dict in self.feature_list_service.iterate_online_enabled_feature_lists_as_dict():
-            feature_lists.append(FeatureListModel(**feature_list_dict))
+        fl_has_been_enabled = False
+        async for (
+            feature_list_dict
+        ) in self.feature_list_service.iterate_online_enabled_feature_lists_as_dict():
+            feature_list = FeatureListModel(**feature_list_dict)
+            if (
+                feature_list_to_online_disable
+                and feature_list.id == feature_list_to_online_disable.id
+            ):
+                # skip the feature list to be disabled
+                continue
+
+            if feature_list.store_info.feast_enabled:
+                feature_lists.append(feature_list)
+                if (
+                    feature_list_to_online_enable
+                    and feature_list.id == feature_list_to_online_enable.id
+                ):
+                    fl_has_been_enabled = True
+
+        if feature_list_to_online_enable and not fl_has_been_enabled:
+            # add the feature list to be enabled
+            feature_lists.append(feature_list_to_online_enable)
         return feature_lists
 
     async def _create_or_update_feast_registry(
-        self, feature_lists: List[FeatureListModel]
-    ) -> FeastRegistryModel:
-        feast_registry = await self.feast_registry_service.get_feast_registry_for_catalog()
+        self,
+        feature_lists: List[FeatureListModel],
+        active_feature_list: FeatureListModel,
+        deployment: DeploymentModel,
+        to_enable: bool,
+    ) -> Optional[FeastRegistryModel]:
+        feast_registry = await self.feast_registry_service.get_feast_registry(deployment)
         if feast_registry is None:
-            return await self.feast_registry_service.create_document(
-                FeastRegistryCreate(feature_lists=feature_lists)
+            if to_enable:
+                # create a new deployment specific feast registry
+                return await self.feast_registry_service.create_document(
+                    FeastRegistryCreate(
+                        feature_lists=[active_feature_list], deployment_id=deployment.id
+                    )
+                )
+
+            # no feast registry to update required for disabling a deployment that doesn't have feast registry
+            return None
+
+        if feast_registry.deployment_id is None:
+            # for non-deployment specific feast registry, no need to update the feast registry using
+            # existing online-enabled feature lists
+            output = await self.feast_registry_service.update_document(
+                document_id=feast_registry.id,
+                data=FeastRegistryUpdate(feature_lists=feature_lists),
+                populate_remote_attributes=False,
             )
+            return output
 
+        # for deployment specific feast registry, update the feast registry with specific feature list
         output = await self.feast_registry_service.update_document(
             document_id=feast_registry.id,
-            data=FeastRegistryUpdate(feature_lists=feature_lists),
+            data=FeastRegistryUpdate(
+                feature_lists=[active_feature_list] if to_enable else [],
+            ),
+            populate_remote_attributes=False,
         )
-        assert output is not None
         return output
 
-    async def _create_or_update_entity_lookup_feature_tables(
+    async def _update_precomputed_lookup_feature_tables_enable_deployment(
         self,
-        feature_lists: List[FeatureListModel],
+        active_feature_list: FeatureListModel,
         feature_store_model: FeatureStoreModel,
-        update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
-    ) -> List[OfflineStoreFeatureTableModel]:
-        """
-        Create or update feature tables to support parent entities lookup for a specific offline
-        feature table. This should be called when online enabling features.
+        deployment: DeploymentModel,
+    ) -> None:
+        async for (
+            feature_table_dict
+        ) in self.offline_store_feature_table_service.list_documents_as_dict_iterator(
+            query_filter={"precomputed_lookup_feature_table_info": None},
+            projection={"_id": 1, "feature_ids": 1},
+        ):
+            feature_ids = feature_table_dict["feature_ids"]
+            await self._update_precomputed_lookup_feature_table_enable_deployment(
+                feature_table_dict["_id"],
+                feature_ids,
+                active_feature_list,
+                feature_store_model,
+                deployment,
+            )
 
-        Parameters
-        ----------
-        feature_lists: List[FeatureListModel]
-            Currently online enabled feature lists
-        feature_store_model: FeatureStoreModel
-            Feature store model
-        update_progress: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]
-            Optional callback to update progress
+    async def _update_precomputed_lookup_feature_table_enable_deployment(
+        self,
+        feature_table_id: PydanticObjectId,
+        feature_ids: List[PydanticObjectId],
+        active_feature_list: FeatureListModel,
+        feature_store_model: FeatureStoreModel,
+        deployment: DeploymentModel,
+    ) -> None:
+        # Reload feature table to get the most updated state of the feature table document
+        feature_table_dict = await self.offline_store_feature_table_service.get_document_as_dict(
+            feature_table_id,
+            projection={
+                "_id": 1,
+                "name": 1,
+                "primary_entity_ids": 1,
+                "has_ttl": 1,
+            },
+        )
+
+        all_entities = await self._get_entities(entity_ids=None)
+        with timer("Get precomputed lookup feature table", logger=logger):
+            precomputed_lookup_feature_table = (
+                await self.entity_lookup_feature_table_service.get_precomputed_lookup_feature_table(
+                    primary_entity_ids=feature_table_dict["primary_entity_ids"],
+                    feature_ids=feature_ids,
+                    feature_list=active_feature_list,
+                    full_serving_entity_ids=(
+                        deployment.serving_entity_ids or active_feature_list.primary_entity_ids
+                    ),
+                    feature_table_name=feature_table_dict["name"],
+                    feature_table_has_ttl=feature_table_dict["has_ttl"],
+                    entity_id_to_serving_name={
+                        entity.id: entity.serving_names[0] for entity in all_entities
+                    },
+                    feature_table_id=feature_table_id,
+                    feature_store_model=feature_store_model,
+                )
+            )
 
-        Returns
-        -------
-        List[OfflineStoreFeatureTableModel]
-        """
-        # Get list of entity lookup feature tables that are currently required
-        service = self.entity_lookup_feature_table_service
-        entity_lookup_feature_table_models = await service.get_entity_lookup_feature_tables(
-            feature_lists, feature_store_model
-        )
-        if entity_lookup_feature_table_models is None:
-            entity_lookup_feature_table_models = []
-
-        # Commission tables that should exist if they don't already exist
-        existing_lookup_feature_tables = {
-            feature_table_dict["name"]: feature_table_dict
-            async for feature_table_dict in self.offline_store_feature_table_service.list_documents_as_dict_iterator(
-                query_filter={"entity_lookup_info": {"$ne": None}}, projection={"_id": 1, "name": 1}
+        if precomputed_lookup_feature_table is None:
+            return
+
+        # Create the table if it doesn't
+        existing_table = None
+        async for doc in self.offline_store_feature_table_service.list_documents_iterator(
+            query_filter={"name": precomputed_lookup_feature_table.name},
+        ):
+            existing_table = doc
+            break
+
+        if existing_table is None:
+            created_table = await self.offline_store_feature_table_service.create_document(
+                precomputed_lookup_feature_table
             )
-        }
-        new_tables = []
-        for idx, entity_lookup_feature_table in enumerate(entity_lookup_feature_table_models):
-            if update_progress:
-                await update_progress(
-                    int(idx / len(entity_lookup_feature_table_models) * 100),
-                    f"Materializing entity look up table {entity_lookup_feature_table.name}",
-                )
+            table_id = created_table.id
+        else:
+            created_table = None
+            table_id = existing_table.id
 
-            if entity_lookup_feature_table.name not in existing_lookup_feature_tables:
-                entity_lookup_feature_table = (
-                    await self.offline_store_feature_table_service.create_document(
-                        entity_lookup_feature_table
-                    )
-                )
-                await self.feature_materialize_service.initialize_new_columns(
-                    entity_lookup_feature_table
-                )
-                await self.feature_materialize_scheduler_service.start_job_if_not_exist(
-                    entity_lookup_feature_table
-                )
-                new_tables.append(entity_lookup_feature_table)
+        # Update deployment_ids references
+        await self.offline_store_feature_table_service.add_deployment_id(
+            document_id=table_id, deployment_id=deployment.id
+        )
 
-        return new_tables
+        # Initialize the table if necessary. This has to be done after deployment_ids is updated.
+        if created_table is not None:
+            await self.feature_materialize_service.initialize_precomputed_lookup_feature_table(
+                feature_table_id, [created_table]
+            )
 
-    async def _delete_entity_lookup_feature_tables(
-        self, feature_lists: List[FeatureListModel], feature_store_model: FeatureStoreModel
+    async def _update_precomputed_lookup_feature_tables_disable_deployment(
+        self,
+        deployment_id: PydanticObjectId,
     ) -> None:
-        """
-        Delete offline store feature tables that were created for entity lookup purpose but are not
-        no longer required. This should be called when online disabling features.
-
-        Parameters
-        ----------
-        feature_lists: List[FeatureListModel]
-            Currently online enabled feature lists
-        feature_store_model: FeatureStoreModel
-            Feature store
-        """
-        # Get list of entity lookup feature tables that are currently required
-        current_active_table_names = set()
-        service = self.entity_lookup_feature_table_service
-        entity_lookup_feature_table_models = await service.get_entity_lookup_feature_tables(
-            feature_lists, feature_store_model
-        )
-        if entity_lookup_feature_table_models is not None:
-            for table in entity_lookup_feature_table_models:
-                current_active_table_names.add(table.name)
-
-        # Decommission tables that should no longer exist
-        async for feature_table_dict in self.offline_store_feature_table_service.list_documents_as_dict_iterator(
-            query_filter={"entity_lookup_info": {"$ne": None}}, projection={"_id": 1, "name": 1}
+        service = self.offline_store_feature_table_service
+        async for table in service.list_precomputed_lookup_feature_tables_for_deployment(
+            deployment_id
         ):
-            if feature_table_dict["name"] not in current_active_table_names:
-                await self._delete_offline_store_feature_table(feature_table_dict["_id"])
+            await self.offline_store_feature_table_service.remove_deployment_id(
+                document_id=table.id, deployment_id=deployment_id
+            )
+            updated_table_dict = (
+                await self.offline_store_feature_table_service.get_document_as_dict(
+                    table.id,
+                    projection={"deployment_ids": 1},
+                )
+            )
+            if len(updated_table_dict.get("deployment_ids", [])) == 0:
+                await self._delete_offline_store_feature_table(table.id)
+
+    async def _delete_deprecated_entity_lookup_feature_tables(self) -> None:
+        # Clean up dynamic entity lookup feature tables which are no longer used
+        service = self.offline_store_feature_table_service
+        async for doc in service.list_deprecated_entity_lookup_feature_tables_as_dict():
+            await self._delete_offline_store_feature_table(doc["_id"])
 
     async def _delete_offline_store_feature_table(self, feature_table_id: ObjectId) -> None:
+        feature_table_dict = await self.offline_store_feature_table_service.get_document_as_dict(
+            feature_table_id, projection={"_id": 1, "precomputed_lookup_feature_table_info": 1}
+        )
+        if feature_table_dict.get("precomputed_lookup_feature_table_info") is None:
+            await self.feature_materialize_scheduler_service.stop_job(
+                feature_table_id,
+            )
         await self.feature_materialize_service.drop_table(
             await self.offline_store_feature_table_service.get_document(
                 feature_table_id,
             )
         )
-        await self.feature_materialize_scheduler_service.stop_job(
-            feature_table_id,
-        )
         await self.offline_store_feature_table_service.delete_document(feature_table_id)
 
+        # Clean up precomputed lookup feature tables. Usually this is a no-op since those tables
+        # would have been cleaned up by _update_precomputed_lookup_feature_tables_disable_deployment
+        # already. The cleanup here is useful for handling bad state.
+        async for (
+            lookup_feature_table
+        ) in self.offline_store_feature_table_service.list_precomputed_lookup_feature_tables_from_source(
+            source_feature_table_id=feature_table_id,
+        ):
+            await self.feature_materialize_service.drop_table(lookup_feature_table)
+            await self.offline_store_feature_table_service.delete_document(lookup_feature_table.id)
+
     async def _get_feature_store_model(self) -> FeatureStoreModel:
         catalog_model = await self.catalog_service.get_document(self.catalog_id)
         feature_store_model = await self.feature_store_service.get_document(
             catalog_model.default_feature_store_ids[0]
         )
         return feature_store_model
 
@@ -570,7 +734,32 @@
             await self.offline_store_feature_table_comment_service.generate_column_comments(
                 new_features
             )
         )
         await self.offline_store_feature_table_comment_service.apply_comments(
             feature_store_model, comments, update_progress
         )
+
+    async def update_table_deployment_reference(
+        self, feature_ids: List[PydanticObjectId], deployment_id: ObjectId, to_enable: bool
+    ) -> None:
+        """
+        Update deployment reference in offline store feature tables
+
+        Parameters
+        ----------
+        feature_ids: List[PydanticObjectId]
+            Features IDs used to find offline store feature tables
+        deployment_id: ObjectId
+            Deployment to update
+        to_enable: bool
+            Whether to enable or disable the deployment
+        """
+        if to_enable:
+            update = {"$addToSet": {"deployment_ids": deployment_id}}
+        else:
+            update = {"$pull": {"deployment_ids": deployment_id}}
+
+        await self.offline_store_feature_table_service.update_documents(
+            query_filter={"feature_ids": {"$in": feature_ids}},
+            update=update,
+        )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/online_serving.py` & `featurebyte-1.0.3/featurebyte/service/online_serving.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,43 +1,45 @@
 """
 OnlineServingService class
 """
+
 from __future__ import annotations
 
-from typing import Any, Dict, List, Optional, Set, Union
+from typing import Any, Dict, List, Optional, Union
 
 import json
 import os
 import time
 from dataclasses import dataclass
 from datetime import datetime
 from unittest.mock import patch
 
 import pandas as pd
-from bson import ObjectId
+from feast.base_feature_view import BaseFeatureView
 from feast.feature_store import FeatureStore as FeastFeatureStore
+from feast.on_demand_feature_view import OnDemandFeatureView
 from jinja2 import Template
 
 from featurebyte.enum import SpecialColumnName
 from featurebyte.exception import (
     FeatureListNotOnlineEnabledError,
     RequiredEntityNotProvidedError,
     UnsupportedRequestCodeTemplateLanguage,
 )
-from featurebyte.feast.patch import augment_response_with_on_demand_transforms
+from featurebyte.feast.patch import (
+    augment_response_with_on_demand_transforms,
+    get_transformed_features_df,
+    with_projection,
+)
 from featurebyte.logging import get_logger
 from featurebyte.models.base import PydanticObjectId, VersionIdentifier
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.models.deployment import DeploymentModel
-from featurebyte.models.entity import EntityModel
-from featurebyte.models.entity_lookup_feature_table import get_lookup_feature_table_name
 from featurebyte.models.entity_validation import EntityInfo
 from featurebyte.models.feature_list import FeatureCluster, FeatureListModel
-from featurebyte.query_graph.model.entity_lookup_plan import EntityLookupPlanner
-from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.query_graph.node.generic import GroupByNode
 from featurebyte.query_graph.node.request import RequestColumnNode
 from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.entity import (
     get_combined_serving_names,
     get_combined_serving_names_python,
 )
@@ -169,98 +171,87 @@
         if features is None:
             return None
         return OnlineFeaturesResponseModel(features=features)
 
     async def get_online_features_by_feast(  # pylint: disable=too-many-locals
         self,
         feature_list: FeatureListModel,
+        deployment: DeploymentModel,
         feast_store: FeastFeatureStore,
         request_data: List[Dict[str, Any]],
     ) -> OnlineFeaturesResponseModel:
         """
         Get online features for a Feature List via feast online store
 
         Parameters
         ----------
         feature_list: FeatureListModel
             Feature List
+        deployment: DeploymentModel
+            Deployment model
         feast_store: FeastFeatureStore
             FeastFeatureStore object
         request_data: List[Dict[str, Any]]
             Request data containing entity serving names
 
         Returns
         -------
         OnlineFeaturesResponseModel
+
+        Raises
+        ------
+        RequiredEntityNotProvidedError
+            If required entities for serving are not provided
         """
         assert feature_list.feature_clusters is not None
         feature_cluster = feature_list.feature_clusters[0]
 
         # Original request data to be concatenated with features retrieved from feast
         df_features = [pd.DataFrame(request_data)]
-        entity_id_to_model = await self._get_entity_id_to_model_mapping(feature_list)
 
         # Lookup parent entities to retrieve feature list's primary entity. This will validate that
         # the required entities are present.
         request_column_names = set(request_data[0].keys())
         required_entities = await self.entity_service.get_entities(
-            set(feature_list.primary_entity_ids)
+            set(deployment.serving_entity_ids or feature_list.primary_entity_ids)
         )
         provided_entities = await self.entity_service.get_entities_with_serving_names(
             request_column_names,
         )
-        entity_info = EntityInfo(
-            required_entities=required_entities,
-            provided_entities=provided_entities,
-        )
-
-        if entity_info.missing_entities:
-            request_data = self._lookup_parent_entities_by_feast(
-                request_data=request_data,
-                feature_list=feature_list,
-                entity_info=entity_info,
-                feast_store=feast_store,
-                entity_id_to_model=entity_id_to_model,
+        provided_entity_ids = {entity.id for entity in provided_entities}
+        if not provided_entity_ids.issuperset([entity.id for entity in required_entities]):
+            # Provided entities cannot be served, raise an error message with information
+            entity_info = EntityInfo(
+                required_entities=required_entities,
+                provided_entities=provided_entities,
+            )
+            raise RequiredEntityNotProvidedError(  # pylint: disable=raise-missing-from
+                entity_info.format_missing_entities_error(
+                    [entity.id for entity in entity_info.missing_entities]
+                )
             )
 
         # Map feature names to the original names
-        feature_docs = await self.feature_service.list_documents_as_dict(
-            query_filter={"_id": {"$in": feature_list.feature_ids}},
-            projection={"name": 1, "version": 1},
-        )
         feature_name_map = {}
         feature_id_to_versioned_name = {}
-        for feature_doc in feature_docs["data"]:
+        async for feature_doc in self.feature_service.list_documents_as_dict_iterator(
+            query_filter={"_id": {"$in": feature_list.feature_ids}},
+            projection={"name": 1, "version": 1},
+        ):
             feature_name = feature_doc["name"]
             feature_version = VersionIdentifier(**feature_doc["version"]).to_str()
             feature_name_map[f"{feature_name}_{feature_version}"] = feature_name
             feature_id_to_versioned_name[feature_doc["_id"]] = f"{feature_name}_{feature_version}"
 
         # Include point in time column if it is required
         if self._require_point_in_time_request_column(feature_cluster):
             point_in_time_value = datetime.utcnow().isoformat()
         else:
             point_in_time_value = None
 
-        # Now request data has the feature list's primary entity. Lookup additional entities that
-        # are their parents, if needed.
-        combined_lookup_steps: List[EntityRelationshipInfo] = []
-        if feature_list.features_entity_lookup_info:
-            for info in feature_list.features_entity_lookup_info:
-                for step in info.join_steps:
-                    if step not in combined_lookup_steps:
-                        combined_lookup_steps.extend(info.join_steps)
-
-        self._apply_entity_lookup_steps(
-            feast_store=feast_store,
-            entity_rows=request_data,
-            lookup_steps=combined_lookup_steps,
-            entity_id_to_model=entity_id_to_model,
-        )
-
         tic = time.time()
         df_feast_online_features = await self._get_online_features_feast(
             feast_store=feast_store,
             feast_service_name=feature_list.versioned_name,
             feature_id_to_versioned_name=feature_id_to_versioned_name,
             point_in_time_value=point_in_time_value,
             request_data=request_data,
@@ -270,126 +261,14 @@
 
         online_features_df = pd.concat(df_features, axis=1)
         online_features_df.rename(columns=feature_name_map, inplace=True)
 
         features = online_features_df.to_dict(orient="records")
         return OnlineFeaturesResponseModel(features=features)
 
-    async def _get_entity_id_to_model_mapping(
-        self, feature_list: FeatureListModel
-    ) -> Dict[PydanticObjectId, EntityModel]:
-        # Get entity models required for validation and lookup steps
-        entity_ids: Set[ObjectId] = set()
-        if feature_list.relationships_info is not None:
-            for join_step in feature_list.relationships_info:
-                entity_ids.add(join_step.entity_id)
-                entity_ids.add(join_step.related_entity_id)
-        if feature_list.features_entity_lookup_info is not None:
-            for lookup_info in feature_list.features_entity_lookup_info:
-                if not lookup_info.join_steps:
-                    continue
-                for join_step in lookup_info.join_steps:
-                    entity_ids.add(join_step.entity_id)
-                    entity_ids.add(join_step.related_entity_id)
-        entity_id_to_model = {
-            entity.id: entity for entity in await self.entity_service.get_entities(entity_ids)
-        }
-        return entity_id_to_model
-
-    @classmethod
-    def _lookup_parent_entities_by_feast(
-        cls,
-        request_data: List[Dict[str, Any]],
-        feature_list: FeatureListModel,
-        entity_info: EntityInfo,
-        feast_store: FeastFeatureStore,
-        entity_id_to_model: Dict[PydanticObjectId, EntityModel],
-    ) -> List[Dict[str, Any]]:
-        # Validate missing entities can be looked up based on available relationships
-        try:
-            assert feature_list.relationships_info is not None
-            lookup_steps = EntityLookupPlanner.generate_lookup_steps(
-                available_entity_ids=[entity.id for entity in entity_info.provided_entities],
-                required_entity_ids=[entity.id for entity in entity_info.required_entities],
-                relationships_info=feature_list.relationships_info,
-            )
-        except RequiredEntityNotProvidedError as exc:
-            assert exc.missing_entity_ids is not None
-            raise RequiredEntityNotProvidedError(  # pylint: disable=raise-missing-from
-                entity_info.format_missing_entities_error(exc.missing_entity_ids)
-            )
-
-        # Validate that there are existing entity lookup tables that support the required lookup
-        # steps
-        feature_view_names = {
-            feature_view.name
-            for feature_view in feast_store._list_feature_views(  # pylint: disable=protected-access
-                allow_cache=False, hide_dummy_entity=False
-            )
-        }
-        for lookup_step in lookup_steps:
-            if get_lookup_feature_table_name(lookup_step.id) not in feature_view_names:
-                raise RequiredEntityNotProvidedError(
-                    entity_info.format_missing_entities_error(
-                        [entity.id for entity in entity_info.missing_entities]
-                    )
-                )
-
-        # Lookup parent entities through feast store
-        cls._apply_entity_lookup_steps(
-            feast_store=feast_store,
-            entity_rows=request_data,
-            lookup_steps=lookup_steps,
-            entity_id_to_model=entity_id_to_model,
-        )
-        return request_data
-
-    @staticmethod
-    def _apply_entity_lookup_steps(
-        feast_store: FeastFeatureStore,
-        entity_rows: List[Dict[str, Any]],
-        lookup_steps: List[EntityRelationshipInfo],
-        entity_id_to_model: Dict[PydanticObjectId, EntityModel],
-    ) -> None:
-        """
-        Apply a list of entity lookup steps using the feast online store and update df_entity_rows
-        in place with the retrieved parent entities
-
-        Parameters
-        ----------
-        feast_store: FeastFeatureStore
-            Feast feature store
-        entity_rows: List[Dict[str, Any]]
-            List of the request data with the provided entities, to be updated in-place
-        lookup_steps: List[EntityRelationshipInfo]
-            The list of lookup steps in terms of EntityRelationshipInfo. Each relationship will
-            retrieve its parent entity (related_entity_id) using its child entity (entity_id)
-        entity_id_to_model: Dict[PydanticObjectId, EntityModel]
-            Mapping from entity identifier to EntityModel objects
-        """
-        for lookup_step in lookup_steps:
-            child_entity = entity_id_to_model[lookup_step.entity_id]
-            parent_entity = entity_id_to_model[lookup_step.related_entity_id]
-            input_feature_name = child_entity.serving_names[0]
-            lookup_feature_name = parent_entity.serving_names[0]
-            if lookup_feature_name in entity_rows[0]:
-                continue
-            entity_lookup_rows = [
-                {input_feature_name: row[input_feature_name]} for row in entity_rows
-            ]
-            entity_lookup_feast_spec = [
-                f"{get_lookup_feature_table_name(lookup_step.id)}:{lookup_feature_name}"
-            ]
-            response = feast_store.get_online_features(entity_lookup_feast_spec, entity_lookup_rows)
-            response_dict = response.to_dict()
-            for entity_row, looked_up_value in zip(entity_rows, response_dict[lookup_feature_name]):
-                if looked_up_value is None:
-                    looked_up_value = ""
-                entity_row[lookup_feature_name] = looked_up_value
-
     async def _get_online_features_feast(
         self,
         feast_store: FeastFeatureStore,
         feast_service_name: str,
         feature_id_to_versioned_name: Dict[PydanticObjectId, str],
         request_data: List[Dict[str, Any]],
         point_in_time_value: Optional[str],
@@ -425,40 +304,41 @@
         if point_in_time_value is not None:
             for row in request_data:
                 row[SpecialColumnName.POINT_IN_TIME] = point_in_time_value
 
         # Get required serving names and composite serving names that need further processing
         offline_store_table_docs = (
             await self.offline_store_feature_table_service.list_documents_as_dict(
-                query_filter={"feature_ids": {"$in": list(feature_id_to_versioned_name.keys())}},
-                project_name={"serving_names"},
+                query_filter={},
+                projection={"serving_names": 1},
             )
         )
-        required_serving_names = set()
         composite_serving_names = set()
         for offline_store_table_doc in offline_store_table_docs["data"]:
             serving_names = tuple(offline_store_table_doc["serving_names"])
-            if len(serving_names) == 1:
-                required_serving_names.add(serving_names[0])
-            elif len(serving_names) > 1:
+            if len(serving_names) > 1 and all(
+                serving_name in request_data[0] for serving_name in serving_names
+            ):
                 composite_serving_names.add(serving_names)
 
         # Add concatenated composite serving names
+        required_feast_entity_columns = {entity.name for entity in feast_store.list_entities()}
         added_column_names = []
         if composite_serving_names:
             for serving_names in composite_serving_names:
                 combined_serving_names_col = get_combined_serving_names(list(serving_names))
-                for row in request_data:
-                    row[combined_serving_names_col] = get_combined_serving_names_python(
-                        [row[serving_name] for serving_name in serving_names]
-                    )
-                added_column_names.append(combined_serving_names_col)
+                if combined_serving_names_col in required_feast_entity_columns:
+                    for row in request_data:
+                        row[combined_serving_names_col] = get_combined_serving_names_python(
+                            [row[serving_name] for serving_name in serving_names]
+                        )
+                    added_column_names.append(combined_serving_names_col)
 
         # Get exactly the columns that are required by feast
-        needed_columns = list(required_serving_names) + added_column_names
+        needed_columns = list(required_feast_entity_columns) + added_column_names
         if point_in_time_value:
             needed_columns.append(SpecialColumnName.POINT_IN_TIME.value)
 
         updated_request_data = []
         for row in request_data:
             updated_request_data.append({k: v for (k, v) in row.items() if k in needed_columns})
         versioned_feature_names = [
@@ -468,19 +348,32 @@
 
         # FIXME: This is a temporary fix to avoid the bug in feast 0.35.0
         with patch.object(
             feast_store,
             "_augment_response_with_on_demand_transforms",
             new=augment_response_with_on_demand_transforms,
         ):
-            df_feast_online_features = feast_store.get_online_features(
-                feast_store.get_feature_service(feast_service_name),
-                updated_request_data,
-            ).to_df()[versioned_feature_names]
-            return df_feast_online_features
+            # FIXME: This is a temporary fix to performance issues due to highly fragmented dataframe
+            with patch.object(
+                OnDemandFeatureView,
+                "get_transformed_features_df",
+                new=get_transformed_features_df,
+            ):
+                # FIXME: This is a temporary fix to avoid O(N^2) complexity in with_projection method
+                with patch.object(
+                    BaseFeatureView,
+                    "with_projection",
+                    new=with_projection,
+                ):
+                    feature_service = feast_store.get_feature_service(feast_service_name)
+                    df_feast_online_features = feast_store.get_online_features(
+                        feature_service,
+                        updated_request_data,
+                    ).to_df()[versioned_feature_names]
+                    return df_feast_online_features
 
     @staticmethod
     def _require_point_in_time_request_column(feature_cluster: FeatureCluster) -> bool:
         for node in feature_cluster.nodes:
             for node in feature_cluster.graph.iterate_nodes(node, node_type=None):
                 if isinstance(node, RequestColumnNode):
                     if node.parameters.column_name == SpecialColumnName.POINT_IN_TIME:
```

### Comparing `featurebyte-1.0.2/featurebyte/service/online_store.py` & `featurebyte-1.0.3/featurebyte/service/online_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStoreService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Optional, Type
 
 from bson import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/online_store_cleanup.py` & `featurebyte-1.0.3/featurebyte/service/online_store_cleanup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStoreCleanupService class
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 from sqlglot import expressions
 from sqlglot.expressions import alias_, select
 
 from featurebyte.enum import InternalName
```

### Comparing `featurebyte-1.0.2/featurebyte/service/online_store_cleanup_scheduler.py` & `featurebyte-1.0.3/featurebyte/service/online_store_cleanup_scheduler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStoreCleanupSchedulerService class
 """
+
 from typing import Optional
 
 from bson import ObjectId
 
 from featurebyte.logging import get_logger
 from featurebyte.models.base import User
 from featurebyte.models.periodic_task import Interval, PeriodicTask
```

### Comparing `featurebyte-1.0.2/featurebyte/service/online_store_compute_query_service.py` & `featurebyte-1.0.3/featurebyte/service/online_store_compute_query_service.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStoreComputeQueryService class
 """
+
 from __future__ import annotations
 
 from typing import AsyncIterator
 
 from featurebyte.exception import DocumentNotFoundError
 from featurebyte.models.online_store_compute_query import OnlineStoreComputeQueryModel
 from featurebyte.schema.common.base import BaseDocumentServiceUpdateSchema
```

### Comparing `featurebyte-1.0.2/featurebyte/service/online_store_table_version.py` & `featurebyte-1.0.3/featurebyte/service/online_store_table_version.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 OnlineStoreTableVersionService class
 """
+
 from __future__ import annotations
 
 from typing import Dict, Optional
 
 from featurebyte.exception import DocumentNotFoundError
 from featurebyte.models.online_store_table_version import (
     OnlineStoreTableVersion,
```

### Comparing `featurebyte-1.0.2/featurebyte/service/parent_serving.py` & `featurebyte-1.0.3/featurebyte/service/parent_serving.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,65 +1,77 @@
 """
 Module to support serving parent features using child entities
 """
+
 from __future__ import annotations
 
 from typing import List, Optional
 
 from collections import OrderedDict
 
+from featurebyte.enum import TableDataType
 from featurebyte.models.entity_validation import EntityInfo
 from featurebyte.models.parent_serving import EntityLookupStep, EntityLookupStepCreator
 from featurebyte.query_graph.model.entity_lookup_plan import EntityLookupPlanner
 from featurebyte.query_graph.model.entity_relationship_info import EntityRelationshipInfo
 from featurebyte.service.entity import EntityService
+from featurebyte.service.item_table import ExtendedItemTableService
 from featurebyte.service.relationship_info import RelationshipInfoService
 from featurebyte.service.table import TableService
 
 
 class ParentEntityLookupService:
     """
     ParentEntityLookupService is responsible for identifying the joins required to lookup parent
     entities in order to serve parent features given child entities
     """
 
     def __init__(
         self,
         entity_service: EntityService,
         table_service: TableService,
+        extended_item_table_service: ExtendedItemTableService,
         relationship_info_service: RelationshipInfoService,
     ):
         self.entity_service = entity_service
         self.table_service = table_service
+        self.extended_item_table_service = extended_item_table_service
         self.relationship_info_service = relationship_info_service
 
-    async def get_required_join_steps(self, entity_info: EntityInfo) -> list[EntityLookupStep]:
+    async def get_required_join_steps(
+        self,
+        entity_info: EntityInfo,
+        relationships_info: Optional[list[EntityRelationshipInfo]] = None,
+    ) -> list[EntityLookupStep]:
         """
         Get the list of required JoinStep to lookup the missing entities in the request
 
         Parameters
         ----------
         entity_info: EntityInfo
             Entity information
+        relationships_info: Optional[list[EntityRelationshipInfo]]
+            Relationships that can be used to derive the join steps. If not provided, the currently
+            available relationships will be queried from persistent and used instead.
 
         Returns
         -------
         list[EntityLookupStep]
         """
 
         if entity_info.are_all_required_entities_provided():
             return []
 
-        # Use currently available relationships. Later to be updated to use frozen relationships
-        # stored in the feature list.
-        relationships_info = []
-        async for info in self.relationship_info_service.list_documents_iterator(
-            query_filter={},
-        ):
-            relationships_info.append(EntityRelationshipInfo(**info.dict(by_alias=True)))
+        if relationships_info is None:
+            # Use currently available relationships if frozen relationships are not available
+            relationships_info = []
+            async for info in self.relationship_info_service.list_documents_iterator(
+                query_filter={},
+            ):
+                relationships_info.append(EntityRelationshipInfo(**info.dict(by_alias=True)))
 
         lookup_steps = EntityLookupPlanner.generate_lookup_steps(
             available_entity_ids=list(entity_info.provided_entity_ids),
             required_entity_ids=list(entity_info.required_entity_ids),
             relationships_info=relationships_info,
         )
 
@@ -103,14 +115,22 @@
         }
         tables_by_id = {
             table.id: table
             async for table in self.table_service.list_documents_iterator(
                 query_filter={"_id": {"$in": list(all_table_ids)}}
             )
         }
+        for table_id, table in tables_by_id.items():
+            if table.type == TableDataType.ITEM_TABLE:
+                item_table = (
+                    await self.extended_item_table_service.get_document_with_event_table_model(
+                        table_id
+                    )
+                )
+                tables_by_id[table_id] = item_table
 
         return EntityLookupStepCreator(
             entity_relationships_info=entity_relationships_info,
             entities_by_id=entities_by_id,
             tables_by_id=tables_by_id,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/service/relationship.py` & `featurebyte-1.0.3/featurebyte/service/relationship.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 RelationshipService class
 """
+
 from __future__ import annotations
 
 from typing import TypeVar, cast
 
 from abc import abstractmethod
 
 from bson import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/service/relationship_info.py` & `featurebyte-1.0.3/featurebyte/service/relationship_info.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/service/sanitizer.py` & `featurebyte-1.0.3/featurebyte/service/sanitizer.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Sanitizer module
 """
+
 from featurebyte.query_graph.enum import NodeType
 from featurebyte.query_graph.model.graph import QueryGraphModel
 
 
 def sanitize_query_graph_for_feature_definition(graph: QueryGraphModel) -> QueryGraphModel:
     """
     Sanitize the query graph for feature creation
```

### Comparing `featurebyte-1.0.2/featurebyte/service/semantic.py` & `featurebyte-1.0.3/featurebyte/service/semantic.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SemanticService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.semantic import SemanticModel
 from featurebyte.schema.semantic import SemanticCreate, SemanticServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
 from featurebyte.service.mixin import GetOrCreateMixin
```

### Comparing `featurebyte-1.0.2/featurebyte/service/session_manager.py` & `featurebyte-1.0.3/featurebyte/service/session_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SessionManager service
 """
+
 from typing import Any, Optional
 
 from pydantic import ValidationError
 
 from featurebyte.exception import CredentialsError
 from featurebyte.models.base import User
 from featurebyte.models.feature_store import FeatureStoreModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/session_validator.py` & `featurebyte-1.0.3/featurebyte/service/session_validator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SessionValidator service
 """
+
 from typing import Any, Optional
 
 from featurebyte.enum import SourceType, StrEnum
 from featurebyte.exception import FeatureStoreSchemaCollisionError, NoFeatureStorePresentError
 from featurebyte.logging import get_logger
 from featurebyte.models.base import PydanticObjectId
 from featurebyte.query_graph.node.schema import DatabaseDetails
```

### Comparing `featurebyte-1.0.2/featurebyte/service/specialized_dtype.py` & `featurebyte-1.0.3/featurebyte/service/specialized_dtype.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Module with extra services for data type info detection like embedding columns or flat (non-nested) dicts"""
+
 from __future__ import annotations
 
 from typing import Any
 
 import json
 from abc import ABC, abstractmethod
```

### Comparing `featurebyte-1.0.2/featurebyte/service/static_source_table.py` & `featurebyte-1.0.3/featurebyte/service/static_source_table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTableService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict
 
 from bson import ObjectId
 
 from featurebyte.models.base import FeatureByteBaseDocumentModel
```

### Comparing `featurebyte-1.0.2/featurebyte/service/table.py` & `featurebyte-1.0.3/featurebyte/service/table.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TableService class
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from bson import ObjectId
 
 from featurebyte.models.feature_store import TableModel as BaseDataModel
@@ -17,20 +18,35 @@
 class TableService(BaseDocumentService[BaseDataModel, TableCreate, TableServiceUpdate]):
     """
     TableService class
     """
 
     document_class = ProxyTableModel
 
+    @property
+    def is_catalog_specific(self) -> bool:
+        return True
+
     async def create_document(self, data: DocumentCreateSchema) -> Document:
         raise NotImplementedError
 
     async def update_document(
         self,
         document_id: ObjectId,
         data: DocumentUpdateSchema,
         exclude_none: bool = True,
         document: Optional[Document] = None,
         return_document: bool = True,
         skip_block_modification_check: bool = False,
+        populate_remote_attributes: bool = True,
     ) -> Optional[Document]:
         raise NotImplementedError
+
+
+class AllTableService(TableService):  # pylint: disable=abstract-method
+    """
+    TableService class
+    """
+
+    @property
+    def is_catalog_specific(self) -> bool:
+        return False
```

### Comparing `featurebyte-1.0.2/featurebyte/service/table_columns_info.py` & `featurebyte-1.0.3/featurebyte/service/table_columns_info.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TableColumnsInfoService
 """
+
 from __future__ import annotations
 
 from typing import List, Tuple, Union
 
 from collections import defaultdict
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/service/table_facade.py` & `featurebyte-1.0.3/featurebyte/service/table_facade.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Table  Facade Service which is responsible for handling high level table operations
 """
+
 from typing import List, Optional
 
 from bson import ObjectId
 
 from featurebyte import ColumnCleaningOperation
 from featurebyte.enum import TableDataType
 from featurebyte.models.feature_store import TableStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/service/table_info.py` & `featurebyte-1.0.3/featurebyte/service/table_info.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Table info service
 """
+
 from typing import Any, Dict
 
 from bson import ObjectId
 
 from featurebyte.models.feature_store import TableModel
 from featurebyte.routes.catalog.catalog_name_injector import CatalogNameInjector
 from featurebyte.schema.info import EntityBriefInfoList
```

### Comparing `featurebyte-1.0.2/featurebyte/service/table_status.py` & `featurebyte-1.0.3/featurebyte/service/table_status.py`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/service/target.py` & `featurebyte-1.0.3/featurebyte/service/target.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Target class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from bson import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/target_helper/base_feature_or_target_computer.py` & `featurebyte-1.0.3/featurebyte/service/target_helper/base_feature_or_target_computer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base class for feature or target computer
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, Coroutine, Generic, List, Optional, Tuple, TypeVar, Union
 
 from abc import abstractmethod
 from dataclasses import dataclass
 
@@ -61,28 +62,41 @@
     # than those defined in Entities
     serving_names_mapping: Optional[dict[str, str]] = None
 
 
 ExecutorParamsT = TypeVar("ExecutorParamsT", bound=ExecutorParams)
 
 
+@dataclass
+class ExecutionResult:
+    """
+    Execution result
+    """
+
+    is_output_view: bool
+
+
 class QueryExecutor(Generic[ExecutorParamsT]):
     """
     Query executor
     """
 
     @abstractmethod
-    async def execute(self, executor_params: ExecutorParamsT) -> None:
+    async def execute(self, executor_params: ExecutorParamsT) -> ExecutionResult:
         """
         Execute queries
 
         Parameters
         ----------
         executor_params: ExecutorParamsT
             Executor parameters
+
+        Returns
+        -------
+        ExecutionResult
         """
 
 
 class Computer(Generic[ComputeRequestT, ExecutorParamsT]):
     """
     Base target or feature computer
     """
@@ -143,26 +157,30 @@
         """
 
     async def compute(
         self,
         observation_set: Union[pd.DataFrame, ObservationTableModel],
         compute_request: ComputeRequestT,
         output_table_details: TableDetails,
-    ) -> None:
+    ) -> ExecutionResult:
         """
         Compute targets or features
 
         Parameters
         ----------
         observation_set: pd.DataFrame
             Observation set data
         compute_request: ComputeRequestT
             Compute request
         output_table_details: TableDetails
             Table details to write the results to
+
+        Returns
+        -------
+        ExecutionResult
         """
         validation_parameters = await self.get_validation_parameters(compute_request)
 
         if isinstance(observation_set, pd.DataFrame):
             request_column_names = set(observation_set.columns)
         else:
             request_column_names = {col.name for col in observation_set.columns_info}
@@ -186,8 +204,8 @@
                 output_table_details=output_table_details,
                 parent_serving_preparation=parent_serving_preparation,
                 progress_callback=self.task_progress_updater.update_progress,
                 observation_set=observation_set,
             ),
             validation_parameters=validation_parameters,
         )
-        await self.query_executor.execute(params)
+        return await self.query_executor.execute(params)
```

### Comparing `featurebyte-1.0.2/featurebyte/service/target_helper/compute_target.py` & `featurebyte-1.0.3/featurebyte/service/target_helper/compute_target.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 """
 Get targets module
 """
+
 from __future__ import annotations
 
 from featurebyte.logging import get_logger
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.routes.common.feature_or_target_table import ValidationParameters
 from featurebyte.schema.target import ComputeTargetRequest
 from featurebyte.service.entity_validation import EntityValidationService
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.feature_table_cache import FeatureTableCacheService
 from featurebyte.service.historical_features_and_target import get_target
 from featurebyte.service.session_manager import SessionManagerService
 from featurebyte.service.target_helper.base_feature_or_target_computer import (
     BasicExecutorParams,
     Computer,
+    ExecutionResult,
     ExecutorParams,
     QueryExecutor,
 )
 from featurebyte.worker.util.task_progress_updater import TaskProgressUpdater
 
 logger = get_logger(__name__)
 
@@ -29,28 +31,32 @@
     """
 
     def __init__(self, feature_table_cache_service: FeatureTableCacheService):
         self.feature_table_cache_service = feature_table_cache_service
 
     async def execute(  # pylint: disable=too-many-locals
         self, executor_params: ExecutorParams
-    ) -> None:
+    ) -> ExecutionResult:
         """
         Get targets.
 
         Parameters
         ----------
         executor_params: ExecutorParams
             Executor parameters
+
+        Returns
+        -------
+        ExecutionResult
         """
         if (
             isinstance(executor_params.observation_set, ObservationTableModel)
             and executor_params.observation_set.has_row_index
         ):
-            await self.feature_table_cache_service.create_view_from_cache(
+            is_output_view = await self.feature_table_cache_service.create_view_or_table_from_cache(
                 feature_store=executor_params.feature_store,
                 observation_table=executor_params.observation_set,
                 graph=executor_params.graph,
                 nodes=executor_params.nodes,
                 output_view_details=executor_params.output_table_details,
                 is_target=True,
                 serving_names_mapping=executor_params.serving_names_mapping,
@@ -63,14 +69,16 @@
                 observation_set=executor_params.observation_set,
                 feature_store=executor_params.feature_store,
                 output_table_details=executor_params.output_table_details,
                 serving_names_mapping=executor_params.serving_names_mapping,
                 parent_serving_preparation=executor_params.parent_serving_preparation,
                 progress_callback=executor_params.progress_callback,
             )
+            is_output_view = False
+        return ExecutionResult(is_output_view=is_output_view)
 
 
 class TargetComputer(Computer[ComputeTargetRequest, ExecutorParams]):
     """
     Target computer
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/service/target_namespace.py` & `featurebyte-1.0.3/featurebyte/service/target_namespace.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TargetNamespaceService class
 """
+
 from __future__ import annotations
 
 from featurebyte.models.target_namespace import TargetNamespaceModel
 from featurebyte.schema.target_namespace import TargetNamespaceCreate, TargetNamespaceServiceUpdate
 from featurebyte.service.base_document import BaseDocumentService
```

### Comparing `featurebyte-1.0.2/featurebyte/service/target_table.py` & `featurebyte-1.0.3/featurebyte/service/target_table.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 """
 TargetTableService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from pathlib import Path
 
 import pandas as pd
 from bson import ObjectId
 from redis import Redis
 
 from featurebyte.enum import MaterializedTableNamePrefix
 from featurebyte.models.base import FeatureByteBaseDocumentModel
+from featurebyte.models.persistent import QueryFilter
 from featurebyte.models.target_table import TargetTableModel
 from featurebyte.persistent import Persistent
 from featurebyte.routes.block_modification_handler import BlockModificationHandler
 from featurebyte.schema.target_table import TargetTableCreate
 from featurebyte.schema.worker.task.target_table import TargetTableTaskPayload
 from featurebyte.service.entity import EntityService
 from featurebyte.service.feature_store import FeatureStoreService
@@ -59,14 +61,35 @@
         )
         self.temp_storage = temp_storage
 
     @property
     def class_name(self) -> str:
         return "TargetTable"
 
+    def _construct_get_query_filter(
+        self, document_id: ObjectId, use_raw_query_filter: bool = False, **kwargs: Any
+    ) -> QueryFilter:
+        query_filter = super()._construct_get_query_filter(
+            document_id=document_id, use_raw_query_filter=use_raw_query_filter, **kwargs
+        )
+        query_filter["request_input.type"] = {"$in": ["observation_table", "dataframe"]}
+        return query_filter
+
+    def construct_list_query_filter(
+        self,
+        query_filter: Optional[QueryFilter] = None,
+        use_raw_query_filter: bool = False,
+        **kwargs: Any,
+    ) -> QueryFilter:
+        query_filter = super().construct_list_query_filter(
+            query_filter=query_filter, use_raw_query_filter=use_raw_query_filter, **kwargs
+        )
+        query_filter["request_input.type"] = {"$in": ["observation_table", "dataframe"]}
+        return query_filter
+
     async def get_target_table_task_payload(
         self,
         data: TargetTableCreate,
         observation_set_dataframe: Optional[pd.DataFrame],
     ) -> TargetTableTaskPayload:
         """
         Validate and convert a TargetTableCreate schema to a TargetTableTaskPayload schema
```

### Comparing `featurebyte-1.0.2/featurebyte/service/task_manager.py` & `featurebyte-1.0.3/featurebyte/service/task_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TaskManager service is responsible to submit task message
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 import datetime
 from uuid import UUID
 
@@ -139,14 +140,15 @@
             Task result
         """
         await self.persistent.update_one(
             collection_name=TaskModel.collection_name(),
             query_filter={"_id": task_id},
             update={"$set": {"task_result": result}},
             user_id=self.user.id,
+            disable_audit=True,
         )
 
     async def get_task_result(self, task_id: str) -> Any:
         """
         Get task result
 
         Parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/service/templates/online_serving/python.tpl` & `featurebyte-1.0.3/featurebyte/service/templates/online_serving/python.tpl`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/service/tile/tile_task_executor.py` & `featurebyte-1.0.3/featurebyte/service/tile/tile_task_executor.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Tile Generate Schedule script
 """
+
 from typing import Any, Dict, List, Optional
 
 import time
 import traceback
 from datetime import datetime, timedelta
 
 import dateutil.parser
```

### Comparing `featurebyte-1.0.2/featurebyte/service/tile_cache.py` & `featurebyte-1.0.3/featurebyte/service/tile_cache.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 """
 TileCacheService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, Coroutine, Optional
 
 from bson import ObjectId
 
+from featurebyte.common.progress import divide_progress_callback
+from featurebyte.common.utils import timer
 from featurebyte.logging import get_logger
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
 from featurebyte.service.tile_manager import TileManagerService
 from featurebyte.session.base import BaseSession
 from featurebyte.tile.tile_cache import TileCache
 
@@ -62,15 +65,39 @@
             Optional progress callback function
         """
         tile_cache = TileCache(
             session=session,
             tile_manager_service=self.tile_manager_service,
             feature_store_id=feature_store_id,
         )
-        await tile_cache.compute_tiles_on_demand(
+        if progress_callback is not None:
+            tile_check_progress_callback, tile_compute_progress_callback = divide_progress_callback(
+                progress_callback=progress_callback,
+                at_percent=20,
+            )
+        else:
+            tile_check_progress_callback, tile_compute_progress_callback = None, None
+
+        required_tile_computations = await tile_cache.get_required_computation(
+            request_id=request_id,
             graph=graph,
             nodes=nodes,
-            request_id=request_id,
             request_table_name=request_table_name,
             serving_names_mapping=serving_names_mapping,
-            progress_callback=progress_callback,
+            progress_callback=tile_check_progress_callback,
         )
+
+        # Execute tile computations
+        try:
+            if required_tile_computations:
+                logger.info(
+                    "Obtained required tile computations",
+                    extra={"n": len(required_tile_computations)},
+                )
+                with timer("Compute tiles on demand", logger):
+                    await tile_cache.invoke_tile_manager(
+                        required_tile_computations, tile_compute_progress_callback
+                    )
+            else:
+                logger.debug("All required tiles can be reused")
+        finally:
+            await tile_cache.cleanup_temp_tables()
```

### Comparing `featurebyte-1.0.2/featurebyte/service/tile_job_log.py` & `featurebyte-1.0.3/featurebyte/service/tile_job_log.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TileJobStatusService class
 """
+
 from __future__ import annotations
 
 from typing import Any, List
 
 import datetime
 from collections import defaultdict
```

### Comparing `featurebyte-1.0.2/featurebyte/service/tile_manager.py` & `featurebyte-1.0.3/featurebyte/service/tile_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TileManagerService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Callable, Coroutine, List, Optional, Tuple
 
 import time
 
 from featurebyte.enum import InternalName
@@ -12,14 +13,15 @@
 from featurebyte.models.tile import TileScheduledJobParameters, TileSpec, TileType
 from featurebyte.service.feature import FeatureService
 from featurebyte.service.online_store_compute_query_service import OnlineStoreComputeQueryService
 from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
 from featurebyte.service.tile_registry_service import TileRegistryService
 from featurebyte.service.tile_scheduler import TileSchedulerService
 from featurebyte.session.base import BaseSession
+from featurebyte.session.session_helper import run_coroutines
 from featurebyte.sql.tile_generate import TileGenerate
 from featurebyte.sql.tile_generate_entity_tracking import TileGenerateEntityTracking
 from featurebyte.sql.tile_schedule_online_store import TileScheduleOnlineStore
 
 logger = get_logger(__name__)
 
 
@@ -58,45 +60,69 @@
             Instance of BaseSession to interact with the data warehouse
         tile_inputs: List[Tuple[TileSpec, str]]
             list of TileSpec, temp_entity_table to update the feature store
         progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]]
             Optional progress callback function
         """
         num_jobs = len(tile_inputs)
-        if progress_callback:
-            await progress_callback(0, f"0/{num_jobs} completed")
+        processed = 0
 
-        for index, (tile_spec, entity_table) in enumerate(tile_inputs):
-            tic = time.time()
-            await self.generate_tiles(
-                session=session,
-                tile_spec=tile_spec,
-                tile_type=TileType.OFFLINE,
-                start_ts_str=None,
-                end_ts_str=None,
-            )
-            logger.debug(
-                "Done generating tiles",
-                extra={"tile_id": tile_spec.tile_id, "duration": time.time() - tic},
-            )
+        async def _progress_callback() -> None:
+            nonlocal processed
+            processed += 1
+            if progress_callback:
+                pct = int(100 * processed / num_jobs)
+                await progress_callback(pct, f"Computed {processed} out of {num_jobs} tiles")
 
-            tic = time.time()
-            await self.update_tile_entity_tracker(
-                session=session, tile_spec=tile_spec, temp_entity_table=entity_table
-            )
-            logger.debug(
-                "Done update_tile_entity_tracker",
-                extra={"tile_id": tile_spec.tile_id, "duration": time.time() - tic},
+        if progress_callback:
+            await progress_callback(0, "Computing tiles on demand")
+        coroutines = []
+        for tile_spec, entity_table in tile_inputs:
+            coroutines.append(
+                self._generate_tiles_on_demand_for_tile_spec(
+                    session=session,
+                    tile_spec=tile_spec,
+                    entity_table=entity_table,
+                    progress_callback=_progress_callback,
+                )
             )
+        await run_coroutines(coroutines)
 
-            if progress_callback:
-                await progress_callback(
-                    int(100 * (index + 1) / num_jobs),
-                    f"{index+1}/{num_jobs} completed",
-                )
+    async def _generate_tiles_on_demand_for_tile_spec(
+        self,
+        session: BaseSession,
+        tile_spec: TileSpec,
+        entity_table: str,
+        progress_callback: Optional[Callable[[], Coroutine[Any, Any, None]]] = None,
+    ) -> None:
+        tic = time.time()
+        session = await session.clone_if_not_threadsafe()
+        await self.generate_tiles(
+            session=session,
+            tile_spec=tile_spec,
+            tile_type=TileType.OFFLINE,
+            start_ts_str=None,
+            end_ts_str=None,
+        )
+        logger.debug(
+            "Done generating tiles",
+            extra={"tile_id": tile_spec.tile_id, "duration": time.time() - tic},
+        )
+
+        tic = time.time()
+        await self.update_tile_entity_tracker(
+            session=session, tile_spec=tile_spec, temp_entity_table=entity_table
+        )
+        logger.debug(
+            "Done update_tile_entity_tracker",
+            extra={"tile_id": tile_spec.tile_id, "duration": time.time() - tic},
+        )
+
+        if progress_callback:
+            await progress_callback()
 
     async def tile_job_exists(self, tile_spec: TileSpec) -> bool:
         """
         Get existing tile jobs for the given tile_spec
 
         Parameters
         ----------
```

### Comparing `featurebyte-1.0.2/featurebyte/service/tile_registry_service.py` & `featurebyte-1.0.3/featurebyte/service/tile_registry_service.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,25 @@
 """
 TileRegistryService class
 """
+
 from __future__ import annotations
 
 from typing import Optional
 
 from datetime import datetime
 
 from featurebyte.exception import DocumentNotFoundError
 from featurebyte.models.tile import TileType
-from featurebyte.models.tile_registry import LastRunMetadata, TileModel, TileUpdate
+from featurebyte.models.tile_registry import (
+    BackfillMetadata,
+    LastRunMetadata,
+    TileModel,
+    TileUpdate,
+)
 from featurebyte.service.base_document import BaseDocumentService
 
 
 class TileRegistryService(BaseDocumentService[TileModel, TileModel, TileUpdate]):
     """
     TileRegistryService class
 
@@ -80,7 +86,36 @@
             )
         metadata_model = LastRunMetadata(index=tile_index, tile_end_date=tile_end_date)
         if tile_type == TileType.ONLINE:
             update_model = TileUpdate(last_run_metadata_online=metadata_model)
         else:
             update_model = TileUpdate(last_run_metadata_offline=metadata_model)
         await self.update_document(document.id, update_model, document=document)
+
+    async def update_backfill_metadata(
+        self, tile_id: str, aggregation_id: str, backfill_start_date: datetime
+    ) -> None:
+        """
+        Update information about the tile backfill process
+
+        Parameters
+        ----------
+        tile_id: str
+            Tile id
+        aggregation_id: str
+            Aggregation id
+        backfill_start_date: datetime
+            Start date of the backfill process
+
+        Raises
+        ------
+        DocumentNotFoundError
+            If the tile model is not found
+        """
+        document = await self.get_tile_model(tile_id, aggregation_id)
+        if document is None:
+            raise DocumentNotFoundError(
+                f"TileRegistryService: TileModel with tile_id={tile_id} and aggregation_id={aggregation_id} not found"
+            )
+        metadata_model = BackfillMetadata(start_date=backfill_start_date)
+        update_model = TileUpdate(backfill_metadata=metadata_model)
+        await self.update_document(document.id, update_model, document=document)
```

### Comparing `featurebyte-1.0.2/featurebyte/service/tile_scheduler.py` & `featurebyte-1.0.3/featurebyte/service/tile_scheduler.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 TileSchedulerService class
 """
+
 from typing import Optional
 
 from bson import ObjectId
 
 from featurebyte.models.base import User
 from featurebyte.models.periodic_task import Interval, PeriodicTask
 from featurebyte.models.tile import TileScheduledJobParameters
```

### Comparing `featurebyte-1.0.2/featurebyte/service/use_case.py` & `featurebyte-1.0.3/featurebyte/service/use_case.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 UseCaseService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, cast
 
 from bson import ObjectId
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/service/user_defined_function.py` & `featurebyte-1.0.3/featurebyte/service/user_defined_function.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 UserDefinedFunctionService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson import ObjectId
 
 from featurebyte.exception import DocumentConflictError
```

### Comparing `featurebyte-1.0.2/featurebyte/service/user_service.py` & `featurebyte-1.0.3/featurebyte/service/user_service.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 User service module
 """
+
 from typing import Optional
 
 from bson import ObjectId
 
 from featurebyte.models.base import FeatureByteBaseDocumentModel, PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/service/validator/entity_relationship_validator.py` & `featurebyte-1.0.3/featurebyte/service/validator/entity_relationship_validator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Entity Relationship Combiner Service
 """
+
 from typing import Dict, List, Set, Tuple
 
 from collections import defaultdict
 from dataclasses import dataclass
 
 from bson import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/service/validator/materialized_table_delete.py` & `featurebyte-1.0.3/featurebyte/service/validator/materialized_table_delete.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This file contains the error messages used in the featurebyte package that is used in multiple places.
 """
+
 from bson import ObjectId
 
 from featurebyte.exception import DocumentDeletionError
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.models.static_source_table import StaticSourceTableModel
 from featurebyte.service.batch_feature_table import BatchFeatureTableService
```

### Comparing `featurebyte-1.0.2/featurebyte/service/validator/production_ready_validator.py` & `featurebyte-1.0.3/featurebyte/service/validator/production_ready_validator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Production ready validator
 """
+
 from typing import Any, Dict, List, cast
 
 from featurebyte import ColumnCleaningOperation, FeatureJobSetting
 from featurebyte.exception import DocumentUpdateError, NoChangesInFeatureVersionError
 from featurebyte.models.feature import FeatureModel
 from featurebyte.models.feature_namespace import FeatureReadiness
 from featurebyte.models.feature_store import TableStatus
```

### Comparing `featurebyte-1.0.2/featurebyte/service/version.py` & `featurebyte-1.0.3/featurebyte/service/version.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 VersionService class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from bson.objectid import ObjectId
 
 from featurebyte.enum import TableDataType
```

### Comparing `featurebyte-1.0.2/featurebyte/service/view_construction.py` & `featurebyte-1.0.3/featurebyte/service/view_construction.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ViewConstructionService class
 """
+
 from __future__ import annotations
 
 from typing import Any, cast
 
 from bson import ObjectId
 
 from featurebyte import ColumnCleaningOperation, TableCleaningOperation
@@ -84,17 +85,17 @@
             view_parameters["event_view_node"] = event_view_node
             view_parameters["event_view_columns_info"] = event_view_columns_info
             view_parameters["event_view_event_id_column"] = table_input_node.parameters.id_column  # type: ignore
 
             # prepare additional parameters for metadata update
             event_metadata = cast(ViewMetadata, event_view_node.parameters.metadata)  # type: ignore
             metadata_parameters["event_drop_column_names"] = event_metadata.drop_column_names
-            metadata_parameters[
-                "event_column_cleaning_operations"
-            ] = event_metadata.column_cleaning_operations
+            metadata_parameters["event_column_cleaning_operations"] = (
+                event_metadata.column_cleaning_operations
+            )
 
         return view_parameters, metadata_parameters
 
     @staticmethod
     def _prepare_target_columns_and_input_nodes(
         query_graph: QueryGraphModel,
         view_node_name_to_table_info: dict[str, tuple[Node, list[ColumnInfo], InputNode]],
```

### Comparing `featurebyte-1.0.2/featurebyte/service/working_schema.py` & `featurebyte-1.0.3/featurebyte/service/working_schema.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 WorkingSchemaService class
 """
+
 from __future__ import annotations
 
 from bson import ObjectId
 
 from featurebyte.logging import get_logger
 from featurebyte.models.base import User
 from featurebyte.persistent import Persistent
@@ -112,9 +113,10 @@
 
             async for feature in online_enabled_features:
                 logger.info(f"Rescheduling jobs for online enabled feature: {feature.name}")
                 await OnlineEnableService.update_data_warehouse_with_session(
                     session=session,
                     feature_manager_service=self.feature_manager_service,
                     feature=feature,
+                    target_online_enabled=True,
                     is_recreating_schema=True,
                 )
```

### Comparing `featurebyte-1.0.2/featurebyte/session/base.py` & `featurebyte-1.0.3/featurebyte/session/base.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,60 +1,61 @@
 """
 Session class
 """
+
 # pylint: disable=too-many-lines
 from __future__ import annotations
 
-from typing import Any, AsyncGenerator, ClassVar, Dict, Optional, OrderedDict, Type
+from typing import Any, AsyncGenerator, ClassVar, Dict, Literal, Optional, OrderedDict, Type
 
 import asyncio
 import contextvars
 import ctypes
 import functools
 import os
 import threading
 import time
 from abc import ABC, abstractmethod
 from asyncio import events
 from io import BytesIO
+from random import randint
 
 import aiofiles
 import pandas as pd
 import pyarrow as pa
+from bson import ObjectId
+from cachetools import TTLCache
 from pydantic import BaseModel, PrivateAttr
 from sqlglot import expressions
-from sqlglot.expressions import Expression
+from sqlglot.expressions import Expression, Select
 
 from featurebyte.common.path_util import get_package_root
-from featurebyte.common.utils import (
-    create_new_arrow_stream_writer,
-    dataframe_from_arrow_stream,
-    pa_table_to_record_batches,
-)
+from featurebyte.common.utils import create_new_arrow_stream_writer, dataframe_from_arrow_stream
 from featurebyte.enum import InternalName, MaterializedTableNamePrefix, SourceType, StrEnum
-from featurebyte.exception import QueryExecutionTimeOut
+from featurebyte.exception import DataWarehouseOperationError, QueryExecutionTimeOut
 from featurebyte.logging import get_logger
 from featurebyte.models.user_defined_function import UserDefinedFunctionModel
 from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
 from featurebyte.query_graph.model.table import TableDetails, TableSpec
+from featurebyte.query_graph.node.schema import TableDetails as NodeTableDetails
 from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
 from featurebyte.query_graph.sql.common import (
     get_fully_qualified_table_name,
     quoted_identifier,
     sql_to_string,
 )
 
 INTERACTIVE_SESSION_TIMEOUT_SECONDS = 30
 NON_INTERACTIVE_SESSION_TIMEOUT_SECONDS = 120
 MINUTES_IN_SECONDS = 60
 HOUR_IN_SECONDS = 60 * MINUTES_IN_SECONDS
 DEFAULT_EXECUTE_QUERY_TIMEOUT_SECONDS = 10 * MINUTES_IN_SECONDS
 LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS = 24 * HOUR_IN_SECONDS
 APPLICATION_NAME = "FeatureByte"
-
+session_cache: TTLCache[Any, Any] = TTLCache(maxsize=1024, ttl=600)
 
 logger = get_logger(__name__)
 
 
 async def to_thread(func: Any, timeout: float, /, *args: Any, **kwargs: Any) -> Any:
     """
     Run blocking function in a thread pool and wait for the result.
@@ -99,42 +100,70 @@
 
     thread_info: Dict[str, int] = {}
     func_call = functools.partial(ctx.run, _func_wrapper, func, thread_info, *args, **kwargs)
 
     try:
         return await asyncio.wait_for(loop.run_in_executor(None, func_call), timeout)
     except asyncio.exceptions.TimeoutError:
-        _raise_timeout_exception_in_thread(thread_info["tid"])
+        tid = thread_info.get("tid")
+        if tid:
+            _raise_timeout_exception_in_thread(thread_info["tid"])
         raise
 
 
 class BaseSession(BaseModel):
     """
     Abstract session class to extract data warehouse table metadata & execute query
     """
 
     # pylint: disable=too-many-public-methods
 
     source_type: SourceType
     database_name: str = ""
     schema_name: str = ""
     _connection: Any = PrivateAttr(default=None)
-    _unique_id: int = PrivateAttr(default=0)
+    _cache_key: Any = PrivateAttr(default=None)
     _no_schema_error: ClassVar[Type[Exception]] = Exception
 
     def __init__(self, **data: Any) -> None:
         super().__init__(**data)
         self._initialize_connection()
 
     def _initialize_connection(self) -> None:
         """Initialize connection"""
 
-    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
+    def __del__(self) -> None:
         # close connection
-        self._connection.close()
+        if self._connection is not None:
+            self._connection.close()
+
+    def set_cache_key(self, key: Any) -> None:
+        """
+        Set hash key used to cache session object
+
+        Parameters
+        ----------
+        key: Any
+            Hash key
+        """
+        self._cache_key = key
+
+    async def clone_if_not_threadsafe(self) -> BaseSession:
+        """
+        Create a new session object from a session that is not threadsafe
+
+        Returns
+        -------
+        BaseSession
+        """
+        if self.is_threadsafe():
+            return self
+        new_session = self.copy()
+        new_session._initialize_connection()  # pylint: disable=protected-access
+        return new_session
 
     @property
     def no_schema_error(self) -> Type[Exception]:
         """
         Exception to raise when schema is not found
 
         Returns
@@ -175,23 +204,23 @@
 
         Returns
         -------
         Any
         """
         return self._connection
 
-    def generate_session_unique_id(self) -> str:
+    @classmethod
+    def generate_session_unique_id(cls) -> str:
         """Generate unique id within the session
 
         Returns
         -------
         str
         """
-        self._unique_id += 1
-        return str(self._unique_id)
+        return str(ObjectId()).upper()
 
     @abstractmethod
     async def list_databases(self) -> list[str]:
         """
         Execute SQL query to retrieve database names
 
         Returns
@@ -212,50 +241,58 @@
         Returns
         -------
         list[str]
         """
 
     @abstractmethod
     async def list_tables(
-        self, database_name: str | None = None, schema_name: str | None = None
+        self,
+        database_name: str | None = None,
+        schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> list[TableSpec]:
         """
         Execute SQL query to retrieve table names
 
         Parameters
         ----------
         database_name: str | None
             Database name
         schema_name: str | None
             Schema name
+        timeout: float
+            Timeout in seconds
 
         Returns
         -------
         list[TableSpec]
         """
 
     @abstractmethod
     async def list_table_schema(
         self,
         table_name: str | None,
         database_name: str | None = None,
         schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> OrderedDict[str, ColumnSpecWithDescription]:
         """
         Execute SQL query to retrieve table schema of a given table name and convert the
         schema type to internal variable type
 
         Parameters
         ----------
         database_name: str | None
             Database name
         schema_name: str | None
             Schema name
         table_name: str
             Table name
+        timeout: float
+            Timeout in seconds
 
         Returns
         -------
         OrderedDict[str, ColumnSpecWithDescription]
         """
 
     @abstractmethod
@@ -372,17 +409,15 @@
         Yields
         ------
         pa.RecordBatch
             Pyarrow record batch
         """
         # fetch all results into a single dataframe and write batched records to the stream
         dataframe = self.fetch_query_result_impl(cursor)
-        table = pa.Table.from_pandas(dataframe)
-        for batch in pa_table_to_record_batches(table):
-            yield batch
+        yield pa.record_batch(dataframe)
 
     async def get_async_query_stream(
         self, query: str, timeout: float = LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS
     ) -> AsyncGenerator[bytes, None]:
         """
         Stream results from asynchronous query as compressed arrow bytestream
 
@@ -456,15 +491,15 @@
         -------
         If no working schema version is found, return -1. This should indicate
         to callers that we probably want to initialize the working schema.
         """
 
         query = "SELECT WORKING_SCHEMA_VERSION, FEATURE_STORE_ID FROM METADATA_SCHEMA"
         try:
-            results = await self.execute_query(query)
+            results = await self.execute_query(query, to_log_error=False)
         except self._no_schema_error:  # pylint: disable=broad-except
             # Snowflake and Databricks will error if the table is not initialized
             # We will need to catch more errors here if/when we add support for
             # more platforms.
             return {
                 "version": MetadataSchemaInitializer.SCHEMA_NOT_REGISTERED,
             }
@@ -474,32 +509,54 @@
             return {"version": MetadataSchemaInitializer.SCHEMA_NO_RESULTS_FOUND}
         return {
             "version": int(results["WORKING_SCHEMA_VERSION"][0]),
             "feature_store_id": results["FEATURE_STORE_ID"][0],
         }
 
     async def execute_query(
-        self, query: str, timeout: float = DEFAULT_EXECUTE_QUERY_TIMEOUT_SECONDS
+        self,
+        query: str,
+        timeout: float = DEFAULT_EXECUTE_QUERY_TIMEOUT_SECONDS,
+        to_log_error: bool = True,
     ) -> pd.DataFrame | None:
         """
         Execute SQL query
 
         Parameters
         ----------
         query: str
             sql query to execute
         timeout: float
             timeout in seconds
+        to_log_error: bool
+            If True, log error
 
         Returns
         -------
         pd.DataFrame | None
             Query result as a pandas DataFrame if the query expects result
+
+        Raises
+        ------
+        Exception
+            If query execution fails
         """
-        bytestream = self.get_async_query_stream(query=query, timeout=timeout)
+        try:
+            bytestream = self.get_async_query_stream(query=query, timeout=timeout)
+        except Exception as exc:
+            if to_log_error:
+                logger.error(
+                    "Error executing query", extra={"query": query, "source_type": self.source_type}
+                )
+
+            # remove session from cache if query fails
+            if self._cache_key:
+                session_cache.pop(self._cache_key, None)
+            raise exc
+
         buffer = BytesIO()
         async for chunk in bytestream:
             buffer.write(chunk)
         buffer.flush()
         if buffer.tell() == 0:
             return None
         buffer.seek(0)
@@ -522,52 +579,67 @@
         -------
         pd.DataFrame | None
             Query result as a pandas DataFrame if the query expects result
         """
         return await self.execute_query(query=query, timeout=timeout)
 
     async def execute_query_long_running(
-        self, query: str, timeout: float = LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS
+        self,
+        query: str,
+        timeout: float = LONG_RUNNING_EXECUTE_QUERY_TIMEOUT_SECONDS,
+        to_log_error: bool = True,
     ) -> pd.DataFrame | None:
         """
         Execute SQL query that is expected to run for a long time
 
         Parameters
         ----------
         query: str
             sql query to execute
         timeout: float
             timeout in seconds
+        to_log_error: bool
+            If True, log error
 
         Returns
         -------
         pd.DataFrame | None
             Query result as a pandas DataFrame if the query expects result
         """
-        return await self.execute_query(query=query, timeout=timeout)
+        return await self.execute_query(query=query, timeout=timeout, to_log_error=to_log_error)
 
     def execute_query_blocking(self, query: str) -> pd.DataFrame | None:
         """
         Execute SQL query without await
 
         Parameters
         ----------
         query: str
             sql query to execute
 
         Returns
         -------
         pd.DataFrame | None
             Query result as a pandas DataFrame if the query expects result
+
+        Raises
+        ------
+        Exception
+            If query execution fails
         """
         cursor = self.connection.cursor()
         try:
             cursor.execute(query)
             result = self.fetch_query_result_impl(cursor)
             return result
+        except Exception as exc:
+            # remove session from cache if query fails
+            if self._cache_key:
+                session_cache.pop(self._cache_key, None)
+            raise exc
         finally:
             cursor.close()
 
     def fetch_query_result_impl(self, cursor: Any) -> pd.DataFrame | None:
         """
         Fetch the result of executed SQL query from connection cursor
 
@@ -610,44 +682,65 @@
 
     async def drop_table(
         self,
         table_name: str,
         schema_name: str,
         database_name: str,
         if_exists: bool = False,
-        is_view: bool = False,
     ) -> None:
         """
         Drop a table
 
         Parameters
         ----------
         table_name : str
             Table name
         schema_name : str
             Schema name
         database_name : str
             Database name
         if_exists : bool
             If True, drop the table only if it exists
-        is_view : bool
-            If True - it is view not table.
+
+        Raises
+        ------
+        DataWarehouseOperationError
+            If the operation failed
         """
-        fully_qualified_table_name = get_fully_qualified_table_name(
-            {"table_name": table_name, "schema_name": schema_name, "database_name": database_name}
-        )
-        query = sql_to_string(
-            expressions.Drop(
-                this=expressions.Table(this=fully_qualified_table_name),
-                kind="VIEW" if is_view else "TABLE",
-                exists=if_exists,
-            ),
-            source_type=self.source_type,
-        )
-        await self.execute_query(query)
+
+        async def _drop(is_view: bool) -> None:
+            fully_qualified_table_name = get_fully_qualified_table_name(
+                {
+                    "table_name": table_name,
+                    "schema_name": schema_name,
+                    "database_name": database_name,
+                }
+            )
+            query = sql_to_string(
+                expressions.Drop(
+                    this=expressions.Table(this=fully_qualified_table_name),
+                    kind="VIEW" if is_view else "TABLE",
+                    exists=if_exists,
+                ),
+                source_type=self.source_type,
+            )
+            await self.execute_query(query)
+
+        try:
+            await _drop(is_view=False)
+        except Exception as exc:  # pylint: disable=bare-except
+            msg = str(exc)
+            if "VIEW" in msg:
+                try:
+                    await _drop(is_view=True)
+                    return
+                except Exception as exc_view:  # pylint: disable=bare-except
+                    msg = str(exc_view)
+                    raise DataWarehouseOperationError(msg) from exc_view
+            raise DataWarehouseOperationError(msg) from exc
 
     def format_quoted_identifier(self, identifier_name: str) -> str:
         """
         Quote an identifier using the session's convention and return the result as a string
 
         Parameters
         ----------
@@ -671,14 +764,152 @@
 
         Returns
         -------
         str
         """
         return sql_to_string(expr, source_type=self.source_type)
 
+    async def retry_sql(
+        self,
+        sql: str,
+        retry_num: int = 10,
+        sleep_interval: int = 5,
+    ) -> pd.DataFrame | None:
+        """
+        Retry sql operation
+
+        Parameters
+        ----------
+        sql: str
+            SQL query
+        retry_num: int
+            Number of retries
+        sleep_interval: int
+            Sleep interval between retries
+
+        Returns
+        -------
+        pd.DataFrame
+            Result of the sql operation
+
+        Raises
+        ------
+        Exception
+            if the sql operation fails after retry_num retries
+        """
+
+        for i in range(retry_num):
+            try:
+                return await self.execute_query_long_running(sql)
+            except Exception as exc:  # pylint: disable=broad-exception-caught
+                logger.warning(
+                    "SQL query failed",
+                    extra={"attempt": i, "query": sql.strip()[:50].replace("\n", " ")},
+                )
+                if i == retry_num - 1:
+                    logger.error(
+                        "SQL query failed", extra={"attempts": retry_num, "exception": exc}
+                    )
+                    raise
+
+            random_interval = randint(1, sleep_interval)
+            await asyncio.sleep(random_interval)
+
+        return None
+
+    async def table_exists(self, table_name: str) -> bool:
+        """
+        Check if table exists
+
+        Parameters
+        ----------
+        table_name: str
+            input table name
+
+        Returns
+        -------
+            True if table exists, False otherwise
+        """
+        try:
+            expr = (
+                expressions.Select(expressions=[expressions.Star()])
+                .from_(quoted_identifier(table_name.upper()))
+                .limit(1)
+            )
+            await self.execute_query_long_running(
+                sql_to_string(expr, self.source_type), to_log_error=False
+            )
+            return True
+        except self._no_schema_error:  # pylint: disable=broad-except
+            pass
+        return False
+
+    async def create_table_as(
+        self,
+        table_details: NodeTableDetails | str,
+        select_expr: Select | str,
+        kind: Literal["TABLE", "VIEW"] = "TABLE",
+        partition_keys: list[str] | None = None,
+        replace: bool = False,
+        retry: bool = False,
+        retry_num: int = 10,
+        sleep_interval: int = 5,
+    ) -> pd.DataFrame | None:
+        """
+        Create a table using a select statement
+
+        Parameters
+        ----------
+        table_details: NodeTableDetails | str
+            Details or name of the table to be created
+        select_expr: Select | str
+            Select expression or SQL to create the table
+        partition_keys: list[str] | None
+            Partition keys
+        kind: Literal["TABLE", "VIEW"]
+            Kind of table to create
+        replace: bool
+            Whether to replace the table if exists
+        retry: bool
+            Whether to retry the operation
+        retry_num: int
+            Number of retries
+        sleep_interval: int
+            Sleep interval between retries
+
+        Returns
+        -------
+        pd.DataFrame | None
+        """
+        adapter = get_sql_adapter(self.source_type)
+
+        if isinstance(table_details, str):
+            table_details = NodeTableDetails(
+                database_name=None,
+                schema_name=None,
+                table_name=table_details.upper(),  # use upper case for unquoted identifiers
+            )
+        if isinstance(select_expr, str):
+            select_expr = f"SELECT * FROM ({select_expr})"
+
+        query = sql_to_string(
+            adapter.create_table_as(
+                table_details=table_details,
+                select_expr=select_expr,
+                kind=kind,
+                partition_keys=partition_keys,
+                replace=replace,
+            ),
+            source_type=self.source_type,
+        )
+
+        if retry:
+            return await self.retry_sql(query, retry_num=retry_num, sleep_interval=sleep_interval)
+        return await self.execute_query_long_running(query)
+
 
 class SqlObjectType(StrEnum):
     """Enum for type of SQL objects to initialize in Snowflake"""
 
     FUNCTION = "function"
     PROCEDURE = "procedure"
     TABLE = "table"
@@ -919,14 +1150,17 @@
             elif filename.startswith("T_"):
                 sql_object_type = SqlObjectType.TABLE
 
             identifier = filename.replace(".sql", "")
             if sql_object_type == SqlObjectType.TABLE:
                 # Table naming convention does not include "T_" prefix
                 identifier = identifier[len("T_") :]
+            elif sql_object_type == SqlObjectType.FUNCTION and identifier == "F_OBJECT_DELETE":
+                # OBJECT_DELETE does not include "F_" prefix
+                identifier = identifier[len("F_") :]
 
             full_filename = os.path.join(sql_directory, filename)
 
             sql_object = {
                 "type": sql_object_type,
                 "filename": full_filename,
                 "identifier": self._normalize_casing(identifier),
```

### Comparing `featurebyte-1.0.2/featurebyte/session/base_spark.py` & `featurebyte-1.0.3/featurebyte/session/base_spark.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,33 +1,80 @@
 """
 BaseSparkSession class
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, OrderedDict, cast
 
 import collections
 import os
 from abc import ABC, abstractmethod
 
 import pandas as pd
+import pyarrow as pa
 from bson import ObjectId
 from pyhive.exc import OperationalError
 
 from featurebyte.common.path_util import get_package_root
 from featurebyte.enum import DBVarType, InternalName
 from featurebyte.logging import get_logger
 from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
 from featurebyte.query_graph.model.table import TableDetails, TableSpec
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import get_fully_qualified_table_name, sql_to_string
-from featurebyte.session.base import BaseSchemaInitializer, BaseSession, MetadataSchemaInitializer
+from featurebyte.session.base import (
+    INTERACTIVE_SESSION_TIMEOUT_SECONDS,
+    BaseSchemaInitializer,
+    BaseSession,
+    MetadataSchemaInitializer,
+)
 
 logger = get_logger(__name__)
 
+db_vartype_mapping = {
+    "INT": DBVarType.INT,
+    "BINARY": DBVarType.BINARY,
+    "BOOLEAN": DBVarType.BOOL,
+    "DATE": DBVarType.DATE,
+    "DECIMAL": DBVarType.FLOAT,
+    "DOUBLE": DBVarType.FLOAT,
+    "FLOAT": DBVarType.FLOAT,
+    "INTERVAL": DBVarType.TIMEDELTA,
+    "VOID": DBVarType.VOID,
+    "TIMESTAMP": DBVarType.TIMESTAMP,
+    "TIMESTAMP_NTZ": DBVarType.TIMESTAMP,
+    "MAP": DBVarType.DICT,
+    "STRUCT": DBVarType.DICT,
+    "STRING": DBVarType.VARCHAR,
+}
+
+
+pa_type_mapping = {
+    "STRING": pa.string(),
+    "TINYINT": pa.int8(),
+    "SMALLINT": pa.int16(),
+    "INT": pa.int32(),
+    "BIGINT": pa.int64(),
+    "BINARY": pa.large_binary(),
+    "BOOLEAN": pa.bool_(),
+    "DATE": pa.string(),
+    "TIME": pa.time32("ms"),
+    "DOUBLE": pa.float64(),
+    "FLOAT": pa.float32(),
+    # https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.types.DecimalType.html
+    "DECIMAL": pa.decimal128(38, 18),
+    "INTERVAL": pa.duration("ns"),
+    "NULL": pa.null(),
+    "TIMESTAMP": pa.timestamp("ns", tz=None),
+    "ARRAY": pa.string(),
+    "MAP": pa.string(),
+    "STRUCT": pa.string(),
+}
+
 
 class BaseSparkSession(BaseSession, ABC):
     """
     BaseSpark session class
     """
 
     host: str
@@ -101,45 +148,69 @@
         Parameters
         ----------
         remote_path: str
             Remote file path
         """
 
     @staticmethod
-    def _convert_to_internal_variable_type(spark_type: str) -> DBVarType:
+    def _get_pyarrow_type(datatype: str) -> pa.DataType:
+        """
+        Get pyarrow type from Spark data type
+
+        Parameters
+        ----------
+        datatype: str
+            Spark data type
+
+        Returns
+        -------
+        pa.DataType
+        """
+        if datatype.startswith("INTERVAL"):
+            pyarrow_type = pa.int64()
+        elif datatype.startswith("DECIMAL("):
+            # e.g. DECIMAL(10, 2)
+            precision, scale = map(int, datatype[8:-1].split(","))
+            if scale > 0:
+                pyarrow_type = pa.decimal128(precision, scale)
+            else:
+                pyarrow_type = pa.int64()
+        else:
+            pyarrow_type = pa_type_mapping.get(datatype)
+
+        if not pyarrow_type:
+            # warn and fallback to string for unrecognized types
+            logger.warning("Cannot infer pyarrow type", extra={"datatype": datatype})
+            pyarrow_type = pa.string()
+        return pyarrow_type
+
+    @staticmethod
+    def _convert_to_internal_variable_type(  # pylint: disable=too-many-return-statements
+        spark_type: str,
+    ) -> DBVarType:
         if spark_type.endswith("INT"):
             # BIGINT, INT, SMALLINT, TINYINT
             return DBVarType.INT
-        if spark_type.startswith("DECIMAL"):
+        if spark_type.startswith("DECIMAL("):
             # DECIMAL(10, 2)
-            return DBVarType.FLOAT
+            _, scale = map(int, spark_type[8:-1].split(","))
+            if scale > 0:
+                return DBVarType.FLOAT
+            return DBVarType.INT
         if spark_type.startswith("ARRAY"):
             # ARRAY<BIGINT>
             return DBVarType.ARRAY
         if spark_type.startswith("STRUCT"):
-            return DBVarType.STRUCT
-
-        mapping = {
-            "BINARY": DBVarType.BINARY,
-            "BOOLEAN": DBVarType.BOOL,
-            "DATE": DBVarType.DATE,
-            "DECIMAL": DBVarType.FLOAT,
-            "DOUBLE": DBVarType.FLOAT,
-            "FLOAT": DBVarType.FLOAT,
-            "INTERVAL": DBVarType.TIMEDELTA,
-            "VOID": DBVarType.VOID,
-            "TIMESTAMP": DBVarType.TIMESTAMP,
-            "TIMESTAMP_NTZ": DBVarType.TIMESTAMP,
-            "MAP": DBVarType.MAP,
-            "STRUCT": DBVarType.STRUCT,
-            "STRING": DBVarType.VARCHAR,
-        }
-        if spark_type not in mapping:
+            return DBVarType.DICT
+        if spark_type.startswith("MAP"):
+            return DBVarType.DICT
+        db_vartype = db_vartype_mapping.get(spark_type, DBVarType.UNKNOWN)
+        if db_vartype == DBVarType.UNKNOWN:
             logger.warning(f"Spark: Not supported data type '{spark_type}'")
-        return mapping.get(spark_type, DBVarType.UNKNOWN)
+        return db_vartype
 
     async def register_table_with_query(
         self, table_name: str, query: str, temporary: bool = True
     ) -> None:
         if temporary:
             create_command = "CREATE OR REPLACE TEMPORARY VIEW"
         else:
@@ -179,15 +250,15 @@
                 temp_view_name = f"__TEMP_TABLE_{request_id}"
                 await self.execute_query(
                     f"CREATE OR REPLACE TEMPORARY VIEW `{temp_view_name}` USING parquet OPTIONS "
                     f"(path '{self.storage_path}/{temp_filename}')"
                 )
 
                 await self.execute_query(
-                    f"CREATE TABLE `{table_name}` USING DELTA "
+                    f"CREATE OR REPLACE TABLE `{table_name}` USING DELTA "
                     f"TBLPROPERTIES('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5') "
                     f"AS SELECT * FROM `{temp_view_name}`"
                 )
         finally:
             # clean up staging file
             try:
                 self.delete_path_from_storage(remote_path=temp_filename)
@@ -219,33 +290,38 @@
         output = []
         if schemas is not None:
             output.extend(schemas.get("namespace", schemas.get("databaseName")))
             # in DataBricks the header is databaseName instead of namespace
         return output
 
     async def list_tables(
-        self, database_name: str | None = None, schema_name: str | None = None
+        self,
+        database_name: str | None = None,
+        schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> list[TableSpec]:
         tables = await self.execute_query_interactive(
-            f"SHOW TABLES IN `{database_name}`.`{schema_name}`"
+            f"SHOW TABLES IN `{database_name}`.`{schema_name}`", timeout=timeout
         )
         output = []
         if tables is not None:
             for _, (name,) in tables[["tableName"]].iterrows():
                 output.append(TableSpec(name=name))
         return output
 
     async def list_table_schema(
         self,
         table_name: str | None,
         database_name: str | None = None,
         schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> OrderedDict[str, ColumnSpecWithDescription]:
         schema = await self.execute_query_interactive(
             f"DESCRIBE `{database_name}`.`{schema_name}`.`{table_name}`",
+            timeout=timeout,
         )
         column_name_type_map = collections.OrderedDict()
         if schema is not None:
             for _, (column_name, var_info, comment) in schema[
                 ["col_name", "data_type", "comment"]
             ].iterrows():
                 # Sometimes describe include metadata after column details with and empty row as a separator.
@@ -371,15 +447,15 @@
     def __init__(self, session: BaseSparkSession):
         super().__init__(session=session)
         self.metadata_schema_initializer = BaseSparkMetadataSchemaInitializer(session)
 
     @property
     def current_working_schema_version(self) -> int:
         # NOTE: Please also update the version in hive-udf/lib/build.gradle
-        return 13
+        return 15
 
     @property
     def sql_directory_name(self) -> str:
         return "spark"
 
     async def drop_all_objects_in_working_schema(self) -> None:
         if not await self.schema_exists():
@@ -491,17 +567,17 @@
                 "com.featurebyte.hive.udf.VectorAggregateSimpleAverageV1",
             ),
             ("OBJECT_DELETE", "com.featurebyte.hive.udf.ObjectDeleteV1"),
             ("F_TIMESTAMP_TO_INDEX", "com.featurebyte.hive.udf.TimestampToIndexV1"),
             ("F_INDEX_TO_TIMESTAMP", "com.featurebyte.hive.udf.IndexToTimestampV1"),
             (
                 "F_COUNT_DICT_COSINE_SIMILARITY",
-                "com.featurebyte.hive.udf.CountDictCosineSimilarityV1",
+                "com.featurebyte.hive.udf.CountDictCosineSimilarityV2",
             ),
-            ("F_COUNT_DICT_ENTROPY", "com.featurebyte.hive.udf.CountDictEntropyV2"),
+            ("F_COUNT_DICT_ENTROPY", "com.featurebyte.hive.udf.CountDictEntropyV3"),
             ("F_COUNT_DICT_MOST_FREQUENT", "com.featurebyte.hive.udf.CountDictMostFrequentV1"),
             (
                 "F_COUNT_DICT_MOST_FREQUENT_VALUE",
                 "com.featurebyte.hive.udf.CountDictMostFrequentValueV1",
             ),
             ("F_COUNT_DICT_LEAST_FREQUENT", "com.featurebyte.hive.udf.CountDictLeastFrequentV1"),
             ("F_COUNT_DICT_NUM_UNIQUE", "com.featurebyte.hive.udf.CountDictNumUniqueV1"),
```

### Comparing `featurebyte-1.0.2/featurebyte/session/databricks.py` & `featurebyte-1.0.3/featurebyte/session/databricks.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,75 +1,43 @@
 """
 DatabricksSession class
 """
+
 # pylint: disable=duplicate-code
-from typing import Any, AsyncGenerator, BinaryIO, Dict, Optional
+from typing import Any, AsyncGenerator, BinaryIO, Optional
 
-import json
 import os
 from io import BytesIO
 
 import pandas as pd
 import pyarrow as pa
 from bson import ObjectId
 from pydantic import Field, PrivateAttr
 
 from featurebyte import AccessTokenCredential, logging
-from featurebyte.common.utils import pa_table_to_record_batches
+from featurebyte.common.utils import ARROW_METADATA_DB_VAR_TYPE
 from featurebyte.enum import SourceType
 from featurebyte.session.base import APPLICATION_NAME
 from featurebyte.session.base_spark import BaseSparkSession
 
 try:
     from databricks import sql as databricks_sql
-    from databricks.sdk import DbfsExt, WorkspaceClient
+    from databricks.sdk import WorkspaceClient
+    from databricks.sdk.mixins.files import DbfsExt
     from databricks.sql.exc import ServerOperationError
 
     HAS_DATABRICKS_SQL_CONNECTOR = True
 except ImportError:
     HAS_DATABRICKS_SQL_CONNECTOR = False
 
 
 logger = logging.get_logger(__name__)
 
 
-class ArrowTablePostProcessor:
-    """
-    Post processor for Arrow table to fix databricks return format
-    """
-
-    def __init__(self, schema: Dict[str, str]):
-        self._map_columns = []
-        for col_name, var_type in schema.items():
-            if var_type.upper() == "MAP":
-                self._map_columns.append(col_name)
-
-    def to_dataframe(self, arrow_table: pa.Table) -> pd.DataFrame:
-        """
-        Convert Arrow table to Pandas dataframe
-
-        Parameters
-        ----------
-        arrow_table: pa.Table
-            Arrow table to convert
-
-        Returns
-        -------
-        pd.DataFrame:
-            Pandas dataframe
-        """
-        # handle map type. Databricks returns map as list of tuples
-        # https://docs.databricks.com/sql/language-manual/sql-ref-datatypes.html#map
-        # which is not supported by pyarrow. Below converts the tuple list to json string
-        dataframe = arrow_table.to_pandas()
-        for col_name in self._map_columns:
-            dataframe[col_name] = dataframe[col_name].apply(
-                lambda x: json.dumps(dict(x)) if x is not None else None
-            )
-        return dataframe
+DATABRICKS_BATCH_FETCH_SIZE = 1000
 
 
 class DatabricksSession(BaseSparkSession):
     """
     Databricks session class
     """
 
@@ -87,14 +55,15 @@
         self._connection = databricks_sql.connect(
             server_hostname=self.host,
             http_path=self.http_path,
             access_token=self.database_credential.access_token,
             catalog=self.catalog_name,
             schema=self.schema_name,
             _user_agent_entry=APPLICATION_NAME,
+            _use_arrow_native_complex_types=False,
         )
 
     @property
     def _workspace_client(self) -> WorkspaceClient:
         # ensure google credentials not in environment variables to avoid conflict
         os.environ.pop("GOOGLE_CREDENTIALS", None)
         return WorkspaceClient(
@@ -142,39 +111,59 @@
         path = f"{self._storage_base_path}/{remote_path}"
         self._delete_file_from_storage(path)
 
     @classmethod
     def is_threadsafe(cls) -> bool:
         return True
 
+    def _get_schema_from_cursor(self, cursor: Any) -> pa.Schema:
+        """
+        Get schema from a cursor
+
+        Parameters
+        ----------
+        cursor: Any
+            Cursor to fetch data from
+
+        Returns
+        -------
+        pa.Schema
+        """
+        fields = []
+        for row in cursor.description:
+            field_name = row[0]
+            field_type = row[1].upper()
+            db_var_type = self._convert_to_internal_variable_type(field_type)
+            fields.append(
+                pa.field(
+                    field_name,
+                    self._get_pyarrow_type(field_type),
+                    metadata={ARROW_METADATA_DB_VAR_TYPE: db_var_type},
+                )
+            )
+        return pa.schema(fields)
+
     def fetch_query_result_impl(self, cursor: Any) -> Optional[pd.DataFrame]:
         schema = None
         if cursor.description:
-            schema = {row[0]: row[1] for row in cursor.description}
+            schema = self._get_schema_from_cursor(cursor)
 
         if schema:
-            post_processor = ArrowTablePostProcessor(schema=schema)
-            arrow_table = cursor.fetchall_arrow()
-            return post_processor.to_dataframe(arrow_table)
+            return cursor.fetchall_arrow().cast(schema).to_pandas()
 
         return None
 
     async def fetch_query_stream_impl(self, cursor: Any) -> AsyncGenerator[pa.RecordBatch, None]:
         schema = None
         if cursor.description:
-            schema = {row[0]: row[1] for row in cursor.description}
+            schema = self._get_schema_from_cursor(cursor)
 
         if schema:
-            post_processor = ArrowTablePostProcessor(schema=schema)
             # fetch results in batches
-            counter = 0
             while True:
-                dataframe = post_processor.to_dataframe(cursor.fetchmany_arrow(size=1000))
-                arrow_table = pa.Table.from_pandas(dataframe)
-                if arrow_table.num_rows == 0:
-                    if counter == 0:
-                        # return empty dataframe with correct schema
-                        yield pa_table_to_record_batches(arrow_table)[0]
+                table = cursor.fetchmany_arrow(size=DATABRICKS_BATCH_FETCH_SIZE)
+                if table.shape[0] == 0:
+                    # return empty table to ensure correct schema is returned
+                    yield pa.record_batch([[]] * len(schema), schema=schema)
                     break
-                for record_batch in arrow_table.to_batches():
-                    counter += 1
-                    yield record_batch
+                for batch in table.cast(schema).to_batches():
+                    yield batch
```

### Comparing `featurebyte-1.0.2/featurebyte/session/databricks_unity.py` & `featurebyte-1.0.3/featurebyte/session/sqlite.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,118 +1,125 @@
 """
-Databricks unity
+SQLiteSession class
 """
+
 from __future__ import annotations
 
-from typing import Any, BinaryIO
+from typing import Optional, OrderedDict
 
-from pydantic import Field, PrivateAttr
+import collections
+import os
+import sqlite3
 
-from featurebyte import SourceType
-from featurebyte.query_graph.model.table import TableSpec
-from featurebyte.session.base import BaseSchemaInitializer
-from featurebyte.session.base_spark import BaseSparkSchemaInitializer
-from featurebyte.session.databricks import DatabricksSession
+import pandas as pd
+from pydantic import Field
 
-try:
-    from databricks.sdk import FilesAPI
-except ImportError:
-    pass
+from featurebyte.enum import DBVarType, SourceType
+from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
+from featurebyte.query_graph.model.table import TableSpec
+from featurebyte.session.base import (
+    INTERACTIVE_SESSION_TIMEOUT_SECONDS,
+    BaseSchemaInitializer,
+    BaseSession,
+)
 
 
-class DatabricksUnitySchemaInitializer(BaseSparkSchemaInitializer):
+class SQLiteSession(BaseSession):
     """
-    Databricks unity schema initializer
+    SQLite session class
     """
 
-    @property
-    def current_working_schema_version(self) -> int:
-        return 13
-
-    async def create_schema(self) -> None:
-        await super().create_schema()
-        # grant permissions on schema to the group
-        assert isinstance(self.session, DatabricksUnitySession)
-        grant_permissions_query = f"GRANT ALL PRIVILEGES ON SCHEMA `{self.session.schema_name}` TO `{self.session.group_name}`"
-        await self.session.execute_query(grant_permissions_query)
-        # create staging volume if not exists
-        await self.session.execute_query(f"CREATE VOLUME IF NOT EXISTS {self.session.volume_name}")
-
-    @property
-    def sql_directory_name(self) -> str:
-        return "databricks_unity"
-
-    def register_jar(self) -> None:
-        """
-        Override since we don't need to register any jar.
-        """
-
-    async def register_functions_from_jar(self) -> None:
-        """
-        Override to not do anything, and just do the default registration.
-        """
-
-
-class DatabricksUnitySession(DatabricksSession):
-    """
-    Databricks Unity session class
-    """
+    filename: str
+    source_type: SourceType = Field(SourceType.SQLITE, const=True)
 
-    _files_client: FilesAPI = PrivateAttr()
+    def initializer(self) -> Optional[BaseSchemaInitializer]:
+        return None
 
-    source_type: SourceType = Field(SourceType.DATABRICKS_UNITY, const=True)
-    group_name: str
+    def _initialize_connection(self) -> None:
+        filename = self.filename
+        if not os.path.exists(filename):
+            raise FileNotFoundError(f"SQLite file '{filename}' not found!")
 
-    def __init__(self, **data: Any) -> None:
-        # set storage path based on catalog and schema name
-        catalog_name = data.get("catalog_name")
-        schema_name = data.get("schema_name")
-        data["storage_path"] = f"/Volumes/{catalog_name}/{schema_name}/staging"
-        super().__init__(**data)
-
-    def initializer(self) -> BaseSchemaInitializer:
-        return DatabricksUnitySchemaInitializer(self)
-
-    @property
-    def volume_name(self) -> str:
-        """
-        Volume name for storage
-
-        Returns
-        -------
-        str:
-            Volume name
-        """
-        return f"`{self.catalog_name}`.`{self.schema_name}`.`staging`"
-
-    def _initialize_storage(self) -> None:
-        self.storage_path = self.storage_path.rstrip("/")
-        self._storage_base_path = self.storage_path
-        self._files_client = FilesAPI(self._workspace_client.api_client)
+        self._connection = sqlite3.connect(filename)
 
-    def _upload_file_to_storage(self, path: str, src: BinaryIO) -> None:
-        self._files_client.upload(file_path=path, contents=src, overwrite=True)
+    @classmethod
+    def is_threadsafe(cls) -> bool:
+        return False
 
-    def _delete_file_from_storage(self, path: str) -> None:
-        self._files_client.delete(file_path=path)
+    async def list_databases(self) -> list[str]:
+        return []
 
     async def list_schemas(self, database_name: str | None = None) -> list[str]:
-        schemas = await self.execute_query_interactive(
-            f"SELECT SCHEMA_NAME FROM `{database_name}`.INFORMATION_SCHEMA.SCHEMATA"
-        )
-        output = []
-        if schemas is not None:
-            output.extend(schemas["SCHEMA_NAME"].tolist())
-        return output
+        return []
 
     async def list_tables(
-        self, database_name: str | None = None, schema_name: str | None = None
+        self,
+        database_name: str | None = None,
+        schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> list[TableSpec]:
-        tables = await self.execute_query_interactive(
-            f"SELECT TABLE_NAME FROM `{database_name}`.INFORMATION_SCHEMA.TABLES "
-            f"WHERE TABLE_SCHEMA = '{schema_name}'"
+        tables = await self.execute_query(
+            "SELECT name FROM sqlite_master WHERE type = 'table'", timeout=timeout
         )
         output = []
         if tables is not None:
-            for _, (name,) in tables[["TABLE_NAME"]].iterrows():
+            for _, (name,) in tables[["name"]].iterrows():
                 output.append(TableSpec(name=name))
         return output
+
+    @staticmethod
+    def _convert_to_internal_variable_type(sqlite_data_type: str) -> DBVarType:
+        if "INT" in sqlite_data_type:
+            return DBVarType.INT
+        if "CHAR" in sqlite_data_type or "TEXT" in sqlite_data_type:
+            return DBVarType.VARCHAR
+        if (
+            "REAL" in sqlite_data_type
+            or "DOUBLE" in sqlite_data_type
+            or "FLOAT" in sqlite_data_type
+            or "DECIMAL" in sqlite_data_type
+        ):
+            return DBVarType.FLOAT
+        if "BOOLEAN" in sqlite_data_type:
+            return DBVarType.BOOL
+        if "DATETIME" in sqlite_data_type or "TIMESTAMP" in sqlite_data_type:
+            return DBVarType.TIMESTAMP
+        if "DATE" in sqlite_data_type:
+            return DBVarType.DATE
+        raise ValueError(f"Not supported data type '{sqlite_data_type}'")
+
+    async def list_table_schema(
+        self,
+        table_name: str | None,
+        database_name: str | None = None,
+        schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
+    ) -> OrderedDict[str, ColumnSpecWithDescription]:
+        schema = await self.execute_query(f'PRAGMA table_info("{table_name}")', timeout=timeout)
+        column_name_type_map = collections.OrderedDict()
+        if schema is not None:
+            for _, (column_name, data_type) in schema[["name", "type"]].iterrows():
+                dtype = self._convert_to_internal_variable_type(data_type)
+                column_name_type_map[column_name] = ColumnSpecWithDescription(
+                    name=column_name,
+                    dtype=dtype,
+                    description=None,  # sqlite doesn't provide any meta for description
+                )
+        return column_name_type_map
+
+    async def register_table(
+        self, table_name: str, dataframe: pd.DataFrame, temporary: bool = True
+    ) -> None:
+        raise NotImplementedError()
+
+    async def execute_query(
+        self, query: str, timeout: float = 600, to_log_error: bool = True
+    ) -> pd.DataFrame | None:
+        # sqlite session cannot be used in across threads
+        _ = timeout
+        return super().execute_query_blocking(query=query)
+
+    async def comment_table(self, table_name: str, comment: str) -> None:
+        pass
+
+    async def comment_column(self, table_name: str, column_name: str, comment: str) -> None:
+        pass
```

### Comparing `featurebyte-1.0.2/featurebyte/session/hive.py` & `featurebyte-1.0.3/featurebyte/session/hive.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Customized Hive Connection class
 """
+
 from typing import Any, Mapping, Optional
 
 import logging
 from ssl import CERT_NONE, create_default_context
 
 from pyhive import hive
 from pyhive.exc import OperationalError
```

### Comparing `featurebyte-1.0.2/featurebyte/session/manager.py` & `featurebyte-1.0.3/featurebyte/session/manager.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,59 +1,62 @@
 """
 SessionManager class
 """
+
 from __future__ import annotations
 
 from typing import Any, Dict, Hashable
 
 import json
 import time
 from asyncio.exceptions import TimeoutError as AsyncioTimeoutError
 
 from asyncache import cached
-from cachetools import TTLCache, keys
-from pydantic import BaseModel
+from cachetools import keys
+from pydantic import BaseModel, Field
 
 from featurebyte.enum import SourceType
 from featurebyte.exception import SessionInitializationTimeOut
 from featurebyte.logging import get_logger
 from featurebyte.models.credential import CredentialModel
 from featurebyte.models.feature_store import FeatureStoreModel
 from featurebyte.query_graph.node.schema import DatabaseDetails
-from featurebyte.session.base import NON_INTERACTIVE_SESSION_TIMEOUT_SECONDS, BaseSession, to_thread
+from featurebyte.session.base import (
+    NON_INTERACTIVE_SESSION_TIMEOUT_SECONDS,
+    BaseSession,
+    session_cache,
+    to_thread,
+)
 from featurebyte.session.databricks import DatabricksSession
 from featurebyte.session.databricks_unity import DatabricksUnitySession
 from featurebyte.session.snowflake import SnowflakeSession
 from featurebyte.session.spark import SparkSession
 from featurebyte.session.sqlite import SQLiteSession
 
 SOURCE_TYPE_SESSION_MAP = {
     SourceType.SQLITE: SQLiteSession,
     SourceType.SNOWFLAKE: SnowflakeSession,
     SourceType.DATABRICKS: DatabricksSession,
     SourceType.DATABRICKS_UNITY: DatabricksUnitySession,
     SourceType.SPARK: SparkSession,
 }
 
-session_cache: TTLCache[Any, Any] = TTLCache(maxsize=1024, ttl=600)
-
-
 logger = get_logger(__name__)
 
 
-async def get_new_session(item: str, credential_params: str, timeout: float) -> BaseSession:
+async def get_new_session(item: str, params: str, timeout: float) -> BaseSession:
     """
     Create a new session for the given database source key
 
     Parameters
     ----------
     item: str
         JSON dumps of feature store type & details
-    credential_params: str
-        JSON dumps of credential parameters used to initiate a new session
+    params: str
+        JSON dumps of parameters used to initiate a new session
     timeout: float
         timeout for session creation
 
     Returns
     -------
     BaseSession
         Newly created session
@@ -62,86 +65,93 @@
     ------
     SessionInitializationTimeOut
         If session creation timed out
     """
     tic = time.time()
     item_dict = json.loads(item)
     logger.debug(f'Create a new session for {item_dict["type"]}')
-    credential_params_dict = json.loads(credential_params)
+    params_dict = json.loads(params)
 
     def _create_session() -> BaseSession:
         """
         Create a new session for the given database source key
 
         Returns
         -------
         BaseSession
         """
         return SOURCE_TYPE_SESSION_MAP[item_dict["type"]](  # type: ignore
-            **item_dict["details"], **credential_params_dict
+            **item_dict["details"], **params_dict
         )
 
     try:
         session: BaseSession = await to_thread(_create_session, timeout)
         await session.initialize()
         logger.debug(f"Session creation time: {time.time() - tic:.3f}s")
     except AsyncioTimeoutError as exc:
         raise SessionInitializationTimeOut(
             f"Session creation timed out after {time.time() - tic:.3f}s"
         ) from exc
     return session
 
 
-def _session_hash_key(*args: Any, **kwargs: Any) -> tuple[Hashable, ...]:
+def _session_hash_key(item: str, params: str, timeout: float) -> tuple[Hashable, ...]:
     """
-    Return a cache key for the specified hashable arguments.
+    Return a cache key for the specified hashable arguments. The signature of this function must match the
+    signature of the `get_session` function.
 
     Parameters
     ----------
-    *args: Any
-        Positional arguments
-    **kwargs: Any
-        Keyword arguments
+    item: str
+        JSON dumps of feature store type & details
+    params: str
+        JSON dumps of parameters used to initiate a new session
+    timeout: float
+        timeout for session creation
 
     Returns
     -------
     tuple[Hashable, ...]
     """
     # exclude timeout from hash
-    return keys.hashkey(*args, **{key: value for key, value in kwargs.items() if key != "timeout"})
+    _ = timeout
+    return keys.hashkey(item=item, params=params)
 
 
 @cached(cache=session_cache, key=_session_hash_key)
-async def get_session(item: str, credential_params: str, timeout: float) -> BaseSession:
+async def get_session(item: str, params: str, timeout: float) -> BaseSession:
     """
     Retrieve or create a new session for the given database source key. If a new session is created,
     it will be cached.
 
     Parameters
     ----------
     item: str
         JSON dumps of feature store type & details
-    credential_params: str
-        JSON dumps of credential parameters used to initiate a new session
+    params: str
+        JSON dumps of parameters used to initiate a new session
     timeout: float
         timeout for session creation
 
     Returns
     -------
     BaseSession
         Retrieved or created session object
     """
-    return await get_new_session(item, credential_params, timeout=timeout)
+    session = await get_new_session(item, params, timeout=timeout)
+    session.set_cache_key(_session_hash_key(item, params, timeout))
+    return session
 
 
 class SessionManager(BaseModel):
     """
     Session manager to manage session of different database sources
     """
 
+    parameters: Dict[str, Any] = Field(default_factory=dict)
     credentials: Dict[str, CredentialModel]
 
     async def get_session_with_params(
         self,
         feature_store_name: str,
         session_type: SourceType,
         details: DatabaseDetails,
@@ -176,38 +186,42 @@
         elif session_type not in SourceType.credential_required_types():
             credential = None
         else:
             raise ValueError(
                 f'Credentials do not contain info for the feature store "{feature_store_name}"!'
             )
 
+        params = {**self.parameters}
+
+        # add credentials to session parameters
         credential_params = (
             {
                 key: value
                 for key, value in credential.json_dict().items()
                 if key in ["database_credential", "storage_credential"]
             }
             if credential
             else {}
         )
+        params.update(credential_params)
 
         json_str = json.dumps(
             {
                 "type": session_type,
                 "details": details.json_dict(),
             },
             sort_keys=True,
         )
         if SOURCE_TYPE_SESSION_MAP[session_type].is_threadsafe():
             get_session_func = get_session
         else:
             get_session_func = get_new_session
         session = await get_session_func(
             item=json_str,
-            credential_params=json.dumps(credential_params, sort_keys=True),
+            params=json.dumps(params, sort_keys=True),
             timeout=timeout,
         )
         assert isinstance(session, BaseSession)
         return session
 
     async def get_session(
         self, item: FeatureStoreModel, timeout: float = NON_INTERACTIVE_SESSION_TIMEOUT_SECONDS
```

### Comparing `featurebyte-1.0.2/featurebyte/session/simple_storage.py` & `featurebyte-1.0.3/featurebyte/session/simple_storage.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 SimpleStorage classes for basic object store operations
 """
+
 from typing import Any, Dict, Literal, Optional
 
 import os
 from abc import ABC, abstractmethod
 from contextlib import contextmanager
 from pathlib import Path
```

### Comparing `featurebyte-1.0.2/featurebyte/session/snowflake.py` & `featurebyte-1.0.3/featurebyte/session/snowflake.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,53 +1,66 @@
 """
 SnowflakeSession class
 """
+
 from __future__ import annotations
 
 from typing import Any, AsyncGenerator, OrderedDict
 
 import collections
 import datetime
 import json
 import logging
 
 import pandas as pd
 import pyarrow as pa
 from pydantic import Field
 from snowflake import connector
-from snowflake.connector.cursor import ResultMetadata
-from snowflake.connector.errors import (
-    DatabaseError,
-    NotSupportedError,
-    OperationalError,
-    ProgrammingError,
-)
+from snowflake.connector.constants import FIELD_TYPES
+from snowflake.connector.errors import Error as SnowflakeError
+from snowflake.connector.errors import NotSupportedError, ProgrammingError
 from snowflake.connector.pandas_tools import write_pandas
 
-from featurebyte.common.utils import pa_table_to_record_batches
+from featurebyte.common.utils import ARROW_METADATA_DB_VAR_TYPE
 from featurebyte.enum import DBVarType, SourceType
-from featurebyte.exception import CredentialsError
+from featurebyte.exception import CursorSchemaError, DataWarehouseConnectionError
 from featurebyte.logging import get_logger
 from featurebyte.models.credential import UsernamePasswordCredential
 from featurebyte.query_graph.model.column_info import ColumnSpecWithDescription
 from featurebyte.query_graph.model.table import TableDetails, TableSpec
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import (
     get_fully_qualified_table_name,
     quoted_identifier,
     sql_to_string,
 )
-from featurebyte.session.base import APPLICATION_NAME, BaseSchemaInitializer, BaseSession
+from featurebyte.session.base import (
+    APPLICATION_NAME,
+    INTERACTIVE_SESSION_TIMEOUT_SECONDS,
+    BaseSchemaInitializer,
+    BaseSession,
+)
 from featurebyte.session.enum import SnowflakeDataType
 
 logger = get_logger(__name__)
 
 logging.getLogger("snowflake.connector").setLevel(logging.ERROR)
 
 
+db_vartype_mapping = {
+    SnowflakeDataType.REAL: DBVarType.FLOAT,
+    SnowflakeDataType.BINARY: DBVarType.BINARY,
+    SnowflakeDataType.BOOLEAN: DBVarType.BOOL,
+    SnowflakeDataType.DATE: DBVarType.DATE,
+    SnowflakeDataType.TIME: DBVarType.TIME,
+    SnowflakeDataType.ARRAY: DBVarType.ARRAY,
+    SnowflakeDataType.OBJECT: DBVarType.DICT,
+}
+
+
 class SnowflakeSession(BaseSession):
     """
     Snowflake session class
 
 
     References
     ----------
@@ -74,25 +87,26 @@
                 warehouse=self.warehouse,
                 database=self.database_name,
                 schema=self.schema_name,
                 role_name=self.role_name,
                 application=APPLICATION_NAME,
                 client_session_keep_alive=True,
             )
-        except (OperationalError, DatabaseError) as exc:
-            raise CredentialsError("Invalid credentials provided.") from exc
+        except SnowflakeError as exc:
+            raise DataWarehouseConnectionError(exc.msg) from exc
 
-    async def initialize(self) -> None:
-        # If the featurebyte schema does not exist, the self._connection can still be created
-        # without errors. Below checks whether the schema actually exists. If not, it will be
-        # created and initialized with custom functions and procedures.
-        await self.execute_query(f'USE ROLE "{self.role_name}"')
-        await super().initialize()
+        cursor = self._connection.cursor()
+        cursor.execute(f'USE ROLE "{self.role_name}"')
+        try:
+            cursor.execute(f'USE SCHEMA "{self.database_name}"."{self.schema_name}"')
+        except self._no_schema_error:
+            # the schema may not exist yet if the feature store is being created
+            pass
         # set timezone to UTC
-        await self.execute_query(
+        cursor.execute(
             "ALTER SESSION SET TIMEZONE='UTC', TIMESTAMP_OUTPUT_FORMAT='YYYY-MM-DD HH24:MI:SS.FF9 TZHTZM'"
         )
 
     def initializer(self) -> BaseSchemaInitializer:
         return SnowflakeSchemaInitializer(self)
 
     @classmethod
@@ -136,26 +150,68 @@
             output.extend(schemas["SCHEMA_NAME"].tolist())
         return output
 
     async def list_tables(
         self,
         database_name: str | None = None,
         schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> list[TableSpec]:
         tables = await self.execute_query_interactive(
             f'SELECT TABLE_NAME, COMMENT FROM "{database_name}".INFORMATION_SCHEMA.TABLES '
-            f"WHERE TABLE_SCHEMA = '{schema_name}'"
+            f"WHERE TABLE_SCHEMA = '{schema_name}'",
+            timeout=timeout,
         )
         output = []
         if tables is not None:
             for _, (name, comment) in tables[["TABLE_NAME", "COMMENT"]].iterrows():
                 output.append(TableSpec(name=name, description=comment or None))
 
         return output
 
+    def _get_schema_from_cursor(self, cursor: Any) -> pa.Schema:
+        """
+        Get schema from a cursor
+
+        Parameters
+        ----------
+        cursor: Any
+            Cursor to fetch data from
+
+        Returns
+        -------
+        pa.Schema
+
+        Raises
+        ------
+        CursorSchemaError
+            When the cursor description is not as expected
+        """
+        fields = []
+        for field in cursor.description:
+            if not hasattr(field, "type_code"):
+                raise CursorSchemaError()
+            field_type = FIELD_TYPES[field.type_code]
+            if field_type.name == "FIXED" and field.scale and field.scale > 0:
+                # DECIMAL type
+                pa_type = pa.decimal128(field.precision, field.scale)
+            else:
+                pa_type = field_type.pa_type(field)
+            db_var_type = self._convert_to_internal_variable_type(
+                {
+                    "type": field_type.name,
+                    "length": field.internal_size,
+                    "scale": field.scale,
+                }
+            )
+            fields.append(
+                pa.field(field.name, pa_type, metadata={ARROW_METADATA_DB_VAR_TYPE: db_var_type})
+            )
+        return pa.schema(fields)
+
     def fetch_query_result_impl(self, cursor: Any) -> pd.DataFrame | None:
         """
         Fetch the result of executed SQL query from connection cursor
 
         This is an implementation specific to Snowflake that is more efficient when applicable.
 
         Parameters
@@ -175,59 +231,33 @@
             except NotSupportedError:
                 # fetch_pandas_all() raises NotSupportedError when: 1) The executed query does not
                 # support it. Currently, only SELECT statements are supported; 2) pyarrow is not
                 # available as a dependency.
                 return super().fetch_query_result_impl(cursor)
         return None
 
-    @staticmethod
-    def _get_column_name_to_db_var_type(cursor: Any) -> dict[str, DBVarType]:
-        """
-        Get DB var type of data being fetched from the snowflake cursor.
-
-        Parameters
-        ----------
-        cursor: Any
-            The connection cursor
-
-        Returns
-        -------
-        dict[str, DBVarType]
-            A dictionary mapping column names to DB var types
-        """
-        output = {}
-        # See https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#label-python-connector-type-codes
-        # for more details. We currently only map one value since that's the only one we need to do any special
-        # handling for. If we need to apply more special handling in the future, we can add more mappings here.
-        type_code_to_db_var_type_mapping: dict[int, DBVarType] = {10: DBVarType.ARRAY}
-        for metadata in cursor.description:
-            if not isinstance(metadata, ResultMetadata):
-                continue
-            type_code = metadata.type_code
-            if type_code in type_code_to_db_var_type_mapping:
-                output[metadata.name] = type_code_to_db_var_type_mapping[type_code]
-        return output
-
     async def fetch_query_stream_impl(self, cursor: Any) -> AsyncGenerator[pa.RecordBatch, None]:
-        col_name_to_db_var_type = self._get_column_name_to_db_var_type(cursor)
         # fetch results in batches and write to the stream
+        schema = None
         try:
-            counter = 0
+            schema = self._get_schema_from_cursor(cursor)
             for table in cursor.fetch_arrow_batches():
-                for batch in pa_table_to_record_batches(table, col_name_to_db_var_type):
-                    counter += 1
+                for batch in table.cast(schema).to_batches():
                     yield batch
-            if counter == 0:
-                # Arrow batch is empty, need to return empty table with schema
-                table = cursor.get_result_batches()[0].to_arrow()
-                yield pa_table_to_record_batches(table)[0]
-        except NotSupportedError:
+            # return empty table to ensure correct schema is returned
+            yield pa.record_batch(
+                pd.DataFrame(columns=[field.name for field in schema]), schema=schema
+            )
+        except (NotSupportedError, CursorSchemaError):
             batches = super().fetch_query_stream_impl(cursor)
             async for batch in batches:
-                yield batch
+                if schema is None:
+                    yield batch
+                else:
+                    yield pa.record_batch(batch.to_pandas(), schema=schema)
 
     async def register_table(
         self, table_name: str, dataframe: pd.DataFrame, temporary: bool = True
     ) -> None:
         schema = self.get_columns_schema_from_dataframe(dataframe)
         if temporary:
             create_command = "CREATE OR REPLACE TEMP TABLE"
@@ -241,25 +271,16 @@
             """
         )
         dataframe = self._prep_dataframe_before_write_pandas(dataframe, schema)
         write_pandas(self._connection, dataframe, table_name)
 
     @staticmethod
     def _convert_to_internal_variable_type(snowflake_var_info: dict[str, Any]) -> DBVarType:
-        to_internal_variable_map = {
-            SnowflakeDataType.REAL: DBVarType.FLOAT,
-            SnowflakeDataType.BINARY: DBVarType.BINARY,
-            SnowflakeDataType.BOOLEAN: DBVarType.BOOL,
-            SnowflakeDataType.DATE: DBVarType.DATE,
-            SnowflakeDataType.TIME: DBVarType.TIME,
-            SnowflakeDataType.ARRAY: DBVarType.ARRAY,
-            SnowflakeDataType.OBJECT: DBVarType.OBJECT,
-        }
-        if snowflake_var_info["type"] in to_internal_variable_map:
-            return to_internal_variable_map[snowflake_var_info["type"]]
+        if snowflake_var_info["type"] in db_vartype_mapping:
+            return db_vartype_mapping[snowflake_var_info["type"]]
         if snowflake_var_info["type"] == SnowflakeDataType.FIXED:
             # scale is defined as number of digits following the decimal point (see link in reference)
             return DBVarType.INT if snowflake_var_info["scale"] == 0 else DBVarType.FLOAT
         if snowflake_var_info["type"] == SnowflakeDataType.TEXT:
             return DBVarType.CHAR if snowflake_var_info["length"] == 1 else DBVarType.VARCHAR
         if snowflake_var_info["type"] in {
             SnowflakeDataType.TIMESTAMP_LTZ,
@@ -274,17 +295,19 @@
         return DBVarType.UNKNOWN
 
     async def list_table_schema(
         self,
         table_name: str | None,
         database_name: str | None = None,
         schema_name: str | None = None,
+        timeout: float = INTERACTIVE_SESSION_TIMEOUT_SECONDS,
     ) -> OrderedDict[str, ColumnSpecWithDescription]:
         schema = await self.execute_query_interactive(
             f'SHOW COLUMNS IN "{database_name}"."{schema_name}"."{table_name}"',
+            timeout=timeout,
         )
         column_name_type_map = collections.OrderedDict()
         if schema is not None:
             for _, (column_name, var_info, comment) in schema[
                 ["column_name", "data_type", "comment"]
             ].iterrows():
                 dtype = self._convert_to_internal_variable_type(json.loads(var_info))
@@ -310,22 +333,24 @@
         if details is None or details.shape[0] == 0:
             raise self.no_schema_error(f"Table {table_name} not found.")
 
         fully_qualified_table_name = get_fully_qualified_table_name(
             {"table_name": table_name, "schema_name": schema_name, "database_name": database_name}
         )
         return TableDetails(
-            details=details.iloc[0].to_dict(),
+            details=json.loads(details.iloc[0].to_json(orient="index")),
             fully_qualified_name=sql_to_string(
                 fully_qualified_table_name, source_type=self.source_type
             ),
         )
 
     @staticmethod
-    def get_columns_schema_from_dataframe(dataframe: pd.DataFrame) -> list[tuple[str, str]]:
+    def get_columns_schema_from_dataframe(  # pylint: disable=too-many-branches
+        dataframe: pd.DataFrame,
+    ) -> list[tuple[str, str]]:
         """Get schema that can be used in CREATE TABLE statement from pandas DataFrame
 
         Parameters
         ----------
         dataframe : pd.DataFrame
             Input DataFrame
 
@@ -341,14 +366,16 @@
                 else:
                     db_type = "TIMESTAMP_NTZ"
             elif pd.api.types.is_datetime64_any_dtype(dataframe[colname]):
                 if pd.api.types.is_datetime64tz_dtype(dataframe[colname]):
                     db_type = "TIMESTAMP_TZ"
                 else:
                     db_type = "TIMESTAMP_NTZ"
+            elif pd.api.types.is_bool_dtype(dtype):
+                db_type = "BOOLEAN"
             elif pd.api.types.is_float_dtype(dtype):
                 db_type = "DOUBLE"
             elif pd.api.types.is_integer_dtype(dtype):
                 db_type = "INT"
             elif (
                 dataframe.shape[0] > 0
                 and dataframe[colname].apply(lambda x: x is None or isinstance(x, list)).all()
@@ -404,15 +431,15 @@
 
     @property
     def sql_directory_name(self) -> str:
         return "snowflake"
 
     @property
     def current_working_schema_version(self) -> int:
-        return 32
+        return 34
 
     async def create_schema(self) -> None:
         create_schema_query = f'CREATE SCHEMA "{self.session.schema_name}"'
         await self.session.execute_query(create_schema_query)
 
     async def drop_object(self, object_type: str, name: str) -> None:
         query = f"DROP {object_type} {self._fully_qualified(name)}"
```

### Comparing `featurebyte-1.0.2/featurebyte/session/spark.py` & `featurebyte-1.0.3/featurebyte/session/spark.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,31 +1,30 @@
 """
 SparkSession class
 """
+
 # pylint: disable=duplicate-code
 # pylint: disable=wrong-import-order
 from __future__ import annotations
 
 from typing import Any, AsyncGenerator, Optional, Union, cast
 from typing_extensions import Annotated
 
 import os
 import subprocess
 import tempfile
 
 import pandas as pd
 import pyarrow as pa
-from pandas.core.dtypes.common import is_datetime64_dtype, is_float_dtype
-from pyarrow import Schema
+from pyarrow import ArrowTypeError, Schema
 from pydantic import Field, PrivateAttr
 from pyhive.exc import OperationalError
-from pyhive.hive import Cursor
 from thrift.transport.TTransport import TTransportException
 
-from featurebyte.common.utils import literal_eval
+from featurebyte.common.utils import ARROW_METADATA_DB_VAR_TYPE
 from featurebyte.enum import SourceType, StorageType
 from featurebyte.logging import get_logger
 from featurebyte.models.credential import (
     AccessTokenCredential,
     AzureBlobStorageCredential,
     GCSStorageCredential,
     KerberosKeytabCredential,
@@ -49,17 +48,14 @@
 
 SparkDatabaseCredential = Annotated[
     Union[KerberosKeytabCredential, AccessTokenCredential],
     Field(discriminator="type"),
 ]
 
 
-PYARROW_ARRAY_TYPE = pa.list_(pa.float64())
-
-
 class SparkSession(BaseSparkSession):
     """
     Spark session class
     """
 
     _no_schema_error = OperationalError
     _connection: Optional[HiveConnection] = PrivateAttr(None)
@@ -237,154 +233,57 @@
     def delete_path_from_storage(self, remote_path: str) -> None:
         self._storage.delete_object(path=remote_path)
 
     @classmethod
     def is_threadsafe(cls) -> bool:
         return False
 
-    def _get_pyarrow_type(self, datatype: str) -> pa.types:
-        """
-        Get pyarrow type from Spark data type
-
-        Parameters
-        ----------
-        datatype: str
-            Spark data type
-
-        Returns
-        -------
-        pa.types
-        """
-        datatype = datatype.upper()
-        mapping = {
-            "STRING_TYPE": pa.string(),
-            "TINYINT_TYPE": pa.int8(),
-            "SMALLINT_TYPE": pa.int16(),
-            "INT_TYPE": pa.int32(),
-            "BIGINT_TYPE": pa.int64(),
-            "BINARY_TYPE": pa.large_binary(),
-            "BOOLEAN_TYPE": pa.bool_(),
-            "DATE_TYPE": pa.timestamp("ns", tz=None),
-            "TIME_TYPE": pa.time32("ms"),
-            "DOUBLE_TYPE": pa.float64(),
-            "FLOAT_TYPE": pa.float32(),
-            "DECIMAL_TYPE": pa.float64(),
-            "INTERVAL_TYPE": pa.duration("ns"),
-            "NULL_TYPE": pa.null(),
-            "TIMESTAMP_TYPE": pa.timestamp("ns", tz=None),
-            "ARRAY_TYPE": PYARROW_ARRAY_TYPE,
-            "MAP_TYPE": pa.string(),
-            "STRUCT_TYPE": pa.string(),
-        }
-        if datatype.startswith("INTERVAL"):
-            pyarrow_type = pa.int64()
-        else:
-            pyarrow_type = mapping.get(datatype)
-
-        if not pyarrow_type:
-            # warn and fallback to string for unrecognized types
-            logger.warning("Cannot infer pyarrow type", extra={"datatype": datatype})
-            pyarrow_type = pa.string()
-        return pyarrow_type
-
-    def _process_batch_data(self, data: pd.DataFrame, schema: Schema) -> pd.DataFrame:
+    def _get_schema_from_cursor(self, cursor: Any) -> Schema:
         """
-        Process batch data before converting to PyArrow record batch.
+        Get schema from a cursor
 
         Parameters
         ----------
-        data: pd.DataFrame
-            Data to process
-        schema: Schema
-            Schema of the data
-
-        Returns
-        -------
-        pd.DataFrame
-            Processed data
-        """
-        if data.empty:
-            return data
-
-        for i, column in enumerate(schema.names):
-            current_type = schema.field(i).type
-            # Convert decimal columns to float
-            if current_type == pa.float64() and not is_float_dtype(data[column]):
-                data[column] = data[column].astype(float)
-            elif isinstance(current_type, pa.TimestampType) and not is_datetime64_dtype(
-                data[column]
-            ):
-                data[column] = pd.to_datetime(data[column])
-            elif current_type == PYARROW_ARRAY_TYPE:
-                # Check if column is string. If so, convert to a list.
-                is_string_series = data[column].apply(lambda x: isinstance(x, str))
-                if is_string_series.any():
-                    data[column] = data[column].apply(literal_eval)
-        return data
-
-    def _read_batch(self, cursor: Cursor, schema: Schema, batch_size: int = 1000) -> pa.RecordBatch:
-        """
-        Fetch a batch of rows from a query result, returning them as a PyArrow record batch.
-
-        Parameters
-        ----------
-        cursor: Cursor
+        cursor: Any
             Cursor to fetch data from
-        schema: Schema
-            Schema of the data to fetch
-        batch_size: int
-            Number of rows to fetch at a time
 
         Returns
         -------
-        pa.RecordBatch
-            None if no more rows are available
-        """
-        results = cursor.fetchmany(batch_size)
-        # Process data to update types of certain columns based on their schema type
-        processed_data = self._process_batch_data(
-            pd.DataFrame(results if results else None, columns=schema.names), schema
-        )
-        return pa.record_batch(processed_data, schema=schema)
-
-    def fetchall_arrow(self, cursor: Cursor) -> pa.Table:
+        Schema
         """
-        Fetch all (remaining) rows of a query result, returning them as a PyArrow table.
-
-        Parameters
-        ----------
-        cursor: Cursor
-            Cursor to fetch data from
-
-        Returns
-        -------
-        pa.Table
-        """
-        schema = pa.schema(
-            {metadata[0]: self._get_pyarrow_type(metadata[1]) for metadata in cursor.description}
-        )
-        record_batches = []
-        while True:
-            record_batch = self._read_batch(cursor, schema)
-            record_batches.append(record_batch)
-            if record_batch.num_rows == 0:
-                break
-        return pa.Table.from_batches(record_batches)
+        fields = []
+        for row in cursor.description:
+            field_name = row[0]
+            field_type = "_".join(row[1].split("_")[:-1])
+            db_var_type = self._convert_to_internal_variable_type(field_type)
+            fields.append(
+                pa.field(
+                    field_name,
+                    self._get_pyarrow_type(field_type),
+                    metadata={ARROW_METADATA_DB_VAR_TYPE: db_var_type},
+                )
+            )
+        return pa.schema(fields)
 
     def fetch_query_result_impl(self, cursor: Any) -> pd.DataFrame | None:
-        arrow_table = self.fetchall_arrow(cursor)
-        return arrow_table.to_pandas()
+        schema = self._get_schema_from_cursor(cursor)
+        records = cursor.fetchall()
+        if not records:
+            return pa.record_batch([[]] * len(schema), schema=schema).to_pandas()
+        return pa.table(list(zip(*records)), schema=schema).to_pandas()
 
     async def fetch_query_stream_impl(self, cursor: Any) -> AsyncGenerator[pa.RecordBatch, None]:
         # fetch results in batches
-        schema = pa.schema(
-            {metadata[0]: self._get_pyarrow_type(metadata[1]) for metadata in cursor.description}
-        )
+        schema = self._get_schema_from_cursor(cursor)
         while True:
             try:
-                record_batch = self._read_batch(cursor, schema)
-            except TypeError:
+                records = cursor.fetchmany(1000)
+                if not records:
+                    # return empty table to ensure correct schema is returned
+                    yield pa.record_batch([[]] * len(schema), schema=schema)
+                    break
+                yield pa.record_batch(list(zip(*records)), schema=schema)
+            except TypeError as exc:
+                if isinstance(exc, ArrowTypeError):
+                    raise
                 # TypeError is raised on DDL queries in Spark 3.4 and above
                 break
-            yield record_batch
-            if record_batch.num_rows == 0:
-                break
```

### Comparing `featurebyte-1.0.2/featurebyte/session/webhdfs.py` & `featurebyte-1.0.3/featurebyte/session/webhdfs.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Customized smart_open WebHDFS client to support Kerberos authentication
 """
+
 from typing import Any, Dict, List
 
 from http import HTTPStatus
 
 import requests
 import requests_kerberos
 from smart_open.webhdfs import (
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/base.py` & `featurebyte-1.0.3/featurebyte/sql/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 """
 Base Class for SQL related operations
 """
+
 from typing import Any
 
 from pydantic.fields import PrivateAttr
 from pydantic.main import BaseModel
 
 from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
+from featurebyte.query_graph.sql.common import quoted_identifier, sql_to_string
 from featurebyte.session.base import BaseSession
 from featurebyte.session.snowflake import SnowflakeSession
 
 
 class BaseSqlModel(BaseModel):
     """
     Base class for Tile Operation Classes
@@ -52,18 +54,15 @@
         col_val: str
             input column name
 
         Returns
         -------
             quoted column name
         """
-        if isinstance(self._session, SnowflakeSession):
-            return f'"{col_val}"'
-
-        return f"`{col_val}`"
+        return sql_to_string(quoted_identifier(col_val), self._session.source_type)
 
     def quote_column_null_aware_equal(self, left_expr: str, right_expr: str) -> str:
         """
         Compares whether two expressions are null-safe equal
 
         Parameters
         ----------
@@ -90,14 +89,8 @@
         table_name: str
             input table name
 
         Returns
         -------
             True if table exists, False otherwise
         """
-        table_exist_flag = True
-        try:
-            await self._session.execute_query(f"select * from {table_name} limit 1")
-        except self._session._no_schema_error:  # pylint: disable=protected-access
-            table_exist_flag = False
-
-        return table_exist_flag
+        return await self._session.table_exists(table_name)
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_COSINE_SIMILARITY.sql` & `featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_COSINE_SIMILARITY.sql`

 * *Files 24% similar despite different names*

```diff
@@ -25,11 +25,13 @@
     if k in counts_other:
       v_other = counts_other[k] or 0
       dot_product += v * v_other
     norm += v * v
   for k, v in counts_other.items():
     v = v or 0
     norm_other += v * v
-  cosine_sim = dot_product / (math.sqrt(norm) * math.sqrt(norm_other))
-  return cosine_sim
+  norm_product = math.sqrt(norm) * math.sqrt(norm_other)
+  if norm_product == 0:
+    return 0
+  return dot_product / norm_product
 $$
 ;
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql` & `featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_GET_RANK.sql` & `featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_GET_RANK.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/databricks_unity/F_TIMEZONE_OFFSET_TO_SECOND.sql` & `featurebyte-1.0.3/featurebyte/sql/databricks_unity/F_TIMEZONE_OFFSET_TO_SECOND.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_COSINE_SIMILARITY.sql`

 * *Files 6% similar despite different names*

```diff
@@ -30,11 +30,14 @@
     }
     norm += v * v;
   }
   for (const k in counts_other) {
     var v = counts_other[k] || 0;
     norm_other += v * v;
   }
-  var cosine_sim = dot_product / (Math.sqrt(norm) * Math.sqrt(norm_other));
-  return cosine_sim;
+  var norm_product = Math.sqrt(norm) * Math.sqrt(norm_other);
+  if (norm_product == 0) {
+    return 0;
+  }
+  return dot_product / norm_product;
 $$
 ;
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_ENTROPY.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_ENTROPY.sql`

 * *Files 4% similar despite different names*

```diff
@@ -3,21 +3,21 @@
   LANGUAGE JAVASCRIPT
 AS
 $$
   if (!COUNTS) {
     return null;
   }
   var counts_arr = Object.values(COUNTS);
-  var total = counts_arr.reduce((partialSum, a) => partialSum + a, 0);
+  var total = counts_arr.reduce((partialSum, a) => partialSum + Math.abs(a), 0);
   var entropy = 0.0;
   var count_length = counts_arr.length;
   for (var i = 0; i < count_length; i++) {
     if (counts_arr[i] == 0) {
       continue;
     }
-    var p = counts_arr[i] / total;
+    var p = Math.abs(counts_arr[i]) / total;
     entropy += p * Math.log(p);
   }
   entropy *= -1.0;
   return entropy;
 $$
 ;
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_COUNT_DICT_MOST_FREQUENT_KEY_VALUE.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_GET_RANK.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_GET_RANK.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_INDEX_TO_TIMESTAMP.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_TIMESTAMP_TO_INDEX.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_TIMEZONE_OFFSET_TO_SECOND.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_AVG.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_AVG.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_MAX.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_MAX.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SIMPLE_AVERAGE.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SIMPLE_AVERAGE.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SUM.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_AGGREGATE_SUM.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/snowflake/F_VECTOR_COSINE_SIMILARITY.sql` & `featurebyte-1.0.3/featurebyte/sql/snowflake/F_VECTOR_COSINE_SIMILARITY.sql`

 * *Files identical despite different names*

### Comparing `featurebyte-1.0.2/featurebyte/sql/spark/featurebyte-hive-udf-1.0.6-SNAPSHOT-all.jar` & `featurebyte-1.0.3/featurebyte/sql/spark/featurebyte-hive-udf-1.0.8-SNAPSHOT-all.jar`

 * *Files 12% similar despite different names*

#### zipinfo {}

```diff
@@ -1,40 +1,40 @@
-Zip file size: 48457 bytes, number of entries: 38
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-15 07:45 META-INF/
--rw-r--r--  2.0 unx       25 b- defN 24-Mar-15 07:45 META-INF/MANIFEST.MF
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-15 07:45 com/
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-15 07:45 com/featurebyte/
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-15 07:45 com/featurebyte/hive/
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$AverageAggregationBuffer.class
--rw-r--r--  2.0 unx     2788 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/ObjectDeleteV1.class
--rw-r--r--  2.0 unx     4910 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/BaseVectorAggregateV1$VectorAggregateListEvaluator.class
--rw-r--r--  2.0 unx      852 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateMaxV1.class
--rw-r--r--  2.0 unx     3775 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictEntropyV2.class
--rw-r--r--  2.0 unx      917 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/UDFUtils.class
--rw-r--r--  2.0 unx     1968 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictMostFrequentV1.class
--rw-r--r--  2.0 unx     2034 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictMostFrequentValueV1.class
--rw-r--r--  2.0 unx     4552 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictUDFV1.class
--rw-r--r--  2.0 unx     5659 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorCosineSimilarityV1.class
--rw-r--r--  2.0 unx     6395 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictCosineSimilarityV1.class
--rw-r--r--  2.0 unx     3931 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictRelativeFrequencyV1.class
--rw-r--r--  2.0 unx      837 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateSumV1.class
--rw-r--r--  2.0 unx     2578 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictSingleStringArgumentUDFV1.class
--rw-r--r--  2.0 unx     4674 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateAverageV1.class
--rw-r--r--  2.0 unx      878 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAvgAggregationBuffer.class
--rw-r--r--  2.0 unx     2862 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictNumUniqueV1.class
--rw-r--r--  2.0 unx      814 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/BaseVectorAggregateV1$ListAggregationBuffer.class
--rw-r--r--  2.0 unx     1270 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/BaseVectorAggregateV1$1.class
--rw-r--r--  2.0 unx     1425 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateMaxV1$VectorAggregateMaxEvaluator.class
--rw-r--r--  2.0 unx     1042 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1.class
--rw-r--r--  2.0 unx     4172 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/BaseVectorAggregateV1.class
--rw-r--r--  2.0 unx     3775 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/TimezoneOffsetToSecondV1.class
--rw-r--r--  2.0 unx     5571 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/IndexToTimestampV1.class
--rw-r--r--  2.0 unx     3639 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictMostFrequentKeyValueV1.class
--rw-r--r--  2.0 unx     1357 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateSumV1$VectorAggregateSumEvaluator.class
--rw-r--r--  2.0 unx     5642 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictRankV1.class
--rw-r--r--  2.0 unx     6105 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAggregateAverageEvaluator.class
--rw-r--r--  2.0 unx     2024 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/CountDictLeastFrequentV1.class
--rw-r--r--  2.0 unx     1232 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateAverageV1$1.class
--rw-r--r--  2.0 unx     5050 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/TimestampToIndexV1.class
--rw-r--r--  2.0 unx     5522 b- defN 24-Mar-15 07:45 com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$VectorAggregateSimpleAverageEvaluator.class
-38 files, 99171 bytes uncompressed, 40711 bytes compressed:  58.9%
+Zip file size: 48525 bytes, number of entries: 38
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-21 11:12 META-INF/
+-rw-r--r--  2.0 unx       25 b- defN 24-May-21 11:12 META-INF/MANIFEST.MF
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-21 11:12 com/
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-21 11:12 com/featurebyte/
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-21 11:12 com/featurebyte/hive/
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/
+-rw-r--r--  2.0 unx      852 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateMaxV1.class
+-rw-r--r--  2.0 unx     3639 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictMostFrequentKeyValueV1.class
+-rw-r--r--  2.0 unx     3931 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictRelativeFrequencyV1.class
+-rw-r--r--  2.0 unx     2578 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictSingleStringArgumentUDFV1.class
+-rw-r--r--  2.0 unx      896 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$AverageAggregationBuffer.class
+-rw-r--r--  2.0 unx     2034 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictMostFrequentValueV1.class
+-rw-r--r--  2.0 unx      917 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/UDFUtils.class
+-rw-r--r--  2.0 unx     2788 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/ObjectDeleteV1.class
+-rw-r--r--  2.0 unx     3794 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictEntropyV3.class
+-rw-r--r--  2.0 unx     5571 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/IndexToTimestampV1.class
+-rw-r--r--  2.0 unx     5050 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/TimestampToIndexV1.class
+-rw-r--r--  2.0 unx     1968 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictMostFrequentV1.class
+-rw-r--r--  2.0 unx      814 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/BaseVectorAggregateV1$ListAggregationBuffer.class
+-rw-r--r--  2.0 unx      878 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAvgAggregationBuffer.class
+-rw-r--r--  2.0 unx     5522 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$VectorAggregateSimpleAverageEvaluator.class
+-rw-r--r--  2.0 unx     1232 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateAverageV1$1.class
+-rw-r--r--  2.0 unx     6105 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAggregateAverageEvaluator.class
+-rw-r--r--  2.0 unx     4910 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/BaseVectorAggregateV1$VectorAggregateListEvaluator.class
+-rw-r--r--  2.0 unx     4172 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/BaseVectorAggregateV1.class
+-rw-r--r--  2.0 unx     4552 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictUDFV1.class
+-rw-r--r--  2.0 unx     1357 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateSumV1$VectorAggregateSumEvaluator.class
+-rw-r--r--  2.0 unx     5659 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorCosineSimilarityV1.class
+-rw-r--r--  2.0 unx     5642 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictRankV1.class
+-rw-r--r--  2.0 unx      837 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateSumV1.class
+-rw-r--r--  2.0 unx     1425 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateMaxV1$VectorAggregateMaxEvaluator.class
+-rw-r--r--  2.0 unx     4674 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateAverageV1.class
+-rw-r--r--  2.0 unx     2024 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictLeastFrequentV1.class
+-rw-r--r--  2.0 unx     2862 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictNumUniqueV1.class
+-rw-r--r--  2.0 unx     3775 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/TimezoneOffsetToSecondV1.class
+-rw-r--r--  2.0 unx     1270 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/BaseVectorAggregateV1$1.class
+-rw-r--r--  2.0 unx     1042 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1.class
+-rw-r--r--  2.0 unx     6454 b- defN 24-May-21 11:12 com/featurebyte/hive/udf/CountDictCosineSimilarityV2.class
+38 files, 99249 bytes uncompressed, 40779 bytes compressed:  58.9%
```

#### zipnote «TEMP»/diffoscope_0xh6znzj_/tmp4hh1g_n__.zip

```diff
@@ -12,104 +12,104 @@
 
 Filename: com/featurebyte/hive/
 Comment: 
 
 Filename: com/featurebyte/hive/udf/
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$AverageAggregationBuffer.class
+Filename: com/featurebyte/hive/udf/VectorAggregateMaxV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/ObjectDeleteV1.class
+Filename: com/featurebyte/hive/udf/CountDictMostFrequentKeyValueV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1$VectorAggregateListEvaluator.class
+Filename: com/featurebyte/hive/udf/CountDictRelativeFrequencyV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateMaxV1.class
+Filename: com/featurebyte/hive/udf/CountDictSingleStringArgumentUDFV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictEntropyV2.class
+Filename: com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$AverageAggregationBuffer.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/UDFUtils.class
+Filename: com/featurebyte/hive/udf/CountDictMostFrequentValueV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictMostFrequentV1.class
+Filename: com/featurebyte/hive/udf/UDFUtils.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictMostFrequentValueV1.class
+Filename: com/featurebyte/hive/udf/ObjectDeleteV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictUDFV1.class
+Filename: com/featurebyte/hive/udf/CountDictEntropyV3.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorCosineSimilarityV1.class
+Filename: com/featurebyte/hive/udf/IndexToTimestampV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictCosineSimilarityV1.class
+Filename: com/featurebyte/hive/udf/TimestampToIndexV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictRelativeFrequencyV1.class
+Filename: com/featurebyte/hive/udf/CountDictMostFrequentV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateSumV1.class
+Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1$ListAggregationBuffer.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictSingleStringArgumentUDFV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAvgAggregationBuffer.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$VectorAggregateSimpleAverageEvaluator.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAvgAggregationBuffer.class
+Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1$1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictNumUniqueV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAggregateAverageEvaluator.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1$ListAggregationBuffer.class
+Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1$VectorAggregateListEvaluator.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1$1.class
+Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateMaxV1$VectorAggregateMaxEvaluator.class
+Filename: com/featurebyte/hive/udf/CountDictUDFV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateSumV1$VectorAggregateSumEvaluator.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1.class
+Filename: com/featurebyte/hive/udf/VectorCosineSimilarityV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/TimezoneOffsetToSecondV1.class
+Filename: com/featurebyte/hive/udf/CountDictRankV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/IndexToTimestampV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateSumV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictMostFrequentKeyValueV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateMaxV1$VectorAggregateMaxEvaluator.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateSumV1$VectorAggregateSumEvaluator.class
+Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictRankV1.class
+Filename: com/featurebyte/hive/udf/CountDictLeastFrequentV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1$VectorAggregateAverageEvaluator.class
+Filename: com/featurebyte/hive/udf/CountDictNumUniqueV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/CountDictLeastFrequentV1.class
+Filename: com/featurebyte/hive/udf/TimezoneOffsetToSecondV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateAverageV1$1.class
+Filename: com/featurebyte/hive/udf/BaseVectorAggregateV1$1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/TimestampToIndexV1.class
+Filename: com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1.class
 Comment: 
 
-Filename: com/featurebyte/hive/udf/VectorAggregateSimpleAverageV1$VectorAggregateSimpleAverageEvaluator.class
+Filename: com/featurebyte/hive/udf/CountDictCosineSimilarityV2.class
 Comment: 
 
 Zip file comment:
```

#### Comparing `com/featurebyte/hive/udf/CountDictEntropyV2.class` & `com/featurebyte/hive/udf/CountDictEntropyV3.class`

 * *Files 6% similar despite different names*

##### procyon -ec {}

```diff
@@ -8,19 +8,19 @@
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.ql.exec.Description;
 
 @Description(name = "F_COUNT_DICT_ENTROPY", value = "_FUNC_(counts) - compute entropy value from count dictionary")
-public class CountDictEntropyV2 extends CountDictUDFV1
+public class CountDictEntropyV3 extends CountDictUDFV1
 {
     private final DoubleWritable output;
     
-    public CountDictEntropyV2() {
+    public CountDictEntropyV3() {
         this.output = new DoubleWritable();
     }
     
     public ObjectInspector initialize(final ObjectInspector[] arguments) throws UDFArgumentException {
         this.checkArgsSize(arguments, 1, 1);
         if (UDFUtils.isNullOI(arguments[0])) {
             return (ObjectInspector)UDFUtils.nullOI;
@@ -35,15 +35,15 @@
         }
         final Map<String, Object> counts = this.inputMapOI.getMap(arguments[0].get());
         int index = 0;
         double total = 0.0;
         final double[] values = new double[counts.size()];
         for (final Object value : counts.values()) {
             if (value != null) {
-                final double doubleValue = this.convertMapValueAsDouble(value);
+                final double doubleValue = Math.abs(this.convertMapValueAsDouble(value));
                 if (Double.isNaN(doubleValue)) {
                     continue;
                 }
                 if (doubleValue == 0.0) {
                     continue;
                 }
                 total += doubleValue;
```

#### Comparing `com/featurebyte/hive/udf/CountDictCosineSimilarityV1.class` & `com/featurebyte/hive/udf/CountDictCosineSimilarityV2.class`

 * *Files 8% similar despite different names*

##### procyon -ec {}

```diff
@@ -13,22 +13,22 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.ql.exec.Description;
 
 @Description(name = "F_COUNT_COSINE_SIMILARITY", value = "_FUNC_(counts) - compute cosine similarity between two count dictionaries")
-public class CountDictCosineSimilarityV1 extends CountDictUDFV1
+public class CountDictCosineSimilarityV2 extends CountDictUDFV1
 {
     private final DoubleWritable output;
     private transient MapObjectInspector otherInputMapOI;
     private final transient PrimitiveObjectInspector.PrimitiveCategory[] otherInputTypes;
     private final transient ObjectInspectorConverters.Converter[] otherConverters;
     
-    public CountDictCosineSimilarityV1() {
+    public CountDictCosineSimilarityV2() {
         this.output = new DoubleWritable();
         this.otherInputTypes = new PrimitiveObjectInspector.PrimitiveCategory[2];
         this.otherConverters = new ObjectInspectorConverters.Converter[2];
     }
     
     public ObjectInspector initialize(final ObjectInspector[] arguments) throws UDFArgumentException {
         this.checkArgsSize(arguments, 2, 2);
@@ -103,15 +103,21 @@
                 value2 = count2.get();
                 if (Double.isNaN(value2)) {
                     value2 = 0.0;
                 }
             }
             normOther += value2 * value2;
         }
-        this.output.set(dotProduct / (Math.sqrt(norm) * Math.sqrt(normOther)));
+        final double normProduct = Math.sqrt(norm) * Math.sqrt(normOther);
+        if (normProduct == 0.0) {
+            this.output.set(0.0);
+        }
+        else {
+            this.output.set(dotProduct / normProduct);
+        }
         return this.output;
     }
     
     public String getDisplayString(final String[] children) {
         return "F_COUNT_DICT_COSINE_SIMILARITY";
     }
 }
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/tile_common.py` & `featurebyte-1.0.3/featurebyte/sql/tile_common.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base Class for Tile Schedule Instance
 """
+
 from typing import Any
 
 from abc import ABC, abstractmethod
 
 from featurebyte.models.tile import TileCommonParameters
 from featurebyte.session.base import BaseSession
 from featurebyte.sql.base import BaseSqlModel
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/tile_generate.py` & `featurebyte-1.0.3/featurebyte/sql/tile_generate.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 """
 Databricks Tile Generate Job Script
 """
+
 from typing import Optional
 
 import dateutil.parser
 
 from featurebyte.common import date_util
 from featurebyte.logging import get_logger
 from featurebyte.models.tile import TileType
 from featurebyte.service.tile_registry_service import TileRegistryService
-from featurebyte.sql.common import construct_create_table_query, retry_sql
 from featurebyte.sql.tile_common import TileCommon
 from featurebyte.sql.tile_registry import TileRegistry
 
 logger = get_logger(__name__)
 
 
 class TileGenerate(TileCommon):
@@ -73,16 +73,19 @@
 
         value_insert_cols_str = ",".join(value_insert_cols)
         value_update_cols_str = ",".join(value_update_cols)
 
         # insert new records and update existing records
         if not tile_table_exist_flag:
             logger.debug(f"creating tile table: {self.tile_id}")
-            create_sql = construct_create_table_query(self.tile_id, tile_sql, session=self._session)
-            await retry_sql(self._session, create_sql)
+            await self._session.create_table_as(
+                table_details=self.tile_id,
+                select_expr=tile_sql,
+                retry=True,
+            )
             logger.debug(f"done creating table: {self.tile_id}")
         else:
             logger.debug("merging into tile table", extra={"tile_id": self.tile_id})
             if self.entity_column_names:
                 on_condition_str = f"a.INDEX = b.INDEX AND {entity_filter_cols_str}"
                 insert_str = f"INDEX, {self.entity_column_names_str}, {self.value_column_names_str}, CREATED_AT"
                 values_str = f"b.INDEX, {entity_insert_cols_str}, {value_insert_cols_str}, current_timestamp()"
@@ -96,15 +99,15 @@
                     on {on_condition_str}
                     when matched then
                         update set a.created_at = current_timestamp(), {value_update_cols_str}
                     when not matched then
                         insert ({insert_str})
                             values ({values_str})
             """
-            await retry_sql(session=self._session, sql=merge_sql)
+            await self._session.retry_sql(sql=merge_sql)
 
         if self.last_tile_start_str:
             ind_value = date_util.timestamp_utc_to_tile_index(
                 dateutil.parser.isoparse(self.last_tile_start_str),
                 self.time_modulo_frequency_second,
                 self.blind_spot_second,
                 self.frequency_minute,
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/tile_generate_entity_tracking.py` & `featurebyte-1.0.3/featurebyte/sql/tile_generate_entity_tracking.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 """
 Tile Generate entity tracking Job script
 """
+
 from typing import Any, List
 
 from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
 from featurebyte.session.base import BaseSession
 from featurebyte.sql.base import BaseSqlModel
-from featurebyte.sql.common import construct_create_table_query, retry_sql
 
 logger = get_logger(__name__)
 
 
 class TileGenerateEntityTracking(BaseSqlModel):
     """
     Tile Generate entity tracking script
@@ -62,18 +62,19 @@
             cols = ", ".join(
                 [
                     self.quote_column(col)
                     for col in self.entity_column_names + [tile_last_start_date_column]
                 ]
             )
             entity_table = f"select {cols} from ({self.entity_table})"
-            create_sql = construct_create_table_query(
-                tracking_table_name, entity_table, session=self._session
+            await self._session.create_table_as(
+                table_details=tracking_table_name,
+                select_expr=entity_table,
+                retry=True,
             )
-            await retry_sql(self._session, create_sql)
         else:
             if self.entity_column_names:
                 entity_insert_cols_str = ", ".join(entity_insert_cols)
                 entity_filter_cols_str = " AND ".join(entity_filter_cols)
                 merge_sql = f"""
                     merge into {tracking_table_name} a using ({self.entity_table}) b
                         on {entity_filter_cols_str}
@@ -89,8 +90,8 @@
                         on true
                         when matched then
                             update set a.{tile_last_start_date_column} = b.{tile_last_start_date_column}
                         when not matched then
                             insert ({tile_last_start_date_column})
                                 values (b.{tile_last_start_date_column})
                 """
-            await retry_sql(session=self._session, sql=merge_sql)
+            await self._session.retry_sql(merge_sql)
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/tile_monitor.py` & `featurebyte-1.0.3/featurebyte/sql/tile_monitor.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 """
 Tile Monitor Job
 """
+
 import os
 
 from sqlglot import expressions
 
 from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import get_qualified_column_identifier, sql_to_string
 from featurebyte.service.tile_registry_service import TileRegistryService
-from featurebyte.sql.common import construct_create_table_query, retry_sql
 from featurebyte.sql.tile_common import TileCommon
 from featurebyte.sql.tile_registry import TileRegistry
 
 logger = get_logger(__name__)
 
 
 class TileMonitor(TileCommon):
@@ -117,18 +117,19 @@
             """
 
             monitor_table_name = f"{self.tile_id}_MONITOR"
             tile_monitor_exist_flag = await self.table_exists(monitor_table_name)
             logger.debug(f"tile_monitor_exist_flag: {tile_monitor_exist_flag}")
 
             if not tile_monitor_exist_flag:
-                create_sql = construct_create_table_query(
-                    monitor_table_name, compare_sql, session=self._session
+                await self._session.create_table_as(
+                    table_details=monitor_table_name,
+                    select_expr=compare_sql,
+                    retry=True,
                 )
-                await retry_sql(self._session, create_sql)
             else:
                 tile_registry_ins = TileRegistry(
                     session=self._session,
                     sql=tile_sql,
                     table_name=monitor_table_name,
                     table_exist=True,
                     time_modulo_frequency_second=self.time_modulo_frequency_second,
@@ -181,19 +182,19 @@
                             {value_insert_cols_str},
                             {old_value_insert_cols_str},
                             b.TILE_TYPE,
                             b.EXPECTED_CREATED_AT,
                             b.CREATED_AT
                         )
                 """
-                await retry_sql(session=self._session, sql=insert_sql)
+                await self._session.retry_sql(insert_sql)
 
             insert_monitor_summary_sql = f"""
                 INSERT INTO TILE_MONITOR_SUMMARY(TILE_ID, TILE_START_DATE, TILE_TYPE, CREATED_AT)
                 SELECT
                     '{self.tile_id}' as TILE_ID,
                     {InternalName.TILE_START_DATE} as TILE_START_DATE,
                     TILE_TYPE,
                     current_timestamp()
                 FROM ({compare_sql})
             """
-            await retry_sql(self._session, insert_monitor_summary_sql)
+            await self._session.retry_sql(insert_monitor_summary_sql)
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/tile_registry.py` & `featurebyte-1.0.3/featurebyte/sql/tile_registry.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """
 Tile Registry Job Script
 """
+
 from featurebyte.logging import get_logger
 from featurebyte.models.tile_registry import TileModel
 from featurebyte.service.tile_registry_service import TileRegistryService
-from featurebyte.sql.common import retry_sql
 from featurebyte.sql.tile_common import TileCommon
 
 logger = get_logger(__name__)
 
+LIST_TABLE_SCHEMA_TIMEOUT_SECONDS = 10 * 60
+
 
 class TileRegistry(TileCommon):
     """
     Tile Registry script
     """
 
     table_name: str
@@ -51,15 +53,18 @@
             await self.tile_registry_service.create_document(tile_model)
 
         if self.table_exist:
             cols = [
                 c.upper()
                 for c in (
                     await self._session.list_table_schema(
-                        self.table_name, self._session.database_name, self._session.schema_name
+                        self.table_name,
+                        self._session.database_name,
+                        self._session.schema_name,
+                        timeout=LIST_TABLE_SCHEMA_TIMEOUT_SECONDS,
                     )
                 ).keys()
             ]
             tile_add_sql = f"ALTER TABLE {self.table_name} ADD COLUMN\n"
             add_statements = []
             for i, input_column in enumerate(input_value_columns):
                 if input_column.upper() not in cols:
@@ -67,8 +72,8 @@
                     add_statements.append(f"{input_column} {element_type}")
                     if "_MONITOR" in self.table_name:
                         add_statements.append(f"OLD_{input_column} {element_type}")
 
             if add_statements:
                 tile_add_sql += ",\n".join(add_statements)
                 logger.debug(f"tile_add_sql: {tile_add_sql}")
-                await retry_sql(self._session, tile_add_sql)
+                await self._session.retry_sql(tile_add_sql)
```

### Comparing `featurebyte-1.0.2/featurebyte/sql/tile_schedule_online_store.py` & `featurebyte-1.0.3/featurebyte/sql/tile_schedule_online_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 """
 Tile Generate online store Job Script
 """
+
 from typing import List, Optional
 
 import textwrap
 from datetime import datetime
 
 from pydantic import Field
 
 from featurebyte.enum import InternalName
 from featurebyte.logging import get_logger
 from featurebyte.models.online_store_compute_query import OnlineStoreComputeQueryModel
 from featurebyte.models.online_store_table_version import OnlineStoreTableVersion
 from featurebyte.service.online_store_compute_query_service import OnlineStoreComputeQueryService
 from featurebyte.service.online_store_table_version import OnlineStoreTableVersionService
 from featurebyte.sql.base import BaseSqlModel
-from featurebyte.sql.common import construct_create_table_query, register_temporary_physical_table
+from featurebyte.sql.common import register_temporary_physical_table
 
 logger = get_logger(__name__)
 
 
 class TileScheduleOnlineStore(BaseSqlModel):
     """
     Tile Schedule Online Store script
@@ -94,22 +95,19 @@
                       CAST({quoted_result_name_column} AS STRING) AS {quoted_result_name_column},
                       CAST({quoted_value_column} AS {col_type}) AS {quoted_value_column},
                       CAST({next_version} AS INT) AS {quoted_version_column},
                       to_timestamp('{current_ts}') AS UPDATED_AT
                     FROM ({f_sql})
                     """
                 )
-                create_sql = construct_create_table_query(
-                    fs_table,
-                    query,
-                    session=self._session,
-                    partition_keys=quoted_result_name_column,
+                await self._session.create_table_as(
+                    table_details=fs_table,
+                    select_expr=query,
+                    partition_keys=[InternalName.ONLINE_STORE_RESULT_NAME_COLUMN],
                 )
-                await self._session.execute_query_long_running(create_sql)
-
             else:
                 # feature store table already exists, insert records with the input feature sql
                 column_names = ", ".join(
                     quoted_entity_columns + [quoted_result_name_column, quoted_value_column]
                 )
                 async with register_temporary_physical_table(self._session, f_sql) as temp_table:
                     insert_query = textwrap.dedent(
```

### Comparing `featurebyte-1.0.2/featurebyte/storage/azure.py` & `featurebyte-1.0.3/featurebyte/storage/azure.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Azure Storage Blob Class
 """
+
 from __future__ import annotations
 
 from typing import Any, AsyncGenerator
 
 from pathlib import Path
 
 import aiofiles
@@ -90,15 +91,15 @@
         async with self.get_client() as client:
             try:
                 remote_path = self.prefix.joinpath(remote_path)
                 await client.delete_blob(str(remote_path), delete_snapshots="include")
             except ResourceNotFoundError as exc:
                 raise FileNotFoundError("Remote file does not exist") from exc
 
-    async def get(self, remote_path: Path, local_path: Path) -> None:
+    async def _get(self, remote_path: Path, local_path: Path) -> None:
         """
         Download file from storage to local path
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
```

### Comparing `featurebyte-1.0.2/featurebyte/storage/base.py` & `featurebyte-1.0.3/featurebyte/storage/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,29 +1,111 @@
 """
 Storage base class
 """
+
 from __future__ import annotations
 
-from typing import Any, AsyncGenerator
+from typing import Any, AsyncGenerator, Optional
 
+import asyncio
 import json
+import os
+import shutil
+import tempfile
 from abc import ABC, abstractmethod
 from pathlib import Path
 
 import aiofiles
 import pandas as pd
 from pandas import DataFrame
 from pydantic import BaseModel
 
 
+class StorageCache:
+    """Storage cache class"""
+
+    _cache_lock: Any = asyncio.Lock()
+
+    def __init__(self) -> None:
+        self._cache_dir = tempfile.mkdtemp()
+
+    def __del__(self) -> None:
+        if os.path.exists(self._cache_dir):
+            shutil.rmtree(self._cache_dir)
+
+    def clear(self) -> None:
+        """
+        Clear cache
+        """
+        shutil.rmtree(self._cache_dir)
+        os.makedirs(self._cache_dir)
+
+    def exists(self, cache_key: str) -> bool:
+        """
+        Check if cache key exists
+
+        Parameters
+        ----------
+        cache_key: str
+            Cache key
+
+        Returns
+        -------
+        bool
+            True if cache key exists, False otherwise
+        """
+        cache_path = Path(self._cache_dir) / cache_key
+        return cache_path.exists()
+
+    async def write_to_cache(self, cache_key: str, local_path: Path) -> None:
+        """
+        Write file to cache
+
+        Parameters
+        ----------
+        cache_key: str
+            Cache key
+        local_path: Path
+            Local path to store file
+        """
+        cache_path = Path(self._cache_dir) / cache_key
+        async with self._cache_lock:
+            cache_path.parent.mkdir(parents=True, exist_ok=True)
+            shutil.copy(local_path, cache_path)
+
+    async def write_from_cache(self, cache_key: str, local_path: Path) -> bool:
+        """
+        Read file from cache
+
+        Parameters
+        ----------
+        cache_key: str
+            Cache key
+        local_path: Path
+            Local path to store file
+
+        Returns
+        -------
+        bool
+            True if cache hit, False otherwise
+        """
+        cache_path = Path(self._cache_dir) / cache_key
+        if cache_path.exists():
+            shutil.copy(cache_path, local_path)
+            return True
+        return False
+
+
 class Storage(ABC):
     """
     Base storage class
     """
 
+    _cache: Any = StorageCache()
+
     @abstractmethod
     async def put(self, local_path: Path, remote_path: Path) -> None:
         """
         Upload local file to storage
 
         Parameters
         ----------
@@ -63,30 +145,52 @@
             Path of remote file to be deleted
         """
         try:
             await self.delete(remote_path)
         except FileNotFoundError:
             pass
 
-    @abstractmethod
-    async def get(self, remote_path: Path, local_path: Path) -> None:
+    async def get(
+        self, remote_path: Path, local_path: Path, cache_key: Optional[str] = None
+    ) -> None:
         """
-        Download file from storage to local path
+        Download file from storage to local path with caching if cache_key is provided
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
         local_path: Path
             Path to stored downloaded file
+        cache_key: Optional[str]
+            Cache key for storing downloaded file (if provided, the result will be cached).
 
-        Raises
-        ------
-        FileNotFoundError
-            Remote file does not exist
+        Returns
+        -------
+        None
+        """
+        if cache_key is None:
+            return await self._get(remote_path, local_path)
+
+        cache_hit = await self._cache.write_from_cache(cache_key, local_path)
+        if not cache_hit:
+            await self._get(remote_path, local_path)
+            await self._cache.write_to_cache(cache_key, local_path)
+
+    @abstractmethod
+    async def _get(self, remote_path: Path, local_path: Path) -> None:
+        """
+        Retrieve file from storage to local path
+
+        Parameters
+        ----------
+        remote_path: Path
+            Path of remote file to be downloaded
+        local_path: Path
+            Path to stored downloaded file
         """
 
     @abstractmethod
     async def get_file_stream(
         self, remote_path: Path, chunk_size: int = 255 * 1024
     ) -> AsyncGenerator[bytes, None]:
         """
@@ -147,30 +251,32 @@
             Path of remote file to upload to
         """
         async with aiofiles.tempfile.NamedTemporaryFile(mode="w") as file_obj:
             await file_obj.write(text)
             await file_obj.flush()
             await self.put(Path(str(file_obj.name)), remote_path)
 
-    async def get_text(self, remote_path: Path) -> str:
+    async def get_text(self, remote_path: Path, cache_key: Optional[str] = None) -> str:
         """
         Download text content from storage text file
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
+        cache_key: Optional[str]
+            Cache key for storing downloaded file (if provided, the result will be cached)
 
         Returns
         -------
         str
             Text data
         """
         async with aiofiles.tempfile.NamedTemporaryFile(mode="r") as tmp_file:
-            await self.get(remote_path, Path(str(tmp_file.name)))
+            await self.get(remote_path, Path(str(tmp_file.name)), cache_key=cache_key)
             async with aiofiles.open(tmp_file.name, encoding="utf8") as file_obj:
                 text = await file_obj.read()
             return text
 
     async def put_bytes(self, content: bytes, remote_path: Path) -> None:
         """
         Upload bytes content to storage as bytes file
@@ -183,30 +289,32 @@
             Path of remote file to upload to
         """
         async with aiofiles.tempfile.NamedTemporaryFile(mode="w+b") as file_obj:
             await file_obj.write(content)
             await file_obj.flush()
             await self.put(Path(str(file_obj.name)), remote_path)
 
-    async def get_bytes(self, remote_path: Path) -> bytes:
+    async def get_bytes(self, remote_path: Path, cache_key: Any = None) -> bytes:
         """
         Download bytes content from storage bytes file
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
+        cache_key: Any
+            Cache key for storing downloaded file (if provided, the result will be cached)
 
         Returns
         -------
         bytes
             Bytes data
         """
         async with aiofiles.tempfile.NamedTemporaryFile(mode="r+b") as tmp_file:
-            await self.get(remote_path, Path(str(tmp_file.name)))
+            await self.get(remote_path, Path(str(tmp_file.name)), cache_key=cache_key)
             async with aiofiles.open(tmp_file.name, mode="r+b") as file_obj:
                 content = await file_obj.read()
             return content
 
     async def put_dataframe(self, dataframe: DataFrame, remote_path: Path) -> None:
         """
         Upload dataframe to storage as parquet file
@@ -218,24 +326,26 @@
         remote_path: Path
             Path of remote file to upload to
         """
         async with aiofiles.tempfile.NamedTemporaryFile() as file_obj:
             dataframe.to_parquet(file_obj.name)
             await self.put(Path(str(file_obj.name)), remote_path)
 
-    async def get_dataframe(self, remote_path: Path) -> DataFrame:
+    async def get_dataframe(self, remote_path: Path, cache_key: Any = None) -> DataFrame:
         """
         Download dataframe from storage parquet file
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
+        cache_key: Any
+            Cache key for storing downloaded file (if provided, the result will be cached)
 
         Returns
         -------
         DataFrame
             Pandas DataFrame object
         """
         async with aiofiles.tempfile.NamedTemporaryFile(mode="r") as file_obj:
-            await self.get(remote_path, Path(str(file_obj.name)))
+            await self.get(remote_path, Path(str(file_obj.name)), cache_key=cache_key)
             return pd.read_parquet(file_obj.name)
```

### Comparing `featurebyte-1.0.2/featurebyte/storage/local.py` & `featurebyte-1.0.3/featurebyte/storage/local.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Local storage class
 """
+
 from typing import AsyncGenerator
 
 import shutil
 from pathlib import Path
 
 import aiofiles
 from aiofiles import os as async_os
@@ -62,15 +63,15 @@
         destination_path = self._base_path.joinpath(remote_path)
         if destination_path.exists():
             raise FileExistsError("File already exists on remote path")
 
         destination_path.parent.mkdir(parents=True, exist_ok=True)
         shutil.copy(local_path, destination_path)
 
-    async def get(self, remote_path: Path, local_path: Path) -> None:
+    async def _get(self, remote_path: Path, local_path: Path) -> None:
         """
         Download file from storage to local path
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
```

### Comparing `featurebyte-1.0.2/featurebyte/storage/s3.py` & `featurebyte-1.0.3/featurebyte/storage/s3.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 S3 Storage Class
 """
+
 from __future__ import annotations
 
 from typing import Any, AsyncGenerator
 
 from pathlib import Path
 
 import aiofiles
@@ -108,15 +109,15 @@
             try:
                 await client.head_object(Bucket=self.bucket_name, Key=str(remote_path))
             except ClientError as exc:
                 raise FileNotFoundError("Remote file does not exist") from exc
 
             await client.delete_object(Bucket=self.bucket_name, Key=str(remote_path))
 
-    async def get(self, remote_path: Path, local_path: Path) -> None:
+    async def _get(self, remote_path: Path, local_path: Path) -> None:
         """
         Download file from storage to local path
 
         Parameters
         ----------
         remote_path: Path
             Path of remote file to be downloaded
```

### Comparing `featurebyte-1.0.2/featurebyte/tile/tile_cache.py` & `featurebyte-1.0.3/featurebyte/tile/tile_cache.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 """
 Module for TileCache and its implementors
 """
+
 from __future__ import annotations
 
-from typing import Any, Callable, Coroutine, Optional, cast
+from typing import Any, Callable, Coroutine, Iterator, Optional, cast
 
 import time
 from dataclasses import dataclass
 
 from bson import ObjectId
 from sqlglot import expressions, parse_one
 from sqlglot.expressions import Expression, select
 
+from featurebyte.common.progress import divide_progress_callback
 from featurebyte.enum import InternalName, SourceType, SpecialColumnName
 from featurebyte.logging import get_logger
 from featurebyte.models.tile import TileSpec
 from featurebyte.query_graph.graph import QueryGraph
 from featurebyte.query_graph.node import Node
+from featurebyte.query_graph.node.schema import TableDetails
 from featurebyte.query_graph.sql.adapter import BaseAdapter, get_sql_adapter
 from featurebyte.query_graph.sql.ast.datetime import TimedeltaExtractNode
 from featurebyte.query_graph.sql.ast.literal import make_literal_value
 from featurebyte.query_graph.sql.common import (
     apply_serving_names_mapping,
     quoted_identifier,
     sql_to_string,
@@ -29,17 +32,21 @@
 from featurebyte.query_graph.sql.tile_util import (
     construct_entity_table_query,
     get_earliest_tile_start_date_expr,
     get_previous_job_epoch_expr,
 )
 from featurebyte.service.tile_manager import TileManagerService
 from featurebyte.session.base import BaseSession
+from featurebyte.session.session_helper import run_coroutines
 
 logger = get_logger(__name__)
 
+TILE_CACHE_LIST_TABLES_TIMEOUT_SECONDS = 60 * 10
+NUM_TRACKER_TABLES_PER_QUERY = 20
+
 
 @dataclass(frozen=True)
 class TileInfoKey:
     """
     Represents a unique unit of work for tile cache check
     """
 
@@ -89,14 +96,67 @@
         -------
         str
         """
         return f"{self.aggregation_id}_v{self.tile_id_version}"
 
 
 @dataclass
+class TileCacheStatus:
+    """
+    Represents the tile cache status derived for a query graph
+    """
+
+    unique_tile_infos: dict[TileInfoKey, TileGenSql]
+    keys_with_tracker: list[TileInfoKey]
+
+    def subset(self, keys: list[TileInfoKey]) -> TileCacheStatus:
+        """
+        Create a new TileCacheStatus by selecting a subset of keys
+
+        Parameters
+        ----------
+        keys: list[TileInfoKey]
+            List of keys to select
+
+        Returns
+        -------
+        TileCacheStatus
+        """
+        subset_unique_unique_tile_infos = {key: self.unique_tile_infos[key] for key in keys}
+        keys_with_tracker_set = set(self.keys_with_tracker)
+        subset_keys_with_tracker = [key for key in keys if key in keys_with_tracker_set]
+        return TileCacheStatus(
+            unique_tile_infos=subset_unique_unique_tile_infos,
+            keys_with_tracker=subset_keys_with_tracker,
+        )
+
+    def split_batches(self, batch_size: Optional[int] = None) -> Iterator[TileCacheStatus]:
+        """
+        Split TileCacheStatus in batches to be processed in parallel
+
+        Parameters
+        ----------
+        batch_size: Optional[int]
+            Number of entity tracker tables to check per batch
+
+        Yields
+        ------
+        TileCacheStatus
+            New TileCacheStatus objects
+        """
+        if batch_size is None:
+            batch_size = NUM_TRACKER_TABLES_PER_QUERY
+
+        all_keys = list(self.unique_tile_infos.keys())
+        for i in range(0, len(all_keys), batch_size):
+            keys = list(key for key in all_keys[i : i + batch_size])
+            yield self.subset(keys)
+
+
+@dataclass
 class OnDemandTileComputeRequest:
     """Information required to compute and update a single tile table"""
 
     tile_table_id: str
     aggregation_id: str
     tracker_sql: str
     tile_compute_sql: str
@@ -127,20 +187,30 @@
             entity_column_names=entity_column_names,
             value_column_names=self.tile_gen_info.tile_value_columns,
             value_column_types=self.tile_gen_info.tile_value_types,
             tile_id=self.tile_table_id,
             aggregation_id=self.aggregation_id,
             category_column_name=self.tile_gen_info.value_by_column,
             feature_store_id=feature_store_id,
-            entity_tracker_table_name=TileInfoKey.from_tile_info(
-                self.tile_gen_info
-            ).get_entity_tracker_table_name(),
+            entity_tracker_table_name=self.tile_info_key.get_entity_tracker_table_name(),
+            windows=self.tile_gen_info.windows,
         )
         return tile_spec, self.tracker_sql
 
+    @property
+    def tile_info_key(self) -> TileInfoKey:
+        """
+        Returns a TileInfoKey object to uniquely identify a unit of tile compute work
+
+        Returns
+        -------
+        TileInfoKey
+        """
+        return TileInfoKey.from_tile_info(self.tile_gen_info)
+
 
 class TileCache:
     """Responsible for on-demand tile computation for historical features
 
     Parameters
     ----------
     session : BaseSession
@@ -176,67 +246,14 @@
 
         Returns
         -------
         SourceType
         """
         return self.session.source_type
 
-    async def compute_tiles_on_demand(
-        self,
-        graph: QueryGraph,
-        nodes: list[Node],
-        request_id: str,
-        request_table_name: str,
-        serving_names_mapping: dict[str, str] | None = None,
-        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
-    ) -> None:
-        """Check tile status for the provided features and compute missing tiles if required
-
-        Parameters
-        ----------
-        graph : QueryGraph
-            Query graph
-        nodes : list[Node]
-            List of query graph node
-        request_id : str
-            Request ID
-        request_table_name: str
-            Request table name to use
-        serving_names_mapping : dict[str, str] | None
-            Optional mapping from original serving name to new serving name
-        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]]
-            Optional progress callback function
-        """
-        tic = time.time()
-
-        if progress_callback is not None:
-            await progress_callback(0, "Checking tile status")
-
-        required_requests = await self.get_required_computation(
-            request_id=request_id,
-            graph=graph,
-            nodes=nodes,
-            request_table_name=request_table_name,
-            serving_names_mapping=serving_names_mapping,
-        )
-        elapsed = time.time() - tic
-        logger.debug(
-            f"Getting required tiles computation took {elapsed:.2f}s ({len(required_requests)})"
-        )
-
-        if required_requests:
-            tic = time.time()
-            await self.invoke_tile_manager(required_requests, progress_callback=progress_callback)
-            elapsed = time.time() - tic
-            logger.debug(f"Compute tiles on demand took {elapsed:.2f}s")
-        else:
-            logger.debug("All required tiles can be reused")
-
-        await self.cleanup_temp_tables()
-
     async def invoke_tile_manager(
         self,
         required_requests: list[OnDemandTileComputeRequest],
         progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
     ) -> None:
         """Interacts with FeatureListManager to compute tiles and update cache
 
@@ -254,24 +271,30 @@
         await self.tile_manager_service.generate_tiles_on_demand(
             session=self.session, tile_inputs=tile_inputs, progress_callback=progress_callback
         )
 
     async def cleanup_temp_tables(self) -> None:
         """Drops all the temp tables that was created by TileCache"""
         for temp_table_name in self._materialized_temp_table_names:
-            await self.session.execute_query(f"DROP TABLE IF EXISTS {temp_table_name}")
+            await self.session.drop_table(
+                table_name=temp_table_name,
+                schema_name=self.session.schema_name,
+                database_name=self.session.database_name,
+                if_exists=True,
+            )
         self._materialized_temp_table_names = set()
 
     async def get_required_computation(  # pylint: disable=too-many-locals
         self,
         request_id: str,
         graph: QueryGraph,
         nodes: list[Node],
         request_table_name: str,
         serving_names_mapping: dict[str, str] | None = None,
+        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
     ) -> list[OnDemandTileComputeRequest]:
         """Query the entity tracker tables and obtain a list of tile computations that are required
 
         Parameters
         ----------
         request_id : str
             Request ID
@@ -279,41 +302,90 @@
             Query graph
         nodes : list[Node]
             List of query graph node
         request_table_name : str
             Request table name to use
         serving_names_mapping : dict[str, str] | None
             Optional mapping from original serving name to new serving name
+        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]]
+            Optional progress callback function
 
         Returns
         -------
         list[OnDemandTileComputeRequest]
         """
-        unique_tile_infos = self._get_unique_tile_infos(
-            graph=graph, nodes=nodes, serving_names_mapping=serving_names_mapping
+        if progress_callback is None:
+            graph_progress, query_progress = None, None
+        else:
+            graph_progress, query_progress = divide_progress_callback(
+                progress_callback, at_percent=20
+            )
+        tile_cache_status = await self._get_tile_cache_status(
+            graph=graph,
+            nodes=nodes,
+            serving_names_mapping=serving_names_mapping,
+            progress_callback=graph_progress,
         )
-        keys_with_tracker = await self._filter_keys_with_tracker(list(unique_tile_infos.keys()))
-        keys_without_tracker = list(set(unique_tile_infos.keys()) - set(keys_with_tracker))
 
+        # Check tile cache availability concurrently in batches
+        batches = list(tile_cache_status.split_batches())
+        processed = 0
+
+        async def done_callback() -> None:
+            nonlocal processed
+            processed += 1
+            if query_progress is not None:
+                pct = int(100 * processed / len(batches))
+                await query_progress(pct, "Checking tile cache availability")
+
+        coroutines = []
+        for i, subset_tile_cache_status in enumerate(batches):
+            coroutines.append(
+                self._get_compute_requests(
+                    request_id=f"{request_id}_{i}",
+                    request_table_name=request_table_name,
+                    tile_cache_status=subset_tile_cache_status,
+                    done_callback=done_callback,
+                )
+            )
+        result = await run_coroutines(coroutines)
+        all_requests = []
+        for requests in result:
+            all_requests.extend(requests)
+        return all_requests
+
+    async def _get_compute_requests(
+        self,
+        request_id: str,
+        request_table_name: str,
+        tile_cache_status: TileCacheStatus,
+        done_callback: Optional[Callable[[], Coroutine[Any, Any, None]]] = None,
+    ) -> list[OnDemandTileComputeRequest]:
         # Construct a temp table and query from it whether each tile has updated cache
         tic = time.time()
+        unique_tile_infos = tile_cache_status.unique_tile_infos
+        keys_with_tracker = tile_cache_status.keys_with_tracker
+        keys_without_tracker = list(set(unique_tile_infos.keys()) - set(keys_with_tracker))
+        session = await self.session.clone_if_not_threadsafe()
         await self._register_working_table(
+            session=session,
             unique_tile_infos=unique_tile_infos,
             keys_with_tracker=keys_with_tracker,
             keys_no_tracker=keys_without_tracker,
             request_id=request_id,
             request_table_name=request_table_name,
         )
 
         # Create a validity flag for each aggregation id
         tile_cache_validity = {}
         for key in keys_without_tracker:
             tile_cache_validity[key] = False
         if keys_with_tracker:
             existing_validity = await self._get_tile_cache_validity_from_working_table(
+                session=session,
                 request_id=request_id,
                 keys=keys_with_tracker,
                 unique_tile_infos=unique_tile_infos,
             )
             tile_cache_validity.update(existing_validity)
         elapsed = time.time() - tic
         logger.debug(f"Registering working table and validity check took {elapsed:.2f}s")
@@ -328,45 +400,96 @@
                 logger.debug(f"Need to recompute cache for {agg_id}")
                 request = self._construct_request_from_working_table(
                     request_id=request_id,
                     tile_info=unique_tile_infos[key],
                 )
                 requests.append(request)
 
+        if done_callback is not None:
+            await done_callback()
+
         return requests
 
-    def _get_unique_tile_infos(
-        self, graph: QueryGraph, nodes: list[Node], serving_names_mapping: dict[str, str] | None
+    async def _get_tile_cache_status(
+        self,
+        graph: QueryGraph,
+        nodes: list[Node],
+        serving_names_mapping: dict[str, str] | None = None,
+        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
+    ) -> TileCacheStatus:
+        """Get a TileCacheStatus object that corresponds to the graph and nodes
+
+        Parameters
+        ----------
+        graph : QueryGraph
+            Query graph
+        nodes : list[Node]
+            List of query graph node
+        serving_names_mapping : dict[str, str] | None
+            Optional mapping from original serving name to new serving name
+        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]]
+            Optional progress callback function
+
+        Returns
+        -------
+        TileCacheStatus
+        """
+        unique_tile_infos = await self._get_unique_tile_infos(
+            graph=graph,
+            nodes=nodes,
+            serving_names_mapping=serving_names_mapping,
+            progress_callback=progress_callback,
+        )
+        keys_with_tracker = await self._filter_keys_with_tracker(list(unique_tile_infos.keys()))
+        return TileCacheStatus(
+            unique_tile_infos=unique_tile_infos,
+            keys_with_tracker=keys_with_tracker,
+        )
+
+    async def _get_unique_tile_infos(
+        self,
+        graph: QueryGraph,
+        nodes: list[Node],
+        serving_names_mapping: dict[str, str] | None,
+        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]] = None,
     ) -> dict[TileInfoKey, TileGenSql]:
         """Construct mapping from aggregation id to TileGenSql for easier manipulation
 
         Parameters
         ----------
         graph : QueryGraph
             Query graph
         nodes : list[Node]
             List of query graph node
         serving_names_mapping : dict[str, str] | None
             Optional mapping from original serving name to new serving name
+        progress_callback: Optional[Callable[[int, str | None], Coroutine[Any, Any, None]]]
+            Optional progress callback function
 
         Returns
         -------
         dict[TileInfoKey, TileGenSql]
         """
         out = {}
         interpreter = GraphInterpreter(graph, source_type=self.source_type)
-        for node in nodes:
+        for i, node in enumerate(nodes):
             infos = interpreter.construct_tile_gen_sql(node, is_on_demand=True)
             for info in infos:
                 if info.aggregation_id not in out:
                     if serving_names_mapping is not None:
                         info.serving_names = apply_serving_names_mapping(
                             info.serving_names, serving_names_mapping
                         )
                     out[TileInfoKey.from_tile_info(info)] = info
+            if i % 10 == 0 and progress_callback is not None:
+                await progress_callback(
+                    int(i + 1 / len(nodes) * 100), "Checking tile cache availability"
+                )
+        if progress_callback is not None:
+            await progress_callback(100, "Checking tile cache availability")
         return out
 
     async def _filter_keys_with_tracker(
         self, tile_info_keys: list[TileInfoKey]
     ) -> list[TileInfoKey]:
         """Query tracker tables in data warehouse to identify aggregation IDs with existing tracking
         tables
@@ -379,15 +502,17 @@
         Returns
         -------
         list[TileInfoKey]
             List of TileInfoKey with existing entity tracker tables
         """
         all_trackers = set()
         for table in await self.session.list_tables(
-            database_name=self.session.database_name, schema_name=self.session.schema_name
+            database_name=self.session.database_name,
+            schema_name=self.session.schema_name,
+            timeout=TILE_CACHE_LIST_TABLES_TIMEOUT_SECONDS,
         ):
             # always convert to upper case in case some backends change the casing
             table_name = table.name.upper()
             if table_name.endswith(InternalName.TILE_ENTITY_TRACKER_SUFFIX.value):
                 all_trackers.add(table_name)
 
         out = []
@@ -395,14 +520,15 @@
             agg_id_tracker_name = tile_info_key.get_entity_tracker_table_name()
             if agg_id_tracker_name in all_trackers:
                 out.append(tile_info_key)
         return out
 
     async def _register_working_table(
         self,
+        session: BaseSession,
         unique_tile_infos: dict[TileInfoKey, TileGenSql],
         keys_with_tracker: list[TileInfoKey],
         keys_no_tracker: list[TileInfoKey],
         request_id: str,
         request_table_name: str,
     ) -> None:
         """Register a temp table from which we can query whether each (POINT_IN_TIME, ENTITY_ID,
@@ -426,14 +552,16 @@
 
         The table above indicates that the following tile tables need to be recomputed:
         - AGG_ID_1 for C1 (last tile start date is prior to the point in time)
         - AGG_ID_2 for C2 (no tile has been computed for this entity)
 
         Parameters
         ----------
+        session : BaseSession
+            Data warehouse session to use
         unique_tile_infos : dict[str, TileGenSql]
             Mapping from tile id to TileGenSql
         keys_with_tracker : list[TileInfoKey]
             List of tile ids with existing tracker tables
         keys_no_tracker : list[TileInfoKey]
             List of tile ids without existing tracker table
         request_id : str
@@ -470,32 +598,41 @@
                 f"{table_alias}.{InternalName.TILE_LAST_START_DATE} AS {key.get_working_table_column_name()}"
             )
 
         for key in keys_no_tracker:
             columns.append(f"CAST(null AS TIMESTAMP) AS {key.get_working_table_column_name()}")
 
         table_expr = table_expr.select("REQ.*", *columns)
-        table_sql = sql_to_string(table_expr, source_type=self.source_type)
 
         tile_cache_working_table_name = (
             f"{InternalName.TILE_CACHE_WORKING_TABLE.value}_{request_id}"
         )
-        await self.session.register_table_with_query(tile_cache_working_table_name, table_sql)
+        await session.create_table_as(
+            TableDetails(
+                database_name=session.database_name,
+                schema_name=session.schema_name,
+                table_name=tile_cache_working_table_name,
+            ),
+            table_expr,
+        )
         self._materialized_temp_table_names.add(tile_cache_working_table_name)
 
     async def _get_tile_cache_validity_from_working_table(
         self,
+        session: BaseSession,
         request_id: str,
         keys: list[TileInfoKey],
         unique_tile_infos: dict[TileInfoKey, TileGenSql],
     ) -> dict[TileInfoKey, bool]:
         """Get a dictionary indicating whether each tile table has updated enough tiles
 
         Parameters
         ----------
+        session : BaseSession
+            Data warehouse session to use
         request_id : str
             Request ID
         keys : list[TileInfoKey]
             List of aggregation ids
         unique_tile_infos : dict[TileInfoKey, TileGenSql]
             Mapping from tile id to TileGenSql
 
@@ -551,18 +688,19 @@
                 quoted=False,
             )
             validity_exprs.append(expr)
 
         tile_cache_working_table_name = (
             f"{InternalName.TILE_CACHE_WORKING_TABLE.value}_{request_id}"
         )
-        tile_cache_validity_sql = (
-            select(*validity_exprs).from_(tile_cache_working_table_name)
-        ).sql(pretty=True)
-        df_validity = await self.session.execute_query_long_running(tile_cache_validity_sql)
+        tile_cache_validity_sql = sql_to_string(
+            select(*validity_exprs).from_(quoted_identifier(tile_cache_working_table_name)),
+            source_type=session.source_type,
+        )
+        df_validity = await session.execute_query_long_running(tile_cache_validity_sql)
 
         # Result should only have one row
         assert df_validity is not None
         assert df_validity.shape[0] == 1
         out: dict[str, bool] = df_validity.iloc[0].to_dict()
         return {result_name_to_key_mapping[k.lower()]: v for (k, v) in out.items()}
```

### Comparing `featurebyte-1.0.2/featurebyte/utils/credential.py` & `featurebyte-1.0.3/featurebyte/utils/credential.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utility functions for credential management
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
 from abc import ABC, abstractmethod
 
 from bson.objectid import ObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/utils/messaging.py` & `featurebyte-1.0.3/featurebyte/utils/messaging.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Support for asynchronous messaging via Redis and web sockets
 """
+
 from typing import Any, Dict, Optional, cast
 
 import json
 import os
 from uuid import UUID
 
 import redis
```

### Comparing `featurebyte-1.0.2/featurebyte/utils/persistent.py` & `featurebyte-1.0.3/featurebyte/utils/persistent.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """
 Utility functions for persistent
 """
+
 from __future__ import annotations
 
 import os
 
 from featurebyte.persistent.mongo import MongoDB
 
 DATABASE_NAME = os.environ.get("MONGODB_DB", "featurebyte")
-MONGO_URI = os.environ.get("MONGODB_URI", "mongodb://localhost:27021")
+MONGO_URI = os.environ.get("MONGODB_URI", "mongodb://localhost:27017/")
 
 
 class MongoDBImpl(MongoDB):
     """
     Default MongoDB implementation for featurebyte
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/utils/storage.py` & `featurebyte-1.0.3/featurebyte/utils/storage.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utility functions for file storage
 """
+
 from __future__ import annotations
 
 from typing import AsyncIterator
 
 import os
 from contextlib import asynccontextmanager
 from pathlib import Path
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/registry.py` & `featurebyte-1.0.3/featurebyte/worker/registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 """
 Task registry
 """
+
 from typing import Dict, Type
 
 from enum import Enum
 
 from featurebyte.enum import WorkerCommand
 from featurebyte.worker.task.base import BaseTask, TaskT
 from featurebyte.worker.task.batch_feature_create import BatchFeatureCreateTask
 from featurebyte.worker.task.batch_feature_table import BatchFeatureTableTask
 from featurebyte.worker.task.batch_request_table import BatchRequestTableTask
 from featurebyte.worker.task.catalog_online_store_update import CatalogOnlineStoreUpdateTask
+from featurebyte.worker.task.data_description import DataDescriptionTask
 from featurebyte.worker.task.deployment_create_update import DeploymentCreateUpdateTask
 from featurebyte.worker.task.feature_job_setting_analysis import FeatureJobSettingAnalysisTask
 from featurebyte.worker.task.feature_job_setting_analysis_backtest import (
     FeatureJobSettingAnalysisBacktestTask,
 )
 from featurebyte.worker.task.feature_list_batch_feature_create import (
     FeatureListCreateWithBatchFeatureCreationTask,
@@ -50,11 +52,12 @@
     WorkerCommand.FEATURE_LIST_MAKE_PRODUCTION_READY: FeatureListMakeProductionReadyTask,
     WorkerCommand.STATIC_SOURCE_TABLE_CREATE: StaticSourceTableTask,
     WorkerCommand.TARGET_TABLE_CREATE: TargetTableTask,
     WorkerCommand.TILE_COMPUTE: TileTask,
     WorkerCommand.ONLINE_STORE_TABLE_CLEANUP: OnlineStoreCleanupTask,
     WorkerCommand.SCHEDULED_FEATURE_MATERIALIZE: ScheduledFeatureMaterializeTask,
     WorkerCommand.CATALOG_ONLINE_STORE_UPDATE: CatalogOnlineStoreUpdateTask,
+    WorkerCommand.DATA_DESCRIPTION: DataDescriptionTask,
     WorkerCommand.TEST: TestTask,
     # TO BE DEPRECATED
     WorkerCommand.FEATURE_LIST_CREATE_WITH_BATCH_FEATURE_CREATE: FeatureListCreateWithBatchFeatureCreationTask,
 }
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/schedulers.py` & `featurebyte-1.0.3/featurebyte/worker/schedulers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Customized MongoDB Scheduler
 """
+
 from typing import Any
 
 import datetime
 
 from celery import schedules
 from celerybeatmongo.schedulers import MongoScheduleEntry as BaseMongoScheduleEntry
 from celerybeatmongo.schedulers import MongoScheduler as BaseMongoScheduler
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/base.py` & `featurebyte-1.0.3/featurebyte/worker/task/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Base models for task and task payload
 """
+
 from __future__ import annotations
 
 from typing import Any, Generic, Optional, Type, TypeVar
 
 from abc import abstractmethod
 
 from redis import Redis
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/batch_feature_create.py` & `featurebyte-1.0.3/featurebyte/worker/task/batch_feature_create.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Batch feature create task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.schema.worker.task.batch_feature_create import BatchFeatureCreateTaskPayload
 from featurebyte.worker.task.base import BaseTask
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/batch_feature_table.py` & `featurebyte-1.0.3/featurebyte/worker/task/batch_feature_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchFeatureTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.models.batch_feature_table import BatchFeatureTableModel
 from featurebyte.models.deployment import DeploymentModel
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/batch_request_table.py` & `featurebyte-1.0.3/featurebyte/worker/task/batch_request_table.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 BatchRequestTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.models.batch_request_table import BatchRequestTableModel
 from featurebyte.schema.worker.task.batch_request_table import BatchRequestTableTaskPayload
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/catalog_online_store_update.py` & `featurebyte-1.0.3/featurebyte/worker/task/catalog_online_store_update.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 """
 Online store initialization task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.feast.service.feature_store import FeastFeatureStoreService
 from featurebyte.feast.service.registry import FeastRegistryService
 from featurebyte.logging import get_logger
 from featurebyte.schema.catalog import CatalogOnlineStoreUpdate
 from featurebyte.schema.worker.task.online_store_initialize import (
     CatalogOnlineStoreInitializeTaskPayload,
 )
 from featurebyte.service.catalog import CatalogService
+from featurebyte.service.deployment import DeploymentService
 from featurebyte.service.feature_materialize import FeatureMaterializeService
 from featurebyte.service.offline_store_feature_table import OfflineStoreFeatureTableService
 from featurebyte.worker.task.base import BaseTask
 from featurebyte.worker.util.task_progress_updater import TaskProgressUpdater
 
 logger = get_logger(__name__)
 
@@ -37,22 +39,24 @@
     def __init__(
         self,
         feature_materialize_service: FeatureMaterializeService,
         offline_store_feature_table_service: OfflineStoreFeatureTableService,
         feast_registry_service: FeastRegistryService,
         feast_feature_store_service: FeastFeatureStoreService,
         catalog_service: CatalogService,
+        deployment_service: DeploymentService,
         task_progress_updater: TaskProgressUpdater,
     ):
         super().__init__()
         self.feature_materialize_service = feature_materialize_service
         self.offline_store_feature_table_service = offline_store_feature_table_service
         self.feast_registry_service = feast_registry_service
         self.feast_feature_store_service = feast_feature_store_service
         self.catalog_service = catalog_service
+        self.deployment_service = deployment_service
         self.task_progress_updater = task_progress_updater
 
     async def get_task_description(self, payload: CatalogOnlineStoreInitializeTaskPayload) -> str:
         if payload.online_store_id is not None:
             return f'Updating online store "{payload.online_store_id}" for catalog {payload.catalog_id}'
         return f"Disabling online store for catalog {payload.catalog_id}"
 
@@ -68,29 +72,22 @@
         )
         await self.catalog_service.update_online_store(
             document_id=payload.catalog_id,
             data=CatalogOnlineStoreUpdate(online_store_id=payload.online_store_id),
         )
 
     async def _run_materialize(self, payload: CatalogOnlineStoreInitializeTaskPayload) -> None:
-        feast_registry = await self.feast_registry_service.get_feast_registry_for_catalog()
-        if feast_registry is None:
-            return None
-        feast_feature_store = await self.feast_feature_store_service.get_feast_feature_store(
-            feast_registry_id=feast_registry.id,
-            online_store_id=payload.online_store_id,
-        )
         session = None
         total_count = (await self.offline_store_feature_table_service.list_documents_as_dict())[
             "total"
         ]
         current_table_index = 0
-        async for feature_table_model in self.offline_store_feature_table_service.list_documents_iterator(
-            {}
-        ):
+        async for (
+            feature_table_model
+        ) in self.offline_store_feature_table_service.list_documents_iterator({}):
             logger.info(
                 "Updating online store for offline feature store table",
                 extra={
                     "online_store_id": payload.online_store_id,
                     "feature_table_name": feature_table_model.name,
                     "catalog_id": payload.catalog_id,
                 },
@@ -101,12 +98,22 @@
             )
             current_table_index += 1
 
             if session is None:
                 session = await self.feature_materialize_service._get_session(  # pylint: disable=protected-access
                     feature_table_model
                 )
-            await self.feature_materialize_service.update_online_store(
-                feature_store=feast_feature_store,
-                feature_table_model=feature_table_model,
-                session=session,
-            )
+
+            if feature_table_model.deployment_ids:
+                service = self.feast_feature_store_service
+                feast_feature_store = (
+                    await service.get_feast_feature_store_for_feature_materialization(
+                        feature_table_model=feature_table_model,
+                        online_store_id=payload.online_store_id,
+                    )
+                )
+                if feast_feature_store:
+                    await self.feature_materialize_service.update_online_store(
+                        feature_store=feast_feature_store,
+                        feature_table_model=feature_table_model,
+                        session=session,
+                    )
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/deployment_create_update.py` & `featurebyte-1.0.3/featurebyte/worker/task/deployment_create_update.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Deployment Create & Update Task
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional, cast
 
 from redis import Redis
 
 from featurebyte.exception import DocumentCreationError, DocumentUpdateError
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/feature_job_setting_analysis.py` & `featurebyte-1.0.3/featurebyte/worker/task/feature_job_setting_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature Job Setting Analysis task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from pathlib import Path
 
 from featurebyte_freeware.feature_job_analysis.analysis import create_feature_job_settings_analysis
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/feature_job_setting_analysis_backtest.py` & `featurebyte-1.0.3/featurebyte/worker/task/feature_job_setting_analysis_backtest.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature job setting analysis backtest
 """
+
 from __future__ import annotations
 
 from datetime import datetime
 from pathlib import Path
 
 from featurebyte_freeware.feature_job_analysis.analysis import FeatureJobSettingsAnalysisResult
 from featurebyte_freeware.feature_job_analysis.schema import FeatureJobSetting
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/feature_list_batch_feature_create.py` & `featurebyte-1.0.3/featurebyte/worker/task/feature_list_batch_feature_create.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature list creation with batch feature creation task
 """
+
 from __future__ import annotations
 
 from typing import Any, Sequence
 
 from bson import ObjectId
 
 from featurebyte.common.progress import get_ranged_progress_callback
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/feature_list_create.py` & `featurebyte-1.0.3/featurebyte/worker/task/feature_list_create.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Feature list creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from pathlib import Path
 
 from bson import ObjectId, json_util
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/feature_list_make_production_ready.py` & `featurebyte-1.0.3/featurebyte/worker/task/feature_list_make_production_ready.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,44 +1,49 @@
 """
 Feature list make production ready task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.schema.worker.task.feature_list_make_production_ready import (
     FeatureListMakeProductionReadyTaskPayload,
 )
 from featurebyte.service.feature_list import FeatureListService
 from featurebyte.service.feature_list_facade import FeatureListFacadeService
 from featurebyte.worker.task.base import BaseTask
+from featurebyte.worker.util.task_progress_updater import TaskProgressUpdater
 
 
 class FeatureListMakeProductionReadyTask(BaseTask[FeatureListMakeProductionReadyTaskPayload]):
     """
     Feature list make production ready task
     """
 
     payload_class = FeatureListMakeProductionReadyTaskPayload
 
     def __init__(
         self,
         feature_list_service: FeatureListService,
         feature_list_facade_service: FeatureListFacadeService,
+        task_progress_updater: TaskProgressUpdater,
     ):
         super().__init__()
         self.feature_list_service = feature_list_service
         self.feature_list_facade_service = feature_list_facade_service
+        self.task_progress_updater = task_progress_updater
 
     async def get_task_description(self, payload: FeatureListMakeProductionReadyTaskPayload) -> str:
         feature_list_doc = await self.feature_list_service.get_document_as_dict(
             document_id=payload.feature_list_id,
             projection={"name": 1},
         )
         feature_list_name = feature_list_doc["name"]
         return f'Make all features of feature list "{feature_list_name}" production ready'
 
     async def execute(self, payload: FeatureListMakeProductionReadyTaskPayload) -> Any:
         await self.feature_list_facade_service.make_features_production_ready(
             feature_list_id=payload.feature_list_id,
             ignore_guardrails=payload.ignore_guardrails,
+            progress_callback=self.task_progress_updater.update_progress,
         )
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/historical_feature_table.py` & `featurebyte-1.0.3/featurebyte/worker/task/historical_feature_table.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 """
 HistoricalFeatureTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.models.historical_feature_table import HistoricalFeatureTableModel
-from featurebyte.models.observation_table import ObservationTableModel
 from featurebyte.schema.worker.task.historical_feature_table import (
     HistoricalFeatureTableTaskPayload,
 )
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.historical_feature_table import HistoricalFeatureTableService
 from featurebyte.service.historical_features import HistoricalFeaturesService
 from featurebyte.service.session_manager import SessionManagerService
@@ -55,25 +55,20 @@
 
         observation_set = await self.observation_set_helper.get_observation_set(
             payload.observation_table_id, payload.observation_set_storage_path
         )
         location = await self.historical_feature_table_service.generate_materialized_table_location(
             payload.feature_store_id
         )
-        is_view = (
-            isinstance(observation_set, ObservationTableModel)
-            and observation_set.has_row_index is True
-        )
         async with self.drop_table_on_error(
             db_session=db_session,
             table_details=location.table_details,
             payload=payload,
-            is_view=is_view,
         ):
-            await self.historical_features_service.compute(
+            result = await self.historical_features_service.compute(
                 observation_set=observation_set,
                 compute_request=payload.featurelist_get_historical_features,
                 output_table_details=location.table_details,
             )
             (
                 columns_info,
                 num_rows,
@@ -88,10 +83,10 @@
                 user_id=payload.user_id,
                 name=payload.name,
                 location=location,
                 observation_table_id=payload.observation_table_id,
                 feature_list_id=payload.featurelist_get_historical_features.feature_list_id,
                 columns_info=columns_info,
                 num_rows=num_rows,
-                is_view=is_view,
+                is_view=result.is_output_view,
             )
             await self.historical_feature_table_service.create_document(historical_feature_table)
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/materialized_table_delete.py` & `featurebyte-1.0.3/featurebyte/worker/task/materialized_table_delete.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Materialized Table Delete Task
 """
+
 from __future__ import annotations
 
 from typing import Any, cast
 
 from featurebyte.models.materialized_table import MaterializedTableModel
 from featurebyte.schema.worker.task.materialized_table_delete import (
     MaterializedTableCollectionName,
@@ -64,15 +65,14 @@
     async def get_task_description(self, payload: MaterializedTableDeleteTaskPayload) -> str:
         service_map = {
             MaterializedTableCollectionName.BATCH_REQUEST: self.batch_request_table_service,
             MaterializedTableCollectionName.BATCH_FEATURE: self.batch_feature_table_service,
             MaterializedTableCollectionName.OBSERVATION: self.observation_table_service,
             MaterializedTableCollectionName.HISTORICAL_FEATURE: self.historical_feature_table_service,
             MaterializedTableCollectionName.STATIC_SOURCE: self.static_source_table_service,
-            MaterializedTableCollectionName.TARGET: self.target_table_service,
         }
         service = service_map[payload.collection_name]
         materialized_table = await service.get_document(document_id=payload.document_id)  # type: ignore[attr-defined]
         description = payload.collection_name.replace("_", " ")
         if not description.endswith(" table"):
             description = f"{description} table"
         return f'Delete {description} "{materialized_table.name}"'
@@ -116,22 +116,14 @@
         document = await self.historical_feature_table_service.get_document(
             document_id=payload.document_id
         )
         await self._delete_table_at_data_warehouse(document)
         await self.historical_feature_table_service.delete_document(document_id=document.id)
         return cast(MaterializedTableModel, document)
 
-    async def _delete_target_table(
-        self, payload: MaterializedTableDeleteTaskPayload
-    ) -> MaterializedTableModel:
-        document = await self.target_table_service.get_document(document_id=payload.document_id)
-        await self._delete_table_at_data_warehouse(document)
-        await self.target_table_service.delete_document(document_id=document.id)
-        return cast(MaterializedTableModel, document)
-
     async def _delete_static_source_table(
         self, payload: MaterializedTableDeleteTaskPayload
     ) -> MaterializedTableModel:
         document = await check_delete_static_source_table(
             static_source_table_service=self.static_source_table_service,
             table_service=self.table_service,
             document_id=payload.document_id,
@@ -147,23 +139,21 @@
         )
         db_session = await self.session_manager_service.get_feature_store_session(feature_store)
 
         await db_session.drop_table(
             table_name=document.location.table_details.table_name,
             schema_name=document.location.table_details.schema_name,  # type: ignore
             database_name=document.location.table_details.database_name,  # type: ignore
-            is_view=document.is_view,
         )
 
     async def execute(self, payload: MaterializedTableDeleteTaskPayload) -> Any:
         # table to delete action mapping
         table_to_delete_action = {
             MaterializedTableCollectionName.BATCH_REQUEST: self._delete_batch_request_table,
             MaterializedTableCollectionName.BATCH_FEATURE: self._delete_batch_feature_table,
             MaterializedTableCollectionName.OBSERVATION: self._delete_observation_table,
             MaterializedTableCollectionName.HISTORICAL_FEATURE: self._delete_historical_feature_table,
             MaterializedTableCollectionName.STATIC_SOURCE: self._delete_static_source_table,
-            MaterializedTableCollectionName.TARGET: self._delete_target_table,
         }
 
         # delete document stored at mongo
         await table_to_delete_action[payload.collection_name](payload)
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/mixin.py` & `featurebyte-1.0.3/featurebyte/worker/task/mixin.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Mixin classes for tasks
 """
+
 from __future__ import annotations
 
 from typing import AsyncIterator
 
 from contextlib import asynccontextmanager
 
 from featurebyte.logging import get_logger
@@ -22,29 +23,26 @@
 
     @asynccontextmanager
     async def drop_table_on_error(
         self,
         db_session: BaseSession,
         table_details: TableDetails,
         payload: BaseTaskPayload,
-        is_view: bool = False,
     ) -> AsyncIterator[None]:
         """
         Drop the table on error
 
         Parameters
         ----------
         db_session: BaseSession
             The database session
         table_details: TableDetails
             The table details
         payload: BaseTaskPayload
             The task payload
-        is_view: bool
-            Whether it is view, not table
 
         Yields
         ------
         AsyncIterator[None]
             The async iterator
 
         Raises
@@ -62,10 +60,9 @@
             assert table_details.schema_name is not None
             assert table_details.database_name is not None
             await db_session.drop_table(
                 table_name=table_details.table_name,
                 schema_name=table_details.schema_name,
                 database_name=table_details.database_name,
                 if_exists=True,
-                is_view=is_view,
             )
             raise exc
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/observation_table.py` & `featurebyte-1.0.3/featurebyte/worker/task/observation_table.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ObservationTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.models.observation_table import ObservationTableModel, TargetInput
 from featurebyte.schema.worker.task.observation_table import ObservationTableTaskPayload
@@ -61,14 +62,15 @@
             additional_metadata = (
                 await self.observation_table_service.validate_materialized_table_and_get_metadata(
                     db_session,
                     location.table_details,
                     feature_store=feature_store,
                     skip_entity_validation_checks=payload.skip_entity_validation_checks,
                     primary_entity_ids=payload.primary_entity_ids,
+                    target_namespace_id=payload.target_namespace_id,
                 )
             )
 
             logger.debug("Creating a new ObservationTable", extra=location.table_details.dict())
             primary_entity_ids = payload.primary_entity_ids or []
             observation_table = ObservationTableModel(
                 _id=payload.output_document_id,
@@ -76,10 +78,11 @@
                 name=payload.name,
                 location=location,
                 context_id=payload.context_id,
                 request_input=payload.request_input,
                 purpose=payload.purpose,
                 primary_entity_ids=primary_entity_ids,
                 has_row_index=True,
+                target_namespace_id=payload.target_namespace_id,
                 **additional_metadata,
             )
             await self.observation_table_service.create_document(observation_table)
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/observation_table_upload.py` & `featurebyte-1.0.3/featurebyte/worker/task/observation_table_upload.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 ObservationTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from pathlib import Path
 
 from featurebyte.logging import get_logger
@@ -84,14 +85,15 @@
             # Validate table and retrieve metadata about the table
             additional_metadata = (
                 await self.observation_table_service.validate_materialized_table_and_get_metadata(
                     db_session,
                     location.table_details,
                     feature_store=feature_store,
                     primary_entity_ids=payload.primary_entity_ids,
+                    target_namespace_id=payload.target_namespace_id,
                 )
             )
 
             # Store metadata of the observation table in mongo
             logger.debug("Creating a new ObservationTable", extra=location.table_details.dict())
             observation_table = ObservationTableModel(
                 _id=payload.output_document_id,
@@ -101,10 +103,11 @@
                 request_input=UploadedFileInput(
                     type=RequestInputType.UPLOADED_FILE,
                     file_name=payload.uploaded_file_name,
                 ),
                 purpose=payload.purpose,
                 primary_entity_ids=payload.primary_entity_ids,
                 has_row_index=True,
+                target_namespace_id=payload.target_namespace_id,
                 **additional_metadata,
             )
             await self.observation_table_service.create_document(observation_table)
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/online_store_cleanup.py` & `featurebyte-1.0.3/featurebyte/worker/task/online_store_cleanup.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Online store cleanup task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.schema.worker.task.online_store_cleanup import OnlineStoreCleanupTaskPayload
 from featurebyte.service.online_store_cleanup import OnlineStoreCleanupService
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/scheduled_feature_materialize.py` & `featurebyte-1.0.3/featurebyte/worker/task/scheduled_feature_materialize.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Scheduled feature materialize task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.schema.worker.task.scheduled_feature_materialize import (
     ScheduledFeatureMaterializeTaskPayload,
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/static_source_table.py` & `featurebyte-1.0.3/featurebyte/worker/task/static_source_table.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 StaticSourceTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.models.static_source_table import StaticSourceTableModel
 from featurebyte.schema.worker.task.static_source_table import StaticSourceTableTaskPayload
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/target_table.py` & `featurebyte-1.0.3/featurebyte/worker/task/target_table.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 """
 TargetTable creation task
 """
+
 from __future__ import annotations
 
 from typing import Any, Optional
 
+from featurebyte.exception import DocumentNotFoundError
 from featurebyte.logging import get_logger
-from featurebyte.models.observation_table import ObservationTableModel, Purpose
+from featurebyte.models.observation_table import ObservationTableModel, Purpose, TargetInput
+from featurebyte.models.request_input import RequestInputType
 from featurebyte.routes.common.derive_primary_entity_helper import DerivePrimaryEntityHelper
 from featurebyte.schema.target import ComputeTargetRequest
 from featurebyte.schema.worker.task.target_table import TargetTableTaskPayload
 from featurebyte.service.feature_store import FeatureStoreService
 from featurebyte.service.observation_table import ObservationTableService
 from featurebyte.service.session_manager import SessionManagerService
+from featurebyte.service.target import TargetService
 from featurebyte.service.target_helper.compute_target import TargetComputer
 from featurebyte.worker.task.base import BaseTask
 from featurebyte.worker.task.mixin import DataWarehouseMixin
 from featurebyte.worker.util.observation_set_helper import ObservationSetHelper
 
 logger = get_logger(__name__)
 
@@ -32,22 +36,24 @@
         self,
         feature_store_service: FeatureStoreService,
         session_manager_service: SessionManagerService,
         observation_set_helper: ObservationSetHelper,
         observation_table_service: ObservationTableService,
         target_computer: TargetComputer,
         derive_primary_entity_helper: DerivePrimaryEntityHelper,
+        target_service: TargetService,
     ):
         super().__init__()
         self.feature_store_service = feature_store_service
         self.session_manager_service = session_manager_service
         self.observation_set_helper = observation_set_helper
         self.observation_table_service = observation_table_service
         self.target_computer = target_computer
         self.derive_primary_entity_helper = derive_primary_entity_helper
+        self.target_service = target_service
 
     async def get_task_description(self, payload: TargetTableTaskPayload) -> str:
         return f'Save target table "{payload.name}"'
 
     async def execute(self, payload: TargetTableTaskPayload) -> Any:
         feature_store = await self.feature_store_service.get_document(
             document_id=payload.feature_store_id
@@ -59,33 +65,51 @@
         location = await self.observation_table_service.generate_materialized_table_location(
             payload.feature_store_id
         )
         has_row_index = (
             isinstance(observation_set, ObservationTableModel)
             and observation_set.has_row_index is True
         )
+
+        # track target_namespace_id if target_id is provided in the payload
+        target_namespace_id = None
+        if payload.request_input and isinstance(payload.request_input, TargetInput):
+            # Handle backward compatibility for requests from older SDK
+            target_id = payload.request_input.target_id
+        else:
+            target_id = payload.target_id
+
         async with self.drop_table_on_error(
             db_session=db_session,
             table_details=location.table_details,
             payload=payload,
-            is_view=has_row_index,
         ):
             # Graphs and nodes being processed in this task should not be None anymore.
             graph = payload.graph
             node_names = payload.node_names
+            if target_id is not None:
+                try:
+                    target = await self.target_service.get_document(document_id=target_id)
+                    target_namespace_id = target.target_namespace_id
+                    graph = target.graph
+                    node_names = [target.node_name]
+                except DocumentNotFoundError:
+                    pass
+
             assert graph is not None
             assert node_names is not None
-            await self.target_computer.compute(
+
+            result = await self.target_computer.compute(
                 observation_set=observation_set,
                 compute_request=ComputeTargetRequest(
                     feature_store_id=payload.feature_store_id,
                     graph=graph,
                     node_names=node_names,
                     serving_names_mapping=payload.serving_names_mapping,
-                    target_id=payload.target_id,
+                    target_id=target_id,
                 ),
                 output_table_details=location.table_details,
             )
             entity_ids = graph.get_entity_ids(node_names[0])
             primary_entity_ids = await self.derive_primary_entity_helper.derive_primary_entity_ids(
                 entity_ids
             )
@@ -117,15 +141,24 @@
 
             observation_table = ObservationTableModel(
                 _id=payload.output_document_id,
                 user_id=payload.user_id,
                 name=payload.name,
                 location=location,
                 context_id=payload.context_id,
-                request_input=payload.request_input,
+                request_input=TargetInput(
+                    target_id=target_id,
+                    observation_table_id=payload.observation_table_id,
+                    type=(
+                        RequestInputType.OBSERVATION_TABLE
+                        if payload.observation_table_id
+                        else RequestInputType.DATAFRAME
+                    ),
+                ),
                 primary_entity_ids=primary_entity_ids,
                 purpose=purpose,
                 has_row_index=True,
-                is_view=has_row_index,
+                is_view=result.is_output_view,
+                target_namespace_id=target_namespace_id,
                 **additional_metadata,
             )
             await self.observation_table_service.create_document(observation_table)
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/test_task.py` & `featurebyte-1.0.3/featurebyte/worker/task/test_task.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Test task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 import asyncio
 
 from featurebyte.logging import get_logger
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task/tile_task.py` & `featurebyte-1.0.3/featurebyte/worker/task/tile_task.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Test task
 """
+
 from __future__ import annotations
 
 from typing import Any
 
 from featurebyte.logging import get_logger
 from featurebyte.schema.worker.task.tile import TileTaskPayload
 from featurebyte.service.feature_store import FeatureStoreService
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/task_executor.py` & `featurebyte-1.0.3/featurebyte/worker/task_executor.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,103 +1,109 @@
 """
 This module contains TaskExecutor class
 """
+
 from __future__ import annotations
 
-from typing import Any, Awaitable, Optional
+from typing import Any, Coroutine, Optional, Set
 
 import asyncio
 import os
+import time
 from abc import abstractmethod
-from concurrent.futures import ThreadPoolExecutor
 from concurrent.futures import TimeoutError as ConcurrentTimeoutError
 from datetime import datetime
-from threading import Thread
 from uuid import UUID
 
 from bson import ObjectId
 from celery import Task
-from celery.exceptions import SoftTimeLimitExceeded
+from celery.exceptions import SoftTimeLimitExceeded, WorkerTerminate
 
 from featurebyte.config import Configurations, get_home_path
 from featurebyte.enum import WorkerCommand
 from featurebyte.exception import TaskCanceledError, TaskRevokeExceptions
 from featurebyte.logging import get_logger
 from featurebyte.models.base import User
 from featurebyte.models.task import Task as TaskModel
 from featurebyte.routes.lazy_app_container import LazyAppContainer
 from featurebyte.routes.registry import app_container_config
 from featurebyte.utils.messaging import Progress
 from featurebyte.utils.persistent import MongoDBImpl
-from featurebyte.worker import get_celery
+from featurebyte.worker import get_async_loop, get_celery
 from featurebyte.worker.registry import TASK_REGISTRY_MAP
 from featurebyte.worker.util.task_progress_updater import TaskProgressUpdater
 
 logger = get_logger(__name__)
 
 
-def start_background_loop(loop: asyncio.AbstractEventLoop) -> None:
-    """
-    Start background event loop
-
-    Parameters
-    ----------
-    loop: AbstractEventLoop
-        Event loop to run
-    """
-    try:
-        asyncio.set_event_loop(loop)
-        loop.run_forever()
-    finally:
-        try:
-            loop.run_until_complete(loop.shutdown_asyncgens())
-        finally:
-            asyncio.set_event_loop(None)
-            loop.close()
+PENDING_TASKS: Set[UUID] = set()
+EXECUTION_LATENCY_THRESHOLD = 10  # max delay allowed for task execution in seconds
+WORKER_TERMINATED = False
 
 
-def run_async(coro: Awaitable[Any], timeout: Optional[int] = None) -> Any:
+def run_async(
+    coro: Coroutine[Any, Any, Any], request_id: UUID, timeout: Optional[int] = None
+) -> Any:
     """
     Run async function in both async and non-async context
     Parameters
     ----------
-    coro: Coroutine
+    coro: Coroutine[Any, Any, Any]
         Coroutine to run
+    request_id: UUID
+        Request ID
     timeout: Optional[int]
         Timeout in seconds, default to None (no timeout)
 
     Returns
     -------
     Any
         result from function call
 
     Raises
     ------
     SoftTimeLimitExceeded
         timeout is exceeded
+    WorkerTerminate
+        Worker is terminated
     """
-    try:
-        loop = asyncio.get_running_loop()
-        logger.debug("Use existing async loop", extra={"loop": loop})
-    except RuntimeError:
-        loop = asyncio.new_event_loop()
-        loop.set_default_executor(ThreadPoolExecutor(max_workers=1000))
-        logger.debug("Create new async loop", extra={"loop": loop})
-        thread = Thread(target=start_background_loop, args=(loop,), daemon=True)
-        thread.start()
+    loop = get_async_loop()
+    task_received_time = time.time()
+
+    logger.debug(
+        "Running async function",
+        extra={"timeout": timeout, "active_tasks": len(asyncio.all_tasks(loop=loop))},
+    )
+
+    async def _run_task() -> Any:
+        return await loop.create_task(coro, name=str(request_id))
+
+    PENDING_TASKS.add(request_id)
+    future = asyncio.run_coroutine_threadsafe(_run_task(), loop)
 
-    logger.info("Asyncio tasks", extra={"num_tasks": len(asyncio.all_tasks(loop=loop))})
+    if timeout is None or timeout > EXECUTION_LATENCY_THRESHOLD:
+        try:
+            return future.result(timeout=EXECUTION_LATENCY_THRESHOLD)
+        except ConcurrentTimeoutError as exc:
+            # check if task execution is delayed beyond threshold
+            if request_id in PENDING_TASKS:
+                logger.error("Worker is not responsive, shutting down")
+                raise WorkerTerminate(True) from exc
+        # continue waiting for the task to complete
+        if timeout is not None:
+            timeout = max(timeout - int(time.time() - task_received_time), 0)
 
-    logger.info("Start task", extra={"timeout": timeout})
-    future = asyncio.run_coroutine_threadsafe(coro, loop)
     try:
         return future.result(timeout=timeout)
     except ConcurrentTimeoutError as exc:
-        # try to cancel the job if it has not started
-        future.cancel()
+        active_tasks = asyncio.all_tasks(loop=loop)
+        for task in active_tasks:
+            if task.get_name() == str(request_id):
+                task.cancel()
+                break
         raise SoftTimeLimitExceeded(f"Task timed out after {timeout}s") from exc
 
 
 class TaskExecutor:
     """
     TaskExecutor class
     """
@@ -250,14 +256,22 @@
         payload: Any
             Task payload
 
         Returns
         -------
         Any
         """
+        command = str(payload.get("command"))
+        print(
+            f"Running: {command}"
+        )  # Add temporary print statement to confirm logging not causing freezing issue
+        logger.debug(f"Executing: {command}")
+        if request_id in PENDING_TASKS:
+            PENDING_TASKS.remove(request_id)
+
         progress = self.progress_class(user_id=payload.get("user_id"), task_id=request_id)
         app_container = await self.get_app_container(request_id, payload, progress)
         executor = self.executor_class(
             payload=payload, task_id=request_id, app_container=app_container
         )
         try:
             return_val = await executor.execute()
@@ -289,17 +303,26 @@
     """
     Celery task for IO bound task
     """
 
     name = "featurebyte.worker.task_executor.execute_io_task"
 
     def run(self: Any, *args: Any, **payload: Any) -> Any:
-        return run_async(
-            self.execute_task(self.request.id, **payload), timeout=self.request.timelimit[1]
-        )
+        global WORKER_TERMINATED  # pylint: disable=global-statement
+        if WORKER_TERMINATED:
+            raise WorkerTerminate(True)
+        try:
+            return run_async(
+                self.execute_task(self.request.id, **payload),
+                request_id=self.request.id,
+                timeout=self.request.timelimit[1],
+            )
+        except WorkerTerminate as exc:
+            WORKER_TERMINATED = True
+            raise self.retry(countdown=0) from exc
 
 
 class CPUBoundTask(BaseCeleryTask):
     """
     Celery task for CPU bound task
     """
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/test_util/random_task.py` & `featurebyte-1.0.3/featurebyte/worker/test_util/random_task.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Random task util. Mainly used in tests, but placing in src so that we can register for DI.
 """
+
 import time
 
 from featurebyte.enum import StrEnum
 from featurebyte.models.base import User
 from featurebyte.persistent import Persistent
 from featurebyte.schema.worker.task.base import BaseTaskPayload
 from featurebyte.worker.task.base import BaseTask
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/util/batch_feature_creator.py` & `featurebyte-1.0.3/featurebyte/worker/util/batch_feature_creator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Batch feature creator
 """
+
 from typing import Any, Callable, Coroutine, Dict, Iterator, List, Sequence, Set, Union
 
 import asyncio
 import concurrent
 import os
 from contextlib import ExitStack, contextmanager
 from functools import wraps
@@ -245,17 +246,17 @@
         """
         conflict_to_resolution_feature_id_map = {}
         if conflict_resolution == "retrieve":
             async for feat_namespace in self.feature_namespace_service.list_documents_iterator(
                 query_filter={"name": {"$in": feature_names}}
             ):
                 assert feat_namespace.name is not None
-                conflict_to_resolution_feature_id_map[
-                    feat_namespace.name
-                ] = feat_namespace.default_feature_id
+                conflict_to_resolution_feature_id_map[feat_namespace.name] = (
+                    feat_namespace.default_feature_id
+                )
         return conflict_to_resolution_feature_id_map
 
     async def is_generated_feature_consistent(self, document: FeatureModel) -> bool:
         """
         Validate the generated feature against the expected feature
 
         Parameters
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/util/observation_set_helper.py` & `featurebyte-1.0.3/featurebyte/worker/util/observation_set_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Observation table helper
 """
+
 from typing import Optional, Union
 
 from pathlib import Path
 
 import pandas as pd
 
 from featurebyte.models.base import PydanticObjectId
```

### Comparing `featurebyte-1.0.2/featurebyte/worker/util/task_progress_updater.py` & `featurebyte-1.0.3/featurebyte/worker/util/task_progress_updater.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Task progress updater
 """
+
 from typing import Any, Optional
 
 from uuid import UUID
 
 from featurebyte.models.task import Task
 from featurebyte.schema.worker.progress import ProgressModel
```

### Comparing `featurebyte-1.0.2/pyproject.toml` & `featurebyte-1.0.3/pyproject.toml`

 * *Files 7% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 [tool.bandit]
 skips = ["B608"]
 
 [tool.black]  # https://github.com/psf/black
 color = true
 exclude = '/(\.direnv|\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|venv|\.svn|_build|buck-out|build|dist|__pypackages__|docker|tests/fixtures/sdk_code)/'
 line-length = 100
-target-version = ["py38"]
+target-version = ["py39"]
 
 [tool.coverage.report] # https://coverage.readthedocs.io/en/6.4.1/config.html#run
 exclude_lines = [
     "pragma: no cover",
     "if TYPE_CHECKING:",
     "@overload",
     "@abstractmethod",
@@ -49,15 +49,15 @@
 disallow_subclassing_any = false
 disallow_untyped_defs = true
 exclude = ['tests', 'docs', 'docker', '.github', 'featurebyte/api/templates', 'notebooks/prebuilt_catalogs.py']
 ignore_missing_imports = true
 implicit_reexport = false
 no_implicit_optional = true
 pretty = true
-python_version = 3.8
+python_version = 3.9
 show_column_numbers = true
 show_error_codes = true
 show_error_context = true
 show_traceback = true
 strict_equality = true
 strict_optional = true
 warn_no_return = true
@@ -88,94 +88,95 @@
     "featurebyte/sql/spark/*.jar",
 ]
 keywords = []
 license = "Elastic License 2.0"
 name = "featurebyte"
 readme = "README.md"
 repository = "https://github.com/featurebyte/featurebyte"
-version = "1.0.2"
+version = "1.0.3"
 
 [tool.poetry.dependencies]
 PyYAML = "^6.0"
 aiobotocore = { version = "^2.4.0", extras = ["boto3"] }
 aiofiles = "^22.1.0"
-aioredis = { version = "^2.0.1", optional = true }
 alive-progress = "^3.1.1"
 asyncache = "^0.3.1"
-black = "^23.3.0"
+black = "^24.3.0"
 cachetools = { version = "^5.2.0", optional = true }
 celery = { version = "^5.2.6", extras = ["redis"], optional = true }
 celerybeat-mongo = { version = "^0.2.0", optional = true }
-cryptography = "^41.0.3"
-databricks-sdk = { version = "^0.13.0", optional = true }
-databricks-sql-connector = { version = "2.8.0", optional = true }  # Awaiting snowflake-connector-python to support urllib3>=2.0
-fastapi = { version =  "^0.96.0", optional = true }
+cryptography = "^42.0.4"
+databricks-sdk = { version = "^0.24.0", optional = true }
+databricks-sql-connector = { version = "^3.1.1", optional = true }
+fastapi = { version =  "^0.96.0", optional = true }  # Need to bump pydantic to 2.X.X
 feast = { version = "^0.35.0", optional = true, extras = ["snowflake", "redis", "mysql"]}
 featurebyte-freeware = { version = "^0.2.16", optional = true }
 gevent = {version = "^23.9.1", optional = true}
 humanize = "^4.4.0"
-importlib_metadata = { version = "*", python = "^3.8"}
+importlib_metadata = { version = "*", python = "^3.9"}
 jinja2 = "^3.1.2"
 lazy-object-proxy = "^1.7.1"
 motor = { version = "^3.0.0", optional = true }
-orjson = "^3.8.3"
+orjson = "^3.9.15"
 pandas = "^1.5.3"
 pdfkit = { version = "^1.0.0", optional = true }
-pillow = "^10.2.0"
+pillow = "^10.3.0"
 pyarrow = "^14.0.1"
 pydantic = "^1.9.6"
 pyhive = { version = "^0.6.5", optional = true }
-pymongo = "^4.1.1"
-python = ">=3.8,<3.12"
-python-multipart = "^0.0.6"
+pymongo = "^4.6.3"
+pyopenssl = "^24.1.0"
+python = ">=3.9,<3.12"
+python-multipart = "^0.0.9"  # Dependency on fastapi form data
 python-on-whales = "^0.60.0"
-redis = {version = "^4.5.3", optional = true, allow-prereleases = true}
+redis = {version = "^4.2.0", optional = true}
 requests = "^2.27.1"
 requests-kerberos = { version = "^0.14.0", optional = true }
 rich = "^13.3.4"
 sasl = { version = "^0.3.1", optional = true }
 smart-open = { version = "^6.3.0", extras = ["azure", "gcs"], optional = true }
-snowflake-connector-python = { version = "^3.6.0", optional = true }
+snowflake-connector-python = { version = "^3.7.1", optional = true }
 sqlglot = "^10.1.3,<10.4"  # SQL generation doesn't match as >10.4 double quotes are missing
 tenacity = {version ="^8.2.3", optional = true}
 thrift-sasl = { version = "^0.4.3", optional = true }
 typeguard = "^2.13.3"
 typer = "^0.7.0"
-typing-extensions = "4.5.0"
+typing-extensions = "^4.10.0"
 uvicorn = { version = "^0.21.1", extras = ["standard"], optional = true }
 websocket-client = "^1.5.1"
 wheel = "0.40.0"
 
 [tool.poetry.extras]
-server = ["cachetools", "databricks-sdk", "fastapi", "motor", "snowflake-connector-python", "uvicorn", "pdfkit", "pyhive", "sasl", "thrift-sasl", "smart-open", "celery", "redis", "celerybeat-mongo", "databricks-sql-connector", "featurebyte-freeware", "gevent", "aioredis", "requests-kerberos", "tenacity", "feast"]
+server = ["cachetools", "databricks-sdk", "fastapi", "motor", "snowflake-connector-python", "uvicorn", "pdfkit", "pyhive", "sasl", "thrift-sasl", "smart-open", "celery", "redis", "celerybeat-mongo", "databricks-sql-connector", "featurebyte-freeware", "gevent", "requests-kerberos", "tenacity", "feast"]
 
 [tool.poetry.group.dev.dependencies]
 freezegun = "^1.2.1"
 httpx = "^0.24.0"
 junitparser = "^2.8.0"
-jupyterlab = "^4.0.2"
+jupyterlab = "^4.0.11"
 mongomock = "^4.0.0"
 mongomock-motor = "^0.0.12"
 pip-licenses = "^3.5.4"
 pre-commit = "^2.20.0"
 pytest = "^7.2.0"
 pytest-asyncio = "^0.19.0"
 pytest-cov = "^4.0.0"
 pytest-order = "^1.2.0"
 pytest-rerunfailures = "^11.1.2"
+pytest-split = "^0.8.2"
 pytest-timeout = "^2.1.0"
 pytest-xdist = "^3.0.2"
 pyupgrade = "^2.37.2"
 toml-sort = "^0.20.0"
 
 [tool.poetry.group.docs.dependencies]
 docstring-parser = "^0.15"
 humanize = "^4.5.0"
 mkautodoc = "^0.2.0"
-mkdocs = "^1.4.1"
+mkdocs = "1.5.3"
 mkdocs-awesome-pages-plugin = "^2.9.0"
 mkdocs-enumerate-headings-plugin = "^0.5.0"
 mkdocs-gen-files = "^0.4.0"
 mkdocs-git-authors-plugin = "^0.6.5"
 mkdocs-git-revision-date-localized-plugin = "^1.1.0"
 mkdocs-jupyter = "^0.24.2"
 mkdocs-literate-nav = "^0.5.0"
@@ -197,15 +198,17 @@
 types-aiofiles = "^23.1.0.2"
 types-backports = "^0.1.3"
 types-cachetools = "^5.2.1"
 types-chardet = "^5.0.2"
 types-cryptography = "^3.3.21"
 types-decorator = "^5.1.8.4"
 types-paramiko = "^3.2.0.1"
+types-protobuf = "^4.24.0.20240408"
 types-pyOpenSSL = "^22.0.4"
+types-pycurl = "^7.45.2.20240311"
 types-python-dateutil = "^2.8.19.6"
 types-pytz = "^2022.1.2"
 types-redis = "^4.3.7"
 types-requests = "^2.28.2"
 types-setuptools = "^63.2.1"
 types-simplejson = "^3.17.7"
 types-six = "^1.16.21.9"
@@ -226,15 +229,15 @@
 max-attributes = 8
 max-parents = 20
 
 [tool.pylint.format]
 max-line-length = 150
 
 [tool.pylint.main]
-disable = ["Miscellaneous"]
+disable = ["Miscellaneous","wrong-import-order"]
 extension-pkg-whitelist = ["pydantic"]
 ignored-modules = ["featurebyte_freeware"]
 jobs = 0
 output-format = "text"
 recursive = false
 
 [tool.pylint.similarities]
```

### Comparing `featurebyte-1.0.2/PKG-INFO` & `featurebyte-1.0.3/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,72 +1,72 @@
 Metadata-Version: 2.1
 Name: featurebyte
-Version: 1.0.2
+Version: 1.0.3
 Summary: Python Library for FeatureOps
 Home-page: https://featurebyte.com
-License: Elastic License 2.0
+License: Elastic-2.0
 Author: FeatureByte
 Author-email: it-admin@featurebyte.com
-Requires-Python: >=3.8,<3.12
+Requires-Python: >=3.9,<3.12
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
 Classifier: License :: Other/Proprietary License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3.8
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Provides-Extra: server
 Requires-Dist: PyYAML (>=6.0,<7.0)
 Requires-Dist: aiobotocore[boto3] (>=2.4.0,<3.0.0)
 Requires-Dist: aiofiles (>=22.1.0,<23.0.0)
-Requires-Dist: aioredis (>=2.0.1,<3.0.0) ; extra == "server"
 Requires-Dist: alive-progress (>=3.1.1,<4.0.0)
 Requires-Dist: asyncache (>=0.3.1,<0.4.0)
-Requires-Dist: black (>=23.3.0,<24.0.0)
+Requires-Dist: black (>=24.3.0,<25.0.0)
 Requires-Dist: cachetools (>=5.2.0,<6.0.0) ; extra == "server"
 Requires-Dist: celery[redis] (>=5.2.6,<6.0.0) ; extra == "server"
 Requires-Dist: celerybeat-mongo (>=0.2.0,<0.3.0) ; extra == "server"
-Requires-Dist: cryptography (>=41.0.3,<42.0.0)
-Requires-Dist: databricks-sdk (>=0.13.0,<0.14.0) ; extra == "server"
-Requires-Dist: databricks-sql-connector (==2.8.0) ; extra == "server"
+Requires-Dist: cryptography (>=42.0.4,<43.0.0)
+Requires-Dist: databricks-sdk (>=0.24.0,<0.25.0) ; extra == "server"
+Requires-Dist: databricks-sql-connector (>=3.1.1,<4.0.0) ; extra == "server"
 Requires-Dist: fastapi (>=0.96.0,<0.97.0) ; extra == "server"
 Requires-Dist: feast[mysql,redis,snowflake] (>=0.35.0,<0.36.0) ; extra == "server"
 Requires-Dist: featurebyte-freeware (>=0.2.16,<0.3.0) ; extra == "server"
 Requires-Dist: gevent (>=23.9.1,<24.0.0) ; extra == "server"
 Requires-Dist: humanize (>=4.4.0,<5.0.0)
-Requires-Dist: importlib_metadata ; python_version >= "3.8" and python_version < "4.0"
+Requires-Dist: importlib_metadata ; python_version >= "3.9" and python_version < "4.0"
 Requires-Dist: jinja2 (>=3.1.2,<4.0.0)
 Requires-Dist: lazy-object-proxy (>=1.7.1,<2.0.0)
 Requires-Dist: motor (>=3.0.0,<4.0.0) ; extra == "server"
-Requires-Dist: orjson (>=3.8.3,<4.0.0)
+Requires-Dist: orjson (>=3.9.15,<4.0.0)
 Requires-Dist: pandas (>=1.5.3,<2.0.0)
 Requires-Dist: pdfkit (>=1.0.0,<2.0.0) ; extra == "server"
-Requires-Dist: pillow (>=10.2.0,<11.0.0)
+Requires-Dist: pillow (>=10.3.0,<11.0.0)
 Requires-Dist: pyarrow (>=14.0.1,<15.0.0)
 Requires-Dist: pydantic (>=1.9.6,<2.0.0)
 Requires-Dist: pyhive (>=0.6.5,<0.7.0) ; extra == "server"
-Requires-Dist: pymongo (>=4.1.1,<5.0.0)
-Requires-Dist: python-multipart (>=0.0.6,<0.0.7)
+Requires-Dist: pymongo (>=4.6.3,<5.0.0)
+Requires-Dist: pyopenssl (>=24.1.0,<25.0.0)
+Requires-Dist: python-multipart (>=0.0.9,<0.0.10)
 Requires-Dist: python-on-whales (>=0.60.0,<0.61.0)
-Requires-Dist: redis (>=4.5.3,<5.0.0) ; extra == "server"
+Requires-Dist: redis (>=4.2.0,<5.0.0) ; extra == "server"
 Requires-Dist: requests (>=2.27.1,<3.0.0)
 Requires-Dist: requests-kerberos (>=0.14.0,<0.15.0) ; extra == "server"
 Requires-Dist: rich (>=13.3.4,<14.0.0)
 Requires-Dist: sasl (>=0.3.1,<0.4.0) ; extra == "server"
 Requires-Dist: smart-open[azure,gcs] (>=6.3.0,<7.0.0) ; extra == "server"
-Requires-Dist: snowflake-connector-python (>=3.6.0,<4.0.0) ; extra == "server"
+Requires-Dist: snowflake-connector-python (>=3.7.1,<4.0.0) ; extra == "server"
 Requires-Dist: sqlglot (>=10.1.3,<10.4)
 Requires-Dist: tenacity (>=8.2.3,<9.0.0) ; extra == "server"
 Requires-Dist: thrift-sasl (>=0.4.3,<0.5.0) ; extra == "server"
 Requires-Dist: typeguard (>=2.13.3,<3.0.0)
 Requires-Dist: typer (>=0.7.0,<0.8.0)
-Requires-Dist: typing-extensions (==4.5.0)
+Requires-Dist: typing-extensions (>=4.10.0,<5.0.0)
 Requires-Dist: uvicorn[standard] (>=0.21.1,<0.22.0) ; extra == "server"
 Requires-Dist: websocket-client (>=1.5.1,<2.0.0)
 Requires-Dist: wheel (==0.40.0)
 Project-URL: Documentation, https://docs.featurebyte.com
 Project-URL: Repository, https://github.com/featurebyte/featurebyte
 Description-Content-Type: text/markdown
```

