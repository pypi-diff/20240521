# Comparing `tmp/powerlift-0.1.0-py3-none-any.whl.zip` & `tmp/powerlift-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 27886 bytes, number of entries: 20
--rw-rw-rw-  2.0 fat      704 b- defN 24-May-16 06:45 powerlift/bench/__init__.py
--rw-rw-rw-  2.0 fat     6614 b- defN 24-May-16 06:45 powerlift/bench/benchmark.py
--rw-rw-rw-  2.0 fat     6104 b- defN 24-May-16 06:45 powerlift/bench/experiment.py
--rw-rw-rw-  2.0 fat    36544 b- defN 24-May-16 06:45 powerlift/bench/store.py
--rw-rw-rw-  2.0 fat      884 b- defN 24-Feb-29 00:07 powerlift/db/__init__.py
--rw-rw-rw-  2.0 fat     1419 b- defN 24-May-16 06:45 powerlift/db/actions.py
--rw-rw-rw-  2.0 fat     7196 b- defN 24-May-16 06:45 powerlift/db/schema.py
--rw-rw-rw-  2.0 fat      239 b- defN 24-Feb-29 00:07 powerlift/executors/__init__.py
--rw-rw-rw-  2.0 fat     7850 b- defN 24-Feb-29 00:07 powerlift/executors/azure_ci.py
--rw-rw-rw-  2.0 fat     2355 b- defN 24-Mar-18 15:55 powerlift/executors/base.py
--rw-rw-rw-  2.0 fat     3128 b- defN 24-Feb-29 00:07 powerlift/executors/docker.py
--rw-rw-rw-  2.0 fat     3165 b- defN 24-May-16 06:45 powerlift/executors/localmachine.py
--rw-rw-rw-  2.0 fat      140 b- defN 24-Feb-29 00:07 powerlift/measures/__init__.py
--rw-rw-rw-  2.0 fat     4648 b- defN 24-Mar-18 15:55 powerlift/measures/task_measures.py
--rw-rw-rw-  2.0 fat     2712 b- defN 24-May-16 06:45 powerlift/run/__main__.py
--rw-rw-rw-  2.0 fat     1106 b- defN 24-May-16 06:46 powerlift-0.1.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     5897 b- defN 24-May-16 06:46 powerlift-0.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-16 06:46 powerlift-0.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 24-May-16 06:46 powerlift-0.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     1668 b- defN 24-May-16 06:46 powerlift-0.1.0.dist-info/RECORD
-20 files, 92475 bytes uncompressed, 25166 bytes compressed:  72.8%
+Zip file size: 27878 bytes, number of entries: 20
+-rw-r--r--  2.0 unx      712 b- defN 24-May-17 18:23 powerlift/bench/__init__.py
+-rw-r--r--  2.0 unx     6431 b- defN 24-May-12 23:47 powerlift/bench/benchmark.py
+-rw-r--r--  2.0 unx     5888 b- defN 24-May-10 19:49 powerlift/bench/experiment.py
+-rw-r--r--  2.0 unx    36279 b- defN 24-May-20 19:08 powerlift/bench/store.py
+-rw-r--r--  2.0 unx      856 b- defN 24-May-07 19:14 powerlift/db/__init__.py
+-rw-r--r--  2.0 unx     1364 b- defN 24-May-10 18:52 powerlift/db/actions.py
+-rw-r--r--  2.0 unx     6971 b- defN 24-May-10 18:52 powerlift/db/schema.py
+-rw-r--r--  2.0 unx      233 b- defN 24-May-07 19:14 powerlift/executors/__init__.py
+-rw-r--r--  2.0 unx     7639 b- defN 24-May-07 19:14 powerlift/executors/azure_ci.py
+-rw-r--r--  2.0 unx     2275 b- defN 24-May-07 19:14 powerlift/executors/base.py
+-rw-r--r--  2.0 unx     3041 b- defN 24-May-07 19:14 powerlift/executors/docker.py
+-rw-r--r--  2.0 unx     3069 b- defN 24-May-14 00:02 powerlift/executors/localmachine.py
+-rw-r--r--  2.0 unx      136 b- defN 24-May-07 19:14 powerlift/measures/__init__.py
+-rw-r--r--  2.0 unx     4500 b- defN 24-May-07 19:14 powerlift/measures/task_measures.py
+-rw-r--r--  2.0 unx     2639 b- defN 24-May-14 00:03 powerlift/run/__main__.py
+-rw-r--r--  2.0 unx     1085 b- defN 24-May-20 20:20 powerlift-0.1.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5740 b- defN 24-May-20 20:20 powerlift-0.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-20 20:20 powerlift-0.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-May-20 20:20 powerlift-0.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1668 b- defN 24-May-20 20:20 powerlift-0.1.1.dist-info/RECORD
+20 files, 90628 bytes uncompressed, 25158 bytes compressed:  72.2%
```

## zipnote {}

```diff
@@ -39,23 +39,23 @@
 
 Filename: powerlift/measures/task_measures.py
 Comment: 
 
 Filename: powerlift/run/__main__.py
 Comment: 
 
-Filename: powerlift-0.1.0.dist-info/LICENSE
+Filename: powerlift-0.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: powerlift-0.1.0.dist-info/METADATA
+Filename: powerlift-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: powerlift-0.1.0.dist-info/WHEEL
+Filename: powerlift-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: powerlift-0.1.0.dist-info/top_level.txt
+Filename: powerlift-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: powerlift-0.1.0.dist-info/RECORD
+Filename: powerlift-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## powerlift/bench/__init__.py

```diff
@@ -1,19 +1,19 @@
-""" Benchmarking module is responsible for building and running experiments.
-
-The general philosophy is to allow
-methods to be tested on both a single machine in the convenience of a notebook,
-while also providing the same mechanisms across distributed platforms
-when scale becomes a concern.
-
-Key design considerations:
-- API simplicity.
-- Continual feedback to the user.
-- User supplied code should be easily tested.
-"""
-
-from powerlift.bench.experiment import Experiment
-from powerlift.bench.store import Store
-from powerlift.bench.benchmark import Benchmark
-
-from powerlift.bench.store import populate_with_datasets
-from powerlift.bench.store import retrieve_openml, retrieve_pmlb
+""" Benchmarking module is responsible for building and running experiments.
+
+The general philosophy is to allow
+methods to be tested on both a single machine in the convenience of a notebook,
+while also providing the same mechanisms across distributed platforms
+when scale becomes a concern.
+
+Key design considerations:
+- API simplicity.
+- Continual feedback to the user.
+- User supplied code should be easily tested.
+"""
+
+from powerlift.bench.experiment import Experiment
+from powerlift.bench.store import Store
+from powerlift.bench.benchmark import Benchmark
+
+from powerlift.bench.store import populate_with_datasets, DatasetAlreadyExistsError
+from powerlift.bench.store import retrieve_openml, retrieve_pmlb
```

## powerlift/bench/benchmark.py

 * *Ordering differences only*

```diff
@@ -1,183 +1,183 @@
-"""User facing benchmark class."""
-
-from types import FunctionType
-from typing import Optional, Union
-
-from powerlift.bench.experiment import Experiment, Method, Trial
-from powerlift.bench.store import Store
-from powerlift.executors.base import Executor
-from powerlift.executors import LocalMachine
-import pandas as pd
-import random
-
-import os
-
-
-class Benchmark:
-    def __init__(
-        self,
-        store_or_uri: Optional[Union[str, Store]] = None,
-        name: Optional[str] = None,
-        description: Optional[str] = None,
-    ):
-        if store_or_uri is None:
-            self._store = Store(
-                "sqlite:///" + os.path.join(os.getcwd(), "powerlift.db")
-            )
-        elif isinstance(store_or_uri, str):
-            self._store = Store(store_or_uri)
-        elif isinstance(store_or_uri, Store):
-            self._store = store_or_uri
-        else:  # pragma: no cover
-            raise TypeError(f"Incorrect type: {type(store_or_uri)}")
-
-        if name is None:
-            name = f"#{random.randint(0, 9999)}"
-        if description is None:
-            description = f"Experiment: {name}"
-
-        self._name = name
-        self._description = description
-        self._experiment_id = None
-        self._executors = set()
-
-    def run(
-        self,
-        trial_run_fn: FunctionType,
-        trial_gen_fn: FunctionType,
-        timeout: Optional[int] = None,
-        n_replicates: int = 1,
-        executor: Optional[Executor] = None,
-    ) -> Executor:
-        # Create experiment
-        if self._experiment_id is None:
-            self._experiment_id, _ = self._store.get_or_create_experiment(
-                self._name, self._description
-            )
-
-        # Create trials
-        trial_params = []
-        pending_trials = []
-        for task in self._store.iter_tasks():
-            generated_trials = trial_gen_fn(task)
-            for generated_trial in generated_trials:
-                if isinstance(generated_trial, tuple):
-                    # Response: (method, meta)
-                    method = generated_trial[0]
-                    meta = generated_trial[1]
-                else:
-                    # Response: method
-                    method = generated_trial
-                    meta = {}
-                if isinstance(method, str):
-                    method = Method.from_name(method)
-                elif isinstance(method, Method):
-                    pass
-                else:
-                    raise TypeError(f"Cannot handle method type: {type(method)}")
-                method_id, _ = self._store.get_or_create_method(
-                    method.name,
-                    method.description,
-                    method.version,
-                    method.params,
-                    method.env,
-                )
-                method = Method(
-                    method_id,
-                    method.name,
-                    method.description,
-                    method.version,
-                    method.params,
-                    method.env,
-                )
-
-                for replicate_num in range(n_replicates):
-                    trial_param = {
-                        "experiment_id": self._experiment_id,
-                        "task_id": task.id,
-                        "method_id": method.id,
-                        "replicate_num": replicate_num,
-                        "meta": meta,
-                    }
-                    trial_params.append(trial_param)
-                    pending_trials.append(
-                        Trial(None, self._store, task, method, replicate_num, meta, [])
-                    )
-
-        # Save to store
-        trial_ids = self._store.create_trials(trial_params)
-        for _id, trial in zip(trial_ids, pending_trials):
-            trial._id = _id
-
-        # Run trials
-        if executor is None:
-            executor = LocalMachine(self._store)
-        self._executors.add(executor)
-        executor.submit(trial_run_fn, pending_trials, timeout=timeout)
-        return executor
-
-    def wait_until_complete(self):
-        """Wait for all executors to run and then return."""
-        for executor in self._executors:
-            executor.join()
-
-    def _experiment(self) -> Optional[Experiment]:
-        """Retrieves experiment snapshot that contains trial and assets.
-
-        This method is kept private due to an unstable API.
-
-        Returns:
-            Experiment (Optional[Experiment]): Experiment snapshot.
-        """
-        self._experiment_id = self._store.get_experiment(self._name)
-        if self._experiment_id is None:
-            return None
-
-        self._experiment_id, _ = self._store.get_or_create_experiment(
-            self._name, self._description
-        )
-        trials = list(self._store.iter_experiment_trials(self._experiment_id))
-        experiment = Experiment(
-            self._experiment_id, self._name, self._description, trials
-        )
-        return experiment
-
-    def status(self) -> Optional[pd.DataFrame]:
-        """Retrieves all trial's status and associated information.
-
-        Returns:
-            Trial statuses (Optional[pandas.DataFrame]): Experiment's trials' status.
-        """
-        self._experiment_id = self._store.get_experiment(self._name)
-        if self._experiment_id is None:
-            return None
-
-        records = list(self._store.iter_status(self._experiment_id))
-        return pd.DataFrame.from_records(records)
-
-    def results(self) -> Optional[pd.DataFrame]:
-        """Retrieves trial measures of an experiment in long form.
-
-        Returns:
-            Results (pandas.DataFrame): Measures of an experiment in long form.
-        """
-        self._experiment_id = self._store.get_experiment(self._name)
-        if self._experiment_id is None:
-            return None
-
-        records = list(self._store.iter_results(self._experiment_id))
-        return pd.DataFrame.from_records(records)
-
-    def available_tasks(self, include_measures=False) -> Optional[pd.DataFrame]:
-        """Retrieves available tasks to run a benchmark against.
-
-        Args:
-            include_measures (bool): Includes measure columns in long form.
-
-        Returns:
-            Results (pandas.DataFrame): Available tasks including their measures.
-        """
-        records = list(
-            self._store.iter_available_tasks(include_measures=include_measures)
-        )
-        return pd.DataFrame.from_records(records)
+"""User facing benchmark class."""
+
+from types import FunctionType
+from typing import Optional, Union
+
+from powerlift.bench.experiment import Experiment, Method, Trial
+from powerlift.bench.store import Store
+from powerlift.executors.base import Executor
+from powerlift.executors import LocalMachine
+import pandas as pd
+import random
+
+import os
+
+
+class Benchmark:
+    def __init__(
+        self,
+        store_or_uri: Optional[Union[str, Store]] = None,
+        name: Optional[str] = None,
+        description: Optional[str] = None,
+    ):
+        if store_or_uri is None:
+            self._store = Store(
+                "sqlite:///" + os.path.join(os.getcwd(), "powerlift.db")
+            )
+        elif isinstance(store_or_uri, str):
+            self._store = Store(store_or_uri)
+        elif isinstance(store_or_uri, Store):
+            self._store = store_or_uri
+        else:  # pragma: no cover
+            raise TypeError(f"Incorrect type: {type(store_or_uri)}")
+
+        if name is None:
+            name = f"#{random.randint(0, 9999)}"
+        if description is None:
+            description = f"Experiment: {name}"
+
+        self._name = name
+        self._description = description
+        self._experiment_id = None
+        self._executors = set()
+
+    def run(
+        self,
+        trial_run_fn: FunctionType,
+        trial_gen_fn: FunctionType,
+        timeout: Optional[int] = None,
+        n_replicates: int = 1,
+        executor: Optional[Executor] = None,
+    ) -> Executor:
+        # Create experiment
+        if self._experiment_id is None:
+            self._experiment_id, _ = self._store.get_or_create_experiment(
+                self._name, self._description
+            )
+
+        # Create trials
+        trial_params = []
+        pending_trials = []
+        for task in self._store.iter_tasks():
+            generated_trials = trial_gen_fn(task)
+            for generated_trial in generated_trials:
+                if isinstance(generated_trial, tuple):
+                    # Response: (method, meta)
+                    method = generated_trial[0]
+                    meta = generated_trial[1]
+                else:
+                    # Response: method
+                    method = generated_trial
+                    meta = {}
+                if isinstance(method, str):
+                    method = Method.from_name(method)
+                elif isinstance(method, Method):
+                    pass
+                else:
+                    raise TypeError(f"Cannot handle method type: {type(method)}")
+                method_id, _ = self._store.get_or_create_method(
+                    method.name,
+                    method.description,
+                    method.version,
+                    method.params,
+                    method.env,
+                )
+                method = Method(
+                    method_id,
+                    method.name,
+                    method.description,
+                    method.version,
+                    method.params,
+                    method.env,
+                )
+
+                for replicate_num in range(n_replicates):
+                    trial_param = {
+                        "experiment_id": self._experiment_id,
+                        "task_id": task.id,
+                        "method_id": method.id,
+                        "replicate_num": replicate_num,
+                        "meta": meta,
+                    }
+                    trial_params.append(trial_param)
+                    pending_trials.append(
+                        Trial(None, self._store, task, method, replicate_num, meta, [])
+                    )
+
+        # Save to store
+        trial_ids = self._store.create_trials(trial_params)
+        for _id, trial in zip(trial_ids, pending_trials):
+            trial._id = _id
+
+        # Run trials
+        if executor is None:
+            executor = LocalMachine(self._store)
+        self._executors.add(executor)
+        executor.submit(trial_run_fn, pending_trials, timeout=timeout)
+        return executor
+
+    def wait_until_complete(self):
+        """Wait for all executors to run and then return."""
+        for executor in self._executors:
+            executor.join()
+
+    def _experiment(self) -> Optional[Experiment]:
+        """Retrieves experiment snapshot that contains trial and assets.
+
+        This method is kept private due to an unstable API.
+
+        Returns:
+            Experiment (Optional[Experiment]): Experiment snapshot.
+        """
+        self._experiment_id = self._store.get_experiment(self._name)
+        if self._experiment_id is None:
+            return None
+
+        self._experiment_id, _ = self._store.get_or_create_experiment(
+            self._name, self._description
+        )
+        trials = list(self._store.iter_experiment_trials(self._experiment_id))
+        experiment = Experiment(
+            self._experiment_id, self._name, self._description, trials
+        )
+        return experiment
+
+    def status(self) -> Optional[pd.DataFrame]:
+        """Retrieves all trial's status and associated information.
+
+        Returns:
+            Trial statuses (Optional[pandas.DataFrame]): Experiment's trials' status.
+        """
+        self._experiment_id = self._store.get_experiment(self._name)
+        if self._experiment_id is None:
+            return None
+
+        records = list(self._store.iter_status(self._experiment_id))
+        return pd.DataFrame.from_records(records)
+
+    def results(self) -> Optional[pd.DataFrame]:
+        """Retrieves trial measures of an experiment in long form.
+
+        Returns:
+            Results (pandas.DataFrame): Measures of an experiment in long form.
+        """
+        self._experiment_id = self._store.get_experiment(self._name)
+        if self._experiment_id is None:
+            return None
+
+        records = list(self._store.iter_results(self._experiment_id))
+        return pd.DataFrame.from_records(records)
+
+    def available_tasks(self, include_measures=False) -> Optional[pd.DataFrame]:
+        """Retrieves available tasks to run a benchmark against.
+
+        Args:
+            include_measures (bool): Includes measure columns in long form.
+
+        Returns:
+            Results (pandas.DataFrame): Available tasks including their measures.
+        """
+        records = list(
+            self._store.iter_available_tasks(include_measures=include_measures)
+        )
+        return pd.DataFrame.from_records(records)
```

## powerlift/bench/experiment.py

 * *Ordering differences only*

```diff
@@ -1,216 +1,216 @@
-"""Experiment classes for benchmarking.
-
-The experiment and its associate classes.
-"""
-
-import pandas as pd
-from typing import Dict, Iterable
-from typing import Type, TypeVar
-from typing import Union, Optional, List
-from dataclasses import dataclass
-from numbers import Number
-
-from powerlift.bench.store import Store
-
-
-@dataclass(frozen=True)
-class Asset:
-    """Represents an asset (including mimetype) that is produced or used by trial and task objects."""
-
-    id: Optional[int]
-    name: str
-    description: str
-    version: str
-    is_embedded: bool
-    embedded: Optional[bytes]
-    uri: bool
-    mimetype: str
-
-
-@dataclass(frozen=True)
-class Measure:
-    """Represents a measure emitted by a trial or task.
-
-    This is what gets logged by the user in trial runs. Also used by tasks to record statistics (i.e. number of classes for a binary classification task).
-    """
-
-    name: str
-    description: str
-    type: type
-    lower_is_better: bool
-    values: pd.DataFrame
-
-
-@dataclass(frozen=True)
-class Task:
-    """Represents a problem and its associated data. For instance, binary classification on the adult dataset."""
-
-    id: Optional[int]
-    name: str
-    description: str
-    version: str
-    problem: str
-    origin: str
-    config: dict
-    assets: List[Asset]
-    measures: Dict[str, Measure]
-
-    def measure(self, name: str) -> pd.DataFrame:
-        """Returns a named measure as a dataframe with sequence number, timestamp and value for entries.
-
-        Args:
-            name (str): Name of measure.
-
-        Returns:
-            pd.DataFrame: A dataframe with values associated with named measure.
-        """
-        return self.measures[name].values
-
-    def scalar_measure(self, name: str) -> Union[Number, str, dict]:
-        """Returns a named measure as a scalar value.
-
-        Args:
-            name (str): Name of measure.
-
-        Returns:
-            Union[Number, str, dict]: Scalar value associated with named measure.
-        """
-        return self.measures[name].values["val"].iloc[0]
-
-    def data(self, aliases: Iterable[str]) -> List[object]:
-        """Returns assets as a list of objects that maps to the aliases provided.
-
-        Args:
-            aliases (Iterable[str]): Aliases of assets to be retrieved.
-
-        Returns:
-            List[object]: List of objects retrieved by aliases.
-        """
-        from powerlift.bench.store import BytesParser
-
-        outputs = []
-        alias_map = self.config["aliases"]
-        name_to_asset = {asset.name: asset for asset in self.assets}
-        for alias in aliases:
-            name = alias_map[alias]
-            asset = name_to_asset[name]
-            parsed = BytesParser.deserialize(asset.mimetype, asset.embedded)
-            outputs.append(parsed)
-        return outputs
-
-
-T = TypeVar("T", bound="Method")
-
-
-@dataclass(frozen=True)
-class Method:
-    """Represents a method that is ran for a given trial. I.e. support vector machine for a binary classification task."""
-
-    id: Optional[int]
-    name: str
-    description: str
-    version: str
-    params: dict
-    env: dict
-
-    @classmethod
-    def from_name(cls: Type[T], name: str) -> T:
-        """Produces Method object from name.
-
-        Args:
-            name (str): Name of method to be created.
-
-        Returns:
-            Method: Created method.
-        """
-        return cls(None, name, f"Method: {name}", "0.0.1", {}, {})
-
-
-@dataclass(frozen=True)
-class Experiment:
-    """Represents an experiment with its trials."""
-
-    id: Optional[int]
-    name: str
-    description: str
-    trials: List
-
-
-class Trial:
-    def __init__(
-        self,
-        _id: Optional[int],
-        store: Store,
-        task: Task,
-        method: Method,
-        replicate_num: int,
-        meta: dict,
-        input_assets: List[Asset],
-    ):
-        """Represents a single trial within an experiment.
-
-        Args:
-            _id (Optional[int]): ID of trial or None.
-            store (Store): Store to persist measures.
-            task (Task): Task of trial.
-            method (Method): Method of trial.
-            replicate_num (int): Replicate number of trial (when a trial is repeated many times).
-            meta (dict): Metadata associated with the trial.
-            input_assets (List[Asset]): Input assets that are available on trial run.
-        """
-        self._id = _id
-        self._store = store
-        self._task = task
-        self._method = method
-        self._replicate_num = replicate_num
-        self._meta = meta
-        self._input_assets = input_assets
-
-    def log(
-        self,
-        name: str,
-        value: Union[Number, str, dict],
-        description: Optional[str] = None,
-        type_: Union[None, Type, str] = None,
-        lower_is_better: bool = True,
-    ):
-        """Records a measure for the trial. This should be called in the trial run function.
-
-        Args:
-            name (str): Name of measure.
-            value (Union[Number, str, dict]): Value of measure.
-            description (Optional[str], optional): Description of measure. Defaults to None.
-            type_ (Union[None, Type, str], optional): Type of measure. Defaults to None.
-            lower_is_better (bool, optional): Whether the measure is considered better at a lower value. Defaults to True.
-        """
-        self._store.add_measure(
-            self._id, type(self), name, value, description, type_, lower_is_better
-        )
-
-    @property
-    def store(self):
-        return self._store
-
-    @property
-    def task(self):
-        return self._task
-
-    @property
-    def method(self):
-        return self._method
-
-    @property
-    def replicate_num(self):
-        return self._replicate_num
-
-    @property
-    def meta(self):
-        return self._meta
-
-    @property
-    def input_assets(self):
-        return self._input_assets
-
-    @property
-    def id(self):
-        return self._id
+"""Experiment classes for benchmarking.
+
+The experiment and its associate classes.
+"""
+
+import pandas as pd
+from typing import Dict, Iterable
+from typing import Type, TypeVar
+from typing import Union, Optional, List
+from dataclasses import dataclass
+from numbers import Number
+
+from powerlift.bench.store import Store
+
+
+@dataclass(frozen=True)
+class Asset:
+    """Represents an asset (including mimetype) that is produced or used by trial and task objects."""
+
+    id: Optional[int]
+    name: str
+    description: str
+    version: str
+    is_embedded: bool
+    embedded: Optional[bytes]
+    uri: bool
+    mimetype: str
+
+
+@dataclass(frozen=True)
+class Measure:
+    """Represents a measure emitted by a trial or task.
+
+    This is what gets logged by the user in trial runs. Also used by tasks to record statistics (i.e. number of classes for a binary classification task).
+    """
+
+    name: str
+    description: str
+    type: type
+    lower_is_better: bool
+    values: pd.DataFrame
+
+
+@dataclass(frozen=True)
+class Task:
+    """Represents a problem and its associated data. For instance, binary classification on the adult dataset."""
+
+    id: Optional[int]
+    name: str
+    description: str
+    version: str
+    problem: str
+    origin: str
+    config: dict
+    assets: List[Asset]
+    measures: Dict[str, Measure]
+
+    def measure(self, name: str) -> pd.DataFrame:
+        """Returns a named measure as a dataframe with sequence number, timestamp and value for entries.
+
+        Args:
+            name (str): Name of measure.
+
+        Returns:
+            pd.DataFrame: A dataframe with values associated with named measure.
+        """
+        return self.measures[name].values
+
+    def scalar_measure(self, name: str) -> Union[Number, str, dict]:
+        """Returns a named measure as a scalar value.
+
+        Args:
+            name (str): Name of measure.
+
+        Returns:
+            Union[Number, str, dict]: Scalar value associated with named measure.
+        """
+        return self.measures[name].values["val"].iloc[0]
+
+    def data(self, aliases: Iterable[str]) -> List[object]:
+        """Returns assets as a list of objects that maps to the aliases provided.
+
+        Args:
+            aliases (Iterable[str]): Aliases of assets to be retrieved.
+
+        Returns:
+            List[object]: List of objects retrieved by aliases.
+        """
+        from powerlift.bench.store import BytesParser
+
+        outputs = []
+        alias_map = self.config["aliases"]
+        name_to_asset = {asset.name: asset for asset in self.assets}
+        for alias in aliases:
+            name = alias_map[alias]
+            asset = name_to_asset[name]
+            parsed = BytesParser.deserialize(asset.mimetype, asset.embedded)
+            outputs.append(parsed)
+        return outputs
+
+
+T = TypeVar("T", bound="Method")
+
+
+@dataclass(frozen=True)
+class Method:
+    """Represents a method that is ran for a given trial. I.e. support vector machine for a binary classification task."""
+
+    id: Optional[int]
+    name: str
+    description: str
+    version: str
+    params: dict
+    env: dict
+
+    @classmethod
+    def from_name(cls: Type[T], name: str) -> T:
+        """Produces Method object from name.
+
+        Args:
+            name (str): Name of method to be created.
+
+        Returns:
+            Method: Created method.
+        """
+        return cls(None, name, f"Method: {name}", "0.0.1", {}, {})
+
+
+@dataclass(frozen=True)
+class Experiment:
+    """Represents an experiment with its trials."""
+
+    id: Optional[int]
+    name: str
+    description: str
+    trials: List
+
+
+class Trial:
+    def __init__(
+        self,
+        _id: Optional[int],
+        store: Store,
+        task: Task,
+        method: Method,
+        replicate_num: int,
+        meta: dict,
+        input_assets: List[Asset],
+    ):
+        """Represents a single trial within an experiment.
+
+        Args:
+            _id (Optional[int]): ID of trial or None.
+            store (Store): Store to persist measures.
+            task (Task): Task of trial.
+            method (Method): Method of trial.
+            replicate_num (int): Replicate number of trial (when a trial is repeated many times).
+            meta (dict): Metadata associated with the trial.
+            input_assets (List[Asset]): Input assets that are available on trial run.
+        """
+        self._id = _id
+        self._store = store
+        self._task = task
+        self._method = method
+        self._replicate_num = replicate_num
+        self._meta = meta
+        self._input_assets = input_assets
+
+    def log(
+        self,
+        name: str,
+        value: Union[Number, str, dict],
+        description: Optional[str] = None,
+        type_: Union[None, Type, str] = None,
+        lower_is_better: bool = True,
+    ):
+        """Records a measure for the trial. This should be called in the trial run function.
+
+        Args:
+            name (str): Name of measure.
+            value (Union[Number, str, dict]): Value of measure.
+            description (Optional[str], optional): Description of measure. Defaults to None.
+            type_ (Union[None, Type, str], optional): Type of measure. Defaults to None.
+            lower_is_better (bool, optional): Whether the measure is considered better at a lower value. Defaults to True.
+        """
+        self._store.add_measure(
+            self._id, type(self), name, value, description, type_, lower_is_better
+        )
+
+    @property
+    def store(self):
+        return self._store
+
+    @property
+    def task(self):
+        return self._task
+
+    @property
+    def method(self):
+        return self._method
+
+    @property
+    def replicate_num(self):
+        return self._replicate_num
+
+    @property
+    def meta(self):
+        return self._meta
+
+    @property
+    def input_assets(self):
+        return self._input_assets
+
+    @property
+    def id(self):
+        return self._id
```

## powerlift/bench/store.py

```diff
@@ -1,1025 +1,1049 @@
-""" Dataset stores including utility methods. 
-
-The end goal is to allow users to freely register their own benchmarks
-for retrieval by their peers while also providing some basic benchmarks
-for immediate testing.
-
-Currently supported:
-- PMLB
-- OpenML CC18
-
-Near future support:
-- Ikonomovska regression datasets
-- scikit-learn associated datasets
-
-# TODO(nopdive): Review how seq_num (integrity) are done with measure outcomes.
-"""
-
-from collections.abc import Mapping
-import pytz
-import base64
-from dataclasses import dataclass
-from typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, Type
-import random
-import random
-from powerlift.db.actions import drop_tables, create_db, create_tables
-from powerlift.measures import class_stats, data_stats, regression_stats
-from sqlalchemy.exc import IntegrityError
-from tqdm import tqdm
-from itertools import chain
-from sqlalchemy.orm import Session
-import io
-import os
-from powerlift.db import schema as db
-import numbers
-from datetime import datetime
-import pathlib
-import pandas as pd
-import ast
-
-
-@dataclass
-class Wheel:
-    """Python wheel with its name and content as bytes."""
-
-    name: str
-    content: bytes
-
-
-def _parse_function(src):
-    src_ast = ast.parse(src)
-    if isinstance(src_ast, ast.Module) and isinstance(src_ast.body[0], ast.FunctionDef):
-        return src_ast
-    return None
-
-
-def _compile_function(src_ast):
-    func_name = r"wired_function"
-    src_ast.body[0].name = func_name
-    compiled = compile(src_ast, "<string>", "exec")
-    scope = locals()
-    exec(compiled, scope, scope)
-    return locals()[func_name]
-
-
-MIMETYPE_DF = "application/vnd.interpretml/parquet-series"
-MIMETYPE_SERIES = "application/vnd.interpretml/parquet-series"
-MIMETYPE_JSON = "application/json"
-MIMETYPE_FUNC = "application/vnd.interpretml/function-str"
-MIMETYPE_WHEEL = "application/vnd.interpretml/python-wheel"
-
-
-class BytesParser:
-    @classmethod
-    def deserialize(cls, mimetype, bytes):
-        import io
-        import json
-        import pandas as pd
-
-        if not isinstance(bytes, io.BytesIO):
-            bstream = io.BytesIO(bytes)
-        else:
-            bstream = bytes
-
-        if mimetype == MIMETYPE_JSON:
-            return json.load(bstream)
-        elif mimetype == MIMETYPE_DF:
-            return pd.read_parquet(bstream)
-        elif mimetype == MIMETYPE_SERIES:
-            return pd.read_parquet(bstream)["Target"]
-        elif mimetype == MIMETYPE_FUNC:
-            src = bstream.getvalue().decode("utf-8")
-            src_ast = _parse_function(src)
-            if src_ast is None:
-                raise RuntimeError("Serialized code not valid.")
-            compiled_func = _compile_function(src_ast)
-            return compiled_func
-        elif mimetype == MIMETYPE_WHEEL:
-            json_record = json.load(bstream)
-            content = base64.b64decode(json_record["content"].encode("ascii"))
-            return Wheel(json_record["name"], content)
-        else:
-            return None
-
-    @classmethod
-    def serialize(cls, obj):
-        import io
-        import json
-        import pandas as pd
-        from types import FunctionType
-        import inspect
-
-        bstream = io.BytesIO()
-        mimetype = None
-        if isinstance(obj, pd.Series):
-            obj.astype(dtype=object).to_frame(name="Target").to_parquet(bstream)
-            mimetype = MIMETYPE_SERIES
-        elif isinstance(obj, pd.DataFrame):
-            obj.to_parquet(bstream)
-            mimetype = MIMETYPE_DF
-        elif isinstance(obj, dict):
-            bstream.write(json.dumps(obj).encode())
-            mimetype = MIMETYPE_JSON
-        elif isinstance(obj, FunctionType):
-            src = inspect.getsource(obj)
-            src_ast = _parse_function(src)
-            if src_ast is None:
-                raise RuntimeError("Serialized code not valid.")
-            bstream.write(src.encode("utf-8"))
-            mimetype = MIMETYPE_FUNC
-        elif isinstance(obj, Wheel):
-            content = base64.b64encode(obj.content).decode("ascii")
-            json_record = {
-                "name": obj.name,
-                "content": content,
-            }
-            bstream.write(json.dumps(json_record).encode())
-            mimetype = MIMETYPE_WHEEL
-        else:
-            return None, None
-        return mimetype, bstream
-
-
-class Store:
-    """Store that represents persistent state for experiments.
-
-    Apart from initialization, the user should not be using its methods normally.
-    """
-
-    def __init__(self, uri: str, force_recreate: bool = False, **create_engine_kwargs):
-        """Initializes.
-
-        Args:
-            uri (str): Database URI to connect store to.
-            force_recreate (bool, optional): This will delete and create the database associated with the uri if set to true. Defaults to False.
-        """
-        self._engine = create_db(uri, **create_engine_kwargs)
-        if force_recreate:
-            drop_tables(self._engine)
-        create_tables(self._engine)
-
-        self._conn = self._engine.connect()
-        self._session = Session(bind=self._conn)
-
-        self._declared_measures_cache = {}
-        self._measure_counts = {}
-        self._uri = uri
-
-    @property
-    def uri(self):
-        return self._uri
-
-    def __del__(self):
-        self._session.close()
-        self._conn.close()
-
-    def start_trial(self, trial_id):
-        trial_orm = self._session.query(db.Trial).filter_by(id=trial_id).one()
-        start_time = datetime.now(pytz.utc)
-        trial_orm.start_time = start_time
-        trial_orm.status = db.StatusEnum.RUNNING
-        self._session.add(trial_orm)
-        self._session.commit()
-        return start_time
-
-    def end_trial(self, trial_id, errmsg=None):
-        trial_orm = self._session.query(db.Trial).filter_by(id=trial_id).one()
-        end_time = datetime.now(pytz.utc)
-        trial_orm.end_time = end_time
-        if errmsg is not None:
-            trial_orm.errmsg = errmsg
-            trial_orm.status = db.StatusEnum.ERROR
-        else:
-            trial_orm.status = db.StatusEnum.COMPLETE
-
-        self._session.add(trial_orm)
-        self._session.commit()
-        return end_time
-
-    def add_trial_run_fn(self, trial_ids, trial_run_fn, wheel_filepaths=None):
-        import sys
-
-        mimetype, bstream = BytesParser.serialize(trial_run_fn)
-        trial_run_fn_asset_orm = db.Asset(
-            name="trial_run_fn",
-            description="Serialized trial run function.",
-            version=sys.version,
-            is_embedded=True,
-            embedded=bstream.getvalue(),
-            mimetype=mimetype,
-        )
-        wheel_asset_orms = []
-        if wheel_filepaths is not None:
-            for wheel_filepath in wheel_filepaths:
-                with open(wheel_filepath, "rb") as f:
-                    content = f.read()
-                name = pathlib.Path(wheel_filepath).name
-                wheel = Wheel(name, content)
-                mimetype, bstream = BytesParser.serialize(wheel)
-                wheel_asset_orm = db.Asset(
-                    name=name,
-                    description=f"Wheel: {name}",
-                    version=sys.version,
-                    is_embedded=True,
-                    embedded=bstream.getvalue(),
-                    mimetype=mimetype,
-                )
-                wheel_asset_orms.append(wheel_asset_orm)
-
-        trial_orms = self._session.query(db.Trial).filter(db.Trial.id.in_(trial_ids))
-        for trial_orm in trial_orms:
-            trial_orm.input_assets.append(trial_run_fn_asset_orm)
-            if len(wheel_asset_orms) > 0:
-                trial_orm.input_assets.extend(wheel_asset_orms)
-
-        if trial_orms.first() is not None:
-            orms = [trial_run_fn_asset_orm]
-            orms.extend(wheel_asset_orms)
-            self._session.bulk_save_objects(orms, return_defaults=True)
-            self._session.commit()
-        return None
-
-    def measure_from_db_task(self, task_orm):
-        from powerlift.bench.experiment import Measure
-        from collections import defaultdict
-
-        measure_outcomes_orm = task_orm.measure_outcomes
-        desc_id_to_measure_description_orm = {}
-        desc_id_to_values = defaultdict(list)
-        desc_name_to_measure = {}
-
-        for measure_outcome_orm in measure_outcomes_orm:
-            desc_id = measure_outcome_orm.measure_description_id
-            if desc_id not in desc_id_to_measure_description_orm:
-                measure_description_orm = measure_outcome_orm.measure_description
-                desc_id_to_measure_description_orm[desc_id] = measure_description_orm
-            measure_description_orm = desc_id_to_measure_description_orm[desc_id]
-            _type = measure_description_orm.type
-            if _type == db.TypeEnum.NUMBER:
-                val = measure_outcome_orm.num_val
-            elif _type == db.TypeEnum.STR:
-                val = measure_outcome_orm.str_val
-            elif _type == db.TypeEnum.JSON:
-                val = measure_outcome_orm.json_val
-            else:
-                raise RuntimeError("Code branch should be unreachable")
-
-            desc_id_to_values[desc_id].append(
-                {
-                    "seq_num": measure_outcome_orm.seq_num,
-                    "timestamp": measure_outcome_orm.timestamp,
-                    "val": val,
-                }
-            )
-
-        for desc_id in desc_id_to_values.keys():
-            values_df = pd.DataFrame.from_records(
-                desc_id_to_values[desc_id], index="seq_num"
-            )
-            measure_description_orm = desc_id_to_measure_description_orm[desc_id]
-            measure = Measure(
-                measure_description_orm.name,
-                measure_description_orm.description,
-                measure_description_orm.type,
-                measure_description_orm.lower_is_better,
-                values_df,
-            )
-            desc_name_to_measure[measure.name] = measure
-        return desc_name_to_measure
-
-    def from_db_task(self, task_orm):
-        from powerlift.bench.experiment import Task
-
-        assets = [self.from_db_asset(asset) for asset in task_orm.assets]
-        measures = self.measure_from_db_task(task_orm)
-        return Task(
-            task_orm.id,
-            task_orm.name,
-            task_orm.description,
-            task_orm.version,
-            task_orm.problem,
-            task_orm.origin,
-            task_orm.config,
-            assets,
-            measures,
-        )
-
-    def from_db_asset(self, asset_orm):
-        from powerlift.bench.experiment import Asset
-
-        return Asset(
-            asset_orm.id,
-            asset_orm.name,
-            asset_orm.description,
-            asset_orm.version,
-            asset_orm.is_embedded,
-            asset_orm.embedded,
-            asset_orm.uri,
-            asset_orm.mimetype,
-        )
-
-    def from_db_experiment(self, experiment_orm):
-        from powerlift.bench.experiment import Experiment
-
-        return Experiment(
-            self, experiment_orm.name, experiment_orm.description, experiment_orm.id
-        )
-
-    def from_db_method(self, method_orm):
-        from powerlift.bench.experiment import Method
-
-        return Method(
-            method_orm.id,
-            method_orm.name,
-            method_orm.description,
-            method_orm.version,
-            method_orm.params,
-            method_orm.env,
-        )
-
-    def from_db_trial(self, trial_orm):
-        from powerlift.bench.experiment import Trial
-
-        input_assets = [self.from_db_asset(asset) for asset in trial_orm.input_assets]
-        task = self.from_db_task(trial_orm.task)
-        method = self.from_db_method(trial_orm.method)
-        return Trial(
-            trial_orm.id,
-            self,
-            task,
-            method,
-            trial_orm.replicate_num,
-            trial_orm.meta,
-            input_assets,
-        )
-
-    def find_experiment_by_id(self, _id: int):
-        experiment_orm = (
-            self._session.query(db.Experiment).filter_by(id=_id).one_or_none()
-        )
-        if experiment_orm is None:
-            return None
-        return self.from_db_experiment(experiment_orm)
-
-    def find_task_by_id(self, _id: int):
-        task_orm = self._session.query(db.Task).filter_by(id=_id).one_or_none()
-        if task_orm is None:
-            return None
-        return self.from_db_task(task_orm)
-
-    def find_trial_by_id(self, _id: int):
-        trial_orm = self._session.query(db.Trial).filter_by(id=_id).one_or_none()
-        if trial_orm is None:
-            return None
-        return self.from_db_trial(trial_orm)
-
-    def get_experiment(self, name: str) -> Optional[int]:
-        exp_orm = self._session.query(db.Experiment).filter_by(name=name).one_or_none()
-        if exp_orm is None:
-            return None
-        else:
-            return exp_orm.id
-
-    def get_or_create_experiment(self, name: str, description: str) -> Tuple[int, bool]:
-        """Get or create experiment keyed by name."""
-        created = False
-        exp_orm = self._session.query(db.Experiment).filter_by(name=name).one_or_none()
-        if exp_orm is None:
-            created = True
-            exp_orm = db.Experiment(name=name, description=description)
-            try:
-                self._session.add(exp_orm)
-                self._session.commit()
-            except IntegrityError:
-                self._session.rollback()
-                exp_orm = self._session.query(db.Experiment).filter_by(name=name).one()
-        return exp_orm.id, created
-
-    def create_trials(self, trial_params: List[Dict[str, Any]]):
-        trial_orms = []
-        for trial_param in trial_params:
-            trial_orm = db.Trial(
-                status=db.StatusEnum.READY,
-                create_time=datetime.now(pytz.utc),
-                **trial_param,
-            )
-            trial_orms.append(trial_orm)
-        self._session.bulk_save_objects(trial_orms, return_defaults=True)
-        self._session.commit()
-        return [x.id for x in trial_orms]
-
-    def create_trial(
-        self,
-        experiment_id: int,
-        task_id: int,
-        method_id: int,
-        replicate_num: int,
-        meta: dict,
-    ):
-        trial_orm = db.Trial(
-            experiment_id=experiment_id,
-            task_id=task_id,
-            method_id=method_id,
-            replicate_num=replicate_num,
-            meta=meta,
-            status=db.StatusEnum.READY,
-            create_time=datetime.now(pytz.utc),
-        )
-        self._session.add(trial_orm)
-        self._session.commit()
-        return trial_orm.id
-
-    def get_or_create_method(
-        self,
-        name: str,
-        description: str,
-        version: str,
-        params: dict,
-        env: dict,
-    ):
-        """Get or create method keyed by name."""
-
-        created = False
-        method_orm = self._session.query(db.Method).filter_by(name=name).one_or_none()
-        if method_orm is None:
-            created = True
-            method_orm = db.Method(
-                name=name,
-                description=description,
-                version=version,
-                params=params,
-                env=env,
-            )
-            self._session.add(method_orm)
-            self._session.commit()
-        return method_orm.id, created
-
-    def iter_experiment_trials(self, experiment_id: int):
-        trial_orms = self._session.query(db.Trial).filter_by(
-            experiment_id=experiment_id
-        )
-        for trial_orm in trial_orms:
-            trial = self.from_db_trial(trial_orm)
-            yield trial
-
-    def iter_status(self, experiment_id: int) -> Iterable[Mapping[str, object]]:
-        # TODO(nopdive): Should this be in the store?
-        trial_orms = self._session.query(db.Trial).filter_by(
-            experiment_id=experiment_id
-        )
-        for trial_orm in trial_orms:
-            record = {
-                "trial_id": trial_orm.id,
-                "replicate_num": trial_orm.replicate_num,
-                "meta": trial_orm.meta,
-                "method": trial_orm.method.name,
-                "task": trial_orm.task.name,
-                "status": trial_orm.status.name,
-                "errmsg": trial_orm.errmsg,
-                "create_time": trial_orm.create_time,
-                "start_time": trial_orm.start_time,
-                "end_time": trial_orm.end_time,
-            }
-            yield record
-
-    def iter_results(self, experiment_id: int) -> Iterable[Mapping[str, object]]:
-        trial_orms = self._session.query(db.Trial).filter_by(
-            experiment_id=experiment_id
-        )
-        for trial_orm in trial_orms:
-            for measure_outcome in trial_orm.measure_outcomes:
-                record = {
-                    "trial_id": trial_orm.id,
-                    "replicate_num": trial_orm.replicate_num,
-                    "meta": trial_orm.meta,
-                    "method": trial_orm.method.name,
-                    "task": trial_orm.task.name,
-                    "name": measure_outcome.measure_description.name,
-                    "seq_num": measure_outcome.seq_num,
-                    "type": measure_outcome.measure_description.type.name,
-                    "num_val": measure_outcome.num_val,
-                    "str_val": measure_outcome.str_val,
-                    "json_val": measure_outcome.json_val,
-                }
-                yield record
-
-    def iter_available_tasks(
-        self, include_measures: bool = False
-    ) -> Iterable[Mapping[str, object]]:
-        task_orms = self._session.query(db.Task)
-        for task_orm in task_orms:
-            record = {
-                "task_id": task_orm.id,
-                "name": task_orm.name,
-                "version": task_orm.version,
-                "problem": task_orm.problem,
-                "origin": task_orm.origin,
-                "config": task_orm.config,
-            }
-            if include_measures:
-                for measure_outcome in task_orm.measure_outcomes:
-                    record.update(
-                        {
-                            "measure_name": measure_outcome.measure_description.name,
-                            "seq_num": measure_outcome.seq_num,
-                            "type": measure_outcome.measure_description.type.name,
-                            "num_val": measure_outcome.num_val,
-                            "str_val": measure_outcome.str_val,
-                            "json_val": measure_outcome.json_val,
-                        }
-                    )
-            yield record
-
-    def iter_tasks(self):
-        for task_orm in self._session.query(db.Task).all():
-            task = self.from_db_task(task_orm)
-            yield task
-
-    def add_measure(
-        self,
-        trial_or_task_id: int,
-        trial_or_task_type: Type,
-        name,
-        value,
-        description=None,
-        type_=None,
-        lower_is_better=True,
-    ):
-        from powerlift.bench.experiment import Task, Trial
-
-        if type_ is None:
-            if isinstance(value, str):
-                type_ = db.TypeEnum.STR
-            elif isinstance(value, dict):
-                type_ = db.TypeEnum.JSON
-            elif isinstance(value, numbers.Number):
-                type_ = db.TypeEnum.NUMBER
-            else:
-                raise RuntimeError(
-                    f"Value type {type(value)} is not supported for measure"
-                )
-        elif isinstance(type_, str):
-            type_ = db.TypeEnum[type_.upper()]
-
-        # Create measure description if needed
-        is_declared = name in self._declared_measures_cache
-        if not is_declared:
-            if description is None:
-                description = f"Measure: {name}"
-
-            measure_description_orm = (
-                self._session.query(db.MeasureDescription)
-                .filter_by(name=name)
-                .one_or_none()
-            )
-            if measure_description_orm is None:
-                measure_description_orm = db.MeasureDescription(
-                    name=name,
-                    description=description,
-                    type=type_,
-                    lower_is_better=lower_is_better,
-                )
-                try:
-                    self._session.add(measure_description_orm)
-                    self._session.commit()
-                except IntegrityError:
-                    self._session.rollback()
-                    measure_description_orm = (
-                        self._session.query(db.MeasureDescription)
-                        .filter_by(name=name)
-                        .one()
-                    )
-            self._declared_measures_cache[name] = measure_description_orm
-        else:
-            measure_description_orm = self._declared_measures_cache[name]
-
-        # Create measure
-        seq_num = self._measure_counts[name] = self._measure_counts.get(name, -1) + 1
-        timestamp = datetime.now(pytz.utc)
-        measure_outcome_orm = db.MeasureOutcome(
-            measure_description=measure_description_orm,
-            timestamp=timestamp,
-            seq_num=seq_num,
-        )
-        self._session.add(measure_outcome_orm)
-
-        if type_ == db.TypeEnum.STR:
-            measure_outcome_orm.str_val = value
-        elif type_ == db.TypeEnum.JSON:
-            measure_outcome_orm.json_val = value
-        elif type_ == db.TypeEnum.NUMBER:
-            measure_outcome_orm.num_val = value
-        else:
-            raise RuntimeError(f"Value type {type(value)} is not supported for measure")
-
-        if trial_or_task_type == Task:
-            db_type = db.Task
-        elif trial_or_task_type == Trial:
-            db_type = db.Trial
-        else:
-            raise RuntimeError(f"Type {trial_or_task_type} is not Task nor Trial")
-
-        trial_or_task_orm = (
-            self._session.query(db_type).filter_by(id=trial_or_task_id).one()
-        )
-        trial_or_task_orm.measure_outcomes.append(measure_outcome_orm)
-        if not is_declared:
-            self._session.add(measure_description_orm)
-        self._session.commit()
-        return measure_outcome_orm.id
-
-    def create_task_with_data(self, data, version="0.0.1"):
-        if isinstance(data, SupervisedDataset):
-            return self._create_task_with_supervised(data, version)
-        elif isinstance(data, DataFrameDataset):
-            return self._create_task_with_dataframe(data, version)
-        else:  # pragma: no cover
-            raise ValueError(f"Does not support {type(data)}")
-
-    def _create_task_with_supervised(self, supervised, version):
-        X_bstream, y_bstream, meta_bstream = SupervisedDataset.serialize(supervised)
-        X_name, y_name, meta_name = supervised.asset_names()
-        X_mimetype, y_mimetype, meta_mimetype = supervised.mimetypes()
-
-        X_orm = db.Asset(
-            name=X_name,
-            description=f"Training data for {supervised.name()}",
-            version=version,
-            is_embedded=True,
-            mimetype=X_mimetype,
-            embedded=X_bstream.getvalue(),
-        )
-        y_orm = db.Asset(
-            name=y_name,
-            description=f"Labels for {supervised.name()}",
-            version=version,
-            is_embedded=True,
-            mimetype=y_mimetype,
-            embedded=y_bstream.getvalue(),
-        )
-
-        meta_orm = db.Asset(
-            name=meta_name,
-            description=f"Metadata for {supervised.name()}",
-            version=version,
-            is_embedded=True,
-            mimetype=meta_mimetype,
-            embedded=meta_bstream.getvalue(),
-        )
-
-        meta = supervised.meta
-        task_orm = db.Task(
-            name=meta["name"],
-            description=f"Dataset {meta['name']} for {meta['problem']}",
-            version=version,
-            problem=meta["problem"],
-            origin=meta["source"],
-            config={
-                "type": "data_supervised",
-                "aliases": {
-                    "X": X_name,
-                    "y": y_name,
-                    "meta": meta_name,
-                },
-            },
-        )
-        task_orm.assets.append(X_orm)
-        task_orm.assets.append(y_orm)
-        task_orm.assets.append(meta_orm)
-
-        self._session.add(X_orm)
-        self._session.add(y_orm)
-        self._session.add(task_orm)
-        self._session.commit()
-
-        return task_orm.id
-
-    def _create_task_with_dataframe(self, data, version):
-        inputs_bstream, outputs_bstream, meta_bstream = DataFrameDataset.serialize(data)
-        inputs_name, outputs_name, meta_name = data.asset_names()
-        inputs_mimetype, outputs_mimetype, meta_mimetype = data.mimetypes()
-
-        inputs_orm = db.Asset(
-            name=inputs_name,
-            description=f"Inputs for {data.name()}",
-            version=version,
-            is_embedded=True,
-            mimetype=inputs_mimetype,
-            embedded=inputs_bstream.getvalue(),
-        )
-        outputs_orm = db.Asset(
-            name=outputs_name,
-            description=f"Outputs for {data.name()}",
-            version=version,
-            is_embedded=True,
-            mimetype=outputs_mimetype,
-            embedded=outputs_bstream.getvalue(),
-        )
-
-        meta_orm = db.Asset(
-            name=meta_name,
-            description=f"Metadata for {data.name()}",
-            version=version,
-            is_embedded=True,
-            mimetype=meta_mimetype,
-            embedded=meta_bstream.getvalue(),
-        )
-
-        meta = data.meta
-        task_orm = db.Task(
-            name=meta["name"],
-            description=f"Dataset {meta['name']} for {meta['problem']}",
-            version=version,
-            problem=meta["problem"],
-            origin=meta["source"],
-            config={
-                "type": "data_dataframe",
-                "aliases": {
-                    "inputs": inputs_name,
-                    "outputs": outputs_name,
-                    "meta": meta_name,
-                },
-            },
-        )
-        task_orm.assets.append(inputs_orm)
-        task_orm.assets.append(outputs_orm)
-        task_orm.assets.append(meta_orm)
-
-        self._session.add(inputs_orm)
-        self._session.add(outputs_orm)
-        self._session.add(task_orm)
-        self._session.commit()
-
-        return task_orm.id
-
-
-def populate_task_measures(store, task_id, data):
-    from powerlift.bench.experiment import Task
-
-    meta = data.meta
-    unprocessed_measures = []
-    if isinstance(data, SupervisedDataset):
-        if meta["problem"] == "regression":
-            unprocessed_measures.extend(regression_stats(data.y))
-        elif meta["problem"] in ["binary", "multiclass"]:
-            unprocessed_measures.extend(class_stats(data.y))
-        unprocessed_measures.extend(data_stats(data.X, meta["categorical_mask"]))
-    elif isinstance(data, DataFrameDataset):
-        inputs_data_stats = [
-            (f"inputs_{x1}", x2, x3, x4)
-            for x1, x2, x3, x4 in data_stats(
-                data.inputs, meta["inputs_categorical_mask"]
-            )
-        ]
-        outputs_data_stats = [
-            (f"outputs_{x1}", x2, x3, x4)
-            for x1, x2, x3, x4 in data_stats(
-                data.outputs, meta["outputs_categorical_mask"]
-            )
-        ]
-        unprocessed_measures.extend(inputs_data_stats)
-        unprocessed_measures.extend(outputs_data_stats)
-
-    for unprocessed_measure in unprocessed_measures:
-        name, description, value, lower_is_better = unprocessed_measure
-        store.add_measure(
-            task_id,
-            Task,
-            name,
-            value,
-            description=description,
-            lower_is_better=lower_is_better,
-        )
-
-
-def retrieve_cache(
-    cache_dir: Optional[str], names: List[str]
-) -> Optional[List[io.BytesIO]]:
-    if cache_dir is None:
-        return None
-
-    cache_dir = pathlib.Path(os.path.expanduser(cache_dir))
-    cache_dir.mkdir(parents=True, exist_ok=True)
-    outputs = []
-    for name in names:
-        filepath = pathlib.Path(cache_dir, name)
-        if filepath.exists():
-            with open(filepath, "rb") as f:
-                outputs.append(io.BytesIO(f.read()))
-        else:
-            return None
-    return outputs
-
-
-def update_cache(cache_dir, names: List[str], bytes_io: List[io.BytesIO]):
-    cache_dir = pathlib.Path(os.path.expanduser(cache_dir))
-    for name, a_bytes_io in zip(names, bytes_io):
-        filepath = pathlib.Path(cache_dir, name)
-        with open(filepath, "wb") as f:
-            f.write(a_bytes_io.getvalue())
-
-
-class Dataset:
-    pass
-
-
-@dataclass
-class DataFrameDataset(Dataset):
-    inputs: pd.DataFrame
-    outputs: pd.DataFrame
-    meta: dict
-
-    @classmethod
-    def serialize(cls, obj):
-        _, inputs_bstream = BytesParser.serialize(obj.inputs)
-        _, outputs_bstream = BytesParser.serialize(obj.outputs)
-        _, meta_bstream = BytesParser.serialize(obj.meta)
-        return inputs_bstream, outputs_bstream, meta_bstream
-
-    @classmethod
-    def deserialize(
-        cls,
-        inputs_bstream: io.BytesIO,
-        outputs_bstream: io.BytesIO,
-        meta_bstream: io.BytesIO,
-    ):
-        inputs = BytesParser.deserialize(MIMETYPE_DF, inputs_bstream)
-        outputs = BytesParser.deserialize(MIMETYPE_DF, outputs_bstream)
-        meta = BytesParser.deserialize(MIMETYPE_JSON, meta_bstream)
-        return cls(inputs, outputs, meta)
-
-    def asset_names(self):
-        inputs_name = f"{self.meta['name']}.inputs.parquet"
-        outputs_name = f"{self.meta['name']}.outputs.parquet"
-        meta_name = f"{self.meta['name']}.meta.json"
-        return inputs_name, outputs_name, meta_name
-
-    def mimetypes(self):
-        inputs_metadata = MIMETYPE_DF
-        outputs_metadata = MIMETYPE_DF
-        meta_metadata = MIMETYPE_JSON
-        return inputs_metadata, outputs_metadata, meta_metadata
-
-    def name(self):
-        return self.meta["name"]
-
-
-@dataclass
-class SupervisedDataset(Dataset):
-    X: pd.DataFrame
-    y: pd.Series
-    meta: dict
-
-    @classmethod
-    def serialize(cls, obj):
-        _, X_bstream = BytesParser.serialize(obj.X)
-        _, y_bstream = BytesParser.serialize(obj.y)
-        _, meta_bstream = BytesParser.serialize(obj.meta)
-        return X_bstream, y_bstream, meta_bstream
-
-    @classmethod
-    def deserialize(
-        cls, X_bstream: io.BytesIO, y_bstream: io.BytesIO, meta_bstream: io.BytesIO
-    ):
-        X = BytesParser.deserialize(MIMETYPE_DF, X_bstream)
-        y = BytesParser.deserialize(MIMETYPE_SERIES, y_bstream)
-        meta = BytesParser.deserialize(MIMETYPE_JSON, meta_bstream)
-        return cls(X, y, meta)
-
-    def asset_names(self):
-        X_name = f"{self.meta['name']}.X.parquet"
-        y_name = f"{self.meta['name']}.y.parquet"
-        meta_name = f"{self.meta['name']}.meta.json"
-        return X_name, y_name, meta_name
-
-    def mimetypes(self):
-        X_metadata = MIMETYPE_DF
-        y_metadata = MIMETYPE_SERIES
-        meta_metadata = MIMETYPE_JSON
-        return X_metadata, y_metadata, meta_metadata
-
-    def name(self):
-        return self.meta["name"]
-
-
-def populate_with_datasets(
-    store: Store,
-    dataset_iter: Iterable[Dataset] = None,
-    cache_dir: str = None,
-):
-    """Populates store with datasets.
-
-    Args:
-        store (Store): Store for experiment.
-        dataset_iter (Iterable[Dataset], optional): Iterable of supervised datasets. Defaults to None, which populates with OpenML and PMLB.
-        cache_dir (str, optional): If dataset_iter is None, use this cache directory across calls. Defaults to None.
-    """
-    if dataset_iter is None:
-        dataset_iter = chain(
-            retrieve_openml(cache_dir=cache_dir), retrieve_pmlb(cache_dir=cache_dir)
-        )
-
-    for dataset in dataset_iter:
-        task_id = store.create_task_with_data(dataset)
-        populate_task_measures(store, task_id, dataset)
-
-
-def retrieve_openml(cache_dir: str = None) -> Generator[SupervisedDataset, None, None]:
-    """Retrives OpenML CC18 datasets.
-
-    Args:
-        cache_dir (str, optional): Use this cache directory across calls. Defaults to None.
-
-    Yields:
-        Generator[SupervisedDataset]: Yields datasets.
-    """
-    import openml
-
-    if cache_dir is not None:
-        cache_dir = pathlib.Path(cache_dir, "openml")
-
-    suite = openml.study.get_suite(99)
-    tasks = suite.tasks.copy()
-    random.Random(1337).shuffle(tasks)
-    for task_id in tqdm(tasks, desc="openml"):
-        task = openml.tasks.get_task(task_id)
-        dataset = openml.datasets.get_dataset(task.dataset_id, download_data=False)
-        name = dataset.name
-        X_name = f"{name}.X.parquet"
-        y_name = f"{name}.y.parquet"
-        meta_name = f"{name}.meta.json"
-        problem = (
-            "binary" if dataset.qualities["NumberOfClasses"] == 2 else "multiclass"
-        )
-
-        cached = retrieve_cache(cache_dir, [X_name, y_name, meta_name])
-        if cached is None:
-            X, y, categorical_mask, feature_names = task.get_dataset().get_data(
-                target=task.target_name, dataset_format="dataframe"
-            )
-            meta = {
-                "name": name,
-                "problem": problem,
-                "source": "openml",
-                "categorical_mask": categorical_mask,
-                "feature_names": feature_names,
-            }
-            supervised = SupervisedDataset(X, y, meta)
-        else:
-            supervised = SupervisedDataset.deserialize(*cached)
-
-        if cache_dir is not None:
-            serialized = SupervisedDataset.serialize(supervised)
-            update_cache(cache_dir, [X_name, y_name, meta_name], serialized)
-        yield supervised
-
-
-def retrieve_pmlb(cache_dir: str = None) -> Generator[SupervisedDataset, None, None]:
-    """Retrieves PMLB regression and classification datasets.
-
-    Args:
-        cache_dir (str, optional): Use this cache directory across calls. Defaults to None.
-
-    Yields:
-        Generator[SupervisedDataset]: Yields datasets.
-    """
-    from pmlb import fetch_data, classification_dataset_names, regression_dataset_names
-
-    if cache_dir is not None:
-        cache_dir = pathlib.Path(cache_dir, "pmlb")
-
-    dataset_names = []
-    dataset_names.extend(
-        [("classification", name) for name in classification_dataset_names]
-    )
-    dataset_names.extend([("regression", name) for name in regression_dataset_names])
-
-    for problem_type, dataset_name in tqdm(dataset_names, desc="pmlb"):
-        name = dataset_name
-        X_name = f"{name}.X.parquet"
-        y_name = f"{name}.y.parquet"
-        meta_name = f"{name}.meta.json"
-
-        cached = retrieve_cache(cache_dir, [X_name, y_name, meta_name])
-        if cached is None:
-            df = fetch_data(dataset_name)
-            X = df.drop("target", axis=1)
-            y = df["target"]
-            problem = problem_type
-            if problem_type == "classification":
-                problem = "binary" if len(y.unique()) == 2 else "multiclass"
-            meta = {
-                "name": name,
-                "problem": problem,
-                "source": "pmlb",
-                "categorical_mask": [dt.kind == "O" for dt in X.dtypes],
-                "feature_names": list(X.columns),
-            }
-            supervised = SupervisedDataset(X, y, meta)
-        else:
-            supervised = SupervisedDataset.deserialize(*cached)
-
-        if cache_dir is not None:
-            serialized = SupervisedDataset.serialize(supervised)
-            update_cache(cache_dir, [X_name, y_name, meta_name], serialized)
-        yield supervised
+""" Dataset stores including utility methods. 
+
+The end goal is to allow users to freely register their own benchmarks
+for retrieval by their peers while also providing some basic benchmarks
+for immediate testing.
+
+Currently supported:
+- PMLB
+- OpenML CC18
+
+Near future support:
+- Ikonomovska regression datasets
+- scikit-learn associated datasets
+
+# TODO(nopdive): Review how seq_num (integrity) are done with measure outcomes.
+"""
+
+from collections.abc import Mapping
+import pytz
+import base64
+from dataclasses import dataclass
+from typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, Type
+import random
+import random
+from powerlift.db.actions import drop_tables, create_db, create_tables
+from powerlift.measures import class_stats, data_stats, regression_stats
+from sqlalchemy.exc import IntegrityError
+from tqdm import tqdm
+from itertools import chain
+from sqlalchemy.orm import Session
+import io
+import os
+from powerlift.db import schema as db
+import numbers
+from datetime import datetime
+import pathlib
+import pandas as pd
+import ast
+
+
+@dataclass
+class Wheel:
+    """Python wheel with its name and content as bytes."""
+
+    name: str
+    content: bytes
+
+
+def _parse_function(src):
+    src_ast = ast.parse(src)
+    if isinstance(src_ast, ast.Module) and isinstance(src_ast.body[0], ast.FunctionDef):
+        return src_ast
+    return None
+
+
+def _compile_function(src_ast):
+    func_name = r"wired_function"
+    src_ast.body[0].name = func_name
+    compiled = compile(src_ast, "<string>", "exec")
+    scope = locals()
+    exec(compiled, scope, scope)
+    return locals()[func_name]
+
+
+MIMETYPE_DF = "application/vnd.interpretml/parquet-series"
+MIMETYPE_SERIES = "application/vnd.interpretml/parquet-series"
+MIMETYPE_JSON = "application/json"
+MIMETYPE_FUNC = "application/vnd.interpretml/function-str"
+MIMETYPE_WHEEL = "application/vnd.interpretml/python-wheel"
+
+
+class BytesParser:
+    @classmethod
+    def deserialize(cls, mimetype, bytes):
+        import io
+        import json
+        import pandas as pd
+
+        if not isinstance(bytes, io.BytesIO):
+            bstream = io.BytesIO(bytes)
+        else:
+            bstream = bytes
+
+        if mimetype == MIMETYPE_JSON:
+            return json.load(bstream)
+        elif mimetype == MIMETYPE_DF:
+            return pd.read_parquet(bstream)
+        elif mimetype == MIMETYPE_SERIES:
+            return pd.read_parquet(bstream)["Target"]
+        elif mimetype == MIMETYPE_FUNC:
+            src = bstream.getvalue().decode("utf-8")
+            src_ast = _parse_function(src)
+            if src_ast is None:
+                raise RuntimeError("Serialized code not valid.")
+            compiled_func = _compile_function(src_ast)
+            return compiled_func
+        elif mimetype == MIMETYPE_WHEEL:
+            json_record = json.load(bstream)
+            content = base64.b64decode(json_record["content"].encode("ascii"))
+            return Wheel(json_record["name"], content)
+        else:
+            return None
+
+    @classmethod
+    def serialize(cls, obj):
+        import io
+        import json
+        import pandas as pd
+        from types import FunctionType
+        import inspect
+
+        bstream = io.BytesIO()
+        mimetype = None
+        if isinstance(obj, pd.Series):
+            obj.astype(dtype=object).to_frame(name="Target").to_parquet(bstream)
+            mimetype = MIMETYPE_SERIES
+        elif isinstance(obj, pd.DataFrame):
+            obj.to_parquet(bstream)
+            mimetype = MIMETYPE_DF
+        elif isinstance(obj, dict):
+            bstream.write(json.dumps(obj).encode())
+            mimetype = MIMETYPE_JSON
+        elif isinstance(obj, FunctionType):
+            src = inspect.getsource(obj)
+            src_ast = _parse_function(src)
+            if src_ast is None:
+                raise RuntimeError("Serialized code not valid.")
+            bstream.write(src.encode("utf-8"))
+            mimetype = MIMETYPE_FUNC
+        elif isinstance(obj, Wheel):
+            content = base64.b64encode(obj.content).decode("ascii")
+            json_record = {
+                "name": obj.name,
+                "content": content,
+            }
+            bstream.write(json.dumps(json_record).encode())
+            mimetype = MIMETYPE_WHEEL
+        else:
+            return None, None
+        return mimetype, bstream
+
+
+class Store:
+    """Store that represents persistent state for experiments.
+
+    Apart from initialization, the user should not be using its methods normally.
+    """
+
+    def __init__(self, uri: str, force_recreate: bool = False, **create_engine_kwargs):
+        """Initializes.
+
+        Args:
+            uri (str): Database URI to connect store to.
+            force_recreate (bool, optional): This will delete and create the database associated with the uri if set to true. Defaults to False.
+        """
+        self._engine = create_db(uri, **create_engine_kwargs)
+        if force_recreate:
+            drop_tables(self._engine)
+        create_tables(self._engine)
+
+        self._conn = self._engine.connect()
+        self._session = Session(bind=self._conn)
+
+        self._declared_measures_cache = {}
+        self._measure_counts = {}
+        self._uri = uri
+
+    @property
+    def uri(self):
+        return self._uri
+
+    def __del__(self):
+        self._session.close()
+        self._conn.close()
+
+    def rollback(self):
+        self._session.rollback()
+
+    def start_trial(self, trial_id):
+        trial_orm = self._session.query(db.Trial).filter_by(id=trial_id).one()
+        start_time = datetime.now(pytz.utc)
+        trial_orm.start_time = start_time
+        trial_orm.status = db.StatusEnum.RUNNING
+        self._session.add(trial_orm)
+        self._session.commit()
+        return start_time
+
+    def end_trial(self, trial_id, errmsg=None):
+        trial_orm = self._session.query(db.Trial).filter_by(id=trial_id).one()
+        end_time = datetime.now(pytz.utc)
+        trial_orm.end_time = end_time
+        if errmsg is not None:
+            trial_orm.errmsg = errmsg
+            trial_orm.status = db.StatusEnum.ERROR
+        else:
+            trial_orm.status = db.StatusEnum.COMPLETE
+
+        self._session.add(trial_orm)
+        self._session.commit()
+        return end_time
+
+    def add_trial_run_fn(self, trial_ids, trial_run_fn, wheel_filepaths=None):
+        import sys
+
+        mimetype, bstream = BytesParser.serialize(trial_run_fn)
+        trial_run_fn_asset_orm = db.Asset(
+            name="trial_run_fn",
+            description="Serialized trial run function.",
+            version=sys.version,
+            is_embedded=True,
+            embedded=bstream.getvalue(),
+            mimetype=mimetype,
+        )
+        wheel_asset_orms = []
+        if wheel_filepaths is not None:
+            for wheel_filepath in wheel_filepaths:
+                with open(wheel_filepath, "rb") as f:
+                    content = f.read()
+                name = pathlib.Path(wheel_filepath).name
+                wheel = Wheel(name, content)
+                mimetype, bstream = BytesParser.serialize(wheel)
+                wheel_asset_orm = db.Asset(
+                    name=name,
+                    description=f"Wheel: {name}",
+                    version=sys.version,
+                    is_embedded=True,
+                    embedded=bstream.getvalue(),
+                    mimetype=mimetype,
+                )
+                wheel_asset_orms.append(wheel_asset_orm)
+
+        trial_orms = self._session.query(db.Trial).filter(db.Trial.id.in_(trial_ids))
+        for trial_orm in trial_orms:
+            trial_orm.input_assets.append(trial_run_fn_asset_orm)
+            if len(wheel_asset_orms) > 0:
+                trial_orm.input_assets.extend(wheel_asset_orms)
+
+        if trial_orms.first() is not None:
+            orms = [trial_run_fn_asset_orm]
+            orms.extend(wheel_asset_orms)
+            self._session.bulk_save_objects(orms, return_defaults=True)
+            self._session.commit()
+        return None
+
+    def measure_from_db_task(self, task_orm):
+        from powerlift.bench.experiment import Measure
+        from collections import defaultdict
+
+        measure_outcomes_orm = task_orm.measure_outcomes
+        desc_id_to_measure_description_orm = {}
+        desc_id_to_values = defaultdict(list)
+        desc_name_to_measure = {}
+
+        for measure_outcome_orm in measure_outcomes_orm:
+            desc_id = measure_outcome_orm.measure_description_id
+            if desc_id not in desc_id_to_measure_description_orm:
+                measure_description_orm = measure_outcome_orm.measure_description
+                desc_id_to_measure_description_orm[desc_id] = measure_description_orm
+            measure_description_orm = desc_id_to_measure_description_orm[desc_id]
+            _type = measure_description_orm.type
+            if _type == db.TypeEnum.NUMBER:
+                val = measure_outcome_orm.num_val
+            elif _type == db.TypeEnum.STR:
+                val = measure_outcome_orm.str_val
+            elif _type == db.TypeEnum.JSON:
+                val = measure_outcome_orm.json_val
+            else:
+                raise RuntimeError("Code branch should be unreachable")
+
+            desc_id_to_values[desc_id].append(
+                {
+                    "seq_num": measure_outcome_orm.seq_num,
+                    "timestamp": measure_outcome_orm.timestamp,
+                    "val": val,
+                }
+            )
+
+        for desc_id in desc_id_to_values.keys():
+            values_df = pd.DataFrame.from_records(
+                desc_id_to_values[desc_id], index="seq_num"
+            )
+            measure_description_orm = desc_id_to_measure_description_orm[desc_id]
+            measure = Measure(
+                measure_description_orm.name,
+                measure_description_orm.description,
+                measure_description_orm.type,
+                measure_description_orm.lower_is_better,
+                values_df,
+            )
+            desc_name_to_measure[measure.name] = measure
+        return desc_name_to_measure
+
+    def from_db_task(self, task_orm):
+        from powerlift.bench.experiment import Task
+
+        assets = [self.from_db_asset(asset) for asset in task_orm.assets]
+        measures = self.measure_from_db_task(task_orm)
+        return Task(
+            task_orm.id,
+            task_orm.name,
+            task_orm.description,
+            task_orm.version,
+            task_orm.problem,
+            task_orm.origin,
+            task_orm.config,
+            assets,
+            measures,
+        )
+
+    def from_db_asset(self, asset_orm):
+        from powerlift.bench.experiment import Asset
+
+        return Asset(
+            asset_orm.id,
+            asset_orm.name,
+            asset_orm.description,
+            asset_orm.version,
+            asset_orm.is_embedded,
+            asset_orm.embedded,
+            asset_orm.uri,
+            asset_orm.mimetype,
+        )
+
+    def from_db_experiment(self, experiment_orm):
+        from powerlift.bench.experiment import Experiment
+
+        return Experiment(
+            self, experiment_orm.name, experiment_orm.description, experiment_orm.id
+        )
+
+    def from_db_method(self, method_orm):
+        from powerlift.bench.experiment import Method
+
+        return Method(
+            method_orm.id,
+            method_orm.name,
+            method_orm.description,
+            method_orm.version,
+            method_orm.params,
+            method_orm.env,
+        )
+
+    def from_db_trial(self, trial_orm):
+        from powerlift.bench.experiment import Trial
+
+        input_assets = [self.from_db_asset(asset) for asset in trial_orm.input_assets]
+        task = self.from_db_task(trial_orm.task)
+        method = self.from_db_method(trial_orm.method)
+        return Trial(
+            trial_orm.id,
+            self,
+            task,
+            method,
+            trial_orm.replicate_num,
+            trial_orm.meta,
+            input_assets,
+        )
+
+    def find_experiment_by_id(self, _id: int):
+        experiment_orm = (
+            self._session.query(db.Experiment).filter_by(id=_id).one_or_none()
+        )
+        if experiment_orm is None:
+            return None
+        return self.from_db_experiment(experiment_orm)
+
+    def find_task_by_id(self, _id: int):
+        task_orm = self._session.query(db.Task).filter_by(id=_id).one_or_none()
+        if task_orm is None:
+            return None
+        return self.from_db_task(task_orm)
+
+    def find_trial_by_id(self, _id: int):
+        trial_orm = self._session.query(db.Trial).filter_by(id=_id).one_or_none()
+        if trial_orm is None:
+            return None
+        return self.from_db_trial(trial_orm)
+
+    def get_experiment(self, name: str) -> Optional[int]:
+        exp_orm = self._session.query(db.Experiment).filter_by(name=name).one_or_none()
+        if exp_orm is None:
+            return None
+        else:
+            return exp_orm.id
+
+    def get_or_create_experiment(self, name: str, description: str) -> Tuple[int, bool]:
+        """Get or create experiment keyed by name."""
+        created = False
+        exp_orm = self._session.query(db.Experiment).filter_by(name=name).one_or_none()
+        if exp_orm is None:
+            created = True
+            exp_orm = db.Experiment(name=name, description=description)
+            try:
+                self._session.add(exp_orm)
+                self._session.commit()
+            except IntegrityError:
+                self._session.rollback()
+                exp_orm = self._session.query(db.Experiment).filter_by(name=name).one()
+        return exp_orm.id, created
+
+    def create_trials(self, trial_params: List[Dict[str, Any]]):
+        trial_orms = []
+        for trial_param in trial_params:
+            trial_orm = db.Trial(
+                status=db.StatusEnum.READY,
+                create_time=datetime.now(pytz.utc),
+                **trial_param,
+            )
+            trial_orms.append(trial_orm)
+        self._session.bulk_save_objects(trial_orms, return_defaults=True)
+        self._session.commit()
+        return [x.id for x in trial_orms]
+
+    def create_trial(
+        self,
+        experiment_id: int,
+        task_id: int,
+        method_id: int,
+        replicate_num: int,
+        meta: dict,
+    ):
+        trial_orm = db.Trial(
+            experiment_id=experiment_id,
+            task_id=task_id,
+            method_id=method_id,
+            replicate_num=replicate_num,
+            meta=meta,
+            status=db.StatusEnum.READY,
+            create_time=datetime.now(pytz.utc),
+        )
+        self._session.add(trial_orm)
+        self._session.commit()
+        return trial_orm.id
+
+    def get_or_create_method(
+        self,
+        name: str,
+        description: str,
+        version: str,
+        params: dict,
+        env: dict,
+    ):
+        """Get or create method keyed by name."""
+
+        created = False
+        method_orm = self._session.query(db.Method).filter_by(name=name).one_or_none()
+        if method_orm is None:
+            created = True
+            method_orm = db.Method(
+                name=name,
+                description=description,
+                version=version,
+                params=params,
+                env=env,
+            )
+            self._session.add(method_orm)
+            self._session.commit()
+        return method_orm.id, created
+
+    def iter_experiment_trials(self, experiment_id: int):
+        trial_orms = self._session.query(db.Trial).filter_by(
+            experiment_id=experiment_id
+        )
+        for trial_orm in trial_orms:
+            trial = self.from_db_trial(trial_orm)
+            yield trial
+
+    def iter_status(self, experiment_id: int) -> Iterable[Mapping[str, object]]:
+        # TODO(nopdive): Should this be in the store?
+        trial_orms = self._session.query(db.Trial).filter_by(
+            experiment_id=experiment_id
+        )
+        for trial_orm in trial_orms:
+            record = {
+                "trial_id": trial_orm.id,
+                "replicate_num": trial_orm.replicate_num,
+                "meta": trial_orm.meta,
+                "method": trial_orm.method.name,
+                "task": trial_orm.task.name,
+                "status": trial_orm.status.name,
+                "errmsg": trial_orm.errmsg,
+                "create_time": trial_orm.create_time,
+                "start_time": trial_orm.start_time,
+                "end_time": trial_orm.end_time,
+            }
+            yield record
+
+    def iter_results(self, experiment_id: int) -> Iterable[Mapping[str, object]]:
+        trial_orms = self._session.query(db.Trial).filter_by(
+            experiment_id=experiment_id
+        )
+        for trial_orm in trial_orms:
+            for measure_outcome in trial_orm.measure_outcomes:
+                record = {
+                    "trial_id": trial_orm.id,
+                    "replicate_num": trial_orm.replicate_num,
+                    "meta": trial_orm.meta,
+                    "method": trial_orm.method.name,
+                    "task": trial_orm.task.name,
+                    "name": measure_outcome.measure_description.name,
+                    "seq_num": measure_outcome.seq_num,
+                    "type": measure_outcome.measure_description.type.name,
+                    "num_val": measure_outcome.num_val,
+                    "str_val": measure_outcome.str_val,
+                    "json_val": measure_outcome.json_val,
+                }
+                yield record
+
+    def iter_available_tasks(
+        self, include_measures: bool = False
+    ) -> Iterable[Mapping[str, object]]:
+        task_orms = self._session.query(db.Task)
+        for task_orm in task_orms:
+            record = {
+                "task_id": task_orm.id,
+                "name": task_orm.name,
+                "version": task_orm.version,
+                "problem": task_orm.problem,
+                "origin": task_orm.origin,
+                "config": task_orm.config,
+            }
+            if include_measures:
+                for measure_outcome in task_orm.measure_outcomes:
+                    record.update(
+                        {
+                            "measure_name": measure_outcome.measure_description.name,
+                            "seq_num": measure_outcome.seq_num,
+                            "type": measure_outcome.measure_description.type.name,
+                            "num_val": measure_outcome.num_val,
+                            "str_val": measure_outcome.str_val,
+                            "json_val": measure_outcome.json_val,
+                        }
+                    )
+            yield record
+
+    def iter_tasks(self):
+        for task_orm in self._session.query(db.Task).all():
+            task = self.from_db_task(task_orm)
+            yield task
+
+    def add_measure(
+        self,
+        trial_or_task_id: int,
+        trial_or_task_type: Type,
+        name,
+        value,
+        description=None,
+        type_=None,
+        lower_is_better=True,
+    ):
+        from powerlift.bench.experiment import Task, Trial
+
+        if type_ is None:
+            if isinstance(value, str):
+                type_ = db.TypeEnum.STR
+            elif isinstance(value, dict):
+                type_ = db.TypeEnum.JSON
+            elif isinstance(value, numbers.Number):
+                type_ = db.TypeEnum.NUMBER
+            else:
+                raise RuntimeError(
+                    f"Value type {type(value)} is not supported for measure"
+                )
+        elif isinstance(type_, str):
+            type_ = db.TypeEnum[type_.upper()]
+
+        # Create measure description if needed
+        is_declared = name in self._declared_measures_cache
+        if not is_declared:
+            if description is None:
+                description = f"Measure: {name}"
+
+            measure_description_orm = (
+                self._session.query(db.MeasureDescription)
+                .filter_by(name=name)
+                .one_or_none()
+            )
+            if measure_description_orm is None:
+                measure_description_orm = db.MeasureDescription(
+                    name=name,
+                    description=description,
+                    type=type_,
+                    lower_is_better=lower_is_better,
+                )
+                try:
+                    self._session.add(measure_description_orm)
+                    self._session.commit()
+                except IntegrityError:
+                    self._session.rollback()
+                    measure_description_orm = (
+                        self._session.query(db.MeasureDescription)
+                        .filter_by(name=name)
+                        .one()
+                    )
+            self._declared_measures_cache[name] = measure_description_orm
+        else:
+            measure_description_orm = self._declared_measures_cache[name]
+
+        # Create measure
+        seq_num = self._measure_counts[name] = self._measure_counts.get(name, -1) + 1
+        timestamp = datetime.now(pytz.utc)
+        measure_outcome_orm = db.MeasureOutcome(
+            measure_description=measure_description_orm,
+            timestamp=timestamp,
+            seq_num=seq_num,
+        )
+        self._session.add(measure_outcome_orm)
+
+        if type_ == db.TypeEnum.STR:
+            measure_outcome_orm.str_val = value
+        elif type_ == db.TypeEnum.JSON:
+            measure_outcome_orm.json_val = value
+        elif type_ == db.TypeEnum.NUMBER:
+            measure_outcome_orm.num_val = value
+        else:
+            raise RuntimeError(f"Value type {type(value)} is not supported for measure")
+
+        if trial_or_task_type == Task:
+            db_type = db.Task
+        elif trial_or_task_type == Trial:
+            db_type = db.Trial
+        else:
+            raise RuntimeError(f"Type {trial_or_task_type} is not Task nor Trial")
+
+        trial_or_task_orm = (
+            self._session.query(db_type).filter_by(id=trial_or_task_id).one()
+        )
+        trial_or_task_orm.measure_outcomes.append(measure_outcome_orm)
+        if not is_declared:
+            self._session.add(measure_description_orm)
+        self._session.commit()
+        return measure_outcome_orm.id
+
+    def create_task_with_data(self, data, version="0.0.1"):
+        if isinstance(data, SupervisedDataset):
+            return self._create_task_with_supervised(data, version)
+        elif isinstance(data, DataFrameDataset):
+            return self._create_task_with_dataframe(data, version)
+        else:  # pragma: no cover
+            raise ValueError(f"Does not support {type(data)}")
+
+    def _create_task_with_supervised(self, supervised, version):
+        X_bstream, y_bstream, meta_bstream = SupervisedDataset.serialize(supervised)
+        X_name, y_name, meta_name = supervised.asset_names()
+        X_mimetype, y_mimetype, meta_mimetype = supervised.mimetypes()
+
+        X_orm = db.Asset(
+            name=X_name,
+            description=f"Training data for {supervised.name()}",
+            version=version,
+            is_embedded=True,
+            mimetype=X_mimetype,
+            embedded=X_bstream.getvalue(),
+        )
+        y_orm = db.Asset(
+            name=y_name,
+            description=f"Labels for {supervised.name()}",
+            version=version,
+            is_embedded=True,
+            mimetype=y_mimetype,
+            embedded=y_bstream.getvalue(),
+        )
+
+        meta_orm = db.Asset(
+            name=meta_name,
+            description=f"Metadata for {supervised.name()}",
+            version=version,
+            is_embedded=True,
+            mimetype=meta_mimetype,
+            embedded=meta_bstream.getvalue(),
+        )
+
+        meta = supervised.meta
+        task_orm = db.Task(
+            name=meta["name"],
+            description=f"Dataset {meta['name']} for {meta['problem']}",
+            version=version,
+            problem=meta["problem"],
+            origin=meta["source"],
+            config={
+                "type": "data_supervised",
+                "aliases": {
+                    "X": X_name,
+                    "y": y_name,
+                    "meta": meta_name,
+                },
+            },
+        )
+        task_orm.assets.append(X_orm)
+        task_orm.assets.append(y_orm)
+        task_orm.assets.append(meta_orm)
+
+        self._session.add(X_orm)
+        self._session.add(y_orm)
+        self._session.add(task_orm)
+        self._session.commit()
+
+        return task_orm.id
+
+    def _create_task_with_dataframe(self, data, version):
+        inputs_bstream, outputs_bstream, meta_bstream = DataFrameDataset.serialize(data)
+        inputs_name, outputs_name, meta_name = data.asset_names()
+        inputs_mimetype, outputs_mimetype, meta_mimetype = data.mimetypes()
+
+        inputs_orm = db.Asset(
+            name=inputs_name,
+            description=f"Inputs for {data.name()}",
+            version=version,
+            is_embedded=True,
+            mimetype=inputs_mimetype,
+            embedded=inputs_bstream.getvalue(),
+        )
+        outputs_orm = db.Asset(
+            name=outputs_name,
+            description=f"Outputs for {data.name()}",
+            version=version,
+            is_embedded=True,
+            mimetype=outputs_mimetype,
+            embedded=outputs_bstream.getvalue(),
+        )
+
+        meta_orm = db.Asset(
+            name=meta_name,
+            description=f"Metadata for {data.name()}",
+            version=version,
+            is_embedded=True,
+            mimetype=meta_mimetype,
+            embedded=meta_bstream.getvalue(),
+        )
+
+        meta = data.meta
+        task_orm = db.Task(
+            name=meta["name"],
+            description=f"Dataset {meta['name']} for {meta['problem']}",
+            version=version,
+            problem=meta["problem"],
+            origin=meta["source"],
+            config={
+                "type": "data_dataframe",
+                "aliases": {
+                    "inputs": inputs_name,
+                    "outputs": outputs_name,
+                    "meta": meta_name,
+                },
+            },
+        )
+        task_orm.assets.append(inputs_orm)
+        task_orm.assets.append(outputs_orm)
+        task_orm.assets.append(meta_orm)
+
+        self._session.add(inputs_orm)
+        self._session.add(outputs_orm)
+        self._session.add(task_orm)
+        self._session.commit()
+
+        return task_orm.id
+
+
+def populate_task_measures(store, task_id, data):
+    from powerlift.bench.experiment import Task
+
+    meta = data.meta
+    unprocessed_measures = []
+    if isinstance(data, SupervisedDataset):
+        if meta["problem"] == "regression":
+            unprocessed_measures.extend(regression_stats(data.y))
+        elif meta["problem"] in ["binary", "multiclass"]:
+            unprocessed_measures.extend(class_stats(data.y))
+        unprocessed_measures.extend(data_stats(data.X, meta["categorical_mask"]))
+    elif isinstance(data, DataFrameDataset):
+        inputs_data_stats = [
+            (f"inputs_{x1}", x2, x3, x4)
+            for x1, x2, x3, x4 in data_stats(
+                data.inputs, meta["inputs_categorical_mask"]
+            )
+        ]
+        outputs_data_stats = [
+            (f"outputs_{x1}", x2, x3, x4)
+            for x1, x2, x3, x4 in data_stats(
+                data.outputs, meta["outputs_categorical_mask"]
+            )
+        ]
+        unprocessed_measures.extend(inputs_data_stats)
+        unprocessed_measures.extend(outputs_data_stats)
+
+    for unprocessed_measure in unprocessed_measures:
+        name, description, value, lower_is_better = unprocessed_measure
+        store.add_measure(
+            task_id,
+            Task,
+            name,
+            value,
+            description=description,
+            lower_is_better=lower_is_better,
+        )
+
+
+def retrieve_cache(
+    cache_dir: Optional[str], names: List[str]
+) -> Optional[List[io.BytesIO]]:
+    if cache_dir is None:
+        return None
+
+    cache_dir = pathlib.Path(os.path.expanduser(cache_dir))
+    cache_dir.mkdir(parents=True, exist_ok=True)
+    outputs = []
+    for name in names:
+        filepath = pathlib.Path(cache_dir, name)
+        if filepath.exists():
+            with open(filepath, "rb") as f:
+                outputs.append(io.BytesIO(f.read()))
+        else:
+            return None
+    return outputs
+
+
+def update_cache(cache_dir, names: List[str], bytes_io: List[io.BytesIO]):
+    cache_dir = pathlib.Path(os.path.expanduser(cache_dir))
+    for name, a_bytes_io in zip(names, bytes_io):
+        filepath = pathlib.Path(cache_dir, name)
+        with open(filepath, "wb") as f:
+            b = a_bytes_io.getvalue()
+            f.write(b)
+
+
+class Dataset:
+    pass
+
+
+@dataclass
+class DataFrameDataset(Dataset):
+    inputs: pd.DataFrame
+    outputs: pd.DataFrame
+    meta: dict
+
+    @classmethod
+    def serialize(cls, obj):
+        _, inputs_bstream = BytesParser.serialize(obj.inputs)
+        _, outputs_bstream = BytesParser.serialize(obj.outputs)
+        _, meta_bstream = BytesParser.serialize(obj.meta)
+        return inputs_bstream, outputs_bstream, meta_bstream
+
+    @classmethod
+    def deserialize(
+        cls,
+        inputs_bstream: io.BytesIO,
+        outputs_bstream: io.BytesIO,
+        meta_bstream: io.BytesIO,
+    ):
+        inputs = BytesParser.deserialize(MIMETYPE_DF, inputs_bstream)
+        outputs = BytesParser.deserialize(MIMETYPE_DF, outputs_bstream)
+        meta = BytesParser.deserialize(MIMETYPE_JSON, meta_bstream)
+        return cls(inputs, outputs, meta)
+
+    def asset_names(self):
+        inputs_name = f"{self.meta['name']}.inputs.parquet"
+        outputs_name = f"{self.meta['name']}.outputs.parquet"
+        meta_name = f"{self.meta['name']}.meta.json"
+        return inputs_name, outputs_name, meta_name
+
+    def mimetypes(self):
+        inputs_metadata = MIMETYPE_DF
+        outputs_metadata = MIMETYPE_DF
+        meta_metadata = MIMETYPE_JSON
+        return inputs_metadata, outputs_metadata, meta_metadata
+
+    def name(self):
+        return self.meta["name"]
+
+
+@dataclass
+class SupervisedDataset(Dataset):
+    X: pd.DataFrame
+    y: pd.Series
+    meta: dict
+
+    @classmethod
+    def serialize(cls, obj):
+        _, X_bstream = BytesParser.serialize(obj.X)
+        _, y_bstream = BytesParser.serialize(obj.y)
+        _, meta_bstream = BytesParser.serialize(obj.meta)
+        return X_bstream, y_bstream, meta_bstream
+
+    @classmethod
+    def deserialize(
+        cls, X_bstream: io.BytesIO, y_bstream: io.BytesIO, meta_bstream: io.BytesIO
+    ):
+        X = BytesParser.deserialize(MIMETYPE_DF, X_bstream)
+        y = BytesParser.deserialize(MIMETYPE_SERIES, y_bstream)
+        meta = BytesParser.deserialize(MIMETYPE_JSON, meta_bstream)
+        return cls(X, y, meta)
+
+    def asset_names(self):
+        X_name = f"{self.meta['name']}.X.parquet"
+        y_name = f"{self.meta['name']}.y.parquet"
+        meta_name = f"{self.meta['name']}.meta.json"
+        return X_name, y_name, meta_name
+
+    def mimetypes(self):
+        X_metadata = MIMETYPE_DF
+        y_metadata = MIMETYPE_SERIES
+        meta_metadata = MIMETYPE_JSON
+        return X_metadata, y_metadata, meta_metadata
+
+    def name(self):
+        return self.meta["name"]
+
+
+class DatasetAlreadyExistsError(Exception):
+    """Raised when dataset already exists in store."""
+    pass
+
+def populate_with_datasets(
+    store: Store,
+    dataset_iter: Iterable[Dataset] = None,
+    cache_dir: str = None,
+    exist_ok: bool = False,
+) -> bool:
+    """Populates store with datasets.
+
+    Attempts to add datasets to store from iterable.
+    This can be made idempotent by setting `exist_ok` to true.
+
+    Args:
+        store (Store): Store for experiment.
+        dataset_iter (Iterable[Dataset], optional): Iterable of supervised datasets. Defaults to None, which populates with OpenML and PMLB.
+        cache_dir (str, optional): If dataset_iter is None, use this cache directory across calls. Defaults to None.
+        exist_ok (bool, optional): Do not raise exception if a dataset already exist.
+    Returns:
+        bool: True if datasets were created, False otherwise
+    """
+
+    if dataset_iter is None:
+        dataset_iter = chain(
+            retrieve_openml(cache_dir=cache_dir), retrieve_pmlb(cache_dir=cache_dir)
+        )
+
+    for dataset in dataset_iter:
+        try:
+            task_id = store.create_task_with_data(dataset)
+            populate_task_measures(store, task_id, dataset)
+        except IntegrityError as e:
+            store.rollback()
+            if not exist_ok:
+                raise DatasetAlreadyExistsError("Dataset already in store") from e
+            else:
+                return False
+    return True
+
+
+def retrieve_openml(cache_dir: str = None) -> Generator[SupervisedDataset, None, None]:
+    """Retrives OpenML CC18 datasets.
+
+    Args:
+        cache_dir (str, optional): Use this cache directory across calls. Defaults to None.
+
+    Yields:
+        Generator[SupervisedDataset]: Yields datasets.
+    """
+    import openml
+
+    if cache_dir is not None:
+        cache_dir = pathlib.Path(cache_dir, "openml")
+
+    suite = openml.study.get_suite(99)
+    tasks = suite.tasks.copy()
+    random.Random(1337).shuffle(tasks)
+    for task_id in tqdm(tasks, desc="openml"):
+        task = openml.tasks.get_task(task_id)
+        dataset = openml.datasets.get_dataset(task.dataset_id, download_data=False)
+        name = dataset.name
+        X_name = f"{name}.X.parquet"
+        y_name = f"{name}.y.parquet"
+        meta_name = f"{name}.meta.json"
+        problem = (
+            "binary" if dataset.qualities["NumberOfClasses"] == 2 else "multiclass"
+        )
+
+        cached = retrieve_cache(cache_dir, [X_name, y_name, meta_name])
+        if cached is None:
+            X, y, categorical_mask, feature_names = task.get_dataset().get_data(
+                target=task.target_name, dataset_format="dataframe"
+            )
+            meta = {
+                "name": name,
+                "problem": problem,
+                "source": "openml",
+                "categorical_mask": categorical_mask,
+                "feature_names": feature_names,
+            }
+            supervised = SupervisedDataset(X, y, meta)
+        else:
+            supervised = SupervisedDataset.deserialize(*cached)
+
+        if cache_dir is not None:
+            serialized = SupervisedDataset.serialize(supervised)
+            update_cache(cache_dir, [X_name, y_name, meta_name], serialized)
+        yield supervised
+
+
+def retrieve_pmlb(cache_dir: str = None) -> Generator[SupervisedDataset, None, None]:
+    """Retrieves PMLB regression and classification datasets.
+
+    Args:
+        cache_dir (str, optional): Use this cache directory across calls. Defaults to None.
+
+    Yields:
+        Generator[SupervisedDataset]: Yields datasets.
+    """
+    from pmlb import fetch_data, classification_dataset_names, regression_dataset_names
+
+    if cache_dir is not None:
+        cache_dir = pathlib.Path(cache_dir, "pmlb")
+
+    dataset_names = []
+    dataset_names.extend(
+        [("classification", name) for name in classification_dataset_names]
+    )
+    dataset_names.extend([("regression", name) for name in regression_dataset_names])
+
+    for problem_type, dataset_name in tqdm(dataset_names, desc="pmlb"):
+        name = dataset_name
+        X_name = f"{name}.X.parquet"
+        y_name = f"{name}.y.parquet"
+        meta_name = f"{name}.meta.json"
+
+        cached = retrieve_cache(cache_dir, [X_name, y_name, meta_name])
+        if cached is None:
+            df = fetch_data(dataset_name)
+            X = df.drop("target", axis=1)
+            y = df["target"]
+            problem = problem_type
+            if problem_type == "classification":
+                problem = "binary" if len(y.unique()) == 2 else "multiclass"
+            meta = {
+                "name": name,
+                "problem": problem,
+                "source": "pmlb",
+                "categorical_mask": [dt.kind == "O" for dt in X.dtypes],
+                "feature_names": list(X.columns),
+            }
+            supervised = SupervisedDataset(X, y, meta)
+        else:
+            supervised = SupervisedDataset.deserialize(*cached)
+
+        if cache_dir is not None:
+            serialized = SupervisedDataset.serialize(supervised)
+            update_cache(cache_dir, [X_name, y_name, meta_name], serialized)
+        yield supervised
```

## powerlift/db/__init__.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-""" Database schema for benchmarks.
-
-Currently we use SQLAlchemy. The basics of it are well explained in the below link:
-https://docs.sqlalchemy.org/en/14/tutorial/index.html
-
-Design-wise we balance between design simplicity, performance and wide environment support.
-Ideally, we can run the same benchmarks on both a single machine and distributed systems
-with minimal installations (only Python dependencies at the moment).
-
-The database component is currently tested on both sqlite and postgresql.
-
-TODO:
-- Consider migration
-"""
-
-from .schema import (
-    Experiment,
-    Trial,
-    Task,
-    Method,
-    MeasureDescription,
-    MeasureOutcome,
-    Asset,
-)
-from .schema import NAME_LEN, PROBLEM_LEN, ERROR_LEN, MEASURE_STR_LEN, DESCRIPTION_LEN
-from .schema import URI_LEN, MIMETYPE_LEN, TypeEnum, StatusEnum
-
-from .actions import create_db, delete_db
+""" Database schema for benchmarks.
+
+Currently we use SQLAlchemy. The basics of it are well explained in the below link:
+https://docs.sqlalchemy.org/en/14/tutorial/index.html
+
+Design-wise we balance between design simplicity, performance and wide environment support.
+Ideally, we can run the same benchmarks on both a single machine and distributed systems
+with minimal installations (only Python dependencies at the moment).
+
+The database component is currently tested on both sqlite and postgresql.
+
+TODO:
+- Consider migration
+"""
+
+from .schema import (
+    Experiment,
+    Trial,
+    Task,
+    Method,
+    MeasureDescription,
+    MeasureOutcome,
+    Asset,
+)
+from .schema import NAME_LEN, PROBLEM_LEN, ERROR_LEN, MEASURE_STR_LEN, DESCRIPTION_LEN
+from .schema import URI_LEN, MIMETYPE_LEN, TypeEnum, StatusEnum
+
+from .actions import create_db, delete_db
```

## powerlift/db/actions.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-""" Actions that can be performed on the database. """
-
-from sqlalchemy import create_engine
-from sqlalchemy_utils import database_exists, drop_database, create_database
-from . import schema as db
-
-
-def delete_db(uri, **create_engine_kwargs):
-    """Deletes database if it exists.
-
-    Args:
-      uri: Database connection uri.
-      **create_engine_kwargs: Keyword args passed to sqlalchemy.create_engine method.
-    Returns:
-      SQLAlchemy engine.
-    """
-
-    engine = create_engine(uri, **create_engine_kwargs)
-    if database_exists(engine.url):
-        drop_database(engine.url)
-    return engine
-
-
-def create_tables(engine):
-    """Creates tables if needed.
-
-    Args:
-      engine: SQLAlchemy engine.
-    """
-    db.Base.metadata.create_all(engine, checkfirst=True)
-
-
-def drop_tables(engine):
-    """Drops tables if needed.
-
-    Args:
-      engine: SQLAlchemy engine.
-    """
-    db.Base.metadata.drop_all(engine, checkfirst=True)
-
-
-def create_db(uri, **create_engine_kwargs):
-    """Creates database if not already exists.
-
-    Args:
-      uri: Database connection uri.
-      **create_engine_kwargs: Keyword args passed to sqlalchemy.create_engine method.
-    Returns:
-      SQLAlchemy engine.
-    """
-
-    engine = create_engine(uri, **create_engine_kwargs)
-    if not database_exists(engine.url):
-        create_database(engine.url)
-    return engine
+""" Actions that can be performed on the database. """
+
+from sqlalchemy import create_engine
+from sqlalchemy_utils import database_exists, drop_database, create_database
+from . import schema as db
+
+
+def delete_db(uri, **create_engine_kwargs):
+    """Deletes database if it exists.
+
+    Args:
+      uri: Database connection uri.
+      **create_engine_kwargs: Keyword args passed to sqlalchemy.create_engine method.
+    Returns:
+      SQLAlchemy engine.
+    """
+
+    engine = create_engine(uri, **create_engine_kwargs)
+    if database_exists(engine.url):
+        drop_database(engine.url)
+    return engine
+
+
+def create_tables(engine):
+    """Creates tables if needed.
+
+    Args:
+      engine: SQLAlchemy engine.
+    """
+    db.Base.metadata.create_all(engine, checkfirst=True)
+
+
+def drop_tables(engine):
+    """Drops tables if needed.
+
+    Args:
+      engine: SQLAlchemy engine.
+    """
+    db.Base.metadata.drop_all(engine, checkfirst=True)
+
+
+def create_db(uri, **create_engine_kwargs):
+    """Creates database if not already exists.
+
+    Args:
+      uri: Database connection uri.
+      **create_engine_kwargs: Keyword args passed to sqlalchemy.create_engine method.
+    Returns:
+      SQLAlchemy engine.
+    """
+
+    engine = create_engine(uri, **create_engine_kwargs)
+    if not database_exists(engine.url):
+        create_database(engine.url)
+    return engine
```

## powerlift/db/schema.py

 * *Ordering differences only*

```diff
@@ -1,225 +1,225 @@
-""" Entity models for benchmarking.
-"""
-
-import enum
-from sqlalchemy.orm import declarative_base
-from sqlalchemy.orm import relationship
-from sqlalchemy import (
-    Column,
-    Integer,
-    String,
-    ForeignKey,
-    JSON,
-    DateTime,
-    Enum,
-    Table,
-    UniqueConstraint,
-)
-from sqlalchemy.sql.expression import null
-from sqlalchemy.sql.sqltypes import Boolean, Numeric, LargeBinary
-
-# The following are often used as titles, keep length small.
-PROBLEM_LEN = 64
-NAME_LEN = 256
-VERSION_LEN = 256
-
-# Diagnostic fields, length can be long.
-ERROR_LEN = 10000
-
-# Measure related fields.
-MEASURE_STR_LEN = None
-
-# These descriptions shouldn't be much longer than a tweet (use links for more verbosity).
-DESCRIPTION_LEN = 300
-
-# Maximum limit URI length set based on the below link:
-# https://stackoverflow.com/questions/417142/what-is-the-maximum-length-of-a-url-in-different-browsers
-URI_LEN = 2000
-
-# Media type length based on below link:
-# https://datatracker.ietf.org/doc/html/rfc4288#section-4.2
-MIMETYPE_LEN = 127
-
-Base = declarative_base()
-
-
-class TypeEnum(enum.Enum):
-    NUMBER = 0
-    STR = 1
-    JSON = 2
-
-
-class StatusEnum(enum.Enum):
-    READY = 0
-    RUNNING = 1
-    COMPLETE = 2
-    ERROR = 3
-    SUSPENDED = 4
-
-
-task_asset_table = Table(
-    "task_asset",
-    Base.metadata,
-    Column("task_id", ForeignKey("task.id"), primary_key=True),
-    Column("asset_id", ForeignKey("asset.id"), primary_key=True),
-)
-trial_input_asset_table = Table(
-    "trial_input_asset",
-    Base.metadata,
-    Column("trial_id", ForeignKey("trial.id"), primary_key=True),
-    Column("asset_id", ForeignKey("asset.id"), primary_key=True),
-)
-trial_output_asset_table = Table(
-    "trial_output_asset",
-    Base.metadata,
-    Column("trial_id", ForeignKey("trial.id"), primary_key=True),
-    Column("asset_id", ForeignKey("asset.id"), primary_key=True),
-)
-trial_measure_outcome_table = Table(
-    "trial_measure_outcome",
-    Base.metadata,
-    Column("trial_id", ForeignKey("trial.id"), primary_key=True),
-    Column("measure_outcome_id", ForeignKey("measure_outcome.id"), primary_key=True),
-)
-task_measure_outcome_table = Table(
-    "task_measure_outcome",
-    Base.metadata,
-    Column("task_id", ForeignKey("task.id"), primary_key=True),
-    Column("measure_outcome_id", ForeignKey("measure_outcome.id"), primary_key=True),
-)
-
-
-class Experiment(Base):
-    """The overall experiment, includes access to trials."""
-
-    __tablename__ = "experiment"
-    id = Column(Integer, primary_key=True)
-    name = Column(String(NAME_LEN), unique=True)
-    description = Column(String(DESCRIPTION_LEN))
-
-    trials = relationship("Trial", back_populates="experiment")
-
-
-class Trial(Base):
-    """A single trial replicate, consists primarily of task, method and its results."""
-
-    __tablename__ = "trial"
-    id = Column(Integer, primary_key=True)
-
-    experiment_id = Column(Integer, ForeignKey("experiment.id"))
-    experiment = relationship("Experiment", back_populates="trials")
-    task_id = Column(Integer, ForeignKey("task.id"))
-    task = relationship("Task", back_populates="trials")
-    method_id = Column(Integer, ForeignKey("method.id"))
-    method = relationship("Method", back_populates="trials")
-    replicate_num = Column(Integer)
-    meta = Column(JSON)
-
-    status = Column(Enum(StatusEnum))
-    errmsg = Column(String(ERROR_LEN), nullable=True)
-    create_time = Column(DateTime)
-    start_time = Column(DateTime, nullable=True)
-    end_time = Column(DateTime, nullable=True)
-
-    measure_outcomes = relationship(
-        "MeasureOutcome", secondary=trial_measure_outcome_table, back_populates="trials"
-    )
-    input_assets = relationship(
-        "Asset", secondary=trial_input_asset_table, back_populates="trial_inputs"
-    )
-    output_assets = relationship(
-        "Asset", secondary=trial_output_asset_table, back_populates="trial_outputs"
-    )
-
-
-class MeasureDescription(Base):
-    """Describes a measure that could be generated in a trial."""
-
-    __tablename__ = "measure_description"
-    id = Column(Integer, primary_key=True)
-    name = Column(String(NAME_LEN), unique=True)
-    description = Column(String(DESCRIPTION_LEN))
-    type = Column(Enum(TypeEnum))
-    lower_is_better = Column(Boolean)
-
-
-class MeasureOutcome(Base):
-    """The recording of a measure generated in a trial."""
-
-    __tablename__ = "measure_outcome"
-    id = Column(Integer, primary_key=True)
-
-    measure_description_id = Column(Integer, ForeignKey("measure_description.id"))
-    measure_description = relationship("MeasureDescription")
-
-    timestamp = Column(DateTime)
-    seq_num = Column(Integer)
-    num_val = Column(Numeric, nullable=True)
-    str_val = Column(String(MEASURE_STR_LEN), nullable=True)
-    json_val = Column(JSON, nullable=True)
-
-    trials = relationship(
-        "Trial",
-        secondary=trial_measure_outcome_table,
-        back_populates="measure_outcomes",
-    )
-    tasks = relationship(
-        "Task", secondary=task_measure_outcome_table, back_populates="measure_outcomes"
-    )
-
-
-class Method(Base):
-    """A method/technique/treatment that is being studied in a trial."""
-
-    __tablename__ = "method"
-    id = Column(Integer, primary_key=True)
-    name = Column(String(NAME_LEN), unique=True)
-    description = Column(String(DESCRIPTION_LEN))
-    version = Column(String(VERSION_LEN))
-    params = Column(JSON)
-    env = Column(JSON)
-    trials = relationship("Trial", back_populates="method")
-
-
-class Task(Base):
-    """A problem tied with a dataset. I.e. regression on Boston data."""
-
-    __tablename__ = "task"
-    id = Column(Integer, primary_key=True)
-    name = Column(String(NAME_LEN))
-    description = Column(String(DESCRIPTION_LEN))
-    version = Column(String(VERSION_LEN))
-    problem = Column(String(PROBLEM_LEN))
-    origin = Column(String(NAME_LEN))
-    config = Column(JSON)
-
-    trials = relationship("Trial", back_populates="task")
-    assets = relationship("Asset", secondary=task_asset_table, back_populates="tasks")
-    measure_outcomes = relationship(
-        "MeasureOutcome", secondary=task_measure_outcome_table, back_populates="tasks"
-    )
-    __table_args__ = (UniqueConstraint("name", "problem", name="u_name_problem"),)
-
-
-class Asset(Base):
-    """An asset with its associated mimetype and uri."""
-
-    __tablename__ = "asset"
-    id = Column(Integer, primary_key=True)
-
-    name = Column(String(NAME_LEN))
-    description = Column(String(DESCRIPTION_LEN))
-    version = Column(String(VERSION_LEN))
-
-    is_embedded = Column(Boolean)
-    embedded = Column(LargeBinary, nullable=True)
-    uri = Column(String(URI_LEN), nullable=True)
-    mimetype = Column(String(MIMETYPE_LEN))
-
-    trial_inputs = relationship(
-        "Trial", secondary=trial_input_asset_table, back_populates="input_assets"
-    )
-    trial_outputs = relationship(
-        "Trial", secondary=trial_output_asset_table, back_populates="output_assets"
-    )
-    tasks = relationship("Task", secondary=task_asset_table, back_populates="assets")
+""" Entity models for benchmarking.
+"""
+
+import enum
+from sqlalchemy.orm import declarative_base
+from sqlalchemy.orm import relationship
+from sqlalchemy import (
+    Column,
+    Integer,
+    String,
+    ForeignKey,
+    JSON,
+    DateTime,
+    Enum,
+    Table,
+    UniqueConstraint,
+)
+from sqlalchemy.sql.expression import null
+from sqlalchemy.sql.sqltypes import Boolean, Numeric, LargeBinary
+
+# The following are often used as titles, keep length small.
+PROBLEM_LEN = 64
+NAME_LEN = 256
+VERSION_LEN = 256
+
+# Diagnostic fields, length can be long.
+ERROR_LEN = 10000
+
+# Measure related fields.
+MEASURE_STR_LEN = None
+
+# These descriptions shouldn't be much longer than a tweet (use links for more verbosity).
+DESCRIPTION_LEN = 300
+
+# Maximum limit URI length set based on the below link:
+# https://stackoverflow.com/questions/417142/what-is-the-maximum-length-of-a-url-in-different-browsers
+URI_LEN = 2000
+
+# Media type length based on below link:
+# https://datatracker.ietf.org/doc/html/rfc4288#section-4.2
+MIMETYPE_LEN = 127
+
+Base = declarative_base()
+
+
+class TypeEnum(enum.Enum):
+    NUMBER = 0
+    STR = 1
+    JSON = 2
+
+
+class StatusEnum(enum.Enum):
+    READY = 0
+    RUNNING = 1
+    COMPLETE = 2
+    ERROR = 3
+    SUSPENDED = 4
+
+
+task_asset_table = Table(
+    "task_asset",
+    Base.metadata,
+    Column("task_id", ForeignKey("task.id"), primary_key=True),
+    Column("asset_id", ForeignKey("asset.id"), primary_key=True),
+)
+trial_input_asset_table = Table(
+    "trial_input_asset",
+    Base.metadata,
+    Column("trial_id", ForeignKey("trial.id"), primary_key=True),
+    Column("asset_id", ForeignKey("asset.id"), primary_key=True),
+)
+trial_output_asset_table = Table(
+    "trial_output_asset",
+    Base.metadata,
+    Column("trial_id", ForeignKey("trial.id"), primary_key=True),
+    Column("asset_id", ForeignKey("asset.id"), primary_key=True),
+)
+trial_measure_outcome_table = Table(
+    "trial_measure_outcome",
+    Base.metadata,
+    Column("trial_id", ForeignKey("trial.id"), primary_key=True),
+    Column("measure_outcome_id", ForeignKey("measure_outcome.id"), primary_key=True),
+)
+task_measure_outcome_table = Table(
+    "task_measure_outcome",
+    Base.metadata,
+    Column("task_id", ForeignKey("task.id"), primary_key=True),
+    Column("measure_outcome_id", ForeignKey("measure_outcome.id"), primary_key=True),
+)
+
+
+class Experiment(Base):
+    """The overall experiment, includes access to trials."""
+
+    __tablename__ = "experiment"
+    id = Column(Integer, primary_key=True)
+    name = Column(String(NAME_LEN), unique=True)
+    description = Column(String(DESCRIPTION_LEN))
+
+    trials = relationship("Trial", back_populates="experiment")
+
+
+class Trial(Base):
+    """A single trial replicate, consists primarily of task, method and its results."""
+
+    __tablename__ = "trial"
+    id = Column(Integer, primary_key=True)
+
+    experiment_id = Column(Integer, ForeignKey("experiment.id"))
+    experiment = relationship("Experiment", back_populates="trials")
+    task_id = Column(Integer, ForeignKey("task.id"))
+    task = relationship("Task", back_populates="trials")
+    method_id = Column(Integer, ForeignKey("method.id"))
+    method = relationship("Method", back_populates="trials")
+    replicate_num = Column(Integer)
+    meta = Column(JSON)
+
+    status = Column(Enum(StatusEnum))
+    errmsg = Column(String(ERROR_LEN), nullable=True)
+    create_time = Column(DateTime)
+    start_time = Column(DateTime, nullable=True)
+    end_time = Column(DateTime, nullable=True)
+
+    measure_outcomes = relationship(
+        "MeasureOutcome", secondary=trial_measure_outcome_table, back_populates="trials"
+    )
+    input_assets = relationship(
+        "Asset", secondary=trial_input_asset_table, back_populates="trial_inputs"
+    )
+    output_assets = relationship(
+        "Asset", secondary=trial_output_asset_table, back_populates="trial_outputs"
+    )
+
+
+class MeasureDescription(Base):
+    """Describes a measure that could be generated in a trial."""
+
+    __tablename__ = "measure_description"
+    id = Column(Integer, primary_key=True)
+    name = Column(String(NAME_LEN), unique=True)
+    description = Column(String(DESCRIPTION_LEN))
+    type = Column(Enum(TypeEnum))
+    lower_is_better = Column(Boolean)
+
+
+class MeasureOutcome(Base):
+    """The recording of a measure generated in a trial."""
+
+    __tablename__ = "measure_outcome"
+    id = Column(Integer, primary_key=True)
+
+    measure_description_id = Column(Integer, ForeignKey("measure_description.id"))
+    measure_description = relationship("MeasureDescription")
+
+    timestamp = Column(DateTime)
+    seq_num = Column(Integer)
+    num_val = Column(Numeric, nullable=True)
+    str_val = Column(String(MEASURE_STR_LEN), nullable=True)
+    json_val = Column(JSON, nullable=True)
+
+    trials = relationship(
+        "Trial",
+        secondary=trial_measure_outcome_table,
+        back_populates="measure_outcomes",
+    )
+    tasks = relationship(
+        "Task", secondary=task_measure_outcome_table, back_populates="measure_outcomes"
+    )
+
+
+class Method(Base):
+    """A method/technique/treatment that is being studied in a trial."""
+
+    __tablename__ = "method"
+    id = Column(Integer, primary_key=True)
+    name = Column(String(NAME_LEN), unique=True)
+    description = Column(String(DESCRIPTION_LEN))
+    version = Column(String(VERSION_LEN))
+    params = Column(JSON)
+    env = Column(JSON)
+    trials = relationship("Trial", back_populates="method")
+
+
+class Task(Base):
+    """A problem tied with a dataset. I.e. regression on Boston data."""
+
+    __tablename__ = "task"
+    id = Column(Integer, primary_key=True)
+    name = Column(String(NAME_LEN))
+    description = Column(String(DESCRIPTION_LEN))
+    version = Column(String(VERSION_LEN))
+    problem = Column(String(PROBLEM_LEN))
+    origin = Column(String(NAME_LEN))
+    config = Column(JSON)
+
+    trials = relationship("Trial", back_populates="task")
+    assets = relationship("Asset", secondary=task_asset_table, back_populates="tasks")
+    measure_outcomes = relationship(
+        "MeasureOutcome", secondary=task_measure_outcome_table, back_populates="tasks"
+    )
+    __table_args__ = (UniqueConstraint("name", "problem", name="u_name_problem"),)
+
+
+class Asset(Base):
+    """An asset with its associated mimetype and uri."""
+
+    __tablename__ = "asset"
+    id = Column(Integer, primary_key=True)
+
+    name = Column(String(NAME_LEN))
+    description = Column(String(DESCRIPTION_LEN))
+    version = Column(String(VERSION_LEN))
+
+    is_embedded = Column(Boolean)
+    embedded = Column(LargeBinary, nullable=True)
+    uri = Column(String(URI_LEN), nullable=True)
+    mimetype = Column(String(MIMETYPE_LEN))
+
+    trial_inputs = relationship(
+        "Trial", secondary=trial_input_asset_table, back_populates="input_assets"
+    )
+    trial_outputs = relationship(
+        "Trial", secondary=trial_output_asset_table, back_populates="output_assets"
+    )
+    tasks = relationship("Task", secondary=task_asset_table, back_populates="assets")
```

## powerlift/executors/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-""" Executors that run trials in their environment.
-"""
-
-from powerlift.executors.azure_ci import AzureContainerInstance
-from powerlift.executors.docker import InsecureDocker
-from powerlift.executors.localmachine import LocalMachine
+""" Executors that run trials in their environment.
+"""
+
+from powerlift.executors.azure_ci import AzureContainerInstance
+from powerlift.executors.docker import InsecureDocker
+from powerlift.executors.localmachine import LocalMachine
```

## powerlift/executors/azure_ci.py

 * *Ordering differences only*

```diff
@@ -1,211 +1,211 @@
-""" Azure Container Instances runtime.
-
-# See more details here:
-https://docs.microsoft.com/en-us/python/api/overview/azure/container-instance?view=azure-python
-"""
-
-from powerlift.executors.localmachine import LocalMachine
-from powerlift.bench.store import Store
-from typing import Iterable, List
-from powerlift.executors.base import handle_err
-
-
-def _wait_for_completed_worker(results):
-    import time
-
-    if len(results) == 0:
-        return None
-
-    while True:
-        completed = False
-        for worker_id, result in results.items():
-            if result is None or result.done():
-                completed = True
-                break
-        if completed:
-            del results[worker_id]
-            return worker_id
-        else:
-            time.sleep(1)
-
-
-def _run(tasks, azure_json, num_cores, mem_size_gb, n_running_containers):
-    from azure.mgmt.containerinstance.models import (
-        ContainerGroup,
-        Container,
-        ContainerGroupRestartPolicy,
-        EnvironmentVariable,
-        ResourceRequests,
-        ResourceRequirements,
-        OperatingSystemTypes,
-    )
-    from azure.mgmt.containerinstance import ContainerInstanceManagementClient
-    from azure.mgmt.resource import ResourceManagementClient
-    from azure.identity import ClientSecretCredential
-
-    credential = ClientSecretCredential(
-        tenant_id=azure_json["tenant_id"],
-        client_id=azure_json["client_id"],
-        client_secret=azure_json["client_secret"],
-    )
-    resource_group_name = azure_json["resource_group"]
-    aci_client = ContainerInstanceManagementClient(
-        credential, azure_json["subscription_id"]
-    )
-    res_client = ResourceManagementClient(credential, azure_json["subscription_id"])
-    resource_group = res_client.resource_groups.get(resource_group_name)
-
-    # Run until completion.
-    container_counter = 0
-    results = {x: None for x in range(n_running_containers)}
-    while len(tasks) != 0:
-        params = tasks.pop(0)
-        worker_id = _wait_for_completed_worker(results)
-
-        trial_ids, uri, timeout, raise_exception, image = params
-        env_vars = [
-            EnvironmentVariable(
-                name="TRIAL_IDS", value=",".join([str(x) for x in trial_ids])
-            ),
-            EnvironmentVariable(name="DB_URL", secure_value=uri),
-            EnvironmentVariable(name="TIMEOUT", value=timeout),
-            EnvironmentVariable(name="RAISE_EXCEPTION", value=raise_exception),
-        ]
-        container_resource_requests = ResourceRequests(
-            cpu=num_cores,
-            memory_in_gb=mem_size_gb,
-        )
-        container_resource_requirements = ResourceRequirements(
-            requests=container_resource_requests
-        )
-        container_name = f"powerlift-container-{container_counter}"
-        container_counter += 1
-        container = Container(
-            name=container_name,
-            image=image,
-            resources=container_resource_requirements,
-            command=["python", "-m", "powerlift.run"],
-            environment_variables=env_vars,
-        )
-        container_group = ContainerGroup(
-            location=resource_group.location,
-            containers=[container],
-            os_type=OperatingSystemTypes.linux,
-            restart_policy=ContainerGroupRestartPolicy.never,
-        )
-        container_group_name = f"powerlift-container-group-{worker_id}"
-
-        result = aci_client.container_groups.begin_create_or_update(
-            resource_group.name, container_group_name, container_group
-        )
-        results[worker_id] = result
-
-    # Wait for all container groups to complete
-    while _wait_for_completed_worker(results) is not None:
-        pass
-
-    # Delete all container groups
-    for worker_id in range(n_running_containers):
-        container_group_name = f"powerlift-container-group-{worker_id}"
-        aci_client.container_groups.begin_delete(
-            resource_group_name, container_group_name
-        )
-    return None
-
-
-class AzureContainerInstance(LocalMachine):
-    """Runs trials on Azure Container Instances."""
-
-    def __init__(
-        self,
-        store: Store,
-        azure_tenant_id: str,
-        azure_client_id: str,
-        azure_client_secret: str,
-        subscription_id: str,
-        resource_group: str,
-        image: str = "interpretml/powerlift:0.0.1",
-        n_running_containers: int = 1,
-        num_cores: int = 1,
-        mem_size_gb: int = 2,
-        raise_exception: bool = False,
-        wheel_filepaths: List[str] = None,
-        docker_db_uri: str = None,
-    ):
-        """Runs remote execution of trials via Azure Container Instances.
-
-        Args:
-            store (Store): Store that houses trials.
-            azure_tenant_id (str): Azure tentant ID.
-            azure_client_id (str): Azure client ID.
-            azure_client_secret (str): Azure client secret.
-            subscription_id (str): Azure subscription ID.
-            resource_group (str): Azure resource group.
-            image (str, optional): Image to execute. Defaults to "interpretml/powerlift:0.0.1".
-            n_running_containers (int, optional): Max number of containers to run simultaneously. Defaults to 1.
-            num_cores (int, optional): Number of cores per container. Defaults to 1.
-            mem_size_gb (int, optional): RAM size in GB per container. Defaults to 2.
-            raise_exception (bool, optional): Raise exception on failure. Defaults to False.
-            wheel_filepaths (List[str], optional): List of wheel filepaths to install on ACI trial run. Defaults to None.
-            docker_db_uri (str, optional): Database URI for container. Defaults to None.
-        """
-        from multiprocessing import Manager
-
-        self._image = image
-        self._n_running_containers = n_running_containers
-        self._num_cores = num_cores
-        self._mem_size_gb = mem_size_gb
-
-        self._docker_db_uri = docker_db_uri
-        self._azure_json = {
-            "tenant_id": azure_tenant_id,
-            "client_id": azure_client_id,
-            "client_secret": azure_client_secret,
-            "subscription_id": subscription_id,
-            "resource_group": resource_group,
-        }
-        super().__init__(store, n_running_containers, raise_exception, wheel_filepaths)
-
-    def delete_credentials(self):
-        """Deletes credentials in object for accessing Azure Resources."""
-        del self._azure_json
-
-    def submit(self, trial_run_fn, trials: Iterable, timeout=None):
-        uri = (
-            self._docker_db_uri if self._docker_db_uri is not None else self._store.uri
-        )
-        tasks = []
-        self._store.add_trial_run_fn(
-            [x.id for x in trials], trial_run_fn, self._wheel_filepaths
-        )
-        for trial in trials:
-            params = (
-                [trial.id],
-                uri,
-                timeout,
-                self._raise_exception,
-                self._image,
-            )
-            tasks.append(params)
-
-        params = (
-            tasks,
-            self._azure_json,
-            self._num_cores,
-            self._mem_size_gb,
-            self._n_running_containers,
-        )
-        if self._pool is None:
-            try:
-                res = _run(*params)
-                self._trial_id_to_result[0] = res
-            except Exception as e:
-                self._trial_id_to_result[0] = e
-                if self._raise_exception:
-                    raise e
-        else:
-            self._trial_id_to_result[0] = self._pool.apply_async(
-                _run,
-                params,
-                error_callback=handle_err,
-            )
+""" Azure Container Instances runtime.
+
+# See more details here:
+https://docs.microsoft.com/en-us/python/api/overview/azure/container-instance?view=azure-python
+"""
+
+from powerlift.executors.localmachine import LocalMachine
+from powerlift.bench.store import Store
+from typing import Iterable, List
+from powerlift.executors.base import handle_err
+
+
+def _wait_for_completed_worker(results):
+    import time
+
+    if len(results) == 0:
+        return None
+
+    while True:
+        completed = False
+        for worker_id, result in results.items():
+            if result is None or result.done():
+                completed = True
+                break
+        if completed:
+            del results[worker_id]
+            return worker_id
+        else:
+            time.sleep(1)
+
+
+def _run(tasks, azure_json, num_cores, mem_size_gb, n_running_containers):
+    from azure.mgmt.containerinstance.models import (
+        ContainerGroup,
+        Container,
+        ContainerGroupRestartPolicy,
+        EnvironmentVariable,
+        ResourceRequests,
+        ResourceRequirements,
+        OperatingSystemTypes,
+    )
+    from azure.mgmt.containerinstance import ContainerInstanceManagementClient
+    from azure.mgmt.resource import ResourceManagementClient
+    from azure.identity import ClientSecretCredential
+
+    credential = ClientSecretCredential(
+        tenant_id=azure_json["tenant_id"],
+        client_id=azure_json["client_id"],
+        client_secret=azure_json["client_secret"],
+    )
+    resource_group_name = azure_json["resource_group"]
+    aci_client = ContainerInstanceManagementClient(
+        credential, azure_json["subscription_id"]
+    )
+    res_client = ResourceManagementClient(credential, azure_json["subscription_id"])
+    resource_group = res_client.resource_groups.get(resource_group_name)
+
+    # Run until completion.
+    container_counter = 0
+    results = {x: None for x in range(n_running_containers)}
+    while len(tasks) != 0:
+        params = tasks.pop(0)
+        worker_id = _wait_for_completed_worker(results)
+
+        trial_ids, uri, timeout, raise_exception, image = params
+        env_vars = [
+            EnvironmentVariable(
+                name="TRIAL_IDS", value=",".join([str(x) for x in trial_ids])
+            ),
+            EnvironmentVariable(name="DB_URL", secure_value=uri),
+            EnvironmentVariable(name="TIMEOUT", value=timeout),
+            EnvironmentVariable(name="RAISE_EXCEPTION", value=raise_exception),
+        ]
+        container_resource_requests = ResourceRequests(
+            cpu=num_cores,
+            memory_in_gb=mem_size_gb,
+        )
+        container_resource_requirements = ResourceRequirements(
+            requests=container_resource_requests
+        )
+        container_name = f"powerlift-container-{container_counter}"
+        container_counter += 1
+        container = Container(
+            name=container_name,
+            image=image,
+            resources=container_resource_requirements,
+            command=["python", "-m", "powerlift.run"],
+            environment_variables=env_vars,
+        )
+        container_group = ContainerGroup(
+            location=resource_group.location,
+            containers=[container],
+            os_type=OperatingSystemTypes.linux,
+            restart_policy=ContainerGroupRestartPolicy.never,
+        )
+        container_group_name = f"powerlift-container-group-{worker_id}"
+
+        result = aci_client.container_groups.begin_create_or_update(
+            resource_group.name, container_group_name, container_group
+        )
+        results[worker_id] = result
+
+    # Wait for all container groups to complete
+    while _wait_for_completed_worker(results) is not None:
+        pass
+
+    # Delete all container groups
+    for worker_id in range(n_running_containers):
+        container_group_name = f"powerlift-container-group-{worker_id}"
+        aci_client.container_groups.begin_delete(
+            resource_group_name, container_group_name
+        )
+    return None
+
+
+class AzureContainerInstance(LocalMachine):
+    """Runs trials on Azure Container Instances."""
+
+    def __init__(
+        self,
+        store: Store,
+        azure_tenant_id: str,
+        azure_client_id: str,
+        azure_client_secret: str,
+        subscription_id: str,
+        resource_group: str,
+        image: str = "interpretml/powerlift:0.0.1",
+        n_running_containers: int = 1,
+        num_cores: int = 1,
+        mem_size_gb: int = 2,
+        raise_exception: bool = False,
+        wheel_filepaths: List[str] = None,
+        docker_db_uri: str = None,
+    ):
+        """Runs remote execution of trials via Azure Container Instances.
+
+        Args:
+            store (Store): Store that houses trials.
+            azure_tenant_id (str): Azure tentant ID.
+            azure_client_id (str): Azure client ID.
+            azure_client_secret (str): Azure client secret.
+            subscription_id (str): Azure subscription ID.
+            resource_group (str): Azure resource group.
+            image (str, optional): Image to execute. Defaults to "interpretml/powerlift:0.0.1".
+            n_running_containers (int, optional): Max number of containers to run simultaneously. Defaults to 1.
+            num_cores (int, optional): Number of cores per container. Defaults to 1.
+            mem_size_gb (int, optional): RAM size in GB per container. Defaults to 2.
+            raise_exception (bool, optional): Raise exception on failure. Defaults to False.
+            wheel_filepaths (List[str], optional): List of wheel filepaths to install on ACI trial run. Defaults to None.
+            docker_db_uri (str, optional): Database URI for container. Defaults to None.
+        """
+        from multiprocessing import Manager
+
+        self._image = image
+        self._n_running_containers = n_running_containers
+        self._num_cores = num_cores
+        self._mem_size_gb = mem_size_gb
+
+        self._docker_db_uri = docker_db_uri
+        self._azure_json = {
+            "tenant_id": azure_tenant_id,
+            "client_id": azure_client_id,
+            "client_secret": azure_client_secret,
+            "subscription_id": subscription_id,
+            "resource_group": resource_group,
+        }
+        super().__init__(store, n_running_containers, raise_exception, wheel_filepaths)
+
+    def delete_credentials(self):
+        """Deletes credentials in object for accessing Azure Resources."""
+        del self._azure_json
+
+    def submit(self, trial_run_fn, trials: Iterable, timeout=None):
+        uri = (
+            self._docker_db_uri if self._docker_db_uri is not None else self._store.uri
+        )
+        tasks = []
+        self._store.add_trial_run_fn(
+            [x.id for x in trials], trial_run_fn, self._wheel_filepaths
+        )
+        for trial in trials:
+            params = (
+                [trial.id],
+                uri,
+                timeout,
+                self._raise_exception,
+                self._image,
+            )
+            tasks.append(params)
+
+        params = (
+            tasks,
+            self._azure_json,
+            self._num_cores,
+            self._mem_size_gb,
+            self._n_running_containers,
+        )
+        if self._pool is None:
+            try:
+                res = _run(*params)
+                self._trial_id_to_result[0] = res
+            except Exception as e:
+                self._trial_id_to_result[0] = e
+                if self._raise_exception:
+                    raise e
+        else:
+            self._trial_id_to_result[0] = self._pool.apply_async(
+                _run,
+                params,
+                error_callback=handle_err,
+            )
```

## powerlift/executors/base.py

 * *Ordering differences only*

```diff
@@ -1,80 +1,80 @@
-""" Base service classes and methods for services. """
-
-from numbers import Number
-import time
-from types import FunctionType
-from typing import Any, Iterable, Tuple
-from stopit import ThreadingTimeout as Timeout
-
-
-class Executor:
-    """Runs the function with time limit.
-
-    Args:
-        f: Function to call.
-        timeout_seconds: Maximum time limit.
-    Returns:
-        A tuple containing (response, duration, timed_out as boolean).
-    """
-
-    def submit(self, trial_run_fn: FunctionType, trials: Iterable, timeout: int = None):
-        """Submits and executes trials by trial run function asynchronously.
-
-        Args:
-            trial_run_fn (FunctionType): Trial run function that takes tasks as arg.
-            trials (Iterable): Trial objects that are to be run.
-            timeout (int, optional): Timeout in seconds for trial run. Defaults to None.
-
-        Raises:
-            NotImplementedError: Raised when executor did not implement this.
-        """
-        raise NotImplementedError()
-
-    def join(self):
-        """Synchronously blocks until execution is complete.
-
-        Raises:
-            NotImplementedError: Raised when executor did not implement this.
-        """
-        raise NotImplementedError()
-
-    def cancel(self):
-        """Cancels execution if running.
-
-        Raises:
-            NotImplementedError: Raised when executor did not implement this.
-        """
-        raise NotImplementedError()
-
-
-def timed_run(f: FunctionType, timeout_seconds: int = 3600) -> Tuple[Any, Number, bool]:
-    """Runs the function with time limit.
-
-    Args:
-        f (FunctionType): Function to call.
-        timeout_seconds (int, optional): Timeout in seconds. Defaults to 3600.
-
-    Returns:
-        Tuple[Any, Number, bool]: Function response, duration, has timed out.
-    """
-    start_time = time.time()
-    with Timeout(timeout_seconds) as timeout_ctx:
-        res = f()
-    duration = time.time() - start_time
-
-    timed_out = timeout_ctx.state == timeout_ctx.TIMED_OUT
-    if timed_out:
-        res = None
-    return res, duration, timed_out
-
-
-def handle_err(err: Exception):
-    """Raises exception provided.
-
-    Args:
-        err (Exception): Exception to raise.
-
-    Raises:
-        err: Exception provided.
-    """
-    raise err
+""" Base service classes and methods for services. """
+
+from numbers import Number
+import time
+from types import FunctionType
+from typing import Any, Iterable, Tuple
+from stopit import ThreadingTimeout as Timeout
+
+
+class Executor:
+    """Runs the function with time limit.
+
+    Args:
+        f: Function to call.
+        timeout_seconds: Maximum time limit.
+    Returns:
+        A tuple containing (response, duration, timed_out as boolean).
+    """
+
+    def submit(self, trial_run_fn: FunctionType, trials: Iterable, timeout: int = None):
+        """Submits and executes trials by trial run function asynchronously.
+
+        Args:
+            trial_run_fn (FunctionType): Trial run function that takes tasks as arg.
+            trials (Iterable): Trial objects that are to be run.
+            timeout (int, optional): Timeout in seconds for trial run. Defaults to None.
+
+        Raises:
+            NotImplementedError: Raised when executor did not implement this.
+        """
+        raise NotImplementedError()
+
+    def join(self):
+        """Synchronously blocks until execution is complete.
+
+        Raises:
+            NotImplementedError: Raised when executor did not implement this.
+        """
+        raise NotImplementedError()
+
+    def cancel(self):
+        """Cancels execution if running.
+
+        Raises:
+            NotImplementedError: Raised when executor did not implement this.
+        """
+        raise NotImplementedError()
+
+
+def timed_run(f: FunctionType, timeout_seconds: int = 3600) -> Tuple[Any, Number, bool]:
+    """Runs the function with time limit.
+
+    Args:
+        f (FunctionType): Function to call.
+        timeout_seconds (int, optional): Timeout in seconds. Defaults to 3600.
+
+    Returns:
+        Tuple[Any, Number, bool]: Function response, duration, has timed out.
+    """
+    start_time = time.time()
+    with Timeout(timeout_seconds) as timeout_ctx:
+        res = f()
+    duration = time.time() - start_time
+
+    timed_out = timeout_ctx.state == timeout_ctx.TIMED_OUT
+    if timed_out:
+        res = None
+    return res, duration, timed_out
+
+
+def handle_err(err: Exception):
+    """Raises exception provided.
+
+    Args:
+        err (Exception): Exception to raise.
+
+    Raises:
+        err: Exception provided.
+    """
+    raise err
```

## powerlift/executors/docker.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-""" Runs docker containers locally.
-
-This is handy for testing if dockerfiles work with powerlift before
-sending it a remote container service.
-"""
-
-from powerlift.bench.store import Store
-from powerlift.executors.localmachine import LocalMachine
-from powerlift.executors.base import handle_err
-from typing import Iterable, List
-
-
-def _run_docker(trial_ids, db_url, timeout, raise_exception, image):
-    import docker
-
-    client = docker.from_env()
-
-    container = client.containers.run(
-        image,
-        "python -m powerlift.run",
-        environment={
-            "TRIAL_IDS": ",".join([str(x) for x in trial_ids]),
-            "DB_URL": db_url,
-            "TIMEOUT": timeout,
-            "RAISE_EXCEPTION": raise_exception,
-        },
-        network_mode="host",
-        detach=True,
-    )
-    exit_code = container.wait()
-    container.remove()
-    return exit_code
-
-
-class InsecureDocker(LocalMachine):
-    """Runs trials in local docker containers.
-
-    Make sure the machine you run on is fully trusted.
-    Environment variables are used to pass the database connection string,
-    which means other users on the machine may be able to see it via enumerating
-    the processes on the machine and their args.
-    """
-
-    def __init__(
-        self,
-        store: Store,
-        image: str = "interpretml/powerlift:0.0.1",
-        n_running_containers: int = None,
-        raise_exception: bool = False,
-        wheel_filepaths: List[str] = None,
-        docker_db_uri: str = None,
-    ):
-        """Runs trials in local docker containers.
-
-        Args:
-            store (Store): Store that houses trials.
-            image (str, optional): Image to execute in container. Defaults to "interpretml/powerlift:0.0.1".
-            n_running_containers (int, optional): Max number of containers running simultaneously. Defaults to None.
-            raise_exception (bool, optional): Raise exception on failure. Defaults to False.
-            wheel_filepaths (List[str], optional): List of wheel filepaths to install on ACI trial run. Defaults to None.
-            docker_db_uri (str, optional): Database URI for container. Defaults to None.
-        """
-        self._docker_db_uri = docker_db_uri
-        self._image = image
-        super().__init__(store, n_running_containers, raise_exception, wheel_filepaths)
-
-    def submit(self, trial_run_fn, trials: Iterable, timeout=None):
-        uri = (
-            self._docker_db_uri if self._docker_db_uri is not None else self._store.uri
-        )
-        self._store.add_trial_run_fn(
-            [x.id for x in trials], trial_run_fn, self._wheel_filepaths
-        )
-        for trial in trials:
-            self._trial_id_to_result[trial.id] = self._pool.apply_async(
-                _run_docker,
-                ([trial.id], uri, timeout, self._raise_exception, self._image),
-                error_callback=handle_err,
-            )
-
-    @property
-    def docker_db_uri(self):
-        return self._docker_db_uri
-
-    @property
-    def image(self):
-        return self._image
+""" Runs docker containers locally.
+
+This is handy for testing if dockerfiles work with powerlift before
+sending it a remote container service.
+"""
+
+from powerlift.bench.store import Store
+from powerlift.executors.localmachine import LocalMachine
+from powerlift.executors.base import handle_err
+from typing import Iterable, List
+
+
+def _run_docker(trial_ids, db_url, timeout, raise_exception, image):
+    import docker
+
+    client = docker.from_env()
+
+    container = client.containers.run(
+        image,
+        "python -m powerlift.run",
+        environment={
+            "TRIAL_IDS": ",".join([str(x) for x in trial_ids]),
+            "DB_URL": db_url,
+            "TIMEOUT": timeout,
+            "RAISE_EXCEPTION": raise_exception,
+        },
+        network_mode="host",
+        detach=True,
+    )
+    exit_code = container.wait()
+    container.remove()
+    return exit_code
+
+
+class InsecureDocker(LocalMachine):
+    """Runs trials in local docker containers.
+
+    Make sure the machine you run on is fully trusted.
+    Environment variables are used to pass the database connection string,
+    which means other users on the machine may be able to see it via enumerating
+    the processes on the machine and their args.
+    """
+
+    def __init__(
+        self,
+        store: Store,
+        image: str = "interpretml/powerlift:0.0.1",
+        n_running_containers: int = None,
+        raise_exception: bool = False,
+        wheel_filepaths: List[str] = None,
+        docker_db_uri: str = None,
+    ):
+        """Runs trials in local docker containers.
+
+        Args:
+            store (Store): Store that houses trials.
+            image (str, optional): Image to execute in container. Defaults to "interpretml/powerlift:0.0.1".
+            n_running_containers (int, optional): Max number of containers running simultaneously. Defaults to None.
+            raise_exception (bool, optional): Raise exception on failure. Defaults to False.
+            wheel_filepaths (List[str], optional): List of wheel filepaths to install on ACI trial run. Defaults to None.
+            docker_db_uri (str, optional): Database URI for container. Defaults to None.
+        """
+        self._docker_db_uri = docker_db_uri
+        self._image = image
+        super().__init__(store, n_running_containers, raise_exception, wheel_filepaths)
+
+    def submit(self, trial_run_fn, trials: Iterable, timeout=None):
+        uri = (
+            self._docker_db_uri if self._docker_db_uri is not None else self._store.uri
+        )
+        self._store.add_trial_run_fn(
+            [x.id for x in trials], trial_run_fn, self._wheel_filepaths
+        )
+        for trial in trials:
+            self._trial_id_to_result[trial.id] = self._pool.apply_async(
+                _run_docker,
+                ([trial.id], uri, timeout, self._raise_exception, self._image),
+                error_callback=handle_err,
+            )
+
+    @property
+    def docker_db_uri(self):
+        return self._docker_db_uri
+
+    @property
+    def image(self):
+        return self._image
```

## powerlift/executors/localmachine.py

 * *Ordering differences only*

```diff
@@ -1,96 +1,96 @@
-""" Runs all trials on local machine that is running powerlift.
-
-This currently uses multiprocessing pools to handle parallelism.
-"""
-
-from powerlift.bench.store import Store
-from powerlift.executors.base import Executor
-from multiprocessing import Pool
-from typing import Iterable, List
-from powerlift.executors.base import handle_err
-
-
-class LocalMachine(Executor):
-    def __init__(
-        self,
-        store: Store,
-        n_cpus: int = None,
-        debug_mode: bool = False,
-        wheel_filepaths: List[str] = None,
-    ):
-        """Runs trial runs on the local machine.
-
-        Args:
-            store (Store): Store that houses trials.
-            n_cpus (int, optional): Max number of cpus to run on.. Defaults to None.
-            debug_mode (bool, optional): Restricts to a single thread and raises exceptions. Good for debugging.
-            wheel_filepaths (List[str], optional): List of wheel filepaths to install on ACI trial run. Defaults to None.
-        """
-        if debug_mode:
-            self._pool = None
-        else:
-            self._pool = Pool(processes=n_cpus)
-
-        self._trial_id_to_result = {}
-        self._store = store
-        self._n_cpus = n_cpus
-        self._debug_mode = debug_mode
-        self._wheel_filepaths = wheel_filepaths
-
-    def __del__(self):
-        if self._pool is not None:
-            self._pool.close()
-
-    def submit(self, trial_run_fn, trials: Iterable, timeout=None):
-        from powerlift.run import __main__ as runner
-
-        self._store.add_trial_run_fn(
-            [x.id for x in trials], trial_run_fn, self._wheel_filepaths
-        )
-        for trial in trials:
-            if self._pool is None:
-                try:
-                    debug_fn = trial_run_fn if self._debug_mode else None
-                    res = runner.run_trials(
-                        [trial.id],
-                        self._store.uri,
-                        timeout,
-                        self._debug_mode,
-                        debug_fn=debug_fn,
-                    )
-                    self._trial_id_to_result[trial.id] = res
-                except Exception as e:
-                    self._trial_id_to_result[trial.id] = e
-                    if self._debug_mode:
-                        raise e
-            else:
-                self._trial_id_to_result[trial.id] = self._pool.apply_async(
-                    runner.run_trials,
-                    ([trial.id], self._store.uri, timeout, self._debug_mode),
-                    error_callback=handle_err,
-                )
-
-    def join(self):
-        if self._pool is not None:
-            for _, result in self._trial_id_to_result.items():
-                result.get()
-
-    def cancel(self):
-        if self._pool is not None:
-            self._pool.terminate()
-
-    @property
-    def n_cpus(self):
-        return self._n_cpus
-
-    @property
-    def store(self):
-        return self._store
-
-    @property
-    def debug_mode(self):
-        return self._debug_mode
-
-    @property
-    def wheel_filepaths(self):
-        return self._wheel_filepaths
+""" Runs all trials on local machine that is running powerlift.
+
+This currently uses multiprocessing pools to handle parallelism.
+"""
+
+from powerlift.bench.store import Store
+from powerlift.executors.base import Executor
+from multiprocessing import Pool
+from typing import Iterable, List
+from powerlift.executors.base import handle_err
+
+
+class LocalMachine(Executor):
+    def __init__(
+        self,
+        store: Store,
+        n_cpus: int = None,
+        debug_mode: bool = False,
+        wheel_filepaths: List[str] = None,
+    ):
+        """Runs trial runs on the local machine.
+
+        Args:
+            store (Store): Store that houses trials.
+            n_cpus (int, optional): Max number of cpus to run on.. Defaults to None.
+            debug_mode (bool, optional): Restricts to a single thread and raises exceptions. Good for debugging.
+            wheel_filepaths (List[str], optional): List of wheel filepaths to install on ACI trial run. Defaults to None.
+        """
+        if debug_mode:
+            self._pool = None
+        else:
+            self._pool = Pool(processes=n_cpus)
+
+        self._trial_id_to_result = {}
+        self._store = store
+        self._n_cpus = n_cpus
+        self._debug_mode = debug_mode
+        self._wheel_filepaths = wheel_filepaths
+
+    def __del__(self):
+        if self._pool is not None:
+            self._pool.close()
+
+    def submit(self, trial_run_fn, trials: Iterable, timeout=None):
+        from powerlift.run import __main__ as runner
+
+        self._store.add_trial_run_fn(
+            [x.id for x in trials], trial_run_fn, self._wheel_filepaths
+        )
+        for trial in trials:
+            if self._pool is None:
+                try:
+                    debug_fn = trial_run_fn if self._debug_mode else None
+                    res = runner.run_trials(
+                        [trial.id],
+                        self._store.uri,
+                        timeout,
+                        self._debug_mode,
+                        debug_fn=debug_fn,
+                    )
+                    self._trial_id_to_result[trial.id] = res
+                except Exception as e:
+                    self._trial_id_to_result[trial.id] = e
+                    if self._debug_mode:
+                        raise e
+            else:
+                self._trial_id_to_result[trial.id] = self._pool.apply_async(
+                    runner.run_trials,
+                    ([trial.id], self._store.uri, timeout, self._debug_mode),
+                    error_callback=handle_err,
+                )
+
+    def join(self):
+        if self._pool is not None:
+            for _, result in self._trial_id_to_result.items():
+                result.get()
+
+    def cancel(self):
+        if self._pool is not None:
+            self._pool.terminate()
+
+    @property
+    def n_cpus(self):
+        return self._n_cpus
+
+    @property
+    def store(self):
+        return self._store
+
+    @property
+    def debug_mode(self):
+        return self._debug_mode
+
+    @property
+    def wheel_filepaths(self):
+        return self._wheel_filepaths
```

## powerlift/measures/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-""" Measures that may affect task or trial.
-"""
-
-from powerlift.measures.task_measures import class_stats, regression_stats, data_stats
+""" Measures that may affect task or trial.
+"""
+
+from powerlift.measures.task_measures import class_stats, regression_stats, data_stats
```

## powerlift/measures/task_measures.py

 * *Ordering differences only*

```diff
@@ -1,148 +1,148 @@
-""" Task related measures. """
-
-from math import log, e
-from numbers import Number
-from typing import Iterable, List, Tuple
-import numpy as np
-import pandas as pd
-
-
-def entropy(labels: Iterable, base: Number = None, normalized: bool = False) -> Number:
-    """Computes entropy of label distribution.
-
-    Args:
-        labels (Iterable): Labels to compute entropy.
-        base (Number, optional): Logarithmic base. Defaults to None.
-        normalized (bool, optional): Return normalized entropy instead. Defaults to False.
-
-    Returns:
-        Number: Entropy.
-    """
-
-    n_labels = len(labels)
-
-    if n_labels <= 1:
-        return 0
-
-    value, counts = np.unique(labels, return_counts=True)
-    probs = counts / n_labels
-    n_classes = np.count_nonzero(probs)
-
-    if n_classes <= 1:
-        return 0
-
-    ent = 0.0
-
-    # Compute entropy
-    base = e if base is None else base
-    for i in probs:
-        ent -= i * log(i, base)
-
-    if normalized:
-        return ent / log(len(value), base)
-    else:
-        return ent
-
-
-def class_stats(y: pd.Series) -> List[Tuple[str, str, float, bool]]:
-    """Compute classification label statistics.
-
-    Args:
-        y (pd.Series): Labels.
-
-    Returns:
-        List[Tuple[str, str, float, bool]]: Tuples of form: (name, description, value, is_lower_better).
-    """
-    labels = y.values
-    labels_unique = np.unique(labels, return_counts=True)
-    labels_min_cnt = np.min(labels_unique[1])
-    labels_max_cnt = np.max(labels_unique[1])
-
-    return [
-        (
-            "class_normalized_entropy",
-            "Normalized entropy of classes.",
-            float(entropy(labels, normalized=True)),
-            False,
-        ),
-        ("num_classes", "Number of distinct classes.", float(len(labels_unique)), True),
-        ("min_class_count", "Minimum class count.", float(labels_min_cnt), False),
-        ("max_class_count", "Maximum class count.", float(labels_max_cnt), False),
-        (
-            "avg_class_count",
-            "Average class count.",
-            float(np.average(labels_unique[1])),
-            False,
-        ),
-    ]
-
-
-def regression_stats(y: pd.Series) -> List[Tuple[str, str, float, bool]]:
-    """Computes regression statistics on response.
-
-    Args:
-        y (pd.Series): Response.
-
-    Returns:
-        List[Tuple[str, str, float, bool]]: Tuples of form: (name, description, value, is_lower_better).
-    """
-    labels = y.values
-    labels_avg = np.average(labels)
-    labels_max = max(labels)
-    labels_min = min(labels)
-    return [
-        ("response_min_val", "Minimum value of response.", float(labels_min), True),
-        ("response_avg_val", "Average value of response.", float(labels_avg), True),
-        ("response_max_val", "Maximum value of response.", float(labels_max), True),
-    ]
-
-
-def data_stats(
-    X: pd.DataFrame, categorical_mask: Iterable[bool]
-) -> List[Tuple[str, str, float, bool]]:
-    """Computes data statistics on instances.
-
-    Args:
-        X (pd.DataFrame): Instances.
-        categorical_mask (Iterable[bool]): Boolean mask on which columns are categorical.
-
-    Returns:
-        List[Tuple[str, str, float, bool]]: Tuples of form: (name, description, value, is_lower_better).
-    """
-    data = X.values
-
-    avg_prop_special_values = 0.0
-    for index, _ in enumerate(X):
-        prop_special_values = 0.0
-        if categorical_mask[index]:
-            for val in data[:, index]:
-                if pd.isnull(val) or val == " " or val == "":
-                    prop_special_values += 1.0
-        else:
-            for val in data[:, index]:
-                if pd.isnull(val) or val == 0:
-                    prop_special_values += 1.0
-        prop_special_values /= X.shape[0]
-        avg_prop_special_values += prop_special_values
-    avg_prop_special_values /= X.shape[1]
-
-    prop_cat_cols = float(sum([int(x) for x in categorical_mask]))
-    prop_cat_cols /= len(categorical_mask)
-
-    return [
-        ("n_rows", "Number of rows.", float(X.shape[0]), False),
-        ("n_cols", "Number of columns.", float(X.shape[1]), True),
-        (
-            "prop_cat_cols",
-            "Proportion of categorical columns",
-            float(prop_cat_cols),
-            True,
-        ),
-        ("row_col_ratio", "Row to column ratio", float(X.shape[0] / X.shape[1]), True),
-        (
-            "avg_prop_special_values",
-            "Average number of special value proportion per column.",
-            float(avg_prop_special_values),
-            True,
-        ),
-    ]
+""" Task related measures. """
+
+from math import log, e
+from numbers import Number
+from typing import Iterable, List, Tuple
+import numpy as np
+import pandas as pd
+
+
+def entropy(labels: Iterable, base: Number = None, normalized: bool = False) -> Number:
+    """Computes entropy of label distribution.
+
+    Args:
+        labels (Iterable): Labels to compute entropy.
+        base (Number, optional): Logarithmic base. Defaults to None.
+        normalized (bool, optional): Return normalized entropy instead. Defaults to False.
+
+    Returns:
+        Number: Entropy.
+    """
+
+    n_labels = len(labels)
+
+    if n_labels <= 1:
+        return 0
+
+    value, counts = np.unique(labels, return_counts=True)
+    probs = counts / n_labels
+    n_classes = np.count_nonzero(probs)
+
+    if n_classes <= 1:
+        return 0
+
+    ent = 0.0
+
+    # Compute entropy
+    base = e if base is None else base
+    for i in probs:
+        ent -= i * log(i, base)
+
+    if normalized:
+        return ent / log(len(value), base)
+    else:
+        return ent
+
+
+def class_stats(y: pd.Series) -> List[Tuple[str, str, float, bool]]:
+    """Compute classification label statistics.
+
+    Args:
+        y (pd.Series): Labels.
+
+    Returns:
+        List[Tuple[str, str, float, bool]]: Tuples of form: (name, description, value, is_lower_better).
+    """
+    labels = y.values
+    labels_unique = np.unique(labels, return_counts=True)
+    labels_min_cnt = np.min(labels_unique[1])
+    labels_max_cnt = np.max(labels_unique[1])
+
+    return [
+        (
+            "class_normalized_entropy",
+            "Normalized entropy of classes.",
+            float(entropy(labels, normalized=True)),
+            False,
+        ),
+        ("num_classes", "Number of distinct classes.", float(len(labels_unique)), True),
+        ("min_class_count", "Minimum class count.", float(labels_min_cnt), False),
+        ("max_class_count", "Maximum class count.", float(labels_max_cnt), False),
+        (
+            "avg_class_count",
+            "Average class count.",
+            float(np.average(labels_unique[1])),
+            False,
+        ),
+    ]
+
+
+def regression_stats(y: pd.Series) -> List[Tuple[str, str, float, bool]]:
+    """Computes regression statistics on response.
+
+    Args:
+        y (pd.Series): Response.
+
+    Returns:
+        List[Tuple[str, str, float, bool]]: Tuples of form: (name, description, value, is_lower_better).
+    """
+    labels = y.values
+    labels_avg = np.average(labels)
+    labels_max = max(labels)
+    labels_min = min(labels)
+    return [
+        ("response_min_val", "Minimum value of response.", float(labels_min), True),
+        ("response_avg_val", "Average value of response.", float(labels_avg), True),
+        ("response_max_val", "Maximum value of response.", float(labels_max), True),
+    ]
+
+
+def data_stats(
+    X: pd.DataFrame, categorical_mask: Iterable[bool]
+) -> List[Tuple[str, str, float, bool]]:
+    """Computes data statistics on instances.
+
+    Args:
+        X (pd.DataFrame): Instances.
+        categorical_mask (Iterable[bool]): Boolean mask on which columns are categorical.
+
+    Returns:
+        List[Tuple[str, str, float, bool]]: Tuples of form: (name, description, value, is_lower_better).
+    """
+    data = X.values
+
+    avg_prop_special_values = 0.0
+    for index, _ in enumerate(X):
+        prop_special_values = 0.0
+        if categorical_mask[index]:
+            for val in data[:, index]:
+                if pd.isnull(val) or val == " " or val == "":
+                    prop_special_values += 1.0
+        else:
+            for val in data[:, index]:
+                if pd.isnull(val) or val == 0:
+                    prop_special_values += 1.0
+        prop_special_values /= X.shape[0]
+        avg_prop_special_values += prop_special_values
+    avg_prop_special_values /= X.shape[1]
+
+    prop_cat_cols = float(sum([int(x) for x in categorical_mask]))
+    prop_cat_cols /= len(categorical_mask)
+
+    return [
+        ("n_rows", "Number of rows.", float(X.shape[0]), False),
+        ("n_cols", "Number of columns.", float(X.shape[1]), True),
+        (
+            "prop_cat_cols",
+            "Proportion of categorical columns",
+            float(prop_cat_cols),
+            True,
+        ),
+        ("row_col_ratio", "Row to column ratio", float(X.shape[0] / X.shape[1]), True),
+        (
+            "avg_prop_special_values",
+            "Average number of special value proportion per column.",
+            float(avg_prop_special_values),
+            True,
+        ),
+    ]
```

## powerlift/run/__main__.py

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-""" This is called to run a trial by worker nodes (local / remote). """
-
-
-def run_trials(trial_ids, db_url, timeout, raise_exception, debug_fn=None):
-    """Runs trials. Includes wheel installation and timeouts."""
-    from powerlift.bench.store import Store
-    import traceback
-    from powerlift.executors.base import timed_run
-    from powerlift.bench.store import MIMETYPE_FUNC, MIMETYPE_WHEEL, BytesParser
-    from powerlift.bench.experiment import Store
-    import subprocess
-    import tempfile
-    from pathlib import Path
-    import sys
-
-    store = Store(db_url)
-    for trial_id in trial_ids:
-        trial = store.find_trial_by_id(trial_id)
-        if trial is None:
-            raise RuntimeError(f"No trial found for id {trial_id}")
-
-        # Handle input assets
-        trial_run_fn = None
-        with tempfile.TemporaryDirectory() as dirpath:
-            for input_asset in trial.input_assets:
-                if input_asset.mimetype == MIMETYPE_WHEEL:
-                    wheel = BytesParser.deserialize(
-                        input_asset.mimetype, input_asset.embedded
-                    )
-                    filepath = Path(dirpath, input_asset.name)
-                    with open(filepath, "wb") as f:
-                        f.write(wheel.content)
-                    subprocess.check_call(
-                        [sys.executable, "-m", "pip", "install", filepath]
-                    )
-                elif input_asset.mimetype == MIMETYPE_FUNC:
-                    trial_run_fn = BytesParser.deserialize(
-                        MIMETYPE_FUNC, input_asset.embedded
-                    )
-                else:
-                    continue
-        if debug_fn is not None:
-            trial_run_fn = debug_fn
-
-        if trial_run_fn is None:
-            raise RuntimeError("No trial run function found.")
-
-        # Run trial
-        store.start_trial(trial.id)
-        errmsg = None
-        try:
-            _, duration, timed_out = timed_run(
-                lambda: trial_run_fn(trial), timeout_seconds=timeout
-            )
-            if timed_out:
-                raise RuntimeError(f"Timeout failure ({duration})")
-        except Exception as e:
-            errmsg = traceback.format_exc()
-            if raise_exception:
-                raise e
-        finally:
-            store.end_trial(trial.id, errmsg)
-
-
-if __name__ == "__main__":
-    import os
-
-    trial_ids = os.getenv("TRIAL_IDS").split(",")
-    db_url = os.getenv("DB_URL")
-    timeout = float(os.getenv("TIMEOUT", 0.0))
-    raise_exception = os.getenv("RAISE_EXCEPTION", False)
-
-    run_trials(trial_ids, db_url, timeout, raise_exception)
+""" This is called to run a trial by worker nodes (local / remote). """
+
+
+def run_trials(trial_ids, db_url, timeout, raise_exception, debug_fn=None):
+    """Runs trials. Includes wheel installation and timeouts."""
+    from powerlift.bench.store import Store
+    import traceback
+    from powerlift.executors.base import timed_run
+    from powerlift.bench.store import MIMETYPE_FUNC, MIMETYPE_WHEEL, BytesParser
+    from powerlift.bench.experiment import Store
+    import subprocess
+    import tempfile
+    from pathlib import Path
+    import sys
+
+    store = Store(db_url)
+    for trial_id in trial_ids:
+        trial = store.find_trial_by_id(trial_id)
+        if trial is None:
+            raise RuntimeError(f"No trial found for id {trial_id}")
+
+        # Handle input assets
+        trial_run_fn = None
+        with tempfile.TemporaryDirectory() as dirpath:
+            for input_asset in trial.input_assets:
+                if input_asset.mimetype == MIMETYPE_WHEEL:
+                    wheel = BytesParser.deserialize(
+                        input_asset.mimetype, input_asset.embedded
+                    )
+                    filepath = Path(dirpath, input_asset.name)
+                    with open(filepath, "wb") as f:
+                        f.write(wheel.content)
+                    subprocess.check_call(
+                        [sys.executable, "-m", "pip", "install", filepath]
+                    )
+                elif input_asset.mimetype == MIMETYPE_FUNC:
+                    trial_run_fn = BytesParser.deserialize(
+                        MIMETYPE_FUNC, input_asset.embedded
+                    )
+                else:
+                    continue
+        if debug_fn is not None:
+            trial_run_fn = debug_fn
+
+        if trial_run_fn is None:
+            raise RuntimeError("No trial run function found.")
+
+        # Run trial
+        store.start_trial(trial.id)
+        errmsg = None
+        try:
+            _, duration, timed_out = timed_run(
+                lambda: trial_run_fn(trial), timeout_seconds=timeout
+            )
+            if timed_out:
+                raise RuntimeError(f"Timeout failure ({duration})")
+        except Exception as e:
+            errmsg = traceback.format_exc()
+            if raise_exception:
+                raise e
+        finally:
+            store.end_trial(trial.id, errmsg)
+
+
+if __name__ == "__main__":
+    import os
+
+    trial_ids = os.getenv("TRIAL_IDS").split(",")
+    db_url = os.getenv("DB_URL")
+    timeout = float(os.getenv("TIMEOUT", 0.0))
+    raise_exception = os.getenv("RAISE_EXCEPTION", False)
+
+    run_trials(trial_ids, db_url, timeout, raise_exception)
```

## Comparing `powerlift-0.1.0.dist-info/LICENSE` & `powerlift-0.1.1.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) 2023 The InterpretML Contributors
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+MIT License
+
+Copyright (c) 2023 The InterpretML Contributors
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
```

## Comparing `powerlift-0.1.0.dist-info/METADATA` & `powerlift-0.1.1.dist-info/METADATA`

 * *Files 14% similar despite different names*

```diff
@@ -1,157 +1,157 @@
-Metadata-Version: 2.1
-Name: powerlift
-Version: 0.1.0
-Summary: Interactive Benchmarking for Machine Learning.
-Home-page: https://github.com/interpretml/interpret
-Author: The InterpretML Contributors
-Author-email: interpret@microsoft.com
-Project-URL: Bug Tracker, https://github.com/interpretml/interpret/issues
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.7
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: SQLAlchemy >=1.4
-Requires-Dist: sqlalchemy-utils >=0.38
-Requires-Dist: tqdm
-Requires-Dist: fastparquet
-Requires-Dist: stopit
-Requires-Dist: pandas
-Requires-Dist: numpy
-Requires-Dist: pytz
-Provides-Extra: aci
-Requires-Dist: msrestazure ; extra == 'aci'
-Requires-Dist: azure-common ; extra == 'aci'
-Requires-Dist: azure-mgmt-sql ; extra == 'aci'
-Requires-Dist: azure-mgmt-resource ; extra == 'aci'
-Requires-Dist: azure-mgmt-containerinstance ; extra == 'aci'
-Requires-Dist: azure-identity ; extra == 'aci'
-Provides-Extra: datasets
-Requires-Dist: pmlb >=1.0 ; extra == 'datasets'
-Requires-Dist: openml >=0.12 ; extra == 'datasets'
-Provides-Extra: docker
-Requires-Dist: docker ; extra == 'docker'
-Provides-Extra: mssql
-Requires-Dist: pyodbc ; extra == 'mssql'
-Provides-Extra: postgres
-Requires-Dist: psycopg2 >=2.9 ; extra == 'postgres'
-Provides-Extra: testing
-Requires-Dist: pytest ; extra == 'testing'
-Requires-Dist: pytest-cov ; extra == 'testing'
-Requires-Dist: scikit-learn ; extra == 'testing'
-Requires-Dist: python-dotenv ; extra == 'testing'
-
-# Powerlift
-
-![License](https://img.shields.io/github/license/interpretml/interpret.svg?style=flat-square)
-<br/>
-> ### Advancing the state of machine learning?
-> ### With 5-10 datasets? Wake me up when I'm dead.
-
-Powerlift is all about testing machine learning techniques across many, many datasets. So many, that we had run into design of experiment concerns. So many, that we had to develop a package for it.
-
-Yes, we run this for InterpretML on as many docker containers we can run in parallel on. Why wait days for benchmark evalations when you can wait for minutes? Rhetorical question, please don't hurt me.
-
-```python
-def trial_filter(task):
-    if task.problem == "binary" and task.scalar_measure("n_rows") <= 10000:
-        return ["rf", "svm"]
-    return []
-
-def trial_runner(trial):
-    from sklearn.ensemble import RandomForestClassifier
-    from sklearn.svm import LinearSVC
-    from sklearn.calibration import CalibratedClassifierCV
-    from sklearn.model_selection import train_test_split
-    from sklearn.metrics import roc_auc_score
-    from sklearn.pipeline import Pipeline
-    from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
-    from sklearn.compose import ColumnTransformer
-    from sklearn.impute import SimpleImputer
-
-    if trial.task.problem == "binary" and trial.task.origin == "openml":
-        X, y, meta = trial.task.data(["X", "y", "meta"])
-
-        # Holdout split
-        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3)
-
-        # Build preprocessor
-        is_cat = meta["categorical_mask"]
-        cat_cols = [idx for idx in range(X.shape[1]) if is_cat[idx]]
-        num_cols = [idx for idx in range(X.shape[1]) if not is_cat[idx]]
-        cat_ohe_step = ("ohe", OneHotEncoder(sparse_output=True, handle_unknown="ignore"))
-        cat_pipe = Pipeline([cat_ohe_step])
-        num_pipe = Pipeline([("identity", FunctionTransformer())])
-        transformers = [("cat", cat_pipe, cat_cols), ("num", num_pipe, num_cols)]
-        ct = Pipeline(
-            [
-                ("ct", ColumnTransformer(transformers=transformers)),
-                (
-                    "missing",
-                    SimpleImputer(add_indicator=True, strategy="most_frequent"),
-                ),
-            ]
-        )
-        # Connect preprocessor with target learner
-        if trial.method.name == "svm":
-            clf = Pipeline([("ct", ct), ("est", CalibratedClassifierCV(LinearSVC()))])
-        else:
-            clf = Pipeline([("ct", ct), ("est", RandomForestClassifier())])
-
-        # Train
-        clf.fit(X_tr, y_tr)
-
-        # Predict
-        predictions = clf.predict_proba(X_te)[:, 1]
-
-        # Score
-        auc = roc_auc_score(y_te, predictions)
-        trial.log("auc", auc)
-
-# Create experiment within database.
-import os
-from powerlift.bench import Benchmark
-benchmark = Benchmark(f"sqlite:///{os.getcwd()}/powerlift.db", name="SVM vs RF")
-
-# Only run this once for the database (downloads PMLB and OpenML CC18 datasets).
-from powerlift.bench import populate_with_datasets
-populate_with_datasets(benchmark.store, cache_dir="~/.powerlift")
-
-# Run experiment
-benchmark.run(trial_runner, trial_filter)
-benchmark.wait_until_complete()
-```
-
-This can also be run on Azure Container Instances where needed.
-```python
-# Run experiment (but in ACI).
-from powerlift.executors import AzureContainerInstance
-azure_tenant_id = os.getenv("AZURE_TENANT_ID")
-azure_client_id = os.getenv("AZURE_CLIENT_ID")
-azure_client_secret = os.getenv("AZURE_CLIENT_SECRET")
-subscription_id = os.getenv("AZURE_SUBSCRIPTION_ID")
-resource_group = os.getenv("AZURE_RESOURCE_GROUP")
-store = Store(os.getenv("AZURE_DB_URL"))
-
-executor = AzureContainerInstance(
-    store,
-    azure_tenant_id,
-    azure_client_id,
-    azure_client_secret,
-    subscription_id,
-    resource_group,
-    n_running_containers=5,
-    num_cores=1,
-    mem_size_gb=2,
-    raise_exception=True,
-)
-benchmark = Benchmark(store, name="SVM vs RF")
-benchmark.run(trial_runner, trial_filter, timeout=10, executor=executor)
-benchmark.wait_until_complete()
-```
-
-## Install
-`pip install powerlift[datasets]`
-
-That's it, go get 'em boss.
+Metadata-Version: 2.1
+Name: powerlift
+Version: 0.1.1
+Summary: Interactive Benchmarking for Machine Learning.
+Home-page: https://github.com/interpretml/interpret
+Author: The InterpretML Contributors
+Author-email: interpret@microsoft.com
+Project-URL: Bug Tracker, https://github.com/interpretml/interpret/issues
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.7
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: SQLAlchemy >=1.4
+Requires-Dist: sqlalchemy-utils >=0.38
+Requires-Dist: tqdm
+Requires-Dist: fastparquet
+Requires-Dist: stopit
+Requires-Dist: pandas
+Requires-Dist: numpy
+Requires-Dist: pytz
+Provides-Extra: aci
+Requires-Dist: msrestazure ; extra == 'aci'
+Requires-Dist: azure-common ; extra == 'aci'
+Requires-Dist: azure-mgmt-sql ; extra == 'aci'
+Requires-Dist: azure-mgmt-resource ; extra == 'aci'
+Requires-Dist: azure-mgmt-containerinstance ; extra == 'aci'
+Requires-Dist: azure-identity ; extra == 'aci'
+Provides-Extra: datasets
+Requires-Dist: pmlb >=1.0 ; extra == 'datasets'
+Requires-Dist: openml >=0.12 ; extra == 'datasets'
+Provides-Extra: docker
+Requires-Dist: docker ; extra == 'docker'
+Provides-Extra: mssql
+Requires-Dist: pyodbc ; extra == 'mssql'
+Provides-Extra: postgres
+Requires-Dist: psycopg2 >=2.9 ; extra == 'postgres'
+Provides-Extra: testing
+Requires-Dist: pytest ; extra == 'testing'
+Requires-Dist: pytest-cov ; extra == 'testing'
+Requires-Dist: scikit-learn ; extra == 'testing'
+Requires-Dist: python-dotenv ; extra == 'testing'
+
+# Powerlift
+
+![License](https://img.shields.io/github/license/interpretml/interpret.svg?style=flat-square)
+<br/>
+> ### Advancing the state of machine learning?
+> ### With 5-10 datasets? Wake me up when I'm dead.
+
+Powerlift is all about testing machine learning techniques across many, many datasets. So many, that we had run into design of experiment concerns. So many, that we had to develop a package for it.
+
+Yes, we run this for InterpretML on as many docker containers we can run in parallel on. Why wait days for benchmark evalations when you can wait for minutes? Rhetorical question, please don't hurt me.
+
+```python
+def trial_filter(task):
+    if task.problem == "binary" and task.scalar_measure("n_rows") <= 10000:
+        return ["rf", "svm"]
+    return []
+
+def trial_runner(trial):
+    from sklearn.ensemble import RandomForestClassifier
+    from sklearn.svm import LinearSVC
+    from sklearn.calibration import CalibratedClassifierCV
+    from sklearn.model_selection import train_test_split
+    from sklearn.metrics import roc_auc_score
+    from sklearn.pipeline import Pipeline
+    from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
+    from sklearn.compose import ColumnTransformer
+    from sklearn.impute import SimpleImputer
+
+    if trial.task.problem == "binary" and trial.task.origin == "openml":
+        X, y, meta = trial.task.data(["X", "y", "meta"])
+
+        # Holdout split
+        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3)
+
+        # Build preprocessor
+        is_cat = meta["categorical_mask"]
+        cat_cols = [idx for idx in range(X.shape[1]) if is_cat[idx]]
+        num_cols = [idx for idx in range(X.shape[1]) if not is_cat[idx]]
+        cat_ohe_step = ("ohe", OneHotEncoder(sparse_output=True, handle_unknown="ignore"))
+        cat_pipe = Pipeline([cat_ohe_step])
+        num_pipe = Pipeline([("identity", FunctionTransformer())])
+        transformers = [("cat", cat_pipe, cat_cols), ("num", num_pipe, num_cols)]
+        ct = Pipeline(
+            [
+                ("ct", ColumnTransformer(transformers=transformers)),
+                (
+                    "missing",
+                    SimpleImputer(add_indicator=True, strategy="most_frequent"),
+                ),
+            ]
+        )
+        # Connect preprocessor with target learner
+        if trial.method.name == "svm":
+            clf = Pipeline([("ct", ct), ("est", CalibratedClassifierCV(LinearSVC()))])
+        else:
+            clf = Pipeline([("ct", ct), ("est", RandomForestClassifier())])
+
+        # Train
+        clf.fit(X_tr, y_tr)
+
+        # Predict
+        predictions = clf.predict_proba(X_te)[:, 1]
+
+        # Score
+        auc = roc_auc_score(y_te, predictions)
+        trial.log("auc", auc)
+
+# Create experiment within database.
+import os
+from powerlift.bench import Benchmark
+benchmark = Benchmark(f"sqlite:///{os.getcwd()}/powerlift.db", name="SVM vs RF")
+
+# Only run this once for the database (downloads PMLB and OpenML CC18 datasets).
+from powerlift.bench import populate_with_datasets
+populate_with_datasets(benchmark.store, cache_dir="~/.powerlift")
+
+# Run experiment
+benchmark.run(trial_runner, trial_filter)
+benchmark.wait_until_complete()
+```
+
+This can also be run on Azure Container Instances where needed.
+```python
+# Run experiment (but in ACI).
+from powerlift.executors import AzureContainerInstance
+azure_tenant_id = os.getenv("AZURE_TENANT_ID")
+azure_client_id = os.getenv("AZURE_CLIENT_ID")
+azure_client_secret = os.getenv("AZURE_CLIENT_SECRET")
+subscription_id = os.getenv("AZURE_SUBSCRIPTION_ID")
+resource_group = os.getenv("AZURE_RESOURCE_GROUP")
+store = Store(os.getenv("AZURE_DB_URL"))
+
+executor = AzureContainerInstance(
+    store,
+    azure_tenant_id,
+    azure_client_id,
+    azure_client_secret,
+    subscription_id,
+    resource_group,
+    n_running_containers=5,
+    num_cores=1,
+    mem_size_gb=2,
+    raise_exception=True,
+)
+benchmark = Benchmark(store, name="SVM vs RF")
+benchmark.run(trial_runner, trial_filter, timeout=10, executor=executor)
+benchmark.wait_until_complete()
+```
+
+## Install
+`pip install powerlift[datasets]`
+
+That's it, go get 'em boss.
```

## Comparing `powerlift-0.1.0.dist-info/RECORD` & `powerlift-0.1.1.dist-info/RECORD`

 * *Files 23% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-powerlift/bench/__init__.py,sha256=IeN8ltKpuq4R6IWslCMAhJXyriPnEJyAV9FaOd3KCCg,704
-powerlift/bench/benchmark.py,sha256=kQtrqo0u0C8-_aPtREvBHkNU1DxPoENQFqrCE0T-L3c,6614
-powerlift/bench/experiment.py,sha256=8URcsrUBGs3dSTtSocPwpb21CFBa2XfaNJfVYCov26o,6104
-powerlift/bench/store.py,sha256=d-YBPa9g24BKWT-o--STSXiHjZHZgek0o_0RQNeWObE,36544
-powerlift/db/__init__.py,sha256=e_nsY1u2JG6OofJofbHuEqGL14kahF2ws9NIXQIyhsk,884
-powerlift/db/actions.py,sha256=uSMAvmLsYHuApeH4eoJ17ZOegWGUOQNaAUF33kEFmSs,1419
-powerlift/db/schema.py,sha256=gPf6-cHAEl7NkXho70dW6aCVSQXrKsQVUk_2_Xj78Xw,7196
-powerlift/executors/__init__.py,sha256=gBJPY9meaEU-TM5yzD-trnoTAnYWgTJU5a8Y6_dWwOI,239
-powerlift/executors/azure_ci.py,sha256=0MfZRWXQ2gLUOKDOU27GVEQMFhZh7CMvsgfZOZalVkk,7850
-powerlift/executors/base.py,sha256=bcHllrFLqCwXMieuzPAm3rR2lDL7krgzx0VcLYl_xLo,2355
-powerlift/executors/docker.py,sha256=h_npeq1XDC2SajHEiZPPeDz82rO4ZQiZOJG_q7n15EI,3128
-powerlift/executors/localmachine.py,sha256=0H2jGvdMCTueIYsfG7e9YcE_-eyvZKx3k1P_qp7hHIs,3165
-powerlift/measures/__init__.py,sha256=ASBwD_ielUUPWQ3_5JeuY7HtH3KbIe7JmZxB6uNXq_o,140
-powerlift/measures/task_measures.py,sha256=qDeScKJAZxNvpzrKNINBiEvwgnaWZxxxAtthtNQ-094,4648
-powerlift/run/__main__.py,sha256=bosDyQk7zPrmKTz_HSHtsTi3w1PAYV5aJzgwnQ7VYHw,2712
-powerlift-0.1.0.dist-info/LICENSE,sha256=28mzl0WFkl0ZOjIMeYyeinD51oCM_9eEx-LF3Pj1v3I,1106
-powerlift-0.1.0.dist-info/METADATA,sha256=hfE6iSXtAXAsE52Y0dnYTn850XyFvyC10FcDy_g5hBM,5897
-powerlift-0.1.0.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-powerlift-0.1.0.dist-info/top_level.txt,sha256=PqZeILUPLCstbaDfaAG14A4rCLPYynR_EKCJjGGW88s,10
-powerlift-0.1.0.dist-info/RECORD,,
+powerlift/bench/__init__.py,sha256=HqW1l4O5fLWgPdfdCxQCYGYhDOPNiUhAMEyIKvjHEFo,712
+powerlift/bench/benchmark.py,sha256=QHNyfwsOCAac9pXrXSq2O3GI2VB20Rs6i3_3DDH240I,6431
+powerlift/bench/experiment.py,sha256=LCjzoJVSBa96XvFbwYvk1obMbKJHal2NU4fzL0r64PI,5888
+powerlift/bench/store.py,sha256=hhQumyqY9KSO8cmoXuFFc7wFeuZf2SYJyN8RE1AC5ec,36279
+powerlift/db/__init__.py,sha256=f0S6DsKHCEB8iZ6cVh7MX-1kl4KHtgVsmJcbtfEknEU,856
+powerlift/db/actions.py,sha256=qc9NxS9TLNfx9qYaPlpS4KpJhuGfSoKe3andHYJI32g,1364
+powerlift/db/schema.py,sha256=T1pCXaiLew_04Oadpg5IE6aYmwRokh-kU5m68lutMKI,6971
+powerlift/executors/__init__.py,sha256=Xxy4Qzjfe-hFY1ZarMYBEjH076CKHuHOTxqszoPYvuw,233
+powerlift/executors/azure_ci.py,sha256=hpjuyDlNFBFHwqiaVR6ZzHn7upN0NI7xJUHkJiFJN3A,7639
+powerlift/executors/base.py,sha256=Blhm0ZiF6u0_wqw8NjYKmF70fdkZsyL61zX5DfBdZoY,2275
+powerlift/executors/docker.py,sha256=1g-hphvh-g5-QuG_xyo7bhBp0SMdYIzY8EHz0D27Ixc,3041
+powerlift/executors/localmachine.py,sha256=8dYImIq5cQPVk0H4B_AiPpVGZKIhdbB05s46f717TOA,3069
+powerlift/measures/__init__.py,sha256=br7A9iJgQl7cPcPFnnJkRGEDRn_IBJMj24rVZFaSF44,136
+powerlift/measures/task_measures.py,sha256=23zwd25L_KB9Xe6LuWtBS4QBuv1F0mKnCR4HAKjsmg0,4500
+powerlift/run/__main__.py,sha256=YbXDeF33dehQgEEEFfuCctGlr2dTmNNhKjCibEg-EuE,2639
+powerlift-0.1.1.dist-info/LICENSE,sha256=7vlTCQ2WSK8wDjuX9Rv36FpE_bEP1FkMVxceVw-VTGs,1085
+powerlift-0.1.1.dist-info/METADATA,sha256=TTQeBg6P90KVXAirIukD_SuMmxXqFomozzGP7MqpQUU,5740
+powerlift-0.1.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+powerlift-0.1.1.dist-info/top_level.txt,sha256=PqZeILUPLCstbaDfaAG14A4rCLPYynR_EKCJjGGW88s,10
+powerlift-0.1.1.dist-info/RECORD,,
```

