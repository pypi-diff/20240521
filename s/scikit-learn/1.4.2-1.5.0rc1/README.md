# Comparing `tmp/scikit-learn-1.4.2.tar.gz` & `tmp/scikit_learn-1.5.0rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "scikit-learn-1.4.2.tar", last modified: Tue Apr  9 14:20:32 2024, max compression
+gzip compressed data, was "scikit_learn-1.5.0rc1.tar", last modified: Mon May  6 08:55:34 2024, max compression
```

## Comparing `scikit-learn-1.4.2.tar` & `scikit_learn-1.5.0rc1.tar`

### file list

```diff
@@ -1,1604 +1,1444 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.318612 scikit-learn-1.4.2/
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/.coveragerc
--rw-r--r--   0 runner    (1001) docker     (127)     1532 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/COPYING
--rw-r--r--   0 runner    (1001) docker     (127)     1021 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)     1729 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)     9215 2024-04-09 14:20:32.318612 scikit-learn-1.4.2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     7592 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.094610 scikit-learn-1.4.2/doc/
--rw-r--r--   0 runner    (1001) docker     (127)     5038 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/README.md
--rw-r--r--   0 runner    (1001) docker     (127)    16909 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/about.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4750 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/authors.rst
--rw-r--r--   0 runner    (1001) docker     (127)      584 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/authors_emeritus.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.094610 scikit-learn-1.4.2/doc/binder/
--rw-r--r--   0 runner    (1001) docker     (127)      255 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/binder/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    25058 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/common_pitfalls.rst
--rw-r--r--   0 runner    (1001) docker     (127)      552 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/communication_team.rst
--rw-r--r--   0 runner    (1001) docker     (127)       17 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/communication_team_emeritus.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.094610 scikit-learn-1.4.2/doc/computing/
--rw-r--r--   0 runner    (1001) docker     (127)    17218 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/computing/computational_performance.rst
--rw-r--r--   0 runner    (1001) docker     (127)    14899 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/computing/parallelism.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6368 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/computing/scaling_strategies.rst
--rw-r--r--   0 runner    (1001) docker     (127)      313 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/computing.rst
--rw-r--r--   0 runner    (1001) docker     (127)    27224 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/conf.py
--rw-r--r--   0 runner    (1001) docker     (127)     6453 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/conftest.py
--rw-r--r--   0 runner    (1001) docker     (127)      406 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/contents.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1841 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/contributor_experience_team.rst
--rw-r--r--   0 runner    (1001) docker     (127)       15 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/contributor_experience_team_emeritus.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1381 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/data_transforms.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.094610 scikit-learn-1.4.2/doc/datasets/
--rw-r--r--   0 runner    (1001) docker     (127)    12854 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/datasets/loading_other_datasets.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1017 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/datasets/real_world.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4095 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/datasets/sample_generators.rst
--rw-r--r--   0 runner    (1001) docker     (127)      975 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/datasets/toy_dataset.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2657 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/datasets.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.098610 scikit-learn-1.4.2/doc/developers/
--rw-r--r--   0 runner    (1001) docker     (127)    19897 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/advanced_installation.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6569 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/bug_triaging.rst
--rw-r--r--   0 runner    (1001) docker     (127)    61458 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/contributing.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6350 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/cython.rst
--rw-r--r--   0 runner    (1001) docker     (127)    38663 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/develop.rst
--rw-r--r--   0 runner    (1001) docker     (127)      382 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    19748 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/maintainer.rst
--rw-r--r--   0 runner    (1001) docker     (127)    15265 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/minimal_reproducer.rst
--rw-r--r--   0 runner    (1001) docker     (127)    16960 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/performance.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4374 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/plotting.rst
--rw-r--r--   0 runner    (1001) docker     (127)    14388 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/tips.rst
--rw-r--r--   0 runner    (1001) docker     (127)     9447 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/developers/utilities.rst
--rw-r--r--   0 runner    (1001) docker     (127)      186 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/dispatching.rst
--rw-r--r--   0 runner    (1001) docker     (127)      542 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/documentation_team.rst
--rw-r--r--   0 runner    (1001) docker     (127)    23949 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/faq.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10279 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/getting_started.rst
--rw-r--r--   0 runner    (1001) docker     (127)    90170 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/glossary.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10249 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/governance.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.110610 scikit-learn-1.4.2/doc/images/
--rw-r--r--   0 runner    (1001) docker     (127)    11616 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/axa-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    17847 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/axa.png
--rw-r--r--   0 runner    (1001) docker     (127)    31049 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/bcg.png
--rw-r--r--   0 runner    (1001) docker     (127)    54774 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/beta_divergence.png
--rw-r--r--   0 runner    (1001) docker     (127)    12497 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/bnp-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    21156 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/bnp.png
--rw-r--r--   0 runner    (1001) docker     (127)    13205 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/cds-logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     4428 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/chanel-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    20091 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/chanel.png
--rw-r--r--   0 runner    (1001) docker     (127)     1170 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/columbia-small.png
--rw-r--r--   0 runner    (1001) docker     (127)     1769 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/columbia.png
--rw-r--r--   0 runner    (1001) docker     (127)     4037 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/czi_logo.svg
--rw-r--r--   0 runner    (1001) docker     (127)     6101 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/dataiku-small.png
--rw-r--r--   0 runner    (1001) docker     (127)     9040 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/dataiku.png
--rw-r--r--   0 runner    (1001) docker     (127)    18585 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/digicosme.png
--rw-r--r--   0 runner    (1001) docker     (127)    17842 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/dysco.png
--rw-r--r--   0 runner    (1001) docker     (127)     1110 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/fnrs-logo-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    18012 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/fujitsu.png
--rw-r--r--   0 runner    (1001) docker     (127)    89394 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/generated-doc-ci.png
--rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/google-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    45148 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/grid_search_cross_validation.png
--rw-r--r--   0 runner    (1001) docker     (127)   101880 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/grid_search_workflow.png
--rw-r--r--   0 runner    (1001) docker     (127)     6345 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/huggingface_logo-noborder.png
--rw-r--r--   0 runner    (1001) docker     (127)    26245 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/inria-logo.jpg
--rw-r--r--   0 runner    (1001) docker     (127)     7105 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/inria-small.png
--rw-r--r--   0 runner    (1001) docker     (127)     9623 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/intel-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    15019 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/intel.png
--rw-r--r--   0 runner    (1001) docker     (127)    27033 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/iris.pdf
--rw-r--r--   0 runner    (1001) docker     (127)    22002 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/iris.svg
--rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/last_digit.png
--rw-r--r--   0 runner    (1001) docker     (127)    13032 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/lda_model_graph.png
--rw-r--r--   0 runner    (1001) docker     (127)    16452 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/logo_APHP.png
--rw-r--r--   0 runner    (1001) docker     (127)    30396 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/logo_APHP_text.png
--rw-r--r--   0 runner    (1001) docker     (127)     8047 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/microsoft-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    10320 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/microsoft.png
--rw-r--r--   0 runner    (1001) docker     (127)   761071 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/ml_map.png
--rw-r--r--   0 runner    (1001) docker     (127)    26546 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/multi_org_chart.png
--rw-r--r--   0 runner    (1001) docker     (127)    89381 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/multilayerperceptron_network.png
--rw-r--r--   0 runner    (1001) docker     (127)     4315 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/no_image.png
--rw-r--r--   0 runner    (1001) docker     (127)     8070 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/nvidia-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    10764 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/nvidia.png
--rw-r--r--   0 runner    (1001) docker     (127)     5485 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/nyu_short_color.png
--rw-r--r--   0 runner    (1001) docker     (127)    45027 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/permuted_non_predictive_feature.png
--rw-r--r--   0 runner    (1001) docker     (127)    43181 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/permuted_predictive_feature.png
--rw-r--r--   0 runner    (1001) docker     (127)    31108 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/plot_digits_classification.png
--rw-r--r--   0 runner    (1001) docker     (127)   124459 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/plot_face_recognition_1.png
--rw-r--r--   0 runner    (1001) docker     (127)    86623 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/plot_face_recognition_2.png
--rw-r--r--   0 runner    (1001) docker     (127)     6152 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/png-logo-inria-la-fondation.png
--rw-r--r--   0 runner    (1001) docker     (127)    20223 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/probabl.png
--rw-r--r--   0 runner    (1001) docker     (127)     8398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/quansight-labs-small.png
--rw-r--r--   0 runner    (1001) docker     (127)   124931 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/quansight-labs.png
--rw-r--r--   0 runner    (1001) docker     (127)    15495 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/rbm_graph.png
--rw-r--r--   0 runner    (1001) docker     (127)     8053 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/scikit-learn-logo-notext.png
--rw-r--r--   0 runner    (1001) docker     (127)     5468 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/scikit-learn-logo-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    29042 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/sloan_banner.png
--rw-r--r--   0 runner    (1001) docker     (127)     2236 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/sloan_logo-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    38356 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/sydney-primary.jpeg
--rw-r--r--   0 runner    (1001) docker     (127)     1728 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/sydney-stacked-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    32077 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/target_encoder_cross_validation.svg
--rw-r--r--   0 runner    (1001) docker     (127)     3779 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/telecom-small.png
--rw-r--r--   0 runner    (1001) docker     (127)    35103 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/telecom.png
--rw-r--r--   0 runner    (1001) docker     (127)    84599 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/images/visual-studio-build-tools-selection.png
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.110610 scikit-learn-1.4.2/doc/includes/
--rw-r--r--   0 runner    (1001) docker     (127)      730 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/includes/big_toc_css.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1035 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/includes/bigger_toc_css.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1082 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/inspection.rst
--rw-r--r--   0 runner    (1001) docker     (127)    13781 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/install.rst
--rw-r--r--   0 runner    (1001) docker     (127)      250 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/jupyter-lite.json
--rw-r--r--   0 runner    (1001) docker     (127)       57 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/jupyter_lite_config.json
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.114610 scikit-learn-1.4.2/doc/logos/
--rw-r--r--   0 runner    (1001) docker     (127)    48838 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/1280px-scikit-learn-logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     4261 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.114610 scikit-learn-1.4.2/doc/logos/brand_colors/
--rw-r--r--   0 runner    (1001) docker     (127)      472 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/brand_colors/colorswatch_29ABE2_cyan.png
--rw-r--r--   0 runner    (1001) docker     (127)      445 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/brand_colors/colorswatch_9B4600_brown.png
--rw-r--r--   0 runner    (1001) docker     (127)      462 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/brand_colors/colorswatch_F7931E_orange.png
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.114610 scikit-learn-1.4.2/doc/logos/brand_guidelines/
--rw-r--r--   0 runner    (1001) docker     (127)   104582 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/brand_guidelines/scikitlearn_logo_clearspace_updated.png
--rwxr-xr-x   0 runner    (1001) docker     (127)     2238 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/favicon.ico
--rwxr-xr-x   0 runner    (1001) docker     (127)   120865 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/identity.pdf
--rw-r--r--   0 runner    (1001) docker     (127)     8053 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo-notext.png
--rw-r--r--   0 runner    (1001) docker     (127)     5468 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo-small.png
--rw-r--r--   0 runner    (1001) docker     (127)     7069 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo-thumb.png
--rw-r--r--   0 runner    (1001) docker     (127)     7126 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo-without-subtitle.svg
--rw-r--r--   0 runner    (1001) docker     (127)    37902 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo.bmp
--rw-r--r--   0 runner    (1001) docker     (127)    10879 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     4699 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/logos/scikit-learn-logo.svg
--rw-r--r--   0 runner    (1001) docker     (127)     3329 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/make.bat
--rw-r--r--   0 runner    (1001) docker     (127)    13102 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/metadata_routing.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7049 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/model_persistence.rst
--rw-r--r--   0 runner    (1001) docker     (127)      321 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/model_selection.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.122610 scikit-learn-1.4.2/doc/modules/
--rw-r--r--   0 runner    (1001) docker     (127)     5614 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/array_api.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11714 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/biclustering.rst
--rw-r--r--   0 runner    (1001) docker     (127)    15309 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/calibration.rst
--rw-r--r--   0 runner    (1001) docker     (127)    42830 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/classes.rst
--rw-r--r--   0 runner    (1001) docker     (127)    94103 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/clustering.rst
--rw-r--r--   0 runner    (1001) docker     (127)    24659 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/compose.rst
--rw-r--r--   0 runner    (1001) docker     (127)    14886 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/covariance.rst
--rw-r--r--   0 runner    (1001) docker     (127)     9006 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/cross_decomposition.rst
--rw-r--r--   0 runner    (1001) docker     (127)    40831 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/cross_validation.rst
--rw-r--r--   0 runner    (1001) docker     (127)    46874 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/decomposition.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7955 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/density.rst
--rw-r--r--   0 runner    (1001) docker     (127)    70083 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/ensemble.rst
--rw-r--r--   0 runner    (1001) docker     (127)    44474 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/feature_extraction.rst
--rw-r--r--   0 runner    (1001) docker     (127)    14654 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/feature_selection.rst
--rw-r--r--   0 runner    (1001) docker     (127)    24659 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/gaussian_process.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.122610 scikit-learn-1.4.2/doc/modules/glm_data/
--rw-r--r--   0 runner    (1001) docker     (127)    28954 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/glm_data/lasso_enet_coordinate_descent.png
--rw-r--r--   0 runner    (1001) docker     (127)    63830 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/glm_data/poisson_gamma_tweedie_distributions.png
--rw-r--r--   0 runner    (1001) docker     (127)    33777 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/grid_search.rst
--rw-r--r--   0 runner    (1001) docker     (127)    15308 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/impute.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1303 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/isotonic.rst
--rw-r--r--   0 runner    (1001) docker     (127)    13990 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/kernel_approximation.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3225 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/kernel_ridge.rst
--rw-r--r--   0 runner    (1001) docker     (127)    12123 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/lda_qda.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8410 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/learning_curve.rst
--rw-r--r--   0 runner    (1001) docker     (127)    79723 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/linear_model.rst
--rw-r--r--   0 runner    (1001) docker     (127)    30280 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/manifold.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7729 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/metrics.rst
--rw-r--r--   0 runner    (1001) docker     (127)    16434 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/mixture.rst
--rw-r--r--   0 runner    (1001) docker     (127)   121343 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/model_evaluation.rst
--rw-r--r--   0 runner    (1001) docker     (127)    25692 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/multiclass.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11560 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/naive_bayes.rst
--rw-r--r--   0 runner    (1001) docker     (127)    38706 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/neighbors.rst
--rw-r--r--   0 runner    (1001) docker     (127)    15230 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/neural_networks_supervised.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6412 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/neural_networks_unsupervised.rst
--rw-r--r--   0 runner    (1001) docker     (127)    18956 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/outlier_detection.rst
--rw-r--r--   0 runner    (1001) docker     (127)    13207 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/partial_dependence.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10589 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/permutation_importance.rst
--rw-r--r--   0 runner    (1001) docker     (127)      218 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/pipeline.rst
--rw-r--r--   0 runner    (1001) docker     (127)    53915 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/preprocessing.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3522 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/preprocessing_targets.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7917 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/random_projection.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6689 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/semi_supervised.rst
--rw-r--r--   0 runner    (1001) docker     (127)    24946 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/sgd.rst
--rw-r--r--   0 runner    (1001) docker     (127)    34865 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/svm.rst
--rw-r--r--   0 runner    (1001) docker     (127)    27542 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/tree.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1962 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/modules/unsupervised_reduction.rst
--rw-r--r--   0 runner    (1001) docker     (127)      540 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/preface.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3300 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/presentations.rst
--rw-r--r--   0 runner    (1001) docker     (127)    17855 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/related_projects.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11936 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/roadmap.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.122610 scikit-learn-1.4.2/doc/sphinxext/
--rw-r--r--   0 runner    (1001) docker     (127)       43 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/sphinxext/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)     6024 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/sphinxext/add_toctree_functions.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1908 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/sphinxext/allow_nan_estimators.py
--rw-r--r--   0 runner    (1001) docker     (127)     1714 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/sphinxext/doi_role.py
--rw-r--r--   0 runner    (1001) docker     (127)     2645 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/sphinxext/github_link.py
--rw-r--r--   0 runner    (1001) docker     (127)     8116 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/sphinxext/sphinx_issues.py
--rw-r--r--   0 runner    (1001) docker     (127)      634 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/supervised_learning.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2989 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/support.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.126610 scikit-learn-1.4.2/doc/templates/
--rw-r--r--   0 runner    (1001) docker     (127)      424 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/class.rst
--rw-r--r--   0 runner    (1001) docker     (127)      495 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/class_with_call.rst
--rw-r--r--   0 runner    (1001) docker     (127)      558 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/deprecated_class.rst
--rw-r--r--   0 runner    (1001) docker     (127)      587 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/deprecated_class_with_call.rst
--rw-r--r--   0 runner    (1001) docker     (127)      488 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/deprecated_class_without_init.rst
--rw-r--r--   0 runner    (1001) docker     (127)      497 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/deprecated_function.rst
--rw-r--r--   0 runner    (1001) docker     (127)      546 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/display_all_class_methods.rst
--rw-r--r--   0 runner    (1001) docker     (127)      484 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/display_only_from_estimator.rst
--rw-r--r--   0 runner    (1001) docker     (127)      433 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/function.rst
--rwxr-xr-x   0 runner    (1001) docker     (127)      155 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/generate_deprecated.sh
--rw-r--r--   0 runner    (1001) docker     (127)    17331 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/index.html
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/numpydoc_docstring.rst
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/templates/redirects.html
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.126610 scikit-learn-1.4.2/doc/testimonials/
--rw-r--r--   0 runner    (1001) docker     (127)      237 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/README.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.130610 scikit-learn-1.4.2/doc/testimonials/images/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)    41412 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/aweber.png
--rw-r--r--   0 runner    (1001) docker     (127)     3321 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/bestofmedia-logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     4891 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/betaworks.png
--rw-r--r--   0 runner    (1001) docker     (127)    14595 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/birchbox.jpg
--rw-r--r--   0 runner    (1001) docker     (127)    65058 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/bnp_paribas_cardif.png
--rw-r--r--   0 runner    (1001) docker     (127)     5937 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/booking.png
--rw-r--r--   0 runner    (1001) docker     (127)     3294 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/change-logo.png
--rw-r--r--   0 runner    (1001) docker     (127)    10684 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/dataiku_logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     5177 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/datapublica.png
--rw-r--r--   0 runner    (1001) docker     (127)    19895 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/datarobot.png
--rw-r--r--   0 runner    (1001) docker     (127)     2629 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/evernote.png
--rw-r--r--   0 runner    (1001) docker     (127)    24772 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/howaboutwe.png
--rw-r--r--   0 runner    (1001) docker     (127)    31051 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/huggingface.png
--rw-r--r--   0 runner    (1001) docker     (127)    85087 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/infonea.jpg
--rw-r--r--   0 runner    (1001) docker     (127)    23903 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/inria.png
--rw-r--r--   0 runner    (1001) docker     (127)     8359 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/jpmorgan.png
--rw-r--r--   0 runner    (1001) docker     (127)     3307 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/lovely.png
--rw-r--r--   0 runner    (1001) docker     (127)    12363 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/machinalis.png
--rw-r--r--   0 runner    (1001) docker     (127)    47018 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/mars.png
--rw-r--r--   0 runner    (1001) docker     (127)    10246 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/okcupid.png
--rw-r--r--   0 runner    (1001) docker     (127)     8603 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/ottogroup_logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     4689 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/peerindex.png
--rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/phimeca.png
--rw-r--r--   0 runner    (1001) docker     (127)    11944 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/rangespan.png
--rw-r--r--   0 runner    (1001) docker     (127)     6569 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/solido_logo.png
--rw-r--r--   0 runner    (1001) docker     (127)    12293 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/spotify.png
--rw-r--r--   0 runner    (1001) docker     (127)    11473 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/telecomparistech.jpg
--rw-r--r--   0 runner    (1001) docker     (127)     6350 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/yhat.png
--rw-r--r--   0 runner    (1001) docker     (127)    22810 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/images/zopa.png
--rw-r--r--   0 runner    (1001) docker     (127)    30885 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/testimonials/testimonials.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.070609 scikit-learn-1.4.2/doc/themes/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.130610 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/
--rw-r--r--   0 runner    (1001) docker     (127)     2055 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/javascript.html
--rw-r--r--   0 runner    (1001) docker     (127)     6741 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/layout.html
--rw-r--r--   0 runner    (1001) docker     (127)     4311 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/nav.html
--rw-r--r--   0 runner    (1001) docker     (127)      437 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/search.html
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.070609 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.130610 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/css/
--rw-r--r--   0 runner    (1001) docker     (127)    27008 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/css/theme.css
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.130610 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/css/vendor/
--rw-r--r--   0 runner    (1001) docker     (127)   155712 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/css/vendor/bootstrap.min.css
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.130610 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/
--rw-r--r--   0 runner    (1001) docker     (127)     1965 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/details-permalink.js
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.130610 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/vendor/
--rw-r--r--   0 runner    (1001) docker     (127)    58030 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/vendor/bootstrap.min.js
--rw-r--r--   0 runner    (1001) docker     (127)    72818 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/vendor/jquery-3.6.3.slim.min.js
--rw-r--r--   0 runner    (1001) docker     (127)      189 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/themes/scikit-learn-modern/theme.conf
--rw-r--r--   0 runner    (1001) docker     (127)     3678 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tune_toc.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/basic/
--rw-r--r--   0 runner    (1001) docker     (127)    14077 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/basic/tutorial.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/common_includes/
--rw-r--r--   0 runner    (1001) docker     (127)      162 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/common_includes/info.txt
--rw-r--r--   0 runner    (1001) docker     (127)      783 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/machine_learning_map/
--rw-r--r--   0 runner    (1001) docker     (127)     3305 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/machine_learning_map/ML_MAPS_README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     9615 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/machine_learning_map/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/machine_learning_map/parse_path.py
--rw-r--r--   0 runner    (1001) docker     (127)   230678 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/machine_learning_map/pyparsing.py
--rw-r--r--   0 runner    (1001) docker     (127)     3612 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/machine_learning_map/svg2imagemap.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/statistical_inference/
--rw-r--r--   0 runner    (1001) docker     (127)     1360 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/statistical_inference/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     9892 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/statistical_inference/model_selection.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1766 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/statistical_inference/putting_together.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3104 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/statistical_inference/settings.rst
--rw-r--r--   0 runner    (1001) docker     (127)    18658 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/statistical_inference/supervised_learning.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10832 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/statistical_inference/unsupervised_learning.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/text_analytics/
--rw-r--r--   0 runner    (1001) docker     (127)      544 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/.gitignore
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.070609 scikit-learn-1.4.2/doc/tutorial/text_analytics/data/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/text_analytics/data/languages/
--rw-r--r--   0 runner    (1001) docker     (127)     3820 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/data/languages/fetch_data.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/text_analytics/data/movie_reviews/
--rw-r--r--   0 runner    (1001) docker     (127)      989 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/text_analytics/skeletons/
--rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     2410 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.134610 scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/
--rw-r--r--   0 runner    (1001) docker     (127)     2260 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     3140 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py
--rw-r--r--   0 runner    (1001) docker     (127)      988 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/generate_skeletons.py
--rw-r--r--   0 runner    (1001) docker     (127)    21039 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/tutorial/text_analytics/working_with_text_data.rst
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/unsupervised_learning.rst
--rw-r--r--   0 runner    (1001) docker     (127)      626 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/user_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3175 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/visualizations.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.138610 scikit-learn-1.4.2/doc/whats_new/
--rw-r--r--   0 runner    (1001) docker     (127)     5209 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/_contributors.rst
--rw-r--r--   0 runner    (1001) docker     (127)      537 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/changelog_legend.inc
--rw-r--r--   0 runner    (1001) docker     (127)    44662 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/older_versions.rst
--rw-r--r--   0 runner    (1001) docker     (127)    14172 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.13.rst
--rw-r--r--   0 runner    (1001) docker     (127)    14475 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.14.rst
--rw-r--r--   0 runner    (1001) docker     (127)    21266 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.15.rst
--rw-r--r--   0 runner    (1001) docker     (127)    23270 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.16.rst
--rw-r--r--   0 runner    (1001) docker     (127)    22571 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.17.rst
--rw-r--r--   0 runner    (1001) docker     (127)    36439 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.18.rst
--rw-r--r--   0 runner    (1001) docker     (127)    48325 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.19.rst
--rw-r--r--   0 runner    (1001) docker     (127)    79828 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.20.rst
--rw-r--r--   0 runner    (1001) docker     (127)    50044 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.21.rst
--rw-r--r--   0 runner    (1001) docker     (127)    52900 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.22.rst
--rw-r--r--   0 runner    (1001) docker     (127)    38648 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.23.rst
--rw-r--r--   0 runner    (1001) docker     (127)    46490 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v0.24.rst
--rw-r--r--   0 runner    (1001) docker     (127)    57815 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v1.0.rst
--rw-r--r--   0 runner    (1001) docker     (127)    65715 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v1.1.rst
--rw-r--r--   0 runner    (1001) docker     (127)    48895 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v1.2.rst
--rw-r--r--   0 runner    (1001) docker     (127)    45212 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v1.3.rst
--rw-r--r--   0 runner    (1001) docker     (127)    48140 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new/v1.4.rst
--rw-r--r--   0 runner    (1001) docker     (127)      796 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/doc/whats_new.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.138610 scikit-learn-1.4.2/examples/
--rw-r--r--   0 runner    (1001) docker     (127)       41 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/README.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.142610 scikit-learn-1.4.2/examples/applications/
--rw-r--r--   0 runner    (1001) docker     (127)      201 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    31775 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_cyclical_feature_engineering.py
--rw-r--r--   0 runner    (1001) docker     (127)     5242 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_digits_denoising.py
--rw-r--r--   0 runner    (1001) docker     (127)     4770 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_face_recognition.py
--rw-r--r--   0 runner    (1001) docker     (127)    10669 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_model_complexity_influence.py
--rw-r--r--   0 runner    (1001) docker     (127)    13523 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_out_of_core_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     4895 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_outlier_detection_wine.py
--rw-r--r--   0 runner    (1001) docker     (127)    11140 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_prediction_latency.py
--rw-r--r--   0 runner    (1001) docker     (127)     7853 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_species_distribution_modeling.py
--rw-r--r--   0 runner    (1001) docker     (127)     8228 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_stock_market.py
--rw-r--r--   0 runner    (1001) docker     (127)    16513 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_time_series_lagged_features.py
--rw-r--r--   0 runner    (1001) docker     (127)     5369 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_tomography_l1_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (127)     6756 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/plot_topics_extraction_with_nmf_lda.py
--rw-r--r--   0 runner    (1001) docker     (127)    11537 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/svm_gui.py
--rw-r--r--   0 runner    (1001) docker     (127)     7730 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/applications/wikipedia_principal_eigenvector.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.142610 scikit-learn-1.4.2/examples/bicluster/
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/bicluster/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5625 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/bicluster/plot_bicluster_newsgroups.py
--rw-r--r--   0 runner    (1001) docker     (127)     4677 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/bicluster/plot_spectral_biclustering.py
--rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/bicluster/plot_spectral_coclustering.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.142610 scikit-learn-1.4.2/examples/calibration/
--rw-r--r--   0 runner    (1001) docker     (127)      145 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/calibration/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4893 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/calibration/plot_calibration.py
--rw-r--r--   0 runner    (1001) docker     (127)    11608 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/calibration/plot_calibration_curve.py
--rw-r--r--   0 runner    (1001) docker     (127)     9567 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/calibration/plot_calibration_multiclass.py
--rw-r--r--   0 runner    (1001) docker     (127)    12170 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/calibration/plot_compare_calibration.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.146610 scikit-learn-1.4.2/examples/classification/
--rw-r--r--   0 runner    (1001) docker     (127)      120 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/classification/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     3285 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/classification/plot_classification_probability.py
--rw-r--r--   0 runner    (1001) docker     (127)     5017 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/classification/plot_classifier_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     4570 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/classification/plot_digits_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     3055 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/classification/plot_lda.py
--rw-r--r--   0 runner    (1001) docker     (127)     7779 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/classification/plot_lda_qda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.150610 scikit-learn-1.4.2/examples/cluster/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     8745 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_adjusted_for_chance_measures.py
--rw-r--r--   0 runner    (1001) docker     (127)     2180 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_affinity_propagation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3115 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_agglomerative_clustering.py
--rw-r--r--   0 runner    (1001) docker     (127)     4773 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_agglomerative_clustering_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1709 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_agglomerative_dendrogram.py
--rw-r--r--   0 runner    (1001) docker     (127)     3857 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_birch_vs_minibatchkmeans.py
--rw-r--r--   0 runner    (1001) docker     (127)     1996 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_bisect_kmeans.py
--rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_cluster_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_cluster_iris.py
--rw-r--r--   0 runner    (1001) docker     (127)     3956 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_coin_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2376 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_coin_ward_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3083 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_color_quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)     4001 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_dbscan.py
--rw-r--r--   0 runner    (1001) docker     (127)     2644 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_dict_face_patches.py
--rw-r--r--   0 runner    (1001) docker     (127)     1603 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_digits_agglomeration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2740 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_digits_linkage.py
--rw-r--r--   0 runner    (1001) docker     (127)     7191 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_face_compress.py
--rw-r--r--   0 runner    (1001) docker     (127)     3587 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     9805 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_hdbscan.py
--rw-r--r--   0 runner    (1001) docker     (127)     3926 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_inductive_clustering.py
--rw-r--r--   0 runner    (1001) docker     (127)     6654 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_kmeans_assumptions.py
--rw-r--r--   0 runner    (1001) docker     (127)     6811 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_kmeans_digits.py
--rw-r--r--   0 runner    (1001) docker     (127)     1169 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_kmeans_plusplus.py
--rw-r--r--   0 runner    (1001) docker     (127)     5898 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_kmeans_silhouette_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)     4337 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_kmeans_stability_low_dim_dense.py
--rw-r--r--   0 runner    (1001) docker     (127)     5166 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_linkage_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     1665 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_mean_shift.py
--rw-r--r--   0 runner    (1001) docker     (127)     4026 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_mini_batch_kmeans.py
--rw-r--r--   0 runner    (1001) docker     (127)     3542 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_optics.py
--rw-r--r--   0 runner    (1001) docker     (127)     3806 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_segmentation_toy.py
--rw-r--r--   0 runner    (1001) docker     (127)     3661 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cluster/plot_ward_structured_vs_unstructured.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.150610 scikit-learn-1.4.2/examples/compose/
--rw-r--r--   0 runner    (1001) docker     (127)      221 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     6753 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/plot_column_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7779 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/plot_column_transformer_mixed_types.py
--rw-r--r--   0 runner    (1001) docker     (127)     4498 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/plot_compare_reduction.py
--rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/plot_digits_pipe.py
--rw-r--r--   0 runner    (1001) docker     (127)     1951 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/plot_feature_union.py
--rw-r--r--   0 runner    (1001) docker     (127)     7671 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/compose/plot_transformed_target.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.150610 scikit-learn-1.4.2/examples/covariance/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/covariance/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5027 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/covariance/plot_covariance_estimation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2906 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/covariance/plot_lw_vs_oas.py
--rw-r--r--   0 runner    (1001) docker     (127)     8171 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/covariance/plot_mahalanobis_distances.py
--rw-r--r--   0 runner    (1001) docker     (127)     6489 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/covariance/plot_robust_vs_empirical_covariance.py
--rw-r--r--   0 runner    (1001) docker     (127)     5003 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/covariance/plot_sparse_cov.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.150610 scikit-learn-1.4.2/examples/cross_decomposition/
--rw-r--r--   0 runner    (1001) docker     (127)      144 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cross_decomposition/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4806 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cross_decomposition/plot_compare_cross_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (127)     6926 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/cross_decomposition/plot_pcr_vs_pls.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.150610 scikit-learn-1.4.2/examples/datasets/
--rw-r--r--   0 runner    (1001) docker     (127)      121 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/datasets/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)      876 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/datasets/plot_digits_last_image.py
--rw-r--r--   0 runner    (1001) docker     (127)     2708 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/datasets/plot_iris_dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)     2322 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/datasets/plot_random_dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)     3180 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/datasets/plot_random_multilabel_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.154610 scikit-learn-1.4.2/examples/decomposition/
--rw-r--r--   0 runner    (1001) docker     (127)      120 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    10538 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_faces_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (127)     2142 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_ica_blind_source_separation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3242 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_ica_vs_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)     6144 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_image_denoising.py
--rw-r--r--   0 runner    (1001) docker     (127)     1989 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_incremental_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)     6843 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_kernel_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)     1498 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_pca_iris.py
--rw-r--r--   0 runner    (1001) docker     (127)     4499 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_pca_vs_fa_model_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2015 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_pca_vs_lda.py
--rw-r--r--   0 runner    (1001) docker     (127)     4027 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_sparse_coding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2327 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/decomposition/plot_varimax_fa.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.154610 scikit-learn-1.4.2/examples/developing_estimators/
--rw-r--r--   0 runner    (1001) docker     (127)      137 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/developing_estimators/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     2597 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/developing_estimators/sklearn_is_fitted.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.158610 scikit-learn-1.4.2/examples/ensemble/
--rw-r--r--   0 runner    (1001) docker     (127)      115 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    10269 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_adaboost_multiclass.py
--rw-r--r--   0 runner    (1001) docker     (127)     2421 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_adaboost_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     3099 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_adaboost_twoclass.py
--rw-r--r--   0 runner    (1001) docker     (127)     7229 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_bias_variance.py
--rw-r--r--   0 runner    (1001) docker     (127)     3239 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_ensemble_oob.py
--rw-r--r--   0 runner    (1001) docker     (127)     5246 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_feature_transformation.py
--rw-r--r--   0 runner    (1001) docker     (127)     8802 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     4139 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_forest_importances.py
--rw-r--r--   0 runner    (1001) docker     (127)     3040 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_forest_importances_faces.py
--rw-r--r--   0 runner    (1001) docker     (127)     6335 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_forest_iris.py
--rw-r--r--   0 runner    (1001) docker     (127)    10125 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_categorical.py
--rw-r--r--   0 runner    (1001) docker     (127)     6623 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)     4959 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_oob.py
--rw-r--r--   0 runner    (1001) docker     (127)    12353 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_quantile.py
--rw-r--r--   0 runner    (1001) docker     (127)     5012 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2696 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_regularization.py
--rw-r--r--   0 runner    (1001) docker     (127)     4254 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_isolation_forest.py
--rw-r--r--   0 runner    (1001) docker     (127)     3234 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_monotonic_constraints.py
--rw-r--r--   0 runner    (1001) docker     (127)     3659 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_random_forest_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2610 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_random_forest_regression_multioutput.py
--rw-r--r--   0 runner    (1001) docker     (127)     8061 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_stack_predictors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_voting_decision_regions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2982 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_voting_probas.py
--rw-r--r--   0 runner    (1001) docker     (127)     2653 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/ensemble/plot_voting_regressor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.158610 scikit-learn-1.4.2/examples/exercises/
--rw-r--r--   0 runner    (1001) docker     (127)       67 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/exercises/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     2887 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/exercises/plot_cv_diabetes.py
--rw-r--r--   0 runner    (1001) docker     (127)      954 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/exercises/plot_digits_classification_exercise.py
--rw-r--r--   0 runner    (1001) docker     (127)     1691 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/exercises/plot_iris_exercise.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.158610 scikit-learn-1.4.2/examples/feature_selection/
--rw-r--r--   0 runner    (1001) docker     (127)      141 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     1642 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/plot_f_test_vs_mi.py
--rw-r--r--   0 runner    (1001) docker     (127)     3806 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/plot_feature_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2768 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/plot_feature_selection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)      921 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/plot_rfe_digits.py
--rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/plot_rfe_with_cross_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)     7484 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/feature_selection/plot_select_from_model_diabetes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.162611 scikit-learn-1.4.2/examples/gaussian_process/
--rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    13329 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_compare_gpr_krr.py
--rw-r--r--   0 runner    (1001) docker     (127)     3997 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpc.py
--rw-r--r--   0 runner    (1001) docker     (127)     2220 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpc_iris.py
--rw-r--r--   0 runner    (1001) docker     (127)     2925 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpc_isoprobability.py
--rw-r--r--   0 runner    (1001) docker     (127)     2106 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpc_xor.py
--rw-r--r--   0 runner    (1001) docker     (127)     8841 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_co2.py
--rw-r--r--   0 runner    (1001) docker     (127)     6582 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_noisy.py
--rw-r--r--   0 runner    (1001) docker     (127)     5306 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_noisy_targets.py
--rw-r--r--   0 runner    (1001) docker     (127)     5789 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_on_structured_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     8449 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_prior_posterior.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.162611 scikit-learn-1.4.2/examples/impute/
--rw-r--r--   0 runner    (1001) docker     (127)      127 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/impute/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5881 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/impute/plot_iterative_imputer_variants_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     9233 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/impute/plot_missing_values.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.162611 scikit-learn-1.4.2/examples/inspection/
--rw-r--r--   0 runner    (1001) docker     (127)      108 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/inspection/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     7219 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/inspection/plot_causal_interpretation.py
--rw-r--r--   0 runner    (1001) docker     (127)    26400 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/inspection/plot_linear_model_coefficient_interpretation.py
--rw-r--r--   0 runner    (1001) docker     (127)    21224 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/inspection/plot_partial_dependence.py
--rw-r--r--   0 runner    (1001) docker     (127)     9468 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/inspection/plot_permutation_importance.py
--rw-r--r--   0 runner    (1001) docker     (127)     7336 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/inspection/plot_permutation_importance_multicollinear.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.162611 scikit-learn-1.4.2/examples/kernel_approximation/
--rw-r--r--   0 runner    (1001) docker     (127)      147 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/kernel_approximation/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     7725 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/kernel_approximation/plot_scalable_poly_kernels.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.170611 scikit-learn-1.4.2/examples/linear_model/
--rw-r--r--   0 runner    (1001) docker     (127)      135 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     7101 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ard.py
--rw-r--r--   0 runner    (1001) docker     (127)     3091 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_bayesian_ridge_curvefit.py
--rw-r--r--   0 runner    (1001) docker     (127)     2057 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py
--rw-r--r--   0 runner    (1001) docker     (127)     2078 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_huber_vs_ridge.py
--rw-r--r--   0 runner    (1001) docker     (127)     1383 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_iris_logistic.py
--rw-r--r--   0 runner    (1001) docker     (127)     9399 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_lasso_and_elasticnet.py
--rw-r--r--   0 runner    (1001) docker     (127)     2768 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_lasso_coordinate_descent_path.py
--rw-r--r--   0 runner    (1001) docker     (127)     2826 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_lasso_dense_vs_sparse_data.py
--rw-r--r--   0 runner    (1001) docker     (127)      992 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_lasso_lars.py
--rw-r--r--   0 runner    (1001) docker     (127)     3881 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_lasso_lars_ic.py
--rw-r--r--   0 runner    (1001) docker     (127)     8908 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_lasso_model_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     1576 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_logistic.py
--rw-r--r--   0 runner    (1001) docker     (127)     3231 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_logistic_l1_l2_sparsity.py
--rw-r--r--   0 runner    (1001) docker     (127)     2085 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_logistic_multinomial.py
--rw-r--r--   0 runner    (1001) docker     (127)     2160 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_logistic_path.py
--rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_multi_task_lasso_support.py
--rw-r--r--   0 runner    (1001) docker     (127)     2007 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_nnls.py
--rw-r--r--   0 runner    (1001) docker     (127)     2025 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ols.py
--rw-r--r--   0 runner    (1001) docker     (127)     2068 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ols_3d.py
--rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ols_ridge_variance.py
--rw-r--r--   0 runner    (1001) docker     (127)     1895 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_omp.py
--rw-r--r--   0 runner    (1001) docker     (127)    22613 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_poisson_regression_non_normal_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     7828 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_polynomial_interpolation.py
--rw-r--r--   0 runner    (1001) docker     (127)    11579 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_quantile_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2074 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ransac.py
--rw-r--r--   0 runner    (1001) docker     (127)     8758 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ridge_coeffs.py
--rw-r--r--   0 runner    (1001) docker     (127)     2000 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_ridge_path.py
--rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_robust_fit.py
--rw-r--r--   0 runner    (1001) docker     (127)     1898 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     5741 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)     1948 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_iris.py
--rw-r--r--   0 runner    (1001) docker     (127)     1218 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_loss_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     1348 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_penalties.py
--rw-r--r--   0 runner    (1001) docker     (127)     1198 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_separating_hyperplane.py
--rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgd_weighted_samples.py
--rw-r--r--   0 runner    (1001) docker     (127)     6082 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py
--rw-r--r--   0 runner    (1001) docker     (127)     3976 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py
--rw-r--r--   0 runner    (1001) docker     (127)     2675 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_sparse_logistic_regression_mnist.py
--rw-r--r--   0 runner    (1001) docker     (127)     3777 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_theilsen.py
--rw-r--r--   0 runner    (1001) docker     (127)    23618 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/linear_model/plot_tweedie_regression_insurance_claims.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.170611 scikit-learn-1.4.2/examples/manifold/
--rw-r--r--   0 runner    (1001) docker     (127)      124 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     6752 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/plot_compare_methods.py
--rw-r--r--   0 runner    (1001) docker     (127)     6471 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/plot_lle_digits.py
--rw-r--r--   0 runner    (1001) docker     (127)     5107 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/plot_manifold_sphere.py
--rw-r--r--   0 runner    (1001) docker     (127)     2698 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/plot_mds.py
--rw-r--r--   0 runner    (1001) docker     (127)     4021 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/plot_swissroll.py
--rw-r--r--   0 runner    (1001) docker     (127)     4265 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/manifold/plot_t_sne_perplexity.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.174611 scikit-learn-1.4.2/examples/miscellaneous/
--rw-r--r--   0 runner    (1001) docker     (127)      117 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     7505 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_anomaly_comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     3541 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_display_object_visualization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1630 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_estimator_representation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2615 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_isotonic_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     7441 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py
--rw-r--r--   0 runner    (1001) docker     (127)     8685 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_kernel_approximation.py
--rw-r--r--   0 runner    (1001) docker     (127)     6647 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_kernel_ridge_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)    24865 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_metadata_routing.py
--rw-r--r--   0 runner    (1001) docker     (127)     4072 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_multilabel.py
--rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_multioutput_face_completion.py
--rw-r--r--   0 runner    (1001) docker     (127)    16466 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_outlier_detection_bench.py
--rw-r--r--   0 runner    (1001) docker     (127)     5363 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_partial_dependence_visualization_api.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6200 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_pipeline_display.py
--rw-r--r--   0 runner    (1001) docker     (127)     2090 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_roc_curve_visualization_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/miscellaneous/plot_set_output.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.174611 scikit-learn-1.4.2/examples/mixture/
--rw-r--r--   0 runner    (1001) docker     (127)      127 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5883 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_concentration_prior.py
--rw-r--r--   0 runner    (1001) docker     (127)     3203 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_gmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4673 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_gmm_covariances.py
--rw-r--r--   0 runner    (1001) docker     (127)     3702 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_gmm_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     1519 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_gmm_pdf.py
--rw-r--r--   0 runner    (1001) docker     (127)     5487 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_gmm_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     6082 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/mixture/plot_gmm_sin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.178611 scikit-learn-1.4.2/examples/model_selection/
--rw-r--r--   0 runner    (1001) docker     (127)      135 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     2084 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (127)     6095 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_cv_indices.py
--rw-r--r--   0 runner    (1001) docker     (127)     2551 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_cv_predict.py
--rw-r--r--   0 runner    (1001) docker     (127)     3990 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_det.py
--rw-r--r--   0 runner    (1001) docker     (127)     7322 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_grid_search_digits.py
--rw-r--r--   0 runner    (1001) docker     (127)     3644 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_grid_search_refit_callable.py
--rw-r--r--   0 runner    (1001) docker     (127)    23047 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_grid_search_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     9262 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_grid_search_text_feature_extraction.py
--rw-r--r--   0 runner    (1001) docker     (127)     6830 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_learning_curve.py
--rw-r--r--   0 runner    (1001) docker     (127)    12013 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_likelihood_ratios.py
--rw-r--r--   0 runner    (1001) docker     (127)     3692 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_multi_metric_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4476 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_nested_cross_validation_iris.py
--rw-r--r--   0 runner    (1001) docker     (127)     5049 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_permutation_tests_for_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)    10229 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_precision_recall.py
--rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_randomized_search.py
--rw-r--r--   0 runner    (1001) docker     (127)    14496 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_roc.py
--rw-r--r--   0 runner    (1001) docker     (127)     4234 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_roc_crossval.py
--rw-r--r--   0 runner    (1001) docker     (127)     4211 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_successive_halving_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (127)     2764 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_successive_halving_iterations.py
--rw-r--r--   0 runner    (1001) docker     (127)     2506 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_train_error_vs_test_error.py
--rw-r--r--   0 runner    (1001) docker     (127)     2680 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_underfitting_overfitting.py
--rw-r--r--   0 runner    (1001) docker     (127)     1291 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/model_selection/plot_validation_curve.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.178611 scikit-learn-1.4.2/examples/multiclass/
--rw-r--r--   0 runner    (1001) docker     (127)      123 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/multiclass/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     8202 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/multiclass/plot_multiclass_overview.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.178611 scikit-learn-1.4.2/examples/multioutput/
--rw-r--r--   0 runner    (1001) docker     (127)      127 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/multioutput/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5990 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/multioutput/plot_classifier_chain_yeast.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.178611 scikit-learn-1.4.2/examples/neighbors/
--rw-r--r--   0 runner    (1001) docker     (127)      125 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    11310 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/approximate_nearest_neighbors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2683 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_caching_nearest_neighbors.py
--rw-r--r--   0 runner    (1001) docker     (127)     3142 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     1990 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_digits_kde_sampling.py
--rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_kde_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     3616 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_lof_novelty_detection.py
--rw-r--r--   0 runner    (1001) docker     (127)     3097 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_lof_outlier_detection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2774 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_nca_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     3561 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_nca_dim_reduction.py
--rw-r--r--   0 runner    (1001) docker     (127)     3001 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_nca_illustration.py
--rw-r--r--   0 runner    (1001) docker     (127)     1398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_nearest_centroid.py
--rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     4757 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neighbors/plot_species_kde.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.182611 scikit-learn-1.4.2/examples/neural_networks/
--rw-r--r--   0 runner    (1001) docker     (127)      133 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neural_networks/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4600 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neural_networks/plot_mlp_alpha.py
--rw-r--r--   0 runner    (1001) docker     (127)     4067 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neural_networks/plot_mlp_training_curves.py
--rw-r--r--   0 runner    (1001) docker     (127)     2699 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neural_networks/plot_mnist_filters.py
--rw-r--r--   0 runner    (1001) docker     (127)     4267 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/neural_networks/plot_rbm_logistic_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.182611 scikit-learn-1.4.2/examples/preprocessing/
--rw-r--r--   0 runner    (1001) docker     (127)      119 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    14781 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_all_scaling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3375 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_discretization.py
--rw-r--r--   0 runner    (1001) docker     (127)     7720 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_discretization_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     3111 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_discretization_strategies.py
--rw-r--r--   0 runner    (1001) docker     (127)     4631 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_map_data_to_normal.py
--rw-r--r--   0 runner    (1001) docker     (127)     9839 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_scaling_importance.py
--rw-r--r--   0 runner    (1001) docker     (127)     8095 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_target_encoder.py
--rw-r--r--   0 runner    (1001) docker     (127)     7185 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/preprocessing/plot_target_encoder_cross_val.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.182611 scikit-learn-1.4.2/examples/release_highlights/
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    10367 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_0_22_0.py
--rw-r--r--   0 runner    (1001) docker     (127)     7713 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_0_23_0.py
--rw-r--r--   0 runner    (1001) docker     (127)    11514 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_0_24_0.py
--rw-r--r--   0 runner    (1001) docker     (127)    10262 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_0_0.py
--rw-r--r--   0 runner    (1001) docker     (127)     8630 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_1_0.py
--rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_2_0.py
--rw-r--r--   0 runner    (1001) docker     (127)     6199 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_3_0.py
--rw-r--r--   0 runner    (1001) docker     (127)     7884 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_4_0.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.182611 scikit-learn-1.4.2/examples/semi_supervised/
--rw-r--r--   0 runner    (1001) docker     (127)      157 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     3174 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/plot_label_propagation_digits.py
--rw-r--r--   0 runner    (1001) docker     (127)     4203 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/plot_label_propagation_digits_active_learning.py
--rw-r--r--   0 runner    (1001) docker     (127)     2711 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/plot_label_propagation_structure.py
--rw-r--r--   0 runner    (1001) docker     (127)     4007 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/plot_self_training_varying_threshold.py
--rw-r--r--   0 runner    (1001) docker     (127)     3619 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/plot_semi_supervised_newsgroups.py
--rw-r--r--   0 runner    (1001) docker     (127)     2955 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/semi_supervised/plot_semi_supervised_versus_svm_iris.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.186611 scikit-learn-1.4.2/examples/svm/
--rw-r--r--   0 runner    (1001) docker     (127)      119 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     1302 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_custom_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     2850 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_iris_svc.py
--rw-r--r--   0 runner    (1001) docker     (127)     1819 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_linearsvc_support_vectors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2804 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_oneclass.py
--rw-r--r--   0 runner    (1001) docker     (127)     8313 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_rbf_parameters.py
--rw-r--r--   0 runner    (1001) docker     (127)     1114 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_separating_hyperplane.py
--rw-r--r--   0 runner    (1001) docker     (127)     2328 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_separating_hyperplane_unbalanced.py
--rw-r--r--   0 runner    (1001) docker     (127)     2096 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_anova.py
--rw-r--r--   0 runner    (1001) docker     (127)    11619 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_kernels.py
--rw-r--r--   0 runner    (1001) docker     (127)     2546 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_margin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1072 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_nonlinear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2048 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     7543 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_scale_c.py
--rw-r--r--   0 runner    (1001) docker     (127)     2165 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_svm_tie_breaking.py
--rw-r--r--   0 runner    (1001) docker     (127)     2048 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/svm/plot_weighted_samples.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.186611 scikit-learn-1.4.2/examples/text/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/text/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)    16629 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/text/plot_document_classification_20newsgroups.py
--rw-r--r--   0 runner    (1001) docker     (127)    17653 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/text/plot_document_clustering.py
--rw-r--r--   0 runner    (1001) docker     (127)    15130 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/text/plot_hashing_vs_dict_vectorizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.186611 scikit-learn-1.4.2/examples/tree/
--rw-r--r--   0 runner    (1001) docker     (127)      103 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/tree/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4603 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/tree/plot_cost_complexity_pruning.py
--rw-r--r--   0 runner    (1001) docker     (127)     2514 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/tree/plot_iris_dtc.py
--rw-r--r--   0 runner    (1001) docker     (127)     1527 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/tree/plot_tree_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     1959 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/tree/plot_tree_regression_multioutput.py
--rw-r--r--   0 runner    (1001) docker     (127)     8829 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/examples/tree/plot_unveil_tree_structure.py
--rw-r--r--   0 runner    (1001) docker     (127)     1611 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/meson.build
--rw-r--r--   0 runner    (1001) docker     (127)     3087 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/pyproject.toml
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.190611 scikit-learn-1.4.2/scikit_learn.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)     9215 2024-04-09 14:20:31.000000 scikit-learn-1.4.2/scikit_learn.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    58757 2024-04-09 14:20:32.000000 scikit-learn-1.4.2/scikit_learn.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-09 14:20:31.000000 scikit-learn-1.4.2/scikit_learn.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-09 14:20:31.000000 scikit-learn-1.4.2/scikit_learn.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (127)      713 2024-04-09 14:20:31.000000 scikit-learn-1.4.2/scikit_learn.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)        8 2024-04-09 14:20:31.000000 scikit-learn-1.4.2/scikit_learn.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-04-09 14:20:32.318612 scikit-learn-1.4.2/setup.cfg
--rwxr-xr-x   0 runner    (1001) docker     (127)    23006 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.194611 scikit-learn-1.4.2/sklearn/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.194611 scikit-learn-1.4.2/sklearn/__check_build/
--rw-r--r--   0 runner    (1001) docker     (127)     1680 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/__check_build/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)       30 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/__check_build/_check_build.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      143 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/__check_build/meson.build
--rw-r--r--   0 runner    (1001) docker     (127)     5015 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.194611 scikit-learn-1.4.2/sklearn/_build_utils/
--rw-r--r--   0 runner    (1001) docker     (127)     3610 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_build_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4531 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_build_utils/openmp_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     2175 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_build_utils/pre_build_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1580 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_build_utils/tempita.py
--rw-r--r--   0 runner    (1001) docker     (127)      369 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_build_utils/version.py
--rw-r--r--   0 runner    (1001) docker     (127)    13493 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_config.py
--rw-r--r--   0 runner    (1001) docker     (127)      345 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_distributor_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     3708 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_isotonic.pyx
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.194611 scikit-learn-1.4.2/sklearn/_loss/
--rw-r--r--   0 runner    (1001) docker     (127)      607 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4315 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/_loss.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    50281 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/_loss.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     8101 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/link.py
--rw-r--r--   0 runner    (1001) docker     (127)    41236 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)      411 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.194611 scikit-learn-1.4.2/sklearn/_loss/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3954 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/tests/test_link.py
--rw-r--r--   0 runner    (1001) docker     (127)    48281 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_loss/tests/test_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     2481 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/_min_dependencies.py
--rw-r--r--   0 runner    (1001) docker     (127)    53077 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    49547 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/calibration.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.198611 scikit-learn-1.4.2/sklearn/cluster/
--rw-r--r--   0 runner    (1001) docker     (127)     1440 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20512 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_affinity_propagation.py
--rw-r--r--   0 runner    (1001) docker     (127)    49039 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_agglomerative.py
--rw-r--r--   0 runner    (1001) docker     (127)    22157 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_bicluster.py
--rw-r--r--   0 runner    (1001) docker     (127)    26249 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_birch.py
--rw-r--r--   0 runner    (1001) docker     (127)    19040 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_bisect_k_means.py
--rw-r--r--   0 runner    (1001) docker     (127)    18290 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_dbscan.py
--rw-r--r--   0 runner    (1001) docker     (127)     1307 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_dbscan_inner.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     3347 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_feature_agglomeration.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.198611 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10097 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_linkage.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     7891 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_reachability.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     2150 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_tree.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    27800 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_tree.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    41849 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/hdbscan.py
--rw-r--r--   0 runner    (1001) docker     (127)      441 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.202611 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2064 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hdbscan/tests/test_reachibility.py
--rw-r--r--   0 runner    (1001) docker     (127)      245 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hierarchical_fast.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    15905 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_hierarchical_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      887 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_k_means_common.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    10289 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_k_means_common.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    28135 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_k_means_elkan.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    16470 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_k_means_lloyd.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     8156 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_k_means_minibatch.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    82683 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_kmeans.py
--rw-r--r--   0 runner    (1001) docker     (127)    20156 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_mean_shift.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    44710 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_optics.py
--rw-r--r--   0 runner    (1001) docker     (127)    30496 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/_spectral.py
--rw-r--r--   0 runner    (1001) docker     (127)      864 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.202611 scikit-learn-1.4.2/sklearn/cluster/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      880 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/common.py
--rw-r--r--   0 runner    (1001) docker     (127)    11898 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_affinity_propagation.py
--rw-r--r--   0 runner    (1001) docker     (127)     9126 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_bicluster.py
--rw-r--r--   0 runner    (1001) docker     (127)     8606 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_birch.py
--rw-r--r--   0 runner    (1001) docker     (127)     5139 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_bisect_k_means.py
--rw-r--r--   0 runner    (1001) docker     (127)    15704 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_dbscan.py
--rw-r--r--   0 runner    (1001) docker     (127)     2758 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_feature_agglomeration.py
--rw-r--r--   0 runner    (1001) docker     (127)    19392 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_hdbscan.py
--rw-r--r--   0 runner    (1001) docker     (127)    32593 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_hierarchical.py
--rw-r--r--   0 runner    (1001) docker     (127)    48900 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_k_means.py
--rw-r--r--   0 runner    (1001) docker     (127)     6740 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_mean_shift.py
--rw-r--r--   0 runner    (1001) docker     (127)    23214 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_optics.py
--rw-r--r--   0 runner    (1001) docker     (127)    11903 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cluster/tests/test_spectral.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.202611 scikit-learn-1.4.2/sklearn/compose/
--rw-r--r--   0 runner    (1001) docker     (127)      497 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/compose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    57863 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/compose/_column_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)    11914 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/compose/_target.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.202611 scikit-learn-1.4.2/sklearn/compose/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/compose/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    88405 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/compose/tests/test_column_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)    13153 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/compose/tests/test_target.py
--rw-r--r--   0 runner    (1001) docker     (127)    10497 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.206611 scikit-learn-1.4.2/sklearn/covariance/
--rw-r--r--   0 runner    (1001) docker     (127)     1116 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9073 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/_elliptic_envelope.py
--rw-r--r--   0 runner    (1001) docker     (127)    12066 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/_empirical_covariance.py
--rw-r--r--   0 runner    (1001) docker     (127)    39099 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/_graph_lasso.py
--rw-r--r--   0 runner    (1001) docker     (127)    33901 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/_robust_covariance.py
--rw-r--r--   0 runner    (1001) docker     (127)    27837 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/_shrunk_covariance.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.206611 scikit-learn-1.4.2/sklearn/covariance/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14154 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/tests/test_covariance.py
--rw-r--r--   0 runner    (1001) docker     (127)     1587 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/tests/test_elliptic_envelope.py
--rw-r--r--   0 runner    (1001) docker     (127)    10237 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/tests/test_graphical_lasso.py
--rw-r--r--   0 runner    (1001) docker     (127)     6384 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/covariance/tests/test_robust_covariance.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.206611 scikit-learn-1.4.2/sklearn/cross_decomposition/
--rw-r--r--   0 runner    (1001) docker     (127)      121 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cross_decomposition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    36773 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cross_decomposition/_pls.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.206611 scikit-learn-1.4.2/sklearn/cross_decomposition/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cross_decomposition/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    22294 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/cross_decomposition/tests/test_pls.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.210611 scikit-learn-1.4.2/sklearn/datasets/
--rw-r--r--   0 runner    (1001) docker     (127)     5170 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19046 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_arff_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)    46825 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     6707 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_california_housing.py
--rw-r--r--   0 runner    (1001) docker     (127)     7603 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_covtype.py
--rw-r--r--   0 runner    (1001) docker     (127)    13168 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_kddcup99.py
--rw-r--r--   0 runner    (1001) docker     (127)    20477 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_lfw.py
--rw-r--r--   0 runner    (1001) docker     (127)     5322 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_olivetti_faces.py
--rw-r--r--   0 runner    (1001) docker     (127)    41405 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_openml.py
--rw-r--r--   0 runner    (1001) docker     (127)    11086 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_rcv1.py
--rw-r--r--   0 runner    (1001) docker     (127)    74553 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_samples_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9189 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_species_distributions.py
--rw-r--r--   0 runner    (1001) docker     (127)     7269 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_svmlight_format_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    20735 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_svmlight_format_io.py
--rw-r--r--   0 runner    (1001) docker     (127)    18913 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/_twenty_newsgroups.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.210611 scikit-learn-1.4.2/sklearn/datasets/data/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    34742 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/boston_house_prices.csv
--rw-r--r--   0 runner    (1001) docker     (127)   119913 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/breast_cancer.csv
--rw-r--r--   0 runner    (1001) docker     (127)     7105 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/diabetes_data_raw.csv.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/diabetes_target.csv.gz
--rw-r--r--   0 runner    (1001) docker     (127)    57523 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/digits.csv.gz
--rw-r--r--   0 runner    (1001) docker     (127)     2734 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/iris.csv
--rw-r--r--   0 runner    (1001) docker     (127)      212 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/linnerud_exercise.csv
--rw-r--r--   0 runner    (1001) docker     (127)      219 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/linnerud_physiological.csv
--rw-r--r--   0 runner    (1001) docker     (127)    11157 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/data/wine_data.csv
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.214611 scikit-learn-1.4.2/sklearn/datasets/descr/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4811 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/breast_cancer.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1727 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/california_housing.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1191 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/covtype.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1455 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/diabetes.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2024 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/digits.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2665 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/iris.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3950 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/kddcup99.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4305 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/lfw.rst
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/linnerud.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1834 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/olivetti_faces.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2466 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/rcv1.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1547 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/species_distributions.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10823 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/twenty_newsgroups.rst
--rw-r--r--   0 runner    (1001) docker     (127)     3332 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/descr/wine_data.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.214611 scikit-learn-1.4.2/sklearn/datasets/images/
--rw-r--r--   0 runner    (1001) docker     (127)      712 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/images/README.txt
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/images/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)   196653 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/images/china.jpg
--rw-r--r--   0 runner    (1001) docker     (127)   142987 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/images/flower.jpg
--rw-r--r--   0 runner    (1001) docker     (127)      181 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.214611 scikit-learn-1.4.2/sklearn/datasets/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.218611 scikit-learn-1.4.2/sklearn/datasets/tests/data/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.218611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.218611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1786 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/api-v1-jd-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      889 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/api-v1-jdf-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      145 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/api-v1-jdq-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1841 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/data-v1-dl-1.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.218611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      711 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jd-1119.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdf-1119.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      364 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdl-dn-adult-census-l-2-dv-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      363 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdl-dn-adult-census-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1549 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdq-1119.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1190 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/data-v1-dl-54002.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.218611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1544 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/api-v1-jd-1590.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdf-1590.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1507 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdq-1590.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1152 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/data-v1-dl-1595261.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.218611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jd-2.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      866 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jdf-2.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      309 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jdl-dn-anneal-l-2-dv-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      346 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jdl-dn-anneal-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1501 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jdq-2.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1841 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/data-v1-dl-1666876.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.222611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      551 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-292.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      553 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-40981.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      306 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jdf-292.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      306 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jdf-40981.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      327 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jdl-dn-australian-l-2-dv-1-s-dact.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)       99 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jdl-dn-australian-l-2-dv-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      319 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jdl-dn-australian-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     2532 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/data-v1-dl-49822.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.222611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2473 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/api-v1-jd-3.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      535 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/api-v1-jdf-3.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1407 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/api-v1-jdq-3.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)    19485 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/data-v1-dl-3.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.222611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      598 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jd-40589.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      856 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdf-40589.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      315 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdl-dn-emotions-l-2-dv-3.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      318 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdl-dn-emotions-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      913 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdq-40589.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     4344 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/data-v1-dl-4644182.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.222611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      323 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jd-40675.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      307 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdf-40675.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      317 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdl-dn-glass2-l-2-dv-1-s-dact.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)       85 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdl-dn-glass2-l-2-dv-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)       88 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdl-dn-glass2-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      886 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdq-40675.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     3000 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/data-v1-dl-4965250.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.226611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      437 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/api-v1-jd-40945.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      320 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/api-v1-jdf-40945.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1042 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/api-v1-jdq-40945.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)    32243 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/data-v1-dl-16826755.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.226611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1660 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jd-40966.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     3690 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdf-40966.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      325 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdl-dn-miceprotein-l-2-dv-4.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      328 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdl-dn-miceprotein-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      934 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdq-40966.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     6471 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/data-v1-dl-17928620.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.226611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      584 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/api-v1-jd-42074.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      272 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/api-v1-jdf-42074.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      722 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/api-v1-jdq-42074.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     2326 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/data-v1-dl-21552912.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.226611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1492 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/api-v1-jd-42585.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      312 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/api-v1-jdf-42585.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      348 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/api-v1-jdq-42585.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     4519 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/data-v1-dl-21854866.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.226611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1798 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jd-561.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      425 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jdf-561.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      301 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jdl-dn-cpu-l-2-dv-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      347 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jdl-dn-cpu-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1074 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jdq-561.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     3303 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/data-v1-dl-52739.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.230611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jd-61.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      268 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jdf-61.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      293 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jdl-dn-iris-l-2-dv-1.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      330 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jdl-dn-iris-l-2-s-act-.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1121 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jdq-61.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     2342 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/data-v1-dl-61.arff.gz
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.230611 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      656 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/api-v1-jd-62.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      817 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/api-v1-jdf-62.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)      805 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/api-v1-jdq-62.json.gz
--rw-r--r--   0 runner    (1001) docker     (127)     1625 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/data-v1-dl-52352.arff.gz
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/svmlight_classification.txt
--rw-r--r--   0 runner    (1001) docker     (127)       54 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/svmlight_invalid.txt
--rw-r--r--   0 runner    (1001) docker     (127)       23 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/svmlight_invalid_order.txt
--rw-r--r--   0 runner    (1001) docker     (127)      105 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/data/svmlight_multilabel.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5339 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_20news.py
--rw-r--r--   0 runner    (1001) docker     (127)     8088 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_arff_parser.py
--rw-r--r--   0 runner    (1001) docker     (127)    11996 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1368 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_california_housing.py
--rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)     1756 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_covtype.py
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_kddcup99.py
--rw-r--r--   0 runner    (1001) docker     (127)     8229 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_lfw.py
--rw-r--r--   0 runner    (1001) docker     (127)      919 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_olivetti_faces.py
--rw-r--r--   0 runner    (1001) docker     (127)    55329 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_openml.py
--rw-r--r--   0 runner    (1001) docker     (127)     2343 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_rcv1.py
--rw-r--r--   0 runner    (1001) docker     (127)    23704 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_samples_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    20269 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/datasets/tests/test_svmlight_format.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.230611 scikit-learn-1.4.2/sklearn/decomposition/
--rw-r--r--   0 runner    (1001) docker     (127)     1296 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6545 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1118 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_cdnmf_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    76447 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_dict_learning.py
--rw-r--r--   0 runner    (1001) docker     (127)    15301 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_factor_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    26439 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_fastica.py
--rw-r--r--   0 runner    (1001) docker     (127)    15895 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_incremental_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    21922 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_kernel_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    33064 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_lda.py
--rw-r--r--   0 runner    (1001) docker     (127)    82483 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_nmf.py
--rw-r--r--   0 runner    (1001) docker     (127)     2855 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_online_lda_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    28402 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    18014 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_sparse_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    11489 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/_truncated_svd.py
--rw-r--r--   0 runner    (1001) docker     (127)      343 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.234611 scikit-learn-1.4.2/sklearn/decomposition/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    30432 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_dict_learning.py
--rw-r--r--   0 runner    (1001) docker     (127)     4172 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_factor_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    15503 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_fastica.py
--rw-r--r--   0 runner    (1001) docker     (127)    15317 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_incremental_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    20772 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_kernel_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    34178 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_nmf.py
--rw-r--r--   0 runner    (1001) docker     (127)    15825 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_online_lda.py
--rw-r--r--   0 runner    (1001) docker     (127)    35081 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)    12866 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_sparse_pca.py
--rw-r--r--   0 runner    (1001) docker     (127)     7168 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/decomposition/tests/test_truncated_svd.py
--rw-r--r--   0 runner    (1001) docker     (127)    37501 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/discriminant_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    23817 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/dummy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.234611 scikit-learn-1.4.2/sklearn/ensemble/
--rw-r--r--   0 runner    (1001) docker     (127)     1339 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    43259 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_bagging.py
--rw-r--r--   0 runner    (1001) docker     (127)    10069 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)   114121 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_forest.py
--rw-r--r--   0 runner    (1001) docker     (127)    86549 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_gb.py
--rw-r--r--   0 runner    (1001) docker     (127)     8483 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_gradient_boosting.pyx
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.238611 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/
--rw-r--r--   0 runner    (1001) docker     (127)      166 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2719 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_binning.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      692 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     2544 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     1933 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     9507 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    13385 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/binning.py
--rw-r--r--   0 runner    (1001) docker     (127)     1295 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/common.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     1747 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/common.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    92936 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)    31643 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/grower.py
--rw-r--r--   0 runner    (1001) docker     (127)    20692 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/histogram.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      721 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/meson.build
--rw-r--r--   0 runner    (1001) docker     (127)     4971 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/predictor.py
--rw-r--r--   0 runner    (1001) docker     (127)    52483 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/splitting.pyx
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.238611 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16252 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py
--rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py
--rw-r--r--   0 runner    (1001) docker     (127)    10112 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py
--rw-r--r--   0 runner    (1001) docker     (127)    60892 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)    23152 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py
--rw-r--r--   0 runner    (1001) docker     (127)     8681 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py
--rw-r--r--   0 runner    (1001) docker     (127)    16257 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py
--rw-r--r--   0 runner    (1001) docker     (127)     6345 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py
--rw-r--r--   0 runner    (1001) docker     (127)    38639 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py
--rw-r--r--   0 runner    (1001) docker     (127)     7933 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
--rw-r--r--   0 runner    (1001) docker     (127)     5845 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/utils.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    20427 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_iforest.py
--rw-r--r--   0 runner    (1001) docker     (127)    39027 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_stacking.py
--rw-r--r--   0 runner    (1001) docker     (127)    23331 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_voting.py
--rw-r--r--   0 runner    (1001) docker     (127)    45369 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/_weight_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)      232 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/ensemble/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    30204 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_bagging.py
--rw-r--r--   0 runner    (1001) docker     (127)     3637 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     9150 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)    62547 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_forest.py
--rw-r--r--   0 runner    (1001) docker     (127)    58784 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_gradient_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)    12484 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_iforest.py
--rw-r--r--   0 runner    (1001) docker     (127)    29896 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_stacking.py
--rw-r--r--   0 runner    (1001) docker     (127)    24015 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_voting.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    25403 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/ensemble/tests/test_weight_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)     6117 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/exceptions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      252 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/enable_halving_search_cv.py
--rw-r--r--   0 runner    (1001) docker     (127)      746 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/enable_hist_gradient_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)      688 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/enable_iterative_imputer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/experimental/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      666 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py
--rw-r--r--   0 runner    (1001) docker     (127)     1683 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/tests/test_enable_iterative_imputer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1890 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/experimental/tests/test_enable_successive_halving.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/externals/
--rw-r--r--   0 runner    (1001) docker     (127)      270 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/README
--rw-r--r--   0 runner    (1001) docker     (127)       42 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    38341 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_arff.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/externals/_packaging/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_packaging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2922 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_packaging/_structures.py
--rw-r--r--   0 runner    (1001) docker     (127)    16134 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_packaging/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/externals/_scipy/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_scipy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/externals/_scipy/sparse/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_scipy/sparse/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.242612 scikit-learn-1.4.2/sklearn/externals/_scipy/sparse/csgraph/
--rw-r--r--   0 runner    (1001) docker     (127)       34 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_scipy/sparse/csgraph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18150 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/_scipy/sparse/csgraph/_laplacian.py
--rw-r--r--   0 runner    (1001) docker     (127)      301 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/externals/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.246611 scikit-learn-1.4.2/sklearn/feature_extraction/
--rw-r--r--   0 runner    (1001) docker     (127)      439 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15718 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/_dict_vectorizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7382 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/_hash.py
--rw-r--r--   0 runner    (1001) docker     (127)     2996 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/_hashing_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     5645 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/_stop_words.py
--rw-r--r--   0 runner    (1001) docker     (127)    23024 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/image.py
--rw-r--r--   0 runner    (1001) docker     (127)      220 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.246611 scikit-learn-1.4.2/sklearn/feature_extraction/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8272 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_dict_vectorizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_feature_hasher.py
--rw-r--r--   0 runner    (1001) docker     (127)    12154 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_image.py
--rw-r--r--   0 runner    (1001) docker     (127)    53098 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_text.py
--rw-r--r--   0 runner    (1001) docker     (127)    78015 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_extraction/text.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.246611 scikit-learn-1.4.2/sklearn/feature_selection/
--rw-r--r--   0 runner    (1001) docker     (127)     1111 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    18920 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_from_model.py
--rw-r--r--   0 runner    (1001) docker     (127)    18323 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_mutual_info.py
--rw-r--r--   0 runner    (1001) docker     (127)    28067 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_rfe.py
--rw-r--r--   0 runner    (1001) docker     (127)    11461 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_sequential.py
--rw-r--r--   0 runner    (1001) docker     (127)    40350 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_univariate_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4467 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/_variance_threshold.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.246611 scikit-learn-1.4.2/sklearn/feature_selection/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4781 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3139 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_chi2.py
--rw-r--r--   0 runner    (1001) docker     (127)    32506 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_feature_select.py
--rw-r--r--   0 runner    (1001) docker     (127)    23051 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_from_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     9182 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_mutual_info.py
--rw-r--r--   0 runner    (1001) docker     (127)    20881 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_rfe.py
--rw-r--r--   0 runner    (1001) docker     (127)    10592 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_sequential.py
--rw-r--r--   0 runner    (1001) docker     (127)     2640 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/feature_selection/tests/test_variance_threshold.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/gaussian_process/
--rw-r--r--   0 runner    (1001) docker     (127)      504 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    36524 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/_gpc.py
--rw-r--r--   0 runner    (1001) docker     (127)    28140 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/_gpr.py
--rw-r--r--   0 runner    (1001) docker     (127)    85400 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/kernels.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/gaussian_process/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1571 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/tests/_mini_sequence_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)    10020 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/tests/test_gpc.py
--rw-r--r--   0 runner    (1001) docker     (127)    29775 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/tests/test_gpr.py
--rw-r--r--   0 runner    (1001) docker     (127)    13570 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/gaussian_process/tests/test_kernels.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/impute/
--rw-r--r--   0 runner    (1001) docker     (127)      943 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    40012 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    35716 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/_iterative.py
--rw-r--r--   0 runner    (1001) docker     (127)    14662 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/_knn.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/impute/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3367 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     7610 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)    59674 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/tests/test_impute.py
--rw-r--r--   0 runner    (1001) docker     (127)    16638 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/impute/tests/test_knn.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/inspection/
--rw-r--r--   0 runner    (1001) docker     (127)      452 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    31782 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_partial_dependence.py
--rw-r--r--   0 runner    (1001) docker     (127)     2137 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_pd_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    11501 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_permutation_importance.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/inspection/_plot/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_plot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15199 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_plot/decision_boundary.py
--rw-r--r--   0 runner    (1001) docker     (127)    59995 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_plot/partial_dependence.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.250611 scikit-learn-1.4.2/sklearn/inspection/_plot/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_plot/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    21278 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_plot/tests/test_boundary_decision_display.py
--rw-r--r--   0 runner    (1001) docker     (127)    36426 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.254612 scikit-learn-1.4.2/sklearn/inspection/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    33329 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/tests/test_partial_dependence.py
--rw-r--r--   0 runner    (1001) docker     (127)     1640 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/tests/test_pd_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    19940 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/inspection/tests/test_permutation_importance.py
--rw-r--r--   0 runner    (1001) docker     (127)    16637 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/isotonic.py
--rw-r--r--   0 runner    (1001) docker     (127)    40838 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/kernel_approximation.py
--rw-r--r--   0 runner    (1001) docker     (127)     9185 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/kernel_ridge.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.258612 scikit-learn-1.4.2/sklearn/linear_model/
--rw-r--r--   0 runner    (1001) docker     (127)     2529 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    27254 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    29747 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_bayes.py
--rw-r--r--   0 runner    (1001) docker     (127)    33178 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_cd_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)   109065 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_coordinate_descent.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.258612 scikit-learn-1.4.2/sklearn/linear_model/_glm/
--rw-r--r--   0 runner    (1001) docker     (127)      263 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_glm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19275 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_glm/_newton_solver.py
--rw-r--r--   0 runner    (1001) docker     (127)    31930 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_glm/glm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.258612 scikit-learn-1.4.2/sklearn/linear_model/_glm/tests/
--rw-r--r--   0 runner    (1001) docker     (127)       24 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_glm/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    40696 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_glm/tests/test_glm.py
--rw-r--r--   0 runner    (1001) docker     (127)    12346 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_huber.py
--rw-r--r--   0 runner    (1001) docker     (127)    81551 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_least_angle.py
--rw-r--r--   0 runner    (1001) docker     (127)    26796 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_linear_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    84028 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_logistic.py
--rw-r--r--   0 runner    (1001) docker     (127)    37378 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_omp.py
--rw-r--r--   0 runner    (1001) docker     (127)    19323 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_passive_aggressive.py
--rw-r--r--   0 runner    (1001) docker     (127)     7707 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_perceptron.py
--rw-r--r--   0 runner    (1001) docker     (127)    10790 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_quantile.py
--rw-r--r--   0 runner    (1001) docker     (127)    22163 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_ransac.py
--rw-r--r--   0 runner    (1001) docker     (127)    93020 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_ridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    12320 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_sag.py
--rw-r--r--   0 runner    (1001) docker     (127)    31093 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_sag_fast.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)      897 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_sgd_fast.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    23941 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_sgd_fast.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)      614 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_sgd_fast_helpers.h
--rw-r--r--   0 runner    (1001) docker     (127)    89642 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_stochastic_gradient.py
--rw-r--r--   0 runner    (1001) docker     (127)    15814 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/_theil_sen.py
--rw-r--r--   0 runner    (1001) docker     (127)      742 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.262612 scikit-learn-1.4.2/sklearn/linear_model/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    27229 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    11584 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_bayes.py
--rw-r--r--   0 runner    (1001) docker     (127)     4687 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)    56926 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_coordinate_descent.py
--rw-r--r--   0 runner    (1001) docker     (127)     7598 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_huber.py
--rw-r--r--   0 runner    (1001) docker     (127)    29553 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_least_angle.py
--rw-r--r--   0 runner    (1001) docker     (127)    12851 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_linear_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    75541 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_logistic.py
--rw-r--r--   0 runner    (1001) docker     (127)     8913 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_omp.py
--rw-r--r--   0 runner    (1001) docker     (127)     8994 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_passive_aggressive.py
--rw-r--r--   0 runner    (1001) docker     (127)     2608 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_perceptron.py
--rw-r--r--   0 runner    (1001) docker     (127)    11425 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_quantile.py
--rw-r--r--   0 runner    (1001) docker     (127)    16778 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_ransac.py
--rw-r--r--   0 runner    (1001) docker     (127)    70145 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_ridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    31398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_sag.py
--rw-r--r--   0 runner    (1001) docker     (127)    70696 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_sgd.py
--rw-r--r--   0 runner    (1001) docker     (127)    12654 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_sparse_coordinate_descent.py
--rw-r--r--   0 runner    (1001) docker     (127)     9881 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/linear_model/tests/test_theil_sen.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.262612 scikit-learn-1.4.2/sklearn/manifold/
--rw-r--r--   0 runner    (1001) docker     (127)      533 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11610 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_barnes_hut_tsne.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    15587 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_isomap.py
--rw-r--r--   0 runner    (1001) docker     (127)    29408 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_locally_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    23693 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_mds.py
--rw-r--r--   0 runner    (1001) docker     (127)    29120 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_spectral_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    43994 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_t_sne.py
--rw-r--r--   0 runner    (1001) docker     (127)     3949 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/_utils.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      323 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.262612 scikit-learn-1.4.2/sklearn/manifold/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12074 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/tests/test_isomap.py
--rw-r--r--   0 runner    (1001) docker     (127)     5772 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/tests/test_locally_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     3043 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/tests/test_mds.py
--rw-r--r--   0 runner    (1001) docker     (127)    19398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/tests/test_spectral_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    38871 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/manifold/tests/test_t_sne.py
--rw-r--r--   0 runner    (1001) docker     (127)     5175 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.266612 scikit-learn-1.4.2/sklearn/metrics/
--rw-r--r--   0 runner    (1001) docker     (127)     4554 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7292 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)   121687 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)     4378 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_dist_metrics.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    91783 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_dist_metrics.pyx.tp
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.266612 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/
--rw-r--r--   0 runner    (1001) docker     (127)     5122 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      979 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    19252 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     6408 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     3563 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    18353 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_base.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)      151 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_classmode.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     1948 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    15087 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)    29726 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py
--rw-r--r--   0 runner    (1001) docker     (127)     5925 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    20662 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     3254 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    19706 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     7321 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     5739 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/meson.build
--rw-r--r--   0 runner    (1001) docker     (127)     3542 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_pairwise_fast.pyx
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.266612 scikit-learn-1.4.2/sklearn/metrics/_plot/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16355 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (127)    10770 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/det_curve.py
--rw-r--r--   0 runner    (1001) docker     (127)    17688 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/precision_recall_curve.py
--rw-r--r--   0 runner    (1001) docker     (127)    14346 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/regression.py
--rw-r--r--   0 runner    (1001) docker     (127)    13545 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/roc_curve.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.270612 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8815 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_common_curve_display.py
--rw-r--r--   0 runner    (1001) docker     (127)    13705 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_confusion_matrix_display.py
--rw-r--r--   0 runner    (1001) docker     (127)     3426 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_det_curve_display.py
--rw-r--r--   0 runner    (1001) docker     (127)    13100 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_precision_recall_display.py
--rw-r--r--   0 runner    (1001) docker     (127)     5786 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_predict_error_display.py
--rw-r--r--   0 runner    (1001) docker     (127)    10135 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_roc_curve_display.py
--rw-r--r--   0 runner    (1001) docker     (127)    75994 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_ranking.py
--rw-r--r--   0 runner    (1001) docker     (127)    61720 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)    33389 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/_scorer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.270612 scikit-learn-1.4.2/sklearn/metrics/cluster/
--rw-r--r--   0 runner    (1001) docker     (127)     1396 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3378 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/_bicluster.py
--rw-r--r--   0 runner    (1001) docker     (127)     2754 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    44498 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/_supervised.py
--rw-r--r--   0 runner    (1001) docker     (127)    17063 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/_unsupervised.py
--rw-r--r--   0 runner    (1001) docker     (127)      198 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.270612 scikit-learn-1.4.2/sklearn/metrics/cluster/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1719 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_bicluster.py
--rw-r--r--   0 runner    (1001) docker     (127)     7755 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)    17873 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_supervised.py
--rw-r--r--   0 runner    (1001) docker     (127)    12269 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_unsupervised.py
--rw-r--r--   0 runner    (1001) docker     (127)     1187 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/meson.build
--rw-r--r--   0 runner    (1001) docker     (127)    85973 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/pairwise.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.274612 scikit-learn-1.4.2/sklearn/metrics/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)   101469 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)    60017 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)    14802 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_dist_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)    56671 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_pairwise.py
--rw-r--r--   0 runner    (1001) docker     (127)    53017 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_pairwise_distances_reduction.py
--rw-r--r--   0 runner    (1001) docker     (127)    82385 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_ranking.py
--rw-r--r--   0 runner    (1001) docker     (127)    27231 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)    53184 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/metrics/tests/test_score_objects.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.274612 scikit-learn-1.4.2/sklearn/mixture/
--rw-r--r--   0 runner    (1001) docker     (127)      243 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18718 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    33467 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/_bayesian_mixture.py
--rw-r--r--   0 runner    (1001) docker     (127)    31659 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/_gaussian_mixture.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.274612 scikit-learn-1.4.2/sklearn/mixture/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17110 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/tests/test_bayesian_mixture.py
--rw-r--r--   0 runner    (1001) docker     (127)    47721 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/tests/test_gaussian_mixture.py
--rw-r--r--   0 runner    (1001) docker     (127)      992 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/mixture/tests/test_mixture.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.274612 scikit-learn-1.4.2/sklearn/model_selection/
--rw-r--r--   0 runner    (1001) docker     (127)     2316 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    35291 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/_plot.py
--rw-r--r--   0 runner    (1001) docker     (127)    75511 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/_search.py
--rw-r--r--   0 runner    (1001) docker     (127)    43900 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/_search_successive_halving.py
--rw-r--r--   0 runner    (1001) docker     (127)    99167 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/_split.py
--rw-r--r--   0 runner    (1001) docker     (127)    88502 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/_validation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.278612 scikit-learn-1.4.2/sklearn/model_selection/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      641 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/common.py
--rw-r--r--   0 runner    (1001) docker     (127)    19330 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/test_plot.py
--rw-r--r--   0 runner    (1001) docker     (127)    84676 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/test_search.py
--rw-r--r--   0 runner    (1001) docker     (127)    71585 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/test_split.py
--rw-r--r--   0 runner    (1001) docker     (127)    28876 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/test_successive_halving.py
--rw-r--r--   0 runner    (1001) docker     (127)    88874 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/model_selection/tests/test_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)    43819 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/multiclass.py
--rw-r--r--   0 runner    (1001) docker     (127)    40821 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/multioutput.py
--rw-r--r--   0 runner    (1001) docker     (127)    55670 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/naive_bayes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.278612 scikit-learn-1.4.2/sklearn/neighbors/
--rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9326 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_ball_tree.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)    51658 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)   100766 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_binary_tree.pxi.tp
--rw-r--r--   0 runner    (1001) docker     (127)    31731 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_classification.py
--rw-r--r--   0 runner    (1001) docker     (127)    25037 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)    11092 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_kd_tree.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)    12461 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_kde.py
--rw-r--r--   0 runner    (1001) docker     (127)    19708 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_lof.py
--rw-r--r--   0 runner    (1001) docker     (127)    19586 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_nca.py
--rw-r--r--   0 runner    (1001) docker     (127)     9645 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_nearest_centroid.py
--rw-r--r--   0 runner    (1001) docker     (127)      288 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_partition_nodes.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     4120 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_partition_nodes.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     4259 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_quad_tree.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    23721 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_quad_tree.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    18123 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     6179 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/_unsupervised.py
--rw-r--r--   0 runner    (1001) docker     (127)     1447 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.282612 scikit-learn-1.4.2/sklearn/neighbors/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7097 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_ball_tree.py
--rw-r--r--   0 runner    (1001) docker     (127)     3547 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     3898 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_kd_tree.py
--rw-r--r--   0 runner    (1001) docker     (127)     9745 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_kde.py
--rw-r--r--   0 runner    (1001) docker     (127)    12899 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_lof.py
--rw-r--r--   0 runner    (1001) docker     (127)    19052 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_nca.py
--rw-r--r--   0 runner    (1001) docker     (127)     5672 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_nearest_centroid.py
--rw-r--r--   0 runner    (1001) docker     (127)    81900 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_neighbors.py
--rw-r--r--   0 runner    (1001) docker     (127)     8137 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_neighbors_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     9281 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_neighbors_tree.py
--rw-r--r--   0 runner    (1001) docker     (127)     4856 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neighbors/tests/test_quad_tree.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.282612 scikit-learn-1.4.2/sklearn/neural_network/
--rw-r--r--   0 runner    (1001) docker     (127)      273 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6329 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    60524 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/_multilayer_perceptron.py
--rw-r--r--   0 runner    (1001) docker     (127)    15121 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/_rbm.py
--rw-r--r--   0 runner    (1001) docker     (127)     8823 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/_stochastic_optimizers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.282612 scikit-learn-1.4.2/sklearn/neural_network/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      796 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    31862 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/tests/test_mlp.py
--rw-r--r--   0 runner    (1001) docker     (127)     8048 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/tests/test_rbm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4137 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/neural_network/tests/test_stochastic_optimizers.py
--rw-r--r--   0 runner    (1001) docker     (127)    66341 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.286612 scikit-learn-1.4.2/sklearn/preprocessing/
--rw-r--r--   0 runner    (1001) docker     (127)     1460 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9170 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_csr_polynomial_expansion.pyx
--rw-r--r--   0 runner    (1001) docker     (127)   125337 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_data.py
--rw-r--r--   0 runner    (1001) docker     (127)    17376 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_discretization.py
--rw-r--r--   0 runner    (1001) docker     (127)    67754 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_encoders.py
--rw-r--r--   0 runner    (1001) docker     (127)    16633 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_function_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)    30800 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_label.py
--rw-r--r--   0 runner    (1001) docker     (127)    47444 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_polynomial.py
--rw-r--r--   0 runner    (1001) docker     (127)    20476 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_target_encoder.py
--rw-r--r--   0 runner    (1001) docker     (127)     5962 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/_target_encoder_fast.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.286612 scikit-learn-1.4.2/sklearn/preprocessing/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6793 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)    94473 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_data.py
--rw-r--r--   0 runner    (1001) docker     (127)    18054 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_discretization.py
--rw-r--r--   0 runner    (1001) docker     (127)    78684 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_encoders.py
--rw-r--r--   0 runner    (1001) docker     (127)    19827 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_function_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)    23634 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_label.py
--rw-r--r--   0 runner    (1001) docker     (127)    42411 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_polynomial.py
--rw-r--r--   0 runner    (1001) docker     (127)    27915 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/preprocessing/tests/test_target_encoder.py
--rw-r--r--   0 runner    (1001) docker     (127)    28095 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/random_projection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.286612 scikit-learn-1.4.2/sklearn/semi_supervised/
--rw-r--r--   0 runner    (1001) docker     (127)      448 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/semi_supervised/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    21294 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/semi_supervised/_label_propagation.py
--rw-r--r--   0 runner    (1001) docker     (127)    14342 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/semi_supervised/_self_training.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.286612 scikit-learn-1.4.2/sklearn/semi_supervised/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/semi_supervised/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8803 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/semi_supervised/tests/test_label_propagation.py
--rw-r--r--   0 runner    (1001) docker     (127)    12543 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/semi_supervised/tests/test_self_training.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.290612 scikit-learn-1.4.2/sklearn/svm/
--rw-r--r--   0 runner    (1001) docker     (127)      636 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    42474 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3251 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_bounds.py
--rw-r--r--   0 runner    (1001) docker     (127)    66940 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_classes.py
--rw-r--r--   0 runner    (1001) docker     (127)     1686 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_liblinear.pxi
--rw-r--r--   0 runner    (1001) docker     (127)     4145 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_liblinear.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     3231 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_libsvm.pxi
--rw-r--r--   0 runner    (1001) docker     (127)    26994 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_libsvm.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    19398 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_libsvm_sparse.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      298 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/_newrand.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     1338 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.086610 scikit-learn-1.4.2/sklearn/svm/src/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.290612 scikit-learn-1.4.2/sklearn/svm/src/liblinear/
--rw-r--r--   0 runner    (1001) docker     (127)     1486 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/COPYRIGHT
--rw-r--r--   0 runner    (1001) docker     (127)      458 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/_cython_blas_helpers.h
--rw-r--r--   0 runner    (1001) docker     (127)     6366 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/liblinear_helper.c
--rw-r--r--   0 runner    (1001) docker     (127)    62634 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/linear.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     2459 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/linear.h
--rw-r--r--   0 runner    (1001) docker     (127)     4940 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/tron.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      768 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/liblinear/tron.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.294612 scikit-learn-1.4.2/sklearn/svm/src/libsvm/
--rw-r--r--   0 runner    (1001) docker     (127)      769 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/LIBSVM_CHANGES
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/_svm_cython_blas_helpers.h
--rw-r--r--   0 runner    (1001) docker     (127)    11665 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/libsvm_helper.c
--rw-r--r--   0 runner    (1001) docker     (127)    13181 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/libsvm_sparse_helper.c
--rw-r--r--   0 runner    (1001) docker     (127)      173 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/libsvm_template.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    69105 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/svm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     6262 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/libsvm/svm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.294612 scikit-learn-1.4.2/sklearn/svm/src/newrand/
--rw-r--r--   0 runner    (1001) docker     (127)     1840 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/src/newrand/newrand.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.294612 scikit-learn-1.4.2/sklearn/svm/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5232 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/tests/test_bounds.py
--rw-r--r--   0 runner    (1001) docker     (127)    15697 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/tests/test_sparse.py
--rw-r--r--   0 runner    (1001) docker     (127)    49059 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/svm/tests/test_svm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.298612 scikit-learn-1.4.2/sklearn/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15568 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/metadata_routing_common.py
--rw-r--r--   0 runner    (1001) docker     (127)     3311 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/random_seed.py
--rw-r--r--   0 runner    (1001) docker     (127)    28614 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1167 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_build.py
--rw-r--r--   0 runner    (1001) docker     (127)    40422 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_calibration.py
--rw-r--r--   0 runner    (1001) docker     (127)      267 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_check_build.py
--rw-r--r--   0 runner    (1001) docker     (127)    19553 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_common.py
--rw-r--r--   0 runner    (1001) docker     (127)     6807 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_config.py
--rw-r--r--   0 runner    (1001) docker     (127)    23194 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_discriminant_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    11813 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_docstring_parameters.py
--rw-r--r--   0 runner    (1001) docker     (127)     6841 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_docstrings.py
--rw-r--r--   0 runner    (1001) docker     (127)    21319 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_dummy.py
--rw-r--r--   0 runner    (1001) docker     (127)      470 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_init.py
--rw-r--r--   0 runner    (1001) docker     (127)    22169 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_isotonic.py
--rw-r--r--   0 runner    (1001) docker     (127)    16878 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_kernel_approximation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2888 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_kernel_ridge.py
--rw-r--r--   0 runner    (1001) docker     (127)    34758 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_metadata_routing.py
--rw-r--r--   0 runner    (1001) docker     (127)    10298 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_metaestimators.py
--rw-r--r--   0 runner    (1001) docker     (127)    22521 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_metaestimators_metadata_routing.py
--rw-r--r--   0 runner    (1001) docker     (127)     3222 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_min_dependencies_readme.py
--rw-r--r--   0 runner    (1001) docker     (127)    33480 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_multiclass.py
--rw-r--r--   0 runner    (1001) docker     (127)    29175 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_multioutput.py
--rw-r--r--   0 runner    (1001) docker     (127)    35027 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_naive_bayes.py
--rw-r--r--   0 runner    (1001) docker     (127)    63837 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)    16477 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_public_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    19583 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tests/test_random_projection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.302612 scikit-learn-1.4.2/sklearn/tree/
--rw-r--r--   0 runner    (1001) docker     (127)      534 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    75272 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_classes.py
--rw-r--r--   0 runner    (1001) docker     (127)     4760 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_criterion.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    62083 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_criterion.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    39293 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_export.py
--rw-r--r--   0 runner    (1001) docker     (127)     5142 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_reingold_tilford.py
--rw-r--r--   0 runner    (1001) docker     (127)     4784 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_splitter.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    60816 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_splitter.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     4679 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_tree.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    72935 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_tree.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_utils.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    16808 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/_utils.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      725 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/meson.build
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.302612 scikit-learn-1.4.2/sklearn/tree/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17471 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/tests/test_export.py
--rw-r--r--   0 runner    (1001) docker     (127)    18590 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/tests/test_monotonic_tree.py
--rw-r--r--   0 runner    (1001) docker     (127)     1461 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/tests/test_reingold_tilford.py
--rw-r--r--   0 runner    (1001) docker     (127)    94675 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/tree/tests/test_tree.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.314612 scikit-learn-1.4.2/sklearn/utils/
--rw-r--r--   0 runner    (1001) docker     (127)    40703 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_arpack.py
--rw-r--r--   0 runner    (1001) docker     (127)    18876 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_array_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     2873 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_available_if.py
--rw-r--r--   0 runner    (1001) docker     (127)     2096 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_bunch.py
--rw-r--r--   0 runner    (1001) docker     (127)     1565 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_cython_blas.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     7968 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_cython_blas.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    11379 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_encode.py
--rw-r--r--   0 runner    (1001) docker     (127)    11016 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_estimator_html_repr.css
--rw-r--r--   0 runner    (1001) docker     (127)    18308 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_estimator_html_repr.py
--rw-r--r--   0 runner    (1001) docker     (127)      476 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_fast_dict.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     4613 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_fast_dict.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      256 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_heap.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     2253 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_heap.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     1384 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_isfinite.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      710 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_joblib.py
--rw-r--r--   0 runner    (1001) docker     (127)     1798 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_mask.py
--rw-r--r--   0 runner    (1001) docker     (127)    54375 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_metadata_requests.py
--rw-r--r--   0 runner    (1001) docker     (127)    13093 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_mocking.py
--rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_openmp_helpers.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     3143 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_openmp_helpers.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    28445 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_param_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3473 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_plotting.py
--rw-r--r--   0 runner    (1001) docker     (127)    18516 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_pprint.py
--rw-r--r--   0 runner    (1001) docker     (127)     1241 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_random.pxd
--rw-r--r--   0 runner    (1001) docker     (127)    12577 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_random.pyx
--rw-r--r--   0 runner    (1001) docker     (127)    11563 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_response.py
--rw-r--r--   0 runner    (1001) docker     (127)     2527 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_seq_dataset.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)    12337 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_seq_dataset.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)    14015 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_set_output.py
--rw-r--r--   0 runner    (1001) docker     (127)     2491 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_show_versions.py
--rw-r--r--   0 runner    (1001) docker     (127)      161 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_sorting.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_sorting.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     2071 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_tags.py
--rw-r--r--   0 runner    (1001) docker     (127)    39674 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_testing.py
--rw-r--r--   0 runner    (1001) docker     (127)     1403 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_typedefs.pxd
--rw-r--r--   0 runner    (1001) docker     (127)      417 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_typedefs.pyx
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_vector_sentinel.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     4458 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_vector_sentinel.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     1384 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_weight_vector.pxd.tp
--rw-r--r--   0 runner    (1001) docker     (127)     6977 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/_weight_vector.pyx.tp
--rw-r--r--   0 runner    (1001) docker     (127)     2948 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/arrayfuncs.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     8245 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/class_weight.py
--rw-r--r--   0 runner    (1001) docker     (127)     3295 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/deprecation.py
--rw-r--r--   0 runner    (1001) docker     (127)     9109 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/discovery.py
--rw-r--r--   0 runner    (1001) docker     (127)   167648 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/estimator_checks.py
--rw-r--r--   0 runner    (1001) docker     (127)    44378 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/extmath.py
--rw-r--r--   0 runner    (1001) docker     (127)    14146 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/fixes.py
--rw-r--r--   0 runner    (1001) docker     (127)     5852 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     2271 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/meson.build
--rw-r--r--   0 runner    (1001) docker     (127)      958 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/metadata_routing.py
--rw-r--r--   0 runner    (1001) docker     (127)     5869 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/metaestimators.py
--rw-r--r--   0 runner    (1001) docker     (127)    18935 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/multiclass.py
--rw-r--r--   0 runner    (1001) docker     (127)      864 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/murmurhash.pxd
--rw-r--r--   0 runner    (1001) docker     (127)     4589 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/murmurhash.pyx
--rw-r--r--   0 runner    (1001) docker     (127)     9116 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/optimize.py
--rw-r--r--   0 runner    (1001) docker     (127)     4256 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/parallel.py
--rw-r--r--   0 runner    (1001) docker     (127)     3723 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/random.py
--rw-r--r--   0 runner    (1001) docker     (127)    22673 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/sparsefuncs.py
--rw-r--r--   0 runner    (1001) docker     (127)    20393 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/sparsefuncs_fast.pyx
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.314612 scikit-learn-1.4.2/sklearn/utils/src/
--rw-r--r--   0 runner    (1001) docker     (127)     7969 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/src/MurmurHash3.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     1155 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/src/MurmurHash3.h
--rw-r--r--   0 runner    (1001) docker     (127)     2357 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/stats.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:32.318612 scikit-learn-1.4.2/sklearn/utils/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      490 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_arpack.py
--rw-r--r--   0 runner    (1001) docker     (127)    10842 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_array_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     1310 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_arrayfuncs.py
--rw-r--r--   0 runner    (1001) docker     (127)      813 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_bunch.py
--rw-r--r--   0 runner    (1001) docker     (127)    12309 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_class_weight.py
--rw-r--r--   0 runner    (1001) docker     (127)     6459 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_cython_blas.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_cython_templating.py
--rw-r--r--   0 runner    (1001) docker     (127)     2023 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_deprecation.py
--rw-r--r--   0 runner    (1001) docker     (127)     9603 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_encode.py
--rw-r--r--   0 runner    (1001) docker     (127)    43757 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_estimator_checks.py
--rw-r--r--   0 runner    (1001) docker     (127)    18057 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_estimator_html_repr.py
--rw-r--r--   0 runner    (1001) docker     (127)    36993 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_extmath.py
--rw-r--r--   0 runner    (1001) docker     (127)     1356 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_fast_dict.py
--rw-r--r--   0 runner    (1001) docker     (127)     5722 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_fixes.py
--rw-r--r--   0 runner    (1001) docker     (127)     3047 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     2107 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_metaestimators.py
--rw-r--r--   0 runner    (1001) docker     (127)     6075 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_mocking.py
--rw-r--r--   0 runner    (1001) docker     (127)    20238 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_multiclass.py
--rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_murmurhash.py
--rw-r--r--   0 runner    (1001) docker     (127)      768 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_optimize.py
--rw-r--r--   0 runner    (1001) docker     (127)     3650 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_parallel.py
--rw-r--r--   0 runner    (1001) docker     (127)    24201 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_param_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2768 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_plotting.py
--rw-r--r--   0 runner    (1001) docker     (127)    27339 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_pprint.py
--rw-r--r--   0 runner    (1001) docker     (127)     7157 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_random.py
--rw-r--r--   0 runner    (1001) docker     (127)    12608 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_response.py
--rw-r--r--   0 runner    (1001) docker     (127)     5890 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_seq_dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)    15290 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_set_output.py
--rw-r--r--   0 runner    (1001) docker     (127)     1846 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_shortest_path.py
--rw-r--r--   0 runner    (1001) docker     (127)     1006 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_show_versions.py
--rw-r--r--   0 runner    (1001) docker     (127)    34923 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_sparsefuncs.py
--rw-r--r--   0 runner    (1001) docker     (127)     2760 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     1396 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_tags.py
--rw-r--r--   0 runner    (1001) docker     (127)    27802 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_testing.py
--rw-r--r--   0 runner    (1001) docker     (127)      631 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_typedefs.py
--rw-r--r--   0 runner    (1001) docker     (127)    29562 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    69302 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)      665 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/tests/test_weight_vector.py
--rw-r--r--   0 runner    (1001) docker     (127)    88758 2024-04-09 14:20:12.000000 scikit-learn-1.4.2/sklearn/utils/validation.py
+-rw-r--r--   0        0        0      645 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/CODE_OF_CONDUCT.md
+-rw-r--r--   0        0        0     2112 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/CONTRIBUTING.md
+-rw-r--r--   0        0        0     1532 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/COPYING
+-rw-r--r--   0        0        0     1021 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/MANIFEST.in
+-rw-r--r--   0        0        0     1645 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/Makefile
+-rw-r--r--   0        0        0     7608 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/README.rst
+-rw-r--r--   0        0        0      708 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/SECURITY.md
+-rw-r--r--   0        0        0     5038 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/Makefile
+-rw-r--r--   0        0        0      254 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/README.md
+-rw-r--r--   0        0        0    17438 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/about.rst
+-rw-r--r--   0        0        0      255 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/binder/requirements.txt
+-rw-r--r--   0        0        0    25058 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/common_pitfalls.rst
+-rw-r--r--   0        0        0      554 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/communication_team.rst
+-rw-r--r--   0        0        0       17 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/communication_team_emeritus.rst
+-rw-r--r--   0        0        0    17218 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/computing/computational_performance.rst
+-rw-r--r--   0        0        0    14899 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/computing/parallelism.rst
+-rw-r--r--   0        0        0     6368 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/computing/scaling_strategies.rst
+-rw-r--r--   0        0        0      313 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/computing.rst
+-rw-r--r--   0        0        0    27314 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/conf.py
+-rw-r--r--   0        0        0     6430 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/conftest.py
+-rw-r--r--   0        0        0      406 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/contents.rst
+-rw-r--r--   0        0        0     1841 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/contributor_experience_team.rst
+-rw-r--r--   0        0        0       15 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/contributor_experience_team_emeritus.rst
+-rw-r--r--   0        0        0     1381 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/data_transforms.rst
+-rw-r--r--   0        0        0    12854 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/datasets/loading_other_datasets.rst
+-rw-r--r--   0        0        0     1017 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/datasets/real_world.rst
+-rw-r--r--   0        0        0     4095 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/datasets/sample_generators.rst
+-rw-r--r--   0        0        0      975 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/datasets/toy_dataset.rst
+-rw-r--r--   0        0        0     2657 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/datasets.rst
+-rw-r--r--   0        0        0    16858 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/advanced_installation.rst
+-rw-r--r--   0        0        0     6569 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/bug_triaging.rst
+-rw-r--r--   0        0        0    61322 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/contributing.rst
+-rw-r--r--   0        0        0     6833 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/cython.rst
+-rw-r--r--   0        0        0    38849 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/develop.rst
+-rw-r--r--   0        0        0      382 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/index.rst
+-rw-r--r--   0        0        0    19748 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/maintainer.rst
+-rw-r--r--   0        0        0    15265 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/minimal_reproducer.rst
+-rw-r--r--   0        0        0    16960 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/performance.rst
+-rw-r--r--   0        0        0     4374 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/plotting.rst
+-rw-r--r--   0        0        0    15038 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/tips.rst
+-rw-r--r--   0        0        0     9447 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/developers/utilities.rst
+-rw-r--r--   0        0        0      186 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/dispatching.rst
+-rw-r--r--   0        0        0      723 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/documentation_team.rst
+-rw-r--r--   0        0        0    24426 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/faq.rst
+-rw-r--r--   0        0        0    10279 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/getting_started.rst
+-rw-r--r--   0        0        0    91564 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/glossary.rst
+-rw-r--r--   0        0        0    10251 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/governance.rst
+-rw-r--r--   0        0        0    11616 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/axa-small.png
+-rw-r--r--   0        0        0    17847 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/axa.png
+-rw-r--r--   0        0        0    31049 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/bcg.png
+-rw-r--r--   0        0        0    54774 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/beta_divergence.png
+-rw-r--r--   0        0        0    12497 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/bnp-small.png
+-rw-r--r--   0        0        0    21156 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/bnp.png
+-rw-r--r--   0        0        0    13205 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/cds-logo.png
+-rw-r--r--   0        0        0     4428 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/chanel-small.png
+-rw-r--r--   0        0        0    20091 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/chanel.png
+-rw-r--r--   0        0        0     1170 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/columbia-small.png
+-rw-r--r--   0        0        0     1769 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/columbia.png
+-rw-r--r--   0        0        0     4037 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/czi_logo.svg
+-rw-r--r--   0        0        0     6101 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/dataiku-small.png
+-rw-r--r--   0        0        0     9040 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/dataiku.png
+-rw-r--r--   0        0        0    18585 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/digicosme.png
+-rw-r--r--   0        0        0    17842 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/dysco.png
+-rw-r--r--   0        0        0     1110 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/fnrs-logo-small.png
+-rw-r--r--   0        0        0    18012 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/fujitsu.png
+-rw-r--r--   0        0        0    89394 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/generated-doc-ci.png
+-rw-r--r--   0        0        0     4692 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/google-small.png
+-rw-r--r--   0        0        0    45148 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/grid_search_cross_validation.png
+-rw-r--r--   0        0        0   101880 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/grid_search_workflow.png
+-rw-r--r--   0        0        0     6345 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/huggingface_logo-noborder.png
+-rw-r--r--   0        0        0    26245 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/inria-logo.jpg
+-rw-r--r--   0        0        0     7105 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/inria-small.png
+-rw-r--r--   0        0        0     9623 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/intel-small.png
+-rw-r--r--   0        0        0    15019 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/intel.png
+-rw-r--r--   0        0        0    27033 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/iris.pdf
+-rw-r--r--   0        0        0    22002 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/iris.svg
+-rw-r--r--   0        0        0     3037 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/last_digit.png
+-rw-r--r--   0        0        0    13032 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/lda_model_graph.png
+-rw-r--r--   0        0        0    16452 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/logo_APHP.png
+-rw-r--r--   0        0        0    30396 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/logo_APHP_text.png
+-rw-r--r--   0        0        0     8047 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/microsoft-small.png
+-rw-r--r--   0        0        0    10320 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/microsoft.png
+-rw-r--r--   0        0        0   761071 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/ml_map.png
+-rw-r--r--   0        0        0    26546 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/multi_org_chart.png
+-rw-r--r--   0        0        0    89381 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/multilayerperceptron_network.png
+-rw-r--r--   0        0        0     4315 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/no_image.png
+-rw-r--r--   0        0        0     8070 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/nvidia-small.png
+-rw-r--r--   0        0        0    10764 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/nvidia.png
+-rw-r--r--   0        0        0     5485 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/nyu_short_color.png
+-rw-r--r--   0        0        0    45027 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/permuted_non_predictive_feature.png
+-rw-r--r--   0        0        0    43181 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/permuted_predictive_feature.png
+-rw-r--r--   0        0        0    31108 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/plot_digits_classification.png
+-rw-r--r--   0        0        0   124459 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/plot_face_recognition_1.png
+-rw-r--r--   0        0        0    86623 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/plot_face_recognition_2.png
+-rw-r--r--   0        0        0     6152 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/png-logo-inria-la-fondation.png
+-rw-r--r--   0        0        0    20223 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/probabl.png
+-rw-r--r--   0        0        0     8398 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/quansight-labs-small.png
+-rw-r--r--   0        0        0   124931 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/quansight-labs.png
+-rw-r--r--   0        0        0    15495 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/rbm_graph.png
+-rw-r--r--   0        0        0     8053 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/scikit-learn-logo-notext.png
+-rw-r--r--   0        0        0     5468 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/scikit-learn-logo-small.png
+-rw-r--r--   0        0        0    29042 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/sloan_banner.png
+-rw-r--r--   0        0        0     2236 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/sloan_logo-small.png
+-rw-r--r--   0        0        0    38356 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/sydney-primary.jpeg
+-rw-r--r--   0        0        0     1728 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/sydney-stacked-small.png
+-rw-r--r--   0        0        0    32077 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/target_encoder_cross_validation.svg
+-rw-r--r--   0        0        0     3779 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/telecom-small.png
+-rw-r--r--   0        0        0    35103 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/telecom.png
+-rw-r--r--   0        0        0    84599 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/images/visual-studio-build-tools-selection.png
+-rw-r--r--   0        0        0      730 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/includes/big_toc_css.rst
+-rw-r--r--   0        0        0     1035 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/includes/bigger_toc_css.rst
+-rw-r--r--   0        0        0     1082 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/inspection.rst
+-rw-r--r--   0        0        0    13781 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/install.rst
+-rw-r--r--   0        0        0      250 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/jupyter-lite.json
+-rw-r--r--   0        0        0       57 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/jupyter_lite_config.json
+-rw-r--r--   0        0        0    48838 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/1280px-scikit-learn-logo.png
+-rw-r--r--   0        0        0     4261 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/README.md
+-rw-r--r--   0        0        0      472 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/brand_colors/colorswatch_29ABE2_cyan.png
+-rw-r--r--   0        0        0      445 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/brand_colors/colorswatch_9B4600_brown.png
+-rw-r--r--   0        0        0      462 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/brand_colors/colorswatch_F7931E_orange.png
+-rw-r--r--   0        0        0   104582 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/brand_guidelines/scikitlearn_logo_clearspace_updated.png
+-rwxr-xr-x   0        0        0     2238 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/favicon.ico
+-rwxr-xr-x   0        0        0   120865 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/identity.pdf
+-rw-r--r--   0        0        0     8053 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-notext.png
+-rw-r--r--   0        0        0     5468 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-small.png
+-rw-r--r--   0        0        0     7069 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-thumb.png
+-rw-r--r--   0        0        0     7126 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-without-subtitle.svg
+-rw-r--r--   0        0        0    37902 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo.bmp
+-rw-r--r--   0        0        0    10879 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo.png
+-rw-r--r--   0        0        0     4699 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo.svg
+-rw-r--r--   0        0        0     4931 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/maintainers.rst
+-rw-r--r--   0        0        0      584 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/maintainers_emeritus.rst
+-rw-r--r--   0        0        0     3329 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/make.bat
+-rw-r--r--   0        0        0    15721 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/metadata_routing.rst
+-rw-r--r--   0        0        0    12785 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/model_persistence.rst
+-rw-r--r--   0        0        0      358 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/model_selection.rst
+-rw-r--r--   0        0        0     6434 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/array_api.rst
+-rw-r--r--   0        0        0    11714 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/biclustering.rst
+-rw-r--r--   0        0        0    15718 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/calibration.rst
+-rw-r--r--   0        0        0    43059 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/classes.rst
+-rw-r--r--   0        0        0     8109 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/classification_threshold.rst
+-rw-r--r--   0        0        0    95448 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/clustering.rst
+-rw-r--r--   0        0        0    24647 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/compose.rst
+-rw-r--r--   0        0        0    14886 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/covariance.rst
+-rw-r--r--   0        0        0     9006 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/cross_decomposition.rst
+-rw-r--r--   0        0        0    40997 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/cross_validation.rst
+-rw-r--r--   0        0        0    46874 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/decomposition.rst
+-rw-r--r--   0        0        0     7955 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/density.rst
+-rw-r--r--   0        0        0    72711 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/ensemble.rst
+-rw-r--r--   0        0        0    44474 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/feature_extraction.rst
+-rw-r--r--   0        0        0    14641 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/feature_selection.rst
+-rw-r--r--   0        0        0    24659 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/gaussian_process.rst
+-rw-r--r--   0        0        0    28954 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/glm_data/lasso_enet_coordinate_descent.png
+-rw-r--r--   0        0        0    63830 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/glm_data/poisson_gamma_tweedie_distributions.png
+-rw-r--r--   0        0        0    33777 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/grid_search.rst
+-rw-r--r--   0        0        0    15308 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/impute.rst
+-rw-r--r--   0        0        0     1402 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/isotonic.rst
+-rw-r--r--   0        0        0    13990 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/kernel_approximation.rst
+-rw-r--r--   0        0        0     3328 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/kernel_ridge.rst
+-rw-r--r--   0        0        0    12123 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/lda_qda.rst
+-rw-r--r--   0        0        0     8410 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/learning_curve.rst
+-rw-r--r--   0        0        0    80266 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/linear_model.rst
+-rw-r--r--   0        0        0    30280 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/manifold.rst
+-rw-r--r--   0        0        0     7729 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/metrics.rst
+-rw-r--r--   0        0        0    16434 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/mixture.rst
+-rw-r--r--   0        0        0   123824 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/model_evaluation.rst
+-rw-r--r--   0        0        0    26792 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/multiclass.rst
+-rw-r--r--   0        0        0    11560 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/naive_bayes.rst
+-rw-r--r--   0        0        0    38870 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/neighbors.rst
+-rw-r--r--   0        0        0    15444 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/neural_networks_supervised.rst
+-rw-r--r--   0        0        0     6412 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/neural_networks_unsupervised.rst
+-rw-r--r--   0        0        0    18956 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/outlier_detection.rst
+-rw-r--r--   0        0        0    13212 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/partial_dependence.rst
+-rw-r--r--   0        0        0    10589 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/permutation_importance.rst
+-rw-r--r--   0        0        0      218 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/pipeline.rst
+-rw-r--r--   0        0        0    54508 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/preprocessing.rst
+-rw-r--r--   0        0        0     3522 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/preprocessing_targets.rst
+-rw-r--r--   0        0        0     7917 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/random_projection.rst
+-rw-r--r--   0        0        0     6689 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/semi_supervised.rst
+-rw-r--r--   0        0        0    24946 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/sgd.rst
+-rw-r--r--   0        0        0    34900 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/svm.rst
+-rw-r--r--   0        0        0    27542 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/tree.rst
+-rw-r--r--   0        0        0     1962 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/modules/unsupervised_reduction.rst
+-rw-r--r--   0        0        0      540 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/preface.rst
+-rw-r--r--   0        0        0     3300 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/presentations.rst
+-rw-r--r--   0        0        0    17855 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/related_projects.rst
+-rw-r--r--   0        0        0    11936 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/roadmap.rst
+-rw-r--r--   0        0        0       43 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/sphinxext/MANIFEST.in
+-rw-r--r--   0        0        0     6024 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/sphinxext/add_toctree_functions.py
+-rwxr-xr-x   0        0        0     1908 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/sphinxext/allow_nan_estimators.py
+-rw-r--r--   0        0        0     1666 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/sphinxext/doi_role.py
+-rw-r--r--   0        0        0     2645 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/sphinxext/github_link.py
+-rw-r--r--   0        0        0     8117 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/sphinxext/sphinx_issues.py
+-rw-r--r--   0        0        0      634 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/supervised_learning.rst
+-rw-r--r--   0        0        0     3939 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/support.rst
+-rw-r--r--   0        0        0      424 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/class.rst
+-rw-r--r--   0        0        0      495 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/class_with_call.rst
+-rw-r--r--   0        0        0      558 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/deprecated_class.rst
+-rw-r--r--   0        0        0      587 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/deprecated_class_with_call.rst
+-rw-r--r--   0        0        0      488 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/deprecated_class_without_init.rst
+-rw-r--r--   0        0        0      497 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/deprecated_function.rst
+-rw-r--r--   0        0        0      546 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/display_all_class_methods.rst
+-rw-r--r--   0        0        0      484 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/display_only_from_estimator.rst
+-rw-r--r--   0        0        0      433 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/function.rst
+-rwxr-xr-x   0        0        0      155 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/generate_deprecated.sh
+-rw-r--r--   0        0        0    17469 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/index.html
+-rw-r--r--   0        0        0      214 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/numpydoc_docstring.rst
+-rw-r--r--   0        0        0      589 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/templates/redirects.html
+-rw-r--r--   0        0        0      237 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/README.txt
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/Makefile
+-rw-r--r--   0        0        0    41412 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/aweber.png
+-rw-r--r--   0        0        0     3321 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/bestofmedia-logo.png
+-rw-r--r--   0        0        0     4891 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/betaworks.png
+-rw-r--r--   0        0        0    14595 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/birchbox.jpg
+-rw-r--r--   0        0        0    65058 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/bnp_paribas_cardif.png
+-rw-r--r--   0        0        0     5937 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/booking.png
+-rw-r--r--   0        0        0     3294 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/change-logo.png
+-rw-r--r--   0        0        0    10684 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/dataiku_logo.png
+-rw-r--r--   0        0        0     5177 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/datapublica.png
+-rw-r--r--   0        0        0    19895 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/datarobot.png
+-rw-r--r--   0        0        0     2629 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/evernote.png
+-rw-r--r--   0        0        0    24772 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/howaboutwe.png
+-rw-r--r--   0        0        0    31051 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/huggingface.png
+-rw-r--r--   0        0        0    85087 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/infonea.jpg
+-rw-r--r--   0        0        0    23903 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/inria.png
+-rw-r--r--   0        0        0     8359 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/jpmorgan.png
+-rw-r--r--   0        0        0     3307 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/lovely.png
+-rw-r--r--   0        0        0    12363 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/machinalis.png
+-rw-r--r--   0        0        0    47018 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/mars.png
+-rw-r--r--   0        0        0    10246 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/okcupid.png
+-rw-r--r--   0        0        0     8603 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/ottogroup_logo.png
+-rw-r--r--   0        0        0     4689 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/peerindex.png
+-rw-r--r--   0        0        0     2571 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/phimeca.png
+-rw-r--r--   0        0        0    11944 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/rangespan.png
+-rw-r--r--   0        0        0     6569 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/solido_logo.png
+-rw-r--r--   0        0        0    12293 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/spotify.png
+-rw-r--r--   0        0        0    11473 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/telecomparistech.jpg
+-rw-r--r--   0        0        0     6350 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/yhat.png
+-rw-r--r--   0        0        0    22810 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/images/zopa.png
+-rw-r--r--   0        0        0    30885 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/testimonials/testimonials.rst
+-rw-r--r--   0        0        0     2055 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/javascript.html
+-rw-r--r--   0        0        0     6741 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/layout.html
+-rw-r--r--   0        0        0     4311 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/nav.html
+-rw-r--r--   0        0        0      437 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/search.html
+-rw-r--r--   0        0        0    27008 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/css/theme.css
+-rw-r--r--   0        0        0   155712 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/css/vendor/bootstrap.min.css
+-rw-r--r--   0        0        0     1965 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/js/details-permalink.js
+-rw-r--r--   0        0        0    58030 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/js/vendor/bootstrap.min.js
+-rw-r--r--   0        0        0    72818 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/js/vendor/jquery-3.6.3.slim.min.js
+-rw-r--r--   0        0        0      189 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/theme.conf
+-rw-r--r--   0        0        0     3678 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tune_toc.rst
+-rw-r--r--   0        0        0    14077 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/basic/tutorial.rst
+-rw-r--r--   0        0        0      162 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/common_includes/info.txt
+-rw-r--r--   0        0        0      783 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/index.rst
+-rw-r--r--   0        0        0     3305 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/ML_MAPS_README.txt
+-rw-r--r--   0        0        0     9615 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/index.rst
+-rw-r--r--   0        0        0     7398 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/parse_path.py
+-rw-r--r--   0        0        0   230678 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/pyparsing.py
+-rw-r--r--   0        0        0     3612 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/svg2imagemap.py
+-rw-r--r--   0        0        0     1360 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/index.rst
+-rw-r--r--   0        0        0     9892 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/model_selection.rst
+-rw-r--r--   0        0        0     1766 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/putting_together.rst
+-rw-r--r--   0        0        0     3104 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/settings.rst
+-rw-r--r--   0        0        0    18658 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/supervised_learning.rst
+-rw-r--r--   0        0        0    10832 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/unsupervised_learning.rst
+-rw-r--r--   0        0        0     3820 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/data/languages/fetch_data.py
+-rw-r--r--   0        0        0      989 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py
+-rw-r--r--   0        0        0     2013 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py
+-rw-r--r--   0        0        0     2410 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py
+-rw-r--r--   0        0        0     2260 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py
+-rw-r--r--   0        0        0     3140 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py
+-rw-r--r--   0        0        0      988 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/solutions/generate_skeletons.py
+-rw-r--r--   0        0        0    21039 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/working_with_text_data.rst
+-rw-r--r--   0        0        0      431 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/unsupervised_learning.rst
+-rw-r--r--   0        0        0      626 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/user_guide.rst
+-rw-r--r--   0        0        0     3175 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/visualizations.rst
+-rw-r--r--   0        0        0     5209 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/_contributors.rst
+-rw-r--r--   0        0        0      537 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/changelog_legend.inc
+-rw-r--r--   0        0        0    44662 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/older_versions.rst
+-rw-r--r--   0        0        0    14172 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.13.rst
+-rw-r--r--   0        0        0    14475 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.14.rst
+-rw-r--r--   0        0        0    21266 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.15.rst
+-rw-r--r--   0        0        0    23270 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.16.rst
+-rw-r--r--   0        0        0    22571 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.17.rst
+-rw-r--r--   0        0        0    36439 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.18.rst
+-rw-r--r--   0        0        0    48325 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.19.rst
+-rw-r--r--   0        0        0    79828 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.20.rst
+-rw-r--r--   0        0        0    50044 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.21.rst
+-rw-r--r--   0        0        0    52900 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.22.rst
+-rw-r--r--   0        0        0    38648 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.23.rst
+-rw-r--r--   0        0        0    46490 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v0.24.rst
+-rw-r--r--   0        0        0    57815 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v1.0.rst
+-rw-r--r--   0        0        0    65715 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v1.1.rst
+-rw-r--r--   0        0        0    48895 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v1.2.rst
+-rw-r--r--   0        0        0    45212 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v1.3.rst
+-rw-r--r--   0        0        0    47878 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v1.4.rst
+-rw-r--r--   0        0        0    23374 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new/v1.5.rst
+-rw-r--r--   0        0        0      818 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/doc/whats_new.rst
+-rw-r--r--   0        0        0       41 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/README.txt
+-rw-r--r--   0        0        0      201 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/README.txt
+-rw-r--r--   0        0        0    31775 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_cyclical_feature_engineering.py
+-rw-r--r--   0        0        0     5242 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_digits_denoising.py
+-rw-r--r--   0        0        0     4771 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_face_recognition.py
+-rw-r--r--   0        0        0    10669 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_model_complexity_influence.py
+-rw-r--r--   0        0        0    13565 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_out_of_core_classification.py
+-rw-r--r--   0        0        0     4895 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_outlier_detection_wine.py
+-rw-r--r--   0        0        0    11140 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_prediction_latency.py
+-rw-r--r--   0        0        0     7853 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_species_distribution_modeling.py
+-rw-r--r--   0        0        0     8228 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_stock_market.py
+-rw-r--r--   0        0        0    16221 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_time_series_lagged_features.py
+-rw-r--r--   0        0        0     5369 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_tomography_l1_reconstruction.py
+-rw-r--r--   0        0        0     6756 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/plot_topics_extraction_with_nmf_lda.py
+-rw-r--r--   0        0        0     7730 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/applications/wikipedia_principal_eigenvector.py
+-rw-r--r--   0        0        0       97 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/bicluster/README.txt
+-rw-r--r--   0        0        0     5625 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/bicluster/plot_bicluster_newsgroups.py
+-rw-r--r--   0        0        0     4677 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/bicluster/plot_spectral_biclustering.py
+-rw-r--r--   0        0        0     1734 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/bicluster/plot_spectral_coclustering.py
+-rw-r--r--   0        0        0      145 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/calibration/README.txt
+-rw-r--r--   0        0        0     4894 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/calibration/plot_calibration.py
+-rw-r--r--   0        0        0    11595 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/calibration/plot_calibration_curve.py
+-rw-r--r--   0        0        0     9567 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/calibration/plot_calibration_multiclass.py
+-rw-r--r--   0        0        0    12157 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/calibration/plot_compare_calibration.py
+-rw-r--r--   0        0        0      120 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/classification/README.txt
+-rw-r--r--   0        0        0     3294 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/classification/plot_classification_probability.py
+-rw-r--r--   0        0        0     5017 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/classification/plot_classifier_comparison.py
+-rw-r--r--   0        0        0     4570 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/classification/plot_digits_classification.py
+-rw-r--r--   0        0        0     3055 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/classification/plot_lda.py
+-rw-r--r--   0        0        0     7779 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/classification/plot_lda_qda.py
+-rw-r--r--   0        0        0      101 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/README.txt
+-rw-r--r--   0        0        0     8745 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_adjusted_for_chance_measures.py
+-rw-r--r--   0        0        0     2181 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_affinity_propagation.py
+-rw-r--r--   0        0        0     3115 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_agglomerative_clustering.py
+-rw-r--r--   0        0        0     4773 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_agglomerative_clustering_metrics.py
+-rw-r--r--   0        0        0     1709 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_agglomerative_dendrogram.py
+-rw-r--r--   0        0        0     3857 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_birch_vs_minibatchkmeans.py
+-rw-r--r--   0        0        0     1997 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_bisect_kmeans.py
+-rw-r--r--   0        0        0     8482 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_cluster_comparison.py
+-rw-r--r--   0        0        0     2523 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_cluster_iris.py
+-rw-r--r--   0        0        0     3956 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_coin_segmentation.py
+-rw-r--r--   0        0        0     2376 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_coin_ward_segmentation.py
+-rw-r--r--   0        0        0     3083 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_color_quantization.py
+-rw-r--r--   0        0        0     4001 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_dbscan.py
+-rw-r--r--   0        0        0     2644 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_dict_face_patches.py
+-rw-r--r--   0        0        0     1603 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_digits_agglomeration.py
+-rw-r--r--   0        0        0     2740 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_digits_linkage.py
+-rw-r--r--   0        0        0     7145 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_face_compress.py
+-rw-r--r--   0        0        0     3587 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py
+-rw-r--r--   0        0        0     9805 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_hdbscan.py
+-rw-r--r--   0        0        0     3926 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_inductive_clustering.py
+-rw-r--r--   0        0        0     6654 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_assumptions.py
+-rw-r--r--   0        0        0     6811 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_digits.py
+-rw-r--r--   0        0        0     1169 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_plusplus.py
+-rw-r--r--   0        0        0     5898 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_silhouette_analysis.py
+-rw-r--r--   0        0        0     4337 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_stability_low_dim_dense.py
+-rw-r--r--   0        0        0     5166 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_linkage_comparison.py
+-rw-r--r--   0        0        0     1665 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_mean_shift.py
+-rw-r--r--   0        0        0     4026 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_mini_batch_kmeans.py
+-rw-r--r--   0        0        0     3514 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_optics.py
+-rw-r--r--   0        0        0     3806 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_segmentation_toy.py
+-rw-r--r--   0        0        0     3661 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cluster/plot_ward_structured_vs_unstructured.py
+-rw-r--r--   0        0        0      221 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/README.txt
+-rw-r--r--   0        0        0     6753 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/plot_column_transformer.py
+-rw-r--r--   0        0        0     7779 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/plot_column_transformer_mixed_types.py
+-rw-r--r--   0        0        0     4498 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/plot_compare_reduction.py
+-rw-r--r--   0        0        0     2613 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/plot_digits_pipe.py
+-rw-r--r--   0        0        0     1951 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/plot_feature_union.py
+-rw-r--r--   0        0        0     7671 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/compose/plot_transformed_target.py
+-rw-r--r--   0        0        0      129 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/covariance/README.txt
+-rw-r--r--   0        0        0     5026 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/covariance/plot_covariance_estimation.py
+-rw-r--r--   0        0        0     2906 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/covariance/plot_lw_vs_oas.py
+-rw-r--r--   0        0        0     8171 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/covariance/plot_mahalanobis_distances.py
+-rw-r--r--   0        0        0     6489 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/covariance/plot_robust_vs_empirical_covariance.py
+-rw-r--r--   0        0        0     5003 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/covariance/plot_sparse_cov.py
+-rw-r--r--   0        0        0      144 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cross_decomposition/README.txt
+-rw-r--r--   0        0        0     4806 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cross_decomposition/plot_compare_cross_decomposition.py
+-rw-r--r--   0        0        0     6926 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/cross_decomposition/plot_pcr_vs_pls.py
+-rw-r--r--   0        0        0      121 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/datasets/README.txt
+-rw-r--r--   0        0        0      876 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/datasets/plot_digits_last_image.py
+-rw-r--r--   0        0        0     2708 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/datasets/plot_iris_dataset.py
+-rw-r--r--   0        0        0     2322 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/datasets/plot_random_dataset.py
+-rw-r--r--   0        0        0     3180 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/datasets/plot_random_multilabel_dataset.py
+-rw-r--r--   0        0        0      120 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/README.txt
+-rw-r--r--   0        0        0    10538 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_faces_decomposition.py
+-rw-r--r--   0        0        0     2142 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_ica_blind_source_separation.py
+-rw-r--r--   0        0        0     3242 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_ica_vs_pca.py
+-rw-r--r--   0        0        0     6144 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_image_denoising.py
+-rw-r--r--   0        0        0     1989 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_incremental_pca.py
+-rw-r--r--   0        0        0     6843 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_kernel_pca.py
+-rw-r--r--   0        0        0     1498 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_pca_iris.py
+-rw-r--r--   0        0        0     4499 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_pca_vs_fa_model_selection.py
+-rw-r--r--   0        0        0     2015 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_pca_vs_lda.py
+-rw-r--r--   0        0        0     4027 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_sparse_coding.py
+-rw-r--r--   0        0        0     2327 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/decomposition/plot_varimax_fa.py
+-rw-r--r--   0        0        0      137 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/developing_estimators/README.txt
+-rw-r--r--   0        0        0     2597 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/developing_estimators/sklearn_is_fitted.py
+-rw-r--r--   0        0        0      115 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/README.txt
+-rw-r--r--   0        0        0    10269 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_adaboost_multiclass.py
+-rw-r--r--   0        0        0     2628 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_adaboost_regression.py
+-rw-r--r--   0        0        0     3099 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_adaboost_twoclass.py
+-rw-r--r--   0        0        0     7229 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_bias_variance.py
+-rw-r--r--   0        0        0     3239 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_ensemble_oob.py
+-rw-r--r--   0        0        0     5245 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_feature_transformation.py
+-rw-r--r--   0        0        0     8932 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py
+-rw-r--r--   0        0        0     4139 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_importances.py
+-rw-r--r--   0        0        0     3040 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_importances_faces.py
+-rw-r--r--   0        0        0     6335 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_iris.py
+-rw-r--r--   0        0        0    10292 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_categorical.py
+-rw-r--r--   0        0        0     6624 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_early_stopping.py
+-rw-r--r--   0        0        0     4959 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_oob.py
+-rw-r--r--   0        0        0    12519 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_quantile.py
+-rw-r--r--   0        0        0     5180 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_regression.py
+-rw-r--r--   0        0        0     2696 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_regularization.py
+-rw-r--r--   0        0        0    16714 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_hgbt_regression.py
+-rw-r--r--   0        0        0     4254 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_isolation_forest.py
+-rw-r--r--   0        0        0     3235 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_monotonic_constraints.py
+-rw-r--r--   0        0        0     3659 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_random_forest_embedding.py
+-rw-r--r--   0        0        0     2610 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_random_forest_regression_multioutput.py
+-rw-r--r--   0        0        0     8061 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_stack_predictors.py
+-rw-r--r--   0        0        0     2278 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_voting_decision_regions.py
+-rw-r--r--   0        0        0     2982 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_voting_probas.py
+-rw-r--r--   0        0        0     2653 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/ensemble/plot_voting_regressor.py
+-rw-r--r--   0        0        0       67 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/exercises/README.txt
+-rw-r--r--   0        0        0     2887 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/exercises/plot_cv_diabetes.py
+-rw-r--r--   0        0        0      954 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/exercises/plot_digits_classification_exercise.py
+-rw-r--r--   0        0        0     1691 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/exercises/plot_iris_exercise.py
+-rw-r--r--   0        0        0      141 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/README.txt
+-rw-r--r--   0        0        0     1642 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/plot_f_test_vs_mi.py
+-rw-r--r--   0        0        0     3778 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/plot_feature_selection.py
+-rw-r--r--   0        0        0     2757 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/plot_feature_selection_pipeline.py
+-rw-r--r--   0        0        0     1774 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/plot_rfe_digits.py
+-rw-r--r--   0        0        0     2798 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/plot_rfe_with_cross_validation.py
+-rw-r--r--   0        0        0     7484 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/feature_selection/plot_select_from_model_diabetes.py
+-rw-r--r--   0        0        0      174 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/README.txt
+-rw-r--r--   0        0        0    13329 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_compare_gpr_krr.py
+-rw-r--r--   0        0        0     3997 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc.py
+-rw-r--r--   0        0        0     2220 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc_iris.py
+-rw-r--r--   0        0        0     2925 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc_isoprobability.py
+-rw-r--r--   0        0        0     2106 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc_xor.py
+-rw-r--r--   0        0        0     8841 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_co2.py
+-rw-r--r--   0        0        0     6582 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_noisy.py
+-rw-r--r--   0        0        0     5306 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_noisy_targets.py
+-rw-r--r--   0        0        0     5789 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_on_structured_data.py
+-rw-r--r--   0        0        0     8449 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_prior_posterior.py
+-rw-r--r--   0        0        0      127 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/impute/README.txt
+-rw-r--r--   0        0        0     5881 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/impute/plot_iterative_imputer_variants_comparison.py
+-rw-r--r--   0        0        0     9233 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/impute/plot_missing_values.py
+-rw-r--r--   0        0        0      108 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/inspection/README.txt
+-rw-r--r--   0        0        0     7219 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/inspection/plot_causal_interpretation.py
+-rw-r--r--   0        0        0    28270 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/inspection/plot_linear_model_coefficient_interpretation.py
+-rw-r--r--   0        0        0    21224 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/inspection/plot_partial_dependence.py
+-rw-r--r--   0        0        0     9468 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/inspection/plot_permutation_importance.py
+-rw-r--r--   0        0        0     7336 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/inspection/plot_permutation_importance_multicollinear.py
+-rw-r--r--   0        0        0      147 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/kernel_approximation/README.txt
+-rw-r--r--   0        0        0     7703 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/kernel_approximation/plot_scalable_poly_kernels.py
+-rw-r--r--   0        0        0      135 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/README.txt
+-rw-r--r--   0        0        0     7101 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ard.py
+-rw-r--r--   0        0        0     3091 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_bayesian_ridge_curvefit.py
+-rw-r--r--   0        0        0     2057 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py
+-rw-r--r--   0        0        0     2078 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_huber_vs_ridge.py
+-rw-r--r--   0        0        0     1383 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_iris_logistic.py
+-rw-r--r--   0        0        0     9399 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_and_elasticnet.py
+-rw-r--r--   0        0        0     2768 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_coordinate_descent_path.py
+-rw-r--r--   0        0        0     2826 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_dense_vs_sparse_data.py
+-rw-r--r--   0        0        0      992 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_lars.py
+-rw-r--r--   0        0        0     3881 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_lars_ic.py
+-rw-r--r--   0        0        0     8908 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_model_selection.py
+-rw-r--r--   0        0        0     1576 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic.py
+-rw-r--r--   0        0        0     3155 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic_l1_l2_sparsity.py
+-rw-r--r--   0        0        0     2370 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic_multinomial.py
+-rw-r--r--   0        0        0     2160 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic_path.py
+-rw-r--r--   0        0        0     2310 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_multi_task_lasso_support.py
+-rw-r--r--   0        0        0     2007 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_nnls.py
+-rw-r--r--   0        0        0     2025 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ols.py
+-rw-r--r--   0        0        0     2068 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ols_3d.py
+-rw-r--r--   0        0        0     1901 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ols_ridge_variance.py
+-rw-r--r--   0        0        0     1895 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_omp.py
+-rw-r--r--   0        0        0    22593 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_poisson_regression_non_normal_loss.py
+-rw-r--r--   0        0        0     7828 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_polynomial_interpolation.py
+-rw-r--r--   0        0        0    11591 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_quantile_regression.py
+-rw-r--r--   0        0        0     2074 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ransac.py
+-rw-r--r--   0        0        0     8758 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ridge_coeffs.py
+-rw-r--r--   0        0        0     2000 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_ridge_path.py
+-rw-r--r--   0        0        0     3098 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_robust_fit.py
+-rw-r--r--   0        0        0     1898 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_comparison.py
+-rw-r--r--   0        0        0     5741 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_early_stopping.py
+-rw-r--r--   0        0        0     1948 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_iris.py
+-rw-r--r--   0        0        0     1218 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_loss_functions.py
+-rw-r--r--   0        0        0     1348 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_penalties.py
+-rw-r--r--   0        0        0     1198 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_separating_hyperplane.py
+-rw-r--r--   0        0        0     1588 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_weighted_samples.py
+-rw-r--r--   0        0        0     6082 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py
+-rw-r--r--   0        0        0     4209 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py
+-rw-r--r--   0        0        0     2675 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_sparse_logistic_regression_mnist.py
+-rw-r--r--   0        0        0     3777 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_theilsen.py
+-rw-r--r--   0        0        0    23598 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/linear_model/plot_tweedie_regression_insurance_claims.py
+-rw-r--r--   0        0        0      124 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/README.txt
+-rw-r--r--   0        0        0     6754 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/plot_compare_methods.py
+-rw-r--r--   0        0        0     6473 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/plot_lle_digits.py
+-rw-r--r--   0        0        0     5107 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/plot_manifold_sphere.py
+-rw-r--r--   0        0        0     2698 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/plot_mds.py
+-rw-r--r--   0        0        0     4022 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/plot_swissroll.py
+-rw-r--r--   0        0        0     4271 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/manifold/plot_t_sne_perplexity.py
+-rw-r--r--   0        0        0      117 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/README.txt
+-rw-r--r--   0        0        0     7505 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_anomaly_comparison.py
+-rw-r--r--   0        0        0     3541 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_display_object_visualization.py
+-rw-r--r--   0        0        0     1630 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_estimator_representation.py
+-rw-r--r--   0        0        0     2615 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_isotonic_regression.py
+-rw-r--r--   0        0        0     7441 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py
+-rw-r--r--   0        0        0     8646 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_kernel_approximation.py
+-rw-r--r--   0        0        0     6648 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_kernel_ridge_regression.py
+-rw-r--r--   0        0        0    27970 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_metadata_routing.py
+-rw-r--r--   0        0        0     4072 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_multilabel.py
+-rw-r--r--   0        0        0     2805 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_multioutput_face_completion.py
+-rw-r--r--   0        0        0    16899 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_outlier_detection_bench.py
+-rw-r--r--   0        0        0     5363 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_partial_dependence_visualization_api.py
+-rwxr-xr-x   0        0        0     6200 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_pipeline_display.py
+-rw-r--r--   0        0        0     2090 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_roc_curve_visualization_api.py
+-rw-r--r--   0        0        0     4597 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/miscellaneous/plot_set_output.py
+-rw-r--r--   0        0        0      127 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/README.txt
+-rw-r--r--   0        0        0     5883 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_concentration_prior.py
+-rw-r--r--   0        0        0     3203 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_gmm.py
+-rw-r--r--   0        0        0     4673 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_covariances.py
+-rw-r--r--   0        0        0     3701 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_init.py
+-rw-r--r--   0        0        0     1519 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_pdf.py
+-rw-r--r--   0        0        0     5487 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_selection.py
+-rw-r--r--   0        0        0     6082 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_sin.py
+-rw-r--r--   0        0        0      135 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/README.txt
+-rw-r--r--   0        0        0     2084 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_confusion_matrix.py
+-rw-r--r--   0        0        0    30171 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_cost_sensitive_learning.py
+-rw-r--r--   0        0        0     6095 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_cv_indices.py
+-rw-r--r--   0        0        0     2551 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_cv_predict.py
+-rw-r--r--   0        0        0     3977 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_det.py
+-rw-r--r--   0        0        0     7322 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_digits.py
+-rw-r--r--   0        0        0     3631 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_refit_callable.py
+-rw-r--r--   0        0        0    23047 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_stats.py
+-rw-r--r--   0        0        0     9262 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_text_feature_extraction.py
+-rw-r--r--   0        0        0     6830 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_learning_curve.py
+-rw-r--r--   0        0        0    12013 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_likelihood_ratios.py
+-rw-r--r--   0        0        0     3692 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_multi_metric_evaluation.py
+-rw-r--r--   0        0        0     4476 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_nested_cross_validation_iris.py
+-rw-r--r--   0        0        0     5049 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_permutation_tests_for_classification.py
+-rw-r--r--   0        0        0    10197 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_precision_recall.py
+-rw-r--r--   0        0        0     3003 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_randomized_search.py
+-rw-r--r--   0        0        0    14496 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_roc.py
+-rw-r--r--   0        0        0     4234 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_roc_crossval.py
+-rw-r--r--   0        0        0     4211 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_successive_halving_heatmap.py
+-rw-r--r--   0        0        0     2764 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_successive_halving_iterations.py
+-rw-r--r--   0        0        0     2506 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_train_error_vs_test_error.py
+-rw-r--r--   0        0        0     7497 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_tuned_decision_threshold.py
+-rw-r--r--   0        0        0     2680 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_underfitting_overfitting.py
+-rw-r--r--   0        0        0     1291 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/model_selection/plot_validation_curve.py
+-rw-r--r--   0        0        0      123 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/multiclass/README.txt
+-rw-r--r--   0        0        0     8202 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/multiclass/plot_multiclass_overview.py
+-rw-r--r--   0        0        0      127 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/multioutput/README.txt
+-rw-r--r--   0        0        0     5990 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/multioutput/plot_classifier_chain_yeast.py
+-rw-r--r--   0        0        0      125 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/README.txt
+-rw-r--r--   0        0        0    11310 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/approximate_nearest_neighbors.py
+-rw-r--r--   0        0        0     2683 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_caching_nearest_neighbors.py
+-rw-r--r--   0        0        0     3142 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_classification.py
+-rw-r--r--   0        0        0     1990 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_digits_kde_sampling.py
+-rw-r--r--   0        0        0     5224 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_kde_1d.py
+-rw-r--r--   0        0        0     3616 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_lof_novelty_detection.py
+-rw-r--r--   0        0        0     3097 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_lof_outlier_detection.py
+-rw-r--r--   0        0        0     2774 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_nca_classification.py
+-rw-r--r--   0        0        0     3561 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_nca_dim_reduction.py
+-rw-r--r--   0        0        0     3001 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_nca_illustration.py
+-rw-r--r--   0        0        0     1398 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_nearest_centroid.py
+-rw-r--r--   0        0        0     1259 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_regression.py
+-rw-r--r--   0        0        0     4757 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neighbors/plot_species_kde.py
+-rw-r--r--   0        0        0      133 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neural_networks/README.txt
+-rw-r--r--   0        0        0     4600 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neural_networks/plot_mlp_alpha.py
+-rw-r--r--   0        0        0     4067 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neural_networks/plot_mlp_training_curves.py
+-rw-r--r--   0        0        0     2699 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neural_networks/plot_mnist_filters.py
+-rw-r--r--   0        0        0     4267 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/neural_networks/plot_rbm_logistic_classification.py
+-rw-r--r--   0        0        0      119 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/README.txt
+-rw-r--r--   0        0        0    14781 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_all_scaling.py
+-rw-r--r--   0        0        0     3375 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_discretization.py
+-rw-r--r--   0        0        0     7694 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_discretization_classification.py
+-rw-r--r--   0        0        0     3070 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_discretization_strategies.py
+-rw-r--r--   0        0        0     4631 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_map_data_to_normal.py
+-rw-r--r--   0        0        0     9839 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_scaling_importance.py
+-rw-r--r--   0        0        0     8095 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_target_encoder.py
+-rw-r--r--   0        0        0     7185 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/preprocessing/plot_target_encoder_cross_val.py
+-rw-r--r--   0        0        0      150 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/README.txt
+-rw-r--r--   0        0        0    10367 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_0_22_0.py
+-rw-r--r--   0        0        0     7806 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_0_23_0.py
+-rw-r--r--   0        0        0    11514 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_0_24_0.py
+-rw-r--r--   0        0        0    10262 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_0_0.py
+-rw-r--r--   0        0        0     8761 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_1_0.py
+-rw-r--r--   0        0        0     6285 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_2_0.py
+-rw-r--r--   0        0        0     6367 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_3_0.py
+-rw-r--r--   0        0        0     7884 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_4_0.py
+-rw-r--r--   0        0        0      157 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/README.txt
+-rw-r--r--   0        0        0     3174 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/plot_label_propagation_digits.py
+-rw-r--r--   0        0        0     4203 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/plot_label_propagation_digits_active_learning.py
+-rw-r--r--   0        0        0     2711 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/plot_label_propagation_structure.py
+-rw-r--r--   0        0        0     4007 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/plot_self_training_varying_threshold.py
+-rw-r--r--   0        0        0     3618 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/plot_semi_supervised_newsgroups.py
+-rw-r--r--   0        0        0     2955 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/semi_supervised/plot_semi_supervised_versus_svm_iris.py
+-rw-r--r--   0        0        0      119 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/README.txt
+-rw-r--r--   0        0        0     1302 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_custom_kernel.py
+-rw-r--r--   0        0        0     2837 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_iris_svc.py
+-rw-r--r--   0        0        0     1806 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_linearsvc_support_vectors.py
+-rw-r--r--   0        0        0     2804 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_oneclass.py
+-rw-r--r--   0        0        0     8313 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_rbf_parameters.py
+-rw-r--r--   0        0        0     1114 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_separating_hyperplane.py
+-rw-r--r--   0        0        0     2328 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_separating_hyperplane_unbalanced.py
+-rw-r--r--   0        0        0     2096 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_anova.py
+-rw-r--r--   0        0        0    11619 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_kernels.py
+-rw-r--r--   0        0        0     2546 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_margin.py
+-rw-r--r--   0        0        0     1072 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_nonlinear.py
+-rw-r--r--   0        0        0     2048 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_regression.py
+-rw-r--r--   0        0        0     7543 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_scale_c.py
+-rw-r--r--   0        0        0     2165 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_svm_tie_breaking.py
+-rw-r--r--   0        0        0     2048 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/svm/plot_weighted_samples.py
+-rw-r--r--   0        0        0      149 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/text/README.txt
+-rw-r--r--   0        0        0    16629 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/text/plot_document_classification_20newsgroups.py
+-rw-r--r--   0        0        0    17653 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/text/plot_document_clustering.py
+-rw-r--r--   0        0        0    15130 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/text/plot_hashing_vs_dict_vectorizer.py
+-rw-r--r--   0        0        0      103 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/tree/README.txt
+-rw-r--r--   0        0        0     4603 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/tree/plot_cost_complexity_pruning.py
+-rw-r--r--   0        0        0     2515 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/tree/plot_iris_dtc.py
+-rw-r--r--   0        0        0     1527 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/tree/plot_tree_regression.py
+-rw-r--r--   0        0        0     1959 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/tree/plot_tree_regression_multioutput.py
+-rw-r--r--   0        0        0     8870 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/examples/tree/plot_unveil_tree_structure.py
+-rw-r--r--   0        0        0     1611 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/meson.build
+-rw-r--r--   0        0        0     5999 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/pyproject.toml
+-rw-r--r--   0        0        0     2262 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/setup.cfg
+-rwxr-xr-x   0        0        0    22350 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/setup.py
+-rw-r--r--   0        0        0     1702 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/__check_build/__init__.py
+-rw-r--r--   0        0        0       30 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/__check_build/_check_build.pyx
+-rw-r--r--   0        0        0      143 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/__check_build/meson.build
+-rw-r--r--   0        0        0     5771 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/__init__.py
+-rw-r--r--   0        0        0     3611 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_build_utils/__init__.py
+-rw-r--r--   0        0        0     4563 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_build_utils/openmp_helpers.py
+-rw-r--r--   0        0        0     2177 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_build_utils/pre_build_helpers.py
+-rw-r--r--   0        0        0     1580 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_build_utils/tempita.py
+-rw-r--r--   0        0        0      367 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_build_utils/version.py
+-rw-r--r--   0        0        0    13493 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_config.py
+-rw-r--r--   0        0        0      344 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_distributor_init.py
+-rw-r--r--   0        0        0     3708 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_isotonic.pyx
+-rw-r--r--   0        0        0      607 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/__init__.py
+-rw-r--r--   0        0        0     4315 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/_loss.pxd
+-rw-r--r--   0        0        0    50165 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/_loss.pyx.tp
+-rw-r--r--   0        0        0     8102 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/link.py
+-rw-r--r--   0        0        0    41237 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/loss.py
+-rw-r--r--   0        0        0      411 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/tests/__init__.py
+-rw-r--r--   0        0        0     3954 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/tests/test_link.py
+-rw-r--r--   0        0        0    48390 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_loss/tests/test_loss.py
+-rw-r--r--   0        0        0     2528 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/_min_dependencies.py
+-rw-r--r--   0        0        0    53065 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/base.py
+-rw-r--r--   0        0        0    49490 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/calibration.py
+-rw-r--r--   0        0        0     1440 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/__init__.py
+-rw-r--r--   0        0        0    20512 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_affinity_propagation.py
+-rw-r--r--   0        0        0    49383 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_agglomerative.py
+-rw-r--r--   0        0        0    22236 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_bicluster.py
+-rw-r--r--   0        0        0    26249 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_birch.py
+-rw-r--r--   0        0        0    19041 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_bisect_k_means.py
+-rw-r--r--   0        0        0    18403 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_dbscan.py
+-rw-r--r--   0        0        0     1286 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_dbscan_inner.pyx
+-rw-r--r--   0        0        0     3097 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_feature_agglomeration.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/__init__.py
+-rw-r--r--   0        0        0    10097 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_linkage.pyx
+-rw-r--r--   0        0        0     7891 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_reachability.pyx
+-rw-r--r--   0        0        0     2150 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_tree.pxd
+-rw-r--r--   0        0        0    27800 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_tree.pyx
+-rw-r--r--   0        0        0    42370 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/hdbscan.py
+-rw-r--r--   0        0        0      462 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/tests/__init__.py
+-rw-r--r--   0        0        0     2064 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/tests/test_reachibility.py
+-rw-r--r--   0        0        0      245 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hierarchical_fast.pxd
+-rw-r--r--   0        0        0    15905 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_hierarchical_fast.pyx
+-rw-r--r--   0        0        0      887 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_common.pxd
+-rw-r--r--   0        0        0    10289 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_common.pyx
+-rw-r--r--   0        0        0    28135 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_elkan.pyx
+-rw-r--r--   0        0        0    16470 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_lloyd.pyx
+-rw-r--r--   0        0        0     8156 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_minibatch.pyx
+-rw-r--r--   0        0        0    81644 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_kmeans.py
+-rw-r--r--   0        0        0    20156 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_mean_shift.py
+-rwxr-xr-x   0        0        0    44803 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_optics.py
+-rw-r--r--   0        0        0    30614 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/_spectral.py
+-rw-r--r--   0        0        0      890 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/__init__.py
+-rw-r--r--   0        0        0      880 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/common.py
+-rw-r--r--   0        0        0    11898 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_affinity_propagation.py
+-rw-r--r--   0        0        0     9126 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_bicluster.py
+-rw-r--r--   0        0        0     8606 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_birch.py
+-rw-r--r--   0        0        0     5139 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_bisect_k_means.py
+-rw-r--r--   0        0        0    15704 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_dbscan.py
+-rw-r--r--   0        0        0     2754 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_feature_agglomeration.py
+-rw-r--r--   0        0        0    20177 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_hdbscan.py
+-rw-r--r--   0        0        0    32594 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_hierarchical.py
+-rw-r--r--   0        0        0    48450 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_k_means.py
+-rw-r--r--   0        0        0     6740 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_mean_shift.py
+-rw-r--r--   0        0        0    24103 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_optics.py
+-rw-r--r--   0        0        0    11904 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_spectral.py
+-rw-r--r--   0        0        0      497 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/compose/__init__.py
+-rw-r--r--   0        0        0    66062 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/compose/_column_transformer.py
+-rw-r--r--   0        0        0    12511 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/compose/_target.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/compose/tests/__init__.py
+-rw-r--r--   0        0        0    93517 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/compose/tests/test_column_transformer.py
+-rw-r--r--   0        0        0    13391 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/compose/tests/test_target.py
+-rw-r--r--   0        0        0    10719 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/conftest.py
+-rw-r--r--   0        0        0     1116 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/__init__.py
+-rw-r--r--   0        0        0     9073 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/_elliptic_envelope.py
+-rw-r--r--   0        0        0    12066 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/_empirical_covariance.py
+-rw-r--r--   0        0        0    39938 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/_graph_lasso.py
+-rw-r--r--   0        0        0    33902 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/_robust_covariance.py
+-rw-r--r--   0        0        0    27837 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/_shrunk_covariance.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/tests/__init__.py
+-rw-r--r--   0        0        0    14154 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_covariance.py
+-rw-r--r--   0        0        0     1587 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_elliptic_envelope.py
+-rw-r--r--   0        0        0    10953 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_graphical_lasso.py
+-rw-r--r--   0        0        0     6384 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_robust_covariance.py
+-rw-r--r--   0        0        0      121 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cross_decomposition/__init__.py
+-rw-r--r--   0        0        0    39364 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cross_decomposition/_pls.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cross_decomposition/tests/__init__.py
+-rw-r--r--   0        0        0    25437 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/cross_decomposition/tests/test_pls.py
+-rw-r--r--   0        0        0     5193 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/__init__.py
+-rw-r--r--   0        0        0    19080 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_arff_parser.py
+-rw-r--r--   0        0        0    47912 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_base.py
+-rw-r--r--   0        0        0     7255 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_california_housing.py
+-rw-r--r--   0        0        0     8104 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_covtype.py
+-rw-r--r--   0        0        0    13886 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_kddcup99.py
+-rw-r--r--   0        0        0    22397 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_lfw.py
+-rw-r--r--   0        0        0     6099 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_olivetti_faces.py
+-rw-r--r--   0        0        0    41430 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_openml.py
+-rw-r--r--   0        0        0    11834 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_rcv1.py
+-rw-r--r--   0        0        0    75834 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_samples_generator.py
+-rw-r--r--   0        0        0     9776 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_species_distributions.py
+-rw-r--r--   0        0        0     7269 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_svmlight_format_fast.pyx
+-rw-r--r--   0        0        0    21302 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_svmlight_format_io.py
+-rw-r--r--   0        0        0    20820 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/_twenty_newsgroups.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/__init__.py
+-rw-r--r--   0        0        0    34742 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/boston_house_prices.csv
+-rw-r--r--   0        0        0   119913 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/breast_cancer.csv
+-rw-r--r--   0        0        0     7105 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/diabetes_data_raw.csv.gz
+-rw-r--r--   0        0        0     1050 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/diabetes_target.csv.gz
+-rw-r--r--   0        0        0    57523 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/digits.csv.gz
+-rw-r--r--   0        0        0     2734 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/iris.csv
+-rw-r--r--   0        0        0      212 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/linnerud_exercise.csv
+-rw-r--r--   0        0        0      219 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/linnerud_physiological.csv
+-rw-r--r--   0        0        0    11157 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/data/wine_data.csv
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/__init__.py
+-rw-r--r--   0        0        0     4811 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/breast_cancer.rst
+-rw-r--r--   0        0        0     1727 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/california_housing.rst
+-rw-r--r--   0        0        0     1191 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/covtype.rst
+-rw-r--r--   0        0        0     1455 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/diabetes.rst
+-rw-r--r--   0        0        0     2024 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/digits.rst
+-rw-r--r--   0        0        0     2665 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/iris.rst
+-rw-r--r--   0        0        0     3950 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/kddcup99.rst
+-rw-r--r--   0        0        0     4305 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/lfw.rst
+-rw-r--r--   0        0        0      735 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/linnerud.rst
+-rw-r--r--   0        0        0     1834 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/olivetti_faces.rst
+-rw-r--r--   0        0        0     2466 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/rcv1.rst
+-rw-r--r--   0        0        0     1547 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/species_distributions.rst
+-rw-r--r--   0        0        0    10823 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/twenty_newsgroups.rst
+-rw-r--r--   0        0        0     3332 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/descr/wine_data.rst
+-rw-r--r--   0        0        0      712 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/images/README.txt
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/images/__init__.py
+-rw-r--r--   0        0        0   196653 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/images/china.jpg
+-rw-r--r--   0        0        0   142987 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/images/flower.jpg
+-rw-r--r--   0        0        0      181 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/__init__.py
+-rw-r--r--   0        0        0     1786 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/api-v1-jd-1.json.gz
+-rw-r--r--   0        0        0      889 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/api-v1-jdf-1.json.gz
+-rw-r--r--   0        0        0      145 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/api-v1-jdq-1.json.gz
+-rw-r--r--   0        0        0     1841 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/data-v1-dl-1.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/__init__.py
+-rw-r--r--   0        0        0      711 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jd-1119.json.gz
+-rw-r--r--   0        0        0     1108 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdf-1119.json.gz
+-rw-r--r--   0        0        0      364 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdl-dn-adult-census-l-2-dv-1.json.gz
+-rw-r--r--   0        0        0      363 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdl-dn-adult-census-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0     1549 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdq-1119.json.gz
+-rw-r--r--   0        0        0     1190 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/data-v1-dl-54002.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/__init__.py
+-rw-r--r--   0        0        0     1544 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/api-v1-jd-1590.json.gz
+-rw-r--r--   0        0        0     1032 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdf-1590.json.gz
+-rw-r--r--   0        0        0     1507 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdq-1590.json.gz
+-rw-r--r--   0        0        0     1152 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/data-v1-dl-1595261.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/__init__.py
+-rw-r--r--   0        0        0     1363 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jd-2.json.gz
+-rw-r--r--   0        0        0      866 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jdf-2.json.gz
+-rw-r--r--   0        0        0      309 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jdl-dn-anneal-l-2-dv-1.json.gz
+-rw-r--r--   0        0        0      346 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jdl-dn-anneal-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0     1501 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jdq-2.json.gz
+-rw-r--r--   0        0        0     1841 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/data-v1-dl-1666876.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/__init__.py
+-rw-r--r--   0        0        0      551 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-292.json.gz
+-rw-r--r--   0        0        0      553 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-40981.json.gz
+-rw-r--r--   0        0        0      306 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jdf-292.json.gz
+-rw-r--r--   0        0        0      306 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jdf-40981.json.gz
+-rw-r--r--   0        0        0      327 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jdl-dn-australian-l-2-dv-1-s-dact.json.gz
+-rw-r--r--   0        0        0       99 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jdl-dn-australian-l-2-dv-1.json.gz
+-rw-r--r--   0        0        0      319 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jdl-dn-australian-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0     2532 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/data-v1-dl-49822.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/__init__.py
+-rw-r--r--   0        0        0     2473 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/api-v1-jd-3.json.gz
+-rw-r--r--   0        0        0      535 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/api-v1-jdf-3.json.gz
+-rw-r--r--   0        0        0     1407 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/api-v1-jdq-3.json.gz
+-rw-r--r--   0        0        0    19485 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/data-v1-dl-3.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/__init__.py
+-rw-r--r--   0        0        0      598 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jd-40589.json.gz
+-rw-r--r--   0        0        0      856 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdf-40589.json.gz
+-rw-r--r--   0        0        0      315 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdl-dn-emotions-l-2-dv-3.json.gz
+-rw-r--r--   0        0        0      318 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdl-dn-emotions-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0      913 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdq-40589.json.gz
+-rw-r--r--   0        0        0     4344 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/data-v1-dl-4644182.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/__init__.py
+-rw-r--r--   0        0        0      323 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jd-40675.json.gz
+-rw-r--r--   0        0        0      307 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdf-40675.json.gz
+-rw-r--r--   0        0        0      317 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdl-dn-glass2-l-2-dv-1-s-dact.json.gz
+-rw-r--r--   0        0        0       85 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdl-dn-glass2-l-2-dv-1.json.gz
+-rw-r--r--   0        0        0       88 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdl-dn-glass2-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0      886 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdq-40675.json.gz
+-rw-r--r--   0        0        0     3000 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/data-v1-dl-4965250.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/__init__.py
+-rw-r--r--   0        0        0      437 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/api-v1-jd-40945.json.gz
+-rw-r--r--   0        0        0      320 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/api-v1-jdf-40945.json.gz
+-rw-r--r--   0        0        0     1042 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/api-v1-jdq-40945.json.gz
+-rw-r--r--   0        0        0    32243 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/data-v1-dl-16826755.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/__init__.py
+-rw-r--r--   0        0        0     1660 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jd-40966.json.gz
+-rw-r--r--   0        0        0     3690 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdf-40966.json.gz
+-rw-r--r--   0        0        0      325 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdl-dn-miceprotein-l-2-dv-4.json.gz
+-rw-r--r--   0        0        0      328 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdl-dn-miceprotein-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0      934 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdq-40966.json.gz
+-rw-r--r--   0        0        0     6471 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/data-v1-dl-17928620.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/__init__.py
+-rw-r--r--   0        0        0      584 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/api-v1-jd-42074.json.gz
+-rw-r--r--   0        0        0      272 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/api-v1-jdf-42074.json.gz
+-rw-r--r--   0        0        0      722 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/api-v1-jdq-42074.json.gz
+-rw-r--r--   0        0        0     2326 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/data-v1-dl-21552912.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/__init__.py
+-rw-r--r--   0        0        0     1492 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/api-v1-jd-42585.json.gz
+-rw-r--r--   0        0        0      312 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/api-v1-jdf-42585.json.gz
+-rw-r--r--   0        0        0      348 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/api-v1-jdq-42585.json.gz
+-rw-r--r--   0        0        0     4519 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/data-v1-dl-21854866.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/__init__.py
+-rw-r--r--   0        0        0     1798 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jd-561.json.gz
+-rw-r--r--   0        0        0      425 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jdf-561.json.gz
+-rw-r--r--   0        0        0      301 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jdl-dn-cpu-l-2-dv-1.json.gz
+-rw-r--r--   0        0        0      347 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jdl-dn-cpu-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0     1074 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jdq-561.json.gz
+-rw-r--r--   0        0        0     3303 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/data-v1-dl-52739.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/__init__.py
+-rw-r--r--   0        0        0      898 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jd-61.json.gz
+-rw-r--r--   0        0        0      268 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jdf-61.json.gz
+-rw-r--r--   0        0        0      293 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jdl-dn-iris-l-2-dv-1.json.gz
+-rw-r--r--   0        0        0      330 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jdl-dn-iris-l-2-s-act-.json.gz
+-rw-r--r--   0        0        0     1121 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jdq-61.json.gz
+-rw-r--r--   0        0        0     2342 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/data-v1-dl-61.arff.gz
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/__init__.py
+-rw-r--r--   0        0        0      656 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/api-v1-jd-62.json.gz
+-rw-r--r--   0        0        0      817 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/api-v1-jdf-62.json.gz
+-rw-r--r--   0        0        0      805 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/api-v1-jdq-62.json.gz
+-rw-r--r--   0        0        0     1625 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/data-v1-dl-52352.arff.gz
+-rw-r--r--   0        0        0      254 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/svmlight_classification.txt
+-rw-r--r--   0        0        0       54 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/svmlight_invalid.txt
+-rw-r--r--   0        0        0       23 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/svmlight_invalid_order.txt
+-rw-r--r--   0        0        0      105 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/svmlight_multilabel.txt
+-rw-r--r--   0        0        0     5340 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_20news.py
+-rw-r--r--   0        0        0     8196 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_arff_parser.py
+-rw-r--r--   0        0        0    12995 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_base.py
+-rw-r--r--   0        0        0     1369 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_california_housing.py
+-rw-r--r--   0        0        0     4380 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_common.py
+-rw-r--r--   0        0        0     1757 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_covtype.py
+-rw-r--r--   0        0        0     2606 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_kddcup99.py
+-rw-r--r--   0        0        0     7796 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_lfw.py
+-rw-r--r--   0        0        0      919 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_olivetti_faces.py
+-rw-r--r--   0        0        0    55366 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_openml.py
+-rw-r--r--   0        0        0     2343 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_rcv1.py
+-rw-r--r--   0        0        0    22573 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_samples_generator.py
+-rw-r--r--   0        0        0    20269 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_svmlight_format.py
+-rw-r--r--   0        0        0     1295 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/__init__.py
+-rw-r--r--   0        0        0     7246 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_base.py
+-rw-r--r--   0        0        0     1118 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_cdnmf_fast.pyx
+-rw-r--r--   0        0        0    76709 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_dict_learning.py
+-rw-r--r--   0        0        0    15301 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_factor_analysis.py
+-rw-r--r--   0        0        0    26439 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_fastica.py
+-rw-r--r--   0        0        0    15895 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_incremental_pca.py
+-rw-r--r--   0        0        0    21873 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_kernel_pca.py
+-rw-r--r--   0        0        0    33038 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_lda.py
+-rw-r--r--   0        0        0    82240 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_nmf.py
+-rw-r--r--   0        0        0     2842 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_online_lda_fast.pyx
+-rw-r--r--   0        0        0    34666 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_pca.py
+-rw-r--r--   0        0        0    18014 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_sparse_pca.py
+-rw-r--r--   0        0        0    11679 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/_truncated_svd.py
+-rw-r--r--   0        0        0      338 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/__init__.py
+-rw-r--r--   0        0        0    30432 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_dict_learning.py
+-rw-r--r--   0        0        0     4172 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_factor_analysis.py
+-rw-r--r--   0        0        0    15504 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_fastica.py
+-rw-r--r--   0        0        0    15954 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_incremental_pca.py
+-rw-r--r--   0        0        0    20772 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_kernel_pca.py
+-rw-r--r--   0        0        0    34251 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_nmf.py
+-rw-r--r--   0        0        0    15825 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_online_lda.py
+-rw-r--r--   0        0        0    41660 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_pca.py
+-rw-r--r--   0        0        0    13035 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_sparse_pca.py
+-rw-r--r--   0        0        0     7168 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_truncated_svd.py
+-rw-r--r--   0        0        0    38135 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/discriminant_analysis.py
+-rw-r--r--   0        0        0    24449 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/dummy.py
+-rw-r--r--   0        0        0     1340 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/__init__.py
+-rw-r--r--   0        0        0    46502 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_bagging.py
+-rw-r--r--   0        0        0    10322 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_base.py
+-rw-r--r--   0        0        0   114205 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_forest.py
+-rw-r--r--   0        0        0    86619 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_gb.py
+-rw-r--r--   0        0        0     8508 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_gradient_boosting.pyx
+-rw-r--r--   0        0        0      166 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/__init__.py
+-rw-r--r--   0        0        0     2719 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_binning.pyx
+-rw-r--r--   0        0        0      692 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd
+-rw-r--r--   0        0        0     2544 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx
+-rw-r--r--   0        0        0     1933 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx
+-rw-r--r--   0        0        0     9521 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx
+-rw-r--r--   0        0        0    13467 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/binning.py
+-rw-r--r--   0        0        0     1262 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/common.pxd
+-rw-r--r--   0        0        0     1747 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/common.pyx
+-rw-r--r--   0        0        0    94295 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py
+-rw-r--r--   0        0        0    32038 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/grower.py
+-rw-r--r--   0        0        0    20578 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/histogram.pyx
+-rw-r--r--   0        0        0      674 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/meson.build
+-rw-r--r--   0        0        0     4972 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/predictor.py
+-rw-r--r--   0        0        0    52497 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/splitting.pyx
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/__init__.py
+-rw-r--r--   0        0        0    16252 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py
+-rw-r--r--   0        0        0     2100 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py
+-rw-r--r--   0        0        0    10112 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py
+-rw-r--r--   0        0        0    60923 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
+-rw-r--r--   0        0        0    23152 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py
+-rw-r--r--   0        0        0     8681 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py
+-rw-r--r--   0        0        0    16940 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py
+-rw-r--r--   0        0        0     6345 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py
+-rw-r--r--   0        0        0    38639 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py
+-rw-r--r--   0        0        0     7933 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py
+-rw-r--r--   0        0        0     5443 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/utils.py
+-rw-r--r--   0        0        0    20593 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_iforest.py
+-rw-r--r--   0        0        0    39262 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_stacking.py
+-rw-r--r--   0        0        0    26015 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_voting.py
+-rw-r--r--   0        0        0    45559 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/_weight_boosting.py
+-rw-r--r--   0        0        0      232 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/__init__.py
+-rw-r--r--   0        0        0    31320 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_bagging.py
+-rw-r--r--   0        0        0     3637 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_base.py
+-rw-r--r--   0        0        0     9106 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_common.py
+-rw-r--r--   0        0        0    62547 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_forest.py
+-rw-r--r--   0        0        0    58761 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_gradient_boosting.py
+-rw-r--r--   0        0        0    12484 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_iforest.py
+-rw-r--r--   0        0        0    29624 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_stacking.py
+-rw-r--r--   0        0        0    27201 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_voting.py
+-rwxr-xr-x   0        0        0    25403 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_weight_boosting.py
+-rw-r--r--   0        0        0     6117 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/exceptions.py
+-rw-r--r--   0        0        0      252 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/__init__.py
+-rw-r--r--   0        0        0     1210 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/enable_halving_search_cv.py
+-rw-r--r--   0        0        0      747 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/enable_hist_gradient_boosting.py
+-rw-r--r--   0        0        0      688 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/enable_iterative_imputer.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/tests/__init__.py
+-rw-r--r--   0        0        0      672 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py
+-rw-r--r--   0        0        0     1689 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/tests/test_enable_iterative_imputer.py
+-rw-r--r--   0        0        0     1896 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/experimental/tests/test_enable_successive_halving.py
+-rw-r--r--   0        0        0      270 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/README
+-rw-r--r--   0        0        0       42 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/__init__.py
+-rw-r--r--   0        0        0    38341 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_arff.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_packaging/__init__.py
+-rw-r--r--   0        0        0     2922 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_packaging/_structures.py
+-rw-r--r--   0        0        0    16134 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_packaging/version.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_scipy/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_scipy/sparse/__init__.py
+-rw-r--r--   0        0        0       34 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_scipy/sparse/csgraph/__init__.py
+-rw-r--r--   0        0        0    18150 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/_scipy/sparse/csgraph/_laplacian.py
+-rw-r--r--   0        0        0      312 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/externals/conftest.py
+-rw-r--r--   0        0        0      439 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/__init__.py
+-rw-r--r--   0        0        0    15718 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/_dict_vectorizer.py
+-rw-r--r--   0        0        0     7382 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/_hash.py
+-rw-r--r--   0        0        0     2996 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/_hashing_fast.pyx
+-rw-r--r--   0        0        0     5645 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/_stop_words.py
+-rw-r--r--   0        0        0    23378 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/image.py
+-rw-r--r--   0        0        0      241 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/__init__.py
+-rw-r--r--   0        0        0     8272 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_dict_vectorizer.py
+-rw-r--r--   0        0        0     5046 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_feature_hasher.py
+-rw-r--r--   0        0        0    12154 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_image.py
+-rw-r--r--   0        0        0    52459 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_text.py
+-rw-r--r--   0        0        0    76677 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_extraction/text.py
+-rw-r--r--   0        0        0     1111 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/__init__.py
+-rw-r--r--   0        0        0     9377 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_base.py
+-rw-r--r--   0        0        0    18904 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_from_model.py
+-rw-r--r--   0        0        0    19994 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_mutual_info.py
+-rw-r--r--   0        0        0    29440 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_rfe.py
+-rw-r--r--   0        0        0    11462 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_sequential.py
+-rw-r--r--   0        0        0    40350 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_univariate_selection.py
+-rw-r--r--   0        0        0     4467 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/_variance_threshold.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/__init__.py
+-rw-r--r--   0        0        0     4781 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_base.py
+-rw-r--r--   0        0        0     3139 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_chi2.py
+-rw-r--r--   0        0        0    32507 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_feature_select.py
+-rw-r--r--   0        0        0    23038 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_from_model.py
+-rw-r--r--   0        0        0     9853 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_mutual_info.py
+-rw-r--r--   0        0        0    22787 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_rfe.py
+-rw-r--r--   0        0        0    10592 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_sequential.py
+-rw-r--r--   0        0        0     2640 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_variance_threshold.py
+-rw-r--r--   0        0        0      504 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/__init__.py
+-rw-r--r--   0        0        0    36524 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/_gpc.py
+-rw-r--r--   0        0        0    28064 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/_gpr.py
+-rw-r--r--   0        0        0    85362 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/kernels.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/__init__.py
+-rw-r--r--   0        0        0     1571 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/_mini_sequence_kernel.py
+-rw-r--r--   0        0        0     9971 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/test_gpc.py
+-rw-r--r--   0        0        0    29726 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/test_gpr.py
+-rw-r--r--   0        0        0    13570 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/test_kernels.py
+-rw-r--r--   0        0        0      944 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/__init__.py
+-rw-r--r--   0        0        0    40775 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/_base.py
+-rw-r--r--   0        0        0    37559 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/_iterative.py
+-rw-r--r--   0        0        0    14671 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/_knn.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/tests/__init__.py
+-rw-r--r--   0        0        0     3367 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/tests/test_base.py
+-rw-r--r--   0        0        0     7610 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/tests/test_common.py
+-rw-r--r--   0        0        0    60572 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/tests/test_impute.py
+-rw-r--r--   0        0        0    16638 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/impute/tests/test_knn.py
+-rw-r--r--   0        0        0      451 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/__init__.py
+-rw-r--r--   0        0        0    31239 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_partial_dependence.py
+-rw-r--r--   0        0        0     2137 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_pd_utils.py
+-rw-r--r--   0        0        0    11176 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_permutation_importance.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_plot/__init__.py
+-rw-r--r--   0        0        0    15320 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_plot/decision_boundary.py
+-rw-r--r--   0        0        0    60034 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_plot/partial_dependence.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_plot/tests/__init__.py
+-rw-r--r--   0        0        0    21474 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_plot/tests/test_boundary_decision_display.py
+-rw-r--r--   0        0        0    36426 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/tests/__init__.py
+-rw-r--r--   0        0        0    32429 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/tests/test_partial_dependence.py
+-rw-r--r--   0        0        0     1640 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/tests/test_pd_utils.py
+-rw-r--r--   0        0        0    19919 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/inspection/tests/test_permutation_importance.py
+-rw-r--r--   0        0        0    16637 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/isotonic.py
+-rw-r--r--   0        0        0    39401 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/kernel_approximation.py
+-rw-r--r--   0        0        0     9185 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/kernel_ridge.py
+-rw-r--r--   0        0        0     2529 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/__init__.py
+-rw-r--r--   0        0        0    28052 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_base.py
+-rw-r--r--   0        0        0    27479 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_bayes.py
+-rw-r--r--   0        0        0    33137 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_cd_fast.pyx
+-rw-r--r--   0        0        0   110116 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_coordinate_descent.py
+-rw-r--r--   0        0        0      263 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/__init__.py
+-rw-r--r--   0        0        0    19259 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/_newton_solver.py
+-rw-r--r--   0        0        0    31930 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/glm.py
+-rw-r--r--   0        0        0       24 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/tests/__init__.py
+-rw-r--r--   0        0        0    40684 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/tests/test_glm.py
+-rw-r--r--   0        0        0    12352 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_huber.py
+-rw-r--r--   0        0        0    82812 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_least_angle.py
+-rw-r--r--   0        0        0    26797 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_linear_loss.py
+-rw-r--r--   0        0        0    88888 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_logistic.py
+-rw-r--r--   0        0        0    38199 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_omp.py
+-rw-r--r--   0        0        0    19323 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_passive_aggressive.py
+-rw-r--r--   0        0        0     7707 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_perceptron.py
+-rw-r--r--   0        0        0    10790 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_quantile.py
+-rw-r--r--   0        0        0    25736 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_ransac.py
+-rw-r--r--   0        0        0   102556 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_ridge.py
+-rw-r--r--   0        0        0    12253 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_sag.py
+-rw-r--r--   0        0        0    30889 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_sag_fast.pyx.tp
+-rw-r--r--   0        0        0      897 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_sgd_fast.pxd
+-rw-r--r--   0        0        0    23632 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_sgd_fast.pyx.tp
+-rw-r--r--   0        0        0    92282 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_stochastic_gradient.py
+-rw-r--r--   0        0        0    15814 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/_theil_sen.py
+-rw-r--r--   0        0        0      728 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/__init__.py
+-rw-r--r--   0        0        0    27229 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_base.py
+-rw-r--r--   0        0        0    10473 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_bayes.py
+-rw-r--r--   0        0        0     4687 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_common.py
+-rw-r--r--   0        0        0    56926 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_coordinate_descent.py
+-rw-r--r--   0        0        0     7598 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_huber.py
+-rw-r--r--   0        0        0    29553 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_least_angle.py
+-rw-r--r--   0        0        0    12852 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_linear_loss.py
+-rw-r--r--   0        0        0    77214 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_logistic.py
+-rw-r--r--   0        0        0     9312 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_omp.py
+-rw-r--r--   0        0        0     9288 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_passive_aggressive.py
+-rw-r--r--   0        0        0     2608 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_perceptron.py
+-rw-r--r--   0        0        0    11425 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_quantile.py
+-rw-r--r--   0        0        0    16790 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_ransac.py
+-rw-r--r--   0        0        0    77191 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_ridge.py
+-rw-r--r--   0        0        0    29023 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_sag.py
+-rw-r--r--   0        0        0    71021 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_sgd.py
+-rw-r--r--   0        0        0    12654 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_sparse_coordinate_descent.py
+-rw-r--r--   0        0        0     9881 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_theil_sen.py
+-rw-r--r--   0        0        0      533 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/__init__.py
+-rw-r--r--   0        0        0    11327 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_barnes_hut_tsne.pyx
+-rw-r--r--   0        0        0    15587 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_isomap.py
+-rw-r--r--   0        0        0    30509 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_locally_linear.py
+-rw-r--r--   0        0        0    23693 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_mds.py
+-rw-r--r--   0        0        0    29854 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_spectral_embedding.py
+-rw-r--r--   0        0        0    45540 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_t_sne.py
+-rw-r--r--   0        0        0     3908 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/_utils.pyx
+-rw-r--r--   0        0        0      318 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/tests/__init__.py
+-rw-r--r--   0        0        0    12074 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_isomap.py
+-rw-r--r--   0        0        0     5772 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_locally_linear.py
+-rw-r--r--   0        0        0     3043 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_mds.py
+-rw-r--r--   0        0        0    19398 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_spectral_embedding.py
+-rw-r--r--   0        0        0    39698 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_t_sne.py
+-rw-r--r--   0        0        0     6344 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/meson.build
+-rw-r--r--   0        0        0     4601 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/__init__.py
+-rw-r--r--   0        0        0     7293 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_base.py
+-rw-r--r--   0        0        0   124751 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_classification.py
+-rw-r--r--   0        0        0     4378 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_dist_metrics.pxd.tp
+-rw-r--r--   0        0        0    91783 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_dist_metrics.pyx.tp
+-rw-r--r--   0        0        0     5122 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/__init__.py
+-rw-r--r--   0        0        0      979 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pxd.tp
+-rw-r--r--   0        0        0    19759 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx.tp
+-rw-r--r--   0        0        0     6408 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.tp
+-rw-r--r--   0        0        0     3563 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp
+-rw-r--r--   0        0        0    18353 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_base.pyx.tp
+-rw-r--r--   0        0        0      151 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_classmode.pxd
+-rw-r--r--   0        0        0     1948 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pxd.tp
+-rw-r--r--   0        0        0    15087 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp
+-rw-r--r--   0        0        0    29726 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py
+-rw-r--r--   0        0        0     5925 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pxd.tp
+-rw-r--r--   0        0        0    20344 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.tp
+-rw-r--r--   0        0        0     3254 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pxd.tp
+-rw-r--r--   0        0        0    19399 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx.tp
+-rw-r--r--   0        0        0     7326 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx.tp
+-rw-r--r--   0        0        0     6020 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/meson.build
+-rw-r--r--   0        0        0     3510 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_fast.pyx
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/__init__.py
+-rw-r--r--   0        0        0    16378 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/confusion_matrix.py
+-rw-r--r--   0        0        0    10770 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/det_curve.py
+-rw-r--r--   0        0        0    17688 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/precision_recall_curve.py
+-rw-r--r--   0        0        0    14389 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/regression.py
+-rw-r--r--   0        0        0    13545 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/roc_curve.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/__init__.py
+-rw-r--r--   0        0        0     8815 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_common_curve_display.py
+-rw-r--r--   0        0        0    13705 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_confusion_matrix_display.py
+-rw-r--r--   0        0        0     3426 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_det_curve_display.py
+-rw-r--r--   0        0        0    13100 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_precision_recall_display.py
+-rw-r--r--   0        0        0     5786 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_predict_error_display.py
+-rw-r--r--   0        0        0    10135 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_roc_curve_display.py
+-rw-r--r--   0        0        0    77276 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_ranking.py
+-rw-r--r--   0        0        0    62457 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_regression.py
+-rw-r--r--   0        0        0    36135 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/_scorer.py
+-rw-r--r--   0        0        0     1397 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/__init__.py
+-rw-r--r--   0        0        0     3378 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_bicluster.py
+-rw-r--r--   0        0        0     2730 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx
+-rw-r--r--   0        0        0    44498 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_supervised.py
+-rw-r--r--   0        0        0    17107 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_unsupervised.py
+-rw-r--r--   0        0        0      172 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/__init__.py
+-rw-r--r--   0        0        0     1719 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_bicluster.py
+-rw-r--r--   0        0        0     7755 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_common.py
+-rw-r--r--   0        0        0    17873 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_supervised.py
+-rw-r--r--   0        0        0    12269 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_unsupervised.py
+-rw-r--r--   0        0        0     1278 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/meson.build
+-rw-r--r--   0        0        0    87125 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/pairwise.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/__init__.py
+-rw-r--r--   0        0        0   108759 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_classification.py
+-rw-r--r--   0        0        0    61194 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_common.py
+-rw-r--r--   0        0        0    14802 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_dist_metrics.py
+-rw-r--r--   0        0        0    58062 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_pairwise.py
+-rw-r--r--   0        0        0    53036 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_pairwise_distances_reduction.py
+-rw-r--r--   0        0        0    83447 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_ranking.py
+-rw-r--r--   0        0        0    27231 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_regression.py
+-rw-r--r--   0        0        0    55718 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_score_objects.py
+-rw-r--r--   0        0        0      243 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/__init__.py
+-rw-r--r--   0        0        0    18937 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/_base.py
+-rw-r--r--   0        0        0    33488 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/_bayesian_mixture.py
+-rw-r--r--   0        0        0    31672 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/_gaussian_mixture.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/tests/__init__.py
+-rw-r--r--   0        0        0    17110 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/tests/test_bayesian_mixture.py
+-rw-r--r--   0        0        0    47726 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/tests/test_gaussian_mixture.py
+-rw-r--r--   0        0        0      992 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/mixture/tests/test_mixture.py
+-rw-r--r--   0        0        0     2487 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/__init__.py
+-rw-r--r--   0        0        0    36831 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/_classification_threshold.py
+-rw-r--r--   0        0        0    33987 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/_plot.py
+-rw-r--r--   0        0        0    76213 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/_search.py
+-rw-r--r--   0        0        0    43900 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/_search_successive_halving.py
+-rw-r--r--   0        0        0   103402 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/_split.py
+-rw-r--r--   0        0        0    87984 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/_validation.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/__init__.py
+-rw-r--r--   0        0        0      641 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/common.py
+-rw-r--r--   0        0        0    25584 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_classification_threshold.py
+-rw-r--r--   0        0        0    18456 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_plot.py
+-rw-r--r--   0        0        0    87809 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_search.py
+-rw-r--r--   0        0        0    72530 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_split.py
+-rw-r--r--   0        0        0    29054 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_successive_halving.py
+-rw-r--r--   0        0        0    89033 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_validation.py
+-rw-r--r--   0        0        0    43806 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/multiclass.py
+-rw-r--r--   0        0        0    43145 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/multioutput.py
+-rw-r--r--   0        0        0    55670 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/naive_bayes.py
+-rw-r--r--   0        0        0     1219 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/__init__.py
+-rw-r--r--   0        0        0     9326 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_ball_tree.pyx.tp
+-rw-r--r--   0        0        0    51595 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_base.py
+-rw-r--r--   0        0        0   100766 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_binary_tree.pxi.tp
+-rw-r--r--   0        0        0    31731 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_classification.py
+-rw-r--r--   0        0        0    25105 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_graph.py
+-rw-r--r--   0        0        0    11092 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_kd_tree.pyx.tp
+-rw-r--r--   0        0        0    12462 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_kde.py
+-rw-r--r--   0        0        0    19752 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_lof.py
+-rw-r--r--   0        0        0    19669 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_nca.py
+-rw-r--r--   0        0        0     7829 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_nearest_centroid.py
+-rw-r--r--   0        0        0      288 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_partition_nodes.pxd
+-rw-r--r--   0        0        0     4120 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_partition_nodes.pyx
+-rw-r--r--   0        0        0     4259 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_quad_tree.pxd
+-rw-r--r--   0        0        0    23691 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_quad_tree.pyx
+-rw-r--r--   0        0        0    18123 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_regression.py
+-rw-r--r--   0        0        0     6180 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/_unsupervised.py
+-rw-r--r--   0        0        0     1487 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/__init__.py
+-rw-r--r--   0        0        0     7097 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_ball_tree.py
+-rw-r--r--   0        0        0     3547 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_graph.py
+-rw-r--r--   0        0        0     3898 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_kd_tree.py
+-rw-r--r--   0        0        0     9745 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_kde.py
+-rw-r--r--   0        0        0    12899 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_lof.py
+-rw-r--r--   0        0        0    19363 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_nca.py
+-rw-r--r--   0        0        0     4869 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_nearest_centroid.py
+-rw-r--r--   0        0        0    82241 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_neighbors.py
+-rw-r--r--   0        0        0     8147 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_neighbors_pipeline.py
+-rw-r--r--   0        0        0     9281 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_neighbors_tree.py
+-rw-r--r--   0        0        0     4856 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_quad_tree.py
+-rw-r--r--   0        0        0      273 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/__init__.py
+-rw-r--r--   0        0        0     6328 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/_base.py
+-rw-r--r--   0        0        0    60949 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/_multilayer_perceptron.py
+-rw-r--r--   0        0        0    15251 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/_rbm.py
+-rw-r--r--   0        0        0     8822 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/_stochastic_optimizers.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/tests/__init__.py
+-rw-r--r--   0        0        0      796 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_base.py
+-rw-r--r--   0        0        0    31850 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_mlp.py
+-rw-r--r--   0        0        0     8048 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_rbm.py
+-rw-r--r--   0        0        0     4137 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_stochastic_optimizers.py
+-rw-r--r--   0        0        0    73228 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/pipeline.py
+-rw-r--r--   0        0        0     1460 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/__init__.py
+-rw-r--r--   0        0        0     9170 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_csr_polynomial_expansion.pyx
+-rw-r--r--   0        0        0   125637 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_data.py
+-rw-r--r--   0        0        0    16925 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_discretization.py
+-rw-r--r--   0        0        0    67782 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_encoders.py
+-rw-r--r--   0        0        0    15987 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_function_transformer.py
+-rw-r--r--   0        0        0    30834 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_label.py
+-rw-r--r--   0        0        0    47445 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_polynomial.py
+-rw-r--r--   0        0        0    20476 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_target_encoder.py
+-rw-r--r--   0        0        0     5941 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/_target_encoder_fast.pyx
+-rw-r--r--   0        0        0      414 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/__init__.py
+-rw-r--r--   0        0        0     6793 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_common.py
+-rw-r--r--   0        0        0    95032 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_data.py
+-rw-r--r--   0        0        0    17342 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_discretization.py
+-rw-r--r--   0        0        0    78693 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_encoders.py
+-rw-r--r--   0        0        0    19268 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_function_transformer.py
+-rw-r--r--   0        0        0    23645 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_label.py
+-rw-r--r--   0        0        0    42411 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_polynomial.py
+-rw-r--r--   0        0        0    27761 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_target_encoder.py
+-rw-r--r--   0        0        0    28096 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/random_projection.py
+-rw-r--r--   0        0        0      448 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/semi_supervised/__init__.py
+-rw-r--r--   0        0        0    21294 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/semi_supervised/_label_propagation.py
+-rw-r--r--   0        0        0    14342 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/semi_supervised/_self_training.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/semi_supervised/tests/__init__.py
+-rw-r--r--   0        0        0     8801 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/semi_supervised/tests/test_label_propagation.py
+-rw-r--r--   0        0        0    12543 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/semi_supervised/tests/test_self_training.py
+-rw-r--r--   0        0        0      636 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/__init__.py
+-rw-r--r--   0        0        0    42442 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_base.py
+-rw-r--r--   0        0        0     3252 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_bounds.py
+-rw-r--r--   0        0        0    67146 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_classes.py
+-rw-r--r--   0        0        0     1719 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_liblinear.pxi
+-rw-r--r--   0        0        0     4101 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_liblinear.pyx
+-rw-r--r--   0        0        0     3186 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_libsvm.pxi
+-rw-r--r--   0        0        0    26669 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_libsvm.pyx
+-rw-r--r--   0        0        0    18886 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_libsvm_sparse.pyx
+-rw-r--r--   0        0        0      298 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/_newrand.pyx
+-rw-r--r--   0        0        0     1291 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/meson.build
+-rw-r--r--   0        0        0     1486 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/COPYRIGHT
+-rw-r--r--   0        0        0      458 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/_cython_blas_helpers.h
+-rw-r--r--   0        0        0     6380 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/liblinear_helper.c
+-rw-r--r--   0        0        0    62634 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/linear.cpp
+-rw-r--r--   0        0        0     2459 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/linear.h
+-rw-r--r--   0        0        0     4940 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/tron.cpp
+-rw-r--r--   0        0        0      768 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/tron.h
+-rw-r--r--   0        0        0      769 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/LIBSVM_CHANGES
+-rw-r--r--   0        0        0      217 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/_svm_cython_blas_helpers.h
+-rw-r--r--   0        0        0    11723 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/libsvm_helper.c
+-rw-r--r--   0        0        0    13247 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/libsvm_sparse_helper.c
+-rw-r--r--   0        0        0      173 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/libsvm_template.cpp
+-rw-r--r--   0        0        0    69105 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/svm.cpp
+-rw-r--r--   0        0        0     6262 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/svm.h
+-rw-r--r--   0        0        0     1840 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/src/newrand/newrand.h
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/tests/__init__.py
+-rw-r--r--   0        0        0     5232 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/tests/test_bounds.py
+-rw-r--r--   0        0        0    15632 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/tests/test_sparse.py
+-rw-r--r--   0        0        0    48266 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/svm/tests/test_svm.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/__init__.py
+-rw-r--r--   0        0        0    17482 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/metadata_routing_common.py
+-rw-r--r--   0        0        0     3312 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/random_seed.py
+-rw-r--r--   0        0        0    28614 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_base.py
+-rw-r--r--   0        0        0     1181 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_build.py
+-rw-r--r--   0        0        0    40589 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_calibration.py
+-rw-r--r--   0        0        0      267 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_check_build.py
+-rw-r--r--   0        0        0    19664 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_common.py
+-rw-r--r--   0        0        0     6813 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_config.py
+-rw-r--r--   0        0        0    23225 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_discriminant_analysis.py
+-rw-r--r--   0        0        0    11824 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_docstring_parameters.py
+-rw-r--r--   0        0        0     6841 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_docstrings.py
+-rw-r--r--   0        0        0    21859 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_dummy.py
+-rw-r--r--   0        0        0      470 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_init.py
+-rw-r--r--   0        0        0    22169 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_isotonic.py
+-rw-r--r--   0        0        0    16416 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_kernel_approximation.py
+-rw-r--r--   0        0        0     2888 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_kernel_ridge.py
+-rw-r--r--   0        0        0    38826 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_metadata_routing.py
+-rw-r--r--   0        0        0    10299 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_metaestimators.py
+-rw-r--r--   0        0        0    27958 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_metaestimators_metadata_routing.py
+-rw-r--r--   0        0        0     4731 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_min_dependencies_readme.py
+-rw-r--r--   0        0        0    33214 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_multiclass.py
+-rw-r--r--   0        0        0    30083 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_multioutput.py
+-rw-r--r--   0        0        0    35027 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_naive_bayes.py
+-rw-r--r--   0        0        0    70829 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_pipeline.py
+-rw-r--r--   0        0        0    16697 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_public_functions.py
+-rw-r--r--   0        0        0    19583 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tests/test_random_projection.py
+-rw-r--r--   0        0        0      534 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/__init__.py
+-rw-r--r--   0        0        0    75403 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_classes.py
+-rw-r--r--   0        0        0     4738 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_criterion.pxd
+-rw-r--r--   0        0        0    62083 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_criterion.pyx
+-rw-r--r--   0        0        0    40385 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_export.py
+-rw-r--r--   0        0        0     5142 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_reingold_tilford.py
+-rw-r--r--   0        0        0     4678 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_splitter.pxd
+-rw-r--r--   0        0        0    60344 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_splitter.pyx
+-rw-r--r--   0        0        0     5161 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_tree.pxd
+-rw-r--r--   0        0        0    73533 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_tree.pyx
+-rw-r--r--   0        0        0     3818 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_utils.pxd
+-rw-r--r--   0        0        0    16817 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/_utils.pyx
+-rw-r--r--   0        0        0      746 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/meson.build
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/tests/__init__.py
+-rw-r--r--   0        0        0    17964 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/tests/test_export.py
+-rw-r--r--   0        0        0    18590 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/tests/test_monotonic_tree.py
+-rw-r--r--   0        0        0     1461 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/tests/test_reingold_tilford.py
+-rw-r--r--   0        0        0    94696 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/tree/tests/test_tree.py
+-rw-r--r--   0        0        0     3250 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/__init__.py
+-rw-r--r--   0        0        0     1129 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_arpack.py
+-rw-r--r--   0        0        0    27717 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_array_api.py
+-rw-r--r--   0        0        0     2873 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_available_if.py
+-rw-r--r--   0        0        0     2096 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_bunch.py
+-rw-r--r--   0        0        0     5358 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_chunking.py
+-rw-r--r--   0        0        0     1565 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_cython_blas.pxd
+-rw-r--r--   0        0        0     7968 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_cython_blas.pyx
+-rw-r--r--   0        0        0    11387 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_encode.py
+-rw-r--r--   0        0        0    11016 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_estimator_html_repr.css
+-rw-r--r--   0        0        0    18308 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_estimator_html_repr.py
+-rw-r--r--   0        0        0      476 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_fast_dict.pxd
+-rw-r--r--   0        0        0     4613 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_fast_dict.pyx
+-rw-r--r--   0        0        0      256 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_heap.pxd
+-rw-r--r--   0        0        0     2253 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_heap.pyx
+-rw-r--r--   0        0        0    22014 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_indexing.py
+-rw-r--r--   0        0        0     1384 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_isfinite.pyx
+-rw-r--r--   0        0        0      741 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_joblib.py
+-rw-r--r--   0        0        0     4810 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_mask.py
+-rw-r--r--   0        0        0    56000 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_metadata_requests.py
+-rw-r--r--   0        0        0     1399 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_missing.py
+-rw-r--r--   0        0        0    13365 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_mocking.py
+-rw-r--r--   0        0        0     1069 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_openmp_helpers.pxd
+-rw-r--r--   0        0        0     3143 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_openmp_helpers.pyx
+-rw-r--r--   0        0        0     1221 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_optional_dependencies.py
+-rw-r--r--   0        0        0    28422 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_param_validation.py
+-rw-r--r--   0        0        0     3508 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_plotting.py
+-rw-r--r--   0        0        0    18524 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_pprint.py
+-rw-r--r--   0        0        0     1220 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_random.pxd
+-rw-r--r--   0        0        0    12557 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_random.pyx
+-rw-r--r--   0        0        0    12041 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_response.py
+-rw-r--r--   0        0        0     2562 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_seq_dataset.pxd.tp
+-rw-r--r--   0        0        0    12339 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_seq_dataset.pyx.tp
+-rw-r--r--   0        0        0    14716 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_set_output.py
+-rw-r--r--   0        0        0     2493 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_show_versions.py
+-rw-r--r--   0        0        0      161 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_sorting.pxd
+-rw-r--r--   0        0        0     3280 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_sorting.pyx
+-rw-r--r--   0        0        0     2071 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_tags.py
+-rw-r--r--   0        0        0    40673 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_testing.py
+-rw-r--r--   0        0        0     2090 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_typedefs.pxd
+-rw-r--r--   0        0        0      428 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_typedefs.pyx
+-rw-r--r--   0        0        0     1405 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_user_interface.py
+-rw-r--r--   0        0        0      296 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_vector_sentinel.pxd
+-rw-r--r--   0        0        0     4458 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_vector_sentinel.pyx
+-rw-r--r--   0        0        0     1384 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_weight_vector.pxd.tp
+-rw-r--r--   0        0        0     6977 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/_weight_vector.pyx.tp
+-rw-r--r--   0        0        0     3337 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/arrayfuncs.pyx
+-rw-r--r--   0        0        0     8245 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/class_weight.py
+-rw-r--r--   0        0        0     3855 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/deprecation.py
+-rw-r--r--   0        0        0     9116 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/discovery.py
+-rw-r--r--   0        0        0   168101 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/estimator_checks.py
+-rw-r--r--   0        0        0    48004 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/extmath.py
+-rw-r--r--   0        0        0    14919 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/fixes.py
+-rw-r--r--   0        0        0     5852 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/graph.py
+-rw-r--r--   0        0        0     2290 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/meson.build
+-rw-r--r--   0        0        0      958 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/metadata_routing.py
+-rw-r--r--   0        0        0     5869 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/metaestimators.py
+-rw-r--r--   0        0        0    19463 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/multiclass.py
+-rw-r--r--   0        0        0      876 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/murmurhash.pxd
+-rw-r--r--   0        0        0     4529 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/murmurhash.pyx
+-rw-r--r--   0        0        0    11934 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/optimize.py
+-rw-r--r--   0        0        0     4256 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/parallel.py
+-rw-r--r--   0        0        0     3723 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/random.py
+-rw-r--r--   0        0        0    22673 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/sparsefuncs.py
+-rw-r--r--   0        0        0    21677 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/sparsefuncs_fast.pyx
+-rw-r--r--   0        0        0     7969 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/src/MurmurHash3.cpp
+-rw-r--r--   0        0        0     1155 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/src/MurmurHash3.h
+-rw-r--r--   0        0        0     2357 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/stats.py
+-rw-r--r--   0        0        0        0 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/__init__.py
+-rw-r--r--   0        0        0      490 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_arpack.py
+-rw-r--r--   0        0        0    16774 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_array_api.py
+-rw-r--r--   0        0        0     1310 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_arrayfuncs.py
+-rw-r--r--   0        0        0      813 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_bunch.py
+-rw-r--r--   0        0        0     2371 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_chunking.py
+-rw-r--r--   0        0        0    12309 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_class_weight.py
+-rw-r--r--   0        0        0     6459 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_cython_blas.py
+-rw-r--r--   0        0        0      834 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_cython_templating.py
+-rw-r--r--   0        0        0     2023 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_deprecation.py
+-rw-r--r--   0        0        0     9603 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_encode.py
+-rw-r--r--   0        0        0    44936 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_estimator_checks.py
+-rw-r--r--   0        0        0    18057 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_estimator_html_repr.py
+-rw-r--r--   0        0        0    37593 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_extmath.py
+-rw-r--r--   0        0        0     1355 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_fast_dict.py
+-rw-r--r--   0        0        0     5382 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_fixes.py
+-rw-r--r--   0        0        0     3047 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_graph.py
+-rw-r--r--   0        0        0    21886 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_indexing.py
+-rw-r--r--   0        0        0      537 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_mask.py
+-rw-r--r--   0        0        0     2107 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_metaestimators.py
+-rw-r--r--   0        0        0      709 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_missing.py
+-rw-r--r--   0        0        0     5898 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_mocking.py
+-rw-r--r--   0        0        0    20910 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_multiclass.py
+-rw-r--r--   0        0        0     2515 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_murmurhash.py
+-rw-r--r--   0        0        0     5258 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_optimize.py
+-rw-r--r--   0        0        0     3650 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_parallel.py
+-rw-r--r--   0        0        0    24373 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_param_validation.py
+-rw-r--r--   0        0        0     2768 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_plotting.py
+-rw-r--r--   0        0        0    27339 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_pprint.py
+-rw-r--r--   0        0        0     7157 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_random.py
+-rw-r--r--   0        0        0    13452 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_response.py
+-rw-r--r--   0        0        0     5890 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_seq_dataset.py
+-rw-r--r--   0        0        0    15798 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_set_output.py
+-rw-r--r--   0        0        0     1846 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_shortest_path.py
+-rw-r--r--   0        0        0     1001 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_show_versions.py
+-rw-r--r--   0        0        0    34923 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_sparsefuncs.py
+-rw-r--r--   0        0        0     2760 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_stats.py
+-rw-r--r--   0        0        0     1396 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_tags.py
+-rw-r--r--   0        0        0    27815 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_testing.py
+-rw-r--r--   0        0        0      735 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_typedefs.py
+-rw-r--r--   0        0        0     1772 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_user_interface.py
+-rw-r--r--   0        0        0      816 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_utils.py
+-rw-r--r--   0        0        0    71397 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_validation.py
+-rw-r--r--   0        0        0      665 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/tests/test_weight_vector.py
+-rw-r--r--   0        0        0    90456 2024-05-06 08:54:15.000000 scikit_learn-1.5.0rc1/sklearn/utils/validation.py
+-rw-r--r--   0        0        0    11774 2024-05-06 08:55:36.938210 scikit_learn-1.5.0rc1/PKG-INFO
```

### Comparing `scikit-learn-1.4.2/COPYING` & `scikit_learn-1.5.0rc1/COPYING`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 BSD 3-Clause License
 
-Copyright (c) 2007-2023 The scikit-learn developers.
+Copyright (c) 2007-2024 The scikit-learn developers.
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
 * Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
```

### Comparing `scikit-learn-1.4.2/MANIFEST.in` & `scikit_learn-1.5.0rc1/MANIFEST.in`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/Makefile` & `scikit_learn-1.5.0rc1/Makefile`

 * *Files 18% similar despite different names*

```diff
@@ -20,17 +20,15 @@
 	rm -rf dist
 
 in: inplace # just a shortcut
 inplace:
 	$(PYTHON) setup.py build_ext -i
 
 dev-meson:
-	# Temporary script to try the experimental meson build. Once meson is
-	# accepted as the default build tool, this will go away.
-	python build_tools/build-meson-editable-install.py
+	pip install --verbose --no-build-isolation --editable . --config-settings editable-verbose=true
 
 clean-meson:
 	pip uninstall -y scikit-learn
 
 test-code: in
 	$(PYTEST) --showlocals -v sklearn --durations=20
 test-sphinxext:
```

### Comparing `scikit-learn-1.4.2/PKG-INFO` & `scikit_learn-1.5.0rc1/README.rst`

 * *Files 14% similar despite different names*

```diff
@@ -1,85 +1,46 @@
-Metadata-Version: 2.1
-Name: scikit-learn
-Version: 1.4.2
-Summary: A set of python modules for machine learning and data mining
-Home-page: https://scikit-learn.org
-Maintainer: Andreas Mueller
-Maintainer-email: amueller@ais.uni-bonn.de
-License: new BSD
-Download-URL: https://pypi.org/project/scikit-learn/#files
-Project-URL: Bug Tracker, https://github.com/scikit-learn/scikit-learn/issues
-Project-URL: Documentation, https://scikit-learn.org/stable/documentation.html
-Project-URL: Source Code, https://github.com/scikit-learn/scikit-learn
-Platform: UNKNOWN
-Classifier: Intended Audience :: Science/Research
-Classifier: Intended Audience :: Developers
-Classifier: License :: OSI Approved :: BSD License
-Classifier: Programming Language :: C
-Classifier: Programming Language :: Python
-Classifier: Topic :: Software Development
-Classifier: Topic :: Scientific/Engineering
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Operating System :: Microsoft :: Windows
-Classifier: Operating System :: POSIX
-Classifier: Operating System :: Unix
-Classifier: Operating System :: MacOS
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Programming Language :: Python :: 3.12
-Classifier: Programming Language :: Python :: Implementation :: CPython
-Classifier: Programming Language :: Python :: Implementation :: PyPy
-Requires-Python: >=3.9
-Provides-Extra: examples
-Provides-Extra: docs
-Provides-Extra: tests
-Provides-Extra: benchmark
-License-File: COPYING
-
 .. -*- mode: rst -*-
 
-|Azure|_ |CirrusCI|_ |Codecov|_ |CircleCI|_ |Nightly wheels|_ |Black|_ |PythonVersion|_ |PyPi|_ |DOI|_ |Benchmark|_
+|Azure| |CirrusCI| |Codecov| |CircleCI| |Nightly wheels| |Black| |PythonVersion| |PyPi| |DOI| |Benchmark|
 
 .. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main
-.. _Azure: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main
+   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main
 
 .. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield
-.. _CircleCI: https://circleci.com/gh/scikit-learn/scikit-learn
+   :target: https://circleci.com/gh/scikit-learn/scikit-learn
 
 .. |CirrusCI| image:: https://img.shields.io/cirrus/github/scikit-learn/scikit-learn/main?label=Cirrus%20CI
-.. _CirrusCI: https://cirrus-ci.com/github/scikit-learn/scikit-learn/main
+   :target: https://cirrus-ci.com/github/scikit-learn/scikit-learn/main
 
 .. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9
-.. _Codecov: https://codecov.io/gh/scikit-learn/scikit-learn
+   :target: https://codecov.io/gh/scikit-learn/scikit-learn
 
 .. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/workflows/Wheel%20builder/badge.svg?event=schedule
-.. _`Nightly wheels`: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule
+   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule
 
 .. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg
-.. _PythonVersion: https://pypi.org/project/scikit-learn/
+   :target: https://pypi.org/project/scikit-learn/
 
 .. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn
-.. _PyPi: https://pypi.org/project/scikit-learn
+   :target: https://pypi.org/project/scikit-learn
 
 .. |Black| image:: https://img.shields.io/badge/code%20style-black-000000.svg
-.. _Black: https://github.com/psf/black
+   :target: https://github.com/psf/black
 
 .. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg
-.. _DOI: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn
+   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn
 
 .. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue
-.. _`Benchmark`: https://scikit-learn.org/scikit-learn-benchmarks/
+   :target: https://scikit-learn.org/scikit-learn-benchmarks
 
 .. |PythonMinVersion| replace:: 3.9
 .. |NumPyMinVersion| replace:: 1.19.5
 .. |SciPyMinVersion| replace:: 1.6.0
 .. |JoblibMinVersion| replace:: 1.2.0
-.. |ThreadpoolctlMinVersion| replace:: 2.0.0
+.. |ThreadpoolctlMinVersion| replace:: 3.1.0
 .. |MatplotlibMinVersion| replace:: 3.3.4
 .. |Scikit-ImageMinVersion| replace:: 0.17.2
 .. |PandasMinVersion| replace:: 1.1.5
 .. |SeabornMinVersion| replace:: 0.9.0
 .. |PytestMinVersion| replace:: 7.1.2
 .. |PlotlyMinVersion| replace:: 5.14.0
 
@@ -219,27 +180,27 @@
 - HTML documentation (development version): https://scikit-learn.org/dev/
 - FAQ: https://scikit-learn.org/stable/faq.html
 
 Communication
 ~~~~~~~~~~~~~
 
 - Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn
-- Gitter: https://gitter.im/scikit-learn/scikit-learn
 - Logos & Branding: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos
 - Blog: https://blog.scikit-learn.org
 - Calendar: https://blog.scikit-learn.org/calendar/
 - Twitter: https://twitter.com/scikit_learn
 - Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn
-- Github Discussions: https://github.com/scikit-learn/scikit-learn/discussions
+- GitHub Discussions: https://github.com/scikit-learn/scikit-learn/discussions
 - Website: https://scikit-learn.org
 - LinkedIn: https://www.linkedin.com/company/scikit-learn
 - YouTube: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists
 - Facebook: https://www.facebook.com/scikitlearnofficial/
 - Instagram: https://www.instagram.com/scikitlearnofficial/
 - TikTok: https://www.tiktok.com/@scikit.learn
+- Mastodon: https://mastodon.social/@sklearn@fosstodon.org
+- Discord: https://discord.gg/h9qyrK8Jc8
+
 
 Citation
 ~~~~~~~~
 
 If you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn
-
-
```

### Comparing `scikit-learn-1.4.2/doc/Makefile` & `scikit_learn-1.5.0rc1/doc/Makefile`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/about.rst` & `scikit_learn-1.5.0rc1/doc/about.rst`

 * *Files 3% similar despite different names*

```diff
@@ -18,63 +18,77 @@
 
 Governance
 ----------
 
 The decision making process and governance structure of scikit-learn is laid
 out in the :ref:`governance document <governance>`.
 
-Authors
--------
+.. The "author" anchors below is there to ensure that old html links (in
+   the form of "about.html#author" still work)
 
-The following people are currently core contributors to scikit-learn's development
-and maintenance:
+.. _authors:
 
-.. include:: authors.rst
+The people behind scikit-learn
+-------------------------------
+
+Scikit-learn is a community project, developed by a large group of
+people, all across the world. A few teams, listed below, have central
+roles, however a more complete list of contributors can be found `on
+github
+<https://github.com/scikit-learn/scikit-learn/graphs/contributors>`__.
+
+Maintainers Team
+................
+
+The following people are currently maintainers, in charge of
+consolidating scikit-learn's development and maintenance:
+
+.. include:: maintainers.rst
 
 Please do not email the authors directly to ask for assistance or report issues.
 Instead, please see `What's the best way to ask questions about scikit-learn
 <https://scikit-learn.org/stable/faq.html#what-s-the-best-way-to-get-help-on-scikit-learn-usage>`_
 in the FAQ.
 
 .. seealso::
 
    :ref:`How you can contribute to the project <contributing>`
 
 Documentation Team
-------------------
+..................
 
 The following people help with documenting the project:
 
 .. include:: documentation_team.rst
 
 Contributor Experience Team
----------------------------
+...........................
 
 The following people are active contributors who also help with
 :ref:`triaging issues <bug_triaging>`, PRs, and general
 maintenance:
 
 .. include:: contributor_experience_team.rst
 
 Communication Team
-------------------
+..................
 
 The following people help with :ref:`communication around scikit-learn
 <communication_team>`.
 
 .. include:: communication_team.rst
 
 
 Emeritus Core Developers
 ------------------------
 
 The following people have been active contributors in the past, but are no
 longer active in the project:
 
-.. include:: authors_emeritus.rst
+.. include:: maintainers_emeritus.rst
 
 Emeritus Communication Team
 ---------------------------
 
 The following people have been active in the communication team in the
 past, but no longer have communication responsibilities:
```

### Comparing `scikit-learn-1.4.2/doc/authors.rst` & `scikit_learn-1.5.0rc1/doc/maintainers.rst`

 * *Files 1% similar despite different names*

```diff
@@ -94,14 +94,18 @@
     <p>Gael Varoquaux</p>
     </div>
     <div>
     <a href='https://github.com/NelleV'><img src='https://avatars.githubusercontent.com/u/184798?v=4' class='avatar' /></a> <br />
     <p>Nelle Varoquaux</p>
     </div>
     <div>
+    <a href='https://github.com/Charlie-XIAO'><img src='https://avatars.githubusercontent.com/u/108576690?v=4' class='avatar' /></a> <br />
+    <p>Yao Xiao</p>
+    </div>
+    <div>
     <a href='https://github.com/rth'><img src='https://avatars.githubusercontent.com/u/630936?v=4' class='avatar' /></a> <br />
     <p>Roman Yurchak</p>
     </div>
     <div>
     <a href='https://github.com/Micky774'><img src='https://avatars.githubusercontent.com/u/34613774?v=4' class='avatar' /></a> <br />
     <p>Meekail Zain</p>
     </div>
```

#### html2text {}

```diff
@@ -41,11 +41,13 @@
 Bertrand Thirion
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_1_1_0_6_5_5_9_6_?_v_=_4_]
 Tom DuprÃ© la Tour
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_2_0_8_2_1_7_?_v_=_4_]
 Gael Varoquaux
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_1_8_4_7_9_8_?_v_=_4_]
 Nelle Varoquaux
+_[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_1_0_8_5_7_6_6_9_0_?_v_=_4_]
+Yao Xiao
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_6_3_0_9_3_6_?_v_=_4_]
 Roman Yurchak
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_3_4_6_1_3_7_7_4_?_v_=_4_]
 Meekail Zain
```

### Comparing `scikit-learn-1.4.2/doc/authors_emeritus.rst` & `scikit_learn-1.5.0rc1/doc/maintainers_emeritus.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/common_pitfalls.rst` & `scikit_learn-1.5.0rc1/doc/common_pitfalls.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/communication_team.rst` & `scikit_learn-1.5.0rc1/doc/communication_team.rst`

 * *Files 3% similar despite different names*

```diff
@@ -7,10 +7,10 @@
     </style>
     <div>
     <a href='https://github.com/laurburke'><img src='https://avatars.githubusercontent.com/u/35973528?v=4' class='avatar' /></a> <br />
     <p>Lauren Burke</p>
     </div>
     <div>
     <a href='https://github.com/francoisgoupil'><img src='https://avatars.githubusercontent.com/u/98105626?v=4' class='avatar' /></a> <br />
-    <p>francoisgoupil</p>
+    <p>François Goupil</p>
     </div>
     </div>
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

#### html2text {}

```diff
@@ -1,5 +1,5 @@
 .. raw :: html
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_3_5_9_7_3_5_2_8_?_v_=_4_]
 Lauren Burke
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_9_8_1_0_5_6_2_6_?_v_=_4_]
-francoisgoupil
+FranÃ§ois Goupil
```

### Comparing `scikit-learn-1.4.2/doc/computing/computational_performance.rst` & `scikit_learn-1.5.0rc1/doc/computing/computational_performance.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/computing/parallelism.rst` & `scikit_learn-1.5.0rc1/doc/computing/parallelism.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/computing/scaling_strategies.rst` & `scikit_learn-1.5.0rc1/doc/computing/scaling_strategies.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/conf.py` & `scikit_learn-1.5.0rc1/doc/conf.py`

 * *Files 1% similar despite different names*

```diff
@@ -491,14 +491,16 @@
 
     if "seaborn" in notebook_content_str:
         code_lines.append("%pip install seaborn")
     if "plotly.express" in notebook_content_str:
         code_lines.append("%pip install plotly")
     if "skimage" in notebook_content_str:
         code_lines.append("%pip install scikit-image")
+    if "polars" in notebook_content_str:
+        code_lines.append("%pip install polars")
     if "fetch_" in notebook_content_str:
         code_lines.extend(
             [
                 "%pip install pyodide-http",
                 "import pyodide_http",
                 "pyodide_http.patch_all()",
             ]
```

### Comparing `scikit-learn-1.4.2/doc/conftest.py` & `scikit_learn-1.5.0rc1/doc/conftest.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,17 +5,16 @@
 
 import pytest
 from _pytest.doctest import DoctestItem
 
 from sklearn.datasets import get_data_home
 from sklearn.datasets._base import _pkl_filepath
 from sklearn.datasets._twenty_newsgroups import CACHE_NAME
-from sklearn.utils import IS_PYPY
 from sklearn.utils._testing import SkipTest, check_skip_network
-from sklearn.utils.fixes import np_base_version, parse_version
+from sklearn.utils.fixes import _IS_PYPY, np_base_version, parse_version
 
 
 def setup_labeled_faces():
     data_home = get_data_home()
     if not exists(join(data_home, "lfw_home")):
         raise SkipTest("Skipping dataset loading doctests")
 
@@ -31,15 +30,15 @@
 def setup_twenty_newsgroups():
     cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)
     if not exists(cache_path):
         raise SkipTest("Skipping dataset loading doctests")
 
 
 def setup_working_with_text_data():
-    if IS_PYPY and os.environ.get("CI", None):
+    if _IS_PYPY and os.environ.get("CI", None):
         raise SkipTest("Skipping too slow test with PyPy on CI")
     check_skip_network()
     cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)
     if not exists(cache_path):
         raise SkipTest("Skipping dataset loading doctests")
```

### Comparing `scikit-learn-1.4.2/doc/contributor_experience_team.rst` & `scikit_learn-1.5.0rc1/doc/contributor_experience_team.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/data_transforms.rst` & `scikit_learn-1.5.0rc1/doc/data_transforms.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/datasets/loading_other_datasets.rst` & `scikit_learn-1.5.0rc1/doc/datasets/loading_other_datasets.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/datasets/real_world.rst` & `scikit_learn-1.5.0rc1/doc/datasets/real_world.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/datasets/sample_generators.rst` & `scikit_learn-1.5.0rc1/doc/datasets/sample_generators.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/datasets/toy_dataset.rst` & `scikit_learn-1.5.0rc1/doc/datasets/toy_dataset.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/datasets.rst` & `scikit_learn-1.5.0rc1/doc/datasets.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/developers/advanced_installation.rst` & `scikit_learn-1.5.0rc1/doc/developers/advanced_installation.rst`

 * *Files 24% similar despite different names*

```diff
@@ -60,68 +60,71 @@
 
 #. Install a recent version of Python (3.9 is recommended at the time of writing)
    for instance using Miniforge3_. Miniforge provides a conda-based distribution
    of Python and the most popular scientific libraries.
 
    If you installed Python with conda, we recommend to create a dedicated
    `conda environment`_ with all the build dependencies of scikit-learn
-   (namely NumPy_, SciPy_, and Cython_):
+   (namely NumPy_, SciPy_, Cython_, meson-python_ and Ninja_):
 
    .. prompt:: bash $
 
-     conda create -n sklearn-env -c conda-forge python=3.9 numpy scipy cython
+     conda create -n sklearn-env -c conda-forge python=3.9 numpy scipy cython meson-python ninja
 
    It is not always necessary but it is safer to open a new prompt before
    activating the newly created conda environment.
 
    .. prompt:: bash $
 
      conda activate sklearn-env
 
-#. **Alternative to conda:** If you run Linux or similar, you can instead use
-   your system's Python provided it is recent enough (3.8 or higher
-   at the time of writing). In this case, we recommend to create a dedicated
-   virtualenv_ and install the scikit-learn build dependencies with pip:
+#. **Alternative to conda:** You can use alternative installations of Python
+   provided they are recent enough (3.9 or higher at the time of writing).
+   Here is an example on how to create a build environment for a Linux system's
+   Python. Build dependencies are installed with `pip` in a dedicated virtualenv_
+   to avoid disrupting other Python programs installed on the system:
 
    .. prompt:: bash $
 
      python3 -m venv sklearn-env
      source sklearn-env/bin/activate
-     pip install wheel numpy scipy cython
+     pip install wheel numpy scipy cython meson-python ninja
 
 #. Install a compiler with OpenMP_ support for your platform. See instructions
    for :ref:`compiler_windows`, :ref:`compiler_macos`, :ref:`compiler_linux`
    and :ref:`compiler_freebsd`.
 
-#. Build the project with pip in :ref:`editable_mode`:
+#. Build the project with pip:
 
    .. prompt:: bash $
 
-     pip install -v --no-use-pep517 --no-build-isolation -e .
+     pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 #. Check that the installed scikit-learn has a version number ending with
    `.dev0`:
 
    .. prompt:: bash $
 
      python -c "import sklearn; sklearn.show_versions()"
 
 #. Please refer to the :ref:`developers_guide` and :ref:`pytest_tips` to run
    the tests on the module of your choice.
 
 .. note::
 
-    You will have to run the ``pip install -v --no-use-pep517 --no-build-isolation -e .``
-    command every time the source code of a Cython file is updated
-    (ending in `.pyx` or `.pxd`). This can happen when you edit them or when you
-    use certain git commands such as `git pull`. Use the ``--no-build-isolation`` flag
-    to avoid compiling the whole project each time, only the files you have
-    modified. Include the ``--no-use-pep517`` flag because the ``--no-build-isolation``
-    option might not work otherwise (this is due to a bug which will be fixed in the
-    future).
+    `--config-settings editable-verbose=true` is optional but recommended
+    to avoid surprises when you import `sklearn`. `meson-python` implements
+    editable installs by rebuilding `sklearn` when executing `import sklearn`.
+    With the recommended setting you will see a message when this happens,
+    rather than potentially waiting without feed-back and wondering
+    what is taking so long. Bonus: this means you only have to run the `pip
+    install` command once, `sklearn` will automatically be rebuilt when
+    importing `sklearn`.
 
 Dependencies
 ------------
 
 Runtime dependencies
 ~~~~~~~~~~~~~~~~~~~~
 
@@ -177,116 +180,14 @@
 Building a specific version from a tag
 --------------------------------------
 
 If you want to build a stable version, you can ``git checkout <VERSION>``
 to get the code for that particular version, or download an zip archive of
 the version from github.
 
-.. _editable_mode:
-
-Editable mode
--------------
-
-If you run the development version, it is cumbersome to reinstall the package
-each time you update the sources. Therefore it is recommended that you install
-in with the ``pip install -v --no-use-pep517 --no-build-isolation -e .`` command,
-which allows you to edit the code in-place. This builds the extension in place and
-creates a link to the development directory (see `the pip docs
-<https://pip.pypa.io/en/stable/topics/local-project-installs/#editable-installs>`_).
-
-As the doc above explains, this is fundamentally similar to using the command
-``python setup.py develop``. (see `the setuptool docs
-<https://setuptools.pypa.io/en/latest/userguide/development_mode.html>`_).
-It is however preferred to use pip.
-
-On Unix-like systems, you can equivalently type ``make in`` from the top-level
-folder. Have a look at the ``Makefile`` for additional utilities.
-
-.. _building_with_meson:
-
-Building with Meson
--------------------
-
-Support for Meson is experimental, in scikit-learn 1.5.0.dev0.
-`Open an issue <https://github.com/scikit-learn/scikit-learn/issues/new>`__ if
-you encounter any problems!
-
-Make sure you have `meson-python` and `ninja` installed, either with `conda`:
-
-.. code-block:: bash
-
-    conda install -c conda-forge meson-python ninja -y
-
-or with pip:
-
-.. code-block:: bash
-
-    pip install meson-python ninja
-
-Simplest way to build with Meson
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-To build scikit-learn, the simplest way is to run:
-
-.. code-block:: bash
-
-    make dev-meson
-
-You need to do it once after this you can run your code that imports `sklearn`
-and it will recompile as needed.
-
-In case you want to go back to using setuptools:
-
-.. code-block:: bash
-
-    make clean-meson
-
-More advanced way to build with Meson
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-If you can not use `make`, want to do it yourself or understand what goes in
-behind the scenes, you can edit `pyproject.toml` and make sure `build-backend`
-is set to `"mesonpy"`
-
-.. code-block:: toml
-
-    [build-system]
-    build-backend = "mesonpy"
-
-Build with the following `pip` command:
-
-.. code-block:: bash
-
-    pip install --editable . \
-        --verbose --no-build-isolation \
-        --config-settings editable-verbose=true
-
-If you want to go back to using `setuptools`:
-
-.. code-block:: bash
-
-    pip uninstall -y scikit-learn
-
-Note `--config-settings editable-verbose=true` is advised to avoid surprises.
-meson-python implements editable install by recompiling when doing `import
-sklearn`. Even changing python files involves copying files to the Meson build
-directory. You will see the meson output when that happens, rather than
-potentially waiting a while and wondering what is taking so long. Bonus: that
-means you only have to do the `pip install` once, after that your code will
-recompile when doing `import sklearn`.
-
-Other places that may be worth looking at:
-
-- `pandas setup doc
-  <https://pandas.pydata.org/docs/development/contributing_environment.html#step-3-build-and-install-pandas>`_:
-  pandas has a similar setup as ours (no spin or dev.py)
-- `scipy Meson doc
-  <https://scipy.github.io/devdocs/building/understanding_meson.html>`_ gives
-  more background about how Meson works behind the scenes
-
 .. _platform_specific_instructions:
 
 Platform-specific instructions
 ==============================
 
 Here are instructions to install a working C/C++ compiler with OpenMP support
 to build scikit-learn Cython extensions for each supported platform.
@@ -327,19 +228,21 @@
 
 Replace ``x64`` by ``x86`` to build for 32-bit Python.
 
 Please be aware that the path above might be different from user to user. The
 aim is to point to the "vcvarsall.bat" file that will set the necessary
 environment variables in the current command prompt.
 
-Finally, build scikit-learn from this command prompt:
+Finally, build scikit-learn with this command prompt:
 
 .. prompt:: bash $
 
-    pip install -v --no-use-pep517 --no-build-isolation -e .
+    pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 .. _compiler_macos:
 
 macOS
 -----
 
 The default C compiler on macOS, Apple clang (confusingly aliased as
@@ -370,24 +273,26 @@
 
 It is recommended to use a dedicated `conda environment`_ to build
 scikit-learn from source:
 
 .. prompt:: bash $
 
     conda create -n sklearn-dev -c conda-forge python numpy scipy cython \
-        joblib threadpoolctl pytest compilers llvm-openmp
+        joblib threadpoolctl pytest compilers llvm-openmp meson-python ninja
 
 It is not always necessary but it is safer to open a new prompt before
 activating the newly created conda environment.
 
 .. prompt:: bash $
 
     conda activate sklearn-dev
     make clean
-    pip install -v --no-use-pep517 --no-build-isolation -e .
+    pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 .. note::
 
     If you get any conflicting dependency error message, try commenting out
     any custom conda configuration in the ``$HOME/.condarc`` file. In
     particular the ``channel_priority: strict`` directive is known to cause
     problems for this setup.
@@ -453,15 +358,17 @@
 
 Finally, build scikit-learn in verbose mode (to check for the presence of the
 ``-fopenmp`` flag in the compiler commands):
 
 .. prompt:: bash $
 
     make clean
-    pip install -v --no-use-pep517 --no-build-isolation -e .
+    pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 .. _compiler_linux:
 
 Linux
 -----
 
 Linux compilers from the system
@@ -479,15 +386,17 @@
     sudo apt-get install build-essential python3-dev python3-pip
 
 then proceed as usual:
 
 .. prompt:: bash $
 
     pip3 install cython
-    pip3 install --verbose --editable .
+    pip3 install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 Cython and the pre-compiled wheels for the runtime dependencies (numpy, scipy
 and joblib) should automatically be installed in
 ``$HOME/.local/lib/pythonX.Y/site-packages``. Alternatively you can run the
 above commands from a virtualenv_ or a `conda environment`_ to get full
 isolation from the Python packages installed via the system packager. When
 using an isolated environment, ``pip3`` should be replaced by ``pip`` in the
@@ -511,23 +420,25 @@
 
 Alternatively, install a recent version of the GNU C Compiler toolchain (GCC)
 in the user folder using conda:
 
 .. prompt:: bash $
 
     conda create -n sklearn-dev -c conda-forge python numpy scipy cython \
-        joblib threadpoolctl pytest compilers
+        joblib threadpoolctl pytest compilers meson-python ninja
 
 It is not always necessary but it is safer to open a new prompt before
 activating the newly created conda environment.
 
 .. prompt:: bash $
 
     conda activate sklearn-dev
-    pip install -v --no-use-pep517 --no-build-isolation -e .
+    pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 .. _compiler_freebsd:
 
 FreeBSD
 -------
 
 The clang compiler included in FreeBSD 12.0 and 11.2 base systems does not
@@ -548,36 +459,42 @@
     export CXXFLAGS="$CXXFLAGS -I/usr/local/include"
     export LDFLAGS="$LDFLAGS -Wl,-rpath,/usr/local/lib -L/usr/local/lib -lomp"
 
 Finally, build the package using the standard command:
 
 .. prompt:: bash $
 
-    pip install -v --no-use-pep517 --no-build-isolation -e .
+    pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 For the upcoming FreeBSD 12.1 and 11.3 versions, OpenMP will be included in
 the base system and these steps will not be necessary.
 
 .. _OpenMP: https://en.wikipedia.org/wiki/OpenMP
 .. _Cython: https://cython.org
+.. _meson-python: https://mesonbuild.com/meson-python
+.. _Ninja: https://ninja-build.org/
 .. _NumPy: https://numpy.org
 .. _SciPy: https://www.scipy.org
 .. _Homebrew: https://brew.sh
 .. _virtualenv: https://docs.python.org/3/tutorial/venv.html
 .. _conda environment: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html
 .. _Miniforge3: https://github.com/conda-forge/miniforge#miniforge3
 
 Alternative compilers
 =====================
 
 The following command will build scikit-learn using your default C/C++ compiler.
 
 .. prompt:: bash $
 
-    pip install --verbose --editable .
+    pip install --editable . \
+        --verbose --no-build-isolation \
+        --config-settings editable-verbose=true
 
 If you want to build scikit-learn with another compiler handled by ``setuptools``,
 use the following command:
 
 .. prompt:: bash $
 
     python setup.py build_ext --compiler=<compiler> -i build_clib --compiler=<compiler>
@@ -600,21 +517,7 @@
 
     import sysconfig
     print(sysconfig.get_config_var('CC'))
     print(sysconfig.get_config_var('LDFLAGS'))
 
 In addition, since Scikit-learn uses OpenMP, you need to include the appropriate OpenMP
 flag of your compiler into the ``CFLAGS`` and ``CPPFLAGS`` environment variables.
-
-Parallel builds
-===============
-
-It is possible to build scikit-learn compiled extensions in parallel by setting
-and environment variable as follows before calling the ``pip install`` or
-``python setup.py build_ext`` commands::
-
-    export SKLEARN_BUILD_PARALLEL=3
-    pip install -v --no-use-pep517 --no-build-isolation -e .
-
-On a machine with 2 CPU cores, it can be beneficial to use a parallelism level
-of 3 to overlap IO bound tasks (reading and writing files on disk) with CPU
-bound tasks (actually compiling).
```

### Comparing `scikit-learn-1.4.2/doc/developers/bug_triaging.rst` & `scikit_learn-1.5.0rc1/doc/developers/bug_triaging.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/developers/contributing.rst` & `scikit_learn-1.5.0rc1/doc/developers/contributing.rst`

 * *Files 1% similar despite different names*

```diff
@@ -78,15 +78,17 @@
 In case a contribution/issue involves changes to the API principles
 or changes to dependencies or supported versions, it must be backed by a
 :ref:`slep`, where a SLEP must be submitted as a pull-request to
 `enhancement proposals <https://scikit-learn-enhancement-proposals.readthedocs.io>`_
 using the `SLEP template <https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep_template.html>`_
 and follows the decision-making process outlined in :ref:`governance`.
 
-.. topic:: Contributing to related projects
+|details-start|
+**Contributing to related projects**
+|details-split|
 
    Scikit-learn thrives in an ecosystem of several related projects, which also
    may have relevant issues to work on, including smaller projects such as:
 
    * `scikit-learn-contrib <https://github.com/search?q=org%3Ascikit-learn-contrib+is%3Aissue+is%3Aopen+sort%3Aupdated-desc&type=Issues>`__
    * `joblib <https://github.com/joblib/joblib/issues>`__
    * `sphinx-gallery <https://github.com/sphinx-gallery/sphinx-gallery/issues>`__
@@ -100,14 +102,15 @@
    * `matplotlib <https://github.com/matplotlib/matplotlib/issues>`__
    * and so on.
 
    Look for issues marked "help wanted" or similar.
    Helping these projects may help Scikit-learn too.
    See also :ref:`related_projects`.
 
+|details-end|
 
 Submitting a bug report or a feature request
 ============================================
 
 We use GitHub issues to track all bugs and feature requests; feel free to open
 an issue if you have found a bug or wish to see a feature implemented.
 
@@ -250,15 +253,15 @@
 4. Follow steps 2-6 in :ref:`install_bleeding_edge` to build scikit-learn in
    development mode and return to this document.
 
 5. Install the development dependencies:
 
    .. prompt:: bash $
 
-        pip install pytest pytest-cov ruff mypy numpydoc black==23.3.0
+        pip install pytest pytest-cov ruff mypy numpydoc black==24.3.0
 
 .. _upstream:
 
 6. Add the ``upstream`` remote. This saves a reference to the main
    scikit-learn repository, which you can use to keep your repository
    synchronized with the latest changes:
 
@@ -327,26 +330,14 @@
 
 12. Follow `these
     <https://help.github.com/articles/creating-a-pull-request-from-a-fork>`_
     instructions to create a pull request from your fork. This will send an
     email to the committers. You may want to consider sending an email to the
     mailing list for more visibility.
 
-.. note::
-
-    If you are modifying a Cython module, you have to re-compile after
-    modifications and before testing them:
-
-    .. prompt:: bash $
-
-        pip install -v --no-use-pep517 --no-build-isolation -e .
-
-    Use the ``--no-build-isolation`` flag to avoid compiling the whole project
-    each time, only the files you have modified.
-
 It is often helpful to keep your local feature branch synchronized with the
 latest changes of the main scikit-learn repository:
 
 .. prompt:: bash $
 
     git fetch upstream
     git merge upstream/main
@@ -918,15 +909,15 @@
     packaging is not needed once setuptools starts shipping packaging>=17.0
 
 Building the documentation requires installing some additional packages:
 
 .. prompt:: bash $
 
     pip install sphinx sphinx-gallery numpydoc matplotlib Pillow pandas \
-                scikit-image packaging seaborn sphinx-prompt \
+                polars scikit-image packaging seaborn sphinx-prompt \
                 sphinxext-opengraph sphinx-copybutton plotly pooch
 
 To build the documentation, you need to be in the ``doc`` folder:
 
 .. prompt:: bash $
 
     cd doc
@@ -1002,17 +993,17 @@
 
 Running `pytest` in a folder will run all the tests of the corresponding
 subpackages. For a more detailed `pytest` workflow, please refer to the
 :ref:`pr_checklist`.
 
 We expect code coverage of new features to be at least around 90%.
 
-
-Writing matplotlib related tests
---------------------------------
+|details-start|
+**Writing matplotlib related tests**
+|details-split|
 
 Test fixtures ensure that a set of tests will be executing with the appropriate
 initialization and cleanup. The scikit-learn test suite implements a fixture
 which can be used with ``matplotlib``.
 
 ``pyplot``
     The ``pyplot`` fixture should be used when a test function is dealing with
@@ -1023,28 +1014,33 @@
 
 To use this fixture in a test function, one needs to pass it as an
 argument::
 
     def test_requiring_mpl_fixture(pyplot):
         # you can now safely use matplotlib
 
-Workflow to improve test coverage
----------------------------------
+|details-end|
+
+|details-start|
+**Workflow to improve test coverage**
+|details-split|
 
 To test code coverage, you need to install the `coverage
 <https://pypi.org/project/coverage/>`_ package in addition to pytest.
 
 1. Run 'make test-coverage'. The output lists for each file the line
     numbers that are not tested.
 
 2. Find a low hanging fruit, looking at which lines are not tested,
     write or adapt a test specifically for these lines.
 
 3. Loop.
 
+|details-end|
+
 .. _monitoring_performances:
 
 Monitoring performance
 ======================
 
 *This section is heavily inspired from the* `pandas documentation
 <https://pandas.pydata.org/docs/development/contributing_codebase.html#running-the-performance-test-suite>`_.
@@ -1230,15 +1226,15 @@
                 "Default behavior is changed from 'normalize=False' to "
                 "'normalize=True'")
     def zero_one(y_true, y_pred, normalize=False):
         return zero_one_loss(y_true, y_pred, normalize)
 
 If an attribute is to be deprecated,
 use the decorator ``deprecated`` on a property. Please note that the
-``property`` decorator should be placed before the ``deprecated``
+``deprecated`` decorator should be placed before the ``property``
 decorator for the docstrings to be rendered properly.
 E.g., renaming an attribute ``labels_`` to ``classes_`` can be done as::
 
     @deprecated("Attribute `labels_` was deprecated in version 0.13 and "
                 "will be removed in 0.15. Use `classes_` instead")
     @property
     def labels_(self):
@@ -1365,14 +1361,18 @@
   The difference between an objective improvement and a subjective nit isn't
   always clear. Reviewers should recall that code review is primarily about
   reducing risk in the project. When reviewing code, one should aim at
   preventing situations which may require a bug fix, a deprecation, or a
   retraction. Regarding docs: typos, grammar issues and disambiguations are
   better addressed immediately.
 
+|details-start|
+**Important aspects to be covered in any code review**
+|details-split|
+
 Here are a few important aspects that need to be covered in any code review,
 from high-level questions to a more detailed check-list.
 
 - Do we want this in the library? Is it likely to be used? Do you, as
   a scikit-learn user, like the change and intend to use it? Is it in
   the scope of scikit-learn? Will the cost of maintaining a new
   feature be worth its benefits?
@@ -1414,18 +1414,21 @@
 
 - Does the documentation render properly (see the
   :ref:`contribute_documentation` section for more details), and are the plots
   instructive?
 
 :ref:`saved_replies` includes some frequent comments that reviewers may make.
 
+|details-end|
+
 .. _communication:
 
-Communication Guidelines
-------------------------
+|details-start|
+**Communication Guidelines**
+|details-split|
 
 Reviewing open pull requests (PRs) helps move the project forward. It is a
 great way to get familiar with the codebase and should motivate the
 contributor to keep involved in the project. [1]_
 
 - Every PR, good or bad, is an act of generosity. Opening with a positive
   comment will help the author feel rewarded, and your subsequent remarks may
@@ -1446,14 +1449,16 @@
   suggestions.
 - You are the face of the project. Bad days occur to everyone, in that
   occasion you deserve a break: try to take your time and stay offline.
 
 .. [1] Adapted from the numpy `communication guidelines
        <https://numpy.org/devdocs/dev/reviewer_guidelines.html#communication-guidelines>`_.
 
+|details-end|
+
 Reading the existing code base
 ==============================
 
 Reading and digesting an existing code base is always a difficult exercise
 that takes time and experience to master. Even though we try to write simple
 code in general, understanding the code can seem overwhelming at first,
 given the sheer size of the project. Here is a list of tips that may help
```

#### html2text {}

```diff
@@ -37,32 +37,34 @@
 .. raw:: html _S_t_a_r
 In case a contribution/issue involves changes to the API principles or changes
 to dependencies or supported versions, it must be backed by a :ref:`slep`,
 where a SLEP must be submitted as a pull-request to `enhancement proposals
 scikit-learn-enhancement-proposals.readthedocs.io>`_ using the `SLEP template
 scikit-learn-enhancement-proposals.readthedocs.io/en/latest/
 slep_template.html>`_ and follows the decision-making process outlined in :ref:
-`governance`. .. topic:: Contributing to related projects Scikit-learn thrives
-in an ecosystem of several related projects, which also may have relevant
-issues to work on, including smaller projects such as: * `scikit-learn-contrib
+`governance`. |details-start| **Contributing to related projects** |details-
+split| Scikit-learn thrives in an ecosystem of several related projects, which
+also may have relevant issues to work on, including smaller projects such as: *
+`scikit-learn-contrib
 github.com/search?q=org%3Ascikit-learn-
 contrib+is%3Aissue+is%3Aopen+sort%3Aupdated-desc&type=Issues>`__ * `joblib
 github.com/joblib/joblib/issues>`__ * `sphinx-gallery
 github.com/sphinx-gallery/sphinx-gallery/issues>`__ * `numpydoc
 github.com/numpy/numpydoc/issues>`__ * `liac-arff
 github.com/renatopp/liac-arff/issues>`__ and larger projects: * `numpy
 github.com/numpy/numpy/issues>`__ * `scipy
 github.com/scipy/scipy/issues>`__ * `matplotlib
 github.com/matplotlib/matplotlib/issues>`__ * and so on. Look for issues marked
 "help wanted" or similar. Helping these projects may help Scikit-learn too. See
-also :ref:`related_projects`. Submitting a bug report or a feature request
-============================================ We use GitHub issues to track all
-bugs and feature requests; feel free to open an issue if you have found a bug
-or wish to see a feature implemented. In case you experience issues using this
-package, do not hesitate to submit a ticket to the `Bug Tracker
+also :ref:`related_projects`. |details-end| Submitting a bug report or a
+feature request ============================================ We use GitHub
+issues to track all bugs and feature requests; feel free to open an issue if
+you have found a bug or wish to see a feature implemented. In case you
+experience issues using this package, do not hesitate to submit a ticket to the
+`Bug Tracker
 github.com/scikit-learn/scikit-learn/issues>`_. You are also welcome to post
 feature requests or pull requests. It is recommended to check that your issue
 complies with the following rules before submitting: - Verify that your issue
 is not being currently addressed by other `issues
 github.com/scikit-learn/scikit-learn/issues?q=>`_ or `pull requests
 github.com/scikit-learn/scikit-learn/pulls?q=>`_. - If you are submitting an
 algorithm or feature request, please verify that the algorithm fulfills our
@@ -137,15 +139,15 @@
 guide
 help.github.com/articles/fork-a-repo/>`_. 3. Clone your fork of the scikit-
 learn repo from your GitHub account to your local disk: .. prompt:: bash $ git
 clone git@github.com:YourLogin/scikit-learn.git # add --depth 1 if your
 connection is slow cd scikit-learn 4. Follow steps 2-6 in :ref:
 `install_bleeding_edge` to build scikit-learn in development mode and return to
 this document. 5. Install the development dependencies: .. prompt:: bash $ pip
-install pytest pytest-cov ruff mypy numpydoc black==23.3.0 .. _upstream: 6. Add
+install pytest pytest-cov ruff mypy numpydoc black==24.3.0 .. _upstream: 6. Add
 the ``upstream`` remote. This saves a reference to the main scikit-learn
 repository, which you can use to keep your repository synchronized with the
 latest changes: .. prompt:: bash $ git remote add upstream git@github.com:
 scikit-learn/scikit-learn.git 7. Check that the `upstream` and `origin` remote
 aliases are configured correctly by running `git remote -v` which should
 display:: origin git@github.com:YourLogin/scikit-learn.git (fetch) origin
 git@github.com:YourLogin/scikit-learn.git (push) upstream git@github.com:
@@ -169,19 +171,15 @@
 control. When you're done editing, add changed files using ``git add`` and then
 ``git commit``: .. prompt:: bash $ git add modified_files git commit to record
 your changes in Git, then push the changes to your GitHub account with: ..
 prompt:: bash $ git push -u origin my_feature 12. Follow `these
 help.github.com/articles/creating-a-pull-request-from-a-fork>`_ instructions to
 create a pull request from your fork. This will send an email to the
 committers. You may want to consider sending an email to the mailing list for
-more visibility. .. note:: If you are modifying a Cython module, you have to
-re-compile after modifications and before testing them: .. prompt:: bash $ pip
-install -v --no-use-pep517 --no-build-isolation -e . Use the ``--no-build-
-isolation`` flag to avoid compiling the whole project each time, only the files
-you have modified. It is often helpful to keep your local feature branch
+more visibility. It is often helpful to keep your local feature branch
 synchronized with the latest changes of the main scikit-learn repository: ..
 prompt:: bash $ git fetch upstream git merge upstream/main Subsequently, you
 might need to solve the conflicts. You can refer to the `Git documentation
 related to resolving merge conflict using the command line
 help.github.com/articles/resolving-a-merge-conflict-using-the-command-line/>`_.
 .. topic:: Learning git: The `Git documentation
 git-scm.com/documentation>`_ and http://try.github.io are excellent resources
@@ -483,24 +481,24 @@
 _building_documentation: Building the documentation -------------------------
 - **Before submitting a pull request check if your modifications have
 introduced new sphinx warnings by building the documentation locally and try to
 fix them.** First, make sure you have :ref:`properly installed ` the
 development version. .. packaging is not needed once setuptools starts shipping
 packaging>=17.0 Building the documentation requires installing some additional
 packages: .. prompt:: bash $ pip install sphinx sphinx-gallery numpydoc
-matplotlib Pillow pandas \ scikit-image packaging seaborn sphinx-prompt \
-sphinxext-opengraph sphinx-copybutton plotly pooch To build the documentation,
-you need to be in the ``doc`` folder: .. prompt:: bash $ cd doc In the vast
-majority of cases, you only need to generate the full web site, without the
-example gallery: .. prompt:: bash $ make The documentation will be generated in
-the ``_build/html/stable`` directory and are viewable in a web browser, for
-instance by opening the local ``_build/html/stable/index.html`` file. To also
-generate the example gallery you can use: .. prompt:: bash $ make html This
-will run all the examples, which takes a while. If you only want to generate a
-few examples, you can use: .. prompt:: bash $
+matplotlib Pillow pandas \ polars scikit-image packaging seaborn sphinx-prompt
+\ sphinxext-opengraph sphinx-copybutton plotly pooch To build the
+documentation, you need to be in the ``doc`` folder: .. prompt:: bash $ cd doc
+In the vast majority of cases, you only need to generate the full web site,
+without the example gallery: .. prompt:: bash $ make The documentation will be
+generated in the ``_build/html/stable`` directory and are viewable in a web
+browser, for instance by opening the local ``_build/html/stable/index.html``
+file. To also generate the example gallery you can use: .. prompt:: bash $ make
+html This will run all the examples, which takes a while. If you only want to
+generate a few examples, you can use: .. prompt:: bash $
 EXAMPLES_PATTERN=your_regex_goes_here make html This is particularly useful if
 you are modifying a few examples. Set the environment variable `NO_MATHJAX=1`
 if you intend to view the documentation in an offline setting. To build the PDF
 manual, run: .. prompt:: bash $ make latexpdf .. warning:: **Sphinx version**
 While we do our best to have the documentation build under as many versions of
 Sphinx as possible, the different versions tend to behave slightly differently.
 To get the best results, you should use the same version as the one we used on
@@ -518,31 +516,31 @@
 en.wikipedia.org/wiki/Unit_testing>`_ is a corner-stone of the scikit-learn
 development process. For this purpose, we use the `pytest
 docs.pytest.org>`_ package. The tests are functions appropriately named,
 located in `tests` subdirectories, that check the validity of the algorithms
 and the different options of the code. Running `pytest` in a folder will run
 all the tests of the corresponding subpackages. For a more detailed `pytest`
 workflow, please refer to the :ref:`pr_checklist`. We expect code coverage of
-new features to be at least around 90%. Writing matplotlib related tests ------
--------------------------- Test fixtures ensure that a set of tests will be
-executing with the appropriate initialization and cleanup. The scikit-learn
+new features to be at least around 90%. |details-start| **Writing matplotlib
+related tests** |details-split| Test fixtures ensure that a set of tests will
+be executing with the appropriate initialization and cleanup. The scikit-learn
 test suite implements a fixture which can be used with ``matplotlib``.
 ``pyplot`` The ``pyplot`` fixture should be used when a test function is
 dealing with ``matplotlib``. ``matplotlib`` is a soft dependency and is not
 required. This fixture is in charge of skipping the tests if ``matplotlib`` is
 not installed. In addition, figures created during the tests will be
 automatically closed once the test function has been executed. To use this
 fixture in a test function, one needs to pass it as an argument:: def
 test_requiring_mpl_fixture(pyplot): # you can now safely use matplotlib
-Workflow to improve test coverage --------------------------------- To test
-code coverage, you need to install the `coverage
+|details-end| |details-start| **Workflow to improve test coverage** |details-
+split| To test code coverage, you need to install the `coverage
 pypi.org/project/coverage/>`_ package in addition to pytest. 1. Run 'make test-
 coverage'. The output lists for each file the line numbers that are not tested.
 2. Find a low hanging fruit, looking at which lines are not tested, write or
-adapt a test specifically for these lines. 3. Loop. ..
+adapt a test specifically for these lines. 3. Loop. |details-end| ..
 _monitoring_performances: Monitoring performance ====================== *This
 section is heavily inspired from the* `pandas documentation
 pandas.pydata.org/docs/development/contributing_codebase.html#running-the-
 performance-test-suite>`_. When proposing changes to the existing code base,
 it's important to make sure that they don't introduce performance regressions.
 Scikit-learn uses `asv benchmarks
 github.com/airspeed-velocity/asv>`_ to monitor the performance of a selection
@@ -616,16 +614,16 @@
 and call ``zero_one_loss`` from that function:: from ..utils import deprecated
 def zero_one_loss(y_true, y_pred, normalize=True): # actual implementation pass
 @deprecated("Function 'zero_one' was renamed to 'zero_one_loss' " "in version
 0.13 and will be removed in release 0.15. " "Default behavior is changed from
 'normalize=False' to " "'normalize=True'") def zero_one(y_true, y_pred,
 normalize=False): return zero_one_loss(y_true, y_pred, normalize) If an
 attribute is to be deprecated, use the decorator ``deprecated`` on a property.
-Please note that the ``property`` decorator should be placed before the
-``deprecated`` decorator for the docstrings to be rendered properly. E.g.,
+Please note that the ``deprecated`` decorator should be placed before the
+``property`` decorator for the docstrings to be rendered properly. E.g.,
 renaming an attribute ``labels_`` to ``classes_`` can be done as:: @deprecated
 ("Attribute `labels_` was deprecated in version 0.13 and " "will be removed in
 0.15. Use `classes_` instead") @property def labels_(self): return
 self.classes_ If a parameter has to be deprecated, a ``FutureWarning`` warning
 must be raised too. In the following example, k is deprecated and renamed to
 n_clusters:: import warnings def example_function(n_clusters=8,
 k='deprecated'): if k != 'deprecated': warnings.warn("'k' was renamed to
@@ -682,25 +680,26 @@
 critically about whether the PR meets your needs. While each pull request needs
 to be signed off by two core developers, you can speed up this process by
 providing your feedback. .. note:: The difference between an objective
 improvement and a subjective nit isn't always clear. Reviewers should recall
 that code review is primarily about reducing risk in the project. When
 reviewing code, one should aim at preventing situations which may require a bug
 fix, a deprecation, or a retraction. Regarding docs: typos, grammar issues and
-disambiguations are better addressed immediately. Here are a few important
-aspects that need to be covered in any code review, from high-level questions
-to a more detailed check-list. - Do we want this in the library? Is it likely
-to be used? Do you, as a scikit-learn user, like the change and intend to use
-it? Is it in the scope of scikit-learn? Will the cost of maintaining a new
-feature be worth its benefits? - Is the code consistent with the API of scikit-
-learn? Are public functions/classes/parameters well named and intuitively
-designed? - Are all public functions/classes and their parameters, return
-types, and stored attributes named according to scikit-learn conventions and
-documented clearly? - Is any new functionality described in the user-guide and
-illustrated with examples? - Is every public function/class tested? Are a
+disambiguations are better addressed immediately. |details-start| **Important
+aspects to be covered in any code review** |details-split| Here are a few
+important aspects that need to be covered in any code review, from high-level
+questions to a more detailed check-list. - Do we want this in the library? Is
+it likely to be used? Do you, as a scikit-learn user, like the change and
+intend to use it? Is it in the scope of scikit-learn? Will the cost of
+maintaining a new feature be worth its benefits? - Is the code consistent with
+the API of scikit-learn? Are public functions/classes/parameters well named and
+intuitively designed? - Are all public functions/classes and their parameters,
+return types, and stored attributes named according to scikit-learn conventions
+and documented clearly? - Is any new functionality described in the user-guide
+and illustrated with examples? - Is every public function/class tested? Are a
 reasonable set of parameters, their values, value types, and combinations
 tested? Do the tests validate that the code is correct, i.e. doing what the
 documentation says it does? If the change is a bug-fix, is a non-regression
 test included? Look at `this
 jeffknupp.com/blog/2013/12/09/improve-your-python-understanding-unit-
 testing>`__ to get started with testing in Python. - Do the tests pass in the
 continuous integration build? If appropriate, help the contributor understand
@@ -711,52 +710,52 @@
 comments be removed as unhelpful or extraneous? - Could the code easily be
 rewritten to run much more efficiently for relevant settings? - Is the code
 backwards compatible with previous versions? (or is a deprecation cycle
 necessary?) - Will the new code add any dependencies on other libraries? (this
 is unlikely to be accepted) - Does the documentation render properly (see the :
 ref:`contribute_documentation` section for more details), and are the plots
 instructive? :ref:`saved_replies` includes some frequent comments that
-reviewers may make. .. _communication: Communication Guidelines ---------------
---------- Reviewing open pull requests (PRs) helps move the project forward. It
-is a great way to get familiar with the codebase and should motivate the
-contributor to keep involved in the project. [1]_ - Every PR, good or bad, is
-an act of generosity. Opening with a positive comment will help the author feel
-rewarded, and your subsequent remarks may be heard more clearly. You may feel
-good also. - Begin if possible with the large issues, so the author knows
-they've been understood. Resist the temptation to immediately go line by line,
-or to open with small pervasive issues. - Do not let perfect be the enemy of
-the good. If you find yourself making many small suggestions that don't fall
-into the :ref:`code_review`, consider the following approaches: - refrain from
-submitting these; - prefix them as "Nit" so that the contributor knows it's OK
-not to address; - follow up in a subsequent PR, out of courtesy, you may want
-to let the original contributor know. - Do not rush, take the time to make your
-comments clear and justify your suggestions. - You are the face of the project.
-Bad days occur to everyone, in that occasion you deserve a break: try to take
-your time and stay offline. .. [1] Adapted from the numpy `communication
-guidelines
+reviewers may make. |details-end| .. _communication: |details-start|
+**Communication Guidelines** |details-split| Reviewing open pull requests (PRs)
+helps move the project forward. It is a great way to get familiar with the
+codebase and should motivate the contributor to keep involved in the project.
+[1]_ - Every PR, good or bad, is an act of generosity. Opening with a positive
+comment will help the author feel rewarded, and your subsequent remarks may be
+heard more clearly. You may feel good also. - Begin if possible with the large
+issues, so the author knows they've been understood. Resist the temptation to
+immediately go line by line, or to open with small pervasive issues. - Do not
+let perfect be the enemy of the good. If you find yourself making many small
+suggestions that don't fall into the :ref:`code_review`, consider the following
+approaches: - refrain from submitting these; - prefix them as "Nit" so that the
+contributor knows it's OK not to address; - follow up in a subsequent PR, out
+of courtesy, you may want to let the original contributor know. - Do not rush,
+take the time to make your comments clear and justify your suggestions. - You
+are the face of the project. Bad days occur to everyone, in that occasion you
+deserve a break: try to take your time and stay offline. .. [1] Adapted from
+the numpy `communication guidelines
 numpy.org/devdocs/dev/reviewer_guidelines.html#communication-guidelines>`_.
-Reading the existing code base ============================== Reading and
-digesting an existing code base is always a difficult exercise that takes time
-and experience to master. Even though we try to write simple code in general,
-understanding the code can seem overwhelming at first, given the sheer size of
-the project. Here is a list of tips that may help make this task easier and
-faster (in no particular order). - Get acquainted with the :ref:`api_overview`:
-understand what :term:`fit`, :term:`predict`, :term:`transform`, etc. are used
-for. - Before diving into reading the code of a function / class, go through
-the docstrings first and try to get an idea of what each parameter / attribute
-is doing. It may also help to stop a minute and think *how would I do this
-myself if I had to?* - The trickiest thing is often to identify which portions
-of the code are relevant, and which are not. In scikit-learn **a lot** of input
-checking is performed, especially at the beginning of the :term:`fit` methods.
-Sometimes, only a very small portion of the code is doing the actual job. For
-example looking at the ``fit()`` method of :class:
-`~linear_model.LinearRegression`, what you're looking for might just be the
-call the ``scipy.linalg.lstsq``, but it is buried into multiple lines of input
-checking and the handling of different kinds of parameters. - Due to the use of
-`Inheritance
+|details-end| Reading the existing code base ==============================
+Reading and digesting an existing code base is always a difficult exercise that
+takes time and experience to master. Even though we try to write simple code in
+general, understanding the code can seem overwhelming at first, given the sheer
+size of the project. Here is a list of tips that may help make this task easier
+and faster (in no particular order). - Get acquainted with the :ref:
+`api_overview`: understand what :term:`fit`, :term:`predict`, :term:
+`transform`, etc. are used for. - Before diving into reading the code of a
+function / class, go through the docstrings first and try to get an idea of
+what each parameter / attribute is doing. It may also help to stop a minute and
+think *how would I do this myself if I had to?* - The trickiest thing is often
+to identify which portions of the code are relevant, and which are not. In
+scikit-learn **a lot** of input checking is performed, especially at the
+beginning of the :term:`fit` methods. Sometimes, only a very small portion of
+the code is doing the actual job. For example looking at the ``fit()`` method
+of :class:`~linear_model.LinearRegression`, what you're looking for might just
+be the call the ``scipy.linalg.lstsq``, but it is buried into multiple lines of
+input checking and the handling of different kinds of parameters. - Due to the
+use of `Inheritance
 en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)>`_, some
 methods may be implemented in parent classes. All estimators inherit at least
 from :class:`~base.BaseEstimator`, and from a ``Mixin`` class (e.g. :class:
 `~base.ClassifierMixin`) that enables default behaviour depending on the nature
 of the estimator (classifier, regressor, transformer, etc.). - Sometimes,
 reading the tests for a given function will give you an idea of what its
 intended purpose is. You can use ``git grep`` (see below) to find all the tests
```

### Comparing `scikit-learn-1.4.2/doc/developers/cython.rst` & `scikit_learn-1.5.0rc1/doc/developers/cython.rst`

 * *Files 7% similar despite different names*

```diff
@@ -137,7 +137,20 @@
 
    from sklearn.utils._openmp_helpers cimport omp_get_max_threads
    max_threads = omp_get_max_threads()
 
 
 The parallel loop, `prange`, is already protected by cython and can be used directly
 from `cython.parallel`.
+
+Types
+~~~~~
+
+Cython code requires to use explicit types. This is one of the reasons you get a
+performance boost. In order to avoid code duplication, we have a central place
+for the most used types in
+`sklearn/utils/_typedefs.pyd <https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_typedefs.pyd>`_.
+Ideally you start by having a look there and `cimport` types you need, for example
+
+.. code-block:: cython
+
+    from sklear.utils._typedefs cimport float32, float64
```

### Comparing `scikit-learn-1.4.2/doc/developers/develop.rst` & `scikit_learn-1.5.0rc1/doc/developers/develop.rst`

 * *Files 0% similar despite different names*

```diff
@@ -50,15 +50,15 @@
     Classification algorithms usually also offer a way to quantify certainty
     of a prediction, either using ``decision_function`` or ``predict_proba``::
 
       probability = predictor.predict_proba(data)
 
 :Transformer:
 
-    For modifying the data in a supervised or unsupervised way (e.g. by adding, changing, 
+    For modifying the data in a supervised or unsupervised way (e.g. by adding, changing,
     or removing columns, but not by adding or removing rows). Implements::
 
       new_data = transformer.transform(data)
 
     When fitting and transforming can be performed much more efficiently
     together than separately, implements::
 
@@ -278,20 +278,24 @@
     you can prevent a lot of boilerplate code
     by deriving a class from ``BaseEstimator``
     and optionally the mixin classes in ``sklearn.base``.
     For example, below is a custom classifier, with more examples included
     in the scikit-learn-contrib
     `project template <https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py>`__.
 
+    It is particularly important to notice that mixins should be "on the left" while
+    the ``BaseEstimator`` should be "on the right" in the inheritance list for proper
+    MRO.
+
       >>> import numpy as np
       >>> from sklearn.base import BaseEstimator, ClassifierMixin
       >>> from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
       >>> from sklearn.utils.multiclass import unique_labels
       >>> from sklearn.metrics import euclidean_distances
-      >>> class TemplateClassifier(BaseEstimator, ClassifierMixin):
+      >>> class TemplateClassifier(ClassifierMixin, BaseEstimator):
       ...
       ...     def __init__(self, demo_param='demo'):
       ...         self.demo_param = demo_param
       ...
       ...     def fit(self, X, y):
       ...
       ...         # Check that X and y have correct shape
@@ -345,15 +349,15 @@
     subestimator__C -> 1.0
     subestimator__class_weight -> None
     subestimator__dual -> False
     subestimator__fit_intercept -> True
     subestimator__intercept_scaling -> 1
     subestimator__l1_ratio -> None
     subestimator__max_iter -> 100
-    subestimator__multi_class -> auto
+    subestimator__multi_class -> deprecated
     subestimator__n_jobs -> None
     subestimator__penalty -> l2
     subestimator__random_state -> None
     subestimator__solver -> lbfgs
     subestimator__tol -> 0.0001
     subestimator__verbose -> 0
     subestimator__warm_start -> False
```

### Comparing `scikit-learn-1.4.2/doc/developers/maintainer.rst` & `scikit_learn-1.5.0rc1/doc/developers/maintainer.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/developers/minimal_reproducer.rst` & `scikit_learn-1.5.0rc1/doc/developers/minimal_reproducer.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/developers/performance.rst` & `scikit_learn-1.5.0rc1/doc/developers/performance.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/developers/plotting.rst` & `scikit_learn-1.5.0rc1/doc/developers/plotting.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/developers/tips.rst` & `scikit_learn-1.5.0rc1/doc/developers/tips.rst`

 * *Files 3% similar despite different names*

```diff
@@ -351,7 +351,23 @@
 tools and dependencies using apt or conda as usual). Building scikit-learn
 takes a lot of time because of the emulation layer, however it needs to be
 done only once if you put the scikit-learn folder under the `/io` mount
 point.
 
 Then use pytest to run only the tests of the module you are interested in
 debugging.
+
+.. _meson_build_backend:
+
+The Meson Build Backend
+=======================
+
+Since scikit-learn 1.5.0 we use meson-python as the build tool. Meson is
+a new tool for scikit-learn and the PyData ecosystem. It is used by several
+other packages that have written good guides about what it is and how it works.
+
+- `pandas setup doc
+  <https://pandas.pydata.org/docs/development/contributing_environment.html#step-3-build-and-install-pandas>`_:
+  pandas has a similar setup as ours (no spin or dev.py)
+- `scipy Meson doc
+  <https://scipy.github.io/devdocs/building/understanding_meson.html>`_ gives
+  more background about how Meson works behind the scenes
```

### Comparing `scikit-learn-1.4.2/doc/developers/utilities.rst` & `scikit_learn-1.5.0rc1/doc/developers/utilities.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/documentation_team.rst` & `scikit_learn-1.5.0rc1/doc/documentation_team.rst`

 * *Files 10% similar despite different names*

```diff
@@ -9,8 +9,12 @@
     <a href='https://github.com/ArturoAmorQ'><img src='https://avatars.githubusercontent.com/u/86408019?v=4' class='avatar' /></a> <br />
     <p>Arturo Amor</p>
     </div>
     <div>
     <a href='https://github.com/lucyleeow'><img src='https://avatars.githubusercontent.com/u/23182829?v=4' class='avatar' /></a> <br />
     <p>Lucy Liu</p>
     </div>
+    <div>
+    <a href='https://github.com/Charlie-XIAO'><img src='https://avatars.githubusercontent.com/u/108576690?v=4' class='avatar' /></a> <br />
+    <p>Yao Xiao</p>
+    </div>
     </div>
```

#### html2text {}

```diff
@@ -1,5 +1,7 @@
 .. raw :: html
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_8_6_4_0_8_0_1_9_?_v_=_4_]
 Arturo Amor
 _[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_2_3_1_8_2_8_2_9_?_v_=_4_]
 Lucy Liu
+_[_h_t_t_p_s_:_/_/_a_v_a_t_a_r_s_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_u_/_1_0_8_5_7_6_6_9_0_?_v_=_4_]
+Yao Xiao
```

### Comparing `scikit-learn-1.4.2/doc/faq.rst` & `scikit_learn-1.5.0rc1/doc/faq.rst`

 * *Files 2% similar despite different names*

```diff
@@ -36,14 +36,24 @@
 `PyPy <https://pypy.org/>`_ (an alternative Python implementation with
 a built-in just-in-time compiler).
 
 Note however that this support is still considered experimental and specific
 components might behave slightly differently. Please refer to the test
 suite of the specific module of interest for more details.
 
+How can I obtain permission to use the images in scikit-learn for my work?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+The images contained in the `scikit-learn repository
+<https://github.com/scikit-learn/scikit-learn>`_ and the images generated within
+the `scikit-learn documentation <https://scikit-learn.org/stable/index.html>`_
+can be used via the `BSD 3-Clause License
+<https://github.com/scikit-learn/scikit-learn?tab=BSD-3-Clause-1-ov-file>`_ for
+your work. Citations of scikit-learn are highly encouraged and appreciated. See
+:ref:`citing scikit-learn <citing-scikit-learn>`.
 
 Implementation decisions
 ------------------------
 
 Why is there no support for deep or reinforcement learning? Will there be such support in the future?
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
@@ -333,17 +343,14 @@
 
 If your problem raises an exception that you do not understand (even after googling it),
 please make sure to include the full traceback that you obtain when running the
 reproduction script.
 
 For bug reports or feature requests, please make use of the
 `issue tracker on GitHub <https://github.com/scikit-learn/scikit-learn/issues>`_.
-There is also a `scikit-learn Gitter channel
-<https://gitter.im/scikit-learn/scikit-learn>`_ where some users and developers
-might be found.
 
 .. warning::
   Please do not email any authors directly to ask for assistance, report bugs,
   or for any other issue related to scikit-learn.
 
 How should I save, export or deploy estimators for production?
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
@@ -413,16 +420,16 @@
     >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2, algorithm='brute')  # doctest: +SKIP
     (array([0, 1]), array([ 0,  0, -1]))
 
 Note that the example above uses the third-party edit distance package
 `leven <https://pypi.org/project/leven/>`_. Similar tricks can be used,
 with some care, for tree kernels, graph kernels, etc.
 
-Why do I sometime get a crash/freeze with ``n_jobs > 1`` under OSX or Linux?
-^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Why do I sometimes get a crash/freeze with ``n_jobs > 1`` under OSX or Linux?
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 Several scikit-learn tools such as :class:`~model_selection.GridSearchCV` and
 :class:`~model_selection.cross_val_score` rely internally on Python's
 :mod:`multiprocessing` module to parallelize execution
 onto several Python processes by passing ``n_jobs > 1`` as an argument.
 
 The problem is that Python :mod:`multiprocessing` does a ``fork`` system call
```

### Comparing `scikit-learn-1.4.2/doc/getting_started.rst` & `scikit_learn-1.5.0rc1/doc/getting_started.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/glossary.rst` & `scikit_learn-1.5.0rc1/doc/glossary.rst`

 * *Files 1% similar despite different names*

```diff
@@ -62,14 +62,15 @@
         * a list of length-k lists of numbers for some fixed length k
         * a :class:`pandas.DataFrame` with all columns numeric
         * a numeric :class:`pandas.Series`
 
         It excludes:
 
         * a :term:`sparse matrix`
+        * a sparse array
         * an iterator
         * a generator
 
         Note that *output* from scikit-learn estimators and functions (e.g.
         predictions) should generally be arrays or sparse matrices, or lists
         thereof (as in multi-output :class:`tree.DecisionTreeClassifier`'s
         ``predict_proba``). An estimator where ``predict()`` returns a list or
@@ -281,15 +282,34 @@
         Our documentation can sometimes give information about the dtype
         precision, e.g. `np.int32`, `np.int64`, etc. When the precision is
         provided, it refers to the NumPy dtype. If an arbitrary precision is
         used, the documentation will refer to dtype `integer` or `floating`.
         Note that in this case, the precision can be platform dependent.
         The `numeric` dtype refers to accepting both `integer` and `floating`.
 
-        TODO: Mention efficiency and precision issues; casting policy.
+        When it comes to choosing between 64-bit dtype (i.e. `np.float64` and
+        `np.int64`) and 32-bit dtype (i.e. `np.float32` and `np.int32`), it
+        boils down to a trade-off between efficiency and precision. The 64-bit
+        types offer more accurate results due to their lower floating-point
+        error, but demand more computational resources, resulting in slower
+        operations and increased memory usage. In contrast, 32-bit types
+        promise enhanced operation speed and reduced memory consumption, but
+        introduce a larger floating-point error. The efficiency improvement are
+        dependent on lower level optimization such as like vectorization,
+        single instruction multiple dispatch (SIMD), or cache optimization but
+        crucially on the compatibility of the algorithm in use.
+
+        Specifically, the choice of precision should account for whether the
+        employed algorithm can effectively leverage `np.float32`. Some
+        algorithms, especially certain minimization methods, are exclusively
+        coded for `np.float64`, meaning that even if `np.float32` is passed, it
+        triggers an automatic conversion back to `np.float64`. This not only
+        negates the intended computational savings but also introduces
+        additional overhead, making operations with `np.float32` unexpectedly
+        slower and more memory-intensive due to this extra conversion step.
 
     duck typing
         We try to apply `duck typing
         <https://en.wikipedia.org/wiki/Duck_typing>`_ to determine how to
         handle some input values (e.g. checking whether a given estimator is
         a classifier).  That is, we avoid using ``isinstance`` where possible,
         and rely on the presence or absence of attributes to determine an
```

### Comparing `scikit-learn-1.4.2/doc/governance.rst` & `scikit_learn-1.5.0rc1/doc/governance.rst`

 * *Files 9% similar despite different names*

```diff
@@ -54,64 +54,55 @@
 
 Core contributors that have not contributed to the project, corresponding to
 their role, in the past 12 months will be asked if they want to become emeritus
 members and recant their rights until they become active again. The list of
 members, active and emeritus (with dates at which they became active) is public
 on the scikit-learn website.
 
-The following teams form the core contributors group.
+The following teams form the core contributors group:
 
-
-Contributor Experience Team
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-The contributor experience team improves the experience of contributors by
-helping with the triage of issues and pull requests, as well as noticing any
-repeating patterns where people might struggle, and to help with improving
-those aspects of the project.
-
-To this end, they have the required permissions on github to label and close
-issues. :ref:`Their work <bug_triaging>` is crucial to improve the
-communication in the project and limit the crowding of the issue tracker.
-
-.. _communication_team:
-
-Communication team
-~~~~~~~~~~~~~~~~~~
-
-Members of the communication team help with outreach and communication
-for scikit-learn. The goal of the team is to develop public awareness of
-scikit-learn, of its features and usage, as well as branding.
-
-For this, they can operate the scikit-learn accounts on various social networks
-and produce materials. They also have the required rights to our blog
-repository and other relevant accounts and platforms.
-
-Documentation team
-~~~~~~~~~~~~~~~~~~
-
-Members of the documentation team engage with the documentation of the project
-among other things. They might also be involved in other aspects of the
-project, but their reviews on documentation contributions are considered
-authoritative, and can merge such contributions.
-
-To this end, they have permissions to merge pull requests in scikit-learn's
-repository.
-
-Maintainers
-~~~~~~~~~~~
-
-Maintainers are community members who have shown that they are dedicated to the
-continued development of the project through ongoing engagement with the
-community. They have shown they can be trusted to maintain scikit-learn with
-care. Being a maintainer allows contributors to more easily carry on with their
-project related activities by giving them direct access to the project's
-repository. Maintainers are expected to review code contributions, merge
-approved pull requests, cast votes for and against merging a pull-request,
-and to be involved in deciding major changes to the API.
+* **Contributor Experience Team**
+  The contributor experience team improves the experience of contributors by
+  helping with the triage of issues and pull requests, as well as noticing any
+  repeating patterns where people might struggle, and to help with improving
+  those aspects of the project.
+
+  To this end, they have the required permissions on github to label and close
+  issues. :ref:`Their work <bug_triaging>` is crucial to improve the
+  communication in the project and limit the crowding of the issue tracker.
+
+  .. _communication_team:
+
+* **Communication Team**
+  Members of the communication team help with outreach and communication
+  for scikit-learn. The goal of the team is to develop public awareness of
+  scikit-learn, of its features and usage, as well as branding.
+
+  For this, they can operate the scikit-learn accounts on various social networks
+  and produce materials. They also have the required rights to our blog
+  repository and other relevant accounts and platforms.
+
+* **Documentation Team**
+  Members of the documentation team engage with the documentation of the project
+  among other things. They might also be involved in other aspects of the
+  project, but their reviews on documentation contributions are considered
+  authoritative, and can merge such contributions.
+
+  To this end, they have permissions to merge pull requests in scikit-learn's
+  repository.
+
+* **Maintainers Team**
+  Maintainers are community members who have shown that they are dedicated to the
+  continued development of the project through ongoing engagement with the
+  community. They have shown they can be trusted to maintain scikit-learn with
+  care. Being a maintainer allows contributors to more easily carry on with their
+  project related activities by giving them direct access to the project's
+  repository. Maintainers are expected to review code contributions, merge
+  approved pull requests, cast votes for and against merging a pull-request,
+  and to be involved in deciding major changes to the API.
 
 Technical Committee
 -------------------
 
 The Technical Committee (TC) members are maintainers who have additional
 responsibilities to ensure the smooth running of the project. TC members are
 expected to participate in strategic planning, and approve changes to the
```

### Comparing `scikit-learn-1.4.2/doc/images/axa-small.png` & `scikit_learn-1.5.0rc1/doc/images/axa-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/axa.png` & `scikit_learn-1.5.0rc1/doc/images/axa.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/bcg.png` & `scikit_learn-1.5.0rc1/doc/images/bcg.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/beta_divergence.png` & `scikit_learn-1.5.0rc1/doc/images/beta_divergence.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/bnp-small.png` & `scikit_learn-1.5.0rc1/doc/images/bnp-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/bnp.png` & `scikit_learn-1.5.0rc1/doc/images/bnp.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/cds-logo.png` & `scikit_learn-1.5.0rc1/doc/images/cds-logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/chanel-small.png` & `scikit_learn-1.5.0rc1/doc/images/chanel-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/chanel.png` & `scikit_learn-1.5.0rc1/doc/images/chanel.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/columbia-small.png` & `scikit_learn-1.5.0rc1/doc/images/columbia-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/columbia.png` & `scikit_learn-1.5.0rc1/doc/images/columbia.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/czi_logo.svg` & `scikit_learn-1.5.0rc1/doc/images/czi_logo.svg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/dataiku-small.png` & `scikit_learn-1.5.0rc1/doc/images/dataiku-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/dataiku.png` & `scikit_learn-1.5.0rc1/doc/images/dataiku.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/digicosme.png` & `scikit_learn-1.5.0rc1/doc/images/digicosme.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/dysco.png` & `scikit_learn-1.5.0rc1/doc/images/dysco.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/fnrs-logo-small.png` & `scikit_learn-1.5.0rc1/doc/images/fnrs-logo-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/fujitsu.png` & `scikit_learn-1.5.0rc1/doc/images/fujitsu.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/generated-doc-ci.png` & `scikit_learn-1.5.0rc1/doc/images/generated-doc-ci.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/google-small.png` & `scikit_learn-1.5.0rc1/doc/images/google-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/grid_search_cross_validation.png` & `scikit_learn-1.5.0rc1/doc/images/grid_search_cross_validation.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/grid_search_workflow.png` & `scikit_learn-1.5.0rc1/doc/images/grid_search_workflow.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/huggingface_logo-noborder.png` & `scikit_learn-1.5.0rc1/doc/images/huggingface_logo-noborder.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/inria-logo.jpg` & `scikit_learn-1.5.0rc1/doc/images/inria-logo.jpg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/inria-small.png` & `scikit_learn-1.5.0rc1/doc/images/inria-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/intel-small.png` & `scikit_learn-1.5.0rc1/doc/images/intel-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/intel.png` & `scikit_learn-1.5.0rc1/doc/images/intel.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/iris.pdf` & `scikit_learn-1.5.0rc1/doc/images/iris.pdf`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/iris.svg` & `scikit_learn-1.5.0rc1/doc/images/iris.svg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/last_digit.png` & `scikit_learn-1.5.0rc1/doc/images/last_digit.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/lda_model_graph.png` & `scikit_learn-1.5.0rc1/doc/images/lda_model_graph.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/logo_APHP.png` & `scikit_learn-1.5.0rc1/doc/images/logo_APHP.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/logo_APHP_text.png` & `scikit_learn-1.5.0rc1/doc/images/logo_APHP_text.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/microsoft-small.png` & `scikit_learn-1.5.0rc1/doc/images/microsoft-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/microsoft.png` & `scikit_learn-1.5.0rc1/doc/images/microsoft.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/ml_map.png` & `scikit_learn-1.5.0rc1/doc/images/ml_map.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/multi_org_chart.png` & `scikit_learn-1.5.0rc1/doc/images/multi_org_chart.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/multilayerperceptron_network.png` & `scikit_learn-1.5.0rc1/doc/images/multilayerperceptron_network.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/no_image.png` & `scikit_learn-1.5.0rc1/doc/images/no_image.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/nvidia-small.png` & `scikit_learn-1.5.0rc1/doc/images/nvidia-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/nvidia.png` & `scikit_learn-1.5.0rc1/doc/images/nvidia.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/nyu_short_color.png` & `scikit_learn-1.5.0rc1/doc/images/nyu_short_color.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/permuted_non_predictive_feature.png` & `scikit_learn-1.5.0rc1/doc/images/permuted_non_predictive_feature.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/permuted_predictive_feature.png` & `scikit_learn-1.5.0rc1/doc/images/permuted_predictive_feature.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/plot_digits_classification.png` & `scikit_learn-1.5.0rc1/doc/images/plot_digits_classification.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/plot_face_recognition_1.png` & `scikit_learn-1.5.0rc1/doc/images/plot_face_recognition_1.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/plot_face_recognition_2.png` & `scikit_learn-1.5.0rc1/doc/images/plot_face_recognition_2.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/png-logo-inria-la-fondation.png` & `scikit_learn-1.5.0rc1/doc/images/png-logo-inria-la-fondation.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/probabl.png` & `scikit_learn-1.5.0rc1/doc/images/probabl.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/quansight-labs-small.png` & `scikit_learn-1.5.0rc1/doc/images/quansight-labs-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/quansight-labs.png` & `scikit_learn-1.5.0rc1/doc/images/quansight-labs.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/rbm_graph.png` & `scikit_learn-1.5.0rc1/doc/images/rbm_graph.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/scikit-learn-logo-notext.png` & `scikit_learn-1.5.0rc1/doc/images/scikit-learn-logo-notext.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/scikit-learn-logo-small.png` & `scikit_learn-1.5.0rc1/doc/images/scikit-learn-logo-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/sloan_banner.png` & `scikit_learn-1.5.0rc1/doc/images/sloan_banner.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/sloan_logo-small.png` & `scikit_learn-1.5.0rc1/doc/images/sloan_logo-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/sydney-primary.jpeg` & `scikit_learn-1.5.0rc1/doc/images/sydney-primary.jpeg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/sydney-stacked-small.png` & `scikit_learn-1.5.0rc1/doc/images/sydney-stacked-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/target_encoder_cross_validation.svg` & `scikit_learn-1.5.0rc1/doc/images/target_encoder_cross_validation.svg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/telecom-small.png` & `scikit_learn-1.5.0rc1/doc/images/telecom-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/telecom.png` & `scikit_learn-1.5.0rc1/doc/images/telecom.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/images/visual-studio-build-tools-selection.png` & `scikit_learn-1.5.0rc1/doc/images/visual-studio-build-tools-selection.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/includes/big_toc_css.rst` & `scikit_learn-1.5.0rc1/doc/includes/big_toc_css.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/includes/bigger_toc_css.rst` & `scikit_learn-1.5.0rc1/doc/includes/bigger_toc_css.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/inspection.rst` & `scikit_learn-1.5.0rc1/doc/inspection.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/install.rst` & `scikit_learn-1.5.0rc1/doc/install.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/1280px-scikit-learn-logo.png` & `scikit_learn-1.5.0rc1/doc/logos/1280px-scikit-learn-logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/README.md` & `scikit_learn-1.5.0rc1/doc/logos/README.md`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/brand_guidelines/scikitlearn_logo_clearspace_updated.png` & `scikit_learn-1.5.0rc1/doc/logos/brand_guidelines/scikitlearn_logo_clearspace_updated.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/favicon.ico` & `scikit_learn-1.5.0rc1/doc/logos/favicon.ico`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/identity.pdf` & `scikit_learn-1.5.0rc1/doc/logos/identity.pdf`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo-notext.png` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-notext.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo-small.png` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-small.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo-thumb.png` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-thumb.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo-without-subtitle.svg` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo-without-subtitle.svg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo.bmp` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo.bmp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo.png` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/logos/scikit-learn-logo.svg` & `scikit_learn-1.5.0rc1/doc/logos/scikit-learn-logo.svg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/make.bat` & `scikit_learn-1.5.0rc1/doc/make.bat`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/metadata_routing.rst` & `scikit_learn-1.5.0rc1/doc/metadata_routing.rst`

 * *Files 14% similar despite different names*

```diff
@@ -4,50 +4,69 @@
 
 .. _metadata_routing:
 
 Metadata Routing
 ================
 
 .. note::
-  The Metadata Routing API is experimental, and is not implemented yet for many
+  The Metadata Routing API is experimental, and is not yet implemented for all
   estimators. Please refer to the :ref:`list of supported and unsupported
   models <metadata_routing_models>` for more information. It may change without
   the usual deprecation cycle. By default this feature is not enabled. You can
-  enable this feature  by setting the ``enable_metadata_routing`` flag to
+  enable it by setting the ``enable_metadata_routing`` flag to
   ``True``::
 
     >>> import sklearn
     >>> sklearn.set_config(enable_metadata_routing=True)
 
-This guide demonstrates how metadata such as ``sample_weight`` can be routed
-and passed along to estimators, scorers, and CV splitters through
-meta-estimators such as :class:`~pipeline.Pipeline` and
-:class:`~model_selection.GridSearchCV`. In order to pass metadata to a method
-such as ``fit`` or ``score``, the object consuming the metadata, must *request*
-it. For estimators and splitters, this is done via ``set_*_request`` methods,
-e.g. ``set_fit_request(...)``, and for scorers this is done via the
-``set_score_request`` method. For grouped splitters such as
-:class:`~model_selection.GroupKFold`, a ``groups`` parameter is requested by
-default. This is best demonstrated by the following examples.
-
-If you are developing a scikit-learn compatible estimator or meta-estimator,
-you can check our related developer guide:
-:ref:`sphx_glr_auto_examples_miscellaneous_plot_metadata_routing.py`.
-
-.. note::
   Note that the methods and requirements introduced in this document are only
   relevant if you want to pass :term:`metadata` (e.g. ``sample_weight``) to a method.
   If you're only passing ``X`` and ``y`` and no other parameter / metadata to
-  methods such as :term:`fit`, :term:`transform`, etc, then you don't need to set
+  methods such as :term:`fit`, :term:`transform`, etc., then you don't need to set
   anything.
 
+This guide demonstrates how :term:`metadata` can be routed and passed between objects in
+scikit-learn. If you are developing a scikit-learn compatible estimator or
+meta-estimator, you can check our related developer guide:
+:ref:`sphx_glr_auto_examples_miscellaneous_plot_metadata_routing.py`.
+
+Metadata is data that an estimator, scorer, or CV splitter takes into account if the
+user explicitly passes it as a parameter. For instance, :class:`~cluster.KMeans` accepts
+`sample_weight` in its `fit()` method and considers it to calculate its centroids.
+`classes` are consumed by some classifiers and `groups` are used in some splitters, but
+any data that is passed into an object's methods apart from X and y can be considered as
+metadata. Prior to scikit-learn version 1.3, there was no single API for passing
+metadata like that if these objects were used in conjunction with other objects, e.g. a
+scorer accepting `sample_weight` inside a :class:`~model_selection.GridSearchCV`.
+
+With the Metadata Routing API, we can transfer metadata to estimators, scorers, and CV
+splitters using :term:`meta-estimators` (such as :class:`~pipeline.Pipeline` or
+:class:`~model_selection.GridSearchCV`) or functions such as
+:func:`~model_selection.cross_validate` which route data to other objects. In order to
+pass metadata to a method like ``fit`` or ``score``, the object consuming the metadata,
+must *request* it. This is done via `set_{method}_request()` methods, where `{method}`
+is substituted by the name of the method that requests the metadata. For instance,
+estimators that use the metadata in their `fit()` method would use `set_fit_request()`,
+and scorers would use `set_score_request()`. These methods allow us to specify which
+metadata to request, for instance `set_fit_request(sample_weight=True)`.
+
+For grouped splitters such as :class:`~model_selection.GroupKFold`, a
+``groups`` parameter is requested by default. This is best demonstrated by the
+following examples.
+
 Usage Examples
 **************
-Here we present a few examples to show different common use-cases. The examples
-in this section require the following imports and data::
+Here we present a few examples to show some common use-cases. Our goal is to pass
+`sample_weight` and `groups` through :func:`~model_selection.cross_validate`, which
+routes the metadata to :class:`~linear_model.LogisticRegressionCV` and to a custom scorer
+made with :func:`~metrics.make_scorer`, both of which *can* use the metadata in their
+methods. In these examples we want to individually set whether to use the metadata
+within the different :term:`consumers <consumer>`.
+
+The examples in this section require the following imports and data::
 
   >>> import numpy as np
   >>> from sklearn.metrics import make_scorer, accuracy_score
   >>> from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
   >>> from sklearn.model_selection import cross_validate, GridSearchCV, GroupKFold
   >>> from sklearn.feature_selection import SelectKBest
   >>> from sklearn.pipeline import make_pipeline
@@ -58,101 +77,105 @@
   >>> my_groups = rng.randint(0, 10, size=n_samples)
   >>> my_weights = rng.rand(n_samples)
   >>> my_other_weights = rng.rand(n_samples)
 
 Weighted scoring and fitting
 ----------------------------
 
-Here :class:`~model_selection.GroupKFold` requests ``groups`` by default. However, we
-need to explicitly request weights for our scorer and the internal cross validation of
-:class:`~linear_model.LogisticRegressionCV`. Both of these *consumers* know how to use
-metadata called ``sample_weight``::
+The splitter used internally in :class:`~linear_model.LogisticRegressionCV`,
+:class:`~model_selection.GroupKFold`, requests ``groups`` by default. However, we need
+to explicitly request `sample_weight` for it and for our custom scorer by specifying
+`sample_weight=True` in :class:`~linear_model.LogisticRegressionCV`s `set_fit_request()`
+method and in :func:`~metrics.make_scorer`s `set_score_request()` method. Both
+:term:`consumers <consumer>` know how to use ``sample_weight`` in their `fit()` or
+`score()` methods. We can then pass the metadata in
+:func:`~model_selection.cross_validate` which will route it to any active consumers::
 
-  >>> weighted_acc = make_scorer(accuracy_score).set_score_request(
-  ...     sample_weight=True
-  ... )
+  >>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)
   >>> lr = LogisticRegressionCV(
-  ...     cv=GroupKFold(), scoring=weighted_acc,
+  ...     cv=GroupKFold(),
+  ...     scoring=weighted_acc
   ... ).set_fit_request(sample_weight=True)
   >>> cv_results = cross_validate(
   ...     lr,
   ...     X,
   ...     y,
   ...     params={"sample_weight": my_weights, "groups": my_groups},
   ...     cv=GroupKFold(),
   ...     scoring=weighted_acc,
   ... )
 
-Note that in this example, ``my_weights`` is passed to both the scorer and
-:class:`~linear_model.LogisticRegressionCV`.
+Note that in this example, :func:`~model_selection.cross_validate` routes ``my_weights``
+to both the scorer and :class:`~linear_model.LogisticRegressionCV`.
 
-Error handling: if ``params={"sample_weigh": my_weights, ...}`` were passed
-(note the typo), :func:`~model_selection.cross_validate` would raise an error,
-since ``sample_weigh`` was not requested by any of its underlying objects.
+If we would pass `sample_weight` in the params of
+:func:`~model_selection.cross_validate`, but not set any object to request it,
+`UnsetMetadataPassedError` would be raised, hinting to us that we need to explicitly set
+where to route it. The same applies if ``params={"sample_weights": my_weights, ...}``
+were passed (note the typo, i.e. ``weights`` instead of ``weight``), since
+``sample_weights`` was not requested by any of its underlying objects.
 
 Weighted scoring and unweighted fitting
 ---------------------------------------
 
-When passing metadata such as ``sample_weight`` around, all ``sample_weight``
-:term:`consumers <consumer>` require weights to be either explicitly requested
-or not requested (i.e. ``True`` or ``False``) when used in another
-:term:`router` such as a :class:`~pipeline.Pipeline` or a ``*GridSearchCV``. To
-perform an unweighted fit, we need to configure
-:class:`~linear_model.LogisticRegressionCV` to not request sample weights, so
+When passing metadata such as ``sample_weight`` into a :term:`router`
+(:term:`meta-estimators` or routing function), all ``sample_weight`` :term:`consumers
+<consumer>` require weights to be either explicitly requested or explicitly not
+requested (i.e. ``True`` or ``False``). Thus, to perform an unweighted fit, we need to
+configure :class:`~linear_model.LogisticRegressionCV` to not request sample weights, so
 that :func:`~model_selection.cross_validate` does not pass the weights along::
 
-  >>> weighted_acc = make_scorer(accuracy_score).set_score_request(
-  ...     sample_weight=True
-  ... )
+  >>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)
   >>> lr = LogisticRegressionCV(
   ...     cv=GroupKFold(), scoring=weighted_acc,
   ... ).set_fit_request(sample_weight=False)
   >>> cv_results = cross_validate(
   ...     lr,
   ...     X,
   ...     y,
   ...     cv=GroupKFold(),
   ...     params={"sample_weight": my_weights, "groups": my_groups},
   ...     scoring=weighted_acc,
   ... )
 
-If :meth:`linear_model.LogisticRegressionCV.set_fit_request` has not
-been called, :func:`~model_selection.cross_validate` will raise an
-error because ``sample_weight`` is passed in but
-:class:`~linear_model.LogisticRegressionCV` would not be explicitly configured
-to recognize the weights.
+If :meth:`linear_model.LogisticRegressionCV.set_fit_request` had not been called,
+:func:`~model_selection.cross_validate` would raise an error because ``sample_weight``
+is passed but :class:`~linear_model.LogisticRegressionCV` would not be explicitly
+configured to recognize the weights.
 
 Unweighted feature selection
 ----------------------------
 
-Setting request values for metadata are only required if the object, e.g. estimator,
-scorer, etc., is a consumer of that metadata Unlike
-:class:`~linear_model.LogisticRegressionCV`, :class:`~feature_selection.SelectKBest`
-doesn't consume weights and therefore no request value for ``sample_weight`` on its
-instance is set and ``sample_weight`` is not routed to it::
+Routing metadata is only possible if the object's method knows how to use the metadata,
+which in most cases means they have it as an explicit parameter. Only then we can set
+request values for metadata using `set_fit_request(sample_weight=True)`, for instance.
+This makes the object a :term:`consumer <consumer>`.
+
+Unlike :class:`~linear_model.LogisticRegressionCV`,
+:class:`~feature_selection.SelectKBest` can't consume weights and therefore no request
+value for ``sample_weight`` on its instance is set and ``sample_weight`` is not routed
+to it::
 
-  >>> weighted_acc = make_scorer(accuracy_score).set_score_request(
-  ...     sample_weight=True
-  ... )
+  >>> weighted_acc = make_scorer(accuracy_score).set_score_request(sample_weight=True)
   >>> lr = LogisticRegressionCV(
   ...     cv=GroupKFold(), scoring=weighted_acc,
   ... ).set_fit_request(sample_weight=True)
   >>> sel = SelectKBest(k=2)
   >>> pipe = make_pipeline(sel, lr)
   >>> cv_results = cross_validate(
   ...     pipe,
   ...     X,
   ...     y,
   ...     cv=GroupKFold(),
   ...     params={"sample_weight": my_weights, "groups": my_groups},
   ...     scoring=weighted_acc,
   ... )
 
-Advanced: Different scoring and fitting weights
------------------------------------------------
+Different scoring and fitting weights
+-------------------------------------
 
 Despite :func:`~metrics.make_scorer` and
 :class:`~linear_model.LogisticRegressionCV` both expecting the key
 ``sample_weight``, we can use aliases to pass different weights to different
 consumers. In this example, we pass ``scoring_weight`` to the scorer, and
 ``fitting_weight`` to :class:`~linear_model.LogisticRegressionCV`::
 
@@ -174,69 +197,71 @@
   ...     },
   ...     scoring=weighted_acc,
   ... )
 
 API Interface
 *************
 
-A :term:`consumer` is an object (estimator, meta-estimator, scorer, splitter)
-which accepts and uses some :term:`metadata` in at least one of its methods
-(``fit``, ``predict``, ``inverse_transform``, ``transform``, ``score``,
-``split``). Meta-estimators which only forward the metadata to other objects
-(the child estimator, scorers, or splitters) and don't use the metadata
-themselves are not consumers. (Meta-)Estimators which route metadata to other
-objects are :term:`routers <router>`. A(n) (meta-)estimator can be a
-:term:`consumer` and a :term:`router` at the same time. (Meta-)Estimators and
-splitters expose a ``set_*_request`` method for each method which accepts at
-least one metadata. For instance, if an estimator supports ``sample_weight`` in
-``fit`` and ``score``, it exposes
+A :term:`consumer` is an object (estimator, meta-estimator, scorer, splitter) which
+accepts and uses some :term:`metadata` in at least one of its methods (for instance
+``fit``, ``predict``, ``inverse_transform``, ``transform``, ``score``, ``split``).
+Meta-estimators which only forward the metadata to other objects (child estimators,
+scorers, or splitters) and don't use the metadata themselves are not consumers.
+(Meta-)Estimators which route metadata to other objects are :term:`routers <router>`.
+A(n) (meta-)estimator can be a :term:`consumer` and a :term:`router` at the same time.
+(Meta-)Estimators and splitters expose a `set_{method}_request` method for each method
+which accepts at least one metadata. For instance, if an estimator supports
+``sample_weight`` in ``fit`` and ``score``, it exposes
 ``estimator.set_fit_request(sample_weight=value)`` and
 ``estimator.set_score_request(sample_weight=value)``. Here ``value`` can be:
 
-- ``True``: method requests a ``sample_weight``. This means if the metadata is
-  provided, it will be used, otherwise no error is raised.
+- ``True``: method requests a ``sample_weight``. This means if the metadata is provided,
+  it will be used, otherwise no error is raised.
 - ``False``: method does not request a ``sample_weight``.
-- ``None``: router will raise an error if ``sample_weight`` is passed. This is
-  in almost all cases the default value when an object is instantiated and
-  ensures the user sets the metadata requests explicitly when a metadata is
-  passed. The only exception are ``Group*Fold`` splitters.
-- ``"param_name"``: if this estimator is used in a meta-estimator, the
-  meta-estimator should forward ``"param_name"`` as ``sample_weight`` to this
-  estimator. This means the mapping between the metadata required by the
-  object, e.g. ``sample_weight`` and what is provided by the user, e.g.
-  ``my_weights`` is done at the router level, and not by the object, e.g.
-  estimator, itself.
+- ``None``: router will raise an error if ``sample_weight`` is passed. This is in almost
+  all cases the default value when an object is instantiated and ensures the user sets
+  the metadata requests explicitly when a metadata is passed. The only exception are
+  ``Group*Fold`` splitters.
+- ``"param_name"``: alias for ``sample_weight`` if we want to pass different weights to
+  different consumers. If aliasing is used the meta-estimator should not forward
+  ``"param_name"`` to the consumer, but ``sample_weight`` instead, because the consumer
+  will expect a param called ``sample_weight``. This means the mapping between the
+  metadata required by the object, e.g. ``sample_weight`` and the variable name provided
+  by the user, e.g. ``my_weights`` is done at the router level, and not by the consuming
+  object itself.
 
 Metadata are requested in the same way for scorers using ``set_score_request``.
 
-If a metadata, e.g. ``sample_weight``, is passed by the user, the metadata
-request for all objects which potentially can consume ``sample_weight`` should
-be set by the user, otherwise an error is raised by the router object. For
-example, the following code raises an error, since it hasn't been explicitly
-specified whether ``sample_weight`` should be passed to the estimator's scorer
-or not::
+If a metadata, e.g. ``sample_weight``, is passed by the user, the metadata request for
+all objects which potentially can consume ``sample_weight`` should be set by the user,
+otherwise an error is raised by the router object. For example, the following code
+raises an error, since it hasn't been explicitly specified whether ``sample_weight``
+should be passed to the estimator's scorer or not::
 
     >>> param_grid = {"C": [0.1, 1]}
     >>> lr = LogisticRegression().set_fit_request(sample_weight=True)
     >>> try:
     ...     GridSearchCV(
     ...         estimator=lr, param_grid=param_grid
     ...     ).fit(X, y, sample_weight=my_weights)
     ... except ValueError as e:
     ...     print(e)
-    [sample_weight] are passed but are not explicitly set as requested or not for
-    LogisticRegression.score
+    [sample_weight] are passed but are not explicitly set as requested or not
+    requested for LogisticRegression.score, which is used within GridSearchCV.fit.
+    Call `LogisticRegression.set_score_request({metadata}=True/False)` for each metadata
+    you want to request/ignore.
 
 The issue can be fixed by explicitly setting the request value::
 
     >>> lr = LogisticRegression().set_fit_request(
     ...     sample_weight=True
     ... ).set_score_request(sample_weight=False)
 
-At the end we disable the configuration flag for metadata routing::
+At the end of the **Usage Examples** section, we disable the configuration flag for
+metadata routing::
 
     >>> sklearn.set_config(enable_metadata_routing=False)
 
 .. _metadata_routing_models:
 
 Metadata Routing Support Status
 *******************************
@@ -247,22 +272,31 @@
 meta-estimators and tools which support and don't yet support metadata routing.
 
 
 Meta-estimators and functions supporting metadata routing:
 
 - :class:`sklearn.calibration.CalibratedClassifierCV`
 - :class:`sklearn.compose.ColumnTransformer`
+- :class:`sklearn.covariance.GraphicalLassoCV`
+- :class:`sklearn.ensemble.VotingClassifier`
+- :class:`sklearn.ensemble.VotingRegressor`
+- :class:`sklearn.ensemble.BaggingClassifier`
+- :class:`sklearn.ensemble.BaggingRegressor`
 - :class:`sklearn.feature_selection.SelectFromModel`
+- :class:`sklearn.impute.IterativeImputer`
 - :class:`sklearn.linear_model.ElasticNetCV`
 - :class:`sklearn.linear_model.LarsCV`
 - :class:`sklearn.linear_model.LassoCV`
 - :class:`sklearn.linear_model.LassoLarsCV`
 - :class:`sklearn.linear_model.LogisticRegressionCV`
 - :class:`sklearn.linear_model.MultiTaskElasticNetCV`
 - :class:`sklearn.linear_model.MultiTaskLassoCV`
+- :class:`sklearn.linear_model.RANSACRegressor`
+- :class:`sklearn.linear_model.RidgeClassifierCV`
+- :class:`sklearn.linear_model.RidgeCV`
 - :class:`sklearn.model_selection.GridSearchCV`
 - :class:`sklearn.model_selection.HalvingGridSearchCV`
 - :class:`sklearn.model_selection.HalvingRandomSearchCV`
 - :class:`sklearn.model_selection.RandomizedSearchCV`
 - :func:`sklearn.model_selection.cross_validate`
 - :func:`sklearn.model_selection.cross_val_score`
 - :func:`sklearn.model_selection.cross_val_predict`
@@ -270,33 +304,26 @@
 - :class:`sklearn.multiclass.OneVsRestClassifier`
 - :class:`sklearn.multiclass.OutputCodeClassifier`
 - :class:`sklearn.multioutput.ClassifierChain`
 - :class:`sklearn.multioutput.MultiOutputClassifier`
 - :class:`sklearn.multioutput.MultiOutputRegressor`
 - :class:`sklearn.linear_model.OrthogonalMatchingPursuitCV`
 - :class:`sklearn.multioutput.RegressorChain`
+- :class:`sklearn.pipeline.FeatureUnion`
 - :class:`sklearn.pipeline.Pipeline`
 
 Meta-estimators and tools not supporting metadata routing yet:
 
 - :class:`sklearn.compose.TransformedTargetRegressor`
-- :class:`sklearn.covariance.GraphicalLassoCV`
 - :class:`sklearn.ensemble.AdaBoostClassifier`
 - :class:`sklearn.ensemble.AdaBoostRegressor`
-- :class:`sklearn.ensemble.BaggingClassifier`
-- :class:`sklearn.ensemble.BaggingRegressor`
 - :class:`sklearn.ensemble.StackingClassifier`
 - :class:`sklearn.ensemble.StackingRegressor`
-- :class:`sklearn.ensemble.VotingClassifier`
-- :class:`sklearn.ensemble.VotingRegressor`
 - :class:`sklearn.feature_selection.RFE`
 - :class:`sklearn.feature_selection.RFECV`
 - :class:`sklearn.feature_selection.SequentialFeatureSelector`
 - :class:`sklearn.impute.IterativeImputer`
 - :class:`sklearn.linear_model.RANSACRegressor`
-- :class:`sklearn.linear_model.RidgeClassifierCV`
-- :class:`sklearn.linear_model.RidgeCV`
 - :class:`sklearn.model_selection.learning_curve`
 - :class:`sklearn.model_selection.permutation_test_score`
 - :class:`sklearn.model_selection.validation_curve`
-- :class:`sklearn.pipeline.FeatureUnion`
 - :class:`sklearn.semi_supervised.SelfTrainingClassifier`
```

### Comparing `scikit-learn-1.4.2/doc/modules/array_api.rst` & `scikit_learn-1.5.0rc1/doc/modules/array_api.rst`

 * *Files 6% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 datastructures and automatically dispatch operations to the underlying namespace
 instead of relying on NumPy.
 
 At this stage, this support is **considered experimental** and must be enabled
 explicitly as explained in the following.
 
 .. note::
-    Currently, only `cupy.array_api`, `numpy.array_api`, `cupy`, and `PyTorch`
+    Currently, only `cupy.array_api`, `array-api-strict`, `cupy`, and `PyTorch`
     are known to work with scikit-learn's estimators.
 
 Example usage
 =============
 
 Here is an example code snippet to demonstrate how to use `CuPy
 <https://cupy.dev/>`_ to run
@@ -91,34 +91,52 @@
 Estimators and other tools in scikit-learn that support Array API compatible inputs.
 
 Estimators
 ----------
 
 - :class:`decomposition.PCA` (with `svd_solver="full"`,
   `svd_solver="randomized"` and `power_iteration_normalizer="QR"`)
+- :class:`linear_model.Ridge` (with `solver="svd"`)
 - :class:`discriminant_analysis.LinearDiscriminantAnalysis` (with `solver="svd"`)
 - :class:`preprocessing.KernelCenterer`
 - :class:`preprocessing.MaxAbsScaler`
 - :class:`preprocessing.MinMaxScaler`
 - :class:`preprocessing.Normalizer`
 
 Metrics
 -------
 
 - :func:`sklearn.metrics.accuracy_score`
+- :func:`sklearn.metrics.r2_score`
 - :func:`sklearn.metrics.zero_one_loss`
 
 Tools
 -----
 
 - :func:`model_selection.train_test_split`
 
 Coverage is expected to grow over time. Please follow the dedicated `meta-issue on GitHub
 <https://github.com/scikit-learn/scikit-learn/issues/22352>`_ to track progress.
 
+Type of return values and fitted attributes
+-------------------------------------------
+
+When calling functions or methods with Array API compatible inputs, the
+convention is to return array values of the same array container type and
+device as the input data.
+
+Similarly, when an estimator is fitted with Array API compatible inputs, the
+fitted attributes will be arrays from the same library as the input and stored
+on the same device. The `predict` and `transform` method subsequently expect
+inputs from the same array library and device as the data passed to the `fit`
+method.
+
+Note however that scoring functions that return scalar values return Python
+scalars (typically a `float` instance) instead of an array scalar value.
+
 Common estimator checks
 =======================
 
 Add the `array_api_support` tag to an estimator's set of tags to indicate that
 it supports the Array API. This will enable dedicated checks as part of the
 common tests to verify that the estimators result's are the same when using
 vanilla NumPy and Array API inputs.
```

### Comparing `scikit-learn-1.4.2/doc/modules/biclustering.rst` & `scikit_learn-1.5.0rc1/doc/modules/biclustering.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/calibration.rst` & `scikit_learn-1.5.0rc1/doc/modules/calibration.rst`

 * *Files 1% similar despite different names*

```diff
@@ -70,18 +70,22 @@
 
 .. figure:: ../auto_examples/calibration/images/sphx_glr_plot_compare_calibration_001.png
    :target: ../auto_examples/calibration/plot_compare_calibration.html
    :align: center
 
 .. currentmodule:: sklearn.linear_model
 
-:class:`LogisticRegression` returns well calibrated predictions by default as it has a
+:class:`LogisticRegression` is more likely to return well calibrated predictions by itself as it has a
 canonical link function for its loss, i.e. the logit-link for the :ref:`log_loss`.
-This leads to the so-called **balance property**, see [8]_ and
-:ref:`Logistic_regression`.
+In the unpenalized case, this leads to the so-called **balance property**, see [8]_ and :ref:`Logistic_regression`.
+In the plot above, data is generated according to a linear mechanism, which is
+consistent with the :class:`LogisticRegression` model (the model is 'well specified'),
+and the value of the regularization parameter `C` is tuned to be
+appropriate (neither too strong nor too low). As a consequence, this model returns
+accurate predictions from its `predict_proba` method.
 In contrast to that, the other shown models return biased probabilities; with
 different biases per model.
 
 .. currentmodule:: sklearn.naive_bayes
 
 :class:`GaussianNB` (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts
 in the histograms). This is mainly because it makes the assumption that
```

### Comparing `scikit-learn-1.4.2/doc/modules/classes.rst` & `scikit_learn-1.5.0rc1/doc/modules/classes.rst`

 * *Files 0% similar despite different names*

```diff
@@ -1244,14 +1244,25 @@
    model_selection.GridSearchCV
    model_selection.HalvingGridSearchCV
    model_selection.ParameterGrid
    model_selection.ParameterSampler
    model_selection.RandomizedSearchCV
    model_selection.HalvingRandomSearchCV
 
+Post-fit model tuning
+---------------------
+
+.. currentmodule:: sklearn
+
+.. autosummary::
+   :toctree: generated/
+   :template: class.rst
+
+   model_selection.FixedThresholdClassifier
+   model_selection.TunedThresholdClassifierCV
 
 Model validation
 ----------------
 
 .. currentmodule:: sklearn
 
 .. autosummary::
```

### Comparing `scikit-learn-1.4.2/doc/modules/clustering.rst` & `scikit_learn-1.5.0rc1/doc/modules/clustering.rst`

 * *Files 7% similar despite different names*

```diff
@@ -255,25 +255,30 @@
 :class:`KMeans` benefits from OpenMP based parallelism through Cython. Small
 chunks of data (256 samples) are processed in parallel, which in addition
 yields a low memory footprint. For more details on how to control the number of
 threads, please refer to our :ref:`parallelism` notes.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`: Demonstrating when
-   k-means performs intuitively and when it does not
- * :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`: Clustering handwritten digits
+  * :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`: Demonstrating
+    when k-means performs intuitively and when it does not
+  * :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`: Clustering
+    handwritten digits
+
+
+|details-start|
+**References**
+|details-split|
+
+* `"k-means++: The advantages of careful seeding"
+  <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_ Arthur, David, and
+  Sergei Vassilvitskii, *Proceedings of the eighteenth annual ACM-SIAM symposium
+  on Discrete algorithms*, Society for Industrial and Applied Mathematics (2007)
 
-.. topic:: References:
-
- * `"k-means++: The advantages of careful seeding"
-   <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_
-   Arthur, David, and Sergei Vassilvitskii,
-   *Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
-   algorithms*, Society for Industrial and Applied Mathematics (2007)
+|details-end|
 
 .. _mini_batch_kmeans:
 
 Mini Batch K-Means
 ------------------
 
 The :class:`MiniBatchKMeans` is a variant of the :class:`KMeans` algorithm
@@ -309,23 +314,24 @@
 
  * :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`: Comparison of
    :class:`KMeans` and :class:`MiniBatchKMeans`
 
  * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: Document clustering
    using :class:`KMeans` and :class:`MiniBatchKMeans` based on sparse data
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`
-
+|details-start|
+**References**
+|details-split|
+
+* `"Web Scale K-Means clustering"
+  <https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_
+  D. Sculley, *Proceedings of the 19th international conference on World
+  wide web* (2010)
 
-.. topic:: References:
-
- * `"Web Scale K-Means clustering"
-   <https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_
-   D. Sculley, *Proceedings of the 19th international conference on World
-   wide web* (2010)
+|details-end|
 
 .. _affinity_propagation:
 
 Affinity Propagation
 ====================
 
 :class:`AffinityPropagation` creates clusters by sending messages between
@@ -354,101 +360,112 @@
 algorithm has a time complexity of the order :math:`O(N^2 T)`, where :math:`N`
 is the number of samples and :math:`T` is the number of iterations until
 convergence. Further, the memory complexity is of the order
 :math:`O(N^2)` if a dense similarity matrix is used, but reducible if a
 sparse similarity matrix is used. This makes Affinity Propagation most
 appropriate for small to medium sized datasets.
 
-.. topic:: Examples:
+|details-start|
+**Algorithm description**
+|details-split|
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`: Affinity
-   Propagation on a synthetic 2D datasets with 3 classes.
-
- * :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` Affinity Propagation on
-   Financial time series to find groups of companies
-
-
-**Algorithm description:**
 The messages sent between points belong to one of two categories. The first is
-the responsibility :math:`r(i, k)`,
-which is the accumulated evidence that sample :math:`k`
-should be the exemplar for sample :math:`i`.
-The second is the availability :math:`a(i, k)`
-which is the accumulated evidence that sample :math:`i`
-should choose sample :math:`k` to be its exemplar,
-and considers the values for all other samples that :math:`k` should
-be an exemplar. In this way, exemplars are chosen by samples if they are (1)
-similar enough to many samples and (2) chosen by many samples to be
-representative of themselves.
+the responsibility :math:`r(i, k)`, which is the accumulated evidence that
+sample :math:`k` should be the exemplar for sample :math:`i`. The second is the
+availability :math:`a(i, k)` which is the accumulated evidence that sample
+:math:`i` should choose sample :math:`k` to be its exemplar, and considers the
+values for all other samples that :math:`k` should be an exemplar. In this way,
+exemplars are chosen by samples if they are (1) similar enough to many samples
+and (2) chosen by many samples to be representative of themselves.
 
-More formally, the responsibility of a sample :math:`k`
-to be the exemplar of sample :math:`i` is given by:
+More formally, the responsibility of a sample :math:`k` to be the exemplar of
+sample :math:`i` is given by:
 
 .. math::
 
     r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]
 
 Where :math:`s(i, k)` is the similarity between samples :math:`i` and :math:`k`.
-The availability of sample :math:`k`
-to be the exemplar of sample :math:`i` is given by:
+The availability of sample :math:`k` to be the exemplar of sample :math:`i` is
+given by:
 
 .. math::
 
-    a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i', k)}]
+    a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i',
+    k)}]
 
-To begin with, all values for :math:`r` and :math:`a` are set to zero,
-and the calculation of each iterates until convergence.
-As discussed above, in order to avoid numerical oscillations when updating the
-messages, the damping factor :math:`\lambda` is introduced to iteration process:
+To begin with, all values for :math:`r` and :math:`a` are set to zero, and the
+calculation of each iterates until convergence. As discussed above, in order to
+avoid numerical oscillations when updating the messages, the damping factor
+:math:`\lambda` is introduced to iteration process:
 
 .. math:: r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)
 .. math:: a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)
 
 where :math:`t` indicates the iteration times.
 
+|details-end|
+
+
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`: Affinity
+    Propagation on a synthetic 2D datasets with 3 classes.
+
+  * :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` Affinity
+    Propagation on Financial time series to find groups of companies
+
+
 .. _mean_shift:
 
 Mean Shift
 ==========
 :class:`MeanShift` clustering aims to discover *blobs* in a smooth density of
 samples. It is a centroid based algorithm, which works by updating candidates
 for centroids to be the mean of the points within a given region. These
 candidates are then filtered in a post-processing stage to eliminate
 near-duplicates to form the final set of centroids.
 
-The position of centroid candidates is iteratively adjusted using a technique called hill
-climbing, which finds local maxima of the estimated probability density.
-Given a candidate centroid :math:`x` for iteration :math:`t`, the candidate
-is updated according to the following equation:
+|details-start|
+**Mathematical details**
+|details-split|
+
+The position of centroid candidates is iteratively adjusted using a technique
+called hill climbing, which finds local maxima of the estimated probability
+density. Given a candidate centroid :math:`x` for iteration :math:`t`, the
+candidate is updated according to the following equation:
 
 .. math::
 
     x^{t+1} = x^t + m(x^t)
 
-Where :math:`m` is the *mean shift* vector that is computed for each
-centroid that points towards a region of the maximum increase in the density of points.
-To compute :math:`m` we define :math:`N(x)` as the neighborhood of samples within
-a given distance around :math:`x`. Then :math:`m` is computed using the following
-equation, effectively updating a centroid to be the mean of the samples within
-its neighborhood:
+Where :math:`m` is the *mean shift* vector that is computed for each centroid
+that points towards a region of the maximum increase in the density of points.
+To compute :math:`m` we define :math:`N(x)` as the neighborhood of samples
+within a given distance around :math:`x`. Then :math:`m` is computed using the
+following equation, effectively updating a centroid to be the mean of the
+samples within its neighborhood:
 
 .. math::
 
     m(x) = \frac{1}{|N(x)|} \sum_{x_j \in N(x)}x_j - x
 
-In general, the equation for :math:`m` depends on a kernel used for density estimation.
-The generic formula is:
+In general, the equation for :math:`m` depends on a kernel used for density
+estimation. The generic formula is:
 
 .. math::
 
-    m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j - x)} - x
+    m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j -
+    x)} - x
+
+In our implementation, :math:`K(x)` is equal to 1 if :math:`x` is small enough
+and is equal to 0 otherwise. Effectively :math:`K(y - x)` indicates whether
+:math:`y` is in the neighborhood of :math:`x`.
 
-In our implementation, :math:`K(x)` is equal to 1 if :math:`x` is small enough and is
-equal to 0 otherwise. Effectively :math:`K(y - x)` indicates whether :math:`y` is in
-the neighborhood of :math:`x`.
+|details-end|
 
 The algorithm automatically sets the number of clusters, instead of relying on a
 parameter ``bandwidth``, which dictates the size of the region to search through.
 This parameter can be set manually, but can be estimated using the provided
 ``estimate_bandwidth`` function, which is called if the bandwidth is not set.
 
 The algorithm is not highly scalable, as it requires multiple nearest neighbor
@@ -464,23 +481,27 @@
    :target: ../auto_examples/cluster/plot_mean_shift.html
    :align: center
    :scale: 50
 
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`: Mean Shift clustering
-   on a synthetic 2D datasets with 3 classes.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`: Mean Shift
+    clustering on a synthetic 2D datasets with 3 classes.
 
-.. topic:: References:
 
- * :doi:`"Mean shift: A robust approach toward feature space analysis"
-   <10.1109/34.1000236>`
-   D. Comaniciu and P. Meer, *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2002)
+|details-start|
+**References**
+|details-split|
+
+* :doi:`"Mean shift: A robust approach toward feature space analysis"
+  <10.1109/34.1000236>` D. Comaniciu and P. Meer, *IEEE Transactions on Pattern
+  Analysis and Machine Intelligence* (2002)
 
+|details-end|
 
 .. _spectral_clustering:
 
 Spectral clustering
 ===================
 
 :class:`SpectralClustering` performs a low-dimension embedding of the
@@ -524,31 +545,32 @@
 
         similarity = np.exp(-beta * distance / distance.std())
 
     See the examples for such an application.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: Segmenting objects
-   from a noisy background using spectral clustering.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: Segmenting
+    objects from a noisy background using spectral clustering.
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: Spectral clustering
-   to split the image of coins in regions.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: Spectral
+    clustering to split the image of coins in regions.
 
 .. |coin_kmeans| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_001.png
-    :target: ../auto_examples/cluster/plot_coin_segmentation.html
-    :scale: 35
+  :target: ../auto_examples/cluster/plot_coin_segmentation.html
+  :scale: 35
 
 .. |coin_discretize| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_002.png
-    :target: ../auto_examples/cluster/plot_coin_segmentation.html
-    :scale: 35
+  :target: ../auto_examples/cluster/plot_coin_segmentation.html
+  :scale: 35
 
 .. |coin_cluster_qr| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_003.png
-    :target: ../auto_examples/cluster/plot_coin_segmentation.html
-    :scale: 35
+  :target: ../auto_examples/cluster/plot_coin_segmentation.html
+  :scale: 35
+
 
 Different label assignment strategies
 -------------------------------------
 
 Different label assignment strategies can be used, corresponding to the
 ``assign_labels`` parameter of :class:`SpectralClustering`.
 ``"kmeans"`` strategy can match finer details, but can be unstable.
@@ -562,22 +584,26 @@
 
 ================================  ================================  ================================
  ``assign_labels="kmeans"``        ``assign_labels="discretize"``    ``assign_labels="cluster_qr"``
 ================================  ================================  ================================
 |coin_kmeans|                          |coin_discretize|                  |coin_cluster_qr|
 ================================  ================================  ================================
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
+
+* `"Multiclass spectral clustering"
+  <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_
+  Stella X. Yu, Jianbo Shi, 2003
 
- * `"Multiclass spectral clustering"
-   <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_
-   Stella X. Yu, Jianbo Shi, 2003
+* :doi:`"Simple, direct, and efficient multi-way spectral clustering"<10.1093/imaiai/iay008>`
+  Anil Damle, Victor Minden, Lexing Ying, 2019
 
- * :doi:`"Simple, direct, and efficient multi-way spectral clustering"<10.1093/imaiai/iay008>`
-   Anil Damle, Victor Minden, Lexing Ying, 2019
+|details-end|
 
 .. _spectral_clustering_graph:
 
 Spectral Clustering Graphs
 --------------------------
 
 Spectral Clustering can also be used to partition graphs via their spectral
@@ -585,36 +611,36 @@
 graph, and SpectralClustering is initialized with `affinity='precomputed'`::
 
     >>> from sklearn.cluster import SpectralClustering
     >>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,
     ...                         assign_labels='discretize')
     >>> sc.fit_predict(adjacency_matrix)  # doctest: +SKIP
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
- * :doi:`"A Tutorial on Spectral Clustering"
-   <10.1007/s11222-007-9033-z>`
-   Ulrike von Luxburg, 2007
-
- * :doi:`"Normalized cuts and image segmentation"
-   <10.1109/34.868688>`
-   Jianbo Shi, Jitendra Malik, 2000
-
- * `"A Random Walks View of Spectral Segmentation"
-   <https://citeseerx.ist.psu.edu/doc_view/pid/84a86a69315e994cfd1e0c7debb86d62d7bd1f44>`_
-   Marina Meila, Jianbo Shi, 2001
-
- * `"On Spectral Clustering: Analysis and an algorithm"
-   <https://citeseerx.ist.psu.edu/doc_view/pid/796c5d6336fc52aa84db575fb821c78918b65f58>`_
-   Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
-
- * :arxiv:`"Preconditioned Spectral Clustering for Stochastic
-   Block Partition Streaming Graph Challenge"
-   <1708.07481>`
-   David Zhuzhunashvili, Andrew Knyazev
+* :doi:`"A Tutorial on Spectral Clustering" <10.1007/s11222-007-9033-z>` Ulrike
+  von Luxburg, 2007
+
+* :doi:`"Normalized cuts and image segmentation" <10.1109/34.868688>` Jianbo
+  Shi, Jitendra Malik, 2000
+
+* `"A Random Walks View of Spectral Segmentation"
+  <https://citeseerx.ist.psu.edu/doc_view/pid/84a86a69315e994cfd1e0c7debb86d62d7bd1f44>`_
+  Marina Meila, Jianbo Shi, 2001
+
+* `"On Spectral Clustering: Analysis and an algorithm"
+  <https://citeseerx.ist.psu.edu/doc_view/pid/796c5d6336fc52aa84db575fb821c78918b65f58>`_
+  Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
+
+* :arxiv:`"Preconditioned Spectral Clustering for Stochastic Block Partition
+  Streaming Graph Challenge" <1708.07481>` David Zhuzhunashvili, Andrew Knyazev
+
+|details-end|
 
 .. _hierarchical_clustering:
 
 Hierarchical clustering
 =======================
 
 Hierarchical clustering is a general family of clustering algorithms that
@@ -669,28 +695,35 @@
 Euclidean metrics, average linkage is a good alternative. Single linkage,
 while not robust to noisy data, can be computed very efficiently and can
 therefore be useful to provide hierarchical clustering of larger datasets.
 Single linkage can also perform well on non-globular data.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_digits_linkage.py`: exploration of the
-   different linkage strategies in a real dataset.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_digits_linkage.py`: exploration of
+    the different linkage strategies in a real dataset.
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`: exploration of
+    the different linkage strategies in toy datasets.
+
 
 Visualization of cluster hierarchy
 ----------------------------------
 
 It's possible to visualize the tree representing the hierarchical merging of clusters
 as a dendrogram. Visual inspection can often be useful for understanding the structure
 of the data, though more so in the case of small sample sizes.
 
 .. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_dendrogram_001.png
     :target: ../auto_examples/cluster/plot_agglomerative_dendrogram.html
     :scale: 42
 
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`
 
 
 Adding connectivity constraints
 -------------------------------
 
 An interesting aspect of :class:`AgglomerativeClustering` is that
 connectivity constraints can be added to this algorithm (only adjacent
@@ -724,29 +757,14 @@
 using :func:`sklearn.neighbors.kneighbors_graph` to restrict
 merging to nearest neighbors as in :ref:`this example
 <sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py>`, or
 using :func:`sklearn.feature_extraction.image.grid_to_graph` to
 enable only merging of neighboring pixels on an image, as in the
 :ref:`coin <sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py>` example.
 
-.. topic:: Examples:
-
- * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: Ward clustering
-   to split the image of coins in regions.
-
- * :ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: Example of
-   Ward algorithm on a swiss-roll, comparison of structured approaches
-   versus unstructured approaches.
-
- * :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`:
-   Example of dimensionality reduction with feature agglomeration based on
-   Ward hierarchical clustering.
-
- * :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`
-
 .. warning:: **Connectivity constraints with single, average and complete linkage**
 
     Connectivity constraints and single, complete or average linkage can enhance
     the 'rich getting richer' aspect of agglomerative clustering,
     particularly so if they are built with
     :func:`sklearn.neighbors.kneighbors_graph`. In the limit of a small
     number of clusters, they tend to give a few macroscopically occupied
@@ -766,14 +784,29 @@
     :target: ../auto_examples/cluster/plot_agglomerative_clustering.html
     :scale: 38
 
 .. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_004.png
     :target: ../auto_examples/cluster/plot_agglomerative_clustering.html
     :scale: 38
 
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: Ward
+    clustering to split the image of coins in regions.
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: Example
+    of Ward algorithm on a swiss-roll, comparison of structured approaches
+    versus unstructured approaches.
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`: Example
+    of dimensionality reduction with feature agglomeration based on Ward
+    hierarchical clustering.
+
+  * :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`
+
 
 Varying the metric
 -------------------
 
 Single, average and complete linkage can be used with a variety of distances (or
 affinities), in particular Euclidean distance (*l2*), Manhattan distance
 (or Cityblock, or *l1*), cosine distance, or any precomputed affinity
@@ -800,15 +833,16 @@
 
 .. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_007.png
     :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html
     :scale: 32
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering_metrics.py`
+  * :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering_metrics.py`
+
 
 Bisecting K-Means
 -----------------
 
 .. _bisect_k_means:
 
 The :class:`BisectingKMeans` is an iterative variant of :class:`KMeans`, using
@@ -843,32 +877,34 @@
 sizes while `KMeans` is known to produce clusters of different sizes.
 
 Difference between Bisecting K-Means and regular K-Means can be seen on example
 :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`.
 While the regular K-Means algorithm tends to create non-related clusters,
 clusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
+
+* `"A Comparison of Document Clustering Techniques"
+  <http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf>`_ Michael
+  Steinbach, George Karypis and Vipin Kumar, Department of Computer Science and
+  Egineering, University of Minnesota (June 2000)
+* `"Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog
+  Data"
+  <https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf>`_
+  K.Abirami and Dr.P.Mayilvahanan, International Journal of Emerging
+  Technologies in Engineering Research (IJETER) Volume 4, Issue 8, (August 2016)
+* `"Bisecting K-means Algorithm Based on K-valued Self-determining and
+  Clustering Center Optimization"
+  <http://www.jcomputers.us/vol13/jcp1306-01.pdf>`_ Jian Di, Xinyue Gou School
+  of Control and Computer Engineering,North China Electric Power University,
+  Baoding, Hebei, China (August 2017)
 
- * `"A Comparison of Document Clustering Techniques"
-   <http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf>`_
-   Michael Steinbach, George Karypis and Vipin Kumar,
-   Department of Computer Science and Egineering, University of Minnesota
-   (June 2000)
- * `"Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data"
-   <https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf>`_
-   K.Abirami and Dr.P.Mayilvahanan,
-   International Journal of Emerging Technologies in Engineering Research (IJETER)
-   Volume 4, Issue 8, (August 2016)
- * `"Bisecting K-means Algorithm Based on K-valued Self-determining
-   and Clustering Center Optimization"
-   <http://www.jcomputers.us/vol13/jcp1306-01.pdf>`_
-   Jian Di, Xinyue Gou
-   School of Control and Computer Engineering,North China Electric Power University,
-   Baoding, Hebei, China (August 2017)
+|details-end|
 
 .. _dbscan:
 
 DBSCAN
 ======
 
 The :class:`DBSCAN` algorithm views clusters as areas of high density
@@ -923,70 +959,78 @@
 
 .. centered:: |dbscan_results|
 
 .. topic:: Examples:
 
     * :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`
 
-.. topic:: Implementation
-
-    The DBSCAN algorithm is deterministic, always generating the same clusters
-    when given the same data in the same order.  However, the results can differ when
-    data is provided in a different order. First, even though the core samples
-    will always be assigned to the same clusters, the labels of those clusters
-    will depend on the order in which those samples are encountered in the data.
-    Second and more importantly, the clusters to which non-core samples are assigned
-    can differ depending on the data order.  This would happen when a non-core sample
-    has a distance lower than ``eps`` to two core samples in different clusters. By the
-    triangular inequality, those two core samples must be more distant than
-    ``eps`` from each other, or they would be in the same cluster. The non-core
-    sample is assigned to whichever cluster is generated first in a pass
-    through the data, and so the results will depend on the data ordering.
-
-    The current implementation uses ball trees and kd-trees
-    to determine the neighborhood of points,
-    which avoids calculating the full distance matrix
-    (as was done in scikit-learn versions before 0.14).
-    The possibility to use custom metrics is retained;
-    for details, see :class:`~sklearn.neighbors.NearestNeighbors`.
-
-.. topic:: Memory consumption for large sample sizes
-
-    This implementation is by default not memory efficient because it constructs
-    a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot
-    be used (e.g., with sparse matrices). This matrix will consume :math:`n^2` floats.
-    A couple of mechanisms for getting around this are:
-
-    - Use :ref:`OPTICS <optics>` clustering in conjunction with the
-      `extract_dbscan` method. OPTICS clustering also calculates the full
-      pairwise matrix, but only keeps one row in memory at a time (memory
-      complexity n).
-
-    - A sparse radius neighborhood graph (where missing entries are presumed to
-      be out of eps) can be precomputed in a memory-efficient way and dbscan
-      can be run over this with ``metric='precomputed'``.  See
-      :meth:`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`.
-
-    - The dataset can be compressed, either by removing exact duplicates if
-      these occur in your data, or by using BIRCH. Then you only have a
-      relatively small number of representatives for a large number of points.
-      You can then provide a ``sample_weight`` when fitting DBSCAN.
+|details-start|
+**Implementation**
+|details-split|
+
+The DBSCAN algorithm is deterministic, always generating the same clusters when
+given the same data in the same order.  However, the results can differ when
+data is provided in a different order. First, even though the core samples will
+always be assigned to the same clusters, the labels of those clusters will
+depend on the order in which those samples are encountered in the data. Second
+and more importantly, the clusters to which non-core samples are assigned can
+differ depending on the data order.  This would happen when a non-core sample
+has a distance lower than ``eps`` to two core samples in different clusters. By
+the triangular inequality, those two core samples must be more distant than
+``eps`` from each other, or they would be in the same cluster. The non-core
+sample is assigned to whichever cluster is generated first in a pass through the
+data, and so the results will depend on the data ordering.
+
+The current implementation uses ball trees and kd-trees to determine the
+neighborhood of points, which avoids calculating the full distance matrix (as
+was done in scikit-learn versions before 0.14). The possibility to use custom
+metrics is retained; for details, see :class:`NearestNeighbors`.
+
+|details-end|
+
+|details-start|
+**Memory consumption for large sample sizes**
+|details-split|
+
+This implementation is by default not memory efficient because it constructs a
+full pairwise similarity matrix in the case where kd-trees or ball-trees cannot
+be used (e.g., with sparse matrices). This matrix will consume :math:`n^2`
+floats. A couple of mechanisms for getting around this are:
+
+- Use :ref:`OPTICS <optics>` clustering in conjunction with the `extract_dbscan`
+  method. OPTICS clustering also calculates the full pairwise matrix, but only
+  keeps one row in memory at a time (memory complexity n).
+
+- A sparse radius neighborhood graph (where missing entries are presumed to be
+  out of eps) can be precomputed in a memory-efficient way and dbscan can be run
+  over this with ``metric='precomputed'``.  See
+  :meth:`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`.
+
+- The dataset can be compressed, either by removing exact duplicates if these
+  occur in your data, or by using BIRCH. Then you only have a relatively small
+  number of representatives for a large number of points. You can then provide a
+  ``sample_weight`` when fitting DBSCAN.
+
+|details-end|
+
+|details-start|
+**References**
+|details-split|
+
+* `A Density-Based Algorithm for Discovering Clusters in Large Spatial
+  Databases with Noise <https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf>`_
+  Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd
+  International Conference on Knowledge Discovery and Data Mining, Portland, OR,
+  AAAI Press, pp. 226–231. 1996
+
+* :doi:`DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
+  <10.1145/3068335>` Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu,
+  X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.
 
-.. topic:: References:
-
- * `"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
-   with Noise" <https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf>`_
-   Ester, M., H. P. Kriegel, J. Sander, and X. Xu,
-   In Proceedings of the 2nd International Conference on Knowledge Discovery
-   and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996
-
- * :doi:`"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN."
-   <10.1145/3068335>`
-   Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
-   In ACM Transactions on Database Systems (TODS), 42(3), 19.
+|details-end|
 
 .. _hdbscan:
 
 HDBSCAN
 =======
 
 The :class:`HDBSCAN` algorithm can be seen as an extension of :class:`DBSCAN`
@@ -998,14 +1042,18 @@
 scales by building an alternative representation of the clustering problem.
 
 .. note::
 
   This implementation is adapted from the original implementation of HDBSCAN,
   `scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_ based on [LJ2017]_.
 
+.. topic:: Examples:
+
+    * :ref:`sphx_glr_auto_examples_cluster_plot_hdbscan.py`
+
 Mutual Reachability Graph
 -------------------------
 
 HDBSCAN first defines :math:`d_c(x_p)`, the *core distance* of a sample :math:`x_p`, as the
 distance to its `min_samples` th-nearest neighbor, counting itself. For example,
 if `min_samples=5` and :math:`x_*` is the 5th-nearest neighbor of :math:`x_p`
 then the core distance is:
@@ -1074,25 +1122,25 @@
 which specifies that during the hierarchical clustering, components with fewer
 than `minimum_cluster_size` many samples are considered noise. In practice, one
 can set `minimum_cluster_size = min_samples` to couple the parameters and
 simplify the hyperparameter space.
 
 .. topic:: References:
 
- .. [CM2013] Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based Clustering
-   Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S., Cao, L.,
-   Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data Mining.
-   PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer, Berlin,
-   Heidelberg.
-   :doi:`Density-Based Clustering Based on Hierarchical Density Estimates <10.1007/978-3-642-37456-2_14>`
-
- .. [LJ2017] L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density Based
-   Clustering. In: IEEE International Conference on Data Mining Workshops (ICDMW),
-   2017, pp. 33-42.
-   :doi:`Accelerated Hierarchical Density Based Clustering <10.1109/ICDMW.2017.12>`
+ .. [CM2013] Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based
+   Clustering Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S.,
+   Cao, L., Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data
+   Mining. PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer,
+   Berlin, Heidelberg. :doi:`Density-Based Clustering Based on Hierarchical
+   Density Estimates <10.1007/978-3-642-37456-2_14>`
+
+ .. [LJ2017] L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density
+   Based Clustering. In: IEEE International Conference on Data Mining Workshops
+   (ICDMW), 2017, pp. 33-42. :doi:`Accelerated Hierarchical Density Based
+   Clustering <10.1109/ICDMW.2017.12>`
 
 .. _optics:
 
 OPTICS
 ======
 
 The :class:`OPTICS` algorithm shares many similarities with the :class:`DBSCAN`
@@ -1132,53 +1180,64 @@
 plot above has been color-coded so that cluster colors in planar space match
 the linear segment clusters of the reachability plot. Note that the blue and
 red clusters are adjacent in the reachability plot, and can be hierarchically
 represented as children of a larger parent cluster.
 
 .. topic:: Examples:
 
-     * :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`
+  * :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`
 
 
-.. topic:: Comparison with DBSCAN
+|details-start|
+**Comparison with DBSCAN**
+|details-split|
+
+The results from OPTICS ``cluster_optics_dbscan`` method and DBSCAN are very
+similar, but not always identical; specifically, labeling of periphery and noise
+points. This is in part because the first samples of each dense area processed
+by OPTICS have a large reachability value while being close to other points in
+their area, and will thus sometimes be marked as noise rather than periphery.
+This affects adjacent points when they are considered as candidates for being
+marked as either periphery or noise.
+
+Note that for any single value of ``eps``, DBSCAN will tend to have a shorter
+run time than OPTICS; however, for repeated runs at varying ``eps`` values, a
+single run of OPTICS may require less cumulative runtime than DBSCAN. It is also
+important to note that OPTICS' output is close to DBSCAN's only if ``eps`` and
+``max_eps`` are close.
+
+|details-end|
+
+|details-start|
+**Computational Complexity**
+|details-split|
+
+Spatial indexing trees are used to avoid calculating the full distance matrix,
+and allow for efficient memory usage on large sets of samples. Different
+distance metrics can be supplied via the ``metric`` keyword.
+
+For large datasets, similar (but not identical) results can be obtained via
+:class:`HDBSCAN`. The HDBSCAN implementation is multithreaded, and has better
+algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling.
+For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS
+will maintain :math:`n` (as opposed to :math:`n^2`) memory scaling; however,
+tuning of the ``max_eps`` parameter will likely need to be used to give a
+solution in a reasonable amount of wall time.
+
+|details-end|
+
+|details-start|
+**References**
+|details-split|
+
+* "OPTICS: ordering points to identify the clustering structure." Ankerst,
+  Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod
+  Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.
 
-    The results from OPTICS ``cluster_optics_dbscan`` method and DBSCAN are
-    very similar, but not always identical; specifically, labeling of periphery
-    and noise points. This is in part because the first samples of each dense
-    area processed by OPTICS have a large reachability value while being close
-    to other points in their area, and will thus sometimes be marked as noise
-    rather than periphery. This affects adjacent points when they are
-    considered as candidates for being marked as either periphery or noise.
-
-    Note that for any single value of ``eps``, DBSCAN will tend to have a
-    shorter run time than OPTICS; however, for repeated runs at varying ``eps``
-    values, a single run of OPTICS may require less cumulative runtime than
-    DBSCAN. It is also important to note that OPTICS' output is close to
-    DBSCAN's only if ``eps`` and ``max_eps`` are close.
-
-.. topic:: Computational Complexity
-
-    Spatial indexing trees are used to avoid calculating the full distance
-    matrix, and allow for efficient memory usage on large sets of samples.
-    Different distance metrics can be supplied via the ``metric`` keyword.
-
-    For large datasets, similar (but not identical) results can be obtained via
-    :class:`HDBSCAN`. The HDBSCAN implementation is
-    multithreaded, and has better algorithmic runtime complexity than OPTICS,
-    at the cost of worse memory scaling. For extremely large datasets that
-    exhaust system memory using HDBSCAN, OPTICS will maintain :math:`n` (as opposed
-    to :math:`n^2`) memory scaling; however, tuning of the ``max_eps`` parameter
-    will likely need to be used to give a solution in a reasonable amount of
-    wall time.
-
-.. topic:: References:
-
- *  "OPTICS: ordering points to identify the clustering structure."
-    Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.
-    In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.
+|details-end|
 
 .. _birch:
 
 BIRCH
 =====
 
 The :class:`Birch` builds a tree called the Clustering Feature Tree (CFT)
@@ -1206,68 +1265,83 @@
 since it reduces the input data to a set of subclusters which are obtained directly
 from the leaves of the CFT. This reduced data can be further processed by feeding
 it into a global clusterer. This global clusterer can be set by ``n_clusters``.
 If ``n_clusters`` is set to None, the subclusters from the leaves are directly
 read off, otherwise a global clustering step labels these subclusters into global
 clusters (labels) and the samples are mapped to the global label of the nearest subcluster.
 
-**Algorithm description:**
-
-- A new sample is inserted into the root of the CF Tree which is a CF Node.
-  It is then merged with the subcluster of the root, that has the smallest
-  radius after merging, constrained by the threshold and branching factor conditions.
-  If the subcluster has any child node, then this is done repeatedly till it reaches
-  a leaf. After finding the nearest subcluster in the leaf, the properties of this
-  subcluster and the parent subclusters are recursively updated.
+|details-start|
+**Algorithm description**
+|details-split|
+
+- A new sample is inserted into the root of the CF Tree which is a CF Node. It
+  is then merged with the subcluster of the root, that has the smallest radius
+  after merging, constrained by the threshold and branching factor conditions.
+  If the subcluster has any child node, then this is done repeatedly till it
+  reaches a leaf. After finding the nearest subcluster in the leaf, the
+  properties of this subcluster and the parent subclusters are recursively
+  updated.
 
 - If the radius of the subcluster obtained by merging the new sample and the
   nearest subcluster is greater than the square of the threshold and if the
-  number of subclusters is greater than the branching factor, then a space is temporarily
-  allocated to this new sample. The two farthest subclusters are taken and
-  the subclusters are divided into two groups on the basis of the distance
-  between these subclusters.
-
-- If this split node has a parent subcluster and there is room
-  for a new subcluster, then the parent is split into two. If there is no room,
-  then this node is again split into two and the process is continued
-  recursively, till it reaches the root.
+  number of subclusters is greater than the branching factor, then a space is
+  temporarily allocated to this new sample. The two farthest subclusters are
+  taken and the subclusters are divided into two groups on the basis of the
+  distance between these subclusters.
+
+- If this split node has a parent subcluster and there is room for a new
+  subcluster, then the parent is split into two. If there is no room, then this
+  node is again split into two and the process is continued recursively, till it
+  reaches the root.
 
+|details-end|
+
+|details-start|
 **BIRCH or MiniBatchKMeans?**
+|details-split|
 
 - BIRCH does not scale very well to high dimensional data. As a rule of thumb if
   ``n_features`` is greater than twenty, it is generally better to use MiniBatchKMeans.
 - If the number of instances of data needs to be reduced, or if one wants a
   large number of subclusters either as a preprocessing step or otherwise,
   BIRCH is more useful than MiniBatchKMeans.
 
+.. image:: ../auto_examples/cluster/images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png
+    :target: ../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html
+
+|details-end|
 
+|details-start|
 **How to use partial_fit?**
+|details-split|
 
 To avoid the computation of global clustering, for every call of ``partial_fit``
 the user is advised
 
 1. To set ``n_clusters=None`` initially
 2. Train all data by multiple calls to partial_fit.
 3. Set ``n_clusters`` to a required value using
    ``brc.set_params(n_clusters=n_clusters)``.
 4. Call ``partial_fit`` finally with no arguments, i.e. ``brc.partial_fit()``
    which performs the global clustering.
 
-.. image:: ../auto_examples/cluster/images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png
-    :target: ../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html
+|details-end|
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
+
+* Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data
+  clustering method for large databases.
+  https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
 
- * Tian Zhang, Raghu Ramakrishnan, Maron Livny
-   BIRCH: An efficient data clustering method for large databases.
-   https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
-
- * Roberto Perdisci
-   JBirch - Java implementation of BIRCH clustering algorithm
-   https://code.google.com/archive/p/jbirch
+* Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm
+  https://code.google.com/archive/p/jbirch
+
+|details-end|
 
 
 .. _clustering_evaluation:
 
 Clustering performance evaluation
 =================================
 
@@ -1342,113 +1416,112 @@
   >>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]
   >>> metrics.rand_score(labels_true, labels_pred)
   0.39...
   >>> metrics.adjusted_rand_score(labels_true, labels_pred)
   -0.07...
 
 
-Advantages
-~~~~~~~~~~
+.. topic:: Advantages:
 
-- **Interpretability**: The unadjusted Rand index is proportional
-  to the number of sample pairs whose labels are the same in both
-  `labels_pred` and `labels_true`, or are different in both.
-
-- **Random (uniform) label assignments have an adjusted Rand index
-  score close to 0.0** for any value of ``n_clusters`` and
-  ``n_samples`` (which is not the case for the unadjusted Rand index
-  or the V-measure for instance).
-
-- **Bounded range**: Lower values indicate different labelings,
-  similar clusterings have a high (adjusted or unadjusted) Rand index,
-  1.0 is the perfect match score. The score range is [0, 1] for the
-  unadjusted Rand index and [-1, 1] for the adjusted Rand index.
-
-- **No assumption is made on the cluster structure**: The (adjusted or
-  unadjusted) Rand index can be used to compare all kinds of
-  clustering algorithms, and can be used to compare clustering
-  algorithms such as k-means which assumes isotropic blob shapes with
-  results of spectral clustering algorithms which can find cluster
-  with "folded" shapes.
-
-
-Drawbacks
-~~~~~~~~~
-
-- Contrary to inertia, the **(adjusted or unadjusted) Rand index
-  requires knowledge of the ground truth classes** which is almost
-  never available in practice or requires manual assignment by human
-  annotators (as in the supervised learning setting).
-
-  However (adjusted or unadjusted) Rand index can also be useful in a
-  purely unsupervised setting as a building block for a Consensus
-  Index that can be used for clustering model selection (TODO).
-
-- The **unadjusted Rand index is often close to 1.0** even if the
-  clusterings themselves differ significantly. This can be understood
-  when interpreting the Rand index as the accuracy of element pair
-  labeling resulting from the clusterings: In practice there often is
-  a majority of element pairs that are assigned the ``different`` pair
-  label under both the predicted and the ground truth clustering
-  resulting in a high proportion of pair labels that agree, which
-  leads subsequently to a high score.
+  - **Interpretability**: The unadjusted Rand index is proportional to the
+    number of sample pairs whose labels are the same in both `labels_pred` and
+    `labels_true`, or are different in both.
+
+  - **Random (uniform) label assignments have an adjusted Rand index score close
+    to 0.0** for any value of ``n_clusters`` and ``n_samples`` (which is not the
+    case for the unadjusted Rand index or the V-measure for instance).
+
+  - **Bounded range**: Lower values indicate different labelings, similar
+    clusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the
+    perfect match score. The score range is [0, 1] for the unadjusted Rand index
+    and [-1, 1] for the adjusted Rand index.
+
+  - **No assumption is made on the cluster structure**: The (adjusted or
+    unadjusted) Rand index can be used to compare all kinds of clustering
+    algorithms, and can be used to compare clustering algorithms such as k-means
+    which assumes isotropic blob shapes with results of spectral clustering
+    algorithms which can find cluster with "folded" shapes.
+
+.. topic:: Drawbacks:
+
+  - Contrary to inertia, the **(adjusted or unadjusted) Rand index requires
+    knowledge of the ground truth classes** which is almost never available in
+    practice or requires manual assignment by human annotators (as in the
+    supervised learning setting).
+
+    However (adjusted or unadjusted) Rand index can also be useful in a purely
+    unsupervised setting as a building block for a Consensus Index that can be
+    used for clustering model selection (TODO).
+
+  - The **unadjusted Rand index is often close to 1.0** even if the clusterings
+    themselves differ significantly. This can be understood when interpreting
+    the Rand index as the accuracy of element pair labeling resulting from the
+    clusterings: In practice there often is a majority of element pairs that are
+    assigned the ``different`` pair label under both the predicted and the
+    ground truth clustering resulting in a high proportion of pair labels that
+    agree, which leads subsequently to a high score.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`:
-   Analysis of the impact of the dataset size on the value of
-   clustering measures for random assignments.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`:
+    Analysis of the impact of the dataset size on the value of clustering measures
+    for random assignments.
 
 
-Mathematical formulation
-~~~~~~~~~~~~~~~~~~~~~~~~
+|details-start|
+**Mathematical formulation**
+|details-split|
 
-If C is a ground truth class assignment and K the clustering, let us
-define :math:`a` and :math:`b` as:
+If C is a ground truth class assignment and K the clustering, let us define
+:math:`a` and :math:`b` as:
 
-- :math:`a`, the number of pairs of elements that are in the same set
-  in C and in the same set in K
+- :math:`a`, the number of pairs of elements that are in the same set in C and
+  in the same set in K
 
-- :math:`b`, the number of pairs of elements that are in different sets
-  in C and in different sets in K
+- :math:`b`, the number of pairs of elements that are in different sets in C and
+  in different sets in K
 
 The unadjusted Rand index is then given by:
 
 .. math:: \text{RI} = \frac{a + b}{C_2^{n_{samples}}}
 
-where :math:`C_2^{n_{samples}}` is the total number of possible pairs
-in the dataset. It does not matter if the calculation is performed on
-ordered pairs or unordered pairs as long as the calculation is
-performed consistently.
-
-However, the Rand index does not guarantee that random label assignments
-will get a value close to zero (esp. if the number of clusters is in
-the same order of magnitude as the number of samples).
+where :math:`C_2^{n_{samples}}` is the total number of possible pairs in the
+dataset. It does not matter if the calculation is performed on ordered pairs or
+unordered pairs as long as the calculation is performed consistently.
+
+However, the Rand index does not guarantee that random label assignments will
+get a value close to zero (esp. if the number of clusters is in the same order
+of magnitude as the number of samples).
 
 To counter this effect we can discount the expected RI :math:`E[\text{RI}]` of
 random labelings by defining the adjusted Rand index as follows:
 
 .. math:: \text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}
 
-.. topic:: References
+|details-end|
+
+|details-start|
+**References**
+|details-split|
 
- * `Comparing Partitions
-   <https://link.springer.com/article/10.1007%2FBF01908075>`_
-   L. Hubert and P. Arabie, Journal of Classification 1985
+* `Comparing Partitions
+  <https://link.springer.com/article/10.1007%2FBF01908075>`_ L. Hubert and P.
+  Arabie, Journal of Classification 1985
 
- * `Properties of the Hubert-Arabie adjusted Rand index
-   <https://psycnet.apa.org/record/2004-17801-007>`_
-   D. Steinley, Psychological Methods 2004
+* `Properties of the Hubert-Arabie adjusted Rand index
+  <https://psycnet.apa.org/record/2004-17801-007>`_ D. Steinley, Psychological
+  Methods 2004
 
- * `Wikipedia entry for the Rand index
-   <https://en.wikipedia.org/wiki/Rand_index>`_
+* `Wikipedia entry for the Rand index
+  <https://en.wikipedia.org/wiki/Rand_index>`_
 
- * `Wikipedia entry for the adjusted Rand index
-   <https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_
+* `Wikipedia entry for the adjusted Rand index
+  <https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_
 
+|details-end|
 
 .. _mutual_info_score:
 
 Mutual Information based scores
 -------------------------------
 
 Given the knowledge of the ground truth class assignments ``labels_true`` and
@@ -1498,52 +1571,47 @@
 
   >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
   >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
   >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
   -0.10526...
 
 
-Advantages
-~~~~~~~~~~
-
-- **Random (uniform) label assignments have a AMI score close to 0.0**
-  for any value of ``n_clusters`` and ``n_samples`` (which is not the
-  case for raw Mutual Information or the V-measure for instance).
+.. topic:: Advantages:
 
-- **Upper bound  of 1**:  Values close to zero indicate two label
-  assignments that are largely independent, while values close to one
-  indicate significant agreement. Further, an AMI of exactly 1 indicates
-  that the two label assignments are equal (with or without permutation).
+  - **Random (uniform) label assignments have a AMI score close to 0.0** for any
+    value of ``n_clusters`` and ``n_samples`` (which is not the case for raw
+    Mutual Information or the V-measure for instance).
 
+  - **Upper bound  of 1**:  Values close to zero indicate two label assignments
+    that are largely independent, while values close to one indicate significant
+    agreement. Further, an AMI of exactly 1 indicates that the two label
+    assignments are equal (with or without permutation).
 
-Drawbacks
-~~~~~~~~~
+.. topic:: Drawbacks:
 
-- Contrary to inertia, **MI-based measures require the knowledge
-  of the ground truth classes** while almost never available in practice or
-  requires manual assignment by human annotators (as in the supervised learning
-  setting).
+  - Contrary to inertia, **MI-based measures require the knowledge of the ground
+    truth classes** while almost never available in practice or requires manual
+    assignment by human annotators (as in the supervised learning setting).
 
-  However MI-based measures can also be useful in purely unsupervised setting as a
-  building block for a Consensus Index that can be used for clustering
-  model selection.
-
-- NMI and MI are not adjusted against chance.
+    However MI-based measures can also be useful in purely unsupervised setting
+    as a building block for a Consensus Index that can be used for clustering
+    model selection.
 
+  - NMI and MI are not adjusted against chance.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis of
-   the impact of the dataset size on the value of clustering measures
-   for random assignments. This example also includes the Adjusted Rand
-   Index.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis
+    of the impact of the dataset size on the value of clustering measures for
+    random assignments. This example also includes the Adjusted Rand Index.
 
 
-Mathematical formulation
-~~~~~~~~~~~~~~~~~~~~~~~~
+|details-start|
+**Mathematical formulation**
+|details-split|
 
 Assume two label assignments (of the same N objects), :math:`U` and :math:`V`.
 Their entropy is the amount of uncertainty for a partition set, defined by:
 
 .. math:: H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))
 
 where :math:`P(i) = |U_i| / N` is the probability that an object picked at
@@ -1569,71 +1637,70 @@
 
 This value of the mutual information and also the normalized variant is not
 adjusted for chance and will tend to increase as the number of different labels
 (clusters) increases, regardless of the actual amount of "mutual information"
 between the label assignments.
 
 The expected value for the mutual information can be calculated using the
-following equation [VEB2009]_. In this equation,
-:math:`a_i = |U_i|` (the number of elements in :math:`U_i`) and
-:math:`b_j = |V_j|` (the number of elements in :math:`V_j`).
-
+following equation [VEB2009]_. In this equation, :math:`a_i = |U_i|` (the number
+of elements in :math:`U_i`) and :math:`b_j = |V_j|` (the number of elements in
+:math:`V_j`).
 
 .. math:: E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
-   }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
-   \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
-   (N-a_i-b_j+n_{ij})!}
+  }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
+  \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
+  (N-a_i-b_j+n_{ij})!}
 
-Using the expected value, the adjusted mutual information can then be
-calculated using a similar form to that of the adjusted Rand index:
+Using the expected value, the adjusted mutual information can then be calculated
+using a similar form to that of the adjusted Rand index:
 
 .. math:: \text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}
 
-For normalized mutual information and adjusted mutual information, the normalizing
-value is typically some *generalized* mean of the entropies of each clustering.
-Various generalized means exist, and no firm rules exist for preferring one over the
-others.  The decision is largely a field-by-field basis; for instance, in community
-detection, the arithmetic mean is most common. Each
-normalizing method provides "qualitatively similar behaviours" [YAT2016]_. In our
-implementation, this is controlled by the ``average_method`` parameter.
-
-Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]_. Their
-'sqrt' and 'sum' averages are the geometric and arithmetic means; we use these
-more broadly common names.
-
-.. topic:: References
-
- * Strehl, Alexander, and Joydeep Ghosh (2002). "Cluster ensembles – a
-   knowledge reuse framework for combining multiple partitions". Journal of
-   Machine Learning Research 3: 583–617.
-   `doi:10.1162/153244303321897735 <http://strehl.com/download/strehl-jmlr02.pdf>`_.
-
- * `Wikipedia entry for the (normalized) Mutual Information
-   <https://en.wikipedia.org/wiki/Mutual_Information>`_
-
- * `Wikipedia entry for the Adjusted Mutual Information
-   <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
-
- .. [VEB2009] Vinh, Epps, and Bailey, (2009). "Information theoretic measures
-   for clusterings comparison". Proceedings of the 26th Annual International
-   Conference on Machine Learning - ICML '09.
-   `doi:10.1145/1553374.1553511 <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_.
-   ISBN 9781605585161.
-
- .. [VEB2010] Vinh, Epps, and Bailey, (2010). "Information Theoretic Measures for
-   Clusterings Comparison: Variants, Properties, Normalization and
-   Correction for Chance". JMLR
-   <https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>
-
- .. [YAT2016] Yang, Algesheimer, and Tessone, (2016). "A comparative analysis of
-   community
-   detection algorithms on artificial networks". Scientific Reports 6: 30750.
-   `doi:10.1038/srep30750 <https://www.nature.com/articles/srep30750>`_.
+For normalized mutual information and adjusted mutual information, the
+normalizing value is typically some *generalized* mean of the entropies of each
+clustering. Various generalized means exist, and no firm rules exist for
+preferring one over the others.  The decision is largely a field-by-field basis;
+for instance, in community detection, the arithmetic mean is most common. Each
+normalizing method provides "qualitatively similar behaviours" [YAT2016]_. In
+our implementation, this is controlled by the ``average_method`` parameter.
+
+Vinh et al. (2010) named variants of NMI and AMI by their averaging method
+[VEB2010]_. Their 'sqrt' and 'sum' averages are the geometric and arithmetic
+means; we use these more broadly common names.
 
+.. topic:: References:
+
+  * Strehl, Alexander, and Joydeep Ghosh (2002). "Cluster ensembles – a
+    knowledge reuse framework for combining multiple partitions". Journal of
+    Machine Learning Research 3: 583–617. `doi:10.1162/153244303321897735
+    <http://strehl.com/download/strehl-jmlr02.pdf>`_.
+
+  * `Wikipedia entry for the (normalized) Mutual Information
+    <https://en.wikipedia.org/wiki/Mutual_Information>`_
+
+  * `Wikipedia entry for the Adjusted Mutual Information
+    <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_
+
+  .. [VEB2009] Vinh, Epps, and Bailey, (2009). "Information theoretic measures
+    for clusterings comparison". Proceedings of the 26th Annual International
+    Conference on Machine Learning - ICML '09. `doi:10.1145/1553374.1553511
+    <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_. ISBN
+    9781605585161.
+
+  .. [VEB2010] Vinh, Epps, and Bailey, (2010). "Information Theoretic Measures
+    for Clusterings Comparison: Variants, Properties, Normalization and
+    Correction for Chance". JMLR
+    <https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>
+
+  .. [YAT2016] Yang, Algesheimer, and Tessone, (2016). "A comparative analysis
+    of community detection algorithms on artificial networks". Scientific
+    Reports 6: 30750. `doi:10.1038/srep30750
+    <https://www.nature.com/articles/srep30750>`_.
 
+|details-end|
 
 .. _homogeneity_completeness:
 
 Homogeneity, completeness and V-measure
 ---------------------------------------
 
 Given the knowledge of the ground truth class assignments of the samples,
@@ -1707,98 +1774,97 @@
 
   This is not the case for :func:`completeness_score` and
   :func:`homogeneity_score`: both are bound by the relationship::
 
     homogeneity_score(a, b) == completeness_score(b, a)
 
 
-Advantages
-~~~~~~~~~~
-
-- **Bounded scores**: 0.0 is as bad as it can be, 1.0 is a perfect score.
+.. topic:: Advantages:
 
-- Intuitive interpretation: clustering with bad V-measure can be
-  **qualitatively analyzed in terms of homogeneity and completeness**
-  to better feel what 'kind' of mistakes is done by the assignment.
-
-- **No assumption is made on the cluster structure**: can be used
-  to compare clustering algorithms such as k-means which assumes isotropic
-  blob shapes with results of spectral clustering algorithms which can
-  find cluster with "folded" shapes.
-
-
-Drawbacks
-~~~~~~~~~
-
-- The previously introduced metrics are **not normalized with regards to
-  random labeling**: this means that depending on the number of samples,
-  clusters and ground truth classes, a completely random labeling will
-  not always yield the same values for homogeneity, completeness and
-  hence v-measure. In particular **random labeling won't yield zero
-  scores especially when the number of clusters is large**.
-
-  This problem can safely be ignored when the number of samples is more
-  than a thousand and the number of clusters is less than 10. **For
-  smaller sample sizes or larger number of clusters it is safer to use
-  an adjusted index such as the Adjusted Rand Index (ARI)**.
-
-.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_adjusted_for_chance_measures_001.png
-   :target: ../auto_examples/cluster/plot_adjusted_for_chance_measures.html
-   :align: center
-   :scale: 100
-
-- These metrics **require the knowledge of the ground truth classes** while
-  almost never available in practice or requires manual assignment by
-  human annotators (as in the supervised learning setting).
+  - **Bounded scores**: 0.0 is as bad as it can be, 1.0 is a perfect score.
 
+  - Intuitive interpretation: clustering with bad V-measure can be
+    **qualitatively analyzed in terms of homogeneity and completeness** to
+    better feel what 'kind' of mistakes is done by the assignment.
+
+  - **No assumption is made on the cluster structure**: can be used to compare
+    clustering algorithms such as k-means which assumes isotropic blob shapes
+    with results of spectral clustering algorithms which can find cluster with
+    "folded" shapes.
+
+.. topic:: Drawbacks:
+
+  - The previously introduced metrics are **not normalized with regards to
+    random labeling**: this means that depending on the number of samples,
+    clusters and ground truth classes, a completely random labeling will not
+    always yield the same values for homogeneity, completeness and hence
+    v-measure. In particular **random labeling won't yield zero scores
+    especially when the number of clusters is large**.
+
+    This problem can safely be ignored when the number of samples is more than a
+    thousand and the number of clusters is less than 10. **For smaller sample
+    sizes or larger number of clusters it is safer to use an adjusted index such
+    as the Adjusted Rand Index (ARI)**.
+
+  .. figure:: ../auto_examples/cluster/images/sphx_glr_plot_adjusted_for_chance_measures_001.png
+    :target: ../auto_examples/cluster/plot_adjusted_for_chance_measures.html
+    :align: center
+    :scale: 100
+
+  - These metrics **require the knowledge of the ground truth classes** while
+    almost never available in practice or requires manual assignment by human
+    annotators (as in the supervised learning setting).
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis of
-   the impact of the dataset size on the value of clustering measures
-   for random assignments.
+  * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis
+    of the impact of the dataset size on the value of clustering measures for
+    random assignments.
 
 
-Mathematical formulation
-~~~~~~~~~~~~~~~~~~~~~~~~
+|details-start|
+**Mathematical formulation**
+|details-split|
 
 Homogeneity and completeness scores are formally given by:
 
 .. math:: h = 1 - \frac{H(C|K)}{H(C)}
 
 .. math:: c = 1 - \frac{H(K|C)}{H(K)}
 
-where :math:`H(C|K)` is the **conditional entropy of the classes given
-the cluster assignments** and is given by:
+where :math:`H(C|K)` is the **conditional entropy of the classes given the
+cluster assignments** and is given by:
 
 .. math:: H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
           \cdot \log\left(\frac{n_{c,k}}{n_k}\right)
 
 and :math:`H(C)` is the **entropy of the classes** and is given by:
 
 .. math:: H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)
 
-with :math:`n` the total number of samples, :math:`n_c` and :math:`n_k`
-the number of samples respectively belonging to class :math:`c` and
-cluster :math:`k`, and finally :math:`n_{c,k}` the number of samples
-from class :math:`c` assigned to cluster :math:`k`.
+with :math:`n` the total number of samples, :math:`n_c` and :math:`n_k` the
+number of samples respectively belonging to class :math:`c` and cluster
+:math:`k`, and finally :math:`n_{c,k}` the number of samples from class
+:math:`c` assigned to cluster :math:`k`.
 
 The **conditional entropy of clusters given class** :math:`H(K|C)` and the
 **entropy of clusters** :math:`H(K)` are defined in a symmetric manner.
 
-Rosenberg and Hirschberg further define **V-measure** as the **harmonic
-mean of homogeneity and completeness**:
+Rosenberg and Hirschberg further define **V-measure** as the **harmonic mean of
+homogeneity and completeness**:
 
 .. math:: v = 2 \cdot \frac{h \cdot c}{h + c}
 
-.. topic:: References
+|details-end|
+
+.. topic:: References:
 
- * `V-Measure: A conditional entropy-based external cluster evaluation
-   measure <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_
-   Andrew Rosenberg and Julia Hirschberg, 2007
+ * `V-Measure: A conditional entropy-based external cluster evaluation measure
+   <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_ Andrew Rosenberg and Julia
+   Hirschberg, 2007
 
  .. [B2011] `Identification and Characterization of Events in Social Media
    <http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf>`_, Hila
    Becker, PhD Thesis.
 
 .. _fowlkes_mallows_scores:
 
@@ -1847,49 +1913,51 @@
 Bad (e.g. independent labelings) have zero scores::
 
   >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
   >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
   >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
   0.0
 
-Advantages
-~~~~~~~~~~
-
-- **Random (uniform) label assignments have a FMI score close to 0.0**
-  for any value of ``n_clusters`` and ``n_samples`` (which is not the
-  case for raw Mutual Information or the V-measure for instance).
-
-- **Upper-bounded at 1**:  Values close to zero indicate two label
-  assignments that are largely independent, while values close to one
-  indicate significant agreement. Further, values of exactly 0 indicate
-  **purely** independent label assignments and a FMI of exactly 1 indicates
-  that the two label assignments are equal (with or without permutation).
+.. topic:: Advantages:
 
-- **No assumption is made on the cluster structure**: can be used
-  to compare clustering algorithms such as k-means which assumes isotropic
-  blob shapes with results of spectral clustering algorithms which can
-  find cluster with "folded" shapes.
+  - **Random (uniform) label assignments have a FMI score close to 0.0** for any
+    value of ``n_clusters`` and ``n_samples`` (which is not the case for raw
+    Mutual Information or the V-measure for instance).
+
+  - **Upper-bounded at 1**:  Values close to zero indicate two label assignments
+    that are largely independent, while values close to one indicate significant
+    agreement. Further, values of exactly 0 indicate **purely** independent
+    label assignments and a FMI of exactly 1 indicates that the two label
+    assignments are equal (with or without permutation).
+
+  - **No assumption is made on the cluster structure**: can be used to compare
+    clustering algorithms such as k-means which assumes isotropic blob shapes
+    with results of spectral clustering algorithms which can find cluster with
+    "folded" shapes.
+
+.. topic:: Drawbacks:
+
+  - Contrary to inertia, **FMI-based measures require the knowledge of the
+    ground truth classes** while almost never available in practice or requires
+    manual assignment by human annotators (as in the supervised learning
+    setting).
+
+|details-start|
+**References**
+|details-split|
+
+* E. B. Fowkles and C. L. Mallows, 1983. "A method for comparing two
+  hierarchical clusterings". Journal of the American Statistical
+  Association.
+  https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
 
+* `Wikipedia entry for the Fowlkes-Mallows Index
+  <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_
 
-Drawbacks
-~~~~~~~~~
-
-- Contrary to inertia, **FMI-based measures require the knowledge
-  of the ground truth classes** while almost never available in practice or
-  requires manual assignment by human annotators (as in the supervised learning
-  setting).
-
-.. topic:: References
-
-  * E. B. Fowkles and C. L. Mallows, 1983. "A method for comparing two
-    hierarchical clusterings". Journal of the American Statistical Association.
-    https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
-
-  * `Wikipedia entry for the Fowlkes-Mallows Index
-    <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_
+|details-end|
 
 .. _silhouette_coefficient:
 
 Silhouette Coefficient
 ----------------------
 
 If the ground truth labels are not known, evaluation must be performed using
@@ -1925,43 +1993,46 @@
   >>> import numpy as np
   >>> from sklearn.cluster import KMeans
   >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
   >>> labels = kmeans_model.labels_
   >>> metrics.silhouette_score(X, labels, metric='euclidean')
   0.55...
 
-.. topic:: References
 
- * Peter J. Rousseeuw (1987). :doi:`"Silhouettes: a Graphical Aid to the
-   Interpretation and Validation of Cluster Analysis"<10.1016/0377-0427(87)90125-7>`
-   . Computational and Applied Mathematics 20: 53–65.
+.. topic:: Advantages:
 
+  - The score is bounded between -1 for incorrect clustering and +1 for highly
+    dense clustering. Scores around zero indicate overlapping clusters.
 
-Advantages
-~~~~~~~~~~
+  - The score is higher when clusters are dense and well separated, which
+    relates to a standard concept of a cluster.
 
-- The score is bounded between -1 for incorrect clustering and +1 for highly
-  dense clustering. Scores around zero indicate overlapping clusters.
+.. topic:: Drawbacks:
 
-- The score is higher when clusters are dense and well separated, which relates
-  to a standard concept of a cluster.
+  - The Silhouette Coefficient is generally higher for convex clusters than
+    other concepts of clusters, such as density based clusters like those
+    obtained through DBSCAN.
 
+.. topic:: Examples:
 
-Drawbacks
-~~~~~~~~~
+  * :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py` : In
+    this example the silhouette analysis is used to choose an optimal value for
+    n_clusters.
 
-- The Silhouette Coefficient is generally higher for convex clusters than other
-  concepts of clusters, such as density based clusters like those obtained
-  through DBSCAN.
 
-.. topic:: Examples:
+|details-start|
+**References**
+|details-split|
 
- * :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py` : In this example
-   the silhouette analysis is used to choose an optimal value for n_clusters.
+* Peter J. Rousseeuw (1987). :doi:`"Silhouettes: a Graphical Aid to the
+  Interpretation and Validation of Cluster
+  Analysis"<10.1016/0377-0427(87)90125-7>` . Computational and Applied
+  Mathematics 20: 53–65.
 
+|details-end|
 
 .. _calinski_harabasz_index:
 
 Calinski-Harabasz Index
 -----------------------
 
 
@@ -1985,59 +2056,64 @@
   >>> import numpy as np
   >>> from sklearn.cluster import KMeans
   >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
   >>> labels = kmeans_model.labels_
   >>> metrics.calinski_harabasz_score(X, labels)
   561.59...
 
-Advantages
-~~~~~~~~~~
 
-- The score is higher when clusters are dense and well separated, which relates
-  to a standard concept of a cluster.
+.. topic:: Advantages:
 
-- The score is fast to compute.
+  - The score is higher when clusters are dense and well separated, which
+    relates to a standard concept of a cluster.
 
+  - The score is fast to compute.
 
-Drawbacks
-~~~~~~~~~
+.. topic:: Drawbacks:
 
-- The Calinski-Harabasz index is generally higher for convex clusters than other
-  concepts of clusters, such as density based clusters like those obtained
-  through DBSCAN.
+  - The Calinski-Harabasz index is generally higher for convex clusters than
+    other concepts of clusters, such as density based clusters like those
+    obtained through DBSCAN.
 
-Mathematical formulation
-~~~~~~~~~~~~~~~~~~~~~~~~
+|details-start|
+**Mathematical formulation**
+|details-split|
 
 For a set of data :math:`E` of size :math:`n_E` which has been clustered into
 :math:`k` clusters, the Calinski-Harabasz score :math:`s` is defined as the
-ratio of the between-clusters dispersion mean and the within-cluster dispersion:
+ratio of the between-clusters dispersion mean and the within-cluster
+dispersion:
 
 .. math::
   s = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1}
 
 where :math:`\mathrm{tr}(B_k)` is trace of the between group dispersion matrix
 and :math:`\mathrm{tr}(W_k)` is the trace of the within-cluster dispersion
 matrix defined by:
 
 .. math:: W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T
 
 .. math:: B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T
 
-with :math:`C_q` the set of points in cluster :math:`q`, :math:`c_q` the center
-of cluster :math:`q`, :math:`c_E` the center of :math:`E`, and :math:`n_q` the
-number of points in cluster :math:`q`.
-
-.. topic:: References
-
- * Caliński, T., & Harabasz, J. (1974).
-   `"A Dendrite Method for Cluster Analysis"
-   <https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis>`_.
-   :doi:`Communications in Statistics-theory and Methods 3: 1-27 <10.1080/03610927408827101>`.
+with :math:`C_q` the set of points in cluster :math:`q`, :math:`c_q` the
+center of cluster :math:`q`, :math:`c_E` the center of :math:`E`, and
+:math:`n_q` the number of points in cluster :math:`q`.
+
+|details-end|
+
+|details-start|
+**References**
+|details-split|
+
+* Caliński, T., & Harabasz, J. (1974). `"A Dendrite Method for Cluster Analysis"
+  <https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis>`_.
+  :doi:`Communications in Statistics-theory and Methods 3: 1-27
+  <10.1080/03610927408827101>`.
 
+|details-end|
 
 .. _davies-bouldin_index:
 
 Davies-Bouldin Index
 --------------------
 
 If the ground truth labels are not known, the Davies-Bouldin index
@@ -2062,66 +2138,71 @@
   >>> from sklearn.metrics import davies_bouldin_score
   >>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)
   >>> labels = kmeans.labels_
   >>> davies_bouldin_score(X, labels)
   0.666...
 
 
-Advantages
-~~~~~~~~~~
+.. topic:: Advantages:
 
-- The computation of Davies-Bouldin is simpler than that of Silhouette scores.
-- The index is solely based on quantities and features inherent to the dataset
-  as its computation only uses point-wise distances.
-
-Drawbacks
-~~~~~~~~~
-
-- The Davies-Boulding index is generally higher for convex clusters than other
-  concepts of clusters, such as density based clusters like those obtained from
-  DBSCAN.
-- The usage of centroid distance limits the distance metric to Euclidean space.
+  - The computation of Davies-Bouldin is simpler than that of Silhouette scores.
+  - The index is solely based on quantities and features inherent to the dataset
+    as its computation only uses point-wise distances.
 
-Mathematical formulation
-~~~~~~~~~~~~~~~~~~~~~~~~
+.. topic:: Drawbacks:
+
+  - The Davies-Boulding index is generally higher for convex clusters than other
+    concepts of clusters, such as density based clusters like those obtained
+    from DBSCAN.
+  - The usage of centroid distance limits the distance metric to Euclidean
+    space.
+
+
+|details-start|
+**Mathematical formulation**
+|details-split|
 
 The index is defined as the average similarity between each cluster :math:`C_i`
 for :math:`i=1, ..., k` and its most similar one :math:`C_j`. In the context of
 this index, similarity is defined as a measure :math:`R_{ij}` that trades off:
 
 - :math:`s_i`, the average distance between each point of cluster :math:`i` and
   the centroid of that cluster -- also know as cluster diameter.
-- :math:`d_{ij}`, the distance between cluster centroids :math:`i` and :math:`j`.
+- :math:`d_{ij}`, the distance between cluster centroids :math:`i` and
+  :math:`j`.
 
 A simple choice to construct :math:`R_{ij}` so that it is nonnegative and
 symmetric is:
 
 .. math::
-   R_{ij} = \frac{s_i + s_j}{d_{ij}}
+  R_{ij} = \frac{s_i + s_j}{d_{ij}}
 
 Then the Davies-Bouldin index is defined as:
 
 .. math::
-   DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}
+  DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}
 
+|details-end|
 
-.. topic:: References
+|details-start|
+**References**
+|details-split|
 
- * Davies, David L.; Bouldin, Donald W. (1979).
-   :doi:`"A Cluster Separation Measure" <10.1109/TPAMI.1979.4766909>`
-   IEEE Transactions on Pattern Analysis and Machine Intelligence.
-   PAMI-1 (2): 224-227.
+* Davies, David L.; Bouldin, Donald W. (1979). :doi:`"A Cluster Separation
+  Measure" <10.1109/TPAMI.1979.4766909>` IEEE Transactions on Pattern Analysis
+  and Machine Intelligence. PAMI-1 (2): 224-227.
 
- * Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001).
-   :doi:`"On Clustering Validation Techniques" <10.1023/A:1012801612483>`
-   Journal of Intelligent Information Systems, 17(2-3), 107-145.
+* Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). :doi:`"On
+  Clustering Validation Techniques" <10.1023/A:1012801612483>` Journal of
+  Intelligent Information Systems, 17(2-3), 107-145.
 
- * `Wikipedia entry for Davies-Bouldin index
-   <https://en.wikipedia.org/wiki/Davies–Bouldin_index>`_.
+* `Wikipedia entry for Davies-Bouldin index
+  <https://en.wikipedia.org/wiki/Davies–Bouldin_index>`_.
 
+|details-end|
 
 .. _contingency_matrix:
 
 Contingency Matrix
 ------------------
 
 Contingency matrix (:func:`sklearn.metrics.cluster.contingency_matrix`)
@@ -2146,38 +2227,40 @@
 1 and two are in 2.
 
 A :ref:`confusion matrix <confusion_matrix>` for classification is a square
 contingency matrix where the order of rows and columns correspond to a list
 of classes.
 
 
-Advantages
-~~~~~~~~~~
+.. topic:: Advantages:
 
-- Allows to examine the spread of each true cluster across predicted
-  clusters and vice versa.
+  - Allows to examine the spread of each true cluster across predicted clusters
+    and vice versa.
 
-- The contingency table calculated is typically utilized in the calculation
-  of a similarity statistic (like the others listed in this document) between
-  the two clusterings.
+  - The contingency table calculated is typically utilized in the calculation of
+    a similarity statistic (like the others listed in this document) between the
+    two clusterings.
 
-Drawbacks
-~~~~~~~~~
+.. topic:: Drawbacks:
 
-- Contingency matrix is easy to interpret for a small number of clusters, but
-  becomes very hard to interpret for a large number of clusters.
+  - Contingency matrix is easy to interpret for a small number of clusters, but
+    becomes very hard to interpret for a large number of clusters.
 
-- It doesn't give a single metric to use as an objective for clustering
-  optimisation.
+  - It doesn't give a single metric to use as an objective for clustering
+    optimisation.
 
 
-.. topic:: References
+|details-start|
+**References**
+|details-split|
 
- * `Wikipedia entry for contingency matrix
-   <https://en.wikipedia.org/wiki/Contingency_table>`_
+* `Wikipedia entry for contingency matrix
+  <https://en.wikipedia.org/wiki/Contingency_table>`_
+
+|details-end|
 
 .. _pair_confusion_matrix:
 
 Pair Confusion Matrix
 ---------------------
 
 The pair confusion matrix
@@ -2247,11 +2330,15 @@
 assignment is totally incomplete, hence the matrix has all zero
 diagonal entries::
 
    >>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])
    array([[ 0,  0],
           [12,  0]])
 
-.. topic:: References
+|details-start|
+**References**
+|details-split|
+
+ * :doi:`"Comparing Partitions" <10.1007/BF01908075>` L. Hubert and P. Arabie,
+   Journal of Classification 1985
 
- * :doi:`"Comparing Partitions" <10.1007/BF01908075>`
-   L. Hubert and P. Arabie, Journal of Classification 1985
+|details-end|
```

### Comparing `scikit-learn-1.4.2/doc/modules/compose.rst` & `scikit_learn-1.5.0rc1/doc/modules/compose.rst`

 * *Files 2% similar despite different names*

```diff
@@ -250,43 +250,43 @@
 |details-split|
 
 Using a :class:`Pipeline` without cache enabled, it is possible to
 inspect the original instance such as::
 
     >>> from sklearn.datasets import load_digits
     >>> X_digits, y_digits = load_digits(return_X_y=True)
-    >>> pca1 = PCA()
+    >>> pca1 = PCA(n_components=10)
     >>> svm1 = SVC()
     >>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])
     >>> pipe.fit(X_digits, y_digits)
-    Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])
+    Pipeline(steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])
     >>> # The pca instance can be inspected directly
-    >>> print(pca1.components_)
-        [[-1.77484909e-19  ... 4.07058917e-18]]
+    >>> pca1.components_.shape
+    (10, 64)
 
 
 Enabling caching triggers a clone of the transformers before fitting.
 Therefore, the transformer instance given to the pipeline cannot be
 inspected directly.
 In following example, accessing the :class:`~sklearn.decomposition.PCA`
 instance ``pca2`` will raise an ``AttributeError`` since ``pca2`` will be an
 unfitted transformer.
 Instead, use the attribute ``named_steps`` to inspect estimators within
 the pipeline::
 
     >>> cachedir = mkdtemp()
-    >>> pca2 = PCA()
+    >>> pca2 = PCA(n_components=10)
     >>> svm2 = SVC()
     >>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],
     ...                        memory=cachedir)
     >>> cached_pipe.fit(X_digits, y_digits)
     Pipeline(memory=...,
-             steps=[('reduce_dim', PCA()), ('clf', SVC())])
-    >>> print(cached_pipe.named_steps['reduce_dim'].components_)
-        [[-1.77484909e-19  ... 4.07058917e-18]]
+             steps=[('reduce_dim', PCA(n_components=10)), ('clf', SVC())])
+    >>> cached_pipe.named_steps['reduce_dim'].components_.shape
+    (10, 64)
     >>> # Remove the cache directory
     >>> rmtree(cachedir)
 
 
 |details-end|
 
 .. topic:: Examples:
```

### Comparing `scikit-learn-1.4.2/doc/modules/covariance.rst` & `scikit_learn-1.5.0rc1/doc/modules/covariance.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/cross_decomposition.rst` & `scikit_learn-1.5.0rc1/doc/modules/cross_decomposition.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/cross_validation.rst` & `scikit_learn-1.5.0rc1/doc/modules/cross_validation.rst`

 * *Files 2% similar despite different names*

```diff
@@ -166,15 +166,17 @@
   ...         yield idx, idx
   ...         i += 1
   ...
   >>> custom_cv = custom_cv_2folds(X)
   >>> cross_val_score(clf, X, y, cv=custom_cv)
   array([1.        , 0.973...])
 
-.. topic:: Data transformation with held out data
+|details-start|
+**Data transformation with held out data**
+|details-split|
 
     Just as it is important to test a predictor on data held-out from
     training, preprocessing (such as standardization, feature selection, etc.)
     and similar :ref:`data transformations <data-transforms>` similarly should
     be learnt from a training set and applied to held-out data for prediction::
 
       >>> from sklearn import preprocessing
@@ -193,14 +195,15 @@
       >>> from sklearn.pipeline import make_pipeline
       >>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
       >>> cross_val_score(clf, X, y, cv=cv)
       array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])
 
     See :ref:`combining_estimators`.
 
+|details-end|
 
 .. _multimetric_cross_validation:
 
 The cross_validate function and multiple metric evaluation
 ----------------------------------------------------------
 
 The :func:`cross_validate` function differs from :func:`cross_val_score` in
@@ -435,29 +438,32 @@
 
 However, if the learning curve is steep for the training size in question,
 then 5- or 10- fold cross validation can overestimate the generalization error.
 
 As a general rule, most authors, and empirical evidence, suggest that 5- or 10-
 fold cross validation should be preferred to LOO.
 
-
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
  * `<http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>`_;
  * T. Hastie, R. Tibshirani, J. Friedman,  `The Elements of Statistical Learning
    <https://web.stanford.edu/~hastie/ElemStatLearn/>`_, Springer 2009
  * L. Breiman, P. Spector `Submodel selection and evaluation in regression: The X-random case
    <https://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf>`_, International Statistical Review 1992;
  * R. Kohavi, `A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection
    <https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf>`_, Intl. Jnt. Conf. AI
  * R. Bharat Rao, G. Fung, R. Rosales, `On the Dangers of Cross-Validation. An Experimental Evaluation
    <https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf>`_, SIAM 2008;
  * G. James, D. Witten, T. Hastie, R Tibshirani, `An Introduction to
    Statistical Learning <https://www.statlearning.com>`_, Springer 2013.
 
+|details-end|
+
 .. _leave_p_out:
 
 Leave P Out (LPO)
 ^^^^^^^^^^^^^^^^^
 
 :class:`LeavePOut` is very similar to :class:`LeaveOneOut` as it creates all
 the possible training/test sets by removing :math:`p` samples from the complete
@@ -587,14 +593,27 @@
 Here is a visualization of the cross-validation behavior.
 
 .. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_012.png
    :target: ../auto_examples/model_selection/plot_cv_indices.html
    :align: center
    :scale: 75%
 
+.. _predefined_split:
+
+Predefined fold-splits / Validation-sets
+----------------------------------------
+
+For some datasets, a pre-defined split of the data into training- and
+validation fold or into several cross-validation folds already
+exists. Using :class:`PredefinedSplit` it is possible to use these folds
+e.g. when searching for hyperparameters.
+
+For example, when using a validation set, set the ``test_fold`` to 0 for all
+samples that are part of the validation set, and to -1 for all other samples.
+
 .. _group_cv:
 
 Cross-validation iterators for grouped data
 -------------------------------------------
 
 The i.i.d. assumption is broken if the underlying generative process yield
 groups of dependent samples.
@@ -677,15 +696,17 @@
   >>> sgkf = StratifiedGroupKFold(n_splits=3)
   >>> for train, test in sgkf.split(X, y, groups=groups):
   ...     print("%s %s" % (train, test))
   [ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]
   [ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]
   [ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]
 
-Implementation notes:
+|details-start|
+**Implementation notes**
+|details-split|
 
 - With the current implementation full shuffle is not possible in most
   scenarios. When shuffle=True, the following happens:
 
   1. All groups are shuffled.
   2. Groups are sorted by standard deviation of classes using stable sort.
   3. Sorted groups are iterated over and assigned to folds.
@@ -698,14 +719,16 @@
   across test sets. Group assignment proceeds from groups with highest to
   lowest variance in class frequency, i.e. large groups peaked on one or few
   classes are assigned first.
 - This split is suboptimal in a sense that it might produce imbalanced splits
   even if perfect stratification is possible. If you have relatively close
   distribution of classes in each group, using :class:`GroupKFold` is better.
 
+|details-end|
+
 Here is a visualization of cross-validation behavior for uneven groups:
 
 .. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_005.png
    :target: ../auto_examples/model_selection/plot_cv_indices.html
    :align: center
    :scale: 75%
 
@@ -804,27 +827,14 @@
 This class is useful when the behavior of :class:`LeavePGroupsOut` is
 desired, but the number of groups is large enough that generating all
 possible partitions with :math:`P` groups withheld would be prohibitively
 expensive. In such a scenario, :class:`GroupShuffleSplit` provides
 a random sample (with replacement) of the train / test splits
 generated by :class:`LeavePGroupsOut`.
 
-.. _predefined_split:
-
-Predefined fold-splits / Validation-sets
-----------------------------------------
-
-For some datasets, a pre-defined split of the data into training- and
-validation fold or into several cross-validation folds already
-exists. Using :class:`PredefinedSplit` it is possible to use these folds
-e.g. when searching for hyperparameters.
-
-For example, when using a validation set, set the ``test_fold`` to 0 for all
-samples that are part of the validation set, and to -1 for all other samples.
-
 Using cross-validation iterators to split train and test
 --------------------------------------------------------
 
 The above group cross-validation functions may also be useful for splitting a
 dataset into training and testing subsets. Note that the convenience
 function :func:`train_test_split` is a wrapper around :func:`ShuffleSplit`
 and thus only allows for stratified splitting (using the class labels)
@@ -989,12 +999,16 @@
 It is therefore only tractable with small datasets for which fitting an
 individual model is very fast.
 
 .. topic:: Examples
 
     * :ref:`sphx_glr_auto_examples_model_selection_plot_permutation_tests_for_classification.py`
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
  * Ojala and Garriga. `Permutation Tests for Studying Classifier Performance
    <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_.
    J. Mach. Learn. Res. 2010.
+
+|details-end|
```

### Comparing `scikit-learn-1.4.2/doc/modules/decomposition.rst` & `scikit_learn-1.5.0rc1/doc/modules/decomposition.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/density.rst` & `scikit_learn-1.5.0rc1/doc/modules/density.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/ensemble.rst` & `scikit_learn-1.5.0rc1/doc/modules/ensemble.rst`

 * *Files 3% similar despite different names*

```diff
@@ -76,15 +76,16 @@
 sorted continuous values when building the trees. The API of these
 estimators is slightly different, and some of the features from
 :class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`
 are not yet supported, for instance some loss functions.
 
 .. topic:: Examples:
 
- * :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`
+  * :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`
+  * :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`
 
 Usage
 ^^^^^
 
 Most of the parameters are unchanged from
 :class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`.
 One exception is the ``max_iter`` parameter that replaces ``n_estimators``, and
@@ -110,29 +111,74 @@
 and categorical cross-entropy as alternative names. The appropriate loss version is
 selected based on :term:`y` passed to :term:`fit`.
 
 The size of the trees can be controlled through the ``max_leaf_nodes``,
 ``max_depth``, and ``min_samples_leaf`` parameters.
 
 The number of bins used to bin the data is controlled with the ``max_bins``
-parameter. Using less bins acts as a form of regularization. It is
-generally recommended to use as many bins as possible (256), which is the default.
+parameter. Using less bins acts as a form of regularization. It is generally
+recommended to use as many bins as possible (255), which is the default.
+
+The ``l2_regularization`` parameter acts as a regularizer for the loss function,
+and corresponds to :math:`\lambda` in the following expression (see equation (2)
+in [XGBoost]_):
+
+.. math::
 
-The ``l2_regularization`` parameter is a regularizer on the loss function and
-corresponds to :math:`\lambda` in equation (2) of [XGBoost]_.
+    \mathcal{L}(\phi) =  \sum_i l(\hat{y}_i, y_i) + \frac12 \sum_k \lambda ||w_k||^2
+
+|details-start|
+**Details on l2 regularization**:
+|details-split|
+
+It is important to notice that the loss term :math:`l(\hat{y}_i, y_i)` describes
+only half of the actual loss function except for the pinball loss and absolute
+error.
+
+The index :math:`k` refers to the k-th tree in the ensemble of trees. In the
+case of regression and binary classification, gradient boosting models grow one
+tree per iteration, then :math:`k` runs up to `max_iter`. In the case of
+multiclass classification problems, the maximal value of the index :math:`k` is
+`n_classes` :math:`\times` `max_iter`.
+
+If :math:`T_k` denotes the number of leaves in the k-th tree, then :math:`w_k`
+is a vector of length :math:`T_k`, which contains the leaf values of the form `w
+= -sum_gradient / (sum_hessian + l2_regularization)` (see equation (5) in
+[XGBoost]_).
+
+The leaf values :math:`w_k` are derived by dividing the sum of the gradients of
+the loss function by the combined sum of hessians. Adding the regularization to
+the denominator penalizes the leaves with small hessians (flat regions),
+resulting in smaller updates. Those :math:`w_k` values contribute then to the
+model's prediction for a given input that ends up in the corresponding leaf. The
+final prediction is the sum of the base prediction and the contributions from
+each tree. The result of that sum is then transformed by the inverse link
+function depending on the choice of the loss function (see
+:ref:`gradient_boosting_formulation`).
+
+Notice that the original paper [XGBoost]_ introduces a term :math:`\gamma\sum_k
+T_k` that penalizes the number of leaves (making it a smooth version of
+`max_leaf_nodes`) not presented here as it is not implemented in scikit-learn;
+whereas :math:`\lambda` penalizes the magnitude of the individual tree
+predictions before being rescaled by the learning rate, see
+:ref:`gradient_boosting_shrinkage`.
+
+|details-end|
 
 Note that **early-stopping is enabled by default if the number of samples is
 larger than 10,000**. The early-stopping behaviour is controlled via the
 ``early_stopping``, ``scoring``, ``validation_fraction``,
 ``n_iter_no_change``, and ``tol`` parameters. It is possible to early-stop
 using an arbitrary :term:`scorer`, or just the training or validation loss.
 Note that for technical reasons, using a callable as a scorer is significantly slower
 than using the loss. By default, early-stopping is performed if there are at least
 10,000 samples in the training set, using the validation loss.
 
+.. _nan_support_hgbt:
+
 Missing values support
 ^^^^^^^^^^^^^^^^^^^^^^
 
 :class:`HistGradientBoostingClassifier` and
 :class:`HistGradientBoostingRegressor` have built-in support for missing
 values (NaNs).
 
@@ -163,14 +209,18 @@
   >>> gbdt.predict(X)
   array([0, 1, 0, 0, 1])
 
 If no missing values were encountered for a given feature during training,
 then samples with missing values are mapped to whichever child has the most
 samples.
 
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`
+
 .. _sw_hgbdt:
 
 Sample weight support
 ^^^^^^^^^^^^^^^^^^^^^
 
 :class:`HistGradientBoostingClassifier` and
 :class:`HistGradientBoostingRegressor` support sample weights during
@@ -248,29 +298,35 @@
 If there are missing values during training, the missing values will be
 treated as a proper category. If there are no missing values during training,
 then at prediction time, missing values are mapped to the child node that has
 the most samples (just like for continuous features). When predicting,
 categories that were not seen during fit time will be treated as missing
 values.
 
-**Split finding with categorical features**: The canonical way of considering
+|details-start|
+**Split finding with categorical features**:
+|details-split|
+
+The canonical way of considering
 categorical splits in a tree is to consider
 all of the :math:`2^{K - 1} - 1` partitions, where :math:`K` is the number of
 categories. This can quickly become prohibitive when :math:`K` is large.
 Fortunately, since gradient boosting trees are always regression trees (even
 for classification problems), there exist a faster strategy that can yield
 equivalent splits. First, the categories of a feature are sorted according to
 the variance of the target, for each category `k`. Once the categories are
 sorted, one can consider *continuous partitions*, i.e. treat the categories
 as if they were ordered continuous values (see Fisher [Fisher1958]_ for a
 formal proof). As a result, only :math:`K - 1` splits need to be considered
 instead of :math:`2^{K - 1} - 1`. The initial sorting is a
 :math:`\mathcal{O}(K \log(K))` operation, leading to a total complexity of
 :math:`\mathcal{O}(K \log(K) + K)`, instead of :math:`\mathcal{O}(2^K)`.
 
+|details-end|
+
 .. topic:: Examples:
 
   * :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`
 
 .. _monotonic_cst_gbdt:
 
 Monotonic Constraints
@@ -321,14 +377,15 @@
 .. note::
     Since categories are unordered quantities, it is not possible to enforce
     monotonic constraints on categorical features.
 
 .. topic:: Examples:
 
   * :ref:`sphx_glr_auto_examples_ensemble_plot_monotonic_constraints.py`
+  * :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`
 
 .. _interaction_cst_hgbt:
 
 Interaction constraints
 ^^^^^^^^^^^^^^^^^^^^^^^
 
 A priori, the histogram gradient boosted trees are allowed to use any feature
@@ -440,16 +497,17 @@
 :class:`GradientBoostingClassifier` and :class:`GradientBoostingRegressor`
 ----------------------------------------------------------------------------
 
 The usage and the parameters of :class:`GradientBoostingClassifier` and
 :class:`GradientBoostingRegressor` are described below. The 2 most important
 parameters of these estimators are `n_estimators` and `learning_rate`.
 
-Classification
-^^^^^^^^^^^^^^^
+|details-start|
+**Classification**
+|details-split|
 
 :class:`GradientBoostingClassifier` supports both binary and multi-class
 classification.
 The following example shows how to fit a gradient boosting classifier
 with 100 decision stumps as weak learners::
 
     >>> from sklearn.datasets import make_hastie_10_2
@@ -478,16 +536,19 @@
    of ``n_classes`` regression trees at each iteration,
    thus, the total number of induced trees equals
    ``n_classes * n_estimators``. For datasets with a large number
    of classes we strongly recommend to use
    :class:`HistGradientBoostingClassifier` as an alternative to
    :class:`GradientBoostingClassifier` .
 
-Regression
-^^^^^^^^^^^
+|details-end|
+
+|details-start|
+**Regression**
+|details-split|
 
 :class:`GradientBoostingRegressor` supports a number of
 :ref:`different loss functions <gradient_boosting_loss>`
 for regression which can be specified via the argument
 ``loss``; the default loss function for regression is squared error
 (``'squared_error'``).
 
@@ -520,14 +581,16 @@
 to determine the optimal number of trees (i.e. ``n_estimators``) by early stopping.
 
 .. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regression_001.png
    :target: ../auto_examples/ensemble/plot_gradient_boosting_regression.html
    :align: center
    :scale: 75
 
+|details-end|
+
 .. topic:: Examples:
 
  * :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`
  * :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`
 
 .. _gradient_boosting_warm_start:
 
@@ -570,22 +633,25 @@
 We found that ``max_leaf_nodes=k`` gives comparable results to ``max_depth=k-1``
 but is significantly faster to train at the expense of a slightly higher
 training error.
 The parameter ``max_leaf_nodes`` corresponds to the variable ``J`` in the
 chapter on gradient boosting in [Friedman2001]_ and is related to the parameter
 ``interaction.depth`` in R's gbm package where ``max_leaf_nodes == interaction.depth + 1`` .
 
+.. _gradient_boosting_formulation:
+
 Mathematical formulation
 ^^^^^^^^^^^^^^^^^^^^^^^^
 
 We first present GBRT for regression, and then detail the classification
 case.
 
-Regression
-...........
+|details-start|
+**Regression**
+|details-split|
 
 GBRT regressors are additive models whose prediction :math:`\hat{y}_i` for a
 given input :math:`x_i` is of the following form:
 
 .. math::
 
   \hat{y}_i = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)
@@ -659,16 +725,19 @@
   are :math:`\pm 1`, the values predicted by a fitted :math:`h_m` are not
   accurate enough: the tree can only output integer values. As a result, the
   leaves values of the tree :math:`h_m` are modified once the tree is
   fitted, such that the leaves values minimize the loss :math:`L_m`. The
   update is loss-dependent: for the absolute error loss, the value of
   a leaf is updated to the median of the samples in that leaf.
 
-Classification
-..............
+|details-end|
+
+|details-start|
+**Classification**
+|details-split|
 
 Gradient boosting for classification is very similar to the regression case.
 However, the sum of the trees :math:`F_M(x_i) = \sum_m h_m(x_i)` is not
 homogeneous to a prediction: it cannot be a class, since the trees predict
 continuous values.
 
 The mapping from the value :math:`F_M(x_i)` to a class or a probability is
@@ -681,23 +750,27 @@
 k is modeled as a softmax of the :math:`F_{M,k}(x_i)` values.
 
 Note that even for a classification task, the :math:`h_m` sub-estimator is
 still a regressor, not a classifier. This is because the sub-estimators are
 trained to predict (negative) *gradients*, which are always continuous
 quantities.
 
+|details-end|
+
 .. _gradient_boosting_loss:
 
 Loss Functions
 ^^^^^^^^^^^^^^
 
 The following loss functions are supported and can be specified using
 the parameter ``loss``:
 
-* Regression
+|details-start|
+**Regression**
+|details-split|
 
   * Squared error (``'squared_error'``): The natural choice for regression
     due to its superior computational properties. The initial model is
     given by the mean of the target values.
   * Absolute error (``'absolute_error'``): A robust loss function for
     regression. The initial model is given by the median of the
     target values.
@@ -706,15 +779,20 @@
     control the sensitivity with regards to outliers (see [Friedman2001]_ for
     more details).
   * Quantile (``'quantile'``): A loss function for quantile regression.
     Use ``0 < alpha < 1`` to specify the quantile. This loss function
     can be used to create prediction intervals
     (see :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`).
 
-* Classification
+|details-end|
+
+
+|details-start|
+**Classification**
+|details-split|
 
   * Binary log-loss (``'log-loss'``): The binomial
     negative log-likelihood loss function for binary classification. It provides
     probability estimates.  The initial model is given by the
     log odds-ratio.
   * Multi-class log-loss (``'log-loss'``): The multinomial
     negative log-likelihood loss function for multi-class classification with
@@ -724,14 +802,16 @@
     regression trees have to be constructed which makes GBRT rather
     inefficient for data sets with a large number of classes.
   * Exponential loss (``'exponential'``): The same loss function
     as :class:`AdaBoostClassifier`. Less robust to mislabeled
     examples than ``'log-loss'``; can only be used for binary
     classification.
 
+|details-end|
+
 .. _gradient_boosting_shrinkage:
 
 Shrinkage via learning rate
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 [Friedman2001]_ proposed a simple regularization strategy that scales
 the contribution of each weak learner by a constant factor :math:`\nu`:
@@ -1352,16 +1432,36 @@
    >>> eclf = eclf.fit(X, y)
 
 .. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_voting_decision_regions_001.png
     :target: ../auto_examples/ensemble/plot_voting_decision_regions.html
     :align: center
     :scale: 75%
 
-Using the `VotingClassifier` with `GridSearchCV`
-------------------------------------------------
+Usage
+-----
+
+In order to predict the class labels based on the predicted
+class-probabilities (scikit-learn estimators in the VotingClassifier
+must support ``predict_proba`` method)::
+
+   >>> eclf = VotingClassifier(
+   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
+   ...     voting='soft'
+   ... )
+
+Optionally, weights can be provided for the individual classifiers::
+
+   >>> eclf = VotingClassifier(
+   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
+   ...     voting='soft', weights=[2,5,1]
+   ... )
+
+|details-start|
+**Using the `VotingClassifier` with `GridSearchCV`**
+|details-split|
 
 The :class:`VotingClassifier` can also be used together with
 :class:`~sklearn.model_selection.GridSearchCV` in order to tune the
 hyperparameters of the individual estimators::
 
    >>> from sklearn.model_selection import GridSearchCV
    >>> clf1 = LogisticRegression(random_state=1)
@@ -1373,32 +1473,15 @@
    ... )
 
    >>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}
 
    >>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
    >>> grid = grid.fit(iris.data, iris.target)
 
-Usage
------
-
-In order to predict the class labels based on the predicted
-class-probabilities (scikit-learn estimators in the VotingClassifier
-must support ``predict_proba`` method)::
-
-   >>> eclf = VotingClassifier(
-   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
-   ...     voting='soft'
-   ... )
-
-Optionally, weights can be provided for the individual classifiers::
-
-   >>> eclf = VotingClassifier(
-   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
-   ...     voting='soft', weights=[2,5,1]
-   ... )
+|details-end|
 
 .. _voting_regressor:
 
 Voting Regressor
 ================
 
 The idea behind the :class:`VotingRegressor` is to combine conceptually
```

### Comparing `scikit-learn-1.4.2/doc/modules/feature_extraction.rst` & `scikit_learn-1.5.0rc1/doc/modules/feature_extraction.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/feature_selection.rst` & `scikit_learn-1.5.0rc1/doc/modules/feature_selection.rst`

 * *Files 1% similar despite different names*

```diff
@@ -336,15 +336,15 @@
 =======================================
 
 Feature selection is usually used as a pre-processing step before doing
 the actual learning. The recommended way to do this in scikit-learn is
 to use a :class:`~pipeline.Pipeline`::
 
   clf = Pipeline([
-    ('feature_selection', SelectFromModel(LinearSVC(dual="auto", penalty="l1"))),
+    ('feature_selection', SelectFromModel(LinearSVC(penalty="l1"))),
     ('classification', RandomForestClassifier())
   ])
   clf.fit(X, y)
 
 In this snippet we make use of a :class:`~svm.LinearSVC`
 coupled with :class:`~feature_selection.SelectFromModel`
 to evaluate feature importances and select the most relevant features.
```

### Comparing `scikit-learn-1.4.2/doc/modules/gaussian_process.rst` & `scikit_learn-1.5.0rc1/doc/modules/gaussian_process.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/glm_data/lasso_enet_coordinate_descent.png` & `scikit_learn-1.5.0rc1/doc/modules/glm_data/lasso_enet_coordinate_descent.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/glm_data/poisson_gamma_tweedie_distributions.png` & `scikit_learn-1.5.0rc1/doc/modules/glm_data/poisson_gamma_tweedie_distributions.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/grid_search.rst` & `scikit_learn-1.5.0rc1/doc/modules/grid_search.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/impute.rst` & `scikit_learn-1.5.0rc1/doc/modules/impute.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/isotonic.rst` & `scikit_learn-1.5.0rc1/doc/modules/isotonic.rst`

 * *Files 5% similar despite different names*

```diff
@@ -27,7 +27,11 @@
 :math:`y` in terms of mean squared error. These predictions are interpolated
 for predicting to unseen data. The predictions of :class:`IsotonicRegression`
 thus form a function that is piecewise linear:
 
 .. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_isotonic_regression_001.png
    :target: ../auto_examples/miscellaneous/plot_isotonic_regression.html
    :align: center
+
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_miscellaneous_plot_isotonic_regression.py`
```

### Comparing `scikit-learn-1.4.2/doc/modules/kernel_approximation.rst` & `scikit_learn-1.5.0rc1/doc/modules/kernel_approximation.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/kernel_ridge.rst` & `scikit_learn-1.5.0rc1/doc/modules/kernel_ridge.rst`

 * *Files 3% similar despite different names*

```diff
@@ -51,12 +51,15 @@
 the :class:`~sklearn.svm.SVR`; :math:`\epsilon = 0` would correspond to a
 dense model.
 
 .. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_kernel_ridge_regression_002.png
    :target: ../auto_examples/miscellaneous/plot_kernel_ridge_regression.html
    :align: center
 
+.. topic:: Examples
+
+    * :ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_ridge_regression.py`
 
 .. topic:: References:
 
     .. [M2012] "Machine Learning: A Probabilistic Perspective"
       Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012
```

### Comparing `scikit-learn-1.4.2/doc/modules/lda_qda.rst` & `scikit_learn-1.5.0rc1/doc/modules/lda_qda.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/learning_curve.rst` & `scikit_learn-1.5.0rc1/doc/modules/learning_curve.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/linear_model.rst` & `scikit_learn-1.5.0rc1/doc/modules/linear_model.rst`

 * *Files 1% similar despite different names*

```diff
@@ -189,17 +189,22 @@
 .. the method of normal equations (Cholesky), there is a big flop difference
 .. between these
 
 
 Setting the regularization parameter: leave-one-out Cross-Validation
 --------------------------------------------------------------------
 
-:class:`RidgeCV` implements ridge regression with built-in
-cross-validation of the alpha parameter. The object works in the same way
-as GridSearchCV except that it defaults to Leave-One-Out Cross-Validation::
+:class:`RidgeCV` and :class:`RidgeClassifierCV` implement ridge
+regression/classification with built-in cross-validation of the alpha parameter.
+They work in the same way as :class:`~sklearn.model_selection.GridSearchCV` except
+that it defaults to efficient Leave-One-Out :term:`cross-validation`.
+When using the default :term:`cross-validation`, alpha cannot be 0 due to the
+formulation used to calculate Leave-One-Out error. See [RL2007]_ for details.
+
+Usage example::
 
     >>> import numpy as np
     >>> from sklearn import linear_model
     >>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
     >>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
     RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,
           1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))
@@ -207,24 +212,21 @@
     0.01
 
 Specifying the value of the :term:`cv` attribute will trigger the use of
 cross-validation with :class:`~sklearn.model_selection.GridSearchCV`, for
 example `cv=10` for 10-fold cross-validation, rather than Leave-One-Out
 Cross-Validation.
 
-|details-start|
-**References**
-|details-split|
+.. topic:: References:
 
-* "Notes on Regularized Least Squares", Rifkin & Lippert (`technical report
-  <http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf>`_,
-  `course slides
-  <https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf>`_).
 
-|details-end|
+  .. [RL2007] "Notes on Regularized Least Squares", Rifkin & Lippert (`technical report
+    <http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf>`_,
+    `course slides
+    <https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf>`_).
 
 .. _lasso:
 
 Lasso
 =====
 
 The :class:`Lasso` is a linear model that estimates sparse coefficients.
@@ -679,15 +681,15 @@
 on the number of non-zero coefficients (ie. the :math:`\ell_0` pseudo-norm).
 
 Being a forward feature selection method like :ref:`least_angle_regression`,
 orthogonal matching pursuit can approximate the optimum solution vector with a
 fixed number of non-zero elements:
 
 .. math::
-    \underset{w}{\operatorname{arg\,min\,}}  ||y - Xw||_2^2 \text{ subject to } ||w||_0 \leq n_{\text{nonzero\_coefs}}
+    \underset{w}{\operatorname{arg\,min\,}}  ||y - Xw||_2^2 \text{ subject to } ||w||_0 \leq n_{\text{nonzero_coefs}}
 
 Alternatively, orthogonal matching pursuit can target a specific error instead
 of a specific number of non-zero coefficients. This can be expressed as:
 
 .. math::
     \underset{w}{\operatorname{arg\,min\,}} ||w||_0 \text{ subject to } ||y-Xw||_2^2 \leq \text{tol}
 
@@ -942,21 +944,24 @@
 As an optimization problem, binary
 class logistic regression with regularization term :math:`r(w)` minimizes the
 following cost function:
 
 .. math::
     :name: regularized-logistic-loss
 
-    \min_{w} C \sum_{i=1}^n s_i \left(-y_i \log(\hat{p}(X_i)) - (1 - y_i) \log(1 - \hat{p}(X_i))\right) + r(w),
+    \min_{w} \frac{1}{S}\sum_{i=1}^n s_i
+    \left(-y_i \log(\hat{p}(X_i)) - (1 - y_i) \log(1 - \hat{p}(X_i))\right)
+    + \frac{r(w)}{S C}\,,
 
 where :math:`{s_i}` corresponds to the weights assigned by the user to a
 specific training sample (the vector :math:`s` is formed by element-wise
-multiplication of the class weights and sample weights).
+multiplication of the class weights and sample weights),
+and the sum :math:`S = \sum_{i=1}^n s_i`.
 
-We currently provide four choices for the regularization term  :math:`r(w)`  via
+We currently provide four choices for the regularization term  :math:`r(w)` via
 the `penalty` argument:
 
 +----------------+-------------------------------------------------+
 | penalty        | :math:`r(w)`                                    |
 +================+=================================================+
 | `None`         | :math:`0`                                       |
 +----------------+-------------------------------------------------+
@@ -1004,18 +1009,25 @@
 :math:`k`. We aim at predicting the class probabilities :math:`P(y_i=k|X_i)` via
 :meth:`~sklearn.linear_model.LogisticRegression.predict_proba` as:
 
 .. math:: \hat{p}_k(X_i) = \frac{\exp(X_i W_k + W_{0, k})}{\sum_{l=0}^{K-1} \exp(X_i W_l + W_{0, l})}.
 
 The objective for the optimization becomes
 
-.. math:: \min_W -C \sum_{i=1}^n \sum_{k=0}^{K-1} [y_i = k] \log(\hat{p}_k(X_i)) + r(W).
+.. math::
+  \min_W -\frac{1}{S}\sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik} [y_i = k] \log(\hat{p}_k(X_i))
+  + \frac{r(W)}{S C}\,.
 
 Where :math:`[P]` represents the Iverson bracket which evaluates to :math:`0`
-if :math:`P` is false, otherwise it evaluates to :math:`1`. We currently provide four choices
+if :math:`P` is false, otherwise it evaluates to :math:`1`.
+
+Again, :math:`s_{ik}` are the weights assigned by the user (multiplication of sample
+weights and class weights) with their sum :math:`S = \sum_{i=1}^n \sum_{k=0}^{K-1} s_{ik}`.
+
+We currently provide four choices
 for the regularization term :math:`r(W)` via the `penalty` argument, where :math:`m`
 is the number of features:
 
 +----------------+----------------------------------------------------------------------------------+
 | penalty        | :math:`r(W)`                                                                     |
 +================+==================================================================================+
 | `None`         | :math:`0`                                                                        |
@@ -1031,32 +1043,32 @@
 
 Solvers
 -------
 
 The solvers implemented in the class :class:`LogisticRegression`
 are "lbfgs", "liblinear", "newton-cg", "newton-cholesky", "sag" and "saga":
 
-The following table summarizes the penalties supported by each solver:
+The following table summarizes the penalties and multinomial multiclass supported by each solver:
 
 +------------------------------+-----------------+-------------+-----------------+-----------------------+-----------+------------+
 |                              |                       **Solvers**                                                                |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
 | **Penalties**                | **'lbfgs'** | **'liblinear'** | **'newton-cg'** | **'newton-cholesky'** | **'sag'** | **'saga'** |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
-| Multinomial + L2 penalty     |     yes     |       no        |       yes       |     no                |    yes    |    yes     |
+| L2 penalty                   |     yes     |       no        |       yes       |     no                |    yes    |    yes     |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
-| OVR + L2 penalty             |     yes     |       yes       |       yes       |     yes               |    yes    |    yes     |
+| L1 penalty                   |     no      |       yes       |       no        |     no                |    no     |    yes     |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
-| Multinomial + L1 penalty     |     no      |       no        |       no        |     no                |    no     |    yes     |
+| Elastic-Net (L1 + L2)        |     no      |       no        |       no        |     no                |    no     |    yes     |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
-| OVR + L1 penalty             |     no      |       yes       |       no        |     no                |    no     |    yes     |
+| No penalty ('none')          |     yes     |       no        |       yes       |     yes               |    yes    |    yes     |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
-| Elastic-Net                  |     no      |       no        |       no        |     no                |    no     |    yes     |
+| **Multiclass support**       |                                                                                                  |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
-| No penalty ('none')          |     yes     |       no        |       yes       |     yes               |    yes    |    yes     |
+| multinomial multiclass       |     yes     |       no        |       yes       |     no                |    yes    |    yes     |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
 | **Behaviors**                |                                                                                                  |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
 | Penalize the intercept (bad) |     no      |       yes       |       no        |     no                |    no     |    no      |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
 | Faster for large datasets    |     no      |       no        |       no        |     no                |    yes    |    yes     |
 +------------------------------+-------------+-----------------+-----------------+-----------------------+-----------+------------+
@@ -1530,18 +1542,18 @@
 **Details of the algorithm**
 |details-split|
 
 Each iteration performs the following steps:
 
 1. Select ``min_samples`` random samples from the original data and check
    whether the set of data is valid (see ``is_data_valid``).
-2. Fit a model to the random subset (``base_estimator.fit``) and check
+2. Fit a model to the random subset (``estimator.fit``) and check
    whether the estimated model is valid (see ``is_model_valid``).
 3. Classify all data as inliers or outliers by calculating the residuals
-   to the estimated model (``base_estimator.predict(X) - y``) - all data
+   to the estimated model (``estimator.predict(X) - y``) - all data
    samples with absolute residuals smaller than or equal to the
    ``residual_threshold`` are considered as inliers.
 4. Save fitted model as best model if number of inlier samples is
    maximal. In case the current estimated model has the same number of
    inliers, it is only considered as the best model if it has better score.
 
 These steps are performed either a maximum number of times (``max_trials``) or
```

### Comparing `scikit-learn-1.4.2/doc/modules/manifold.rst` & `scikit_learn-1.5.0rc1/doc/modules/manifold.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/metrics.rst` & `scikit_learn-1.5.0rc1/doc/modules/metrics.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/mixture.rst` & `scikit_learn-1.5.0rc1/doc/modules/mixture.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/model_evaluation.rst` & `scikit_learn-1.5.0rc1/doc/modules/model_evaluation.rst`

 * *Files 2% similar despite different names*

```diff
@@ -98,20 +98,17 @@
 'neg_mean_squared_log_error'           :func:`metrics.mean_squared_log_error`
 'neg_root_mean_squared_log_error'      :func:`metrics.root_mean_squared_log_error`
 'neg_median_absolute_error'            :func:`metrics.median_absolute_error`
 'r2'                                   :func:`metrics.r2_score`
 'neg_mean_poisson_deviance'            :func:`metrics.mean_poisson_deviance`
 'neg_mean_gamma_deviance'              :func:`metrics.mean_gamma_deviance`
 'neg_mean_absolute_percentage_error'   :func:`metrics.mean_absolute_percentage_error`
-'d2_absolute_error_score'              :func:`metrics.d2_absolute_error_score`
-'d2_pinball_score'                     :func:`metrics.d2_pinball_score`
-'d2_tweedie_score'                     :func:`metrics.d2_tweedie_score`
+'d2_absolute_error_score' 	           :func:`metrics.d2_absolute_error_score`
 ====================================   ==============================================     ==================================
 
-
 Usage examples:
 
     >>> from sklearn import svm, datasets
     >>> from sklearn.model_selection import cross_val_score
     >>> X, y = datasets.load_iris(return_X_y=True)
     >>> clf = svm.SVC(random_state=0)
     >>> cross_val_score(clf, X, y, cv=5, scoring='recall_macro')
@@ -126,47 +123,57 @@
 .. currentmodule:: sklearn.metrics
 
 .. _scoring:
 
 Defining your scoring strategy from metric functions
 -----------------------------------------------------
 
-The module :mod:`sklearn.metrics` also exposes a set of simple functions
-measuring a prediction error given ground truth and prediction:
-
-- functions ending with ``_score`` return a value to
-  maximize, the higher the better.
-
-- functions ending with ``_error`` or ``_loss`` return a
-  value to minimize, the lower the better.  When converting
-  into a scorer object using :func:`make_scorer`, set
-  the ``greater_is_better`` parameter to ``False`` (``True`` by default; see the
-  parameter description below).
-
-Metrics available for various machine learning tasks are detailed in sections
-below.
-
-Many metrics are not given names to be used as ``scoring`` values,
+The following metrics functions are not implemented as named scorers,
 sometimes because they require additional parameters, such as
-:func:`fbeta_score`. In such cases, you need to generate an appropriate
-scoring object.  The simplest way to generate a callable object for scoring
-is by using :func:`make_scorer`. That function converts metrics
-into callables that can be used for model evaluation.
+:func:`fbeta_score`. They cannot be passed to the ``scoring``
+parameters; instead their callable needs to be passed to
+:func:`make_scorer` together with the value of the user-settable
+parameters.
+
+=====================================  =========  ==============================================
+Function                               Parameter  Example usage
+=====================================  =========  ==============================================
+**Classification**
+:func:`metrics.fbeta_score`            ``beta``   ``make_scorer(fbeta_score, beta=2)``
+
+**Regression**
+:func:`metrics.mean_tweedie_deviance`  ``power``  ``make_scorer(mean_tweedie_deviance, power=1.5)``
+:func:`metrics.mean_pinball_loss`      ``alpha``  ``make_scorer(mean_pinball_loss, alpha=0.95)``
+:func:`metrics.d2_tweedie_score`       ``power``  ``make_scorer(d2_tweedie_score, power=1.5)``
+:func:`metrics.d2_pinball_score`       ``alpha``  ``make_scorer(d2_pinball_score, alpha=0.95)``
+=====================================  =========  ==============================================
 
 One typical use case is to wrap an existing metric function from the library
 with non-default values for its parameters, such as the ``beta`` parameter for
 the :func:`fbeta_score` function::
 
     >>> from sklearn.metrics import fbeta_score, make_scorer
     >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
     >>> from sklearn.model_selection import GridSearchCV
     >>> from sklearn.svm import LinearSVC
-    >>> grid = GridSearchCV(LinearSVC(dual="auto"), param_grid={'C': [1, 10]},
+    >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},
     ...                     scoring=ftwo_scorer, cv=5)
 
+The module :mod:`sklearn.metrics` also exposes a set of simple functions
+measuring a prediction error given ground truth and prediction:
+
+- functions ending with ``_score`` return a value to
+  maximize, the higher the better.
+
+- functions ending with ``_error``, ``_loss``, or ``_deviance`` return a
+  value to minimize, the lower the better.  When converting
+  into a scorer object using :func:`make_scorer`, set
+  the ``greater_is_better`` parameter to ``False`` (``True`` by default; see the
+  parameter description below).
+
 
 |details-start|
 **Custom scorer objects**
 |details-split|
 
 
 The second use case is to build a completely custom scorer object
@@ -297,15 +304,15 @@
 
 - As a callable that returns a dictionary of scores::
 
     >>> from sklearn.model_selection import cross_validate
     >>> from sklearn.metrics import confusion_matrix
     >>> # A sample toy binary classification dataset
     >>> X, y = datasets.make_classification(n_classes=2, random_state=0)
-    >>> svm = LinearSVC(dual="auto", random_state=0)
+    >>> svm = LinearSVC(random_state=0)
     >>> def confusion_matrix_scorer(clf, X, y):
     ...      y_pred = clf.predict(X)
     ...      cm = confusion_matrix(y, y_pred)
     ...      return {'tn': cm[0, 0], 'fp': cm[0, 1],
     ...              'fn': cm[1, 0], 'tp': cm[1, 1]}
     >>> cv_results = cross_validate(svm, X, y, cv=5,
     ...                             scoring=confusion_matrix_scorer)
@@ -845,15 +852,14 @@
     for an example of :func:`precision_score` and :func:`recall_score` usage
     to estimate parameters using grid search with nested cross-validation.
 
   * See :ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`
     for an example of :func:`precision_recall_curve` usage to evaluate
     classifier output quality.
 
-
 .. topic:: References:
 
   .. [Manning2008] C.D. Manning, P. Raghavan, H. Schütze, `Introduction to Information Retrieval
      <https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html>`_,
      2008.
   .. [Everingham2010] M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,
      `The Pascal Visual Object Classes (VOC) Challenge
@@ -862,15 +868,14 @@
   .. [Davis2006] J. Davis, M. Goadrich, `The Relationship Between Precision-Recall and ROC Curves
      <https://www.biostat.wisc.edu/~page/rocpr.pdf>`_,
      ICML 2006.
   .. [Flach2015] P.A. Flach, M. Kull, `Precision-Recall-Gain Curves: PR Analysis Done Right
      <https://papers.nips.cc/paper/5867-precision-recall-gain-curves-pr-analysis-done-right.pdf>`_,
      NIPS 2015.
 
-
 Binary classification
 ^^^^^^^^^^^^^^^^^^^^^
 
 In a binary classification task, the terms ''positive'' and ''negative'' refer
 to the classifier's prediction, and the terms ''true'' and ''false'' refer to
 whether that prediction corresponds to the external judgment (sometimes known
 as the ''observation''). Given these definitions, we can formulate the
@@ -959,18 +964,25 @@
 In a multiclass and multilabel classification task, the notions of precision,
 recall, and F-measures can be applied to each label independently.
 There are a few ways to combine results across labels,
 specified by the ``average`` argument to the
 :func:`average_precision_score`, :func:`f1_score`,
 :func:`fbeta_score`, :func:`precision_recall_fscore_support`,
 :func:`precision_score` and :func:`recall_score` functions, as described
-:ref:`above <average>`. Note that if all labels are included, "micro"-averaging
-in a multiclass setting will produce precision, recall and :math:`F`
-that are all identical to accuracy. Also note that "weighted" averaging may
-produce an F-score that is not between precision and recall.
+:ref:`above <average>`.
+
+Note the following behaviors when averaging:
+
+* If all labels are included, "micro"-averaging in a multiclass setting will produce
+  precision, recall and :math:`F` that are all identical to accuracy.
+* "weighted" averaging may produce a F-score that is not between precision and recall.
+* "macro" averaging for F-measures is calculated as the arithmetic mean over
+  per-label/class F-measures, not the harmonic mean over the arithmetic precision and
+  recall means. Both calculations can be seen in the literature but are not equivalent,
+  see [OB2019]_ for details.
 
 To make this more explicit, consider the following notation:
 
 * :math:`y` the set of *true* :math:`(sample, label)` pairs
 * :math:`\hat{y}` the set of *predicted* :math:`(sample, label)` pairs
 * :math:`L` the set of labels
 * :math:`S` the set of samples
@@ -1023,14 +1035,19 @@
   0.0
 
 Similarly, labels not present in the data sample may be accounted for in macro-averaging.
 
   >>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
   0.166...
 
+.. topic:: References:
+
+    .. [OB2019] :arxiv:`Opitz, J., & Burst, S. (2019). "Macro f1 and macro f1."
+       <1911.03347>`
+
 .. _jaccard_similarity_score:
 
 Jaccard similarity coefficient score
 -------------------------------------
 
 The :func:`jaccard_score` function computes the average of `Jaccard similarity
 coefficients <https://en.wikipedia.org/wiki/Jaccard_index>`_, also called the
@@ -1127,32 +1144,32 @@
 Here is a small example demonstrating the use of the :func:`hinge_loss` function
 with a svm classifier in a binary class problem::
 
   >>> from sklearn import svm
   >>> from sklearn.metrics import hinge_loss
   >>> X = [[0], [1]]
   >>> y = [-1, 1]
-  >>> est = svm.LinearSVC(dual="auto", random_state=0)
+  >>> est = svm.LinearSVC(random_state=0)
   >>> est.fit(X, y)
-  LinearSVC(dual='auto', random_state=0)
+  LinearSVC(random_state=0)
   >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
   >>> pred_decision
   array([-2.18...,  2.36...,  0.09...])
   >>> hinge_loss([-1, 1, 1], pred_decision)
   0.3...
 
 Here is an example demonstrating the use of the :func:`hinge_loss` function
 with a svm classifier in a multiclass problem::
 
   >>> X = np.array([[0], [1], [2], [3]])
   >>> Y = np.array([0, 1, 2, 3])
   >>> labels = np.array([0, 1, 2, 3])
-  >>> est = svm.LinearSVC(dual="auto")
+  >>> est = svm.LinearSVC()
   >>> est.fit(X, Y)
-  LinearSVC(dual='auto')
+  LinearSVC()
   >>> pred_decision = est.decision_function([[-1], [2], [3]])
   >>> y_true = [0, 2, 3]
   >>> hinge_loss(y_true, pred_decision, labels=labels)
   0.56...
 
 .. _log_loss:
 
@@ -1473,15 +1490,19 @@
 the one-vs-rest algorithm computes the average of the ROC AUC scores for each
 class against all other classes. In both cases, the predicted labels are
 provided in an array with values from 0 to ``n_classes``, and the scores
 correspond to the probability estimates that a sample belongs to a particular
 class. The OvO and OvR algorithms support weighting uniformly
 (``average='macro'``) and by prevalence (``average='weighted'``).
 
-**One-vs-one Algorithm**: Computes the average AUC of all possible pairwise
+|details-start|
+**One-vs-one Algorithm**
+|details-split|
+
+Computes the average AUC of all possible pairwise
 combinations of classes. [HT2001]_ defines a multiclass AUC metric weighted
 uniformly:
 
 .. math::
 
    \frac{1}{c(c-1)}\sum_{j=1}^{c}\sum_{k > j}^c (\text{AUC}(j | k) +
    \text{AUC}(k | j))
@@ -1502,15 +1523,21 @@
    \text{AUC}(j | k) + \text{AUC}(k | j))
 
 where :math:`c` is the number of classes. This algorithm is used by setting
 the keyword argument ``multiclass`` to ``'ovo'`` and ``average`` to
 ``'weighted'``. The ``'weighted'`` option returns a prevalence-weighted average
 as described in [FC2009]_.
 
-**One-vs-rest Algorithm**: Computes the AUC of each class against the rest
+|details-end|
+
+|details-start|
+**One-vs-rest Algorithm**
+|details-split|
+
+Computes the AUC of each class against the rest
 [PD2000]_. The algorithm is functionally the same as the multilabel case. To
 enable this algorithm set the keyword argument ``multiclass`` to ``'ovr'``.
 Additionally to ``'macro'`` [F2006]_ and ``'weighted'`` [F2001]_ averaging, OvR
 supports ``'micro'`` averaging.
 
 In applications where a high false positive rate is not tolerable the parameter
 ``max_fpr`` of :func:`roc_auc_score` can be used to summarize the ROC curve up
@@ -1521,14 +1548,16 @@
 the :ref:`iris_dataset`:
 
 .. image:: ../auto_examples/model_selection/images/sphx_glr_plot_roc_002.png
    :target: ../auto_examples/model_selection/plot_roc.html
    :scale: 75
    :align: center
 
+|details-end|
+
 .. _roc_auc_multilabel:
 
 Multi-label case
 ^^^^^^^^^^^^^^^^
 
 In **multi-label classification**, the :func:`roc_auc_score` function is
 extended by averaging over the labels as :ref:`above <average>`. In this case,
@@ -1624,15 +1653,23 @@
 same classification task:
 
 .. image:: ../auto_examples/model_selection/images/sphx_glr_plot_det_001.png
    :target: ../auto_examples/model_selection/plot_det.html
    :scale: 75
    :align: center
 
-**Properties:**
+.. topic:: Examples:
+
+  * See :ref:`sphx_glr_auto_examples_model_selection_plot_det.py`
+    for an example comparison between receiver operating characteristic (ROC)
+    curves and Detection error tradeoff (DET) curves.
+
+|details-start|
+**Properties**
+|details-split|
 
 * DET curves form a linear curve in normal deviate scale if the detection
   scores are normally (or close-to normally) distributed.
   It was shown by [Navratil2007]_ that the reverse is not necessarily true and
   even more general distributions are able to produce linear DET curves.
 
 * The normal deviate scale transformation spreads out the points such that a
@@ -1640,32 +1677,32 @@
   Therefore curves with similar classification performance might be easier to
   distinguish on a DET plot.
 
 * With False Negative Rate being "inverse" to True Positive Rate the point
   of perfection for DET curves is the origin (in contrast to the top left
   corner for ROC curves).
 
-**Applications and limitations:**
+|details-end|
+
+|details-start|
+**Applications and limitations**
+|details-split|
 
 DET curves are intuitive to read and hence allow quick visual assessment of a
 classifier's performance.
 Additionally DET curves can be consulted for threshold analysis and operating
 point selection.
 This is particularly helpful if a comparison of error types is required.
 
 On the other hand DET curves do not provide their metric as a single number.
 Therefore for either automated evaluation or comparison to other
 classification tasks metrics like the derived area under ROC curve might be
 better suited.
 
-.. topic:: Examples:
-
-  * See :ref:`sphx_glr_auto_examples_model_selection_plot_det.py`
-    for an example comparison between receiver operating characteristic (ROC)
-    curves and Detection error tradeoff (DET) curves.
+|details-end|
 
 .. topic:: References:
 
   .. [WikipediaDET2017] Wikipedia contributors. Detection error tradeoff.
      Wikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.
      Available at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.
      Accessed February 19, 2018.
@@ -1858,15 +1895,21 @@
 
 Notice that probabilities differ from counts, for instance
 :math:`\operatorname{PR}(P+|T+)` is not equal to the number of true positive
 counts ``tp`` (see `the wikipedia page
 <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_ for
 the actual formulas).
 
-**Interpretation across varying prevalence:**
+.. topic:: Examples:
+
+  * :ref:`sphx_glr_auto_examples_model_selection_plot_likelihood_ratios.py`
+
+|details-start|
+**Interpretation across varying prevalence**
+|details-split|
 
 Both class likelihood ratios are interpretable in terms of an odds ratio
 (pre-test and post-tests):
 
 .. math::
 
    \text{post-test odds} = \text{Likelihood ratio} \times \text{pre-test odds}.
@@ -1893,15 +1936,19 @@
    \text{post-test odds} = \text{Likelihood ratio} \times
    \frac{\text{pre-test probability}}{1 - \text{pre-test probability}},
 
 .. math::
 
    \text{post-test probability} = \frac{\text{post-test odds}}{1 + \text{post-test odds}}.
 
-**Mathematical divergences:**
+|details-end|
+
+|details-start|
+**Mathematical divergences**
+|details-split|
 
 The positive likelihood ratio is undefined when :math:`fp = 0`, which can be
 interpreted as the classifier perfectly identifying positive cases. If :math:`fp
 = 0` and additionally :math:`tp = 0`, this leads to a zero/zero division. This
 happens, for instance, when using a `DummyClassifier` that always predicts the
 negative class and therefore the interpretation as a perfect classifier is lost.
 
@@ -1919,28 +1966,29 @@
 In all the previous cases the :func:`class_likelihood_ratios` function raises by
 default an appropriate warning message and returns `nan` to avoid pollution when
 averaging over cross-validation folds.
 
 For a worked-out demonstration of the :func:`class_likelihood_ratios` function,
 see the example below.
 
-.. topic:: Examples:
-
-  * :ref:`sphx_glr_auto_examples_model_selection_plot_likelihood_ratios.py`
+|details-end|
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
   * `Wikipedia entry for Likelihood ratios in diagnostic testing
     <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_
 
   * Brenner, H., & Gefeller, O. (1997).
     Variation of sensitivity, specificity, likelihood ratios and predictive
     values with disease prevalence.
     Statistics in medicine, 16(9), 981-991.
 
+|details-end|
 
 .. _multilabel_ranking_metrics:
 
 Multilabel ranking metrics
 ==========================
 
 .. currentmodule:: sklearn.metrics
@@ -2072,19 +2120,23 @@
     0.75...
     >>> # With the following prediction, we have perfect and minimal loss
     >>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])
     >>> label_ranking_loss(y_true, y_score)
     0.0
 
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
   * Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In
     Data mining and knowledge discovery handbook (pp. 667-685). Springer US.
 
+|details-end|
+
 .. _ndcg:
 
 Normalized Discounted Cumulative Gain
 -------------------------------------
 
 Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain
 (NDCG) are ranking metrics implemented in :func:`~sklearn.metrics.dcg_score`
@@ -2121,15 +2173,17 @@
 
 .. math::
    \sum_{r=1}^{\min(K, M)}\frac{y_{f(r)}}{\log(1 + r)}
 
 and the NDCG score is the DCG score divided by the DCG score obtained for
 :math:`y`.
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
   * `Wikipedia entry for Discounted Cumulative Gain
     <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_
 
   * Jarvelin, K., & Kekalainen, J. (2002).
     Cumulated gain-based evaluation of IR techniques. ACM Transactions on
     Information Systems (TOIS), 20(4), 422-446.
@@ -2139,14 +2193,16 @@
     Annual Conference on Learning Theory (COLT 2013)
 
   * McSherry, F., & Najork, M. (2008, March). Computing information retrieval
     performance measures efficiently in the presence of tied scores. In
     European conference on information retrieval (pp. 414-421). Springer,
     Berlin, Heidelberg.
 
+|details-end|
+
 .. _regression_metrics:
 
 Regression metrics
 ===================
 
 .. currentmodule:: sklearn.metrics
 
@@ -2694,32 +2750,36 @@
 error and the alpha-quantile for pinball loss).
 
 Like R², the best possible score is 1.0 and it can be negative (because the
 model can be arbitrarily worse). A constant model that always predicts
 :math:`y_{\text{null}}`, disregarding the input features, would get a D² score
 of 0.0.
 
-D² Tweedie score
-^^^^^^^^^^^^^^^^
+|details-start|
+**D² Tweedie score**
+|details-split|
 
 The :func:`d2_tweedie_score` function implements the special case of D²
 where :math:`\text{dev}(y, \hat{y})` is the Tweedie deviance, see :ref:`mean_tweedie_deviance`.
 It is also known as D² Tweedie and is related to McFadden's likelihood ratio index.
 
 The argument ``power`` defines the Tweedie power as for
 :func:`mean_tweedie_deviance`. Note that for `power=0`,
 :func:`d2_tweedie_score` equals :func:`r2_score` (for single targets).
 
 A scorer object with a specific choice of ``power`` can be built by::
 
   >>> from sklearn.metrics import d2_tweedie_score, make_scorer
   >>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)
 
-D² pinball score
-^^^^^^^^^^^^^^^^^^^^^
+|details-end|
+
+|details-start|
+**D² pinball score**
+|details-split|
 
 The :func:`d2_pinball_score` function implements the special case
 of D² with the pinball loss, see :ref:`pinball_loss`, i.e.:
 
 .. math::
 
   \text{dev}(y, \hat{y}) = \text{pinball}(y, \hat{y}).
@@ -2731,16 +2791,19 @@
 equals :func:`d2_absolute_error_score`.
 
 A scorer object with a specific choice of ``alpha`` can be built by::
 
   >>> from sklearn.metrics import d2_pinball_score, make_scorer
   >>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)
 
-D² absolute error score
-^^^^^^^^^^^^^^^^^^^^^^^
+|details-end|
+
+|details-start|
+**D² absolute error score**
+|details-split|
 
 The :func:`d2_absolute_error_score` function implements the special case of
 the :ref:`mean_absolute_error`:
 
 .. math::
 
   \text{dev}(y, \hat{y}) = \text{MAE}(y, \hat{y}).
@@ -2757,14 +2820,61 @@
   >>> d2_absolute_error_score(y_true, y_pred)
   1.0
   >>> y_true = [1, 2, 3]
   >>> y_pred = [2, 2, 2]
   >>> d2_absolute_error_score(y_true, y_pred)
   0.0
 
+|details-end|
+
+|details-start|
+**D² log loss score**
+|details-split|
+
+The :func:`d2_log_loss_score` function implements the special case
+of D² with the log loss, see :ref:`log_loss`, i.e.:
+
+.. math::
+
+  \text{dev}(y, \hat{y}) = \text{log_loss}(y, \hat{y}).
+
+The :math:`y_{\text{null}}` for the :func:`log_loss` is the per-class
+proportion.
+
+Here are some usage examples of the :func:`d2_log_loss_score` function::
+
+  >>> from sklearn.metrics import d2_log_loss_score
+  >>> y_true = [1, 1, 2, 3]
+  >>> y_pred = [
+  ...    [0.5, 0.25, 0.25],
+  ...    [0.5, 0.25, 0.25],
+  ...    [0.5, 0.25, 0.25],
+  ...    [0.5, 0.25, 0.25],
+  ... ]
+  >>> d2_log_loss_score(y_true, y_pred)
+  0.0
+  >>> y_true = [1, 2, 3]
+  >>> y_pred = [
+  ...     [0.98, 0.01, 0.01],
+  ...     [0.01, 0.98, 0.01],
+  ...     [0.01, 0.01, 0.98],
+  ... ]
+  >>> d2_log_loss_score(y_true, y_pred)
+  0.981...
+  >>> y_true = [1, 2, 3]
+  >>> y_pred = [
+  ...     [0.1, 0.6, 0.3],
+  ...     [0.1, 0.6, 0.3],
+  ...     [0.4, 0.5, 0.1],
+  ... ]
+  >>> d2_log_loss_score(y_true, y_pred)
+  -0.552...
+
+|details-end|
+
 .. _visualization_regression_evaluation:
 
 Visual evaluation of regression models
 --------------------------------------
 
 Among methods to assess the quality of regression models, scikit-learn provides
 the :class:`~sklearn.metrics.PredictionErrorDisplay` class. It allows to
```

### Comparing `scikit-learn-1.4.2/doc/modules/multiclass.rst` & `scikit_learn-1.5.0rc1/doc/modules/multiclass.rst`

 * *Files 4% similar despite different names*

```diff
@@ -59,16 +59,16 @@
   - :class:`ensemble.ExtraTreesClassifier`
   - :class:`naive_bayes.GaussianNB`
   - :class:`neighbors.KNeighborsClassifier`
   - :class:`semi_supervised.LabelPropagation`
   - :class:`semi_supervised.LabelSpreading`
   - :class:`discriminant_analysis.LinearDiscriminantAnalysis`
   - :class:`svm.LinearSVC` (setting multi_class="crammer_singer")
-  - :class:`linear_model.LogisticRegression` (setting multi_class="multinomial")
-  - :class:`linear_model.LogisticRegressionCV` (setting multi_class="multinomial")
+  - :class:`linear_model.LogisticRegression` (with most solvers)
+  - :class:`linear_model.LogisticRegressionCV` (with most solvers)
   - :class:`neural_network.MLPClassifier`
   - :class:`neighbors.NearestCentroid`
   - :class:`discriminant_analysis.QuadraticDiscriminantAnalysis`
   - :class:`neighbors.RadiusNeighborsClassifier`
   - :class:`ensemble.RandomForestClassifier`
   - :class:`linear_model.RidgeClassifier`
   - :class:`linear_model.RidgeClassifierCV`
@@ -82,16 +82,16 @@
 
 
 - **Multiclass as One-Vs-The-Rest:**
 
   - :class:`ensemble.GradientBoostingClassifier`
   - :class:`gaussian_process.GaussianProcessClassifier` (setting multi_class = "one_vs_rest")
   - :class:`svm.LinearSVC` (setting multi_class="ovr")
-  - :class:`linear_model.LogisticRegression` (setting multi_class="ovr")
-  - :class:`linear_model.LogisticRegressionCV` (setting multi_class="ovr")
+  - :class:`linear_model.LogisticRegression` (most solvers)
+  - :class:`linear_model.LogisticRegressionCV` (most solvers)
   - :class:`linear_model.SGDClassifier`
   - :class:`linear_model.Perceptron`
   - :class:`linear_model.PassiveAggressiveClassifier`
 
 
 - **Support multilabel:**
 
@@ -197,15 +197,15 @@
 
 Below is an example of multiclass learning using OvR::
 
   >>> from sklearn import datasets
   >>> from sklearn.multiclass import OneVsRestClassifier
   >>> from sklearn.svm import LinearSVC
   >>> X, y = datasets.load_iris(return_X_y=True)
-  >>> OneVsRestClassifier(LinearSVC(dual="auto", random_state=0)).fit(X, y).predict(X)
+  >>> OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
   array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,
          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
@@ -249,15 +249,15 @@
 
 Below is an example of multiclass learning using OvO::
 
   >>> from sklearn import datasets
   >>> from sklearn.multiclass import OneVsOneClassifier
   >>> from sklearn.svm import LinearSVC
   >>> X, y = datasets.load_iris(return_X_y=True)
-  >>> OneVsOneClassifier(LinearSVC(dual="auto", random_state=0)).fit(X, y).predict(X)
+  >>> OneVsOneClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
   array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
@@ -307,16 +307,15 @@
 
 Below is an example of multiclass learning using Output-Codes::
 
   >>> from sklearn import datasets
   >>> from sklearn.multiclass import OutputCodeClassifier
   >>> from sklearn.svm import LinearSVC
   >>> X, y = datasets.load_iris(return_X_y=True)
-  >>> clf = OutputCodeClassifier(LinearSVC(dual="auto", random_state=0),
-  ...                            code_size=2, random_state=0)
+  >>> clf = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)
   >>> clf.fit(X, y).predict(X)
   array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
          1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,
          1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
          2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,
@@ -525,14 +524,45 @@
 estimators.
 
 For example, prediction of both wind speed and wind direction, in degrees,
 using data obtained at a certain location. Each sample would be data
 obtained at one location and both wind speed and direction would be
 output for each sample.
 
+The following regressors natively support multioutput regression:
+
+  - :class:`cross_decomposition.CCA`
+  - :class:`tree.DecisionTreeRegressor`
+  - :class:`dummy.DummyRegressor`
+  - :class:`linear_model.ElasticNet`
+  - :class:`tree.ExtraTreeRegressor`
+  - :class:`ensemble.ExtraTreesRegressor`
+  - :class:`gaussian_process.GaussianProcessRegressor`
+  - :class:`neighbors.KNeighborsRegressor`
+  - :class:`kernel_ridge.KernelRidge`
+  - :class:`linear_model.Lars`
+  - :class:`linear_model.Lasso`
+  - :class:`linear_model.LassoLars`
+  - :class:`linear_model.LinearRegression`
+  - :class:`multioutput.MultiOutputRegressor`
+  - :class:`linear_model.MultiTaskElasticNet`
+  - :class:`linear_model.MultiTaskElasticNetCV`
+  - :class:`linear_model.MultiTaskLasso`
+  - :class:`linear_model.MultiTaskLassoCV`
+  - :class:`linear_model.OrthogonalMatchingPursuit`
+  - :class:`cross_decomposition.PLSCanonical`
+  - :class:`cross_decomposition.PLSRegression`
+  - :class:`linear_model.RANSACRegressor`
+  - :class:`neighbors.RadiusNeighborsRegressor`
+  - :class:`ensemble.RandomForestRegressor`
+  - :class:`multioutput.RegressorChain`
+  - :class:`linear_model.Ridge`
+  - :class:`linear_model.RidgeCV`
+  - :class:`compose.TransformedTargetRegressor`
+
 Target format
 -------------
 
 A valid representation of :term:`multioutput` `y` is a dense matrix of shape
 ``(n_samples, n_output)`` of floats. A column wise concatenation of
 :term:`continuous` variables. An example of ``y`` for 3 samples:
```

### Comparing `scikit-learn-1.4.2/doc/modules/naive_bayes.rst` & `scikit_learn-1.5.0rc1/doc/modules/naive_bayes.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/neighbors.rst` & `scikit_learn-1.5.0rc1/doc/modules/neighbors.rst`

 * *Files 14% similar despite different names*

```diff
@@ -300,21 +300,23 @@
 Though the KD tree approach is very fast for low-dimensional (:math:`D < 20`)
 neighbors searches, it becomes inefficient as :math:`D` grows very large:
 this is one manifestation of the so-called "curse of dimensionality".
 In scikit-learn, KD tree neighbors searches are specified using the
 keyword ``algorithm = 'kd_tree'``, and are computed using the class
 :class:`KDTree`.
 
-
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
    * `"Multidimensional binary search trees used for associative searching"
      <https://dl.acm.org/citation.cfm?doid=361002.361007>`_,
      Bentley, J.L., Communications of the ACM (1975)
 
+|details-end|
 
 .. _ball_tree:
 
 Ball Tree
 ---------
 
 To address the inefficiencies of KD Trees in higher dimensions, the *ball tree*
@@ -339,23 +341,29 @@
 a *KD-tree* in high dimensions, though the actual performance is highly
 dependent on the structure of the training data.
 In scikit-learn, ball-tree-based
 neighbors searches are specified using the keyword ``algorithm = 'ball_tree'``,
 and are computed using the class :class:`BallTree`.
 Alternatively, the user can work with the :class:`BallTree` class directly.
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
    * `"Five Balltree Construction Algorithms"
      <https://citeseerx.ist.psu.edu/doc_view/pid/17ac002939f8e950ffb32ec4dc8e86bdd8cb5ff1>`_,
      Omohundro, S.M., International Computer Science Institute
      Technical Report (1989)
 
-Choice of Nearest Neighbors Algorithm
--------------------------------------
+|details-end|
+
+|details-start|
+**Choice of Nearest Neighbors Algorithm**
+|details-split|
+
 The optimal algorithm for a given dataset is a complicated choice, and
 depends on a number of factors:
 
 * number of samples :math:`N` (i.e. ``n_samples``) and dimensionality
   :math:`D` (i.e. ``n_features``).
 
   * *Brute force* query time grows as :math:`O[D N]`
@@ -432,16 +440,20 @@
 
 * the number of query points is at least the same order as the number of
   training points
 * ``leaf_size`` is close to its default value of ``30``
 * when :math:`D > 15`, the intrinsic dimensionality of the data is generally
   too high for tree-based methods
 
-Effect of ``leaf_size``
------------------------
+|details-end|
+
+|details-start|
+**Effect of ``leaf_size``**
+|details-split|
+
 As noted above, for small sample sizes a brute force search can be more
 efficient than a tree-based query.  This fact is accounted for in the ball
 tree and KD tree by internally switching to brute force searches within
 leaf nodes.  The level of this switch can be specified with the parameter
 ``leaf_size``.  This parameter choice has many effects:
 
 **construction time**
@@ -460,30 +472,33 @@
   As ``leaf_size`` increases, the memory required to store a tree structure
   decreases.  This is especially important in the case of ball tree, which
   stores a :math:`D`-dimensional centroid for each node.  The required
   storage space for :class:`BallTree` is approximately ``1 / leaf_size`` times
   the size of the training set.
 
 ``leaf_size`` is not referenced for brute force queries.
+|details-end|
 
-Valid Metrics for Nearest Neighbor Algorithms
----------------------------------------------
+|details-start|
+**Valid Metrics for Nearest Neighbor Algorithms**
+|details-split|
 
 For a list of available metrics, see the documentation of the
 :class:`~sklearn.metrics.DistanceMetric` class and the metrics listed in
 `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the "cosine"
 metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.
 
 A list of valid metrics for any of the above algorithms can be obtained by using their
 ``valid_metric`` attribute. For example, valid metrics for ``KDTree`` can be generated by:
 
     >>> from sklearn.neighbors import KDTree
     >>> print(sorted(KDTree.valid_metrics))
     ['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']
 
+|details-end|
 
 .. _nearest_centroid_classifier:
 
 Nearest Centroid Classifier
 ===========================
 
 The :class:`NearestCentroid` classifier is a simple algorithm that represents
@@ -787,27 +802,28 @@
 space:
 
 .. math::
 
   p_{i j} = \frac{\exp(-||L x_i - L x_j||^2)}{\sum\limits_{k \ne
             i} {\exp{-(||L x_i - L x_k||^2)}}} , \quad p_{i i} = 0
 
-
-Mahalanobis distance
-^^^^^^^^^^^^^^^^^^^^
+|details-start|
+**Mahalanobis distance**
+|details-split|
 
 NCA can be seen as learning a (squared) Mahalanobis distance metric:
 
 .. math::
 
     || L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),
 
 where :math:`M = L^T L` is a symmetric positive semi-definite matrix of size
 ``(n_features, n_features)``.
 
+|details-end|
 
 Implementation
 --------------
 
 This implementation follows what is explained in the original paper [1]_. For
 the optimisation method, it currently uses scipy's L-BFGS-B with a full
 gradient computation at each iteration, to avoid to tune the learning rate and
@@ -840,7 +856,9 @@
     .. [1] `"Neighbourhood Components Analysis"
       <http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf>`_,
       J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in
       Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.
 
     `Wikipedia entry on Neighborhood Components Analysis
     <https://en.wikipedia.org/wiki/Neighbourhood_components_analysis>`_
+
+|details-end|
```

### Comparing `scikit-learn-1.4.2/doc/modules/neural_networks_supervised.rst` & `scikit_learn-1.5.0rc1/doc/modules/neural_networks_supervised.rst`

 * *Files 4% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 
 .. _multilayer_perceptron:
 
 Multi-layer Perceptron
 ======================
 
 **Multi-layer Perceptron (MLP)** is a supervised learning algorithm that learns
-a function :math:`f(\cdot): R^m \rightarrow R^o` by training on a dataset,
+a function :math:`f: R^m \rightarrow R^o` by training on a dataset,
 where :math:`m` is the number of dimensions for input and :math:`o` is the
 number of dimensions for output. Given a set of features :math:`X = {x_1, x_2, ..., x_m}`
 and a target :math:`y`, it can learn a non-linear function approximator for either
 classification or regression. It is different from logistic regression, in that
 between the input and the output layer, there can be one or more non-linear
 layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar
 output.
@@ -45,14 +45,18 @@
 
 The module contains the public attributes ``coefs_`` and ``intercepts_``.
 ``coefs_`` is a list of weight matrices, where weight matrix at index
 :math:`i` represents the weights between layer :math:`i` and layer
 :math:`i+1`. ``intercepts_`` is a list of bias vectors, where the vector
 at index :math:`i` represents the bias values added to layer :math:`i+1`.
 
+|details-start|
+**Advantages and disadvantages of Multi-layer Perceptron**
+|details-split|
+
 The advantages of Multi-layer Perceptron are:
 
 + Capability to learn non-linear models.
 
 + Capability to learn models in real-time (on-line learning)
   using ``partial_fit``.
 
@@ -67,14 +71,15 @@
   hidden neurons, layers, and iterations.
 
 + MLP is sensitive to feature scaling.
 
 Please see :ref:`Tips on Practical Use <mlp_tips>` section that addresses
 some of these disadvantages.
 
+|details-end|
 
 Classification
 ==============
 
 Class :class:`MLPClassifier` implements a multi-layer perceptron (MLP) algorithm
 that trains using `Backpropagation <http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm>`_.
 
@@ -142,15 +147,16 @@
 
 See the examples below and the docstring of
 :meth:`MLPClassifier.fit` for further information.
 
 .. topic:: Examples:
 
  * :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`
- * :ref:`sphx_glr_auto_examples_neural_networks_plot_mnist_filters.py`
+ * See :ref:`sphx_glr_auto_examples_neural_networks_plot_mnist_filters.py` for
+   visualized representation of trained weights.
 
 Regression
 ==========
 
 Class :class:`MLPRegressor` implements a multi-layer perceptron (MLP) that
 trains using backpropagation with no activation function in the output layer,
 which can also be seen as using the identity function as activation function.
@@ -224,17 +230,17 @@
 hidden layers, each containing :math:`h` neurons - for simplicity, and :math:`o`
 output neurons.  The time complexity of backpropagation is
 :math:`O(n\cdot m \cdot h^k \cdot o \cdot i)`, where :math:`i` is the number
 of iterations. Since backpropagation has a high time complexity, it is advisable
 to start with smaller number of hidden neurons and few hidden layers for
 training.
 
-
+|details-start|
 Mathematical formulation
-========================
+|details-split|
 
 Given a set of training examples :math:`(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)`
 where :math:`x_i \in \mathbf{R}^n` and :math:`y_i \in \{0, 1\}`, a one hidden
 layer one hidden neuron MLP learns the function :math:`f(x) = W_2 g(W_1^T x + b_1) + b_2`
 where :math:`W_1 \in \mathbf{R}^m` and :math:`W_2, b_1, b_2 \in \mathbf{R}` are
 model parameters. :math:`W_1, W_2` represent the weights of the input layer and
 hidden layer, respectively; and :math:`b_1, b_2` represent the bias added to
@@ -300,15 +306,15 @@
 
 where :math:`i` is the iteration step, and :math:`\epsilon` is the learning rate
 with a value larger than 0.
 
 The algorithm stops when it reaches a preset maximum number of iterations; or
 when the improvement in loss is below a certain, small number.
 
-
+|details-end|
 
 .. _mlp_tips:
 
 Tips on Practical Use
 =====================
 
 * Multi-layer Perceptron is sensitive to feature scaling, so it
@@ -351,15 +357,17 @@
     >>> y = [0, 1]
     >>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)
     >>> for i in range(10):
     ...     clf.fit(X, y)
     ...     # additional monitoring / inspection
     MLPClassifier(...
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
     * `"Learning representations by back-propagating errors."
       <https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf>`_
       Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.
 
     * `"Stochastic Gradient Descent" <https://leon.bottou.org/projects/sgd>`_ L. Bottou - Website, 2010.
 
@@ -369,7 +377,9 @@
     * `"Efficient BackProp" <http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>`_
       Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
       of the Trade 1998.
 
     *  :arxiv:`"Adam: A method for stochastic optimization."
        <1412.6980>`
        Kingma, Diederik, and Jimmy Ba (2014)
+
+|details-end|
```

### Comparing `scikit-learn-1.4.2/doc/modules/neural_networks_unsupervised.rst` & `scikit_learn-1.5.0rc1/doc/modules/neural_networks_unsupervised.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/outlier_detection.rst` & `scikit_learn-1.5.0rc1/doc/modules/outlier_detection.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/partial_dependence.rst` & `scikit_learn-1.5.0rc1/doc/modules/partial_dependence.rst`

 * *Files 0% similar despite different names*

```diff
@@ -104,15 +104,15 @@
 :func:`sklearn.inspection.partial_dependence` function::
 
     >>> from sklearn.inspection import partial_dependence
 
     >>> results = partial_dependence(clf, X, [0])
     >>> results["average"]
     array([[ 2.466...,  2.466..., ...
-    >>> results["values"]
+    >>> results["grid_values"]
     [array([-1.624..., -1.592..., ...
 
 The values at which the partial dependence should be evaluated are directly
 generated from ``X``. For 2-way partial dependence, a 2D-grid of values is
 generated. The ``values`` field returned by
 :func:`sklearn.inspection.partial_dependence` gives the actual values
 used in the grid for each input feature of interest. They also correspond to
```

### Comparing `scikit-learn-1.4.2/doc/modules/permutation_importance.rst` & `scikit_learn-1.5.0rc1/doc/modules/permutation_importance.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/preprocessing.rst` & `scikit_learn-1.5.0rc1/doc/modules/preprocessing.rst`

 * *Files 3% similar despite different names*

```diff
@@ -215,44 +215,53 @@
 --------------------------
 
 If your data contains many outliers, scaling using the mean and variance
 of the data is likely to not work very well. In these cases, you can use
 :class:`RobustScaler` as a drop-in replacement instead. It uses
 more robust estimates for the center and range of your data.
 
+|details-start|
+**References**
+|details-split|
 
-.. topic:: References:
+Further discussion on the importance of centering and scaling data is
+available on this FAQ: `Should I normalize/standardize/rescale the data?
+<http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html>`_
 
-  Further discussion on the importance of centering and scaling data is
-  available on this FAQ: `Should I normalize/standardize/rescale the data?
-  <http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html>`_
+|details-end|
 
-.. topic:: Scaling vs Whitening
+|details-start|
+**Scaling vs Whitening**
+|details-split|
 
-  It is sometimes not enough to center and scale the features
-  independently, since a downstream model can further make some assumption
-  on the linear independence of the features.
+It is sometimes not enough to center and scale the features
+independently, since a downstream model can further make some assumption
+on the linear independence of the features.
 
-  To address this issue you can use :class:`~sklearn.decomposition.PCA` with
-  ``whiten=True`` to further remove the linear correlation across features.
+To address this issue you can use :class:`~sklearn.decomposition.PCA` with
+``whiten=True`` to further remove the linear correlation across features.
+
+|details-end|
 
 .. _kernel_centering:
 
 Centering kernel matrices
 -------------------------
 
 If you have a kernel matrix of a kernel :math:`K` that computes a dot product
 in a feature space (possibly implicitly) defined by a function
 :math:`\phi(\cdot)`, a :class:`KernelCenterer` can transform the kernel matrix
 so that it contains inner products in the feature space defined by :math:`\phi`
 followed by the removal of the mean in that space. In other words,
 :class:`KernelCenterer` computes the centered Gram matrix associated to a
 positive semidefinite kernel :math:`K`.
 
+|details-start|
 **Mathematical formulation**
+|details-split|
 
 We can have a look at the mathematical formulation now that we have the
 intuition. Let :math:`K` be a kernel matrix of shape `(n_samples, n_samples)`
 computed from :math:`X`, a data matrix of shape `(n_samples, n_features)`,
 during the `fit` step. :math:`K` is defined by
 
 .. math::
@@ -297,14 +306,16 @@
 .. topic:: References
 
   .. [Scholkopf1998] B. Schölkopf, A. Smola, and K.R. Müller,
     `"Nonlinear component analysis as a kernel eigenvalue problem."
     <https://www.mlpack.org/papers/kpca.pdf>`_
     Neural computation 10.5 (1998): 1299-1319.
 
+|details-end|
+
 .. _preprocessing_transformer:
 
 Non-linear transformation
 =========================
 
 Two types of transformations are available: quantile transforms and power
 transforms. Both quantile and power transforms are based on monotonic
@@ -368,26 +379,32 @@
 Power transforms are a family of parametric, monotonic transformations that aim
 to map data from any distribution to as close to a Gaussian distribution as
 possible in order to stabilize variance and minimize skewness.
 
 :class:`PowerTransformer` currently provides two such power transformations,
 the Yeo-Johnson transform and the Box-Cox transform.
 
-The Yeo-Johnson transform is given by:
+|details-start|
+**Yeo-Johnson transform**
+|details-split|
 
 .. math::
     x_i^{(\lambda)} =
     \begin{cases}
      [(x_i + 1)^\lambda - 1] / \lambda & \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt]
     \ln{(x_i + 1)} & \text{if } \lambda = 0, x_i \geq 0 \\[8pt]
     -[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) & \text{if } \lambda \neq 2, x_i < 0, \\[8pt]
      - \ln (- x_i + 1) & \text{if } \lambda = 2, x_i < 0
     \end{cases}
 
-while the Box-Cox transform is given by:
+|details-end|
+
+|details-start|
+**Box-Cox transform**
+|details-split|
 
 .. math::
     x_i^{(\lambda)} =
     \begin{cases}
     \dfrac{x_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\[8pt]
     \ln{(x_i)} & \text{if } \lambda = 0,
     \end{cases}
@@ -409,14 +426,16 @@
          [-0.05...,  0.58..., -0.57...],
          [ 0.69..., -0.84...,  0.10...]])
 
 While the above example sets the `standardize` option to `False`,
 :class:`PowerTransformer` will apply zero-mean, unit-variance normalization
 to the transformed output by default.
 
+|details-end|
+
 Below are examples of Box-Cox and Yeo-Johnson applied to various probability
 distributions.  Note that when applied to certain distributions, the power
 transforms achieve very Gaussian-like results, but with others, they are
 ineffective. This highlights the importance of visualizing the data before and
 after transformation.
 
 .. figure:: ../auto_examples/preprocessing/images/sphx_glr_plot_map_data_to_normal_001.png
@@ -495,24 +514,27 @@
 
   >>> normalizer.transform([[-1.,  1., 0.]])
   array([[-0.70...,  0.70...,  0.  ...]])
 
 
 Note: L2 normalization is also known as spatial sign preprocessing.
 
-.. topic:: Sparse input
-
+|details-start|
+**Sparse input**
+|details-split|
   :func:`normalize` and :class:`Normalizer` accept **both dense array-like
   and sparse matrices from scipy.sparse as input**.
 
   For sparse input the data is **converted to the Compressed Sparse Rows
   representation** (see ``scipy.sparse.csr_matrix``) before being fed to
   efficient Cython routines. To avoid unnecessary memory copies, it is
   recommended to choose the CSR representation upstream.
 
+|details-end|
+
 .. _preprocessing_categorical_features:
 
 Encoding categorical features
 =============================
 Often features are not given as continuous values but categorical.
 For example a person could have features ``["male", "female"]``,
 ``["from Europe", "from US", "from Asia"]``,
@@ -695,14 +717,18 @@
     >>> X_test = [['unknown', 'America', 'IE']]
     >>> X_trans = drop_enc.transform(X_test)
     >>> X_trans
     array([[0., 0., 0., 0., 0., 0., 0.]])
     >>> drop_enc.inverse_transform(X_trans)
     array([['female', None, None]], dtype=object)
 
+|details-start|
+**Support of categorical features with missing values**
+|details-split|
+
 :class:`OneHotEncoder` supports categorical features with missing values by
 considering the missing values as an additional category::
 
     >>> X = [['male', 'Safari'],
     ...      ['female', None],
     ...      [np.nan, 'Firefox']]
     >>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)
@@ -726,14 +752,16 @@
            [0., 0., 1., 0.],
            [0., 0., 0., 1.],
            [1., 0., 0., 0.]])
 
 See :ref:`dict_feature_extraction` for categorical features that are
 represented as a dict, not as scalars.
 
+|details-end|
+
 .. _encoder_infrequent_categories:
 
 Infrequent categories
 ---------------------
 
 :class:`OneHotEncoder` and :class:`OrdinalEncoder` support aggregating
 infrequent categories into a single output for each feature. The parameters to
@@ -876,16 +904,21 @@
 .. currentmodule:: sklearn.preprocessing
 
 The :class:`TargetEncoder` uses the target mean conditioned on the categorical
 feature for encoding unordered categories, i.e. nominal categories [PAR]_
 [MIC]_. This encoding scheme is useful with categorical features with high
 cardinality, where one-hot encoding would inflate the feature space making it
 more expensive for a downstream model to process. A classical example of high
-cardinality categories are location based such as zip code or region. For the
-binary classification target, the target encoding is given by:
+cardinality categories are location based such as zip code or region.
+
+|details-start|
+**Binary classification targets**
+|details-split|
+
+For the binary classification target, the target encoding is given by:
 
 .. math::
     S_i = \lambda_i\frac{n_{iY}}{n_i} + (1 - \lambda_i)\frac{n_Y}{n}
 
 where :math:`S_i` is the encoding for category :math:`i`, :math:`n_{iY}` is the
 number of observations with :math:`Y=1` and category :math:`i`, :math:`n_i` is
 the number of observations with category :math:`i`, :math:`n_Y` is the number of
@@ -899,35 +932,49 @@
 where :math:`m` is a smoothing factor, which is controlled with the `smooth`
 parameter in :class:`TargetEncoder`. Large smoothing factors will put more
 weight on the global mean. When `smooth="auto"`, the smoothing factor is
 computed as an empirical Bayes estimate: :math:`m=\sigma_i^2/\tau^2`, where
 :math:`\sigma_i^2` is the variance of `y` with category :math:`i` and
 :math:`\tau^2` is the global variance of `y`.
 
+|details-end|
+
+|details-start|
+**Multiclass classification targets**
+|details-split|
+
 For multiclass classification targets, the formulation is similar to binary
 classification:
 
 .. math::
     S_{ij} = \lambda_i\frac{n_{iY_j}}{n_i} + (1 - \lambda_i)\frac{n_{Y_j}}{n}
 
 where :math:`S_{ij}` is the encoding for category :math:`i` and class :math:`j`,
 :math:`n_{iY_j}` is the number of observations with :math:`Y=j` and category
 :math:`i`, :math:`n_i` is the number of observations with category :math:`i`,
 :math:`n_{Y_j}` is the number of observations with :math:`Y=j`, :math:`n` is the
 number of observations, and :math:`\lambda_i` is a shrinkage factor for category
 :math:`i`.
 
+|details-end|
+
+|details-start|
+**Continuous targets**
+|details-split|
+
 For continuous targets, the formulation is similar to binary classification:
 
 .. math::
     S_i = \lambda_i\frac{\sum_{k\in L_i}Y_k}{n_i} + (1 - \lambda_i)\frac{\sum_{k=1}^{n}Y_k}{n}
 
 where :math:`L_i` is the set of observations with category :math:`i` and
 :math:`n_i` is the number of observations with category :math:`i`.
 
+|details-end|
+
 :meth:`~TargetEncoder.fit_transform` internally relies on a :term:`cross fitting`
 scheme to prevent target information from leaking into the train-time
 representation, especially for non-informative high-cardinality categorical
 variables, and help prevent the downstream model from overfitting spurious
 correlations. Note that as a result, `fit(X, y).transform(X)` does not equal
 `fit_transform(X, y)`. In :meth:`~TargetEncoder.fit_transform`, the training
 data is split into *k* folds (determined by the `cv` parameter) and each fold is
@@ -1248,23 +1295,27 @@
 ``knots = strategy``.
 
 .. topic:: Examples:
 
     * :ref:`sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py`
     * :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`
 
-.. topic:: References:
+|details-start|
+**References**
+|details-split|
 
     * Eilers, P., & Marx, B. (1996). :doi:`Flexible Smoothing with B-splines and
       Penalties <10.1214/ss/1038425655>`. Statist. Sci. 11 (1996), no. 2, 89--121.
 
     * Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. :doi:`A review of
       spline function procedures in R <10.1186/s12874-019-0666-3>`.
       BMC Med Res Methodol 19, 46 (2019).
 
+|details-end|
+
 .. _function_transformer:
 
 Custom transformers
 ===================
 
 Often, you will want to convert an existing Python function into a transformer
 to assist in data cleaning or processing. You can implement a transformer from
```

### Comparing `scikit-learn-1.4.2/doc/modules/preprocessing_targets.rst` & `scikit_learn-1.5.0rc1/doc/modules/preprocessing_targets.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/random_projection.rst` & `scikit_learn-1.5.0rc1/doc/modules/random_projection.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/semi_supervised.rst` & `scikit_learn-1.5.0rc1/doc/modules/semi_supervised.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/sgd.rst` & `scikit_learn-1.5.0rc1/doc/modules/sgd.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/svm.rst` & `scikit_learn-1.5.0rc1/doc/modules/svm.rst`

 * *Files 1% similar despite different names*

```diff
@@ -130,27 +130,27 @@
 
     >>> X = [[0], [1], [2], [3]]
     >>> Y = [0, 1, 2, 3]
     >>> clf = svm.SVC(decision_function_shape='ovo')
     >>> clf.fit(X, Y)
     SVC(decision_function_shape='ovo')
     >>> dec = clf.decision_function([[1]])
-    >>> dec.shape[1] # 4 classes: 4*3/2 = 6
+    >>> dec.shape[1] # 6 classes: 4*3/2 = 6
     6
     >>> clf.decision_function_shape = "ovr"
     >>> dec = clf.decision_function([[1]])
     >>> dec.shape[1] # 4 classes
     4
 
 On the other hand, :class:`LinearSVC` implements "one-vs-the-rest"
 multi-class strategy, thus training `n_classes` models.
 
-    >>> lin_clf = svm.LinearSVC(dual="auto")
+    >>> lin_clf = svm.LinearSVC()
     >>> lin_clf.fit(X, Y)
-    LinearSVC(dual='auto')
+    LinearSVC()
     >>> dec = lin_clf.decision_function([[1]])
     >>> dec.shape[1]
     4
 
 See :ref:`svm_mathematical_formulation` for a complete description of
 the decision function.
 
@@ -516,14 +516,15 @@
 is advised to use :class:`~sklearn.model_selection.GridSearchCV` with
 ``C`` and ``gamma`` spaced exponentially far apart to choose good values.
 
 .. topic:: Examples:
 
  * :ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`
  * :ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`
+ * :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`
 
 Custom Kernels
 --------------
 
 You can define your own kernels by either giving the kernel as a
 python function or by precomputing the Gram matrix.
```

### Comparing `scikit-learn-1.4.2/doc/modules/tree.rst` & `scikit_learn-1.5.0rc1/doc/modules/tree.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/modules/unsupervised_reduction.rst` & `scikit_learn-1.5.0rc1/doc/modules/unsupervised_reduction.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/preface.rst` & `scikit_learn-1.5.0rc1/doc/preface.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/presentations.rst` & `scikit_learn-1.5.0rc1/doc/presentations.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/related_projects.rst` & `scikit_learn-1.5.0rc1/doc/related_projects.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/roadmap.rst` & `scikit_learn-1.5.0rc1/doc/roadmap.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/sphinxext/add_toctree_functions.py` & `scikit_learn-1.5.0rc1/doc/sphinxext/add_toctree_functions.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/sphinxext/allow_nan_estimators.py` & `scikit_learn-1.5.0rc1/doc/sphinxext/allow_nan_estimators.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/sphinxext/doi_role.py` & `scikit_learn-1.5.0rc1/doc/sphinxext/doi_role.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 """
-    doilinks
-    ~~~~~~~~
-    Extension to add links to DOIs. With this extension you can use e.g.
-    :doi:`10.1016/S0022-2836(05)80360-2` in your documents. This will
-    create a link to a DOI resolver
-    (``https://doi.org/10.1016/S0022-2836(05)80360-2``).
-    The link caption will be the raw DOI.
-    You can also give an explicit caption, e.g.
-    :doi:`Basic local alignment search tool <10.1016/S0022-2836(05)80360-2>`.
+doilinks
+~~~~~~~~
+Extension to add links to DOIs. With this extension you can use e.g.
+:doi:`10.1016/S0022-2836(05)80360-2` in your documents. This will
+create a link to a DOI resolver
+(``https://doi.org/10.1016/S0022-2836(05)80360-2``).
+The link caption will be the raw DOI.
+You can also give an explicit caption, e.g.
+:doi:`Basic local alignment search tool <10.1016/S0022-2836(05)80360-2>`.
 
-    :copyright: Copyright 2015  Jon Lund Steffensen. Based on extlinks by
-        the Sphinx team.
-    :license: BSD.
+:copyright: Copyright 2015  Jon Lund Steffensen. Based on extlinks by
+    the Sphinx team.
+:license: BSD.
 """
 
 from docutils import nodes, utils
 from sphinx.util.nodes import split_explicit_title
 
 
 def reference_role(typ, rawtext, text, lineno, inliner, options={}, content=[]):
```

### Comparing `scikit-learn-1.4.2/doc/sphinxext/github_link.py` & `scikit_learn-1.5.0rc1/doc/sphinxext/github_link.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/sphinxext/sphinx_issues.py` & `scikit_learn-1.5.0rc1/doc/sphinxext/sphinx_issues.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 THE SOFTWARE.
 """
+
 import re
 
 from docutils import nodes, utils
 from sphinx.util.nodes import split_explicit_title
 
 __version__ = "1.2.0"
 __author__ = "Steven Loria"
```

### Comparing `scikit-learn-1.4.2/doc/supervised_learning.rst` & `scikit_learn-1.5.0rc1/doc/supervised_learning.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/templates/deprecated_class.rst` & `scikit_learn-1.5.0rc1/doc/templates/deprecated_class.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/templates/deprecated_class_with_call.rst` & `scikit_learn-1.5.0rc1/doc/templates/deprecated_class_with_call.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/templates/display_all_class_methods.rst` & `scikit_learn-1.5.0rc1/doc/templates/display_all_class_methods.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/templates/index.html` & `scikit_learn-1.5.0rc1/doc/templates/index.html`

 * *Files 4% similar despite different names*

```diff
@@ -66,16 +66,16 @@
           <a href="modules/ensemble.html#histogram-based-gradient-boosting">Gradient boosting</a>,
           <a href="modules/neighbors.html#regression">nearest neighbors</a>,
           <a href="modules/ensemble.html#forest">random forest</a>,
           <a href="modules/linear_model.html#ridge-regression-and-classification">ridge</a>,
           and <a href="supervised_learning.html#supervised-learning">more...</a></p>
         </div>
         <div class="overflow-hidden mx-2 text-center flex-fill">
-          <a href="auto_examples/ensemble/plot_adaboost_regression.html"  aria-label="Regression">
-          <img src="_images/sphx_glr_plot_adaboost_regression_thumb.png" class="sk-index-img" alt="Decision Tree Regression with AdaBoost">
+          <a href="auto_examples/ensemble/plot_hgbt_regression.html"  aria-label="Regression">
+          <img src="_images/sphx_glr_plot_hgbt_regression_002.png" class="sk-index-img" alt="Decision Tree Regression with HGBT">
           </a>
         </div>
           <a href="auto_examples/index.html#examples" class="sk-btn-primary btn text-white btn-block" role="button">Examples</a>
       </div>
     </div>
     <div class="col-md-4 mb-3 px-md-2 sk-px-xl-4">
       <div class="card h-100">
@@ -185,28 +185,30 @@
         <a href="https://scikit-learn.org/dev/whats_new.html"><strong>What's new</strong> (Changelog)</a>
         </li>
         </ul>
       </div>
       <div class="col-md-4">
         <h4 class="sk-landing-call-header">Community</h4>
         <ul class="sk-landing-call-list list-unstyled">
-        <li><strong>About us:</strong> See <a href="about.html#people">authors</a> and <a href="{{ contributing_link }}" {{ contributing_attrs }}>contributing</a></li>
+        <li><strong>About us:</strong> See <a
+href="about.html#the-people-behind-scikit-learn">people</a> and <a href="{{ contributing_link }}" {{ contributing_attrs }}>contributing</a></li>
         <li><strong>More Machine Learning:</strong> Find <a href="related_projects.html">related projects</a></li>
-        <li><strong>Questions?</strong> See <a href="faq.html">FAQ</a> and <a href="https://stackoverflow.com/questions/tagged/scikit-learn">stackoverflow</a></li>
+        <li><strong>Questions?</strong> See <a href="faq.html">FAQ</a>, <a href="support.html">support</a>, and <a href="https://stackoverflow.com/questions/tagged/scikit-learn">stackoverflow</a></li>
         <li><strong>Subscribe to the</strong> <a href="https://mail.python.org/mailman/listinfo/scikit-learn">mailing list</a></li>
-        <li><strong>Gitter:</strong> <a href="https://gitter.im/scikit-learn/scikit-learn">gitter.im/scikit-learn</a></li>
         <li><strong>Blog:</strong> <a href="https://blog.scikit-learn.org">blog.scikit-learn.org</a></li>
         <li><strong>Logos & Branding:</strong> <a href="https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos">logos and branding</a></li>
         <li><strong>Calendar:</strong> <a href="https://blog.scikit-learn.org/calendar/">calendar</a></li>
         <li><strong>Twitter:</strong> <a href="https://twitter.com/scikit_learn">@scikit_learn</a></li>
         <li><strong>LinkedIn:</strong> <a href="https://www.linkedin.com/company/scikit-learn">linkedin/scikit-learn</a></li>
         <li><strong>YouTube:</strong> <a href="https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists">youtube.com/scikit-learn</a></li>
         <li><strong>Facebook:</strong> <a href="https://www.facebook.com/scikitlearnofficial/">@scikitlearnofficial</a></li>
         <li><strong>Instagram:</strong> <a href="https://www.instagram.com/scikitlearnofficial/">@scikitlearnofficial</a></li>
         <li><strong>TikTok:</strong> <a href="https://www.tiktok.com/@scikit.learn">@scikit.learn</a></li>
+        <li><strong>Mastodon:</strong> <a href="https://mastodon.social/@sklearn@fosstodon.org">@sklearn</a></li>
+        <li><strong>Discord:</strong> <a href="https://discord.gg/h9qyrK8Jc8">@scikit-learn</a></li>
         <li>Communication on all channels should respect <a href="https://www.python.org/psf/conduct/">PSF's code of conduct.</a></li>
         </ul>
 
         <a class="btn btn-warning btn-big sk-donate-btn mb-1" href="https://numfocus.org/donate-to-scikit-learn">Help us, <strong>donate!</strong></a>
         <a class="btn btn-warning btn-big mb-1" href="about.html#citing-scikit-learn"><strong>Cite us!</strong></a>
       </div>
       <div class="col-md-4">
```

#### html2text {}

```diff
@@ -17,15 +17,15 @@
 _n_e_a_r_e_s_t_ _n_e_i_g_h_b_o_r_s, _r_a_n_d_o_m_ _f_o_r_e_s_t, _l_o_g_i_s_t_i_c_ _r_e_g_r_e_s_s_i_o_n, and _m_o_r_e_._._.
 _[_C_l_a_s_s_i_f_i_e_r_ _c_o_m_p_a_r_i_s_o_n_]
 _E_x_a_m_p_l_e_s
 _**_**_**_ _RR_ee_gg_rr_ee_ss_ss_ii_oo_nn_ _**_**_**
 Predicting a continuous-valued attribute associated with an object.
 AApppplliiccaattiioonnss:: Drug response, Stock prices. AAllggoorriitthhmmss:: _G_r_a_d_i_e_n_t_ _b_o_o_s_t_i_n_g,
 _n_e_a_r_e_s_t_ _n_e_i_g_h_b_o_r_s, _r_a_n_d_o_m_ _f_o_r_e_s_t, _r_i_d_g_e, and _m_o_r_e_._._.
-_[_D_e_c_i_s_i_o_n_ _T_r_e_e_ _R_e_g_r_e_s_s_i_o_n_ _w_i_t_h_ _A_d_a_B_o_o_s_t_]
+_[_D_e_c_i_s_i_o_n_ _T_r_e_e_ _R_e_g_r_e_s_s_i_o_n_ _w_i_t_h_ _H_G_B_T_]
 _E_x_a_m_p_l_e_s
 _**_**_**_ _CC_ll_uu_ss_tt_ee_rr_ii_nn_gg_ _**_**_**
 Automatic grouping of similar objects into sets.
 AApppplliiccaattiioonnss:: Customer segmentation, Grouping experiment outcomes AAllggoorriitthhmmss::
 _k_-_M_e_a_n_s, _H_D_B_S_C_A_N, _h_i_e_r_a_r_c_h_i_c_a_l_ _c_l_u_s_t_e_r_i_n_g, and _m_o_r_e_._._.
 _[_A_ _d_e_m_o_ _o_f_ _K_-_M_e_a_n_s_ _c_l_u_s_t_e_r_i_n_g_ _o_n_ _t_h_e_ _h_a_n_d_w_r_i_t_t_e_n_ _d_i_g_i_t_s_ _d_a_t_a_]
 _E_x_a_m_p_l_e_s
@@ -54,28 +54,29 @@
       (_C_h_a_n_g_e_l_o_g).
     * JJaannuuaarryy 22002244.. scikit-learn 1.4.0 is available for download (_C_h_a_n_g_e_l_o_g).
     * OOccttoobbeerr 22002233.. scikit-learn 1.3.2 is available for download (_C_h_a_n_g_e_l_o_g).
     * SSeepptteemmbbeerr 22002233.. scikit-learn 1.3.1 is available for download (_C_h_a_n_g_e_l_o_g).
     * JJuunnee 22002233.. scikit-learn 1.3.0 is available for download (_C_h_a_n_g_e_l_o_g).
     * AAllll rreelleeaasseess:: _WW_hh_aa_tt_''_ss_ _nn_ee_ww_ _(_C_h_a_n_g_e_l_o_g_)
 ****** CCoommmmuunniittyy ******
-    * AAbboouutt uuss:: See _a_u_t_h_o_r_s and { contributing_attrs }}>contributing
+    * AAbboouutt uuss:: See _p_e_o_p_l_e and { contributing_attrs }}>contributing
 MMoorree MMaacchhiinnee LLeeaarrnniinngg:: Find _r_e_l_a_t_e_d_ _p_r_o_j_e_c_t_s
-QQuueessttiioonnss?? See _F_A_Q and _s_t_a_c_k_o_v_e_r_f_l_o_w
+QQuueessttiioonnss?? See _F_A_Q, _s_u_p_p_o_r_t, and _s_t_a_c_k_o_v_e_r_f_l_o_w
 SSuubbssccrriibbee ttoo tthhee _m_a_i_l_i_n_g_ _l_i_s_t
-GGiitttteerr:: _g_i_t_t_e_r_._i_m_/_s_c_i_k_i_t_-_l_e_a_r_n
 BBlloogg:: _b_l_o_g_._s_c_i_k_i_t_-_l_e_a_r_n_._o_r_g
 LLooggooss && BBrraannddiinngg:: _l_o_g_o_s_ _a_n_d_ _b_r_a_n_d_i_n_g
 CCaalleennddaarr:: _c_a_l_e_n_d_a_r
 TTwwiitttteerr:: _@_s_c_i_k_i_t___l_e_a_r_n
 LLiinnkkeeddIInn:: _l_i_n_k_e_d_i_n_/_s_c_i_k_i_t_-_l_e_a_r_n
 YYoouuTTuubbee:: _y_o_u_t_u_b_e_._c_o_m_/_s_c_i_k_i_t_-_l_e_a_r_n
 FFaacceebbooookk:: _@_s_c_i_k_i_t_l_e_a_r_n_o_f_f_i_c_i_a_l
 IInnssttaaggrraamm:: _@_s_c_i_k_i_t_l_e_a_r_n_o_f_f_i_c_i_a_l
 TTiikkTTookk:: _@_s_c_i_k_i_t_._l_e_a_r_n
+MMaassttooddoonn:: _@_s_k_l_e_a_r_n
+DDiissccoorrdd:: _@_s_c_i_k_i_t_-_l_e_a_r_n
 Communication on all channels should respect _P_S_F_'_s_ _c_o_d_e_ _o_f_ _c_o_n_d_u_c_t_.
 _H_e_l_p_ _u_s_,_ _dd_oo_nn_aa_tt_ee_!! _CC_ii_tt_ee_ _uu_ss_!!
 ****** WWhhoo uusseess sscciikkiitt--lleeaarrnn?? ******
 [inria]""WWee uussee sscciikkiitt--lleeaarrnn ttoo ssuuppppoorrtt lleeaaddiinngg--eeddggee bbaassiicc rreesseeaarrcchh [[......]]""
 [spotify]""II tthhiinnkk iitt''ss tthhee mmoosstt wweellll--ddeessiiggnneedd MMLL ppaacckkaaggee II''vvee sseeeenn ssoo ffaarr..""
 [change-logo]""sscciikkiitt--lleeaarrnn''ss eeaassee--ooff--uussee,, ppeerrffoorrmmaannccee aanndd oovveerraallll vvaarriieettyy ooff
 aallggoorriitthhmmss iimmpplleemmeenntteedd hhaass pprroovveedd iinnvvaalluuaabbllee [[......]]..""
```

### Comparing `scikit-learn-1.4.2/doc/templates/redirects.html` & `scikit_learn-1.5.0rc1/doc/templates/redirects.html`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/aweber.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/aweber.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/bestofmedia-logo.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/bestofmedia-logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/betaworks.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/betaworks.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/birchbox.jpg` & `scikit_learn-1.5.0rc1/doc/testimonials/images/birchbox.jpg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/bnp_paribas_cardif.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/bnp_paribas_cardif.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/booking.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/booking.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/change-logo.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/change-logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/dataiku_logo.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/dataiku_logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/datapublica.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/datapublica.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/datarobot.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/datarobot.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/evernote.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/evernote.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/howaboutwe.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/howaboutwe.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/huggingface.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/huggingface.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/infonea.jpg` & `scikit_learn-1.5.0rc1/doc/testimonials/images/infonea.jpg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/inria.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/inria.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/jpmorgan.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/jpmorgan.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/lovely.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/lovely.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/machinalis.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/machinalis.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/mars.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/mars.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/okcupid.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/okcupid.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/ottogroup_logo.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/ottogroup_logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/peerindex.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/peerindex.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/phimeca.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/phimeca.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/rangespan.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/rangespan.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/solido_logo.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/solido_logo.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/spotify.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/spotify.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/telecomparistech.jpg` & `scikit_learn-1.5.0rc1/doc/testimonials/images/telecomparistech.jpg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/yhat.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/yhat.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/images/zopa.png` & `scikit_learn-1.5.0rc1/doc/testimonials/images/zopa.png`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/testimonials/testimonials.rst` & `scikit_learn-1.5.0rc1/doc/testimonials/testimonials.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/javascript.html` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/javascript.html`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/layout.html` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/layout.html`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/nav.html` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/nav.html`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/css/theme.css` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/css/theme.css`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/css/vendor/bootstrap.min.css` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/css/vendor/bootstrap.min.css`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/details-permalink.js` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/js/details-permalink.js`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/vendor/bootstrap.min.js` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/js/vendor/bootstrap.min.js`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/themes/scikit-learn-modern/static/js/vendor/jquery-3.6.3.slim.min.js` & `scikit_learn-1.5.0rc1/doc/themes/scikit-learn-modern/static/js/vendor/jquery-3.6.3.slim.min.js`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tune_toc.rst` & `scikit_learn-1.5.0rc1/doc/tune_toc.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/basic/tutorial.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/basic/tutorial.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/index.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/index.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/machine_learning_map/ML_MAPS_README.txt` & `scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/ML_MAPS_README.txt`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/machine_learning_map/index.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/index.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/machine_learning_map/parse_path.py` & `scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/parse_path.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/machine_learning_map/pyparsing.py` & `scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/pyparsing.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/machine_learning_map/svg2imagemap.py` & `scikit_learn-1.5.0rc1/doc/tutorial/machine_learning_map/svg2imagemap.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/statistical_inference/index.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/index.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/statistical_inference/model_selection.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/model_selection.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/statistical_inference/putting_together.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/putting_together.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/statistical_inference/settings.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/settings.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/statistical_inference/supervised_learning.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/supervised_learning.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/statistical_inference/unsupervised_learning.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/statistical_inference/unsupervised_learning.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/data/languages/fetch_data.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/data/languages/fetch_data.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/skeletons/exercise_01_language_train_model.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/solutions/exercise_01_language_train_model.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/solutions/generate_skeletons.py` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/solutions/generate_skeletons.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/tutorial/text_analytics/working_with_text_data.rst` & `scikit_learn-1.5.0rc1/doc/tutorial/text_analytics/working_with_text_data.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/user_guide.rst` & `scikit_learn-1.5.0rc1/doc/user_guide.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/visualizations.rst` & `scikit_learn-1.5.0rc1/doc/visualizations.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/_contributors.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/_contributors.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/changelog_legend.inc` & `scikit_learn-1.5.0rc1/doc/whats_new/changelog_legend.inc`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/older_versions.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/older_versions.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.13.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.13.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.14.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.14.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.15.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.15.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.16.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.16.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.17.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.17.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.18.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.18.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.19.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.19.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.20.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.20.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.21.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.21.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.22.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.22.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.23.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.23.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v0.24.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v0.24.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v1.0.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v1.0.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v1.1.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v1.1.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v1.2.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v1.2.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v1.3.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v1.3.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/doc/whats_new/v1.4.rst` & `scikit_learn-1.5.0rc1/doc/whats_new/v1.4.rst`

 * *Files 1% similar despite different names*

```diff
@@ -20,35 +20,35 @@
 
 **April 2024**
 
 This release only includes support for numpy 2.
 
 .. _changes_1_4_1:
 
-Version 1.4.1.post1
-===================
+Version 1.4.1
+=============
 
 **February 2024**
 
-.. note::
-    The 1.4.1.post1 release includes a packaging fix requiring `numpy<2` to account for
-    incompatibilities with NumPy 2.0 ABI. Note that the 1.4.1 release is not available
-    on PyPI and conda-forge.
-
 Metadata Routing
 ----------------
 
 - |FIX| Fix routing issue with :class:`~compose.ColumnTransformer` when used
   inside another meta-estimator.
   :pr:`28188` by `Adrin Jalali`_.
 
 - |Fix| No error is raised when no metadata is passed to a metaestimator that
   includes a sub-estimator which doesn't support metadata routing.
   :pr:`28256` by `Adrin Jalali`_.
 
+- |Fix| Fix :class:`multioutput.MultiOutputRegressor` and
+  :class:`multioutput.MultiOutputClassifier` to work with estimators that don't
+  consume any metadata when metadata routing is enabled.
+  :pr:`28240` by `Adrin Jalali`_.
+
 DataFrame Support
 -----------------
 
 - |Enhancement| |Fix| Pandas and Polars dataframe are validated directly without
   ducktyping checks.
   :pr:`28195` by `Thomas Fan`_.
 
@@ -86,22 +86,14 @@
   :class:`~feature_selection.SelectFromModel`, :class:`~feature_selection.RFE`,
   :class:`~semi_supervised.SelfTrainingClassifier`,
   :class:`~multiclass.OneVsOneClassifier`, :class:`~multiclass.OutputCodeClassifier` or
   :class:`~multiclass.OneVsRestClassifier` that their sub-estimators don't implement,
   the `AttributeError` now reraises in the traceback.
   :pr:`28167` by :user:`Stefanie Senger <StefanieSenger>`.
 
-Metadata Routing
-----------------
-
-- |Fix| Fix :class:`multioutput.MultiOutputRegressor` and
-  :class:`multioutput.MultiOutputClassifier` to work with estimators that don't
-  consume any metadata when metadata routing is enabled.
-  :pr:`28240` by `Adrin Jalali`_.
-
 Changelog
 ---------
 
 :mod:`sklearn.calibration`
 ..........................
 
 - |Fix| `calibration.CalibratedClassifierCV` supports :term:`predict_proba` with
```

### Comparing `scikit-learn-1.4.2/doc/whats_new.rst` & `scikit_learn-1.5.0rc1/doc/whats_new.rst`

 * *Files 4% similar despite different names*

```diff
@@ -11,14 +11,15 @@
 
    `Subscribe to scikit-learn releases <https://libraries.io/pypi/scikit-learn>`__
    on libraries.io to be notified when new versions are released.
 
 .. toctree::
    :maxdepth: 2
 
+   whats_new/v1.5.rst
    whats_new/v1.4.rst
    whats_new/v1.3.rst
    whats_new/v1.2.rst
    whats_new/v1.1.rst
    whats_new/v1.0.rst
    whats_new/v0.24.rst
    whats_new/v0.23.rst
```

### Comparing `scikit-learn-1.4.2/examples/applications/plot_cyclical_feature_engineering.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_cyclical_feature_engineering.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_digits_denoising.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_digits_denoising.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_face_recognition.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_face_recognition.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 "Labeled Faces in the Wild", aka LFW_:
 
   http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)
 
 .. _LFW: http://vis-www.cs.umass.edu/lfw/
 
 """
+
 # %%
 from time import time
 
 import matplotlib.pyplot as plt
 from scipy.stats import loguniform
 
 from sklearn.datasets import fetch_lfw_people
```

### Comparing `scikit-learn-1.4.2/examples/applications/plot_model_complexity_influence.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_model_complexity_influence.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_out_of_core_classification.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_out_of_core_classification.py`

 * *Files 1% similar despite different names*

```diff
@@ -171,15 +171,16 @@
         if _not_in_sphinx():
             sys.stdout.write("\r")
 
         # Check that the archive was not tampered:
         assert sha256(archive_path.read_bytes()).hexdigest() == ARCHIVE_SHA256
 
         print("untarring Reuters dataset...")
-        tarfile.open(archive_path, "r:gz").extractall(data_path)
+        with tarfile.open(archive_path, "r:gz") as fp:
+            fp.extractall(data_path, filter="data")
         print("done.")
 
     parser = ReutersParser()
     for filename in data_path.glob("*.sgm"):
         for doc in parser.parse(open(filename, "rb")):
             yield doc
```

### Comparing `scikit-learn-1.4.2/examples/applications/plot_outlier_detection_wine.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_outlier_detection_wine.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_prediction_latency.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_prediction_latency.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_species_distribution_modeling.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_species_distribution_modeling.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_stock_market.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_stock_market.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_time_series_lagged_features.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_time_series_lagged_features.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """
 ===========================================
 Lagged features for time series forecasting
 ===========================================
 
-This example demonstrates how pandas-engineered lagged features can be used
+This example demonstrates how Polars-engineered lagged features can be used
 for time series forecasting with
 :class:`~sklearn.ensemble.HistGradientBoostingRegressor` on the Bike Sharing
 Demand dataset.
 
 See the example on
 :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`
 for some data exploration on this dataset and a demo on periodic feature
@@ -15,86 +15,86 @@
 
 """
 
 # %%
 # Analyzing the Bike Sharing Demand dataset
 # -----------------------------------------
 #
-# We start by loading the data from the OpenML repository.
+# We start by loading the data from the OpenML repository
+# as a pandas dataframe. This will be replaced with Polars
+# once `fetch_openml` adds a native support for it.
+# We convert to Polars for feature engineering, as it automatically caches
+# common subexpressions which are reused in multiple expressions
+# (like `pl.col("count").shift(1)` below). See
+# https://docs.pola.rs/user-guide/lazy/optimizations/ for more information.
+
 import numpy as np
-import pandas as pd
+import polars as pl
 
 from sklearn.datasets import fetch_openml
 
+pl.Config.set_fmt_str_lengths(20)
+
 bike_sharing = fetch_openml(
     "Bike_Sharing_Demand", version=2, as_frame=True, parser="pandas"
 )
 df = bike_sharing.frame
+df = pl.DataFrame({col: df[col].to_numpy() for col in df.columns})
 
 # %%
 # Next, we take a look at the statistical summary of the dataset
 # so that we can better understand the data that we are working with.
-summary = pd.DataFrame(df.describe())
-summary = (
-    summary.style.background_gradient()
-    .set_table_attributes("style = 'display: inline'")
-    .set_caption("Statistics of the Dataset")
-    .set_table_styles([{"selector": "caption", "props": [("font-size", "16px")]}])
-)
+import polars.selectors as cs
+
+summary = df.select(cs.numeric()).describe()
 summary
 
 # %%
 # Let us look at the count of the seasons `"fall"`, `"spring"`, `"summer"`
 # and `"winter"` present in the dataset to confirm they are balanced.
 
 import matplotlib.pyplot as plt
 
 df["season"].value_counts()
 
 
 # %%
-# Generating pandas-engineered lagged features
+# Generating Polars-engineered lagged features
 # --------------------------------------------
 # Let's consider the problem of predicting the demand at the
 # next hour given past demands. Since the demand is a continuous
 # variable, one could intuitively use any regression model. However, we do
 # not have the usual `(X_train, y_train)` dataset. Instead, we just have
 # the `y_train` demand data sequentially organized by time.
-count = df["count"]
-lagged_df = pd.concat(
-    [
-        count,
-        count.shift(1).rename("lagged_count_1h"),
-        count.shift(2).rename("lagged_count_2h"),
-        count.shift(3).rename("lagged_count_3h"),
-        count.shift(24).rename("lagged_count_1d"),
-        count.shift(24 + 1).rename("lagged_count_1d_1h"),
-        count.shift(7 * 24).rename("lagged_count_7d"),
-        count.shift(7 * 24 + 1).rename("lagged_count_7d_1h"),
-        count.shift(1).rolling(24).mean().rename("lagged_mean_24h"),
-        count.shift(1).rolling(24).max().rename("lagged_max_24h"),
-        count.shift(1).rolling(24).min().rename("lagged_min_24h"),
-        count.shift(1).rolling(7 * 24).mean().rename("lagged_mean_7d"),
-        count.shift(1).rolling(7 * 24).max().rename("lagged_max_7d"),
-        count.shift(1).rolling(7 * 24).min().rename("lagged_min_7d"),
-    ],
-    axis="columns",
+lagged_df = df.select(
+    "count",
+    *[pl.col("count").shift(i).alias(f"lagged_count_{i}h") for i in [1, 2, 3]],
+    lagged_count_1d=pl.col("count").shift(24),
+    lagged_count_1d_1h=pl.col("count").shift(24 + 1),
+    lagged_count_7d=pl.col("count").shift(7 * 24),
+    lagged_count_7d_1h=pl.col("count").shift(7 * 24 + 1),
+    lagged_mean_24h=pl.col("count").shift(1).rolling_mean(24),
+    lagged_max_24h=pl.col("count").shift(1).rolling_max(24),
+    lagged_min_24h=pl.col("count").shift(1).rolling_min(24),
+    lagged_mean_7d=pl.col("count").shift(1).rolling_mean(7 * 24),
+    lagged_max_7d=pl.col("count").shift(1).rolling_max(7 * 24),
+    lagged_min_7d=pl.col("count").shift(1).rolling_min(7 * 24),
 )
 lagged_df.tail(10)
 
 # %%
 # Watch out however, the first lines have undefined values because their own
 # past is unknown. This depends on how much lag we used:
 lagged_df.head(10)
 
 # %%
 # We can now separate the lagged features in a matrix `X` and the target variable
 # (the counts to predict) in an array of the same first dimension `y`.
-lagged_df = lagged_df.dropna()
-X = lagged_df.drop("count", axis="columns")
+lagged_df = lagged_df.drop_nulls()
+X = lagged_df.drop("count")
 y = lagged_df["count"]
 print("X shape: {}\ny shape: {}".format(X.shape, y.shape))
 
 # %%
 # Naive evaluation of the next hour bike demand regression
 # --------------------------------------------------------
 # Let's randomly split our tabularized dataset to train a gradient
@@ -137,16 +137,16 @@
     test_size=3000,  # for 2 or 3 digits of precision in scores
 )
 all_splits = list(ts_cv.split(X, y))
 
 # %%
 # Training the model and evaluating its performance based on MAPE.
 train_idx, test_idx = all_splits[0]
-X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
-y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
+X_train, X_test = X[train_idx, :], X[test_idx, :]
+y_train, y_test = y[train_idx], y[test_idx]
 
 model = HistGradientBoostingRegressor().fit(X_train, y_train)
 y_pred = model.predict(X_test)
 mean_absolute_percentage_error(y_test, y_pred)
 
 # %%
 # The generalization error measured via a shuffled trained test split
@@ -254,62 +254,51 @@
 
     scores["loss"].append(f"quantile {int(quantile*100)}")
     for key, value in cv_results.items():
         if key.startswith("test_"):
             metric = key.split("test_")[1]
             scores = consolidate_scores(cv_results, scores, metric)
 
-df = pd.DataFrame(scores)
-
-styled_df_copy = df.copy()
-
-
-def extract_numeric(value):
-    parts = value.split("±")
-    mean_value = float(parts[0])
-    std_value = float(parts[1].split()[0])
+scores_df = pl.DataFrame(scores)
+scores_df
 
-    return mean_value, std_value
 
+# %%
+# Let us take a look at the losses that minimise each metric.
+def min_arg(col):
+    col_split = pl.col(col).str.split(" ")
+    return pl.arg_sort_by(
+        col_split.list.get(0).cast(pl.Float64),
+        col_split.list.get(2).cast(pl.Float64),
+    ).first()
 
-# Convert columns containing "±" to tuples of numerical values
-cols_to_convert = df.columns[1:]  # Exclude the "loss" column
-for col in cols_to_convert:
-    df[col] = df[col].apply(extract_numeric)
-
-min_values = df.min()
-
-# Create a mask for highlighting minimum values
-mask = pd.DataFrame("", index=df.index, columns=df.columns)
-for col in cols_to_convert:
-    mask[col] = df[col].apply(
-        lambda x: "font-weight: bold" if x == min_values[col] else ""
-    )
-
-styled_df_copy = styled_df_copy.style.apply(lambda x: mask, axis=None)
-styled_df_copy
 
+scores_df.select(
+    pl.col("loss").get(min_arg(col_name)).alias(col_name)
+    for col_name in scores_df.columns
+    if col_name != "loss"
+)
 
 # %%
-# Even if the score distributions overlap due to the variance in the dataset, it
-# is true that the average RMSE is lower when `loss="squared_error"`, whereas
+# Even if the score distributions overlap due to the variance in the dataset,
+# it is true that the average RMSE is lower when `loss="squared_error"`, whereas
 # the average MAPE is lower when `loss="absolute_error"` as expected. That is
 # also the case for the Mean Pinball Loss with the quantiles 5 and 95. The score
 # corresponding to the 50 quantile loss is overlapping with the score obtained
 # by minimizing other loss functions, which is also the case for the MAE.
 #
 # A qualitative look at the predictions
 # -------------------------------------
 # We can now visualize the performance of the model with regards
 # to the 5th percentile, median and the 95th percentile:
 all_splits = list(ts_cv.split(X, y))
 train_idx, test_idx = all_splits[0]
 
-X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
-y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
+X_train, X_test = X[train_idx, :], X[test_idx, :]
+y_train, y_test = y[train_idx], y[test_idx]
 
 max_iter = 50
 gbrt_mean_poisson = HistGradientBoostingRegressor(loss="poisson", max_iter=max_iter)
 gbrt_mean_poisson.fit(X_train, y_train)
 mean_predictions = gbrt_mean_poisson.predict(X_test)
 
 gbrt_median = HistGradientBoostingRegressor(
@@ -332,15 +321,15 @@
 
 # %%
 # We can now take a look at the predictions made by the regression models:
 last_hours = slice(-96, None)
 fig, ax = plt.subplots(figsize=(15, 7))
 plt.title("Predictions by regression models")
 ax.plot(
-    y_test.values[last_hours],
+    y_test[last_hours],
     "x-",
     alpha=0.2,
     label="Actual demand",
     color="black",
 )
 ax.plot(
     median_predictions[last_hours],
@@ -395,15 +384,15 @@
 labels = [
     "Median",
     "5th percentile",
     "95th percentile",
 ]
 for ax, pred, label in zip(axes, predictions, labels):
     PredictionErrorDisplay.from_predictions(
-        y_true=y_test.values,
+        y_true=y_test,
         y_pred=pred,
         kind="residual_vs_predicted",
         scatter_kwargs={"alpha": 0.3},
         ax=ax,
     )
     ax.set(xlabel="Predicted demand", ylabel="True demand")
     ax.legend(["Best model", label])
```

### Comparing `scikit-learn-1.4.2/examples/applications/plot_tomography_l1_reconstruction.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_tomography_l1_reconstruction.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/plot_topics_extraction_with_nmf_lda.py` & `scikit_learn-1.5.0rc1/examples/applications/plot_topics_extraction_with_nmf_lda.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/applications/wikipedia_principal_eigenvector.py` & `scikit_learn-1.5.0rc1/examples/applications/wikipedia_principal_eigenvector.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/bicluster/plot_bicluster_newsgroups.py` & `scikit_learn-1.5.0rc1/examples/bicluster/plot_bicluster_newsgroups.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/bicluster/plot_spectral_biclustering.py` & `scikit_learn-1.5.0rc1/examples/bicluster/plot_spectral_biclustering.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/bicluster/plot_spectral_coclustering.py` & `scikit_learn-1.5.0rc1/examples/bicluster/plot_spectral_coclustering.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/calibration/plot_calibration.py` & `scikit_learn-1.5.0rc1/examples/calibration/plot_calibration.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 isotonic calibration. One can observe that only the non-parametric model is
 able to provide a probability calibration that returns probabilities close
 to the expected 0.5 for most of the samples belonging to the middle
 cluster with heterogeneous labels. This results in a significantly improved
 Brier score.
 
 """
+
 # Authors:
 # Mathieu Blondel <mathieu@mblondel.org>
 # Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
 # Balazs Kegl <balazs.kegl@gmail.com>
 # Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD Style.
```

### Comparing `scikit-learn-1.4.2/examples/calibration/plot_calibration_curve.py` & `scikit_learn-1.5.0rc1/examples/calibration/plot_calibration_curve.py`

 * *Files 1% similar despite different names*

```diff
@@ -218,15 +218,15 @@
         proba = np.c_[proba_neg_class, proba_pos_class]
         return proba
 
 
 # %%
 
 lr = LogisticRegression(C=1.0)
-svc = NaivelyCalibratedLinearSVC(max_iter=10_000, dual="auto")
+svc = NaivelyCalibratedLinearSVC(max_iter=10_000)
 svc_isotonic = CalibratedClassifierCV(svc, cv=2, method="isotonic")
 svc_sigmoid = CalibratedClassifierCV(svc, cv=2, method="sigmoid")
 
 clf_list = [
     (lr, "Logistic"),
     (svc, "SVC"),
     (svc_isotonic, "SVC + Isotonic"),
```

### Comparing `scikit-learn-1.4.2/examples/calibration/plot_calibration_multiclass.py` & `scikit_learn-1.5.0rc1/examples/calibration/plot_calibration_multiclass.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/calibration/plot_compare_calibration.py` & `scikit_learn-1.5.0rc1/examples/calibration/plot_compare_calibration.py`

 * *Files 1% similar despite different names*

```diff
@@ -103,15 +103,15 @@
 # For a fair comparison, we should run a hyper-parameter search for all the
 # classifiers but we don't do it here for the sake of keeping the example code
 # concise and fast to execute.
 lr = LogisticRegressionCV(
     Cs=np.logspace(-6, 6, 101), cv=10, scoring="neg_log_loss", max_iter=1_000
 )
 gnb = GaussianNB()
-svc = NaivelyCalibratedLinearSVC(C=1.0, dual="auto")
+svc = NaivelyCalibratedLinearSVC(C=1.0)
 rfc = RandomForestClassifier(random_state=42)
 
 clf_list = [
     (lr, "Logistic Regression"),
     (gnb, "Naive Bayes"),
     (svc, "SVC"),
     (rfc, "Random forest"),
```

### Comparing `scikit-learn-1.4.2/examples/classification/plot_classification_probability.py` & `scikit_learn-1.5.0rc1/examples/classification/plot_classification_probability.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 """
 ===============================
 Plot classification probability
 ===============================
 
 Plot the classification probability for different classifiers. We use a 3 class
 dataset, and we classify it with a Support Vector classifier, L1 and L2
-penalized logistic regression with either a One-Vs-Rest or multinomial setting,
-and Gaussian process classification.
+penalized logistic regression (multinomial multiclass), a One-Vs-Rest version with
+logistic regression, and Gaussian process classification.
 
 Linear SVC is not a probabilistic classifier by default but it has a built-in
 calibration option enabled in this example (`probability=True`).
 
 The logistic regression with One-Vs-Rest is not a multiclass classifier out of
 the box. As a result it has more trouble in separating class 2 and 3 than the
 other estimators.
@@ -26,35 +26,34 @@
 
 from sklearn import datasets
 from sklearn.gaussian_process import GaussianProcessClassifier
 from sklearn.gaussian_process.kernels import RBF
 from sklearn.inspection import DecisionBoundaryDisplay
 from sklearn.linear_model import LogisticRegression
 from sklearn.metrics import accuracy_score
+from sklearn.multiclass import OneVsRestClassifier
 from sklearn.svm import SVC
 
 iris = datasets.load_iris()
 X = iris.data[:, 0:2]  # we only take the first two features for visualization
 y = iris.target
 
 n_features = X.shape[1]
 
 C = 10
 kernel = 1.0 * RBF([1.0, 1.0])  # for GPC
 
 # Create different classifiers.
 classifiers = {
-    "L1 logistic": LogisticRegression(
-        C=C, penalty="l1", solver="saga", multi_class="multinomial", max_iter=10000
-    ),
+    "L1 logistic": LogisticRegression(C=C, penalty="l1", solver="saga", max_iter=10000),
     "L2 logistic (Multinomial)": LogisticRegression(
-        C=C, penalty="l2", solver="saga", multi_class="multinomial", max_iter=10000
+        C=C, penalty="l2", solver="saga", max_iter=10000
     ),
-    "L2 logistic (OvR)": LogisticRegression(
-        C=C, penalty="l2", solver="saga", multi_class="ovr", max_iter=10000
+    "L2 logistic (OvR)": OneVsRestClassifier(
+        LogisticRegression(C=C, penalty="l2", solver="saga", max_iter=10000)
     ),
     "Linear SVC": SVC(kernel="linear", C=C, probability=True, random_state=0),
     "GPC": GaussianProcessClassifier(kernel),
 }
 
 n_classifiers = len(classifiers)
```

### Comparing `scikit-learn-1.4.2/examples/classification/plot_classifier_comparison.py` & `scikit_learn-1.5.0rc1/examples/classification/plot_classifier_comparison.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/classification/plot_digits_classification.py` & `scikit_learn-1.5.0rc1/examples/classification/plot_digits_classification.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/classification/plot_lda.py` & `scikit_learn-1.5.0rc1/examples/classification/plot_lda.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/classification/plot_lda_qda.py` & `scikit_learn-1.5.0rc1/examples/classification/plot_lda_qda.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_adjusted_for_chance_measures.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_adjusted_for_chance_measures.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_affinity_propagation.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_affinity_propagation.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 =================================================
 
 Reference:
 Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages
 Between Data Points", Science Feb. 2007
 
 """
+
 import numpy as np
 
 from sklearn import metrics
 from sklearn.cluster import AffinityPropagation
 from sklearn.datasets import make_blobs
 
 # %%
```

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_agglomerative_clustering.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_agglomerative_clustering.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_agglomerative_clustering_metrics.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_agglomerative_clustering_metrics.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_agglomerative_dendrogram.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_agglomerative_dendrogram.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_birch_vs_minibatchkmeans.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_birch_vs_minibatchkmeans.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_bisect_kmeans.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_bisect_kmeans.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 Bisecting K-Means clustering builds on top of the previous ones. As a result, it
 tends to create clusters that have a more regular large-scale structure. This
 difference can be visually observed: for all numbers of clusters, there is a
 dividing line cutting the overall data cloud in two for BisectingKMeans, which is not
 present for regular K-Means.
 
 """
+
 import matplotlib.pyplot as plt
 
 from sklearn.cluster import BisectingKMeans, KMeans
 from sklearn.datasets import make_blobs
 
 print(__doc__)
```

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_cluster_comparison.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_cluster_comparison.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_cluster_iris.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_cluster_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_coin_segmentation.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_coin_segmentation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_coin_ward_segmentation.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_coin_ward_segmentation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_color_quantization.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_color_quantization.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_dbscan.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_dbscan.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_dict_face_patches.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_dict_face_patches.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_digits_agglomeration.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_digits_agglomeration.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_digits_linkage.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_digits_linkage.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_face_compress.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_face_compress.py`

 * *Files 2% similar despite different names*

```diff
@@ -77,15 +77,14 @@
 
 n_bins = 8
 encoder = KBinsDiscretizer(
     n_bins=n_bins,
     encode="ordinal",
     strategy="uniform",
     random_state=0,
-    subsample=200_000,
 )
 compressed_raccoon_uniform = encoder.fit_transform(raccoon_face.reshape(-1, 1)).reshape(
     raccoon_face.shape
 )
 
 fig, ax = plt.subplots(ncols=2, figsize=(12, 4))
 ax[0].imshow(compressed_raccoon_uniform, cmap=plt.cm.gray)
@@ -126,15 +125,14 @@
 # find a more optimal mapping.
 
 encoder = KBinsDiscretizer(
     n_bins=n_bins,
     encode="ordinal",
     strategy="kmeans",
     random_state=0,
-    subsample=200_000,
 )
 compressed_raccoon_kmeans = encoder.fit_transform(raccoon_face.reshape(-1, 1)).reshape(
     raccoon_face.shape
 )
 
 fig, ax = plt.subplots(ncols=2, figsize=(12, 4))
 ax[0].imshow(compressed_raccoon_kmeans, cmap=plt.cm.gray)
```

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_feature_agglomeration_vs_univariate_selection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_hdbscan.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_hdbscan.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_inductive_clustering.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_inductive_clustering.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_kmeans_assumptions.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_assumptions.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_kmeans_digits.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_digits.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_kmeans_plusplus.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_plusplus.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_kmeans_silhouette_analysis.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_silhouette_analysis.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_kmeans_stability_low_dim_dense.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_kmeans_stability_low_dim_dense.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_linkage_comparison.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_linkage_comparison.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_mean_shift.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_mean_shift.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_mini_batch_kmeans.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_mini_batch_kmeans.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_optics.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_optics.py`

 * *Files 5% similar despite different names*

```diff
@@ -67,43 +67,43 @@
 ax1 = plt.subplot(G[0, :])
 ax2 = plt.subplot(G[1, 0])
 ax3 = plt.subplot(G[1, 1])
 ax4 = plt.subplot(G[1, 2])
 
 # Reachability plot
 colors = ["g.", "r.", "b.", "y.", "c."]
-for klass, color in zip(range(0, 5), colors):
+for klass, color in enumerate(colors):
     Xk = space[labels == klass]
     Rk = reachability[labels == klass]
     ax1.plot(Xk, Rk, color, alpha=0.3)
 ax1.plot(space[labels == -1], reachability[labels == -1], "k.", alpha=0.3)
 ax1.plot(space, np.full_like(space, 2.0, dtype=float), "k-", alpha=0.5)
 ax1.plot(space, np.full_like(space, 0.5, dtype=float), "k-.", alpha=0.5)
 ax1.set_ylabel("Reachability (epsilon distance)")
 ax1.set_title("Reachability Plot")
 
 # OPTICS
 colors = ["g.", "r.", "b.", "y.", "c."]
-for klass, color in zip(range(0, 5), colors):
+for klass, color in enumerate(colors):
     Xk = X[clust.labels_ == klass]
     ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
 ax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], "k+", alpha=0.1)
 ax2.set_title("Automatic Clustering\nOPTICS")
 
 # DBSCAN at 0.5
 colors = ["g.", "r.", "b.", "c."]
-for klass, color in zip(range(0, 4), colors):
+for klass, color in enumerate(colors):
     Xk = X[labels_050 == klass]
     ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
 ax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], "k+", alpha=0.1)
 ax3.set_title("Clustering at 0.5 epsilon cut\nDBSCAN")
 
 # DBSCAN at 2.
 colors = ["g.", "m.", "y.", "c."]
-for klass, color in zip(range(0, 4), colors):
+for klass, color in enumerate(colors):
     Xk = X[labels_200 == klass]
     ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
 ax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], "k+", alpha=0.1)
 ax4.set_title("Clustering at 2.0 epsilon cut\nDBSCAN")
 
 plt.tight_layout()
 plt.show()
```

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_segmentation_toy.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_segmentation_toy.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cluster/plot_ward_structured_vs_unstructured.py` & `scikit_learn-1.5.0rc1/examples/cluster/plot_ward_structured_vs_unstructured.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/compose/plot_column_transformer.py` & `scikit_learn-1.5.0rc1/examples/compose/plot_column_transformer.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/compose/plot_column_transformer_mixed_types.py` & `scikit_learn-1.5.0rc1/examples/compose/plot_column_transformer_mixed_types.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/compose/plot_compare_reduction.py` & `scikit_learn-1.5.0rc1/examples/compose/plot_compare_reduction.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/compose/plot_digits_pipe.py` & `scikit_learn-1.5.0rc1/examples/compose/plot_digits_pipe.py`

 * *Files 13% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 
 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
 # License: BSD 3 clause
 
 import matplotlib.pyplot as plt
 import numpy as np
-import pandas as pd
+import polars as pl
 
 from sklearn import datasets
 from sklearn.decomposition import PCA
 from sklearn.linear_model import LogisticRegression
 from sklearn.model_selection import GridSearchCV
 from sklearn.pipeline import Pipeline
 from sklearn.preprocessing import StandardScaler
@@ -59,19 +59,23 @@
     search.best_estimator_.named_steps["pca"].n_components,
     linestyle=":",
     label="n_components chosen",
 )
 ax0.legend(prop=dict(size=12))
 
 # For each number of components, find the best classifier results
-results = pd.DataFrame(search.cv_results_)
 components_col = "param_pca__n_components"
-best_clfs = results.groupby(components_col)[
-    [components_col, "mean_test_score", "std_test_score"]
-].apply(lambda g: g.nlargest(1, "mean_test_score"))
+is_max_test_score = pl.col("mean_test_score") == pl.col("mean_test_score").max()
+best_clfs = (
+    pl.LazyFrame(search.cv_results_)
+    .filter(is_max_test_score.over(components_col))
+    .unique(components_col)
+    .sort(components_col)
+    .collect()
+)
 ax1.errorbar(
     best_clfs[components_col],
     best_clfs["mean_test_score"],
     yerr=best_clfs["std_test_score"],
 )
 ax1.set_ylabel("Classification accuracy (val)")
 ax1.set_xlabel("n_components")
```

### Comparing `scikit-learn-1.4.2/examples/compose/plot_feature_union.py` & `scikit_learn-1.5.0rc1/examples/compose/plot_feature_union.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/compose/plot_transformed_target.py` & `scikit_learn-1.5.0rc1/examples/compose/plot_transformed_target.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/covariance/plot_covariance_estimation.py` & `scikit_learn-1.5.0rc1/examples/covariance/plot_covariance_estimation.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,14 @@
 order to reduce its variance; this, in turn, introduces some bias. This
 example illustrates the simple regularization used in
 :ref:`shrunk_covariance` estimators. In particular, it focuses on how to
 set the amount of regularization, i.e. how to choose the bias-variance
 trade-off.
 """
 
-
 # %%
 # Generate sample data
 # --------------------
 
 import numpy as np
 
 n_features, n_samples = 40, 20
```

### Comparing `scikit-learn-1.4.2/examples/covariance/plot_lw_vs_oas.py` & `scikit_learn-1.5.0rc1/examples/covariance/plot_lw_vs_oas.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/covariance/plot_mahalanobis_distances.py` & `scikit_learn-1.5.0rc1/examples/covariance/plot_mahalanobis_distances.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/covariance/plot_robust_vs_empirical_covariance.py` & `scikit_learn-1.5.0rc1/examples/covariance/plot_robust_vs_empirical_covariance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/covariance/plot_sparse_cov.py` & `scikit_learn-1.5.0rc1/examples/covariance/plot_sparse_cov.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cross_decomposition/plot_compare_cross_decomposition.py` & `scikit_learn-1.5.0rc1/examples/cross_decomposition/plot_compare_cross_decomposition.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/cross_decomposition/plot_pcr_vs_pls.py` & `scikit_learn-1.5.0rc1/examples/cross_decomposition/plot_pcr_vs_pls.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/datasets/plot_digits_last_image.py` & `scikit_learn-1.5.0rc1/examples/datasets/plot_digits_last_image.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/datasets/plot_iris_dataset.py` & `scikit_learn-1.5.0rc1/examples/datasets/plot_iris_dataset.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/datasets/plot_random_dataset.py` & `scikit_learn-1.5.0rc1/examples/datasets/plot_random_dataset.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/datasets/plot_random_multilabel_dataset.py` & `scikit_learn-1.5.0rc1/examples/datasets/plot_random_multilabel_dataset.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_faces_decomposition.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_faces_decomposition.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_ica_blind_source_separation.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_ica_blind_source_separation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_ica_vs_pca.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_ica_vs_pca.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_image_denoising.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_image_denoising.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_incremental_pca.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_incremental_pca.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_kernel_pca.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_kernel_pca.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_pca_iris.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_pca_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_pca_vs_fa_model_selection.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_pca_vs_fa_model_selection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_pca_vs_lda.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_pca_vs_lda.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_sparse_coding.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_sparse_coding.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/decomposition/plot_varimax_fa.py` & `scikit_learn-1.5.0rc1/examples/decomposition/plot_varimax_fa.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/developing_estimators/sklearn_is_fitted.py` & `scikit_learn-1.5.0rc1/examples/developing_estimators/sklearn_is_fitted.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_adaboost_multiclass.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_adaboost_multiclass.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_adaboost_regression.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_adaboost_regression.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,14 +5,18 @@
 
 A decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D
 sinusoidal dataset with a small amount of Gaussian noise.
 299 boosts (300 decision trees) is compared with a single decision tree
 regressor. As the number of boosts is increased the regressor can fit more
 detail.
 
+See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an
+example showcasing the benefits of using more efficient regression models such
+as :class:`~ensemble.HistGradientBoostingRegressor`.
+
 .. [1] `H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
         <https://citeseerx.ist.psu.edu/doc_view/pid/8d49e2dedb817f2c3330e74b63c5fc86d2399ce3>`_
 
 """
 
 # %%
 # Preparing the data
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_adaboost_twoclass.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_adaboost_twoclass.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_bias_variance.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_bias_variance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_ensemble_oob.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_ensemble_oob.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_feature_transformation.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_feature_transformation.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 leaves to 1 and the other feature values to 0.
 
 The resulting transformer has then learned a supervised, sparse,
 high-dimensional categorical embedding of the data.
 
 """
 
-
 # Author: Tim Head <betatim@gmail.com>
 #
 # License: BSD 3 clause
 
 # %%
 # First, we will create a large dataset and split it into three sets:
 #
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_hist_grad_boosting_comparison.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,15 +18,17 @@
   of trees required by the model depends on the stopping criteria.
 
 HGBT uses gradient boosting to iteratively improve the model's performance by
 fitting each tree to the negative gradient of the loss function with respect to
 the predicted value. RFs, on the other hand, are based on bagging and use a
 majority vote to predict the outcome.
 
-For more information on ensemble models, see the :ref:`User Guide <ensemble>`.
+See the :ref:`User Guide <ensemble>` for more information on ensemble models or
+see :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an
+example showcasing some other features of HGBT models.
 """
 
 # Author:  Arturo Amor <david-arturo.amor-quiroz@inria.fr>
 # License: BSD 3 clause
 
 # %%
 # Load dataset
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_forest_importances.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_importances.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_forest_importances_faces.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_importances_faces.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_forest_iris.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_forest_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_categorical.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_categorical.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,14 +17,18 @@
 - using an :class:`~preprocessing.OrdinalEncoder` and rely on the :ref:`native
   category support <categorical_support_gbdt>` of the
   :class:`~ensemble.HistGradientBoostingRegressor` estimator.
 
 We will work with the Ames Iowa Housing dataset which consists of numerical
 and categorical features, where the houses' sales prices is the target.
 
+See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an
+example showcasing some other features of
+:class:`~ensemble.HistGradientBoostingRegressor`.
+
 """
 
 # %%
 # Load Ames Housing dataset
 # -------------------------
 # First, we load the Ames Housing data as a pandas dataframe. The features
 # are either categorical or numerical:
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_early_stopping.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_early_stopping.py`

 * *Files 0% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 applied, can be accessed using the `n_estimators_` attribute. Overall, early
 stopping is a valuable tool to strike a balance between model performance and
 efficiency in gradient boosting.
 
 License: BSD 3 clause
 
 """
+
 # %%
 # Data Preparation
 # ----------------
 # First we load and prepares the California Housing Prices dataset for
 # training and evaluation. It subsets the dataset, splits it into training
 # and validation sets.
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_oob.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_oob.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_quantile.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_quantile.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """
 =====================================================
 Prediction Intervals for Gradient Boosting Regression
 =====================================================
 
 This example shows how quantile regression can be used to create prediction
-intervals.
+intervals. See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`
+for an example showcasing some other features of
+:class:`~ensemble.HistGradientBoostingRegressor`.
 
 """
 
 # %%
 # Generate some data for a synthetic regression problem by applying the
 # function f to uniformly sampled random inputs.
 import numpy as np
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_regression.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_regression.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,18 @@
 model from an ensemble of weak predictive models. Gradient boosting can be used
 for regression and classification problems. Here, we will train a model to
 tackle a diabetes regression task. We will obtain the results from
 :class:`~sklearn.ensemble.GradientBoostingRegressor` with least squares loss
 and 500 regression trees of depth 4.
 
 Note: For larger datasets (n_samples >= 10000), please refer to
-:class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
+:class:`~sklearn.ensemble.HistGradientBoostingRegressor`. See
+:ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an example
+showcasing some other advantages of
+:class:`~ensemble.HistGradientBoostingRegressor`.
 
 """
 
 # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #         Maria Telenczuk <https://github.com/maikia>
 #         Katrina Ni <https://github.com/nilichen>
 #
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_gradient_boosting_regularization.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_gradient_boosting_regularization.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_isolation_forest.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_isolation_forest.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_monotonic_constraints.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_monotonic_constraints.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,14 +15,15 @@
 on the features during the learning process, the estimator is able to properly follow
 the general trend instead of being subject to the variations.
 
 This example was inspired by the `XGBoost documentation
 <https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html>`_.
 
 """
+
 # %%
 import matplotlib.pyplot as plt
 import numpy as np
 
 from sklearn.ensemble import HistGradientBoostingRegressor
 from sklearn.inspection import PartialDependenceDisplay
```

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_random_forest_embedding.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_random_forest_embedding.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_random_forest_regression_multioutput.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_random_forest_regression_multioutput.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_stack_predictors.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_stack_predictors.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_voting_decision_regions.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_voting_decision_regions.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_voting_probas.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_voting_probas.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/ensemble/plot_voting_regressor.py` & `scikit_learn-1.5.0rc1/examples/ensemble/plot_voting_regressor.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/exercises/plot_cv_diabetes.py` & `scikit_learn-1.5.0rc1/examples/exercises/plot_cv_diabetes.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/exercises/plot_digits_classification_exercise.py` & `scikit_learn-1.5.0rc1/examples/exercises/plot_digits_classification_exercise.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/exercises/plot_iris_exercise.py` & `scikit_learn-1.5.0rc1/examples/exercises/plot_iris_exercise.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/feature_selection/plot_f_test_vs_mi.py` & `scikit_learn-1.5.0rc1/examples/feature_selection/plot_f_test_vs_mi.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/feature_selection/plot_feature_selection.py` & `scikit_learn-1.5.0rc1/examples/feature_selection/plot_feature_selection.py`

 * *Files 1% similar despite different names*

```diff
@@ -73,30 +73,28 @@
 # -----------------
 #
 # Without univariate feature selection
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import MinMaxScaler
 from sklearn.svm import LinearSVC
 
-clf = make_pipeline(MinMaxScaler(), LinearSVC(dual="auto"))
+clf = make_pipeline(MinMaxScaler(), LinearSVC())
 clf.fit(X_train, y_train)
 print(
     "Classification accuracy without selecting features: {:.3f}".format(
         clf.score(X_test, y_test)
     )
 )
 
 svm_weights = np.abs(clf[-1].coef_).sum(axis=0)
 svm_weights /= svm_weights.sum()
 
 # %%
 # After univariate feature selection
-clf_selected = make_pipeline(
-    SelectKBest(f_classif, k=4), MinMaxScaler(), LinearSVC(dual="auto")
-)
+clf_selected = make_pipeline(SelectKBest(f_classif, k=4), MinMaxScaler(), LinearSVC())
 clf_selected.fit(X_train, y_train)
 print(
     "Classification accuracy after univariate feature selection: {:.3f}".format(
         clf_selected.score(X_test, y_test)
     )
 )
```

### Comparing `scikit-learn-1.4.2/examples/feature_selection/plot_feature_selection_pipeline.py` & `scikit_learn-1.5.0rc1/examples/feature_selection/plot_feature_selection_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -42,15 +42,15 @@
 # classifier which will be trained.
 
 from sklearn.feature_selection import SelectKBest, f_classif
 from sklearn.pipeline import make_pipeline
 from sklearn.svm import LinearSVC
 
 anova_filter = SelectKBest(f_classif, k=3)
-clf = LinearSVC(dual="auto")
+clf = LinearSVC()
 anova_svm = make_pipeline(anova_filter, clf)
 anova_svm.fit(X_train, y_train)
 
 # %%
 # Once the training is complete, we can predict on new unseen samples. In this
 # case, the feature selector will only select the most discriminative features
 # based on the information stored during training. Then, the data will be
```

### Comparing `scikit-learn-1.4.2/examples/feature_selection/plot_rfe_with_cross_validation.py` & `scikit_learn-1.5.0rc1/examples/feature_selection/plot_rfe_with_cross_validation.py`

 * *Files 11% similar despite different names*

```diff
@@ -62,23 +62,24 @@
 # In the present case, the model with 3 features (which corresponds to the true
 # generative model) is found to be the most optimal.
 #
 # Plot number of features VS. cross-validation scores
 # ---------------------------------------------------
 
 import matplotlib.pyplot as plt
+import pandas as pd
 
-n_scores = len(rfecv.cv_results_["mean_test_score"])
+cv_results = pd.DataFrame(rfecv.cv_results_)
 plt.figure()
 plt.xlabel("Number of features selected")
 plt.ylabel("Mean test accuracy")
 plt.errorbar(
-    range(min_features_to_select, n_scores + min_features_to_select),
-    rfecv.cv_results_["mean_test_score"],
-    yerr=rfecv.cv_results_["std_test_score"],
+    x=cv_results["n_features"],
+    y=cv_results["mean_test_score"],
+    yerr=cv_results["std_test_score"],
 )
 plt.title("Recursive Feature Elimination \nwith correlated features")
 plt.show()
 
 # %%
 # From the plot above one can further notice a plateau of equivalent scores
 # (similar mean value and overlapping errorbars) for 3 to 5 selected features.
```

### Comparing `scikit-learn-1.4.2/examples/feature_selection/plot_select_from_model_diabetes.py` & `scikit_learn-1.5.0rc1/examples/feature_selection/plot_select_from_model_diabetes.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_compare_gpr_krr.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_compare_gpr_krr.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpc.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpc_iris.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpc_isoprobability.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc_isoprobability.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpc_xor.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpc_xor.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_co2.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_co2.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_noisy.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_noisy.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_noisy_targets.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_noisy_targets.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_on_structured_data.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_on_structured_data.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/gaussian_process/plot_gpr_prior_posterior.py` & `scikit_learn-1.5.0rc1/examples/gaussian_process/plot_gpr_prior_posterior.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/impute/plot_iterative_imputer_variants_comparison.py` & `scikit_learn-1.5.0rc1/examples/impute/plot_iterative_imputer_variants_comparison.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/impute/plot_missing_values.py` & `scikit_learn-1.5.0rc1/examples/impute/plot_missing_values.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/inspection/plot_causal_interpretation.py` & `scikit_learn-1.5.0rc1/examples/inspection/plot_causal_interpretation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/inspection/plot_linear_model_coefficient_interpretation.py` & `scikit_learn-1.5.0rc1/examples/inspection/plot_linear_model_coefficient_interpretation.py`

 * *Files 4% similar despite different names*

```diff
@@ -302,14 +302,42 @@
 # dependencies**. An increase of the AGE will induce a decrease
 # of the WAGE when all other features remain constant. On the contrary, an
 # increase of the EXPERIENCE will induce an increase of the WAGE when all
 # other features remain constant.
 # Also, AGE, EXPERIENCE and EDUCATION are the three variables that most
 # influence the model.
 #
+# Interpreting coefficients: being cautious about causality
+# ---------------------------------------------------------
+#
+# Linear models are a great tool for measuring statistical association, but we
+# should be cautious when making statements about causality, after all
+# correlation doesn't always imply causation. This is particularly difficult in
+# the social sciences because the variables we observe only function as proxies
+# for the underlying causal process.
+#
+# In our particular case we can think of the EDUCATION of an individual as a
+# proxy for their professional aptitude, the real variable we're interested in
+# but can't observe. We'd certainly like to think that staying in school for
+# longer would increase technical competency, but it's also quite possible that
+# causality goes the other way too. That is, those who are technically
+# competent tend to stay in school for longer.
+#
+# An employer is unlikely to care which case it is (or if it's a mix of both),
+# as long as they remain convinced that a person with more EDUCATION is better
+# suited for the job, they will be happy to pay out a higher WAGE.
+#
+# This confounding of effects becomes problematic when thinking about some
+# form of intervention e.g. government subsidies of university degrees or
+# promotional material encouraging individuals to take up higher education.
+# The usefulness of these measures could end up being overstated, especially if
+# the degree of confounding is strong. Our model predicts a :math:`0.054699`
+# increase in hourly wage for each year of education. The actual causal effect
+# might be lower because of this confounding.
+#
 # Checking the variability of the coefficients
 # --------------------------------------------
 #
 # We can check the coefficient variability through cross-validation:
 # it is a form of data perturbation (related to
 # `resampling <https://en.wikipedia.org/wiki/Resampling_(statistics)>`_).
 #
@@ -738,14 +766,17 @@
 #
 # Lessons learned
 # ---------------
 #
 # * Coefficients must be scaled to the same unit of measure to retrieve
 #   feature importance. Scaling them with the standard-deviation of the
 #   feature is a useful proxy.
+# * Interpreting causality is difficult when there are confounding effects. If
+#   the relationship between two variables is also affected by something
+#   unobserved, we should be careful when making conclusions about causality.
 # * Coefficients in multivariate linear models represent the dependency
 #   between a given feature and the target, **conditional** on the other
 #   features.
 # * Correlated features induce instabilities in the coefficients of linear
 #   models and their effects cannot be well teased apart.
 # * Different linear models respond differently to feature correlation and
 #   coefficients could significantly vary from one another.
```

### Comparing `scikit-learn-1.4.2/examples/inspection/plot_partial_dependence.py` & `scikit_learn-1.5.0rc1/examples/inspection/plot_partial_dependence.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/inspection/plot_permutation_importance.py` & `scikit_learn-1.5.0rc1/examples/inspection/plot_permutation_importance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/inspection/plot_permutation_importance_multicollinear.py` & `scikit_learn-1.5.0rc1/examples/inspection/plot_permutation_importance_multicollinear.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/kernel_approximation/plot_scalable_poly_kernels.py` & `scikit_learn-1.5.0rc1/examples/kernel_approximation/plot_scalable_poly_kernels.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,15 +81,15 @@
 
 import time
 
 from sklearn.svm import LinearSVC
 
 results = {}
 
-lsvm = LinearSVC(dual="auto")
+lsvm = LinearSVC()
 start = time.time()
 lsvm.fit(X_train, y_train)
 lsvm_time = time.time() - start
 lsvm_score = 100 * lsvm.score(X_test, y_test)
 
 results["LSVM"] = {"time": lsvm_time, "score": lsvm_score}
 print(f"Linear SVM score on raw features: {lsvm_score:.2f}%")
@@ -122,15 +122,15 @@
 
 for n_components in N_COMPONENTS:
     ps_lsvm_time = 0
     ps_lsvm_score = 0
     for _ in range(n_runs):
         pipeline = make_pipeline(
             PolynomialCountSketch(n_components=n_components, degree=4),
-            LinearSVC(dual="auto"),
+            LinearSVC(),
         )
 
         start = time.time()
         pipeline.fit(X_train, y_train)
         ps_lsvm_time += time.time() - start
         ps_lsvm_score += 100 * pipeline.score(X_test, y_test)
```

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ard.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ard.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_bayesian_ridge_curvefit.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_bayesian_ridge_curvefit.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_huber_vs_ridge.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_huber_vs_ridge.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_iris_logistic.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_iris_logistic.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_lasso_and_elasticnet.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_and_elasticnet.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_lasso_coordinate_descent_path.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_coordinate_descent_path.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_lasso_dense_vs_sparse_data.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_dense_vs_sparse_data.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_lasso_lars.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_lars.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_lasso_lars_ic.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_lars_ic.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_lasso_model_selection.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_lasso_model_selection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_logistic.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_logistic_l1_l2_sparsity.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic_l1_l2_sparsity.py`

 * *Files 10% similar despite different names*

```diff
@@ -57,23 +57,21 @@
     # coef_l1_LR contains zeros due to the
     # L1 sparsity inducing norm
 
     sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100
     sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100
     sparsity_en_LR = np.mean(coef_en_LR == 0) * 100
 
-    print("C=%.2f" % C)
-    print("{:<40} {:.2f}%".format("Sparsity with L1 penalty:", sparsity_l1_LR))
-    print("{:<40} {:.2f}%".format("Sparsity with Elastic-Net penalty:", sparsity_en_LR))
-    print("{:<40} {:.2f}%".format("Sparsity with L2 penalty:", sparsity_l2_LR))
-    print("{:<40} {:.2f}".format("Score with L1 penalty:", clf_l1_LR.score(X, y)))
-    print(
-        "{:<40} {:.2f}".format("Score with Elastic-Net penalty:", clf_en_LR.score(X, y))
-    )
-    print("{:<40} {:.2f}".format("Score with L2 penalty:", clf_l2_LR.score(X, y)))
+    print(f"C={C:.2f}")
+    print(f"{'Sparsity with L1 penalty:':<40} {sparsity_l1_LR:.2f}%")
+    print(f"{'Sparsity with Elastic-Net penalty:':<40} {sparsity_en_LR:.2f}%")
+    print(f"{'Sparsity with L2 penalty:':<40} {sparsity_l2_LR:.2f}%")
+    print(f"{'Score with L1 penalty:':<40} {clf_l1_LR.score(X, y):.2f}")
+    print(f"{'Score with Elastic-Net penalty:':<40} {clf_en_LR.score(X, y):.2f}")
+    print(f"{'Score with L2 penalty:':<40} {clf_l2_LR.score(X, y):.2f}")
 
     if i == 0:
         axes_row[0].set_title("L1 penalty")
         axes_row[1].set_title("Elastic-Net\nl1_ratio = %s" % l1_ratio)
         axes_row[2].set_title("L2 penalty")
 
     for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):
@@ -83,10 +81,10 @@
             cmap="binary",
             vmax=1,
             vmin=0,
         )
         ax.set_xticks(())
         ax.set_yticks(())
 
-    axes_row[0].set_ylabel("C = %s" % C)
+    axes_row[0].set_ylabel(f"C = {C}")
 
 plt.show()
```

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_logistic_multinomial.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic_multinomial.py`

 * *Files 16% similar despite different names*

```diff
@@ -14,25 +14,27 @@
 
 import matplotlib.pyplot as plt
 import numpy as np
 
 from sklearn.datasets import make_blobs
 from sklearn.inspection import DecisionBoundaryDisplay
 from sklearn.linear_model import LogisticRegression
+from sklearn.multiclass import OneVsRestClassifier
 
 # make 3-class dataset for classification
 centers = [[-5, 0], [0, 1.5], [5, -1]]
 X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)
 transformation = [[0.4, 0.2], [-0.4, 1.2]]
 X = np.dot(X, transformation)
 
 for multi_class in ("multinomial", "ovr"):
-    clf = LogisticRegression(
-        solver="sag", max_iter=100, random_state=42, multi_class=multi_class
-    ).fit(X, y)
+    clf = LogisticRegression(solver="sag", max_iter=100, random_state=42)
+    if multi_class == "ovr":
+        clf = OneVsRestClassifier(clf)
+    clf.fit(X, y)
 
     # print the training scores
     print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))
 
     _, ax = plt.subplots()
     DecisionBoundaryDisplay.from_estimator(
         clf, X, response_method="predict", cmap=plt.cm.Paired, ax=ax
@@ -47,16 +49,20 @@
         plt.scatter(
             X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor="black", s=20
         )
 
     # Plot the three one-against-all classifiers
     xmin, xmax = plt.xlim()
     ymin, ymax = plt.ylim()
-    coef = clf.coef_
-    intercept = clf.intercept_
+    if multi_class == "ovr":
+        coef = np.concatenate([est.coef_ for est in clf.estimators_])
+        intercept = np.concatenate([est.intercept_ for est in clf.estimators_])
+    else:
+        coef = clf.coef_
+        intercept = clf.intercept_
 
     def plot_hyperplane(c, color):
         def line(x0):
             return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]
 
         plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color=color)
```

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_logistic_path.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_logistic_path.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_multi_task_lasso_support.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_multi_task_lasso_support.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_nnls.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_nnls.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ols.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ols.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ols_3d.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ols_3d.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ols_ridge_variance.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ols_ridge_variance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_omp.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_omp.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_poisson_regression_non_normal_loss.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_poisson_regression_non_normal_loss.py`

 * *Files 1% similar despite different names*

```diff
@@ -108,15 +108,15 @@
 )
 
 linear_model_preprocessor = ColumnTransformer(
     [
         ("passthrough_numeric", "passthrough", ["BonusMalus"]),
         (
             "binned_numeric",
-            KBinsDiscretizer(n_bins=10, subsample=int(2e5), random_state=0),
+            KBinsDiscretizer(n_bins=10, random_state=0),
             ["VehAge", "DrivAge"],
         ),
         ("log_scaled_numeric", log_scale_transformer, ["Density"]),
         (
             "onehot_categorical",
             OneHotEncoder(),
             ["VehBrand", "VehPower", "VehGas", "Region", "Area"],
```

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_polynomial_interpolation.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_polynomial_interpolation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_quantile_regression.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_quantile_regression.py`

 * *Files 0% similar despite different names*

```diff
@@ -257,22 +257,24 @@
 
 linear_regression = LinearRegression()
 quantile_regression = QuantileRegressor(quantile=0.5, alpha=0, solver=solver)
 
 y_pred_lr = linear_regression.fit(X, y_pareto).predict(X)
 y_pred_qr = quantile_regression.fit(X, y_pareto).predict(X)
 
-print(f"""Training error (in-sample performance)
+print(
+    f"""Training error (in-sample performance)
     {linear_regression.__class__.__name__}:
     MAE = {mean_absolute_error(y_pareto, y_pred_lr):.3f}
     MSE = {mean_squared_error(y_pareto, y_pred_lr):.3f}
     {quantile_regression.__class__.__name__}:
     MAE = {mean_absolute_error(y_pareto, y_pred_qr):.3f}
     MSE = {mean_squared_error(y_pareto, y_pred_qr):.3f}
-    """)
+    """
+)
 
 # %%
 # On the training set, we see that MAE is lower for
 # :class:`~sklearn.linear_model.QuantileRegressor` than
 # :class:`~sklearn.linear_model.LinearRegression`. In contrast to that, MSE is
 # lower for :class:`~sklearn.linear_model.LinearRegression` than
 # :class:`~sklearn.linear_model.QuantileRegressor`. These results confirms that
@@ -294,18 +296,20 @@
 cv_results_qr = cross_validate(
     quantile_regression,
     X,
     y_pareto,
     cv=3,
     scoring=["neg_mean_absolute_error", "neg_mean_squared_error"],
 )
-print(f"""Test error (cross-validated performance)
+print(
+    f"""Test error (cross-validated performance)
     {linear_regression.__class__.__name__}:
     MAE = {-cv_results_lr["test_neg_mean_absolute_error"].mean():.3f}
     MSE = {-cv_results_lr["test_neg_mean_squared_error"].mean():.3f}
     {quantile_regression.__class__.__name__}:
     MAE = {-cv_results_qr["test_neg_mean_absolute_error"].mean():.3f}
     MSE = {-cv_results_qr["test_neg_mean_squared_error"].mean():.3f}
-    """)
+    """
+)
 
 # %%
 # We reach similar conclusions on the out-of-sample evaluation.
```

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ransac.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ransac.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ridge_coeffs.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ridge_coeffs.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_ridge_path.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_ridge_path.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_robust_fit.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_robust_fit.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_comparison.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_comparison.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_early_stopping.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_early_stopping.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_iris.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_loss_functions.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_loss_functions.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_penalties.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_penalties.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_separating_hyperplane.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_separating_hyperplane.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgd_weighted_samples.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgd_weighted_samples.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sgdocsvm_vs_ocsvm.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,14 +28,15 @@
 import matplotlib.pyplot as plt
 import numpy as np
 
 from sklearn.datasets import fetch_20newsgroups_vectorized
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.linear_model import LogisticRegression
 from sklearn.model_selection import train_test_split
+from sklearn.multiclass import OneVsRestClassifier
 
 warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn")
 t0 = timeit.default_timer()
 
 # We use SAGA solver
 solver = "saga"
 
@@ -72,28 +73,33 @@
 
     # Small number of epochs for fast runtime
     for this_max_iter in model_params["iters"]:
         print(
             "[model=%s, solver=%s] Number of epochs: %s"
             % (model_params["name"], solver, this_max_iter)
         )
-        lr = LogisticRegression(
+        clf = LogisticRegression(
             solver=solver,
-            multi_class=model,
             penalty="l1",
             max_iter=this_max_iter,
             random_state=42,
         )
+        if model == "ovr":
+            clf = OneVsRestClassifier(clf)
         t1 = timeit.default_timer()
-        lr.fit(X_train, y_train)
+        clf.fit(X_train, y_train)
         train_time = timeit.default_timer() - t1
 
-        y_pred = lr.predict(X_test)
+        y_pred = clf.predict(X_test)
         accuracy = np.sum(y_pred == y_test) / y_test.shape[0]
-        density = np.mean(lr.coef_ != 0, axis=1) * 100
+        if model == "ovr":
+            coef = np.concatenate([est.coef_ for est in clf.estimators_])
+        else:
+            coef = clf.coef_
+        density = np.mean(coef != 0, axis=1) * 100
         accuracies.append(accuracy)
         densities.append(density)
         times.append(train_time)
     models[model]["times"] = times
     models[model]["densities"] = densities
     models[model]["accuracies"] = accuracies
     print("Test accuracy for model %s: %.4f" % (model, accuracies[-1]))
```

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_sparse_logistic_regression_mnist.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_sparse_logistic_regression_mnist.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_theilsen.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_theilsen.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/linear_model/plot_tweedie_regression_insurance_claims.py` & `scikit_learn-1.5.0rc1/examples/linear_model/plot_tweedie_regression_insurance_claims.py`

 * *Files 0% similar despite different names*

```diff
@@ -237,15 +237,15 @@
     FunctionTransformer(func=np.log), StandardScaler()
 )
 
 column_trans = ColumnTransformer(
     [
         (
             "binned_numeric",
-            KBinsDiscretizer(n_bins=10, subsample=int(2e5), random_state=0),
+            KBinsDiscretizer(n_bins=10, random_state=0),
             ["VehAge", "DrivAge"],
         ),
         (
             "onehot_categorical",
             OneHotEncoder(),
             ["VehBrand", "VehPower", "VehGas", "Region", "Area"],
         ),
```

### Comparing `scikit-learn-1.4.2/examples/manifold/plot_compare_methods.py` & `scikit_learn-1.5.0rc1/examples/manifold/plot_compare_methods.py`

 * *Files 0% similar despite different names*

```diff
@@ -198,15 +198,15 @@
 # function that is not convex, i.e. with different initializations we can get
 # different results. Read more in the :ref:`User Guide <t_sne>`.
 
 t_sne = manifold.TSNE(
     n_components=n_components,
     perplexity=30,
     init="random",
-    n_iter=250,
+    max_iter=250,
     random_state=0,
 )
 S_t_sne = t_sne.fit_transform(S_points)
 
 plot_2d(S_t_sne, S_color, "T-distributed Stochastic  \n Neighbor Embedding")
 
 # %%
```

### Comparing `scikit-learn-1.4.2/examples/manifold/plot_lle_digits.py` & `scikit_learn-1.5.0rc1/examples/manifold/plot_lle_digits.py`

 * *Files 0% similar despite different names*

```diff
@@ -141,15 +141,15 @@
         TruncatedSVD(n_components=2),
     ),
     "Spectral embedding": SpectralEmbedding(
         n_components=2, random_state=0, eigen_solver="arpack"
     ),
     "t-SNE embedding": TSNE(
         n_components=2,
-        n_iter=500,
+        max_iter=500,
         n_iter_without_progress=150,
         n_jobs=2,
         random_state=0,
     ),
     "NCA embedding": NeighborhoodComponentsAnalysis(
         n_components=2, init="pca", random_state=0
     ),
```

### Comparing `scikit-learn-1.4.2/examples/manifold/plot_manifold_sphere.py` & `scikit_learn-1.5.0rc1/examples/manifold/plot_manifold_sphere.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/manifold/plot_mds.py` & `scikit_learn-1.5.0rc1/examples/manifold/plot_mds.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/manifold/plot_swissroll.py` & `scikit_learn-1.5.0rc1/examples/manifold/plot_swissroll.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 ===================================
 This notebook seeks to compare two popular non-linear dimensionality
 techniques, T-distributed Stochastic Neighbor Embedding (t-SNE) and
 Locally Linear Embedding (LLE), on the classic Swiss Roll dataset.
 Then, we will explore how they both deal with the addition of a hole
 in the data.
 """
+
 # %%
 # Swiss Roll
 # ---------------------------------------------------
 #
 # We start by generating the Swiss Roll dataset.
 
 import matplotlib.pyplot as plt
```

### Comparing `scikit-learn-1.4.2/examples/manifold/plot_t_sne_perplexity.py` & `scikit_learn-1.5.0rc1/examples/manifold/plot_t_sne_perplexity.py`

 * *Files 2% similar despite different names*

```diff
@@ -59,15 +59,15 @@
 
     t0 = time()
     tsne = manifold.TSNE(
         n_components=n_components,
         init="random",
         random_state=0,
         perplexity=perplexity,
-        n_iter=300,
+        max_iter=300,
     )
     Y = tsne.fit_transform(X)
     t1 = time()
     print("circles, perplexity=%d in %.2g sec" % (perplexity, t1 - t0))
     ax.set_title("Perplexity=%d" % perplexity)
     ax.scatter(Y[red, 0], Y[red, 1], c="r")
     ax.scatter(Y[green, 0], Y[green, 1], c="g")
@@ -89,15 +89,15 @@
     t0 = time()
     tsne = manifold.TSNE(
         n_components=n_components,
         init="random",
         random_state=0,
         perplexity=perplexity,
         learning_rate="auto",
-        n_iter=300,
+        max_iter=300,
     )
     Y = tsne.fit_transform(X)
     t1 = time()
     print("S-curve, perplexity=%d in %.2g sec" % (perplexity, t1 - t0))
 
     ax.set_title("Perplexity=%d" % perplexity)
     ax.scatter(Y[:, 0], Y[:, 1], c=color)
@@ -126,15 +126,15 @@
 
     t0 = time()
     tsne = manifold.TSNE(
         n_components=n_components,
         init="random",
         random_state=0,
         perplexity=perplexity,
-        n_iter=400,
+        max_iter=400,
     )
     Y = tsne.fit_transform(X)
     t1 = time()
     print("uniform grid, perplexity=%d in %.2g sec" % (perplexity, t1 - t0))
 
     ax.set_title("Perplexity=%d" % perplexity)
     ax.scatter(Y[:, 0], Y[:, 1], c=color)
```

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_anomaly_comparison.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_anomaly_comparison.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_display_object_visualization.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_display_object_visualization.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_estimator_representation.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_estimator_representation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_isotonic_regression.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_isotonic_regression.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_johnson_lindenstrauss_bound.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_kernel_approximation.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_kernel_approximation.py`

 * *Files 2% similar despite different names*

```diff
@@ -68,31 +68,31 @@
 
 # Now predict the value of the digit on the second half:
 data_test, targets_test = (data[n_samples // 2 :], digits.target[n_samples // 2 :])
 # data_test = scaler.transform(data_test)
 
 # Create a classifier: a support vector classifier
 kernel_svm = svm.SVC(gamma=0.2)
-linear_svm = svm.LinearSVC(dual="auto", random_state=42)
+linear_svm = svm.LinearSVC(random_state=42)
 
 # create pipeline from kernel approximation
 # and linear svm
 feature_map_fourier = RBFSampler(gamma=0.2, random_state=1)
 feature_map_nystroem = Nystroem(gamma=0.2, random_state=1)
 fourier_approx_svm = pipeline.Pipeline(
     [
         ("feature_map", feature_map_fourier),
-        ("svm", svm.LinearSVC(dual="auto", random_state=42)),
+        ("svm", svm.LinearSVC(random_state=42)),
     ]
 )
 
 nystroem_approx_svm = pipeline.Pipeline(
     [
         ("feature_map", feature_map_nystroem),
-        ("svm", svm.LinearSVC(dual="auto", random_state=42)),
+        ("svm", svm.LinearSVC(random_state=42)),
     ]
 )
 
 # fit and predict using linear and kernel svm:
 
 kernel_svm_time = time()
 kernel_svm.fit(data_train, targets_train)
```

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_kernel_ridge_regression.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_kernel_ridge_regression.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 prediction-time.
 
 This example illustrates both methods on an artificial dataset, which
 consists of a sinusoidal target function and strong noise added to every fifth
 datapoint.
 
 """
+
 # %%
 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
 # %%
 # Generate sample data
 # --------------------
```

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_metadata_routing.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_metadata_routing.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,28 +2,35 @@
 ================
 Metadata Routing
 ================
 
 .. currentmodule:: sklearn
 
 This document shows how you can use the :ref:`metadata routing mechanism
-<metadata_routing>` in scikit-learn to route metadata through meta-estimators
-to the estimators consuming them. To better understand the rest of the
-document, we need to introduce two concepts: routers and consumers. A router is
-an object, in most cases a meta-estimator, which forwards given data and
-metadata to other objects and estimators. A consumer, on the other hand, is an
-object which accepts and uses a certain given metadata. For instance, an
-estimator taking into account ``sample_weight`` in its :term:`fit` method is a
-consumer of ``sample_weight``. It is possible for an object to be both a router
-and a consumer. For instance, a meta-estimator may take into account
-``sample_weight`` in certain calculations, but it may also route it to the
-underlying estimator.
+<metadata_routing>` in scikit-learn to route metadata to the estimators,
+scorers, and CV splitters consuming them.
+
+To better understand the following document, we need to introduce two concepts:
+routers and consumers. A router is an object which forwards some given data and
+metadata to other objects. In most cases, a router is a :term:`meta-estimator`,
+i.e. an estimator which takes another estimator as a parameter. A function such
+as :func:`sklearn.model_selection.cross_validate` which takes an estimator as a
+parameter and forwards data and metadata, is also a router.
+
+A consumer, on the other hand, is an object which accepts and uses some given
+metadata. For instance, an estimator taking into account ``sample_weight`` in
+its :term:`fit` method is a consumer of ``sample_weight``.
+
+It is possible for an object to be both a router and a consumer. For instance,
+a meta-estimator may take into account ``sample_weight`` in certain
+calculations, but it may also route it to the underlying estimator.
 
 First a few imports and some random data for the rest of the script.
 """
+
 # %%
 
 import warnings
 from pprint import pprint
 
 import numpy as np
 
@@ -51,40 +58,39 @@
 X = rng.rand(n_samples, n_features)
 y = rng.randint(0, 2, size=n_samples)
 my_groups = rng.randint(0, 10, size=n_samples)
 my_weights = rng.rand(n_samples)
 my_other_weights = rng.rand(n_samples)
 
 # %%
-# This feature is only available if explicitly enabled:
+# Metadata routing is only available if explicitly enabled:
 set_config(enable_metadata_routing=True)
 
-# %%
-# This utility function is a dummy to check if a metadata is passed.
-
 
+# %%
+# This utility function is a dummy to check if a metadata is passed:
 def check_metadata(obj, **kwargs):
     for key, value in kwargs.items():
         if value is not None:
             print(
                 f"Received {key} of length = {len(value)} in {obj.__class__.__name__}."
             )
         else:
             print(f"{key} is None in {obj.__class__.__name__}.")
 
 
 # %%
-# A utility function to nicely print the routing information of an object
+# A utility function to nicely print the routing information of an object:
 def print_routing(obj):
     pprint(obj.get_metadata_routing()._serialize())
 
 
 # %%
-# Estimators
-# ----------
+# Consuming Estimator
+# -------------------
 # Here we demonstrate how an estimator can expose the required API to support
 # metadata routing as a consumer. Imagine a simple classifier accepting
 # ``sample_weight`` as a metadata on its ``fit`` and ``groups`` in its
 # ``predict`` method:
 
 
 class ExampleClassifier(ClassifierMixin, BaseEstimator):
@@ -112,375 +118,433 @@
 #
 # By default, no metadata is requested, which we can see as:
 
 print_routing(ExampleClassifier())
 
 # %%
 # The above output means that ``sample_weight`` and ``groups`` are not
-# requested, but if a router is given those metadata, it should raise an error,
-# since the user has not explicitly set whether they are required or not. The
-# same is true for ``sample_weight`` in the ``score`` method, which is
-# inherited from :class:`~base.ClassifierMixin`. In order to explicitly set
-# request values for those metadata, we can use these methods:
+# requested by `ExampleClassifier`, and if a router is given those metadata, it
+# should raise an error, since the user has not explicitly set whether they are
+# required or not. The same is true for ``sample_weight`` in the ``score``
+# method, which is inherited from :class:`~base.ClassifierMixin`. In order to
+# explicitly set request values for those metadata, we can use these methods:
 
 est = (
     ExampleClassifier()
     .set_fit_request(sample_weight=False)
     .set_predict_request(groups=True)
     .set_score_request(sample_weight=False)
 )
 print_routing(est)
 
 # %%
 # .. note ::
-#     Please note that as long as the above estimator is not used in another
+#     Please note that as long as the above estimator is not used in a
 #     meta-estimator, the user does not need to set any requests for the
 #     metadata and the set values are ignored, since a consumer does not
 #     validate or route given metadata. A simple usage of the above estimator
 #     would work as expected.
 
 est = ExampleClassifier()
 est.fit(X, y, sample_weight=my_weights)
 est.predict(X[:3, :], groups=my_groups)
 
 # %%
-# Now let's have a meta-estimator, which doesn't do much other than routing the
-# metadata.
+# Routing Meta-Estimator
+# ----------------------
+# Now, we show how to design a meta-estimator to be a router. As a simplified
+# example, here is a meta-estimator, which doesn't do much other than routing
+# the metadata.
 
 
 class MetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):
     def __init__(self, estimator):
         self.estimator = estimator
 
     def get_metadata_routing(self):
         # This method defines the routing for this meta-estimator.
         # In order to do so, a `MetadataRouter` instance is created, and the
-        # right routing is added to it. More explanations follow.
+        # routing is added to it. More explanations follow below.
         router = MetadataRouter(owner=self.__class__.__name__).add(
-            estimator=self.estimator, method_mapping="one-to-one"
+            estimator=self.estimator,
+            method_mapping=MethodMapping()
+            .add(caller="fit", callee="fit")
+            .add(caller="predict", callee="predict")
+            .add(caller="score", callee="score"),
         )
         return router
 
     def fit(self, X, y, **fit_params):
-        # meta-estimators are responsible for validating the given metadata.
-        # `get_routing_for_object` is a safe way to construct a
-        # `MetadataRouter` or a `MetadataRequest` from the given object.
+        # `get_routing_for_object` returns a copy of the `MetadataRouter`
+        # constructed by the above `get_metadata_routing` method, that is
+        # internally called.
         request_router = get_routing_for_object(self)
+        # Meta-estimators are responsible for validating the given metadata.
+        # `method` refers to the parent's method, i.e. `fit` in this example.
         request_router.validate_metadata(params=fit_params, method="fit")
-        # we can use provided utility methods to map the given metadata to what
-        # is required by the underlying estimator. Here `method` refers to the
-        # parent's method, i.e. `fit` in this example.
+        # `MetadataRouter.route_params` maps the given metadata to the metadata
+        # required by the underlying estimator based on the routing information
+        # defined by the MetadataRouter. The output of type `Bunch` has a key
+        # for each consuming object and those hold keys for their consuming
+        # methods, which then contain key for the metadata which should be
+        # routed to them.
         routed_params = request_router.route_params(params=fit_params, caller="fit")
 
-        # the output has a key for each object's method which is used here,
-        # i.e. parent's `fit` method, containing the metadata which should be
-        # routed to them, based on the information provided in
-        # `get_metadata_routing`.
+        # A sub-estimator is fitted and its classes are attributed to the
+        # meta-estimator.
         self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)
         self.classes_ = self.estimator_.classes_
         return self
 
     def predict(self, X, **predict_params):
         check_is_fitted(self)
-        # same as in `fit`, we validate the given metadata
+        # As in `fit`, we get a copy of the object's MetadataRouter,
         request_router = get_routing_for_object(self)
+        # then we validate the given metadata,
         request_router.validate_metadata(params=predict_params, method="predict")
         # and then prepare the input to the underlying `predict` method.
         routed_params = request_router.route_params(
             params=predict_params, caller="predict"
         )
         return self.estimator_.predict(X, **routed_params.estimator.predict)
 
 
 # %%
 # Let's break down different parts of the above code.
 #
-# First, the :meth:`~utils.metadata_routing.get_routing_for_object` takes an
-# estimator (``self``) and returns a
-# :class:`~utils.metadata_routing.MetadataRouter` or a
-# :class:`~utils.metadata_routing.MetadataRequest` based on the output of the
-# estimator's ``get_metadata_routing`` method.
+# First, the :meth:`~utils.metadata_routing.get_routing_for_object` takes our
+# meta-estimator (``self``) and returns a
+# :class:`~utils.metadata_routing.MetadataRouter` or, a
+# :class:`~utils.metadata_routing.MetadataRequest` if the object is a consumer,
+# based on the output of the estimator's ``get_metadata_routing`` method.
 #
 # Then in each method, we use the ``route_params`` method to construct a
 # dictionary of the form ``{"object_name": {"method_name": {"metadata":
 # value}}}`` to pass to the underlying estimator's method. The ``object_name``
 # (``estimator`` in the above ``routed_params.estimator.fit`` example) is the
 # same as the one added in the ``get_metadata_routing``. ``validate_metadata``
-# makes sure all given metadata are requested to avoid silent bugs. Now, we
-# illustrate the different behaviors and notably the type of errors raised:
+# makes sure all given metadata are requested to avoid silent bugs.
+#
+# Next, we illustrate the different behaviors and notably the type of errors
+# raised.
 
-est = MetaClassifier(estimator=ExampleClassifier().set_fit_request(sample_weight=True))
-est.fit(X, y, sample_weight=my_weights)
+meta_est = MetaClassifier(
+    estimator=ExampleClassifier().set_fit_request(sample_weight=True)
+)
+meta_est.fit(X, y, sample_weight=my_weights)
 
 # %%
-# Note that the above example checks that ``sample_weight`` is correctly passed
-# to ``ExampleClassifier``, or else it would print that ``sample_weight`` is
-# ``None``:
+# Note that the above example is calling our utility function
+# `check_metadata()` via the `ExampleClassifier`. It checks that
+# ``sample_weight`` is correctly passed to it. If it is not, like in the
+# following example, it would print that ``sample_weight`` is ``None``:
 
-est.fit(X, y)
+meta_est.fit(X, y)
 
 # %%
 # If we pass an unknown metadata, an error is raised:
 try:
-    est.fit(X, y, test=my_weights)
+    meta_est.fit(X, y, test=my_weights)
 except TypeError as e:
     print(e)
 
 # %%
 # And if we pass a metadata which is not explicitly requested:
 try:
-    est.fit(X, y, sample_weight=my_weights).predict(X, groups=my_groups)
+    meta_est.fit(X, y, sample_weight=my_weights).predict(X, groups=my_groups)
 except ValueError as e:
     print(e)
 
 # %%
 # Also, if we explicitly set it as not requested, but it is provided:
-est = MetaClassifier(
+meta_est = MetaClassifier(
     estimator=ExampleClassifier()
     .set_fit_request(sample_weight=True)
     .set_predict_request(groups=False)
 )
 try:
-    est.fit(X, y, sample_weight=my_weights).predict(X[:3, :], groups=my_groups)
+    meta_est.fit(X, y, sample_weight=my_weights).predict(X[:3, :], groups=my_groups)
 except TypeError as e:
     print(e)
 
 # %%
-# Another concept to introduce is **aliased metadata**. This is when an estimator
-# requests a metadata with a different name than the default value. For
-# instance, in a setting where there are two estimators in a pipeline, one
-# could request ``sample_weight1`` and the other ``sample_weight2``. Note that
-# this doesn't change what the estimator expects, it only tells the
-# meta-estimator how to map the provided metadata to what's required. Here's an
-# example, where we pass ``aliased_sample_weight`` to the meta-estimator, but
-# the meta-estimator understands that ``aliased_sample_weight`` is an alias for
-# ``sample_weight``, and passes it as ``sample_weight`` to the underlying
-# estimator:
-est = MetaClassifier(
+# Another concept to introduce is **aliased metadata**. This is when an
+# estimator requests a metadata with a different variable name than the default
+# variable name. For instance, in a setting where there are two estimators in a
+# pipeline, one could request ``sample_weight1`` and the other
+# ``sample_weight2``. Note that this doesn't change what the estimator expects,
+# it only tells the meta-estimator how to map the provided metadata to what is
+# required. Here's an example, where we pass ``aliased_sample_weight`` to the
+# meta-estimator, but the meta-estimator understands that
+# ``aliased_sample_weight`` is an alias for ``sample_weight``, and passes it as
+# ``sample_weight`` to the underlying estimator:
+meta_est = MetaClassifier(
     estimator=ExampleClassifier().set_fit_request(sample_weight="aliased_sample_weight")
 )
-est.fit(X, y, aliased_sample_weight=my_weights)
+meta_est.fit(X, y, aliased_sample_weight=my_weights)
 
 # %%
-# And passing ``sample_weight`` here will fail since it is requested with an
+# Passing ``sample_weight`` here will fail since it is requested with an
 # alias and ``sample_weight`` with that name is not requested:
 try:
-    est.fit(X, y, sample_weight=my_weights)
+    meta_est.fit(X, y, sample_weight=my_weights)
 except TypeError as e:
     print(e)
 
 # %%
 # This leads us to the ``get_metadata_routing``. The way routing works in
 # scikit-learn is that consumers request what they need, and routers pass that
 # along. Additionally, a router exposes what it requires itself so that it can
 # be used inside another router, e.g. a pipeline inside a grid search object.
 # The output of the ``get_metadata_routing`` which is a dictionary
 # representation of a :class:`~utils.metadata_routing.MetadataRouter`, includes
 # the complete tree of requested metadata by all nested objects and their
 # corresponding method routings, i.e. which method of a sub-estimator is used
 # in which method of a meta-estimator:
 
-print_routing(est)
+print_routing(meta_est)
 
 # %%
 # As you can see, the only metadata requested for method ``fit`` is
 # ``"sample_weight"`` with ``"aliased_sample_weight"`` as the alias. The
 # ``~utils.metadata_routing.MetadataRouter`` class enables us to easily create
 # the routing object which would create the output we need for our
-# ``get_metadata_routing``. In the above implementation,
-# ``mapping="one-to-one"`` means there is a one to one mapping between
-# sub-estimator's methods and meta-estimator's ones, i.e. ``fit`` used in
-# ``fit`` and so on. In order to understand how aliases work in
-# meta-estimators, imagine our meta-estimator inside another one:
+# ``get_metadata_routing``.
+#
+# In order to understand how aliases work in meta-estimators, imagine our
+# meta-estimator inside another one:
 
-meta_est = MetaClassifier(estimator=est).fit(X, y, aliased_sample_weight=my_weights)
+meta_meta_est = MetaClassifier(estimator=meta_est).fit(
+    X, y, aliased_sample_weight=my_weights
+)
 
 # %%
-# In the above example, this is how each ``fit`` method will call the
-# sub-estimator's ``fit``::
+# In the above example, this is how the ``fit`` method of `meta_meta_est`
+# will call their sub-estimator's ``fit`` methods::
 #
-#     meta_est.fit(X, y, aliased_sample_weight=my_weights):
-#         ...  # this estimator (est), expects aliased_sample_weight as seen above
+#     # user feeds `my_weights` as `aliased_sample_weight` into `meta_meta_est`:
+#     meta_meta_est.fit(X, y, aliased_sample_weight=my_weights):
+#         ...
+#
+#         # the first sub-estimator (`meta_est`) expects `aliased_sample_weight`
 #         self.estimator_.fit(X, y, aliased_sample_weight=aliased_sample_weight):
-#             ...  # now est passes aliased_sample_weight's value as sample_weight,
-#                  # which is expected by the sub-estimator
-#             self.estimator_.fit(X, y, sample_weight=aliased_sample_weight)
-#    ...
+#             ...
+#
+#             # the second sub-estimator (`est`) expects `sample_weight`
+#             self.estimator_.fit(X, y, sample_weight=aliased_sample_weight):
+#                 ...
 
 # %%
-# Router and Consumer
-# -------------------
-# To show how a slightly more complex case would work, consider a case
-# where a meta-estimator uses some metadata, but it also routes them to an
-# underlying estimator. In this case, this meta-estimator is a consumer and a
-# router at the same time. This is how we can implement one, and it is very
-# similar to what we had before, with a few tweaks.
+# Consuming and routing Meta-Estimator
+# ------------------------------------
+# For a slightly more complex example, consider a meta-estimator that routes
+# metadata to an underlying estimator as before, but it also uses some metadata
+# in its own methods. This meta-estimator is a consumer and a router at the
+# same time. Implementing one is very similar to what we had before, but with a
+# few tweaks.
 
 
 class RouterConsumerClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):
     def __init__(self, estimator):
         self.estimator = estimator
 
     def get_metadata_routing(self):
         router = (
             MetadataRouter(owner=self.__class__.__name__)
+            # defining metadata routing request values for usage in the meta-estimator
             .add_self_request(self)
-            .add(estimator=self.estimator, method_mapping="one-to-one")
+            # defining metadata routing request values for usage in the sub-estimator
+            .add(
+                estimator=self.estimator,
+                method_mapping=MethodMapping()
+                .add(caller="fit", callee="fit")
+                .add(caller="predict", callee="predict")
+                .add(caller="score", callee="score"),
+            )
         )
         return router
 
+    # Since `sample_weight` is used and consumed here, it should be defined as
+    # an explicit argument in the method's signature. All other metadata which
+    # are only routed, will be passed as `**fit_params`:
     def fit(self, X, y, sample_weight, **fit_params):
         if self.estimator is None:
             raise ValueError("estimator cannot be None!")
 
         check_metadata(self, sample_weight=sample_weight)
 
+        # We add `sample_weight` to the `fit_params` dictionary.
         if sample_weight is not None:
             fit_params["sample_weight"] = sample_weight
 
-        # meta-estimators are responsible for validating the given metadata
         request_router = get_routing_for_object(self)
         request_router.validate_metadata(params=fit_params, method="fit")
-        # we can use provided utility methods to map the given metadata to what
-        # is required by the underlying estimator
-        params = request_router.route_params(params=fit_params, caller="fit")
-        self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)
+        routed_params = request_router.route_params(params=fit_params, caller="fit")
+        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)
         self.classes_ = self.estimator_.classes_
         return self
 
     def predict(self, X, **predict_params):
         check_is_fitted(self)
-        # same as in ``fit``, we validate the given metadata
+        # As in `fit`, we get a copy of the object's MetadataRouter,
         request_router = get_routing_for_object(self)
+        # we validate the given metadata,
         request_router.validate_metadata(params=predict_params, method="predict")
         # and then prepare the input to the underlying ``predict`` method.
-        params = request_router.route_params(params=predict_params, caller="predict")
-        return self.estimator_.predict(X, **params.estimator.predict)
+        routed_params = request_router.route_params(
+            params=predict_params, caller="predict"
+        )
+        return self.estimator_.predict(X, **routed_params.estimator.predict)
 
 
 # %%
-# The key parts where the above estimator differs from our previous
+# The key parts where the above meta-estimator differs from our previous
 # meta-estimator is accepting ``sample_weight`` explicitly in ``fit`` and
-# including it in ``fit_params``. Making ``sample_weight`` an explicit argument
-# makes sure ``set_fit_request(sample_weight=...)`` is present for this class.
-# In a sense, this means the estimator is both a consumer, as well as a router
-# of ``sample_weight``.
+# including it in ``fit_params``. Since ``sample_weight`` is an explicit
+# argument, we can be sure that ``set_fit_request(sample_weight=...)`` is
+# present for this method. The meta-estimator is both a consumer, as well as a
+# router of ``sample_weight``.
 #
 # In ``get_metadata_routing``, we add ``self`` to the routing using
 # ``add_self_request`` to indicate this estimator is consuming
 # ``sample_weight`` as well as being a router; which also adds a
 # ``$self_request`` key to the routing info as illustrated below. Now let's
 # look at some examples:
 
 # %%
 # - No metadata requested
-est = RouterConsumerClassifier(estimator=ExampleClassifier())
-print_routing(est)
+meta_est = RouterConsumerClassifier(estimator=ExampleClassifier())
+print_routing(meta_est)
 
 
 # %%
-# - ``sample_weight`` requested by underlying estimator
-est = RouterConsumerClassifier(
+# - ``sample_weight`` requested by sub-estimator
+meta_est = RouterConsumerClassifier(
     estimator=ExampleClassifier().set_fit_request(sample_weight=True)
 )
-print_routing(est)
+print_routing(meta_est)
 
 # %%
 # - ``sample_weight`` requested by meta-estimator
-est = RouterConsumerClassifier(estimator=ExampleClassifier()).set_fit_request(
+meta_est = RouterConsumerClassifier(estimator=ExampleClassifier()).set_fit_request(
     sample_weight=True
 )
-print_routing(est)
+print_routing(meta_est)
 
 # %%
 # Note the difference in the requested metadata representations above.
 #
-# - We can also alias the metadata to pass different values to them:
+# - We can also alias the metadata to pass different values to the fit methods
+#   of the meta- and the sub-estimator:
 
-est = RouterConsumerClassifier(
+meta_est = RouterConsumerClassifier(
     estimator=ExampleClassifier().set_fit_request(sample_weight="clf_sample_weight"),
 ).set_fit_request(sample_weight="meta_clf_sample_weight")
-print_routing(est)
+print_routing(meta_est)
 
 # %%
 # However, ``fit`` of the meta-estimator only needs the alias for the
-# sub-estimator, since it doesn't validate and route its own required metadata:
-est.fit(X, y, sample_weight=my_weights, clf_sample_weight=my_other_weights)
+# sub-estimator and addresses their own sample weight as `sample_weight`, since
+# it doesn't validate and route its own required metadata:
+meta_est.fit(X, y, sample_weight=my_weights, clf_sample_weight=my_other_weights)
 
 # %%
-# - Alias only on the sub-estimator. This is useful if we don't want the
-#   meta-estimator to use the metadata, and we only want the metadata to be used
-#   by the sub-estimator.
-est = RouterConsumerClassifier(
+# - Alias only on the sub-estimator:
+#
+# This is useful when we don't want the meta-estimator to use the metadata, but
+# the sub-estimator should.
+meta_est = RouterConsumerClassifier(
     estimator=ExampleClassifier().set_fit_request(sample_weight="aliased_sample_weight")
-).set_fit_request(sample_weight=True)
-print_routing(est)
-
+)
+print_routing(meta_est)
+# %%
+# The meta-estimator cannot use `aliased_sample_weight`, because it expects
+# it passed as `sample_weight`. This would apply even if
+# `set_fit_request(sample_weight=True)` was set on it.
 
 # %%
 # Simple Pipeline
 # ---------------
-# A slightly more complicated use-case is a meta-estimator which does something
-# similar to the :class:`~pipeline.Pipeline`. Here is a meta-estimator, which
-# accepts a transformer and a classifier, and applies the transformer before
-# running the classifier.
+# A slightly more complicated use-case is a meta-estimator resembling a
+# :class:`~pipeline.Pipeline`. Here is a meta-estimator, which accepts a
+# transformer and a classifier. When calling its `fit` method, it applies the
+# transformer's `fit` and `transform` before running the classifier on the
+# transformed data. Upon `predict`, it applies the transformer's `transform`
+# before predicting with the classifier's `predict` method on the transformed
+# new data.
 
 
 class SimplePipeline(ClassifierMixin, BaseEstimator):
-    _required_parameters = ["estimator"]
-
     def __init__(self, transformer, classifier):
         self.transformer = transformer
         self.classifier = classifier
 
     def get_metadata_routing(self):
         router = (
             MetadataRouter(owner=self.__class__.__name__)
+            # We add the routing for the transformer.
             .add(
                 transformer=self.transformer,
                 method_mapping=MethodMapping()
-                .add(callee="fit", caller="fit")
-                .add(callee="transform", caller="fit")
-                .add(callee="transform", caller="predict"),
+                # The metadata is routed such that it retraces how
+                # `SimplePipeline` internally calls the transformer's `fit` and
+                # `transform` methods in its own methods (`fit` and `predict`).
+                .add(caller="fit", callee="fit")
+                .add(caller="fit", callee="transform")
+                .add(caller="predict", callee="transform"),
+            )
+            # We add the routing for the classifier.
+            .add(
+                classifier=self.classifier,
+                method_mapping=MethodMapping()
+                .add(caller="fit", callee="fit")
+                .add(caller="predict", callee="predict"),
             )
-            .add(classifier=self.classifier, method_mapping="one-to-one")
         )
         return router
 
     def fit(self, X, y, **fit_params):
-        params = process_routing(self, "fit", **fit_params)
+        routed_params = process_routing(self, "fit", **fit_params)
 
-        self.transformer_ = clone(self.transformer).fit(X, y, **params.transformer.fit)
-        X_transformed = self.transformer_.transform(X, **params.transformer.transform)
+        self.transformer_ = clone(self.transformer).fit(
+            X, y, **routed_params.transformer.fit
+        )
+        X_transformed = self.transformer_.transform(
+            X, **routed_params.transformer.transform
+        )
 
         self.classifier_ = clone(self.classifier).fit(
-            X_transformed, y, **params.classifier.fit
+            X_transformed, y, **routed_params.classifier.fit
         )
         return self
 
     def predict(self, X, **predict_params):
-        params = process_routing(self, "predict", **predict_params)
+        routed_params = process_routing(self, "predict", **predict_params)
 
-        X_transformed = self.transformer_.transform(X, **params.transformer.transform)
-        return self.classifier_.predict(X_transformed, **params.classifier.predict)
+        X_transformed = self.transformer_.transform(
+            X, **routed_params.transformer.transform
+        )
+        return self.classifier_.predict(
+            X_transformed, **routed_params.classifier.predict
+        )
 
 
 # %%
-# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to declare
-# which methods of the child estimator (callee) are used in which methods of
-# the meta estimator (caller). As you can see, we use the transformer's
-# ``transform`` and ``fit`` methods in ``fit``, and its ``transform`` method in
-# ``predict``, and that's what you see implemented in the routing structure of
-# the pipeline class.
+# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to
+# declare which methods of the child estimator (callee) are used in which
+# methods of the meta estimator (caller). As you can see, `SimplePipeline` uses
+# the transformer's ``transform`` and ``fit`` methods in ``fit``, and its
+# ``transform`` method in ``predict``, and that's what you see implemented in
+# the routing structure of the pipeline class.
 #
 # Another difference in the above example with the previous ones is the usage
 # of :func:`~utils.metadata_routing.process_routing`, which processes the input
-# parameters, does the required validation, and returns the `params` which we
-# had created in previous examples. This reduces the boilerplate code a
-# developer needs to write in each meta-estimator's method. Developers are
+# parameters, does the required validation, and returns the `routed_params`
+# which we had created in previous examples. This reduces the boilerplate code
+# a developer needs to write in each meta-estimator's method. Developers are
 # strongly recommended to use this function unless there is a good reason
 # against it.
 #
 # In order to test the above pipeline, let's add an example transformer.
 
 
 class ExampleTransformer(TransformerMixin, BaseEstimator):
@@ -500,35 +564,34 @@
 # Note that in the above example, we have implemented ``fit_transform`` which
 # calls ``fit`` and ``transform`` with the appropriate metadata. This is only
 # required if ``transform`` accepts metadata, since the default ``fit_transform``
 # implementation in :class:`~base.TransformerMixin` doesn't pass metadata to
 # ``transform``.
 #
 # Now we can test our pipeline, and see if metadata is correctly passed around.
-# This example uses our simple pipeline, and our transformer, and our
-# consumer+router estimator which uses our simple classifier.
+# This example uses our `SimplePipeline`, our `ExampleTransformer`, and our
+# `RouterConsumerClassifier` which uses our `ExampleClassifier`.
 
-est = SimplePipeline(
+pipe = SimplePipeline(
     transformer=ExampleTransformer()
-    # we transformer's fit to receive sample_weight
+    # we set transformer's fit to receive sample_weight
     .set_fit_request(sample_weight=True)
-    # we want transformer's transform to receive groups
+    # we set transformer's transform to receive groups
     .set_transform_request(groups=True),
     classifier=RouterConsumerClassifier(
         estimator=ExampleClassifier()
         # we want this sub-estimator to receive sample_weight in fit
         .set_fit_request(sample_weight=True)
         # but not groups in predict
         .set_predict_request(groups=False),
-    ).set_fit_request(
-        # and we want the meta-estimator to receive sample_weight as well
-        sample_weight=True
-    ),
+    )
+    # and we want the meta-estimator to receive sample_weight as well
+    .set_fit_request(sample_weight=True),
 )
-est.fit(X, y, sample_weight=my_weights, groups=my_groups).predict(
+pipe.fit(X, y, sample_weight=my_weights, groups=my_groups).predict(
     X[:3], groups=my_groups
 )
 
 # %%
 # Deprecation / Default Value Change
 # ----------------------------------
 # In this section we show how one should handle the case where a router becomes
@@ -539,71 +602,80 @@
 
 
 class MetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):
     def __init__(self, estimator):
         self.estimator = estimator
 
     def fit(self, X, y, **fit_params):
-        params = process_routing(self, "fit", **fit_params)
-        self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)
+        routed_params = process_routing(self, "fit", **fit_params)
+        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)
 
     def get_metadata_routing(self):
         router = MetadataRouter(owner=self.__class__.__name__).add(
-            estimator=self.estimator, method_mapping="one-to-one"
+            estimator=self.estimator,
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
         return router
 
 
 # %%
-# As explained above, this is now a valid usage:
+# As explained above, this is a valid usage if `my_weights` aren't supposed
+# to be passed as `sample_weight` to `MetaRegressor`:
 
 reg = MetaRegressor(estimator=LinearRegression().set_fit_request(sample_weight=True))
 reg.fit(X, y, sample_weight=my_weights)
 
 
 # %%
 # Now imagine we further develop ``MetaRegressor`` and it now also *consumes*
 # ``sample_weight``:
 
 
 class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):
+    # show warning to remind user to explicitly set the value with
+    # `.set_{method}_request(sample_weight={boolean})`
     __metadata_request__fit = {"sample_weight": metadata_routing.WARN}
 
     def __init__(self, estimator):
         self.estimator = estimator
 
     def fit(self, X, y, sample_weight=None, **fit_params):
-        params = process_routing(self, "fit", sample_weight=sample_weight, **fit_params)
+        routed_params = process_routing(
+            self, "fit", sample_weight=sample_weight, **fit_params
+        )
         check_metadata(self, sample_weight=sample_weight)
-        self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)
+        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)
 
     def get_metadata_routing(self):
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
-            .add(estimator=self.estimator, method_mapping="one-to-one")
+            .add(
+                estimator=self.estimator,
+                method_mapping=MethodMapping().add(caller="fit", callee="fit"),
+            )
         )
         return router
 
 
 # %%
-# The above implementation is almost no different than ``MetaRegressor``, and
+# The above implementation is almost the same as ``MetaRegressor``, and
 # because of the default request value defined in ``__metadata_request__fit``
-# there is a warning raised.
+# there is a warning raised when fitted.
 
 with warnings.catch_warnings(record=True) as record:
     WeightedMetaRegressor(
         estimator=LinearRegression().set_fit_request(sample_weight=False)
     ).fit(X, y, sample_weight=my_weights)
 for w in record:
     print(w.message)
 
 
 # %%
-# When an estimator supports a metadata which wasn't supported before, the
+# When an estimator consumes a metadata which it didn't consume before, the
 # following pattern can be used to warn the users about it.
 
 
 class ExampleRegressor(RegressorMixin, BaseEstimator):
     __metadata_request__fit = {"sample_weight": metadata_routing.WARN}
 
     def fit(self, X, y, sample_weight=None):
@@ -616,24 +688,29 @@
 
 with warnings.catch_warnings(record=True) as record:
     MetaRegressor(estimator=ExampleRegressor()).fit(X, y, sample_weight=my_weights)
 for w in record:
     print(w.message)
 
 # %%
+# At the end we disable the configuration flag for metadata routing:
+
+set_config(enable_metadata_routing=False)
+
+# %%
 # Third Party Development and scikit-learn Dependency
 # ---------------------------------------------------
 #
 # As seen above, information is communicated between classes using
 # :class:`~utils.metadata_routing.MetadataRequest` and
 # :class:`~utils.metadata_routing.MetadataRouter`. It is strongly not advised,
 # but possible to vendor the tools related to metadata-routing if you strictly
 # want to have a scikit-learn compatible estimator, without depending on the
-# scikit-learn package. If the following conditions are met, you do NOT need to
-# modify your code at all:
+# scikit-learn package. If all of the following conditions are met, you do NOT
+# need to modify your code at all:
 #
 # - your estimator inherits from :class:`~base.BaseEstimator`
 # - the parameters consumed by your estimator's methods, e.g. ``fit``, are
 #   explicitly defined in the method's signature, as opposed to being
 #   ``*args`` or ``*kwargs``.
-# - you do not route any metadata to the underlying objects, i.e. you're not a
-#   *router*.
+# - your estimator does not route any metadata to the underlying objects, i.e.
+#   it's not a *router*.
```

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_multilabel.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_multilabel.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_multioutput_face_completion.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_multioutput_face_completion.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_outlier_detection_bench.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_outlier_detection_bench.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,17 +2,19 @@
 ==========================================
 Evaluation of outlier detection estimators
 ==========================================
 
 This example compares two outlier detection algorithms, namely
 :ref:`local_outlier_factor` (LOF) and :ref:`isolation_forest` (IForest), on
 real-world datasets available in :class:`sklearn.datasets`. The goal is to show
-that different algorithms perform well on different datasets.
+that different algorithms perform well on different datasets and contrast their
+training speed and sensitivity to hyperparameters.
 
-The algorithms are trained in an outlier detection context:
+The algorithms are trained (without labels) on the whole dataset assumed to
+contain outliers.
 
 1. The ROC curves are computed using knowledge of the ground-truth labels
 and displayed using :class:`~sklearn.metrics.RocCurveDisplay`.
 
 2. The performance is assessed in terms of the ROC-AUC.
 """
 
@@ -310,14 +312,20 @@
 
 # %%
 # We observe that once the number of neighbors is tuned, LOF and IForest perform
 # similarly in terms of ROC AUC for the forestcover and cardiotocography
 # datasets. The score for IForest is slightly better for the SA dataset and LOF
 # performs considerably better on the Ames housing dataset than IForest.
 #
+# Recall however that Isolation Forest tends to train much faster than LOF on
+# datasets with a large number of samples. LOF needs to compute pairwise
+# distances to find nearest neighbors, which has a quadratic complexity with respect
+# to the number of observations. This can make this method prohibitive on large
+# datasets.
+#
 # Ablation study
 # ==============
 #
 # In this section we explore the impact of the hyperparameter `n_neighbors` and
 # the choice of scaling the numerical variables on the LOF model. Here we use
 # the :ref:`covtype_dataset` dataset as the binary encoded categories introduce
 # a natural scale of euclidean distances between 0 and 1. We then want a scaling
```

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_partial_dependence_visualization_api.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_partial_dependence_visualization_api.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_pipeline_display.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_pipeline_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_roc_curve_visualization_api.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_roc_curve_visualization_api.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/miscellaneous/plot_set_output.py` & `scikit_learn-1.5.0rc1/examples/miscellaneous/plot_set_output.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_concentration_prior.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_concentration_prior.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_gmm.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_gmm.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_gmm_covariances.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_covariances.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_gmm_init.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_init.py`

 * *Files 0% similar despite different names*

```diff
@@ -29,15 +29,14 @@
 alternative methods take less time to initialize when compared to *kmeans*.
 
 In this example, when initialized with *random_from_data* or *random* the model takes
 more iterations to converge. Here *k-means++* does a good job of both low
 time to initialize and low number of GaussianMixture iterations to converge.
 """
 
-
 # Author: Gordon Walsh <gordon.p.walsh@gmail.com>
 # Data generation code from Jake Vanderplas <vanderplas@astro.washington.edu>
 
 from timeit import default_timer as timer
 
 import matplotlib.pyplot as plt
 import numpy as np
```

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_gmm_pdf.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_pdf.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_gmm_selection.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_selection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/mixture/plot_gmm_sin.py` & `scikit_learn-1.5.0rc1/examples/mixture/plot_gmm_sin.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_confusion_matrix.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_confusion_matrix.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_cv_indices.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_cv_indices.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_cv_predict.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_cv_predict.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_det.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_det.py`

 * *Files 2% similar despite different names*

```diff
@@ -62,15 +62,15 @@
 # available in scikit-learn.
 
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.pipeline import make_pipeline
 from sklearn.svm import LinearSVC
 
 classifiers = {
-    "Linear SVM": make_pipeline(StandardScaler(), LinearSVC(C=0.025, dual="auto")),
+    "Linear SVM": make_pipeline(StandardScaler(), LinearSVC(C=0.025)),
     "Random Forest": RandomForestClassifier(
         max_depth=5, n_estimators=10, max_features=1
     ),
 }
 
 # %%
 # Plot ROC and DET curves
```

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_grid_search_digits.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_digits.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_grid_search_refit_callable.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_refit_callable.py`

 * *Files 1% similar despite different names*

```diff
@@ -77,15 +77,15 @@
     ]
     return best_idx
 
 
 pipe = Pipeline(
     [
         ("reduce_dim", PCA(random_state=42)),
-        ("classify", LinearSVC(random_state=42, C=0.01, dual="auto")),
+        ("classify", LinearSVC(random_state=42, C=0.01)),
     ]
 )
 
 param_grid = {"reduce_dim__n_components": [6, 8, 10, 12, 14]}
 
 grid = GridSearchCV(
     pipe,
```

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_grid_search_stats.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_stats.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_grid_search_text_feature_extraction.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_grid_search_text_feature_extraction.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_learning_curve.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_learning_curve.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_likelihood_ratios.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_likelihood_ratios.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_multi_metric_evaluation.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_multi_metric_evaluation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_nested_cross_validation_iris.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_nested_cross_validation_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_permutation_tests_for_classification.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_permutation_tests_for_classification.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_precision_recall.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_precision_recall.py`

 * *Files 1% similar despite different names*

```diff
@@ -121,17 +121,15 @@
 # Linear SVC will expect each feature to have a similar range of values. Thus,
 # we will first scale the data using a
 # :class:`~sklearn.preprocessing.StandardScaler`.
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import StandardScaler
 from sklearn.svm import LinearSVC
 
-classifier = make_pipeline(
-    StandardScaler(), LinearSVC(random_state=random_state, dual="auto")
-)
+classifier = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))
 classifier.fit(X_train, y_train)
 
 # %%
 # Plot the Precision-Recall curve
 # ...............................
 #
 # To plot the precision-recall curve, you should use
@@ -187,15 +185,15 @@
 
 # %%
 # We use :class:`~sklearn.multiclass.OneVsRestClassifier` for multi-label
 # prediction.
 from sklearn.multiclass import OneVsRestClassifier
 
 classifier = OneVsRestClassifier(
-    make_pipeline(StandardScaler(), LinearSVC(random_state=random_state, dual="auto"))
+    make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))
 )
 classifier.fit(X_train, Y_train)
 y_score = classifier.decision_function(X_test)
 
 
 # %%
 # The average precision score in multi-label settings
```

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_randomized_search.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_randomized_search.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_roc.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_roc.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_roc_crossval.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_roc_crossval.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_successive_halving_heatmap.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_successive_halving_heatmap.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_successive_halving_iterations.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_successive_halving_iterations.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_train_error_vs_test_error.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_train_error_vs_test_error.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_underfitting_overfitting.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_underfitting_overfitting.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/model_selection/plot_validation_curve.py` & `scikit_learn-1.5.0rc1/examples/model_selection/plot_validation_curve.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/multiclass/plot_multiclass_overview.py` & `scikit_learn-1.5.0rc1/examples/multiclass/plot_multiclass_overview.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/multioutput/plot_classifier_chain_yeast.py` & `scikit_learn-1.5.0rc1/examples/multioutput/plot_classifier_chain_yeast.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/approximate_nearest_neighbors.py` & `scikit_learn-1.5.0rc1/examples/neighbors/approximate_nearest_neighbors.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_caching_nearest_neighbors.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_caching_nearest_neighbors.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_classification.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_classification.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_digits_kde_sampling.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_digits_kde_sampling.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_kde_1d.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_kde_1d.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_lof_novelty_detection.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_lof_novelty_detection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_lof_outlier_detection.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_lof_outlier_detection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_nca_classification.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_nca_classification.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_nca_dim_reduction.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_nca_dim_reduction.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_nca_illustration.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_nca_illustration.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_nearest_centroid.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_nearest_centroid.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_regression.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_regression.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neighbors/plot_species_kde.py` & `scikit_learn-1.5.0rc1/examples/neighbors/plot_species_kde.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neural_networks/plot_mlp_alpha.py` & `scikit_learn-1.5.0rc1/examples/neural_networks/plot_mlp_alpha.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neural_networks/plot_mlp_training_curves.py` & `scikit_learn-1.5.0rc1/examples/neural_networks/plot_mlp_training_curves.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neural_networks/plot_mnist_filters.py` & `scikit_learn-1.5.0rc1/examples/neural_networks/plot_mnist_filters.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/neural_networks/plot_rbm_logistic_classification.py` & `scikit_learn-1.5.0rc1/examples/neural_networks/plot_rbm_logistic_classification.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_all_scaling.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_all_scaling.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_discretization.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_discretization.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_discretization_classification.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_discretization_classification.py`

 * *Files 2% similar despite different names*

```diff
@@ -64,15 +64,15 @@
 # should be used.
 classifiers = [
     (
         make_pipeline(StandardScaler(), LogisticRegression(random_state=0)),
         {"logisticregression__C": np.logspace(-1, 1, 3)},
     ),
     (
-        make_pipeline(StandardScaler(), LinearSVC(random_state=0, dual="auto")),
+        make_pipeline(StandardScaler(), LinearSVC(random_state=0)),
         {"linearsvc__C": np.logspace(-1, 1, 3)},
     ),
     (
         make_pipeline(
             StandardScaler(),
             KBinsDiscretizer(encode="onehot", random_state=0),
             LogisticRegression(random_state=0),
@@ -82,15 +82,15 @@
             "logisticregression__C": np.logspace(-1, 1, 3),
         },
     ),
     (
         make_pipeline(
             StandardScaler(),
             KBinsDiscretizer(encode="onehot", random_state=0),
-            LinearSVC(random_state=0, dual="auto"),
+            LinearSVC(random_state=0),
         ),
         {
             "kbinsdiscretizer__n_bins": np.arange(5, 8),
             "linearsvc__C": np.logspace(-1, 1, 3),
         },
     ),
     (
```

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_discretization_strategies.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_discretization_strategies.py`

 * *Files 6% similar despite different names*

```diff
@@ -72,17 +72,15 @@
     ax.set_ylim(yy.min(), yy.max())
     ax.set_xticks(())
     ax.set_yticks(())
 
     i += 1
     # transform the dataset with KBinsDiscretizer
     for strategy in strategies:
-        enc = KBinsDiscretizer(
-            n_bins=4, encode="ordinal", strategy=strategy, subsample=200_000
-        )
+        enc = KBinsDiscretizer(n_bins=4, encode="ordinal", strategy=strategy)
         enc.fit(X)
         grid_encoded = enc.transform(grid)
 
         ax = plt.subplot(len(X_list), len(strategies) + 1, i)
 
         # horizontal stripes
         horizontal = grid_encoded[:, 0].reshape(xx.shape)
```

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_map_data_to_normal.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_map_data_to_normal.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_scaling_importance.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_scaling_importance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_target_encoder.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_target_encoder.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/preprocessing/plot_target_encoder_cross_val.py` & `scikit_learn-1.5.0rc1/examples/preprocessing/plot_target_encoder_cross_val.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_0_22_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_0_22_0.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_0_23_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_0_23_0.py`

 * *Files 2% similar despite different names*

```diff
@@ -118,15 +118,16 @@
 # weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:
 # early-stopping is enabled by default when the number of samples exceeds 10k.
 # Finally, users can now define :ref:`monotonic constraints
 # <monotonic_cst_gbdt>` to constrain the predictions based on the variations of
 # specific features. In the following example, we construct a target that is
 # generally positively correlated with the first feature, with some noise.
 # Applying monotoinc constraints allows the prediction to capture the global
-# effect of the first feature, instead of fitting the noise.
+# effect of the first feature, instead of fitting the noise. For a usecase
+# example, see :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`.
 import numpy as np
 from matplotlib import pyplot as plt
 from sklearn.model_selection import train_test_split
 
 # from sklearn.inspection import plot_partial_dependence
 from sklearn.inspection import PartialDependenceDisplay
 from sklearn.ensemble import HistGradientBoostingRegressor
```

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_0_24_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_0_24_0.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_0_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_0_0.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_1_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_1_0.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,14 +18,16 @@
 or with conda::
 
     conda install -c conda-forge scikit-learn
 
 """
 
 # %%
+# .. _quantile_support_hgbdt:
+#
 # Quantile loss in :class:`ensemble.HistGradientBoostingRegressor`
 # ----------------------------------------------------------------
 # :class:`~ensemble.HistGradientBoostingRegressor` can model quantiles with
 # `loss="quantile"` and the new parameter `quantile`.
 from sklearn.ensemble import HistGradientBoostingRegressor
 import numpy as np
 import matplotlib.pyplot as plt
@@ -47,14 +49,17 @@
 
 fig, ax = plt.subplots()
 ax.plot(X_1d, y, "o", alpha=0.5, markersize=1)
 for quantile, hist in hist_quantiles.items():
     ax.plot(X_1d, hist.predict(X), label=quantile)
 _ = ax.legend(loc="lower left")
 
+# %%
+# For a usecase example, see
+# :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`
 
 # %%
 # `get_feature_names_out` Available in all Transformers
 # -----------------------------------------------------
 # :term:`get_feature_names_out` is now available in all Transformers. This enables
 # :class:`~pipeline.Pipeline` to construct the output feature names for more complex
 # pipelines:
```

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_2_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_2_0.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_3_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_3_0.py`

 * *Files 4% similar despite different names*

```diff
@@ -84,15 +84,17 @@
 # %%
 # Missing values support in decision trees
 # ----------------------------------------
 # The classes :class:`tree.DecisionTreeClassifier` and
 # :class:`tree.DecisionTreeRegressor` now support missing values. For each potential
 # threshold on the non-missing data, the splitter will evaluate the split with all the
 # missing values going to the left node or the right node.
-# More details in the :ref:`User Guide <tree_missing_value_support>`.
+# See more details in the :ref:`User Guide <tree_missing_value_support>` or see
+# :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for a usecase
+# example of this feature in :class:`~ensemble.HistGradientBoostingRegressor`.
 import numpy as np
 from sklearn.tree import DecisionTreeClassifier
 
 X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)
 y = [0, 0, 1, 1]
 
 tree = DecisionTreeClassifier(random_state=0).fit(X, y)
```

### Comparing `scikit-learn-1.4.2/examples/release_highlights/plot_release_highlights_1_4_0.py` & `scikit_learn-1.5.0rc1/examples/release_highlights/plot_release_highlights_1_4_0.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/semi_supervised/plot_label_propagation_digits.py` & `scikit_learn-1.5.0rc1/examples/semi_supervised/plot_label_propagation_digits.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/semi_supervised/plot_label_propagation_digits_active_learning.py` & `scikit_learn-1.5.0rc1/examples/semi_supervised/plot_label_propagation_digits_active_learning.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/semi_supervised/plot_label_propagation_structure.py` & `scikit_learn-1.5.0rc1/examples/semi_supervised/plot_label_propagation_structure.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/semi_supervised/plot_self_training_varying_threshold.py` & `scikit_learn-1.5.0rc1/examples/semi_supervised/plot_self_training_varying_threshold.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/semi_supervised/plot_semi_supervised_newsgroups.py` & `scikit_learn-1.5.0rc1/examples/semi_supervised/plot_semi_supervised_newsgroups.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 dataset (which will be automatically downloaded).
 
 You can adjust the number of categories by giving their names to the dataset
 loader or setting them to `None` to get all 20 of them.
 
 """
 
-
 import numpy as np
 
 from sklearn.datasets import fetch_20newsgroups
 from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
 from sklearn.linear_model import SGDClassifier
 from sklearn.metrics import f1_score
 from sklearn.model_selection import train_test_split
```

### Comparing `scikit-learn-1.4.2/examples/semi_supervised/plot_semi_supervised_versus_svm_iris.py` & `scikit_learn-1.5.0rc1/examples/semi_supervised/plot_semi_supervised_versus_svm_iris.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_custom_kernel.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_custom_kernel.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_iris_svc.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_iris_svc.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 y = iris.target
 
 # we create an instance of SVM and fit out data. We do not scale our
 # data since we want to plot the support vectors
 C = 1.0  # SVM regularization parameter
 models = (
     svm.SVC(kernel="linear", C=C),
-    svm.LinearSVC(C=C, max_iter=10000, dual="auto"),
+    svm.LinearSVC(C=C, max_iter=10000),
     svm.SVC(kernel="rbf", gamma=0.7, C=C),
     svm.SVC(kernel="poly", degree=3, gamma="auto", C=C),
 )
 models = (clf.fit(X, y) for clf in models)
 
 # title for the plots
 titles = (
```

### Comparing `scikit-learn-1.4.2/examples/svm/plot_linearsvc_support_vectors.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_linearsvc_support_vectors.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from sklearn.svm import LinearSVC
 
 X, y = make_blobs(n_samples=40, centers=2, random_state=0)
 
 plt.figure(figsize=(10, 5))
 for i, C in enumerate([1, 100]):
     # "hinge" is the standard SVM loss
-    clf = LinearSVC(C=C, loss="hinge", random_state=42, dual="auto").fit(X, y)
+    clf = LinearSVC(C=C, loss="hinge", random_state=42).fit(X, y)
     # obtain the support vectors through the decision function
     decision_function = clf.decision_function(X)
     # we can also calculate the decision function manually
     # decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]
     # The support vectors are the samples that lie within the margin
     # boundaries, whose size is conventionally constrained to 1
     support_vector_indices = np.where(np.abs(decision_function) <= 1 + 1e-15)[0]
```

### Comparing `scikit-learn-1.4.2/examples/svm/plot_oneclass.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_oneclass.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_rbf_parameters.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_rbf_parameters.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_separating_hyperplane.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_separating_hyperplane.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_separating_hyperplane_unbalanced.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_separating_hyperplane_unbalanced.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_anova.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_anova.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_kernels.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_kernels.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_margin.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_margin.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_nonlinear.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_nonlinear.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_regression.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_regression.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_scale_c.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_scale_c.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_svm_tie_breaking.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_svm_tie_breaking.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/svm/plot_weighted_samples.py` & `scikit_learn-1.5.0rc1/examples/svm/plot_weighted_samples.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/text/plot_document_classification_20newsgroups.py` & `scikit_learn-1.5.0rc1/examples/text/plot_document_classification_20newsgroups.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/text/plot_document_clustering.py` & `scikit_learn-1.5.0rc1/examples/text/plot_document_clustering.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/text/plot_hashing_vs_dict_vectorizer.py` & `scikit_learn-1.5.0rc1/examples/text/plot_hashing_vs_dict_vectorizer.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/tree/plot_cost_complexity_pruning.py` & `scikit_learn-1.5.0rc1/examples/tree/plot_cost_complexity_pruning.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/tree/plot_iris_dtc.py` & `scikit_learn-1.5.0rc1/examples/tree/plot_iris_dtc.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 
 For each pair of iris features, the decision tree learns decision
 boundaries made of combinations of simple thresholding rules inferred from
 the training samples.
 
 We also show the tree structure of a model built on all of the features.
 """
+
 # %%
 # First load the copy of the Iris dataset shipped with scikit-learn:
 from sklearn.datasets import load_iris
 
 iris = load_iris()
```

### Comparing `scikit-learn-1.4.2/examples/tree/plot_tree_regression.py` & `scikit_learn-1.5.0rc1/examples/tree/plot_tree_regression.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/tree/plot_tree_regression_multioutput.py` & `scikit_learn-1.5.0rc1/examples/tree/plot_tree_regression_multioutput.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/examples/tree/plot_unveil_tree_structure.py` & `scikit_learn-1.5.0rc1/examples/tree/plot_unveil_tree_structure.py`

 * *Files 1% similar despite different names*

```diff
@@ -64,15 +64,15 @@
 #   - ``threshold[i]``: threshold value at node ``i``
 #   - ``n_node_samples[i]``: the number of training samples reaching node
 #     ``i``
 #   - ``impurity[i]``: the impurity at node ``i``
 #   - ``weighted_n_node_samples[i]``: the weighted number of training samples
 #     reaching node ``i``
 #   - ``value[i, j, k]``: the summary of the training samples that reached node i for
-#     class j and output k.
+#     output j and class k (for regression tree, class is set to 1).
 #
 # Using the arrays, we can traverse the tree structure to compute various
 # properties. Below, we will compute the depth of each node and whether or not
 # it is a leaf.
 
 n_nodes = clf.tree_.node_count
 children_left = clf.tree_.children_left
```

### Comparing `scikit-learn-1.4.2/meson.build` & `scikit_learn-1.5.0rc1/meson.build`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/setup.cfg` & `scikit_learn-1.5.0rc1/setup.cfg`

 * *Files 20% similar despite different names*

```diff
@@ -4,59 +4,61 @@
 [options.packages.find]
 include = sklearn*
 
 [aliases]
 test = pytest
 
 [tool:pytest]
+# disable-pytest-warnings should be removed once we rewrite tests
+# using yield with parametrize
 doctest_optionflags = NORMALIZE_WHITESPACE ELLIPSIS
 testpaths = sklearn
-addopts = 
-	--doctest-modules
-	--disable-pytest-warnings
-	--color=yes
-	-p sklearn.tests.random_seed
+addopts =
+    --doctest-modules
+    --disable-pytest-warnings
+    --color=yes
+    # Activate the plugin explicitly to ensure that the seed is reported
+    # correctly on the CI when running `pytest --pyargs sklearn` from the
+    # source folder.
+    -p sklearn.tests.random_seed
 
 [mypy]
 ignore_missing_imports = True
 allow_redefinition = True
-exclude = 
-	sklearn/externals
+exclude=
+    sklearn/externals
 
 [mypy-joblib.*]
 follow_imports = skip
 
 [check-manifest]
-ignore = 
-	sklearn/_loss/_loss.pyx
-	sklearn/linear_model/_sag_fast.pyx
-	sklearn/linear_model/_sgd_fast.pyx
-	sklearn/utils/_seq_dataset.pyx
-	sklearn/utils/_seq_dataset.pxd
-	sklearn/utils/_weight_vector.pyx
-	sklearn/utils/_weight_vector.pxd
-	sklearn/metrics/_dist_metrics.pyx
-	sklearn/metrics/_dist_metrics.pxd
-	sklearn/metrics/_pairwise_distances_reduction/_argkmin.pxd
-	sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx
-	sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx
-	sklearn/metrics/_pairwise_distances_reduction/_base.pxd
-	sklearn/metrics/_pairwise_distances_reduction/_base.pyx
-	sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pxd
-	sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx
-	sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pxd
-	sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx
-	sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pxd
-	sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx
-	sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx
-	sklearn/neighbors/_ball_tree.pyx
-	sklearn/neighbors/_binary_tree.pxi
-	sklearn/neighbors/_kd_tree.pyx
+# ignore files missing in VCS
+ignore =
+    sklearn/_loss/_loss.pyx
+    sklearn/linear_model/_sag_fast.pyx
+    sklearn/linear_model/_sgd_fast.pyx
+    sklearn/utils/_seq_dataset.pyx
+    sklearn/utils/_seq_dataset.pxd
+    sklearn/utils/_weight_vector.pyx
+    sklearn/utils/_weight_vector.pxd
+    sklearn/metrics/_dist_metrics.pyx
+    sklearn/metrics/_dist_metrics.pxd
+    sklearn/metrics/_pairwise_distances_reduction/_argkmin.pxd
+    sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx
+    sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx
+    sklearn/metrics/_pairwise_distances_reduction/_base.pxd
+    sklearn/metrics/_pairwise_distances_reduction/_base.pyx
+    sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pxd
+    sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx
+    sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pxd
+    sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx
+    sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pxd
+    sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx
+    sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx
+    sklearn/neighbors/_ball_tree.pyx
+    sklearn/neighbors/_binary_tree.pxi
+    sklearn/neighbors/_kd_tree.pyx
+
 
 [codespell]
 skip = ./.git,./.mypy_cache,./doc/themes/scikit-learn-modern/static/js,./sklearn/feature_extraction/_stop_words.py,./doc/_build,./doc/auto_examples,./doc/modules/generated
 ignore-words = build_tools/codespell_ignore_words.txt
-
-[egg_info]
-tag_build = 
-tag_date = 0
-
```

### Comparing `scikit-learn-1.4.2/setup.py` & `scikit_learn-1.5.0rc1/setup.py`

 * *Files 12% similar despite different names*

```diff
@@ -29,16 +29,16 @@
 builtins.__SKLEARN_SETUP__ = True
 
 
 DISTNAME = "scikit-learn"
 DESCRIPTION = "A set of python modules for machine learning and data mining"
 with open("README.rst") as f:
     LONG_DESCRIPTION = f.read()
-MAINTAINER = "Andreas Mueller"
-MAINTAINER_EMAIL = "amueller@ais.uni-bonn.de"
+MAINTAINER = "scikit-learn developers"
+MAINTAINER_EMAIL = "scikit-learn@python.org"
 URL = "https://scikit-learn.org"
 DOWNLOAD_URL = "https://pypi.org/project/scikit-learn/#files"
 LICENSE = "new BSD"
 PROJECT_URLS = {
     "Bug Tracker": "https://github.com/scikit-learn/scikit-learn/issues",
     "Documentation": "https://scikit-learn.org/stable/documentation.html",
     "Source Code": "https://github.com/scikit-learn/scikit-learn",
@@ -197,15 +197,15 @@
     "": [
         {"sources": ["_isotonic.pyx"]},
     ],
     "_loss": [
         {"sources": ["_loss.pyx.tp"]},
     ],
     "cluster": [
-        {"sources": ["_dbscan_inner.pyx"], "language": "c++", "include_np": True},
+        {"sources": ["_dbscan_inner.pyx"], "language": "c++"},
         {"sources": ["_hierarchical_fast.pyx"], "language": "c++", "include_np": True},
         {"sources": ["_k_means_common.pyx"], "include_np": True},
         {"sources": ["_k_means_lloyd.pyx"], "include_np": True},
         {"sources": ["_k_means_elkan.pyx"], "include_np": True},
         {"sources": ["_k_means_minibatch.pyx"], "include_np": True},
     ],
     "cluster._hdbscan": [
@@ -217,51 +217,50 @@
         {
             "sources": ["_svmlight_format_fast.pyx"],
             "include_np": True,
             "compile_for_pypy": False,
         }
     ],
     "decomposition": [
-        {"sources": ["_online_lda_fast.pyx"], "include_np": True},
+        {"sources": ["_online_lda_fast.pyx"]},
         {"sources": ["_cdnmf_fast.pyx"], "include_np": True},
     ],
     "ensemble": [
         {"sources": ["_gradient_boosting.pyx"], "include_np": True},
     ],
     "ensemble._hist_gradient_boosting": [
-        {"sources": ["_gradient_boosting.pyx"], "include_np": True},
-        {"sources": ["histogram.pyx"], "include_np": True},
-        {"sources": ["splitting.pyx"], "include_np": True},
-        {"sources": ["_binning.pyx"], "include_np": True},
-        {"sources": ["_predictor.pyx"], "include_np": True},
-        {"sources": ["_bitset.pyx"], "include_np": True},
-        {"sources": ["common.pyx"], "include_np": True},
-        {"sources": ["utils.pyx"], "include_np": True},
+        {"sources": ["_gradient_boosting.pyx"]},
+        {"sources": ["histogram.pyx"]},
+        {"sources": ["splitting.pyx"]},
+        {"sources": ["_binning.pyx"]},
+        {"sources": ["_predictor.pyx"]},
+        {"sources": ["_bitset.pyx"]},
+        {"sources": ["common.pyx"]},
     ],
     "feature_extraction": [
         {"sources": ["_hashing_fast.pyx"], "language": "c++", "include_np": True},
     ],
     "linear_model": [
-        {"sources": ["_cd_fast.pyx"], "include_np": True},
-        {"sources": ["_sgd_fast.pyx.tp"], "include_np": True},
-        {"sources": ["_sag_fast.pyx.tp"], "include_np": True},
+        {"sources": ["_cd_fast.pyx"]},
+        {"sources": ["_sgd_fast.pyx.tp"]},
+        {"sources": ["_sag_fast.pyx.tp"]},
     ],
     "manifold": [
-        {"sources": ["_utils.pyx"], "include_np": True},
+        {"sources": ["_utils.pyx"]},
         {"sources": ["_barnes_hut_tsne.pyx"], "include_np": True},
     ],
     "metrics": [
-        {"sources": ["_pairwise_fast.pyx"], "include_np": True},
+        {"sources": ["_pairwise_fast.pyx"]},
         {
             "sources": ["_dist_metrics.pyx.tp", "_dist_metrics.pxd.tp"],
             "include_np": True,
         },
     ],
     "metrics.cluster": [
-        {"sources": ["_expected_mutual_info_fast.pyx"], "include_np": True},
+        {"sources": ["_expected_mutual_info_fast.pyx"]},
     ],
     "metrics._pairwise_distances_reduction": [
         {
             "sources": ["_datasets_pair.pyx.tp", "_datasets_pair.pxd.tp"],
             "language": "c++",
             "include_np": True,
             "extra_compile_args": ["-std=c++11"],
@@ -302,30 +301,28 @@
             "extra_compile_args": ["-std=c++11"],
         },
     ],
     "preprocessing": [
         {"sources": ["_csr_polynomial_expansion.pyx"]},
         {
             "sources": ["_target_encoder_fast.pyx"],
-            "include_np": True,
             "language": "c++",
             "extra_compile_args": ["-std=c++11"],
         },
     ],
     "neighbors": [
         {"sources": ["_binary_tree.pxi.tp"], "include_np": True},
         {"sources": ["_ball_tree.pyx.tp"], "include_np": True},
         {"sources": ["_kd_tree.pyx.tp"], "include_np": True},
         {"sources": ["_partition_nodes.pyx"], "language": "c++", "include_np": True},
         {"sources": ["_quad_tree.pyx"], "include_np": True},
     ],
     "svm": [
         {
             "sources": ["_newrand.pyx"],
-            "include_np": True,
             "include_dirs": [join("src", "newrand")],
             "language": "c++",
             # Use C++11 random number generator fix
             "extra_compile_args": ["-std=c++11"],
         },
         {
             "sources": ["_libsvm.pyx"],
@@ -338,25 +335,23 @@
             ],
             "include_dirs": [
                 join("src", "libsvm"),
                 join("src", "newrand"),
             ],
             "libraries": ["libsvm-skl"],
             "extra_link_args": ["-lstdc++"],
-            "include_np": True,
         },
         {
             "sources": ["_liblinear.pyx"],
             "libraries": ["liblinear-skl"],
             "include_dirs": [
                 join("src", "liblinear"),
                 join("src", "newrand"),
                 join("..", "utils"),
             ],
-            "include_np": True,
             "depends": [
                 join("src", "liblinear", "tron.h"),
                 join("src", "liblinear", "linear.h"),
                 join("src", "liblinear", "liblinear_helper.c"),
                 join("src", "newrand", "newrand.h"),
             ],
             "extra_link_args": ["-lstdc++"],
@@ -364,15 +359,14 @@
         {
             "sources": ["_libsvm_sparse.pyx"],
             "libraries": ["libsvm-skl"],
             "include_dirs": [
                 join("src", "libsvm"),
                 join("src", "newrand"),
             ],
-            "include_np": True,
             "depends": [
                 join("src", "libsvm", "svm.h"),
                 join("src", "newrand", "newrand.h"),
                 join("src", "libsvm", "libsvm_sparse_helper.c"),
             ],
             "extra_link_args": ["-lstdc++"],
         },
@@ -385,30 +379,26 @@
             "optimization_level": "O3",
         },
         {"sources": ["_splitter.pyx"], "include_np": True, "optimization_level": "O3"},
         {"sources": ["_criterion.pyx"], "include_np": True, "optimization_level": "O3"},
         {"sources": ["_utils.pyx"], "include_np": True, "optimization_level": "O3"},
     ],
     "utils": [
-        {"sources": ["sparsefuncs_fast.pyx"], "include_np": True},
+        {"sources": ["sparsefuncs_fast.pyx"]},
         {"sources": ["_cython_blas.pyx"]},
         {"sources": ["arrayfuncs.pyx"]},
         {
             "sources": ["murmurhash.pyx", join("src", "MurmurHash3.cpp")],
             "include_dirs": ["src"],
-            "include_np": True,
         },
         {"sources": ["_fast_dict.pyx"], "language": "c++"},
         {"sources": ["_openmp_helpers.pyx"]},
-        {"sources": ["_seq_dataset.pyx.tp", "_seq_dataset.pxd.tp"], "include_np": True},
-        {
-            "sources": ["_weight_vector.pyx.tp", "_weight_vector.pxd.tp"],
-            "include_np": True,
-        },
-        {"sources": ["_random.pyx"], "include_np": True},
+        {"sources": ["_seq_dataset.pyx.tp", "_seq_dataset.pxd.tp"]},
+        {"sources": ["_weight_vector.pyx.tp", "_weight_vector.pxd.tp"]},
+        {"sources": ["_random.pyx"]},
         {"sources": ["_typedefs.pyx"]},
         {"sources": ["_heap.pyx"]},
         {"sources": ["_sorting.pyx"]},
         {"sources": ["_vector_sentinel.pyx"], "language": "c++", "include_np": True},
         {"sources": ["_isfinite.pyx"]},
     ],
 }
```

### Comparing `scikit-learn-1.4.2/sklearn/__check_build/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/__check_build/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-""" Module to give helpful messages to the user that did not
+"""Module to give helpful messages to the user that did not
 compile scikit-learn properly.
 """
+
 import os
 
 INPLACE_MSG = """
 It appears that you are importing a local scikit-learn source tree. For
 this, you need to have an inplace install. Maybe you are in the source
 directory and you need to try from another location."""
 
@@ -24,24 +25,27 @@
         msg = INPLACE_MSG
     dir_content = list()
     for i, filename in enumerate(os.listdir(local_dir)):
         if (i + 1) % 3:
             dir_content.append(filename.ljust(26))
         else:
             dir_content.append(filename + "\n")
-    raise ImportError("""%s
+    raise ImportError(
+        """%s
 ___________________________________________________________________________
 Contents of %s:
 %s
 ___________________________________________________________________________
 It seems that scikit-learn has not been built correctly.
 
 If you have installed scikit-learn from source, please do not forget
 to build the package before using it: run `python setup.py install` or
 `make` in the source directory.
-%s""" % (e, local_dir, "".join(dir_content).strip(), msg))
+%s"""
+        % (e, local_dir, "".join(dir_content).strip(), msg)
+    )
 
 
 try:
     from ._check_build import check_build  # noqa
 except ImportError as e:
     raise_build_error(e)
```

### Comparing `scikit-learn-1.4.2/sklearn/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -38,15 +38,15 @@
 #   X.Y.ZbN   # Beta release
 #   X.Y.ZrcN  # Release Candidate
 #   X.Y.Z     # Final release
 #
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = "1.4.2"
+__version__ = "1.5.0rc1"
 
 
 # On OSX, we can get a runtime error due to multiple OpenMP libraries loaded
 # simultaneously. This can happen for instance when calling BLAS inside a
 # prange. Setting the following environment variable allows multiple OpenMP
 # libraries to be loaded. It should not degrade performances since we manually
 # take care of potential over-subcription performance issues, in sections of
@@ -69,14 +69,23 @@
     __SKLEARN_SETUP__ = False
 
 if __SKLEARN_SETUP__:
     sys.stderr.write("Partial import of sklearn during the build process.\n")
     # We are not importing the rest of scikit-learn during the build
     # process, as it may not be compiled yet
 else:
+    # Import numpy, scipy to make sure that the BLAS libs are loaded before
+    # creating the ThreadpoolController. They would be imported just after
+    # when importing utils anyway. This makes it explicit and robust to changes
+    # in utils.
+    # (OpenMP is loaded by importing show_versions right after this block)
+    import numpy  # noqa
+    import scipy.linalg  # noqa
+    from threadpoolctl import ThreadpoolController
+
     # `_distributor_init` allows distributors to run custom init code.
     # For instance, for the Windows wheel, this is used to pre-load the
     # vcomp shared library runtime for OpenMP embedded in the sklearn/.libs
     # sub-folder.
     # It is necessary to do this prior to importing show_versions as the
     # later is linked to the OpenMP runtime to make it possible to introspect
     # it and importing it first would fail if the OpenMP dll cannot be found.
@@ -137,14 +146,20 @@
     try:
         import sklearn._built_with_meson  # noqa: F401
 
         _BUILT_WITH_MESON = True
     except ModuleNotFoundError:
         pass
 
+    # Set a global controller that can be used to locally limit the number of
+    # threads without looping through all shared libraries every time.
+    # This instantitation should not happen earlier because it needs all BLAS and
+    # OpenMP libs to be loaded first.
+    _threadpool_controller = ThreadpoolController()
+
 
 def setup_module(module):
     """Fixture for the tests to assure globally controllable seeding of RNGs"""
 
     import numpy as np
 
     # Check if a random seed exists in the environment, if not create one.
```

### Comparing `scikit-learn-1.4.2/sklearn/_build_utils/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/_build_utils/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Utilities useful during the build.
 """
+
 # author: Andy Mueller, Gael Varoquaux
 # license: BSD
 
 
 import contextlib
 import os
```

### Comparing `scikit-learn-1.4.2/sklearn/_build_utils/openmp_helpers.py` & `scikit_learn-1.5.0rc1/sklearn/_build_utils/openmp_helpers.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,27 +30,29 @@
         return []
     # Default flag for GCC and clang:
     return ["-fopenmp"]
 
 
 def check_openmp_support():
     """Check whether OpenMP test code can be compiled and run"""
-    if "PYODIDE_PACKAGE_ABI" in os.environ:
+    if "PYODIDE" in os.environ:
         # Pyodide doesn't support OpenMP
         return False
 
-    code = textwrap.dedent("""\
+    code = textwrap.dedent(
+        """\
         #include <omp.h>
         #include <stdio.h>
         int main(void) {
         #pragma omp parallel
         printf("nthreads=%d\\n", omp_get_num_threads());
         return 0;
         }
-        """)
+        """
+    )
 
     extra_preargs = os.getenv("LDFLAGS", None)
     if extra_preargs is not None:
         extra_preargs = extra_preargs.strip().split(" ")
         # FIXME: temporary fix to link against system libraries on linux
         # "-Wl,--sysroot=/" should be removed
         extra_preargs = [
@@ -90,15 +92,16 @@
 
     if not openmp_supported:
         if os.getenv("SKLEARN_FAIL_NO_OPENMP"):
             raise Exception(
                 "Failed to build scikit-learn with OpenMP support"
             ) from openmp_exception
         else:
-            message = textwrap.dedent("""
+            message = textwrap.dedent(
+                """
 
                                 ***********
                                 * WARNING *
                                 ***********
 
                 It seems that scikit-learn cannot be built with OpenMP.
 
@@ -113,11 +116,12 @@
 
                 - The build will continue with OpenMP-based parallelism
                   disabled. Note however that some estimators will run in
                   sequential mode instead of leveraging thread-based
                   parallelism.
 
                                     ***
-                """)
+                """
+            )
             warnings.warn(message)
 
     return openmp_supported
```

### Comparing `scikit-learn-1.4.2/sklearn/_build_utils/pre_build_helpers.py` & `scikit_learn-1.5.0rc1/sklearn/_build_utils/pre_build_helpers.py`

 * *Files 19% similar despite different names*

```diff
@@ -56,18 +56,20 @@
             os.chdir(start_dir)
 
     return output
 
 
 def basic_check_build():
     """Check basic compilation and linking of C code"""
-    if "PYODIDE_PACKAGE_ABI" in os.environ:
+    if "PYODIDE" in os.environ:
         # The following check won't work in pyodide
         return
 
-    code = textwrap.dedent("""\
+    code = textwrap.dedent(
+        """\
         #include <stdio.h>
         int main(void) {
         return 0;
         }
-        """)
+        """
+    )
     compile_test_program(code)
```

### Comparing `scikit-learn-1.4.2/sklearn/_build_utils/tempita.py` & `scikit_learn-1.5.0rc1/sklearn/_build_utils/tempita.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/_config.py` & `scikit_learn-1.5.0rc1/sklearn/_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-"""Global configuration state and functions for management
-"""
+"""Global configuration state and functions for management"""
+
 import os
 import threading
 from contextlib import contextmanager as contextmanager
 
 _global_config = {
     "assume_finite": bool(os.environ.get("SKLEARN_ASSUME_FINITE", False)),
     "working_memory": int(os.environ.get("SKLEARN_WORKING_MEMORY", 1024)),
```

### Comparing `scikit-learn-1.4.2/sklearn/_isotonic.pyx` & `scikit_learn-1.5.0rc1/sklearn/_isotonic.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/_loss/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/_loss/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/_loss/_loss.pxd` & `scikit_learn-1.5.0rc1/sklearn/_loss/_loss.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/_loss/_loss.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/_loss/_loss.pyx.tp`

 * *Files 1% similar despite different names*

```diff
@@ -267,15 +267,15 @@
 
 
 cdef inline void sum_exp_minus_max(
     const int i,
     const floating_in[:, :] raw_prediction,  # IN
     floating_in *p                           # OUT
 ) noexcept nogil:
-    # Thread local buffers are used to stores results of this function via p.
+    # Thread local buffers are used to store results of this function via p.
     # The results are stored as follows:
     #     p[k] = exp(raw_prediction_i_k - max_value) for k = 0 to n_classes-1
     #     p[-2] = max(raw_prediction_i_k, k = 0 to n_classes-1)
     #     p[-1] = sum(p[k], k = 0 to n_classes-1) = sum of exponentials
     # len(p) must be n_classes + 2
     # Notes:
     # - Using "by reference" arguments doesn't work well, therefore we use a
@@ -761,17 +761,16 @@
     cdef double_pair gh
     # See comment in cgradient_half_binomial.
     if raw_prediction > -37:
         gh.val2 = exp(-raw_prediction)  # used as temporary
         gh.val1 = ((1 - y_true) - y_true * gh.val2) / (1 + gh.val2)  # gradient
         gh.val2 = gh.val2 / (1 + gh.val2)**2                         # hessian
     else:
-        gh.val2 = exp(raw_prediction)
+        gh.val2 = exp(raw_prediction)  # = 1. order Taylor in exp(raw_prediction)
         gh.val1 = gh.val2 - y_true
-        gh.val2 *= (1 - gh.val2)
     return gh
 
 
 # Exponential loss with (half) logit-link, aka boosting loss
 cdef inline double closs_exponential(
     double y_true,
     double raw_prediction
@@ -1181,34 +1180,32 @@
 
                 for i in prange(n_samples, schedule='static'):
                     sum_exp_minus_max(i, raw_prediction, p)
                     max_value = p[n_classes]     # p[-2]
                     sum_exps = p[n_classes + 1]  # p[-1]
                     loss_out[i] = log(sum_exps) + max_value
 
-                    for k in range(n_classes):
-                        # label decode y_true
-                        if y_true[i] == k:
-                            loss_out[i] -= raw_prediction[i, k]
+                    # label encoded y_true
+                    k = int(y_true[i])
+                    loss_out[i] -= raw_prediction[i, k]
 
                 free(p)
         else:
             with nogil, parallel(num_threads=n_threads):
                 p = <floating_in *> malloc(sizeof(floating_in) * (n_classes + 2))
 
                 for i in prange(n_samples, schedule='static'):
                     sum_exp_minus_max(i, raw_prediction, p)
                     max_value = p[n_classes]     # p[-2]
                     sum_exps = p[n_classes + 1]  # p[-1]
                     loss_out[i] = log(sum_exps) + max_value
 
-                    for k in range(n_classes):
-                        # label decode y_true
-                        if y_true[i] == k:
-                            loss_out[i] -= raw_prediction[i, k]
+                    # label encoded y_true
+                    k = int(y_true[i])
+                    loss_out[i] -= raw_prediction[i, k]
 
                     loss_out[i] *= sample_weight[i]
 
                 free(p)
 
     def loss_gradient(
         self,
@@ -1237,15 +1234,15 @@
                     sum_exp_minus_max(i, raw_prediction, p)
                     max_value = p[n_classes]  # p[-2]
                     sum_exps = p[n_classes + 1]  # p[-1]
                     loss_out[i] = log(sum_exps) + max_value
 
                     for k in range(n_classes):
                         # label decode y_true
-                        if y_true [i] == k:
+                        if y_true[i] == k:
                             loss_out[i] -= raw_prediction[i, k]
                         p[k] /= sum_exps  # p_k = y_pred_k = prob of class k
                         # gradient_k = p_k - (y_true == k)
                         gradient_out[i, k] = p[k] - (y_true[i] == k)
 
                 free(p)
         else:
@@ -1256,15 +1253,15 @@
                     sum_exp_minus_max(i, raw_prediction, p)
                     max_value = p[n_classes]  # p[-2]
                     sum_exps = p[n_classes + 1]  # p[-1]
                     loss_out[i] = log(sum_exps) + max_value
 
                     for k in range(n_classes):
                         # label decode y_true
-                        if y_true [i] == k:
+                        if y_true[i] == k:
                             loss_out[i] -= raw_prediction[i, k]
                         p[k] /= sum_exps  # p_k = y_pred_k = prob of class k
                         # gradient_k = (p_k - (y_true == k)) * sw
                         gradient_out[i, k] = (p[k] - (y_true[i] == k)) * sample_weight[i]
 
                     loss_out[i] *= sample_weight[i]
```

### Comparing `scikit-learn-1.4.2/sklearn/_loss/link.py` & `scikit_learn-1.5.0rc1/sklearn/_loss/link.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Module contains classes for invertible (and differentiable) link functions.
 """
+
 # Author: Christian Lorentzen <lorentzen.ch@gmail.com>
 
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
 
 import numpy as np
 from scipy.special import expit, logit
```

### Comparing `scikit-learn-1.4.2/sklearn/_loss/loss.py` & `scikit_learn-1.5.0rc1/sklearn/_loss/loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """
 This module contains loss classes suitable for fitting.
 
 It is not part of the public API.
 Specific losses are used for regression, binary classification or multiclass
 classification.
 """
+
 # Goals:
 # - Provide a common private module for loss functions/classes.
 # - To be used in:
 #   - LogisticRegression
 #   - PoissonRegressor, GammaRegressor, TweedieRegressor
 #   - HistGradientBoostingRegressor, HistGradientBoostingClassifier
 #   - GradientBoostingRegressor, GradientBoostingClassifier
```

### Comparing `scikit-learn-1.4.2/sklearn/_loss/tests/test_link.py` & `scikit_learn-1.5.0rc1/sklearn/_loss/tests/test_link.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/_loss/tests/test_loss.py` & `scikit_learn-1.5.0rc1/sklearn/_loss/tests/test_loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -23,16 +23,17 @@
     HalfPoissonLoss,
     HalfSquaredError,
     HalfTweedieLoss,
     HalfTweedieLossIdentity,
     HuberLoss,
     PinballLoss,
 )
-from sklearn.utils import _IS_WASM, assert_all_finite
+from sklearn.utils import assert_all_finite
 from sklearn.utils._testing import create_memmap_backed_data, skip_if_32bit
+from sklearn.utils.fixes import _IS_WASM
 
 ALL_LOSSES = list(_LOSSES.values())
 
 LOSS_INSTANCES = [loss() for loss in ALL_LOSSES]
 # HalfTweedieLoss(power=1.5) is already there as default
 LOSS_INSTANCES += [
     PinballLoss(quantile=0.25),
@@ -116,15 +117,16 @@
 
 
 @pytest.mark.parametrize("loss", LOSS_INSTANCES, ids=loss_instance_name)
 def test_loss_boundary(loss):
     """Test interval ranges of y_true and y_pred in losses."""
     # make sure low and high are always within the interval, used for linspace
     if loss.is_multiclass:
-        y_true = np.linspace(0, 9, num=10)
+        n_classes = 3  # default value
+        y_true = np.tile(np.linspace(0, n_classes - 1, num=n_classes), 3)
     else:
         low, high = _inclusive_low_high(loss.interval_y_true)
         y_true = np.linspace(low, high, num=10)
 
     # add boundaries if they are included
     if loss.interval_y_true.low_inclusive:
         y_true = np.r_[y_true, loss.interval_y_true.low]
@@ -132,15 +134,15 @@
         y_true = np.r_[y_true, loss.interval_y_true.high]
 
     assert loss.in_y_true_range(y_true)
 
     n = y_true.shape[0]
     low, high = _inclusive_low_high(loss.interval_y_pred)
     if loss.is_multiclass:
-        y_pred = np.empty((n, 3))
+        y_pred = np.empty((n, n_classes))
         y_pred[:, 0] = np.linspace(low, high, num=n)
         y_pred[:, 1] = 0.5 * (1 - y_pred[:, 0])
         y_pred[:, 2] = 0.5 * (1 - y_pred[:, 0])
     else:
         y_pred = np.linspace(low, high, num=n)
 
     assert loss.in_y_pred_range(y_pred)
```

### Comparing `scikit-learn-1.4.2/sklearn/_min_dependencies.py` & `scikit_learn-1.5.0rc1/sklearn/_min_dependencies.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,54 +1,56 @@
 """All minimum dependencies for scikit-learn."""
+
 import argparse
 from collections import defaultdict
 
 # scipy and cython should by in sync with pyproject.toml
 NUMPY_MIN_VERSION = "1.19.5"
 SCIPY_MIN_VERSION = "1.6.0"
 JOBLIB_MIN_VERSION = "1.2.0"
-THREADPOOLCTL_MIN_VERSION = "2.0.0"
+THREADPOOLCTL_MIN_VERSION = "3.1.0"
 PYTEST_MIN_VERSION = "7.1.2"
-CYTHON_MIN_VERSION = "3.0.8"
+CYTHON_MIN_VERSION = "3.0.10"
 
 
 # 'build' and 'install' is included to have structured metadata for CI.
 # It will NOT be included in setup's extras_require
 # The values are (version_spec, comma separated tags)
 dependent_packages = {
     "numpy": (NUMPY_MIN_VERSION, "build, install"),
     "scipy": (SCIPY_MIN_VERSION, "build, install"),
     "joblib": (JOBLIB_MIN_VERSION, "install"),
     "threadpoolctl": (THREADPOOLCTL_MIN_VERSION, "install"),
     "cython": (CYTHON_MIN_VERSION, "build"),
+    "meson-python": ("0.15.0", "build"),
     "matplotlib": ("3.3.4", "benchmark, docs, examples, tests"),
     "scikit-image": ("0.17.2", "docs, examples, tests"),
     "pandas": ("1.1.5", "benchmark, docs, examples, tests"),
     "seaborn": ("0.9.0", "docs, examples"),
     "memory_profiler": ("0.57.0", "benchmark, docs"),
     "pytest": (PYTEST_MIN_VERSION, "tests"),
     "pytest-cov": ("2.9.0", "tests"),
-    "ruff": ("0.0.272", "tests"),
-    "black": ("23.3.0", "tests"),
-    "mypy": ("1.3", "tests"),
+    "ruff": ("0.2.1", "tests"),
+    "black": ("24.3.0", "tests"),
+    "mypy": ("1.9", "tests"),
     "pyamg": ("4.0.0", "tests"),
-    "polars": ("0.19.12", "tests"),
+    "polars": ("0.19.12", "docs, tests"),
     "pyarrow": ("12.0.0", "tests"),
     "sphinx": ("6.0.0", "docs"),
     "sphinx-copybutton": ("0.5.2", "docs"),
     "sphinx-gallery": ("0.15.0", "docs"),
     "numpydoc": ("1.2.0", "docs, tests"),
     "Pillow": ("7.1.2", "docs"),
     "pooch": ("1.6.0", "docs, examples, tests"),
     "sphinx-prompt": ("1.3.0", "docs"),
     "sphinxext-opengraph": ("0.4.2", "docs"),
     "plotly": ("5.14.0", "docs, examples"),
     # XXX: Pin conda-lock to the latest released version (needs manual update
     # from time to time)
-    "conda-lock": ("2.4.2", "maintenance"),
+    "conda-lock": ("2.5.6", "maintenance"),
 }
 
 
 # create inverse mapping for setuptools
 tag_to_packages: dict = defaultdict(list)
 for package, (min_version, extras) in dependent_packages.items():
     for extra in extras.split(", "):
```

### Comparing `scikit-learn-1.4.2/sklearn/base.py` & `scikit_learn-1.5.0rc1/sklearn/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,22 +12,22 @@
 from collections import defaultdict
 
 import numpy as np
 
 from . import __version__
 from ._config import config_context, get_config
 from .exceptions import InconsistentVersionWarning
-from .utils import _IS_32BIT
 from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr
 from .utils._metadata_requests import _MetadataRequester, _routing_enabled
 from .utils._param_validation import validate_parameter_constraints
 from .utils._set_output import _SetOutputMixin
 from .utils._tags import (
     _DEFAULT_TAGS,
 )
+from .utils.fixes import _IS_32BIT
 from .utils.validation import (
     _check_feature_names_in,
     _check_y,
     _generate_get_feature_names_out,
     _get_feature_names,
     _is_fitted,
     _num_features,
@@ -1349,17 +1349,16 @@
 
 
 class _UnstableArchMixin:
     """Mark estimators that are non-determinstic on 32bit or PowerPC"""
 
     def _more_tags(self):
         return {
-            "non_deterministic": _IS_32BIT or platform.machine().startswith(
-                ("ppc", "powerpc")
-            )
+            "non_deterministic": _IS_32BIT
+            or platform.machine().startswith(("ppc", "powerpc"))
         }
 
 
 def is_classifier(estimator):
     """Return True if the given estimator is (probably) a classifier.
 
     Parameters
```

### Comparing `scikit-learn-1.4.2/sklearn/calibration.py` & `scikit_learn-1.5.0rc1/sklearn/calibration.py`

 * *Files 0% similar despite different names*

```diff
@@ -276,15 +276,15 @@
         self.ensemble = ensemble
 
     def _get_estimator(self):
         """Resolve which estimator to return (default is LinearSVC)"""
         if self.estimator is None:
             # we want all classifiers that don't expose a random_state
             # to be deterministic (and we don't want to expose this one).
-            estimator = LinearSVC(random_state=0, dual="auto")
+            estimator = LinearSVC(random_state=0)
             if _routing_enabled():
                 estimator.set_fit_request(sample_weight=True)
         else:
             estimator = self.estimator
 
         return estimator
 
@@ -384,17 +384,15 @@
             # example per class
             if isinstance(self.cv, int):
                 n_folds = self.cv
             elif hasattr(self.cv, "n_splits"):
                 n_folds = self.cv.n_splits
             else:
                 n_folds = None
-            if n_folds and np.any(
-                [np.sum(y == class_) < n_folds for class_ in self.classes_]
-            ):
+            if n_folds and np.any(np.unique(y, return_counts=True)[1] < n_folds):
                 raise ValueError(
                     f"Requesting {n_folds}-fold "
                     "cross-validation but provided less than "
                     f"{n_folds} examples for at least one class."
                 )
             cv = check_cv(self.cv, y, classifier=True)
 
@@ -521,19 +519,19 @@
             routing information.
         """
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
             .add(
                 estimator=self._get_estimator(),
-                method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+                method_mapping=MethodMapping().add(caller="fit", callee="fit"),
             )
             .add(
                 splitter=self.cv,
-                method_mapping=MethodMapping().add(callee="split", caller="fit"),
+                method_mapping=MethodMapping().add(caller="fit", callee="split"),
             )
         )
         return router
 
     def _more_tags(self):
         return {
             "_xfail_checks": {
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_affinity_propagation.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_affinity_propagation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_agglomerative.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_agglomerative.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 These routines perform some hierarchical agglomerative clustering of some
 input data.
 
 Authors : Vincent Michel, Bertrand Thirion, Alexandre Gramfort,
           Gael Varoquaux
 License: BSD 3 clause
 """
+
 import warnings
 from heapq import heapify, heappop, heappush, heappushpop
 from numbers import Integral, Real
 
 import numpy as np
 from scipy import sparse
 from scipy.sparse.csgraph import connected_components
@@ -805,15 +806,15 @@
            Let `metric` be the default value (i.e. `"euclidean"`) instead.
 
     memory : str or object with the joblib.Memory interface, default=None
         Used to cache the output of the computation of the tree.
         By default, no caching is done. If a string is given, it is the
         path to the caching directory.
 
-    connectivity : array-like or callable, default=None
+    connectivity : array-like, sparse matrix, or callable, default=None
         Connectivity matrix. Defines for each sample the neighboring
         samples following a given structure of the data.
         This can be a connectivity matrix itself or a callable that transforms
         the data into a connectivity matrix, such as derived from
         `kneighbors_graph`. Default is ``None``, i.e, the
         hierarchical clustering algorithm is unstructured.
 
@@ -841,28 +842,34 @@
           all observations of the two sets.
         - 'single' uses the minimum of the distances between all observations
           of the two sets.
 
         .. versionadded:: 0.20
             Added the 'single' option
 
+        For examples comparing different `linkage` criteria, see
+        :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`.
+
     distance_threshold : float, default=None
         The linkage distance threshold at or above which clusters will not be
         merged. If not ``None``, ``n_clusters`` must be ``None`` and
         ``compute_full_tree`` must be ``True``.
 
         .. versionadded:: 0.21
 
     compute_distances : bool, default=False
         Computes distances between clusters even if `distance_threshold` is not
         used. This can be used to make dendrogram visualization, but introduces
         a computational and memory overhead.
 
         .. versionadded:: 0.24
 
+        For an example of dendrogram visualization, see
+        :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`.
+
     Attributes
     ----------
     n_clusters_ : int
         The number of clusters found by the algorithm. If
         ``distance_threshold=None``, it will be equal to the given
         ``n_clusters``.
 
@@ -925,15 +932,15 @@
         "n_clusters": [Interval(Integral, 1, None, closed="left"), None],
         "metric": [
             StrOptions(set(_VALID_METRICS) | {"precomputed"}),
             callable,
             Hidden(None),
         ],
         "memory": [str, HasMethods("cache"), None],
-        "connectivity": ["array-like", callable, None],
+        "connectivity": ["array-like", "sparse matrix", callable, None],
         "compute_full_tree": [StrOptions({"auto"}), "boolean"],
         "linkage": [StrOptions(set(_TREE_BUILDERS.keys()))],
         "distance_threshold": [Interval(Real, 0, None, closed="left"), None],
         "compute_distances": ["boolean"],
     }
 
     def __init__(
@@ -1147,15 +1154,15 @@
            Let `metric` be the default value (i.e. `"euclidean"`) instead.
 
     memory : str or object with the joblib.Memory interface, default=None
         Used to cache the output of the computation of the tree.
         By default, no caching is done. If a string is given, it is the
         path to the caching directory.
 
-    connectivity : array-like or callable, default=None
+    connectivity : array-like, sparse matrix, or callable, default=None
         Connectivity matrix. Defines for each feature the neighboring
         features following a given structure of the data.
         This can be a connectivity matrix itself or a callable that transforms
         the data into a connectivity matrix, such as derived from
         `kneighbors_graph`. Default is `None`, i.e, the
         hierarchical clustering algorithm is unstructured.
 
@@ -1271,15 +1278,15 @@
         "n_clusters": [Interval(Integral, 1, None, closed="left"), None],
         "metric": [
             StrOptions(set(_VALID_METRICS) | {"precomputed"}),
             callable,
             Hidden(None),
         ],
         "memory": [str, HasMethods("cache"), None],
-        "connectivity": ["array-like", callable, None],
+        "connectivity": ["array-like", "sparse matrix", callable, None],
         "compute_full_tree": [StrOptions({"auto"}), "boolean"],
         "linkage": [StrOptions(set(_TREE_BUILDERS.keys()))],
         "pooling_func": [callable],
         "distance_threshold": [Interval(Real, 0, None, closed="left"), None],
         "compute_distances": ["boolean"],
     }
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_bicluster.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_bicluster.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Spectral biclustering algorithms."""
+
 # Authors : Kemal Eren
 # License: BSD 3 clause
 
 from abc import ABCMeta, abstractmethod
 from numbers import Integral
 
 import numpy as np
@@ -194,15 +195,16 @@
 
     def _more_tags(self):
         return {
             "_xfail_checks": {
                 "check_estimators_dtypes": "raises nan error",
                 "check_fit2d_1sample": "_scale_normalize fails",
                 "check_fit2d_1feature": "raises apply_along_axis error",
-                "check_estimator_sparse_data": "does not fail gracefully",
+                "check_estimator_sparse_matrix": "does not fail gracefully",
+                "check_estimator_sparse_array": "does not fail gracefully",
                 "check_methods_subset_invariance": "empty array passed inside",
                 "check_dont_overwrite_parameters": "empty array passed inside",
                 "check_fit2d_predict1d": "empty array passed inside",
             }
         }
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_birch.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_birch.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_bisect_k_means.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_bisect_k_means.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Bisecting K-means clustering."""
+
 # Author: Michal Krawczyk <mkrwczyk.1@gmail.com>
 
 import warnings
 
 import numpy as np
 import scipy.sparse as sp
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_dbscan.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_dbscan.py`

 * *Files 1% similar despite different names*

```diff
@@ -185,16 +185,18 @@
 class DBSCAN(ClusterMixin, BaseEstimator):
     """Perform DBSCAN clustering from vector array or distance matrix.
 
     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
     Finds core samples of high density and expands clusters from them.
     Good for data which contains clusters of similar density.
 
-    The worst case memory complexity of DBSCAN is :math:`O({n}^2)`, which can
-    occur when the `eps` param is large and `min_samples` is low.
+    This implementation has a worst case memory complexity of :math:`O({n}^2)`,
+    which can occur when the `eps` param is large and `min_samples` is low,
+    while the original DBSCAN only uses linear memory.
+    For further details, see the Notes below.
 
     Read more in the :ref:`User Guide <dbscan>`.
 
     Parameters
     ----------
     eps : float, default=0.5
         The maximum distance between two samples for one to be considered
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_feature_agglomeration.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_feature_agglomeration.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,37 @@
 """
 Feature agglomeration. Base classes and functions for performing feature
 agglomeration.
 """
+
 # Author: V. Michel, A. Gramfort
 # License: BSD 3 clause
 
-import warnings
 
 import numpy as np
 from scipy.sparse import issparse
 
 from ..base import TransformerMixin
 from ..utils import metadata_routing
+from ..utils.deprecation import _deprecate_Xt_in_inverse_transform
 from ..utils.validation import check_is_fitted
 
 ###############################################################################
 # Mixin class for feature agglomeration.
 
 
 class AgglomerationTransform(TransformerMixin):
     """
     A class for feature agglomeration via the transform interface.
     """
 
     # This prevents ``set_split_inverse_transform`` to be generated for the
-    # non-standard ``Xred`` arg on ``inverse_transform``.
-    # TODO(1.5): remove when Xred is removed for inverse_transform.
-    __metadata_request__inverse_transform = {"Xred": metadata_routing.UNUSED}
+    # non-standard ``Xt`` arg on ``inverse_transform``.
+    # TODO(1.7): remove when Xt is removed for inverse_transform.
+    __metadata_request__inverse_transform = {"Xt": metadata_routing.UNUSED}
 
     def transform(self, X):
         """
         Transform a new matrix using the built clustering.
 
         Parameters
         ----------
@@ -58,47 +59,34 @@
             nX = [
                 self.pooling_func(X[:, self.labels_ == l], axis=1)
                 for l in np.unique(self.labels_)
             ]
             nX = np.array(nX).T
         return nX
 
-    def inverse_transform(self, Xt=None, Xred=None):
+    def inverse_transform(self, X=None, *, Xt=None):
         """
         Inverse the transformation and return a vector of size `n_features`.
 
         Parameters
         ----------
-        Xt : array-like of shape (n_samples, n_clusters) or (n_clusters,)
+        X : array-like of shape (n_samples, n_clusters) or (n_clusters,)
             The values to be assigned to each cluster of samples.
 
-        Xred : deprecated
-            Use `Xt` instead.
+        Xt : array-like of shape (n_samples, n_clusters) or (n_clusters,)
+            The values to be assigned to each cluster of samples.
 
-            .. deprecated:: 1.3
+            .. deprecated:: 1.5
+                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.
 
         Returns
         -------
         X : ndarray of shape (n_samples, n_features) or (n_features,)
             A vector of size `n_samples` with the values of `Xred` assigned to
             each of the cluster of samples.
         """
-        if Xt is None and Xred is None:
-            raise TypeError("Missing required positional argument: Xt")
-
-        if Xred is not None and Xt is not None:
-            raise ValueError("Please provide only `Xt`, and not `Xred`.")
-
-        if Xred is not None:
-            warnings.warn(
-                (
-                    "Input argument `Xred` was renamed to `Xt` in v1.3 and will be"
-                    " removed in v1.5."
-                ),
-                FutureWarning,
-            )
-            Xt = Xred
+        X = _deprecate_Xt_in_inverse_transform(X, Xt)
 
         check_is_fitted(self)
 
         unil, inverse = np.unique(self.labels_, return_inverse=True)
-        return Xt[..., inverse]
+        return X[..., inverse]
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_linkage.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_linkage.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_reachability.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_reachability.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_tree.pxd` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_tree.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hdbscan/_tree.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/_tree.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hdbscan/hdbscan.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/hdbscan.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 HDBSCAN: Hierarchical Density-Based Spatial Clustering
          of Applications with Noise
 """
+
 # Authors: Leland McInnes <leland.mcinnes@gmail.com>
 #          Steve Astels <sastels@gmail.com>
 #          John Healy <jchealy@gmail.com>
 #          Meekail Zain <zainmeekail@gmail.com>
 # Copyright (c) 2015, Leland McInnes
 # All rights reserved.
 
@@ -40,14 +41,15 @@
 
 import numpy as np
 from scipy.sparse import csgraph, issparse
 
 from ...base import BaseEstimator, ClusterMixin, _fit_context
 from ...metrics import pairwise_distances
 from ...metrics._dist_metrics import DistanceMetric
+from ...metrics.pairwise import _VALID_METRICS
 from ...neighbors import BallTree, KDTree, NearestNeighbors
 from ...utils._param_validation import Interval, StrOptions
 from ...utils.validation import _allclose_dense_sparse, _assert_all_finite
 from ._linkage import (
     MST_edge_dtype,
     make_single_linkage,
     mst_from_data_matrix,
@@ -590,14 +592,22 @@
     See Also
     --------
     DBSCAN : Density-Based Spatial Clustering of Applications
         with Noise.
     OPTICS : Ordering Points To Identify the Clustering Structure.
     Birch : Memory-efficient, online-learning algorithm.
 
+    Notes
+    -----
+    The `min_samples` parameter includes the point itself, whereas the implementation in
+    `scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_
+    does not. To get the same results in both versions, the value of `min_samples` here
+    must be 1 greater than the value used in `scikit-learn-contrib/hdbscan
+    <https://github.com/scikit-learn-contrib/hdbscan>`_.
+
     References
     ----------
 
     .. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering
       based on hierarchical density estimates.
       <10.1007/978-3-642-37456-2_14>`
     .. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.
@@ -634,15 +644,18 @@
         "cluster_selection_epsilon": [
             Interval(Real, left=0, right=None, closed="left")
         ],
         "max_cluster_size": [
             None,
             Interval(Integral, left=1, right=None, closed="left"),
         ],
-        "metric": [StrOptions(FAST_METRICS | {"precomputed"}), callable],
+        "metric": [
+            StrOptions(FAST_METRICS | set(_VALID_METRICS) | {"precomputed"}),
+            callable,
+        ],
         "metric_params": [dict, None],
         "alpha": [Interval(Real, left=0, right=None, closed="neither")],
         # TODO(1.6): Remove "kdtree" and "balltree"  option
         "algorithm": [
             StrOptions(
                 {"auto", "brute", "kd_tree", "ball_tree", "kdtree", "balltree"},
                 deprecated={"kdtree", "balltree"},
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hdbscan/tests/test_reachibility.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hdbscan/tests/test_reachibility.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_hierarchical_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_hierarchical_fast.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_k_means_common.pxd` & `scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_common.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_k_means_common.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_common.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_k_means_elkan.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_elkan.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_k_means_lloyd.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_lloyd.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_k_means_minibatch.pyx` & `scikit_learn-1.5.0rc1/sklearn/cluster/_k_means_minibatch.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_kmeans.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_kmeans.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,28 +14,28 @@
 import warnings
 from abc import ABC, abstractmethod
 from numbers import Integral, Real
 
 import numpy as np
 import scipy.sparse as sp
 
+from .. import _threadpool_controller
 from ..base import (
     BaseEstimator,
     ClassNamePrefixFeaturesOutMixin,
     ClusterMixin,
     TransformerMixin,
     _fit_context,
 )
 from ..exceptions import ConvergenceWarning
 from ..metrics.pairwise import _euclidean_distances, euclidean_distances
 from ..utils import check_array, check_random_state
 from ..utils._openmp_helpers import _openmp_effective_n_threads
 from ..utils._param_validation import Interval, StrOptions, validate_params
 from ..utils.extmath import row_norms, stable_cumsum
-from ..utils.fixes import threadpool_info, threadpool_limits
 from ..utils.sparsefuncs import mean_variance_axis
 from ..utils.sparsefuncs_fast import assign_rows_csr
 from ..utils.validation import (
     _check_sample_weight,
     _is_arraylike_not_scalar,
     check_is_fitted,
 )
@@ -618,14 +618,17 @@
         )
 
     inertia = _inertia(X, sample_weight, centers, labels, n_threads)
 
     return labels, inertia, centers, i + 1
 
 
+# Threadpoolctl context to limit the number of threads in second level of
+# nested parallelism (i.e. BLAS) to avoid oversubscription.
+@_threadpool_controller.wrap(limits=1, user_api="blas")
 def _kmeans_single_lloyd(
     X,
     sample_weight,
     centers_init,
     max_iter=300,
     verbose=False,
     tol=1e-4,
@@ -693,67 +696,64 @@
         _inertia = _inertia_sparse
     else:
         lloyd_iter = lloyd_iter_chunked_dense
         _inertia = _inertia_dense
 
     strict_convergence = False
 
-    # Threadpoolctl context to limit the number of threads in second level of
-    # nested parallelism (i.e. BLAS) to avoid oversubscription.
-    with threadpool_limits(limits=1, user_api="blas"):
-        for i in range(max_iter):
-            lloyd_iter(
-                X,
-                sample_weight,
-                centers,
-                centers_new,
-                weight_in_clusters,
-                labels,
-                center_shift,
-                n_threads,
-            )
+    for i in range(max_iter):
+        lloyd_iter(
+            X,
+            sample_weight,
+            centers,
+            centers_new,
+            weight_in_clusters,
+            labels,
+            center_shift,
+            n_threads,
+        )
 
-            if verbose:
-                inertia = _inertia(X, sample_weight, centers, labels, n_threads)
-                print(f"Iteration {i}, inertia {inertia}.")
+        if verbose:
+            inertia = _inertia(X, sample_weight, centers, labels, n_threads)
+            print(f"Iteration {i}, inertia {inertia}.")
 
-            centers, centers_new = centers_new, centers
+        centers, centers_new = centers_new, centers
 
-            if np.array_equal(labels, labels_old):
-                # First check the labels for strict convergence.
+        if np.array_equal(labels, labels_old):
+            # First check the labels for strict convergence.
+            if verbose:
+                print(f"Converged at iteration {i}: strict convergence.")
+            strict_convergence = True
+            break
+        else:
+            # No strict convergence, check for tol based convergence.
+            center_shift_tot = (center_shift**2).sum()
+            if center_shift_tot <= tol:
                 if verbose:
-                    print(f"Converged at iteration {i}: strict convergence.")
-                strict_convergence = True
+                    print(
+                        f"Converged at iteration {i}: center shift "
+                        f"{center_shift_tot} within tolerance {tol}."
+                    )
                 break
-            else:
-                # No strict convergence, check for tol based convergence.
-                center_shift_tot = (center_shift**2).sum()
-                if center_shift_tot <= tol:
-                    if verbose:
-                        print(
-                            f"Converged at iteration {i}: center shift "
-                            f"{center_shift_tot} within tolerance {tol}."
-                        )
-                    break
 
-            labels_old[:] = labels
+        labels_old[:] = labels
 
-        if not strict_convergence:
-            # rerun E-step so that predicted labels match cluster centers
-            lloyd_iter(
-                X,
-                sample_weight,
-                centers,
-                centers,
-                weight_in_clusters,
-                labels,
-                center_shift,
-                n_threads,
-                update_centers=False,
-            )
+    if not strict_convergence:
+        # rerun E-step so that predicted labels match cluster centers
+        lloyd_iter(
+            X,
+            sample_weight,
+            centers,
+            centers,
+            weight_in_clusters,
+            labels,
+            center_shift,
+            n_threads,
+            update_centers=False,
+        )
 
     inertia = _inertia(X, sample_weight, centers, labels, n_threads)
 
     return labels, inertia, centers, i + 1
 
 
 def _labels_inertia(X, sample_weight, centers, n_threads=1, return_inertia=True):
@@ -822,22 +822,18 @@
     if return_inertia:
         inertia = _inertia(X, sample_weight, centers, labels, n_threads)
         return labels, inertia
 
     return labels
 
 
-def _labels_inertia_threadpool_limit(
-    X, sample_weight, centers, n_threads=1, return_inertia=True
-):
-    """Same as _labels_inertia but in a threadpool_limits context."""
-    with threadpool_limits(limits=1, user_api="blas"):
-        result = _labels_inertia(X, sample_weight, centers, n_threads, return_inertia)
-
-    return result
+# Same as _labels_inertia but in a threadpool_limits context.
+_labels_inertia_threadpool_limit = _threadpool_controller.wrap(
+    limits=1, user_api="blas"
+)(_labels_inertia)
 
 
 class _BaseKMeans(
     ClassNamePrefixFeaturesOutMixin, TransformerMixin, ClusterMixin, BaseEstimator, ABC
 ):
     """Base class for KMeans and MiniBatchKMeans"""
 
@@ -922,15 +918,15 @@
         # of available threads. It only happens when the OpenMP library is
         # vcomp (microsoft OpenMP) and the BLAS library is MKL. see #18653
         if sp.issparse(X):
             return
 
         n_active_threads = int(np.ceil(n_samples / CHUNK_SIZE))
         if n_active_threads < self._n_threads:
-            modules = threadpool_info()
+            modules = _threadpool_controller.info()
             has_vcomp = "vcomp" in [module["prefix"] for module in modules]
             has_mkl = ("mkl", "intel") in [
                 (module["internal_api"], module.get("threading_layer", None))
                 for module in modules
             ]
             if has_vcomp and has_mkl:
                 self._warn_mkl_vcomp(n_active_threads)
@@ -1066,53 +1062,37 @@
         Returns
         -------
         labels : ndarray of shape (n_samples,)
             Index of the cluster each sample belongs to.
         """
         return self.fit(X, sample_weight=sample_weight).labels_
 
-    def predict(self, X, sample_weight="deprecated"):
+    def predict(self, X):
         """Predict the closest cluster each sample in X belongs to.
 
         In the vector quantization literature, `cluster_centers_` is called
         the code book and each value returned by `predict` is the index of
         the closest code in the code book.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             New data to predict.
 
-        sample_weight : array-like of shape (n_samples,), default=None
-            The weights for each observation in X. If None, all observations
-            are assigned equal weight.
-
-            .. deprecated:: 1.3
-               The parameter `sample_weight` is deprecated in version 1.3
-               and will be removed in 1.5.
-
         Returns
         -------
         labels : ndarray of shape (n_samples,)
             Index of the cluster each sample belongs to.
         """
         check_is_fitted(self)
 
         X = self._check_test_data(X)
-        if not (isinstance(sample_weight, str) and sample_weight == "deprecated"):
-            warnings.warn(
-                (
-                    "'sample_weight' was deprecated in version 1.3 and "
-                    "will be removed in 1.5."
-                ),
-                FutureWarning,
-            )
-            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
-        else:
-            sample_weight = _check_sample_weight(None, X, dtype=X.dtype)
+
+        # sample weights are not used by predict but cython helpers expect an array
+        sample_weight = np.ones(X.shape[0], dtype=X.dtype)
 
         labels = _labels_inertia_threadpool_limit(
             X,
             sample_weight,
             self.cluster_centers_,
             n_threads=self._n_threads,
             return_inertia=False,
@@ -2160,15 +2140,15 @@
         self._no_improvement = 0
 
         # Initialize number of samples seen since last reassignment
         self._n_since_last_reassign = 0
 
         n_steps = (self.max_iter * n_samples) // self._batch_size
 
-        with threadpool_limits(limits=1, user_api="blas"):
+        with _threadpool_controller.limit(limits=1, user_api="blas"):
             # Perform the iterative optimization until convergence
             for i in range(n_steps):
                 # Sample a minibatch from the full dataset
                 minibatch_indices = random_state.randint(0, n_samples, self._batch_size)
 
                 # Perform the actual update step on the minibatch data
                 batch_inertia = _mini_batch_step(
@@ -2286,15 +2266,15 @@
 
             # Initialize counts
             self._counts = np.zeros(self.n_clusters, dtype=X.dtype)
 
             # Initialize number of samples seen since last reassignment
             self._n_since_last_reassign = 0
 
-        with threadpool_limits(limits=1, user_api="blas"):
+        with _threadpool_controller.limit(limits=1, user_api="blas"):
             _mini_batch_step(
                 X,
                 sample_weight=sample_weight,
                 centers=self.cluster_centers_,
                 centers_new=self.cluster_centers_,
                 weight_sums=self._counts,
                 random_state=self._random_state,
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_mean_shift.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_mean_shift.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_optics.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_optics.py`

 * *Files 0% similar despite different names*

```diff
@@ -17,15 +17,16 @@
 from scipy.sparse import SparseEfficiencyWarning, issparse
 
 from ..base import BaseEstimator, ClusterMixin, _fit_context
 from ..exceptions import DataConversionWarning
 from ..metrics import pairwise_distances
 from ..metrics.pairwise import _VALID_METRICS, PAIRWISE_BOOLEAN_FUNCTIONS
 from ..neighbors import NearestNeighbors
-from ..utils import gen_batches, get_chunk_n_rows
+from ..utils import gen_batches
+from ..utils._chunking import get_chunk_n_rows
 from ..utils._param_validation import (
     HasMethods,
     Interval,
     RealNotInt,
     StrOptions,
     validate_params,
 )
@@ -329,14 +330,15 @@
                 f" metric {self.metric}, to avoid this warning,"
                 " you may convert the data prior to calling fit."
             )
             warnings.warn(msg, DataConversionWarning)
 
         X = self._validate_data(X, dtype=dtype, accept_sparse="csr")
         if self.metric == "precomputed" and issparse(X):
+            X = X.copy()  # copy to avoid in-place modification
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore", SparseEfficiencyWarning)
                 # Set each diagonal to an explicit value so each point is its
                 # own neighbor
                 X.setdiag(X.diagonal())
         memory = check_memory(self.memory)
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/_spectral.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/_spectral.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 from numbers import Integral, Real
 
 import numpy as np
 from scipy.linalg import LinAlgError, qr, svd
 from scipy.sparse import csc_matrix
 
 from ..base import BaseEstimator, ClusterMixin, _fit_context
-from ..manifold import spectral_embedding
+from ..manifold._spectral_embedding import _spectral_embedding
 from ..metrics.pairwise import KERNEL_PARAMS, pairwise_kernels
 from ..neighbors import NearestNeighbors, kneighbors_graph
 from ..utils import as_float_array, check_random_state
 from ..utils._param_validation import Interval, StrOptions, validate_params
 from ._kmeans import k_means
 
 
@@ -434,15 +434,16 @@
         Number of time the k-means algorithm will be run with different
         centroid seeds. The final results will be the best output of n_init
         consecutive runs in terms of inertia. Only used if
         ``assign_labels='kmeans'``.
 
     gamma : float, default=1.0
         Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.
-        Ignored for ``affinity='nearest_neighbors'``.
+        Ignored for ``affinity='nearest_neighbors'``, ``affinity='precomputed'``
+        or ``affinity='precomputed_nearest_neighbors'``.
 
     affinity : str or callable, default='rbf'
         How to construct the affinity matrix.
          - 'nearest_neighbors': construct the affinity matrix by computing a
            graph of nearest neighbors.
          - 'rbf': construct the affinity matrix using a radial basis function
            (RBF) kernel.
@@ -736,15 +737,15 @@
         )
         # We now obtain the real valued solution matrix to the
         # relaxed Ncut problem, solving the eigenvalue problem
         # L_sym x = lambda x  and recovering u = D^-1/2 x.
         # The first eigenvector is constant only for fully connected graphs
         # and should be kept for spectral clustering (drop_first = False)
         # See spectral_embedding documentation.
-        maps = spectral_embedding(
+        maps = _spectral_embedding(
             self.affinity_matrix_,
             n_components=n_components,
             eigen_solver=self.eigen_solver,
             random_state=random_state,
             eigen_tol=self.eigen_tol,
             drop_first=False,
         )
@@ -788,12 +789,13 @@
         labels : ndarray of shape (n_samples,)
             Cluster labels.
         """
         return super().fit_predict(X, y)
 
     def _more_tags(self):
         return {
-            "pairwise": self.affinity in [
+            "pairwise": self.affinity
+            in [
                 "precomputed",
                 "precomputed_nearest_neighbors",
             ]
         }
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/meson.build` & `scikit_learn-1.5.0rc1/sklearn/cluster/meson.build`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 cluster_extension_metadata = {
   '_dbscan_inner':
-    {'sources': ['_dbscan_inner.pyx'],'override_options': ['cython_language=cpp']},
+    {'sources': ['_dbscan_inner.pyx'], 'override_options': ['cython_language=cpp']},
   '_hierarchical_fast':
-    {'sources': ['_hierarchical_fast.pyx'], 'override_options': ['cython_language=cpp']},
+    {'sources': ['_hierarchical_fast.pyx', metrics_cython_tree],
+     'override_options': ['cython_language=cpp']},
   '_k_means_common':
     {'sources': ['_k_means_common.pyx']},
   '_k_means_lloyd':
     {'sources': ['_k_means_lloyd.pyx']},
   '_k_means_elkan':
     {'sources': ['_k_means_elkan.pyx']},
   '_k_means_minibatch':
     {'sources': ['_k_means_minibatch.pyx']},
 }
 
 foreach ext_name, ext_dict : cluster_extension_metadata
   py.extension_module(
     ext_name,
-    ext_dict.get('sources') + [utils_cython_tree],
+    [ext_dict.get('sources'), utils_cython_tree],
     dependencies: [np_dep, openmp_dep],
     override_options : ext_dict.get('override_options', []),
     cython_args: cython_args,
     subdir: 'sklearn/cluster',
     install: true
   )
 endforeach
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/common.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/common.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_affinity_propagation.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_affinity_propagation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_bicluster.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_bicluster.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_birch.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_birch.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_bisect_k_means.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_bisect_k_means.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_dbscan.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_dbscan.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_feature_agglomeration.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_feature_agglomeration.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Tests for sklearn.cluster._feature_agglomeration
 """
+
 # Authors: Sergul Aydore 2017
 import warnings
 
 import numpy as np
 import pytest
 from numpy.testing import assert_array_equal
 
@@ -54,27 +55,27 @@
 
     names_out = agglo.get_feature_names_out()
     assert_array_equal(
         [f"featureagglomeration{i}" for i in range(n_clusters)], names_out
     )
 
 
-# TODO(1.5): remove this test
-def test_inverse_transform_Xred_deprecation():
+# TODO(1.7): remove this test
+def test_inverse_transform_Xt_deprecation():
     X = np.array([0, 0, 1]).reshape(1, 3)  # (n_samples, n_features)
 
     est = FeatureAgglomeration(n_clusters=1, pooling_func=np.mean)
     est.fit(X)
-    Xt = est.transform(X)
+    X = est.transform(X)
 
     with pytest.raises(TypeError, match="Missing required positional argument"):
         est.inverse_transform()
 
-    with pytest.raises(ValueError, match="Please provide only"):
-        est.inverse_transform(Xt=Xt, Xred=Xt)
+    with pytest.raises(TypeError, match="Cannot use both X and Xt. Use X only."):
+        est.inverse_transform(X=X, Xt=X)
 
     with warnings.catch_warnings(record=True):
         warnings.simplefilter("error")
-        est.inverse_transform(Xt)
+        est.inverse_transform(X)
 
-    with pytest.warns(FutureWarning, match="Input argument `Xred` was renamed to `Xt`"):
-        est.inverse_transform(Xred=Xt)
+    with pytest.warns(FutureWarning, match="Xt was renamed X in version 1.5"):
+        est.inverse_transform(Xt=X)
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_hdbscan.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_hdbscan.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 Tests for HDBSCAN clustering algorithm
 Based on the DBSCAN test code
 """
+
 import numpy as np
 import pytest
 from scipy import stats
 from scipy.spatial import distance
 
 from sklearn.cluster import HDBSCAN
 from sklearn.cluster._hdbscan._tree import (
@@ -575,7 +576,27 @@
     """
     rng = np.random.RandomState(0)
     X = rng.random((100, 2))
     X_dist = euclidean_distances(X)
     err_msg = "Cannot store centers when using a precomputed distance matrix."
     with pytest.raises(ValueError, match=err_msg):
         HDBSCAN(metric="precomputed", store_centers=store_centers).fit(X_dist)
+
+
+@pytest.mark.parametrize("valid_algo", ["auto", "brute"])
+def test_hdbscan_cosine_metric_valid_algorithm(valid_algo):
+    """Test that HDBSCAN works with the "cosine" metric when the algorithm is set
+    to "brute" or "auto".
+
+    Non-regression test for issue #28631
+    """
+    HDBSCAN(metric="cosine", algorithm=valid_algo).fit_predict(X)
+
+
+@pytest.mark.parametrize("invalid_algo", ["kd_tree", "ball_tree"])
+def test_hdbscan_cosine_metric_invalid_algorithm(invalid_algo):
+    """Test that HDBSCAN raises an informative error is raised when an unsupported
+    algorithm is used with the "cosine" metric.
+    """
+    hdbscan = HDBSCAN(metric="cosine", algorithm=invalid_algo)
+    with pytest.raises(ValueError, match="cosine is not a valid metric"):
+        hdbscan.fit_predict(X)
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_hierarchical.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_hierarchical.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 Several basic tests for hierarchical clustering procedures
 
 """
+
 # Authors: Vincent Michel, 2010, Gael Varoquaux 2012,
 #          Matteo Visconti di Oleggio Castello 2014
 # License: BSD 3 clause
 import itertools
 import shutil
 from functools import partial
 from tempfile import mkdtemp
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_k_means.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_k_means.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 """Testing for K-means"""
+
 import re
 import sys
 from io import StringIO
 
 import numpy as np
 import pytest
 from scipy import sparse as sp
 
+from sklearn import _threadpool_controller
 from sklearn.base import clone
 from sklearn.cluster import KMeans, MiniBatchKMeans, k_means, kmeans_plusplus
 from sklearn.cluster._k_means_common import (
     _euclidean_dense_dense_wrapper,
     _euclidean_sparse_dense_wrapper,
     _inertia_dense,
     _inertia_sparse,
@@ -26,15 +28,15 @@
 from sklearn.metrics.pairwise import euclidean_distances
 from sklearn.utils._testing import (
     assert_allclose,
     assert_array_equal,
     create_memmap_backed_data,
 )
 from sklearn.utils.extmath import row_norms
-from sklearn.utils.fixes import CSR_CONTAINERS, threadpool_limits
+from sklearn.utils.fixes import CSR_CONTAINERS
 
 # non centered, sparse centers to check the
 centers = np.array(
     [
         [0.0, 5.0, 0.0, 0.0, 0.0],
         [1.0, 1.0, 4.0, 0.0, 0.0],
         [1.0, 0.0, 0.0, 5.0, 1.0],
@@ -196,27 +198,14 @@
         tol=0,
         max_iter=max_iter,
     ).fit(X)
 
     assert km.n_iter_ < max_iter
 
 
-@pytest.mark.parametrize("Estimator", [KMeans, MiniBatchKMeans])
-def test_predict_sample_weight_deprecation_warning(Estimator):
-    X = np.random.rand(100, 2)
-    sample_weight = np.random.uniform(size=100)
-    kmeans = Estimator()
-    kmeans.fit(X, sample_weight=sample_weight)
-    warn_msg = (
-        "'sample_weight' was deprecated in version 1.3 and will be removed in 1.5."
-    )
-    with pytest.warns(FutureWarning, match=warn_msg):
-        kmeans.predict(X, sample_weight=sample_weight)
-
-
 @pytest.mark.parametrize("X_csr", X_as_any_csr)
 def test_minibatch_update_consistency(X_csr, global_random_seed):
     # Check that dense and sparse minibatch update give the same results
     rng = np.random.RandomState(global_random_seed)
 
     centers_old = centers + rng.normal(size=centers.shape)
     centers_old_csr = centers_old.copy()
@@ -975,21 +964,21 @@
 @pytest.mark.parametrize("Estimator", [KMeans, MiniBatchKMeans])
 def test_result_equal_in_diff_n_threads(Estimator, global_random_seed):
     # Check that KMeans/MiniBatchKMeans give the same results in parallel mode
     # than in sequential mode.
     rnd = np.random.RandomState(global_random_seed)
     X = rnd.normal(size=(50, 10))
 
-    with threadpool_limits(limits=1, user_api="openmp"):
+    with _threadpool_controller.limit(limits=1, user_api="openmp"):
         result_1 = (
             Estimator(n_clusters=n_clusters, random_state=global_random_seed)
             .fit(X)
             .labels_
         )
-    with threadpool_limits(limits=2, user_api="openmp"):
+    with _threadpool_controller.limit(limits=2, user_api="openmp"):
         result_2 = (
             Estimator(n_clusters=n_clusters, random_state=global_random_seed)
             .fit(X)
             .labels_
         )
     assert_array_equal(result_1, result_2)
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_mean_shift.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_mean_shift.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_optics.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_optics.py`

 * *Files 2% similar despite different names*

```diff
@@ -812,14 +812,35 @@
         )
     clust2 = OPTICS(min_samples=10, algorithm="brute", metric="euclidean").fit(redX)
 
     assert_allclose(clust1.reachability_, clust2.reachability_)
     assert_array_equal(clust1.labels_, clust2.labels_)
 
 
+@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
+def test_optics_input_not_modified_precomputed_sparse_nodiag(csr_container):
+    """Check that we don't modify in-place the pre-computed sparse matrix.
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/27508
+    """
+    X = np.random.RandomState(0).rand(6, 6)
+    # Add zeros on the diagonal that will be implicit when creating
+    # the sparse matrix. If `X` is modified in-place, the zeros from
+    # the diagonal will be made explicit.
+    np.fill_diagonal(X, 0)
+    X = csr_container(X)
+    assert all(row != col for row, col in zip(*X.nonzero()))
+    X_copy = X.copy()
+    OPTICS(metric="precomputed").fit(X)
+    # Make sure that we did not modify `X` in-place even by creating
+    # explicit 0s values.
+    assert X.nnz == X_copy.nnz
+    assert_array_equal(X.toarray(), X_copy.toarray())
+
+
 def test_optics_predecessor_correction_ordering():
     """Check that cluster correction using predecessor is working as expected.
 
     In the following example, the predecessor correction was not working properly
     since it was not using the right indices.
 
     This non-regression test check that reordering the data does not change the results.
```

### Comparing `scikit-learn-1.4.2/sklearn/cluster/tests/test_spectral.py` & `scikit_learn-1.5.0rc1/sklearn/cluster/tests/test_spectral.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Testing for Spectral Clustering methods"""
+
 import pickle
 import re
 
 import numpy as np
 import pytest
 from scipy.linalg import LinAlgError
```

### Comparing `scikit-learn-1.4.2/sklearn/compose/_column_transformer.py` & `scikit_learn-1.5.0rc1/sklearn/compose/_column_transformer.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,26 +4,27 @@
 different columns.
 """
 
 # Author: Andreas Mueller
 #         Joris Van den Bossche
 # License: BSD
 import warnings
-from collections import Counter
+from collections import Counter, UserList
 from itertools import chain
 from numbers import Integral, Real
 
 import numpy as np
 from scipy import sparse
 
 from ..base import TransformerMixin, _fit_context, clone
 from ..pipeline import _fit_transform_one, _name_estimators, _transform_one
 from ..preprocessing import FunctionTransformer
-from ..utils import Bunch, _get_column_indices, _safe_indexing
+from ..utils import Bunch
 from ..utils._estimator_html_repr import _VisualBlock
+from ..utils._indexing import _determine_key_type, _get_column_indices
 from ..utils._metadata_requests import METHODS
 from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions
 from ..utils._set_output import (
     _get_container_adapter,
     _get_output_config,
     _safe_set_output,
 )
@@ -138,28 +139,56 @@
         feature.
         If False, :meth:`ColumnTransformer.get_feature_names_out` will not
         prefix any feature names and will error if feature names are not
         unique.
 
         .. versionadded:: 1.0
 
+    force_int_remainder_cols : bool, default=True
+        Force the columns of the last entry of `transformers_`, which
+        corresponds to the "remainder" transformer, to always be stored as
+        indices (int) rather than column names (str). See description of the
+        `transformers_` attribute for details.
+
+        .. note::
+            If you do not access the list of columns for the remainder columns
+            in the `transformers_` fitted attribute, you do not need to set
+            this parameter.
+
+        .. versionadded:: 1.5
+
+        .. versionchanged:: 1.7
+           The default value for `force_int_remainder_cols` will change from
+           `True` to `False` in version 1.7.
+
     Attributes
     ----------
     transformers_ : list
         The collection of fitted transformers as tuples of (name,
         fitted_transformer, column). `fitted_transformer` can be an estimator,
         or `'drop'`; `'passthrough'` is replaced with an equivalent
         :class:`~sklearn.preprocessing.FunctionTransformer`. In case there were
         no columns selected, this will be the unfitted transformer. If there
         are remaining columns, the final element is a tuple of the form:
         ('remainder', transformer, remaining_columns) corresponding to the
         ``remainder`` parameter. If there are remaining columns, then
         ``len(transformers_)==len(transformers)+1``, otherwise
         ``len(transformers_)==len(transformers)``.
 
+        .. versionchanged:: 1.5
+            If there are remaining columns and `force_int_remainder_cols` is
+            True, the remaining columns are always represented by their
+            positional indices in the input `X` (as in older versions). If
+            `force_int_remainder_cols` is False, the format attempts to match
+            that of the other transformers: if all columns were provided as
+            column names (`str`), the remaining columns are stored as column
+            names; if all columns were provided as mask arrays (`bool`), so are
+            the remaining columns; in all other cases the remaining columns are
+            stored as indices (`int`).
+
     named_transformers_ : :class:`~sklearn.utils.Bunch`
         Read-only attribute to access any transformer by given name.
         Keys are transformer names and values are the fitted transformer
         objects.
 
     sparse_output_ : bool
         Boolean flag indicating whether the output of ``transform`` is a
@@ -251,34 +280,37 @@
             HasMethods(["fit_transform", "transform"]),
         ],
         "sparse_threshold": [Interval(Real, 0, 1, closed="both")],
         "n_jobs": [Integral, None],
         "transformer_weights": [dict, None],
         "verbose": ["verbose"],
         "verbose_feature_names_out": ["boolean"],
+        "force_int_remainder_cols": ["boolean"],
     }
 
     def __init__(
         self,
         transformers,
         *,
         remainder="drop",
         sparse_threshold=0.3,
         n_jobs=None,
         transformer_weights=None,
         verbose=False,
         verbose_feature_names_out=True,
+        force_int_remainder_cols=True,
     ):
         self.transformers = transformers
         self.remainder = remainder
         self.sparse_threshold = sparse_threshold
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
         self.verbose = verbose
         self.verbose_feature_names_out = verbose_feature_names_out
+        self.force_int_remainder_cols = force_int_remainder_cols
 
     @property
     def _transformers(self):
         """
         Internal list of transformer only containing the name and
         transformers, dropping the columns.
 
@@ -309,15 +341,15 @@
         """Set the output container when `"transform"` and `"fit_transform"` are called.
 
         Calling `set_output` will set the output of all estimators in `transformers`
         and `transformers_`.
 
         Parameters
         ----------
-        transform : {"default", "pandas"}, default=None
+        transform : {"default", "pandas", "polars"}, default=None
             Configure output of `transform` and `fit_transform`.
 
             - `"default"`: Default output format of a transformer
             - `"pandas"`: DataFrame output
             - `"polars"`: Polars output
             - `None`: Transform configuration is unchanged
 
@@ -384,15 +416,15 @@
             This estimator.
         """
         self._set_params("_transformers", **kwargs)
         return self
 
     def _iter(self, fitted, column_as_labels, skip_drop, skip_empty_columns):
         """
-        Generate (name, trans, column, weight) tuples.
+        Generate (name, trans, columns, weight) tuples.
 
 
         Parameters
         ----------
         fitted : bool
             If True, use the fitted transformers (``self.transformers_``) to
             iterate through transformers, else use the transformers passed by
@@ -424,14 +456,22 @@
             transformers = [
                 (name, trans, column)
                 for (name, trans, _), column in zip(self.transformers, self._columns)
             ]
             # add transformer tuple for remainder
             if self._remainder[2]:
                 transformers = chain(transformers, [self._remainder])
+
+        # We want the warning about the future change of the remainder
+        # columns dtype to be shown only when a user accesses them
+        # directly, not when they are used by the ColumnTransformer itself.
+        # We disable warnings here; they are enabled when setting
+        # self.transformers_.
+        transformers = _with_dtype_warning_enabled_set_to(False, transformers)
+
         get_weight = (self.transformer_weights or {}).get
 
         for name, trans, columns in transformers:
             if skip_drop and trans == "drop":
                 continue
             if skip_empty_columns and _is_empty_column_selection(columns):
                 continue
@@ -501,16 +541,38 @@
     def _validate_remainder(self, X):
         """
         Validates ``remainder`` and defines ``_remainder`` targeting
         the remaining columns.
         """
         cols = set(chain(*self._transformer_to_input_indices.values()))
         remaining = sorted(set(range(self.n_features_in_)) - cols)
-        self._remainder = ("remainder", self.remainder, remaining)
         self._transformer_to_input_indices["remainder"] = remaining
+        remainder_cols = self._get_remainder_cols(remaining)
+        self._remainder = ("remainder", self.remainder, remainder_cols)
+
+    def _get_remainder_cols_dtype(self):
+        try:
+            all_dtypes = {_determine_key_type(c) for (*_, c) in self.transformers}
+            if len(all_dtypes) == 1:
+                return next(iter(all_dtypes))
+        except ValueError:
+            # _determine_key_type raises a ValueError if some transformer
+            # columns are Callables
+            return "int"
+        return "int"
+
+    def _get_remainder_cols(self, indices):
+        dtype = self._get_remainder_cols_dtype()
+        if self.force_int_remainder_cols and dtype != "int":
+            return _RemainderColsList(indices, future_dtype=dtype)
+        if dtype == "str":
+            return list(self.feature_names_in_[indices])
+        if dtype == "bool":
+            return [i in indices for i in range(self.n_features_in_)]
+        return indices
 
     @property
     def named_transformers_(self):
         """Access the fitted transformer by name.
 
         Read-only attribute to access any transformer by given name.
         Keys are transformer names and values are the fitted transformer
@@ -657,15 +719,15 @@
                 trans = old
             else:
                 trans = next(fitted_transformers)
             transformers_.append((name, trans, column))
 
         # sanity check that transformers is exhausted
         assert not list(fitted_transformers)
-        self.transformers_ = transformers_
+        self.transformers_ = _with_dtype_warning_enabled_set_to(True, transformers_)
 
     def _validate_output(self, result):
         """
         Ensure that the output of each transformer is 2D. Otherwise
         hstack can raise an error or produce incorrect results.
         """
         names = [
@@ -789,15 +851,15 @@
                 column_as_labels=column_as_labels,
                 skip_drop=True,
                 skip_empty_columns=True,
             )
         )
         try:
             jobs = []
-            for idx, (name, trans, column, weight) in enumerate(transformers, start=1):
+            for idx, (name, trans, columns, weight) in enumerate(transformers, start=1):
                 if func is _fit_transform_one:
                     if trans == "passthrough":
                         output_config = _get_output_config("transform", self)
                         trans = FunctionTransformer(
                             accept_sparse=True,
                             check_inverse=False,
                             feature_names_out="one-to-one",
@@ -808,17 +870,18 @@
                         message=self._log_message(name, idx, len(transformers)),
                     )
                 else:  # func is _transform_one
                     extra_args = {}
                 jobs.append(
                     delayed(func)(
                         transformer=clone(trans) if not fitted else trans,
-                        X=_safe_indexing(X, column, axis=1),
+                        X=X,
                         y=y,
                         weight=weight,
+                        columns=columns,
                         **extra_args,
                         params=routed_params[name],
                     )
                 )
 
             return Parallel(n_jobs=self.n_jobs)(jobs)
 
@@ -1155,14 +1218,24 @@
             transformers = chain(self.transformers, [("remainder", self.remainder, "")])
 
         names, transformers, name_details = zip(*transformers)
         return _VisualBlock(
             "parallel", transformers, names=names, name_details=name_details
         )
 
+    def __getitem__(self, key):
+        try:
+            return self.named_transformers_[key]
+        except AttributeError as e:
+            raise TypeError(
+                "ColumnTransformer is subscriptable after it is fitted"
+            ) from e
+        except KeyError as e:
+            raise KeyError(f"'{key}' is not a valid transformer name") from e
+
     def _get_empty_routing(self):
         """Return empty routing.
 
         Used while routing can be disabled.
 
         TODO: Remove when ``set_config(enable_metadata_routing=False)`` is no
         more an option.
@@ -1262,14 +1335,15 @@
 def make_column_transformer(
     *transformers,
     remainder="drop",
     sparse_threshold=0.3,
     n_jobs=None,
     verbose=False,
     verbose_feature_names_out=True,
+    force_int_remainder_cols=True,
 ):
     """Construct a ColumnTransformer from the given transformers.
 
     This is a shorthand for the ColumnTransformer constructor; it does not
     require, and does not permit, naming the transformers. Instead, they will
     be given names automatically based on their types. It also does not allow
     weighting with ``transformer_weights``.
@@ -1334,14 +1408,31 @@
         feature.
         If False, :meth:`ColumnTransformer.get_feature_names_out` will not
         prefix any feature names and will error if feature names are not
         unique.
 
         .. versionadded:: 1.0
 
+    force_int_remainder_cols : bool, default=True
+        Force the columns of the last entry of `transformers_`, which
+        corresponds to the "remainder" transformer, to always be stored as
+        indices (int) rather than column names (str). See description of the
+        :attr:`ColumnTransformer.transformers_` attribute for details.
+
+        .. note::
+            If you do not access the list of columns for the remainder columns
+            in the :attr:`ColumnTransformer.transformers_` fitted attribute,
+            you do not need to set this parameter.
+
+        .. versionadded:: 1.5
+
+        .. versionchanged:: 1.7
+           The default value for `force_int_remainder_cols` will change from
+           `True` to `False` in version 1.7.
+
     Returns
     -------
     ct : ColumnTransformer
         Returns a :class:`ColumnTransformer` object.
 
     See Also
     --------
@@ -1367,14 +1458,15 @@
     return ColumnTransformer(
         transformer_list,
         n_jobs=n_jobs,
         remainder=remainder,
         sparse_threshold=sparse_threshold,
         verbose=verbose,
         verbose_feature_names_out=verbose_feature_names_out,
+        force_int_remainder_cols=force_int_remainder_cols,
     )
 
 
 class make_column_selector:
     """Create a callable to select columns to be used with
     :class:`ColumnTransformer`.
 
@@ -1457,7 +1549,106 @@
             df_row = df_row.select_dtypes(
                 include=self.dtype_include, exclude=self.dtype_exclude
             )
         cols = df_row.columns
         if self.pattern is not None:
             cols = cols[cols.str.contains(self.pattern, regex=True)]
         return cols.tolist()
+
+
+class _RemainderColsList(UserList):
+    """A list that raises a warning whenever items are accessed.
+
+    It is used to store the columns handled by the "remainder" entry of
+    ``ColumnTransformer.transformers_``, ie ``transformers_[-1][-1]``.
+
+    For some values of the ``ColumnTransformer`` ``transformers`` parameter,
+    this list of indices will be replaced by either a list of column names or a
+    boolean mask; in those cases we emit a ``FutureWarning`` the first time an
+    element is accessed.
+
+    Parameters
+    ----------
+    columns : list of int
+        The remainder columns.
+
+    future_dtype : {'str', 'bool'}, default=None
+        The dtype that will be used by a ColumnTransformer with the same inputs
+        in a future release. There is a default value because providing a
+        constructor that takes a single argument is a requirement for
+        subclasses of UserList, but we do not use it in practice. It would only
+        be used if a user called methods that return a new list such are
+        copying or concatenating `_RemainderColsList`.
+
+    warning_was_emitted : bool, default=False
+       Whether the warning for that particular list was already shown, so we
+       only emit it once.
+
+    warning_enabled : bool, default=True
+        When False, the list never emits the warning nor updates
+        `warning_was_emitted``. This is used to obtain a quiet copy of the list
+        for use by the `ColumnTransformer` itself, so that the warning is only
+        shown when a user accesses it directly.
+    """
+
+    def __init__(
+        self,
+        columns,
+        *,
+        future_dtype=None,
+        warning_was_emitted=False,
+        warning_enabled=True,
+    ):
+        super().__init__(columns)
+        self.future_dtype = future_dtype
+        self.warning_was_emitted = warning_was_emitted
+        self.warning_enabled = warning_enabled
+
+    def __getitem__(self, index):
+        self._show_remainder_cols_warning()
+        return super().__getitem__(index)
+
+    def _show_remainder_cols_warning(self):
+        if self.warning_was_emitted or not self.warning_enabled:
+            return
+        self.warning_was_emitted = True
+        future_dtype_description = {
+            "str": "column names (of type str)",
+            "bool": "a mask array (of type bool)",
+            # shouldn't happen because we always initialize it with a
+            # non-default future_dtype
+            None: "a different type depending on the ColumnTransformer inputs",
+        }.get(self.future_dtype, self.future_dtype)
+
+        # TODO(1.7) Update the warning to say that the old behavior will be
+        # removed in 1.9.
+        warnings.warn(
+            (
+                "\nThe format of the columns of the 'remainder' transformer in"
+                " ColumnTransformer.transformers_ will change in version 1.7 to"
+                " match the format of the other transformers.\nAt the moment the"
+                " remainder columns are stored as indices (of type int). With the same"
+                " ColumnTransformer configuration, in the future they will be stored"
+                f" as {future_dtype_description}.\nTo use the new behavior now and"
+                " suppress this warning, use"
+                " ColumnTransformer(force_int_remainder_cols=False).\n"
+            ),
+            category=FutureWarning,
+        )
+
+    def _repr_pretty_(self, printer, *_):
+        """Override display in ipython console, otherwise the class name is shown."""
+        printer.text(repr(self.data))
+
+
+def _with_dtype_warning_enabled_set_to(warning_enabled, transformers):
+    result = []
+    for name, trans, columns in transformers:
+        if isinstance(columns, _RemainderColsList):
+            columns = _RemainderColsList(
+                columns.data,
+                future_dtype=columns.future_dtype,
+                warning_was_emitted=columns.warning_was_emitted,
+                warning_enabled=warning_enabled,
+            )
+        result.append((name, trans, columns))
+    return result
```

### Comparing `scikit-learn-1.4.2/sklearn/compose/_target.py` & `scikit_learn-1.5.0rc1/sklearn/compose/_target.py`

 * *Files 3% similar despite different names*

```diff
@@ -65,23 +65,24 @@
         as `func` and `inverse_func`. If `transformer is None` as well as
         `func` and `inverse_func`, the transformer will be an identity
         transformer. Note that the transformer will be cloned during fitting.
         Also, the transformer is restricting `y` to be a numpy array.
 
     func : function, default=None
         Function to apply to `y` before passing to :meth:`fit`. Cannot be set
-        at the same time as `transformer`. The function needs to return a
-        2-dimensional array. If `func is None`, the function used will be the
-        identity function.
+        at the same time as `transformer`. If `func is None`, the function used will be
+        the identity function. If `func` is set, `inverse_func` also needs to be
+        provided. The function needs to return a 2-dimensional array.
 
     inverse_func : function, default=None
         Function to apply to the prediction of the regressor. Cannot be set at
-        the same time as `transformer`. The function needs to return a
-        2-dimensional array. The inverse function is used to return
-        predictions to the same space of the original training labels.
+        the same time as `transformer`. The inverse function is used to return
+        predictions to the same space of the original training labels. If
+        `inverse_func` is set, `func` also needs to be provided. The inverse
+        function needs to return a 2-dimensional array.
 
     check_inverse : bool, default=True
         Whether to check that `transform` followed by `inverse_transform`
         or `func` followed by `inverse_func` leads to the original targets.
 
     Attributes
     ----------
@@ -169,17 +170,26 @@
         ):
             raise ValueError(
                 "'transformer' and functions 'func'/'inverse_func' cannot both be set."
             )
         elif self.transformer is not None:
             self.transformer_ = clone(self.transformer)
         else:
-            if self.func is not None and self.inverse_func is None:
+            if (self.func is not None and self.inverse_func is None) or (
+                self.func is None and self.inverse_func is not None
+            ):
+                lacking_param, existing_param = (
+                    ("func", "inverse_func")
+                    if self.func is None
+                    else ("inverse_func", "func")
+                )
                 raise ValueError(
-                    "When 'func' is provided, 'inverse_func' must also be provided"
+                    f"When '{existing_param}' is provided, '{lacking_param}' must also"
+                    f" be provided. If {lacking_param} is supposed to be the default,"
+                    " you need to explicitly pass it the identity function."
                 )
             self.transformer_ = FunctionTransformer(
                 func=self.func,
                 inverse_func=self.inverse_func,
                 validate=True,
                 check_inverse=self.check_inverse,
             )
```

### Comparing `scikit-learn-1.4.2/sklearn/compose/tests/test_column_transformer.py` & `scikit_learn-1.5.0rc1/sklearn/compose/tests/test_column_transformer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,29 @@
 """
 Test the ColumnTransformer.
 """
 
 import pickle
 import re
 import warnings
+from unittest.mock import Mock
 
+import joblib
 import numpy as np
 import pytest
 from numpy.testing import assert_allclose
 from scipy import sparse
 
 from sklearn.base import BaseEstimator, TransformerMixin
 from sklearn.compose import (
     ColumnTransformer,
     make_column_selector,
     make_column_transformer,
 )
+from sklearn.compose._column_transformer import _RemainderColsList
 from sklearn.exceptions import NotFittedError
 from sklearn.feature_selection import VarianceThreshold
 from sklearn.preprocessing import (
     FunctionTransformer,
     Normalizer,
     OneHotEncoder,
     StandardScaler,
@@ -32,15 +35,15 @@
 )
 from sklearn.utils._testing import (
     _convert_container,
     assert_allclose_dense_sparse,
     assert_almost_equal,
     assert_array_equal,
 )
-from sklearn.utils.fixes import CSR_CONTAINERS
+from sklearn.utils.fixes import CSR_CONTAINERS, parse_version
 
 
 class Trans(TransformerMixin, BaseEstimator):
     def fit(self, X, y=None):
         return self
 
     def transform(self, X, y=None):
@@ -783,14 +786,15 @@
         "trans2__copy": True,
         "trans2__with_mean": True,
         "trans2__with_std": True,
         "transformers": ct.transformers,
         "transformer_weights": None,
         "verbose_feature_names_out": True,
         "verbose": False,
+        "force_int_remainder_cols": True,
     }
 
     assert ct.get_params() == exp
 
     ct.set_params(trans1__with_mean=False)
     assert not ct.get_params()["trans1__with_mean"]
 
@@ -804,14 +808,15 @@
         "trans2__copy": True,
         "trans2__with_mean": True,
         "trans2__with_std": True,
         "transformers": ct.transformers,
         "transformer_weights": None,
         "verbose_feature_names_out": True,
         "verbose": False,
+        "force_int_remainder_cols": True,
     }
 
     assert ct.get_params() == exp
 
 
 def test_column_transformer_named_estimators():
     X_array = np.array([[0.0, 1.0, 2.0], [2.0, 4.0, 6.0]]).T
@@ -933,82 +938,193 @@
     assert_array_equal(ct.transformers_[-1][2], [1])
 
     # check default for make_column_transformer
     ct = make_column_transformer((Trans(), [0]))
     assert ct.remainder == "drop"
 
 
+# TODO(1.7): check for deprecated force_int_remainder_cols
+# TODO(1.9): remove force_int but keep the test
 @pytest.mark.parametrize(
-    "key", [[0], np.array([0]), slice(0, 1), np.array([True, False])]
+    "cols1, cols2",
+    [
+        ([0], [False, True, False]),  # mix types
+        ([0], [1]),  # ints
+        (lambda x: [0], lambda x: [1]),  # callables
+    ],
+)
+@pytest.mark.parametrize("force_int", [False, True])
+def test_column_transformer_remainder_dtypes_ints(force_int, cols1, cols2):
+    """Check that the remainder columns are always stored as indices when
+    other columns are not all specified as column names or masks, regardless of
+    `force_int_remainder_cols`.
+    """
+    X = np.ones((1, 3))
+
+    ct = make_column_transformer(
+        (Trans(), cols1),
+        (Trans(), cols2),
+        remainder="passthrough",
+        force_int_remainder_cols=force_int,
+    )
+    with warnings.catch_warnings():
+        warnings.simplefilter("error")
+        ct.fit_transform(X)
+        assert ct.transformers_[-1][-1][0] == 2
+
+
+# TODO(1.7): check for deprecated force_int_remainder_cols
+# TODO(1.9): remove force_int but keep the test
+@pytest.mark.parametrize(
+    "force_int, cols1, cols2, expected_cols",
+    [
+        (True, ["A"], ["B"], [2]),
+        (False, ["A"], ["B"], ["C"]),
+        (True, [True, False, False], [False, True, False], [2]),
+        (False, [True, False, False], [False, True, False], [False, False, True]),
+    ],
 )
-def test_column_transformer_remainder_numpy(key):
+def test_column_transformer_remainder_dtypes(force_int, cols1, cols2, expected_cols):
+    """Check that the remainder columns format matches the format of the other
+    columns when they're all strings or masks, unless `force_int = True`.
+    """
+    X = np.ones((1, 3))
+
+    if isinstance(cols1[0], str):
+        pd = pytest.importorskip("pandas")
+        X = pd.DataFrame(X, columns=["A", "B", "C"])
+
+    # if inputs are column names store remainder columns as column names unless
+    # force_int_remainder_cols is True
+    ct = make_column_transformer(
+        (Trans(), cols1),
+        (Trans(), cols2),
+        remainder="passthrough",
+        force_int_remainder_cols=force_int,
+    )
+    with warnings.catch_warnings():
+        warnings.simplefilter("error")
+        ct.fit_transform(X)
+
+    if force_int:
+        # If we forced using ints and we access the remainder columns a warning is shown
+        match = "The format of the columns of the 'remainder' transformer"
+        cols = ct.transformers_[-1][-1]
+        with pytest.warns(FutureWarning, match=match):
+            cols[0]
+    else:
+        with warnings.catch_warnings():
+            warnings.simplefilter("error")
+            cols = ct.transformers_[-1][-1]
+            cols[0]
+
+    assert cols == expected_cols
+
+
+def test_remainder_list_repr():
+    cols = _RemainderColsList([0, 1], warning_enabled=False)
+    assert str(cols) == "[0, 1]"
+    assert repr(cols) == "[0, 1]"
+    mock = Mock()
+    cols._repr_pretty_(mock, False)
+    mock.text.assert_called_once_with("[0, 1]")
+
+
+@pytest.mark.parametrize(
+    "key, expected_cols",
+    [
+        ([0], [1]),
+        (np.array([0]), [1]),
+        (slice(0, 1), [1]),
+        (np.array([True, False]), [False, True]),
+    ],
+)
+def test_column_transformer_remainder_numpy(key, expected_cols):
     # test different ways that columns are specified with passthrough
     X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
     X_res_both = X_array
 
-    ct = ColumnTransformer([("trans1", Trans(), key)], remainder="passthrough")
+    ct = ColumnTransformer(
+        [("trans1", Trans(), key)],
+        remainder="passthrough",
+        force_int_remainder_cols=False,
+    )
     assert_array_equal(ct.fit_transform(X_array), X_res_both)
     assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == "remainder"
     assert isinstance(ct.transformers_[-1][1], FunctionTransformer)
-    assert_array_equal(ct.transformers_[-1][2], [1])
+    assert ct.transformers_[-1][2] == expected_cols
 
 
 @pytest.mark.parametrize(
-    "key",
+    "key, expected_cols",
     [
-        [0],
-        slice(0, 1),
-        np.array([True, False]),
-        ["first"],
-        "pd-index",
-        np.array(["first"]),
-        np.array(["first"], dtype=object),
-        slice(None, "first"),
-        slice("first", "first"),
+        ([0], [1]),
+        (slice(0, 1), [1]),
+        (np.array([True, False]), [False, True]),
+        (["first"], ["second"]),
+        ("pd-index", ["second"]),
+        (np.array(["first"]), ["second"]),
+        (np.array(["first"], dtype=object), ["second"]),
+        (slice(None, "first"), ["second"]),
+        (slice("first", "first"), ["second"]),
     ],
 )
-def test_column_transformer_remainder_pandas(key):
+def test_column_transformer_remainder_pandas(key, expected_cols):
     # test different ways that columns are specified with passthrough
     pd = pytest.importorskip("pandas")
     if isinstance(key, str) and key == "pd-index":
         key = pd.Index(["first"])
 
     X_array = np.array([[0, 1, 2], [2, 4, 6]]).T
     X_df = pd.DataFrame(X_array, columns=["first", "second"])
     X_res_both = X_array
 
-    ct = ColumnTransformer([("trans1", Trans(), key)], remainder="passthrough")
+    ct = ColumnTransformer(
+        [("trans1", Trans(), key)],
+        remainder="passthrough",
+        force_int_remainder_cols=False,
+    )
     assert_array_equal(ct.fit_transform(X_df), X_res_both)
     assert_array_equal(ct.fit(X_df).transform(X_df), X_res_both)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == "remainder"
     assert isinstance(ct.transformers_[-1][1], FunctionTransformer)
-    assert_array_equal(ct.transformers_[-1][2], [1])
+    assert ct.transformers_[-1][2] == expected_cols
 
 
 @pytest.mark.parametrize(
-    "key", [[0], np.array([0]), slice(0, 1), np.array([True, False, False])]
+    "key, expected_cols",
+    [
+        ([0], [1, 2]),
+        (np.array([0]), [1, 2]),
+        (slice(0, 1), [1, 2]),
+        (np.array([True, False, False]), [False, True, True]),
+    ],
 )
-def test_column_transformer_remainder_transformer(key):
+def test_column_transformer_remainder_transformer(key, expected_cols):
     X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T
     X_res_both = X_array.copy()
 
     # second and third columns are doubled when remainder = DoubleTrans
     X_res_both[:, 1:3] *= 2
 
-    ct = ColumnTransformer([("trans1", Trans(), key)], remainder=DoubleTrans())
+    ct = ColumnTransformer(
+        [("trans1", Trans(), key)],
+        remainder=DoubleTrans(),
+        force_int_remainder_cols=False,
+    )
 
     assert_array_equal(ct.fit_transform(X_array), X_res_both)
     assert_array_equal(ct.fit(X_array).transform(X_array), X_res_both)
     assert len(ct.transformers_) == 2
     assert ct.transformers_[-1][0] == "remainder"
     assert isinstance(ct.transformers_[-1][1], DoubleTrans)
-    assert_array_equal(ct.transformers_[-1][2], [1, 2])
+    assert ct.transformers_[-1][2] == expected_cols
 
 
 def test_column_transformer_no_remaining_remainder_transformer():
     X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T
 
     ct = ColumnTransformer([("trans1", Trans(), [0, 1, 2])], remainder=DoubleTrans())
 
@@ -1095,14 +1211,15 @@
         "trans1__copy": True,
         "trans1__with_mean": True,
         "trans1__with_std": True,
         "transformers": ct.transformers,
         "transformer_weights": None,
         "verbose_feature_names_out": True,
         "verbose": False,
+        "force_int_remainder_cols": True,
     }
 
     assert ct.get_params() == exp
 
     ct.set_params(remainder__with_std=False)
     assert not ct.get_params()["remainder__with_std"]
 
@@ -1115,14 +1232,15 @@
         "remainder__with_std": False,
         "sparse_threshold": 0.3,
         "trans1": "passthrough",
         "transformers": ct.transformers,
         "transformer_weights": None,
         "verbose_feature_names_out": True,
         "verbose": False,
+        "force_int_remainder_cols": True,
     }
     assert ct.get_params() == exp
 
 
 def test_column_transformer_no_estimators():
     X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).astype("float").T
     ct = ColumnTransformer([], remainder=StandardScaler())
@@ -1471,15 +1589,17 @@
 
 @pytest.mark.parametrize("remainder", ["passthrough", StandardScaler()])
 def test_sk_visual_block_remainder_fitted_pandas(remainder):
     # Remainder shows the columns after fitting
     pd = pytest.importorskip("pandas")
     ohe = OneHotEncoder()
     ct = ColumnTransformer(
-        transformers=[("ohe", ohe, ["col1", "col2"])], remainder=remainder
+        transformers=[("ohe", ohe, ["col1", "col2"])],
+        remainder=remainder,
+        force_int_remainder_cols=False,
     )
     df = pd.DataFrame(
         {
             "col1": ["a", "b", "c"],
             "col2": ["z", "z", "z"],
             "col3": [1, 2, 3],
             "col4": [3, 4, 5],
@@ -2333,14 +2453,32 @@
     X_test_pd = pd.DataFrame(X_test_np, columns=["a", "b"])
     ct.fit(X_train_pl)
 
     out_pd_in = ct.transform(X_test_pd)
     assert_array_equal(out_pd_in, X_test_np)
 
 
+def test_column_transformer__getitem__():
+    """Check __getitem__ for ColumnTransformer."""
+    X = np.array([[0, 1, 2], [3, 4, 5]])
+    ct = ColumnTransformer([("t1", Trans(), [0, 1]), ("t2", Trans(), [1, 2])])
+
+    msg = "ColumnTransformer is subscriptable after it is fitted"
+    with pytest.raises(TypeError, match=msg):
+        ct["t1"]
+
+    ct.fit(X)
+    assert ct["t1"] is ct.named_transformers_["t1"]
+    assert ct["t2"] is ct.named_transformers_["t2"]
+
+    msg = "'does_not_exist' is not a valid transformer name"
+    with pytest.raises(KeyError, match=msg):
+        ct["does_not_exist"]
+
+
 @pytest.mark.parametrize("transform_output", ["default", "pandas"])
 def test_column_transformer_remainder_passthrough_naming_consistency(transform_output):
     """Check that when `remainder="passthrough"`, inconsistent naming is handled
     correctly by the underlying `FunctionTransformer`.
 
     Non-regression test for:
     https://github.com/scikit-learn/scikit-learn/issues/28232
@@ -2425,14 +2563,38 @@
         "Transformer B has conflicting columns names: ['x1', 'x2'].\n"
         "Transformer C has conflicting columns names: ['x1', 'x3'].\n"
     )
     with pytest.raises(ValueError, match=err_msg):
         transformer.fit_transform(df)
 
 
+@pytest.mark.skipif(
+    parse_version(joblib.__version__) < parse_version("1.3"),
+    reason="requires joblib >= 1.3",
+)
+def test_column_transformer_auto_memmap():
+    """Check that ColumnTransformer works in parallel with joblib's auto-memmapping.
+
+    non-regression test for issue #28781
+    """
+    X = np.random.RandomState(0).uniform(size=(3, 4))
+
+    scaler = StandardScaler(copy=False)
+
+    transformer = ColumnTransformer(
+        transformers=[("scaler", scaler, [0])],
+        n_jobs=2,
+    )
+
+    with joblib.parallel_backend("loky", max_nbytes=1):
+        Xt = transformer.fit_transform(X)
+
+    assert_allclose(Xt, StandardScaler().fit_transform(X[:, [0]]))
+
+
 # Metadata Routing Tests
 # ======================
 
 
 @pytest.mark.parametrize("method", ["transform", "fit_transform", "fit"])
 def test_routing_passed_metadata_not_supported(method):
     """Test that the right error message is raised when metadata is passed while
@@ -2495,15 +2657,14 @@
         def transform(self, X, sample_weight=None, metadata=None):
             assert sample_weight
             assert metadata
             return X
 
     X = np.array([[0, 1, 2], [2, 4, 6]]).T
     y = [1, 2, 3]
-    _Registry()
     sample_weight, metadata = [1], "a"
     trs = ColumnTransformer(
         [
             (
                 "trans",
                 NoFitTransform()
                 .set_fit_request(sample_weight=True, metadata=True)
@@ -2524,15 +2685,15 @@
     X = np.array([[0, 1, 2], [2, 4, 6]]).T
     y = [1, 2, 3]
     sample_weight, metadata = [1], "a"
     trs = ColumnTransformer([("trans", ConsumingTransformer(), [0])])
 
     error_message = (
         "[sample_weight, metadata] are passed but are not explicitly set as requested"
-        f" or not for ConsumingTransformer.{method}"
+        f" or not requested for ConsumingTransformer.{method}"
     )
     with pytest.raises(ValueError, match=re.escape(error_message)):
         if method == "transform":
             trs.fit(X, y)
             trs.transform(X, sample_weight=sample_weight, metadata=metadata)
         else:
             getattr(trs, method)(X, y, sample_weight=sample_weight, metadata=metadata)
```

### Comparing `scikit-learn-1.4.2/sklearn/compose/tests/test_target.py` & `scikit_learn-1.5.0rc1/sklearn/compose/tests/test_target.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,22 +33,30 @@
         regressor=OrthogonalMatchingPursuit(), transformer=StandardScaler()
     )
     with pytest.raises(
         TypeError,
         match=r"fit\(\) got an unexpected " "keyword argument 'sample_weight'",
     ):
         regr.fit(X, y, sample_weight=sample_weight)
-    # func is given but inverse_func is not
+
+    # one of (func, inverse_func) is given but the other one is not
     regr = TransformedTargetRegressor(func=np.exp)
     with pytest.raises(
         ValueError,
         match="When 'func' is provided, 'inverse_func' must also be provided",
     ):
         regr.fit(X, y)
 
+    regr = TransformedTargetRegressor(inverse_func=np.log)
+    with pytest.raises(
+        ValueError,
+        match="When 'inverse_func' is provided, 'func' must also be provided",
+    ):
+        regr.fit(X, y)
+
 
 def test_transform_target_regressor_invertible():
     X, y = friedman
     regr = TransformedTargetRegressor(
         regressor=LinearRegression(),
         func=np.sqrt,
         inverse_func=np.log,
```

### Comparing `scikit-learn-1.4.2/sklearn/conftest.py` & `scikit_learn-1.5.0rc1/sklearn/conftest.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,22 +16,24 @@
 from sklearn._min_dependencies import PYTEST_MIN_VERSION
 from sklearn.datasets import (
     fetch_20newsgroups,
     fetch_20newsgroups_vectorized,
     fetch_california_housing,
     fetch_covtype,
     fetch_kddcup99,
+    fetch_lfw_pairs,
+    fetch_lfw_people,
     fetch_olivetti_faces,
     fetch_rcv1,
     fetch_species_distributions,
 )
 from sklearn.tests import random_seed
-from sklearn.utils import _IS_32BIT
 from sklearn.utils._testing import get_pytest_filterwarning_lines
 from sklearn.utils.fixes import (
+    _IS_32BIT,
     np_base_version,
     parse_version,
     sp_version,
 )
 
 if parse_version(pytest.__version__) < parse_version(PYTEST_MIN_VERSION):
     raise ImportError(
@@ -70,14 +72,16 @@
 
 dataset_fetchers = {
     "fetch_20newsgroups_fxt": fetch_20newsgroups,
     "fetch_20newsgroups_vectorized_fxt": fetch_20newsgroups_vectorized,
     "fetch_california_housing_fxt": fetch_california_housing,
     "fetch_covtype_fxt": fetch_covtype,
     "fetch_kddcup99_fxt": fetch_kddcup99,
+    "fetch_lfw_pairs_fxt": fetch_lfw_pairs,
+    "fetch_lfw_people_fxt": fetch_lfw_people,
     "fetch_olivetti_faces_fxt": fetch_olivetti_faces,
     "fetch_rcv1_fxt": fetch_rcv1,
     "fetch_species_distributions_fxt": fetch_species_distributions,
 }
 
 if scipy_datasets_require_network:
     dataset_fetchers["raccoon_face_fxt"] = raccoon_face_or_skip
@@ -113,14 +117,16 @@
 
 # Adds fixtures for fetching data
 fetch_20newsgroups_fxt = _fetch_fixture(fetch_20newsgroups)
 fetch_20newsgroups_vectorized_fxt = _fetch_fixture(fetch_20newsgroups_vectorized)
 fetch_california_housing_fxt = _fetch_fixture(fetch_california_housing)
 fetch_covtype_fxt = _fetch_fixture(fetch_covtype)
 fetch_kddcup99_fxt = _fetch_fixture(fetch_kddcup99)
+fetch_lfw_pairs_fxt = _fetch_fixture(fetch_lfw_pairs)
+fetch_lfw_people_fxt = _fetch_fixture(fetch_lfw_people)
 fetch_olivetti_faces_fxt = _fetch_fixture(fetch_olivetti_faces)
 fetch_rcv1_fxt = _fetch_fixture(fetch_rcv1)
 fetch_species_distributions_fxt = _fetch_fixture(fetch_species_distributions)
 raccoon_face_fxt = pytest.fixture(raccoon_face_or_skip)
 
 
 def pytest_collection_modifyitems(config, items):
```

### Comparing `scikit-learn-1.4.2/sklearn/covariance/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/covariance/_elliptic_envelope.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/_elliptic_envelope.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/covariance/_empirical_covariance.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/_empirical_covariance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/covariance/_graph_lasso.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/_graph_lasso.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,16 +17,23 @@
 from ..base import _fit_context
 from ..exceptions import ConvergenceWarning
 
 # mypy error: Module 'sklearn.linear_model' has no attribute '_cd_fast'
 from ..linear_model import _cd_fast as cd_fast  # type: ignore
 from ..linear_model import lars_path_gram
 from ..model_selection import check_cv, cross_val_score
+from ..utils import Bunch
 from ..utils._param_validation import Interval, StrOptions, validate_params
-from ..utils.metadata_routing import _RoutingNotSupportedMixin
+from ..utils.metadata_routing import (
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    _routing_enabled,
+    process_routing,
+)
 from ..utils.parallel import Parallel, delayed
 from ..utils.validation import (
     _is_arraylike_not_scalar,
     check_random_state,
     check_scalar,
 )
 from . import EmpiricalCovariance, empirical_covariance, log_likelihood
@@ -211,25 +218,23 @@
     A.flat[:: A.shape[0] + 1] = 0
     return np.max(np.abs(A))
 
 
 @validate_params(
     {
         "emp_cov": ["array-like"],
-        "cov_init": ["array-like", None],
         "return_costs": ["boolean"],
         "return_n_iter": ["boolean"],
     },
     prefer_skip_nested_validation=False,
 )
 def graphical_lasso(
     emp_cov,
     alpha,
     *,
-    cov_init=None,
     mode="cd",
     tol=1e-4,
     enet_tol=1e-4,
     max_iter=100,
     verbose=False,
     return_costs=False,
     eps=np.finfo(np.float64).eps,
@@ -248,22 +253,14 @@
         Empirical covariance from which to compute the covariance estimate.
 
     alpha : float
         The regularization parameter: the higher alpha, the more
         regularization, the sparser the inverse covariance.
         Range is (0, inf].
 
-    cov_init : array of shape (n_features, n_features), default=None
-        The initial guess for the covariance. If None, then the empirical
-        covariance is used.
-
-        .. deprecated:: 1.3
-           `cov_init` is deprecated in 1.3 and will be removed in 1.5.
-           It currently has no effect.
-
     mode : {'cd', 'lars'}, default='cd'
         The Lasso solver to use: coordinate descent or LARS. Use LARS for
         very sparse underlying graphs, where p > n. Elsewhere prefer cd
         which is more numerically stable.
 
     tol : float, default=1e-4
         The tolerance to declare convergence: if the dual gap goes below
@@ -336,24 +333,14 @@
     >>> emp_cov = empirical_covariance(X, assume_centered=True)
     >>> emp_cov, _ = graphical_lasso(emp_cov, alpha=0.05)
     >>> emp_cov
     array([[ 1.68...,  0.21..., -0.20...],
            [ 0.21...,  0.22..., -0.08...],
            [-0.20..., -0.08...,  0.23...]])
     """
-
-    if cov_init is not None:
-        warnings.warn(
-            (
-                "The cov_init parameter is deprecated in 1.3 and will be removed in "
-                "1.5. It does not have any effect."
-            ),
-            FutureWarning,
-        )
-
     model = GraphicalLasso(
         alpha=alpha,
         mode=mode,
         covariance="precomputed",
         tol=tol,
         enet_tol=enet_tol,
         max_iter=max_iter,
@@ -717,15 +704,15 @@
             else:
                 print("[graphical_lasso_path] alpha: %.2e" % alpha)
     if X_test is not None:
         return covariances_, precisions_, scores_
     return covariances_, precisions_
 
 
-class GraphicalLassoCV(_RoutingNotSupportedMixin, BaseGraphicalLasso):
+class GraphicalLassoCV(BaseGraphicalLasso):
     """Sparse inverse covariance w/ cross-validated choice of the l1 penalty.
 
     See glossary entry for :term:`cross-validation estimator`.
 
     Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
 
     .. versionchanged:: v0.20
@@ -938,31 +925,44 @@
         )
         self.alphas = alphas
         self.n_refinements = n_refinements
         self.cv = cv
         self.n_jobs = n_jobs
 
     @_fit_context(prefer_skip_nested_validation=True)
-    def fit(self, X, y=None):
+    def fit(self, X, y=None, **params):
         """Fit the GraphicalLasso covariance model to X.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
             Data from which to compute the covariance estimate.
 
         y : Ignored
             Not used, present for API consistency by convention.
 
+        **params : dict, default=None
+            Parameters to be passed to the CV splitter and the
+            cross_val_score function.
+
+            .. versionadded:: 1.5
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Returns the instance itself.
         """
         # Covariance does not make sense for a single feature
+        _raise_for_params(params, self, "fit")
+
         X = self._validate_data(X, ensure_min_features=2)
         if self.assume_centered:
             self.location_ = np.zeros(X.shape[1])
         else:
             self.location_ = X.mean(0)
         emp_cov = empirical_covariance(X, assume_centered=self.assume_centered)
 
@@ -987,14 +987,19 @@
             n_refinements = 1
         else:
             n_refinements = self.n_refinements
             alpha_1 = alpha_max(emp_cov)
             alpha_0 = 1e-2 * alpha_1
             alphas = np.logspace(np.log10(alpha_0), np.log10(alpha_1), n_alphas)[::-1]
 
+        if _routing_enabled():
+            routed_params = process_routing(self, "fit", **params)
+        else:
+            routed_params = Bunch(splitter=Bunch(split={}))
+
         t0 = time.time()
         for i in range(n_refinements):
             with warnings.catch_warnings():
                 # No need to see the convergence warnings on this grid:
                 # they will always be points that will not converge
                 # during the cross-validation
                 warnings.simplefilter("ignore", ConvergenceWarning)
@@ -1011,15 +1016,15 @@
                         mode=self.mode,
                         tol=self.tol,
                         enet_tol=self.enet_tol,
                         max_iter=int(0.1 * self.max_iter),
                         verbose=inner_verbose,
                         eps=self.eps,
                     )
-                    for train, test in cv.split(X, y)
+                    for train, test in cv.split(X, y, **routed_params.splitter.split)
                 )
 
             # Little danse to transform the list in what we need
             covs, _, scores = zip(*this_path)
             covs = zip(*covs)
             scores = zip(*scores)
             path.extend(zip(alphas, scores, covs))
@@ -1077,14 +1082,15 @@
         grid_scores.append(
             cross_val_score(
                 EmpiricalCovariance(),
                 X,
                 cv=cv,
                 n_jobs=self.n_jobs,
                 verbose=inner_verbose,
+                params=params,
             )
         )
         grid_scores = np.array(grid_scores)
 
         self.cv_results_ = {"alphas": np.array(alphas)}
 
         for i in range(grid_scores.shape[1]):
@@ -1104,7 +1110,27 @@
             tol=self.tol,
             enet_tol=self.enet_tol,
             max_iter=self.max_iter,
             verbose=inner_verbose,
             eps=self.eps,
         )
         return self
+
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
+
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__).add(
+            splitter=check_cv(self.cv),
+            method_mapping=MethodMapping().add(callee="split", caller="fit"),
+        )
+        return router
```

### Comparing `scikit-learn-1.4.2/sklearn/covariance/_robust_covariance.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/_robust_covariance.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 Robust location and covariance estimators.
 
 Here are implemented estimators that are resistant to outliers.
 
 """
+
 # Author: Virgile Fritsch <virgile.fritsch@inria.fr>
 #
 # License: BSD 3 clause
 
 import warnings
 from numbers import Integral, Real
```

### Comparing `scikit-learn-1.4.2/sklearn/covariance/_shrunk_covariance.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/_shrunk_covariance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/covariance/tests/test_covariance.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_covariance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/covariance/tests/test_elliptic_envelope.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_elliptic_envelope.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/covariance/tests/test_graphical_lasso.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_graphical_lasso.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-""" Test the graphical_lasso module.
-"""
+"""Test the graphical_lasso module."""
+
 import sys
 from io import StringIO
 
 import numpy as np
 import pytest
 from numpy.testing import assert_allclose
 from scipy import linalg
@@ -12,14 +12,15 @@
 from sklearn.covariance import (
     GraphicalLasso,
     GraphicalLassoCV,
     empirical_covariance,
     graphical_lasso,
 )
 from sklearn.datasets import make_sparse_spd_matrix
+from sklearn.model_selection import GroupKFold
 from sklearn.utils import check_random_state
 from sklearn.utils._testing import (
     _convert_container,
     assert_array_almost_equal,
     assert_array_less,
 )
 
@@ -250,37 +251,68 @@
     )
     rng = np.random.RandomState(0)
     X = rng.multivariate_normal(mean=[0, 0, 0, 0], cov=true_cov, size=200)
     cov = GraphicalLassoCV(cv=splits, alphas=n_alphas, n_refinements=n_refinements).fit(
         X
     )
 
+    _assert_graphical_lasso_cv_scores(
+        cov=cov,
+        n_splits=splits,
+        n_refinements=n_refinements,
+        n_alphas=n_alphas,
+    )
+
+
+@pytest.mark.usefixtures("enable_slep006")
+def test_graphical_lasso_cv_scores_with_routing(global_random_seed):
+    """Check that `GraphicalLassoCV` internally dispatches metadata to
+    the splitter.
+    """
+    splits = 5
+    n_alphas = 5
+    n_refinements = 3
+    true_cov = np.array(
+        [
+            [0.8, 0.0, 0.2, 0.0],
+            [0.0, 0.4, 0.0, 0.0],
+            [0.2, 0.0, 0.3, 0.1],
+            [0.0, 0.0, 0.1, 0.7],
+        ]
+    )
+    rng = np.random.RandomState(global_random_seed)
+    X = rng.multivariate_normal(mean=[0, 0, 0, 0], cov=true_cov, size=300)
+    n_samples = X.shape[0]
+    groups = rng.randint(0, 5, n_samples)
+    params = {"groups": groups}
+    cv = GroupKFold(n_splits=splits)
+    cv.set_split_request(groups=True)
+
+    cov = GraphicalLassoCV(cv=cv, alphas=n_alphas, n_refinements=n_refinements).fit(
+        X, **params
+    )
+
+    _assert_graphical_lasso_cv_scores(
+        cov=cov,
+        n_splits=splits,
+        n_refinements=n_refinements,
+        n_alphas=n_alphas,
+    )
+
+
+def _assert_graphical_lasso_cv_scores(cov, n_splits, n_refinements, n_alphas):
     cv_results = cov.cv_results_
     # alpha and one for each split
 
     total_alphas = n_refinements * n_alphas + 1
     keys = ["alphas"]
-    split_keys = [f"split{i}_test_score" for i in range(splits)]
+    split_keys = [f"split{i}_test_score" for i in range(n_splits)]
     for key in keys + split_keys:
         assert key in cv_results
         assert len(cv_results[key]) == total_alphas
 
     cv_scores = np.asarray([cov.cv_results_[key] for key in split_keys])
     expected_mean = cv_scores.mean(axis=0)
     expected_std = cv_scores.std(axis=0)
 
     assert_allclose(cov.cv_results_["mean_test_score"], expected_mean)
     assert_allclose(cov.cv_results_["std_test_score"], expected_std)
-
-
-# TODO(1.5): remove in 1.5
-def test_graphical_lasso_cov_init_deprecation():
-    """Check that we raise a deprecation warning if providing `cov_init` in
-    `graphical_lasso`."""
-    rng, dim, n_samples = np.random.RandomState(0), 20, 100
-    prec = make_sparse_spd_matrix(dim, alpha=0.95, random_state=0)
-    cov = linalg.inv(prec)
-    X = rng.multivariate_normal(np.zeros(dim), cov, size=n_samples)
-
-    emp_cov = empirical_covariance(X)
-    with pytest.warns(FutureWarning, match="cov_init parameter is deprecated"):
-        graphical_lasso(emp_cov, alpha=0.1, cov_init=emp_cov)
```

### Comparing `scikit-learn-1.4.2/sklearn/covariance/tests/test_robust_covariance.py` & `scikit_learn-1.5.0rc1/sklearn/covariance/tests/test_robust_covariance.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/cross_decomposition/_pls.py` & `scikit_learn-1.5.0rc1/sklearn/cross_decomposition/_pls.py`

 * *Files 8% similar despite different names*

```diff
@@ -67,15 +67,15 @@
     "update saliences" part.
     """
 
     eps = np.finfo(X.dtype).eps
     try:
         y_score = next(col for col in Y.T if np.any(np.abs(col) > eps))
     except StopIteration as e:
-        raise StopIteration("Y residual is constant") from e
+        raise StopIteration("y residual is constant") from e
 
     x_weights_old = 100  # init to big value for first convergence check
 
     if mode == "B":
         # Precompute pseudo inverse matrices
         # Basically: X_pinv = (X.T X)^-1 X.T
         # Which requires inverting a (n_features, n_features) matrix.
@@ -157,14 +157,36 @@
     # arrays. We don't want that.
     biggest_abs_val_idx = np.argmax(np.abs(u))
     sign = np.sign(u[biggest_abs_val_idx])
     u *= sign
     v *= sign
 
 
+# TODO(1.7): Remove
+def _deprecate_Y_when_optional(y, Y):
+    if Y is not None:
+        warnings.warn(
+            "`Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.",
+            FutureWarning,
+        )
+        if y is not None:
+            raise ValueError(
+                "Cannot use both `y` and `Y`. Use only `y` as `Y` is deprecated."
+            )
+        return Y
+    return y
+
+
+# TODO(1.7): Remove
+def _deprecate_Y_when_required(y, Y):
+    if y is None and Y is None:
+        raise ValueError("y is required.")
+    return _deprecate_Y_when_optional(y, Y)
+
+
 class _PLS(
     ClassNamePrefixFeaturesOutMixin,
     TransformerMixin,
     RegressorMixin,
     MultiOutputMixin,
     BaseEstimator,
     metaclass=ABCMeta,
@@ -208,48 +230,57 @@
         self.scale = scale
         self.algorithm = algorithm
         self.max_iter = max_iter
         self.tol = tol
         self.copy = copy
 
     @_fit_context(prefer_skip_nested_validation=True)
-    def fit(self, X, Y):
+    def fit(self, X, y=None, Y=None):
         """Fit model to data.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
             Training vectors, where `n_samples` is the number of samples and
             `n_features` is the number of predictors.
 
+        y : array-like of shape (n_samples,) or (n_samples, n_targets)
+            Target vectors, where `n_samples` is the number of samples and
+            `n_targets` is the number of response variables.
+
         Y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Target vectors, where `n_samples` is the number of samples and
             `n_targets` is the number of response variables.
 
+            .. deprecated:: 1.5
+               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.
+
         Returns
         -------
         self : object
             Fitted model.
         """
-        check_consistent_length(X, Y)
+        y = _deprecate_Y_when_required(y, Y)
+
+        check_consistent_length(X, y)
         X = self._validate_data(
             X, dtype=np.float64, copy=self.copy, ensure_min_samples=2
         )
-        Y = check_array(
-            Y, input_name="Y", dtype=np.float64, copy=self.copy, ensure_2d=False
+        y = check_array(
+            y, input_name="y", dtype=np.float64, copy=self.copy, ensure_2d=False
         )
-        if Y.ndim == 1:
+        if y.ndim == 1:
             self._predict_1d = True
-            Y = Y.reshape(-1, 1)
+            y = y.reshape(-1, 1)
         else:
             self._predict_1d = False
 
         n = X.shape[0]
         p = X.shape[1]
-        q = Y.shape[1]
+        q = y.shape[1]
 
         n_components = self.n_components
         # With PLSRegression n_components is bounded by the rank of (X.T X) see
         # Wegelin page 25. With CCA and PLSCanonical, n_components is bounded
         # by the rank of X and the rank of Y: see Wegelin page 12
         rank_upper_bound = p if self.deflation_mode == "regression" else min(n, p, q)
         if n_components > rank_upper_bound:
@@ -258,194 +289,211 @@
                 f"Got {n_components} instead. Reduce `n_components`."
             )
 
         self._norm_y_weights = self.deflation_mode == "canonical"  # 1.1
         norm_y_weights = self._norm_y_weights
 
         # Scale (in place)
-        Xk, Yk, self._x_mean, self._y_mean, self._x_std, self._y_std = _center_scale_xy(
-            X, Y, self.scale
+        Xk, yk, self._x_mean, self._y_mean, self._x_std, self._y_std = _center_scale_xy(
+            X, y, self.scale
         )
 
         self.x_weights_ = np.zeros((p, n_components))  # U
         self.y_weights_ = np.zeros((q, n_components))  # V
         self._x_scores = np.zeros((n, n_components))  # Xi
         self._y_scores = np.zeros((n, n_components))  # Omega
         self.x_loadings_ = np.zeros((p, n_components))  # Gamma
         self.y_loadings_ = np.zeros((q, n_components))  # Delta
         self.n_iter_ = []
 
         # This whole thing corresponds to the algorithm in section 4.1 of the
         # review from Wegelin. See above for a notation mapping from code to
         # paper.
-        Y_eps = np.finfo(Yk.dtype).eps
+        y_eps = np.finfo(yk.dtype).eps
         for k in range(n_components):
             # Find first left and right singular vectors of the X.T.dot(Y)
             # cross-covariance matrix.
             if self.algorithm == "nipals":
                 # Replace columns that are all close to zero with zeros
-                Yk_mask = np.all(np.abs(Yk) < 10 * Y_eps, axis=0)
-                Yk[:, Yk_mask] = 0.0
+                yk_mask = np.all(np.abs(yk) < 10 * y_eps, axis=0)
+                yk[:, yk_mask] = 0.0
 
                 try:
                     (
                         x_weights,
                         y_weights,
                         n_iter_,
                     ) = _get_first_singular_vectors_power_method(
                         Xk,
-                        Yk,
+                        yk,
                         mode=self.mode,
                         max_iter=self.max_iter,
                         tol=self.tol,
                         norm_y_weights=norm_y_weights,
                     )
                 except StopIteration as e:
-                    if str(e) != "Y residual is constant":
+                    if str(e) != "y residual is constant":
                         raise
-                    warnings.warn(f"Y residual is constant at iteration {k}")
+                    warnings.warn(f"y residual is constant at iteration {k}")
                     break
 
                 self.n_iter_.append(n_iter_)
 
             elif self.algorithm == "svd":
-                x_weights, y_weights = _get_first_singular_vectors_svd(Xk, Yk)
+                x_weights, y_weights = _get_first_singular_vectors_svd(Xk, yk)
 
             # inplace sign flip for consistency across solvers and archs
             _svd_flip_1d(x_weights, y_weights)
 
             # compute scores, i.e. the projections of X and Y
             x_scores = np.dot(Xk, x_weights)
             if norm_y_weights:
                 y_ss = 1
             else:
                 y_ss = np.dot(y_weights, y_weights)
-            y_scores = np.dot(Yk, y_weights) / y_ss
+            y_scores = np.dot(yk, y_weights) / y_ss
 
             # Deflation: subtract rank-one approx to obtain Xk+1 and Yk+1
             x_loadings = np.dot(x_scores, Xk) / np.dot(x_scores, x_scores)
             Xk -= np.outer(x_scores, x_loadings)
 
             if self.deflation_mode == "canonical":
                 # regress Yk on y_score
-                y_loadings = np.dot(y_scores, Yk) / np.dot(y_scores, y_scores)
-                Yk -= np.outer(y_scores, y_loadings)
+                y_loadings = np.dot(y_scores, yk) / np.dot(y_scores, y_scores)
+                yk -= np.outer(y_scores, y_loadings)
             if self.deflation_mode == "regression":
                 # regress Yk on x_score
-                y_loadings = np.dot(x_scores, Yk) / np.dot(x_scores, x_scores)
-                Yk -= np.outer(x_scores, y_loadings)
+                y_loadings = np.dot(x_scores, yk) / np.dot(x_scores, x_scores)
+                yk -= np.outer(x_scores, y_loadings)
 
             self.x_weights_[:, k] = x_weights
             self.y_weights_[:, k] = y_weights
             self._x_scores[:, k] = x_scores
             self._y_scores[:, k] = y_scores
             self.x_loadings_[:, k] = x_loadings
             self.y_loadings_[:, k] = y_loadings
 
         # X was approximated as Xi . Gamma.T + X_(R+1)
         # Xi . Gamma.T is a sum of n_components rank-1 matrices. X_(R+1) is
         # whatever is left to fully reconstruct X, and can be 0 if X is of rank
         # n_components.
-        # Similarly, Y was approximated as Omega . Delta.T + Y_(R+1)
+        # Similarly, y was approximated as Omega . Delta.T + y_(R+1)
 
         # Compute transformation matrices (rotations_). See User Guide.
         self.x_rotations_ = np.dot(
             self.x_weights_,
             pinv2(np.dot(self.x_loadings_.T, self.x_weights_), check_finite=False),
         )
         self.y_rotations_ = np.dot(
             self.y_weights_,
             pinv2(np.dot(self.y_loadings_.T, self.y_weights_), check_finite=False),
         )
         self.coef_ = np.dot(self.x_rotations_, self.y_loadings_.T)
-        self.coef_ = (self.coef_ * self._y_std).T
+        self.coef_ = (self.coef_ * self._y_std).T / self._x_std
         self.intercept_ = self._y_mean
         self._n_features_out = self.x_rotations_.shape[1]
         return self
 
-    def transform(self, X, Y=None, copy=True):
+    def transform(self, X, y=None, Y=None, copy=True):
         """Apply the dimension reduction.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
             Samples to transform.
 
+        y : array-like of shape (n_samples, n_targets), default=None
+            Target vectors.
+
         Y : array-like of shape (n_samples, n_targets), default=None
             Target vectors.
 
+            .. deprecated:: 1.5
+               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.
+
         copy : bool, default=True
             Whether to copy `X` and `Y`, or perform in-place normalization.
 
         Returns
         -------
         x_scores, y_scores : array-like or tuple of array-like
             Return `x_scores` if `Y` is not given, `(x_scores, y_scores)` otherwise.
         """
+        y = _deprecate_Y_when_optional(y, Y)
+
         check_is_fitted(self)
         X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)
         # Normalize
         X -= self._x_mean
         X /= self._x_std
         # Apply rotation
         x_scores = np.dot(X, self.x_rotations_)
-        if Y is not None:
-            Y = check_array(
-                Y, input_name="Y", ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES
+        if y is not None:
+            y = check_array(
+                y, input_name="y", ensure_2d=False, copy=copy, dtype=FLOAT_DTYPES
             )
-            if Y.ndim == 1:
-                Y = Y.reshape(-1, 1)
-            Y -= self._y_mean
-            Y /= self._y_std
-            y_scores = np.dot(Y, self.y_rotations_)
+            if y.ndim == 1:
+                y = y.reshape(-1, 1)
+            y -= self._y_mean
+            y /= self._y_std
+            y_scores = np.dot(y, self.y_rotations_)
             return x_scores, y_scores
 
         return x_scores
 
-    def inverse_transform(self, X, Y=None):
+    def inverse_transform(self, X, y=None, Y=None):
         """Transform data back to its original space.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_components)
             New data, where `n_samples` is the number of samples
             and `n_components` is the number of pls components.
 
+        y : array-like of shape (n_samples,) or (n_samples, n_components)
+            New target, where `n_samples` is the number of samples
+            and `n_components` is the number of pls components.
+
         Y : array-like of shape (n_samples, n_components)
             New target, where `n_samples` is the number of samples
             and `n_components` is the number of pls components.
 
+            .. deprecated:: 1.5
+               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.
+
         Returns
         -------
         X_reconstructed : ndarray of shape (n_samples, n_features)
             Return the reconstructed `X` data.
 
-        Y_reconstructed : ndarray of shape (n_samples, n_targets)
-            Return the reconstructed `X` target. Only returned when `Y` is given.
+        y_reconstructed : ndarray of shape (n_samples, n_targets)
+            Return the reconstructed `X` target. Only returned when `y` is given.
 
         Notes
         -----
         This transformation will only be exact if `n_components=n_features`.
         """
+        y = _deprecate_Y_when_optional(y, Y)
+
         check_is_fitted(self)
         X = check_array(X, input_name="X", dtype=FLOAT_DTYPES)
         # From pls space to original space
         X_reconstructed = np.matmul(X, self.x_loadings_.T)
         # Denormalize
         X_reconstructed *= self._x_std
         X_reconstructed += self._x_mean
 
-        if Y is not None:
-            Y = check_array(Y, input_name="Y", dtype=FLOAT_DTYPES)
+        if y is not None:
+            y = check_array(y, input_name="y", dtype=FLOAT_DTYPES)
             # From pls space to original space
-            Y_reconstructed = np.matmul(Y, self.y_loadings_.T)
+            y_reconstructed = np.matmul(y, self.y_loadings_.T)
             # Denormalize
-            Y_reconstructed *= self._y_std
-            Y_reconstructed += self._y_mean
-            return X_reconstructed, Y_reconstructed
+            y_reconstructed *= self._y_std
+            y_reconstructed += self._y_mean
+            return X_reconstructed, y_reconstructed
 
         return X_reconstructed
 
     def predict(self, X, copy=True):
         """Predict targets of given samples.
 
         Parameters
@@ -465,17 +513,16 @@
         -----
         This call requires the estimation of a matrix of shape
         `(n_features, n_targets)`, which may be an issue in high dimensional
         space.
         """
         check_is_fitted(self)
         X = self._validate_data(X, copy=copy, dtype=FLOAT_DTYPES, reset=False)
-        # Normalize
+        # Only center X but do not scale it since the coefficients are already scaled
         X -= self._x_mean
-        X /= self._x_std
         Ypred = X @ self.coef_.T + self.intercept_
         return Ypred.ravel() if self._predict_1d else Ypred
 
     def fit_transform(self, X, y=None):
         """Learn and apply the dimension reduction on the train data.
 
         Parameters
@@ -511,16 +558,15 @@
     Read more in the :ref:`User Guide <cross_decomposition>`.
 
     .. versionadded:: 0.8
 
     Parameters
     ----------
     n_components : int, default=2
-        Number of components to keep. Should be in `[1, min(n_samples,
-        n_features, n_targets)]`.
+        Number of components to keep. Should be in `[1, n_features]`.
 
     scale : bool, default=True
         Whether to scale `X` and `Y`.
 
     max_iter : int, default=500
         The maximum number of iterations of the power method when
         `algorithm='nipals'`. Ignored otherwise.
@@ -590,17 +636,17 @@
     --------
     PLSCanonical : Partial Least Squares transformer and regressor.
 
     Examples
     --------
     >>> from sklearn.cross_decomposition import PLSRegression
     >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
-    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
+    >>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
     >>> pls2 = PLSRegression(n_components=2)
-    >>> pls2.fit(X, Y)
+    >>> pls2.fit(X, y)
     PLSRegression()
     >>> Y_pred = pls2.predict(X)
 
     For a comparison between PLS Regression and :class:`~sklearn.decomposition.PCA`, see
     :ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`.
     """
 
@@ -624,33 +670,42 @@
             mode="A",
             algorithm="nipals",
             max_iter=max_iter,
             tol=tol,
             copy=copy,
         )
 
-    def fit(self, X, Y):
+    def fit(self, X, y=None, Y=None):
         """Fit model to data.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
             Training vectors, where `n_samples` is the number of samples and
             `n_features` is the number of predictors.
 
+        y : array-like of shape (n_samples,) or (n_samples, n_targets)
+            Target vectors, where `n_samples` is the number of samples and
+            `n_targets` is the number of response variables.
+
         Y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Target vectors, where `n_samples` is the number of samples and
             `n_targets` is the number of response variables.
 
+            .. deprecated:: 1.5
+               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.
+
         Returns
         -------
         self : object
             Fitted model.
         """
-        super().fit(X, Y)
+        y = _deprecate_Y_when_required(y, Y)
+
+        super().fit(X, y)
         # expose the fitted attributes `x_scores_` and `y_scores_`
         self.x_scores_ = self._x_scores
         self.y_scores_ = self._y_scores
         return self
 
 
 class PLSCanonical(_PLS):
@@ -741,19 +796,19 @@
     CCA : Canonical Correlation Analysis.
     PLSSVD : Partial Least Square SVD.
 
     Examples
     --------
     >>> from sklearn.cross_decomposition import PLSCanonical
     >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
-    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
+    >>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
     >>> plsca = PLSCanonical(n_components=2)
-    >>> plsca.fit(X, Y)
+    >>> plsca.fit(X, y)
     PLSCanonical()
-    >>> X_c, Y_c = plsca.transform(X, Y)
+    >>> X_c, y_c = plsca.transform(X, y)
     """
 
     _parameter_constraints: dict = {**_PLS._parameter_constraints}
     for param in ("deflation_mode", "mode"):
         _parameter_constraints.pop(param)
 
     # This implementation provides the same results that the "plspm" package
@@ -866,19 +921,19 @@
     PLSCanonical : Partial Least Squares transformer and regressor.
     PLSSVD : Partial Least Square SVD.
 
     Examples
     --------
     >>> from sklearn.cross_decomposition import CCA
     >>> X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]
-    >>> Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
+    >>> y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
     >>> cca = CCA(n_components=1)
-    >>> cca.fit(X, Y)
+    >>> cca.fit(X, y)
     CCA(n_components=1)
-    >>> X_c, Y_c = cca.transform(X, Y)
+    >>> X_c, Y_c = cca.transform(X, y)
     """
 
     _parameter_constraints: dict = {**_PLS._parameter_constraints}
     for param in ("deflation_mode", "mode", "algorithm"):
         _parameter_constraints.pop(param)
 
     def __init__(
@@ -950,21 +1005,21 @@
     --------
     >>> import numpy as np
     >>> from sklearn.cross_decomposition import PLSSVD
     >>> X = np.array([[0., 0., 1.],
     ...               [1., 0., 0.],
     ...               [2., 2., 2.],
     ...               [2., 5., 4.]])
-    >>> Y = np.array([[0.1, -0.2],
+    >>> y = np.array([[0.1, -0.2],
     ...               [0.9, 1.1],
     ...               [6.2, 5.9],
     ...               [11.9, 12.3]])
-    >>> pls = PLSSVD(n_components=2).fit(X, Y)
-    >>> X_c, Y_c = pls.transform(X, Y)
-    >>> X_c.shape, Y_c.shape
+    >>> pls = PLSSVD(n_components=2).fit(X, y)
+    >>> X_c, y_c = pls.transform(X, y)
+    >>> X_c.shape, y_c.shape
     ((4, 2), (4, 2))
     """
 
     _parameter_constraints: dict = {
         "n_components": [Interval(Integral, 1, None, closed="left")],
         "scale": ["boolean"],
         "copy": ["boolean"],
@@ -972,97 +1027,112 @@
 
     def __init__(self, n_components=2, *, scale=True, copy=True):
         self.n_components = n_components
         self.scale = scale
         self.copy = copy
 
     @_fit_context(prefer_skip_nested_validation=True)
-    def fit(self, X, Y):
+    def fit(self, X, y=None, Y=None):
         """Fit model to data.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
             Training samples.
 
+        y : array-like of shape (n_samples,) or (n_samples, n_targets)
+            Targets.
+
         Y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Targets.
 
+            .. deprecated:: 1.5
+               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.
+
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        check_consistent_length(X, Y)
+        y = _deprecate_Y_when_required(y, Y)
+        check_consistent_length(X, y)
         X = self._validate_data(
             X, dtype=np.float64, copy=self.copy, ensure_min_samples=2
         )
-        Y = check_array(
-            Y, input_name="Y", dtype=np.float64, copy=self.copy, ensure_2d=False
+        y = check_array(
+            y, input_name="y", dtype=np.float64, copy=self.copy, ensure_2d=False
         )
-        if Y.ndim == 1:
-            Y = Y.reshape(-1, 1)
+        if y.ndim == 1:
+            y = y.reshape(-1, 1)
 
-        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(Y)
+        # we'll compute the SVD of the cross-covariance matrix = X.T.dot(y)
         # This matrix rank is at most min(n_samples, n_features, n_targets) so
         # n_components cannot be bigger than that.
         n_components = self.n_components
-        rank_upper_bound = min(X.shape[0], X.shape[1], Y.shape[1])
+        rank_upper_bound = min(X.shape[0], X.shape[1], y.shape[1])
         if n_components > rank_upper_bound:
             raise ValueError(
                 f"`n_components` upper bound is {rank_upper_bound}. "
                 f"Got {n_components} instead. Reduce `n_components`."
             )
 
-        X, Y, self._x_mean, self._y_mean, self._x_std, self._y_std = _center_scale_xy(
-            X, Y, self.scale
+        X, y, self._x_mean, self._y_mean, self._x_std, self._y_std = _center_scale_xy(
+            X, y, self.scale
         )
 
         # Compute SVD of cross-covariance matrix
-        C = np.dot(X.T, Y)
+        C = np.dot(X.T, y)
         U, s, Vt = svd(C, full_matrices=False)
         U = U[:, :n_components]
         Vt = Vt[:n_components]
         U, Vt = svd_flip(U, Vt)
         V = Vt.T
 
         self.x_weights_ = U
         self.y_weights_ = V
         self._n_features_out = self.x_weights_.shape[1]
         return self
 
-    def transform(self, X, Y=None):
+    def transform(self, X, y=None, Y=None):
         """
         Apply the dimensionality reduction.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
             Samples to be transformed.
 
+        y : array-like of shape (n_samples,) or (n_samples, n_targets), \
+                default=None
+            Targets.
+
         Y : array-like of shape (n_samples,) or (n_samples, n_targets), \
                 default=None
             Targets.
 
+            .. deprecated:: 1.5
+               `Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead.
+
         Returns
         -------
         x_scores : array-like or tuple of array-like
             The transformed data `X_transformed` if `Y is not None`,
             `(X_transformed, Y_transformed)` otherwise.
         """
+        y = _deprecate_Y_when_optional(y, Y)
         check_is_fitted(self)
         X = self._validate_data(X, dtype=np.float64, reset=False)
         Xr = (X - self._x_mean) / self._x_std
         x_scores = np.dot(Xr, self.x_weights_)
-        if Y is not None:
-            Y = check_array(Y, input_name="Y", ensure_2d=False, dtype=np.float64)
-            if Y.ndim == 1:
-                Y = Y.reshape(-1, 1)
-            Yr = (Y - self._y_mean) / self._y_std
-            y_scores = np.dot(Yr, self.y_weights_)
+        if y is not None:
+            y = check_array(y, input_name="y", ensure_2d=False, dtype=np.float64)
+            if y.ndim == 1:
+                y = y.reshape(-1, 1)
+            yr = (y - self._y_mean) / self._y_std
+            y_scores = np.dot(yr, self.y_weights_)
             return x_scores, y_scores
         return x_scores
 
     def fit_transform(self, X, y=None):
         """Learn and apply the dimensionality reduction.
 
         Parameters
```

### Comparing `scikit-learn-1.4.2/sklearn/cross_decomposition/tests/test_pls.py` & `scikit_learn-1.5.0rc1/sklearn/cross_decomposition/tests/test_pls.py`

 * *Files 6% similar despite different names*

```diff
@@ -548,15 +548,15 @@
     """Checks warning when y is constant. Non-regression test for #19831"""
     rng = np.random.RandomState(42)
     x = rng.rand(100, 3)
     y = np.zeros(100)
 
     pls = PLSRegression()
 
-    msg = "Y residual is constant at iteration"
+    msg = "y residual is constant at iteration"
     with pytest.warns(UserWarning, match=msg):
         pls.fit(x, y)
 
     assert_allclose(pls.x_rotations_, 0)
 
 
 @pytest.mark.parametrize("PLSEstimator", [PLSRegression, PLSCanonical, CCA])
@@ -585,16 +585,14 @@
     Y = d.target
 
     pls = PLSEstimator(copy=True, scale=scale).fit(X, Y)
     Y_pred = pls.predict(X, copy=True)
 
     y_mean = Y.mean(axis=0)
     X_trans = X - X.mean(axis=0)
-    if scale:
-        X_trans /= X.std(axis=0, ddof=1)
 
     assert_allclose(pls.intercept_, y_mean)
     assert_allclose(Y_pred, X_trans @ pls.coef_.T + pls.intercept_)
 
 
 @pytest.mark.parametrize("Klass", [CCA, PLSSVD, PLSRegression, PLSCanonical])
 def test_pls_feature_names_out(Klass):
@@ -640,7 +638,102 @@
 
     # Check that it works in VotingRegressor
     lr = LinearRegression().fit(X, y)
     vr = VotingRegressor([("lr", lr), ("plsr", plsr)])
     y_pred = vr.fit(X, y).predict(X)
     assert y_pred.shape == expected.shape
     assert_allclose(y_pred, expected)
+
+
+def test_pls_regression_scaling_coef():
+    """Check that when using `scale=True`, the coefficients are using the std. dev. from
+    both `X` and `Y`.
+
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/27964
+    """
+    # handcrafted data where we can predict Y from X with an additional scaling factor
+    rng = np.random.RandomState(0)
+    coef = rng.uniform(size=(3, 5))
+    X = rng.normal(scale=10, size=(30, 5))  # add a std of 10
+    Y = X @ coef.T
+
+    # we need to make sure that the dimension of the latent space is large enough to
+    # perfectly predict `Y` from `X` (no information loss)
+    pls = PLSRegression(n_components=5, scale=True).fit(X, Y)
+    assert_allclose(pls.coef_, coef)
+
+    # we therefore should be able to predict `Y` from `X`
+    assert_allclose(pls.predict(X), Y)
+
+
+# TODO(1.7): Remove
+@pytest.mark.parametrize("Klass", [PLSRegression, CCA, PLSSVD, PLSCanonical])
+def test_pls_fit_warning_on_deprecated_Y_argument(Klass):
+    # Test warning message is shown when using Y instead of y
+
+    d = load_linnerud()
+    X = d.data
+    Y = d.target
+    y = d.target
+
+    msg = "`Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead."
+    with pytest.warns(FutureWarning, match=msg):
+        Klass().fit(X=X, Y=Y)
+
+    err_msg1 = "Cannot use both `y` and `Y`. Use only `y` as `Y` is deprecated."
+    with (
+        pytest.warns(FutureWarning, match=msg),
+        pytest.raises(ValueError, match=err_msg1),
+    ):
+        Klass().fit(X, y, Y)
+
+    err_msg2 = "y is required."
+    with pytest.raises(ValueError, match=err_msg2):
+        Klass().fit(X)
+
+
+# TODO(1.7): Remove
+@pytest.mark.parametrize("Klass", [PLSRegression, CCA, PLSSVD, PLSCanonical])
+def test_pls_transform_warning_on_deprecated_Y_argument(Klass):
+    # Test warning message is shown when using Y instead of y
+
+    d = load_linnerud()
+    X = d.data
+    Y = d.target
+    y = d.target
+
+    plsr = Klass().fit(X, y)
+    msg = "`Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead."
+    with pytest.warns(FutureWarning, match=msg):
+        plsr.transform(X=X, Y=Y)
+
+    err_msg1 = "Cannot use both `y` and `Y`. Use only `y` as `Y` is deprecated."
+    with (
+        pytest.warns(FutureWarning, match=msg),
+        pytest.raises(ValueError, match=err_msg1),
+    ):
+        plsr.transform(X, y, Y)
+
+
+# TODO(1.7): Remove
+@pytest.mark.parametrize("Klass", [PLSRegression, CCA, PLSCanonical])
+def test_pls_inverse_transform_warning_on_deprecated_Y_argument(Klass):
+    # Test warning message is shown when using Y instead of y
+
+    d = load_linnerud()
+    X = d.data
+    y = d.target
+
+    plsr = Klass().fit(X, y)
+    X_transformed, y_transformed = plsr.transform(X, y)
+
+    msg = "`Y` is deprecated in 1.5 and will be removed in 1.7. Use `y` instead."
+    with pytest.warns(FutureWarning, match=msg):
+        plsr.inverse_transform(X=X_transformed, Y=y_transformed)
+
+    err_msg1 = "Cannot use both `y` and `Y`. Use only `y` as `Y` is deprecated."
+    with (
+        pytest.warns(FutureWarning, match=msg),
+        pytest.raises(ValueError, match=err_msg1),
+    ):
+        plsr.inverse_transform(X=X_transformed, y=y_transformed, Y=y_transformed)
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """
 The :mod:`sklearn.datasets` module includes utilities to load datasets,
 including methods to load and fetch popular reference datasets. It also
 features some artificial data generators.
 """
+
 import textwrap
 
 from ._base import (
     clear_data_home,
     get_data_home,
     load_breast_cancer,
     load_diabetes,
@@ -102,15 +103,16 @@
     "make_spd_matrix",
     "make_swiss_roll",
 ]
 
 
 def __getattr__(name):
     if name == "load_boston":
-        msg = textwrap.dedent("""
+        msg = textwrap.dedent(
+            """
             `load_boston` has been removed from scikit-learn since version 1.2.
 
             The Boston housing prices dataset has an ethical problem: as
             investigated in [1], the authors of this dataset engineered a
             non-invertible variable "B" assuming that racial self-segregation had a
             positive impact on house prices [2]. Furthermore the goal of the
             research that led to the creation of this dataset was to study the
@@ -149,14 +151,15 @@
             "Racist data destruction?"
             <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>
 
             [2] Harrison Jr, David, and Daniel L. Rubinfeld.
             "Hedonic housing prices and the demand for clean air."
             Journal of environmental economics and management 5.1 (1978): 81-102.
             <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>
-            """)
+            """
+        )
         raise ImportError(msg)
     try:
         return globals()[name]
     except KeyError:
         # This is turned into the appropriate ImportError
         raise AttributeError
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_arff_parser.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_arff_parser.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,22 @@
 """Implementation of ARFF parsers: via LIAC-ARFF and pandas."""
+
 import itertools
 import re
 from collections import OrderedDict
 from collections.abc import Generator
 from typing import List
 
 import numpy as np
 import scipy as sp
 
 from ..externals import _arff
 from ..externals._arff import ArffSparseDataType
-from ..utils import (
-    _chunk_generator,
-    check_pandas_support,
-    get_chunk_n_rows,
-)
+from ..utils._chunking import chunk_generator, get_chunk_n_rows
+from ..utils._optional_dependencies import check_pandas_support
 from ..utils.fixes import pd_fillna
 
 
 def _split_sparse_columns(
     arff_data: ArffSparseDataType, include_columns: List
 ) -> ArffSparseDataType:
     """Obtains several columns from sparse ARFF representation. Additionally,
@@ -191,15 +189,15 @@
 
         row_bytes = first_df.memory_usage(deep=True).sum()
         chunksize = get_chunk_n_rows(row_bytes)
 
         # read arff data with chunks
         columns_to_keep = [col for col in columns_names if col in columns_to_select]
         dfs = [first_df[columns_to_keep]]
-        for data in _chunk_generator(arff_container["data"], chunksize):
+        for data in chunk_generator(arff_container["data"], chunksize):
             dfs.append(
                 pd.DataFrame(data, columns=columns_names, copy=False)[columns_to_keep]
             )
         # dfs[0] contains only one row, which may not have enough data to infer to
         # column's dtype. Here we use `dfs[1]` to configure the dtype in dfs[0]
         if len(dfs) >= 2:
             dfs[0] = dfs[0].astype(dfs[1].dtypes)
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_base.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,26 +7,30 @@
 #               2010 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 import csv
 import gzip
 import hashlib
 import os
 import shutil
+import time
+import warnings
 from collections import namedtuple
 from importlib import resources
 from numbers import Integral
 from os import environ, listdir, makedirs
 from os.path import expanduser, isdir, join, splitext
 from pathlib import Path
+from urllib.error import URLError
 from urllib.request import urlretrieve
 
 import numpy as np
 
 from ..preprocessing import scale
-from ..utils import Bunch, check_pandas_support, check_random_state
+from ..utils import Bunch, check_random_state
+from ..utils._optional_dependencies import check_pandas_support
 from ..utils._param_validation import Interval, StrOptions, validate_params
 
 DATA_MODULE = "sklearn.datasets.data"
 DESCR_MODULE = "sklearn.datasets.descr"
 IMAGES_MODULE = "sklearn.datasets.images"
 
 RemoteFileMetadata = namedtuple("RemoteFileMetadata", ["filename", "url", "checksum"])
@@ -59,14 +63,22 @@
         The path to scikit-learn data directory. If `None`, the default path
         is `~/scikit_learn_data`.
 
     Returns
     -------
     data_home: str
         The path to scikit-learn data directory.
+
+    Examples
+    --------
+    >>> import os
+    >>> from sklearn.datasets import get_data_home
+    >>> data_home_path = get_data_home()
+    >>> os.path.exists(data_home_path)
+    True
     """
     if data_home is None:
         data_home = environ.get("SCIKIT_LEARN_DATA", join("~", "scikit_learn_data"))
     data_home = expanduser(data_home)
     makedirs(data_home, exist_ok=True)
     return data_home
 
@@ -1218,14 +1230,23 @@
 
     (data, target) : tuple if ``return_X_y`` is True
         Returns a tuple of two ndarrays or dataframe of shape
         `(20, 3)`. Each row represents one sample and each column represents the
         features in `X` and a target in `y` of a given sample.
 
         .. versionadded:: 0.18
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_linnerud
+    >>> linnerud = load_linnerud()
+    >>> linnerud.data.shape
+    (20, 3)
+    >>> linnerud.target.shape
+    (20, 3)
     """
     data_filename = "linnerud_exercise.csv"
     target_filename = "linnerud_physiological.csv"
 
     data_module_path = resources.files(DATA_MODULE)
     # Read header and data
     data_path = data_module_path / data_filename
@@ -1403,15 +1424,15 @@
             buffer = f.read(chunk_size)
             if not buffer:
                 break
             sha256hash.update(buffer)
     return sha256hash.hexdigest()
 
 
-def _fetch_remote(remote, dirname=None):
+def _fetch_remote(remote, dirname=None, n_retries=3, delay=1):
     """Helper function to download a remote dataset into path
 
     Fetch a dataset pointed by remote's url, save into path using remote's
     filename and ensure its integrity based on the SHA256 Checksum of the
     downloaded file.
 
     Parameters
@@ -1419,22 +1440,43 @@
     remote : RemoteFileMetadata
         Named tuple containing remote dataset meta information: url, filename
         and checksum
 
     dirname : str
         Directory to save the file to.
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : int, default=1
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     file_path: str
         Full path of the created file.
     """
 
     file_path = remote.filename if dirname is None else join(dirname, remote.filename)
-    urlretrieve(remote.url, file_path)
+    while True:
+        try:
+            urlretrieve(remote.url, file_path)
+            break
+        except (URLError, TimeoutError):
+            if n_retries == 0:
+                # If no more retries are left, re-raise the caught exception.
+                raise
+            warnings.warn(f"Retry downloading from url: {remote.url}")
+            n_retries -= 1
+            time.sleep(delay)
+
     checksum = _sha256(file_path)
     if remote.checksum != checksum:
         raise OSError(
             "{} has an SHA256 checksum ({}) "
             "differing from expected ({}), "
             "file may be corrupted.".format(file_path, checksum, remote.checksum)
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_california_housing.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_california_housing.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,27 +14,29 @@
 References
 ----------
 
 Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
 Statistics and Probability Letters, 33 (1997) 291-297.
 
 """
+
 # Authors: Peter Prettenhofer
 # License: BSD 3 clause
 
 import logging
 import tarfile
+from numbers import Integral, Real
 from os import PathLike, makedirs, remove
 from os.path import exists
 
 import joblib
 import numpy as np
 
 from ..utils import Bunch
-from ..utils._param_validation import validate_params
+from ..utils._param_validation import Interval, validate_params
 from . import get_data_home
 from ._base import (
     RemoteFileMetadata,
     _convert_data_dataframe,
     _fetch_remote,
     _pkl_filepath,
     load_descr,
@@ -53,19 +55,27 @@
 
 @validate_params(
     {
         "data_home": [str, PathLike, None],
         "download_if_missing": ["boolean"],
         "return_X_y": ["boolean"],
         "as_frame": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_california_housing(
-    *, data_home=None, download_if_missing=True, return_X_y=False, as_frame=False
+    *,
+    data_home=None,
+    download_if_missing=True,
+    return_X_y=False,
+    as_frame=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the California housing dataset (regression).
 
     ==============   ==============
     Samples total             20640
     Dimensionality                8
     Features                   real
@@ -93,14 +103,24 @@
     as_frame : bool, default=False
         If True, the data is a pandas DataFrame including columns with
         appropriate dtypes (numeric, string or categorical). The target is
         a pandas DataFrame or Series depending on the number of target_columns.
 
         .. versionadded:: 0.23
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     dataset : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : ndarray, shape (20640, 8)
             Each row corresponding to the 8 feature values in order.
@@ -150,15 +170,20 @@
         if not download_if_missing:
             raise OSError("Data not found and `download_if_missing` is False")
 
         logger.info(
             "Downloading Cal. housing from {} to {}".format(ARCHIVE.url, data_home)
         )
 
-        archive_path = _fetch_remote(ARCHIVE, dirname=data_home)
+        archive_path = _fetch_remote(
+            ARCHIVE,
+            dirname=data_home,
+            n_retries=n_retries,
+            delay=delay,
+        )
 
         with tarfile.open(mode="r:gz", name=archive_path) as f:
             cal_housing = np.loadtxt(
                 f.extractfile("CaliforniaHousing/cal_housing.data"), delimiter=","
             )
             # Columns are not in the same order compared to the previous
             # URL resource on lib.stat.cmu.edu
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_covtype.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_covtype.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,22 +13,23 @@
 # Author: Lars Buitinck
 #         Peter Prettenhofer <peter.prettenhofer@gmail.com>
 # License: BSD 3 clause
 
 import logging
 import os
 from gzip import GzipFile
+from numbers import Integral, Real
 from os.path import exists, join
 from tempfile import TemporaryDirectory
 
 import joblib
 import numpy as np
 
 from ..utils import Bunch, check_random_state
-from ..utils._param_validation import validate_params
+from ..utils._param_validation import Interval, validate_params
 from . import get_data_home
 from ._base import (
     RemoteFileMetadata,
     _convert_data_dataframe,
     _fetch_remote,
     _pkl_filepath,
     load_descr,
@@ -67,25 +68,29 @@
     {
         "data_home": [str, os.PathLike, None],
         "download_if_missing": ["boolean"],
         "random_state": ["random_state"],
         "shuffle": ["boolean"],
         "return_X_y": ["boolean"],
         "as_frame": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_covtype(
     *,
     data_home=None,
     download_if_missing=True,
     random_state=None,
     shuffle=False,
     return_X_y=False,
     as_frame=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the covertype dataset (classification).
 
     Download it if necessary.
 
     =================   ============
     Classes                        7
@@ -125,14 +130,24 @@
         appropriate dtypes (numeric). The target is a pandas DataFrame or
         Series depending on the number of target columns. If `return_X_y` is
         True, then (`data`, `target`) will be pandas DataFrames or Series as
         described below.
 
         .. versionadded:: 0.24
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     dataset : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : ndarray of shape (581012, 54)
             Each row corresponds to the 54 features in the dataset.
@@ -179,15 +194,17 @@
         os.makedirs(covtype_dir, exist_ok=True)
 
         # Creating temp_dir as a direct subdirectory of the target directory
         # guarantees that both reside on the same filesystem, so that we can use
         # os.rename to atomically move the data files to their target location.
         with TemporaryDirectory(dir=covtype_dir) as temp_dir:
             logger.info(f"Downloading {ARCHIVE.url}")
-            archive_path = _fetch_remote(ARCHIVE, dirname=temp_dir)
+            archive_path = _fetch_remote(
+                ARCHIVE, dirname=temp_dir, n_retries=n_retries, delay=delay
+            )
             Xy = np.genfromtxt(GzipFile(filename=archive_path), delimiter=",")
 
             X = Xy[:, :-1]
             y = Xy[:, -1].astype(np.int32, copy=False)
 
             samples_tmp_path = _pkl_filepath(temp_dir, "samples")
             joblib.dump(X, samples_tmp_path, compress=9)
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_kddcup99.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_kddcup99.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,22 +8,23 @@
 
 """
 
 import errno
 import logging
 import os
 from gzip import GzipFile
+from numbers import Integral, Real
 from os.path import exists, join
 
 import joblib
 import numpy as np
 
 from ..utils import Bunch, check_random_state
 from ..utils import shuffle as shuffle_method
-from ..utils._param_validation import StrOptions, validate_params
+from ..utils._param_validation import Interval, StrOptions, validate_params
 from . import get_data_home
 from ._base import (
     RemoteFileMetadata,
     _convert_data_dataframe,
     _fetch_remote,
     load_descr,
 )
@@ -53,27 +54,31 @@
         "data_home": [str, os.PathLike, None],
         "shuffle": ["boolean"],
         "random_state": ["random_state"],
         "percent10": ["boolean"],
         "download_if_missing": ["boolean"],
         "return_X_y": ["boolean"],
         "as_frame": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_kddcup99(
     *,
     subset=None,
     data_home=None,
     shuffle=False,
     random_state=None,
     percent10=True,
     download_if_missing=True,
     return_X_y=False,
     as_frame=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the kddcup99 dataset (classification).
 
     Download it if necessary.
 
     =================   ====================================
     Classes                                               23
@@ -123,14 +128,24 @@
     as_frame : bool, default=False
         If `True`, returns a pandas Dataframe for the ``data`` and ``target``
         objects in the `Bunch` returned object; `Bunch` return object will also
         have a ``frame`` member.
 
         .. versionadded:: 0.24
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     data : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : {ndarray, dataframe} of shape (494021, 41)
             The data matrix to learn. If `as_frame=True`, `data` will be a
@@ -156,14 +171,16 @@
         .. versionadded:: 0.20
     """
     data_home = get_data_home(data_home=data_home)
     kddcup99 = _fetch_brute_kddcup99(
         data_home=data_home,
         percent10=percent10,
         download_if_missing=download_if_missing,
+        n_retries=n_retries,
+        delay=delay,
     )
 
     data = kddcup99.data
     target = kddcup99.target
     feature_names = kddcup99.feature_names
     target_names = kddcup99.target_names
 
@@ -239,15 +256,17 @@
         frame=frame,
         target_names=target_names,
         feature_names=feature_names,
         DESCR=fdescr,
     )
 
 
-def _fetch_brute_kddcup99(data_home=None, download_if_missing=True, percent10=True):
+def _fetch_brute_kddcup99(
+    data_home=None, download_if_missing=True, percent10=True, n_retries=3, delay=1.0
+):
     """Load the kddcup99 dataset, downloading it if necessary.
 
     Parameters
     ----------
     data_home : str, default=None
         Specify another download and cache folder for the datasets. By default
         all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
@@ -255,14 +274,20 @@
     download_if_missing : bool, default=True
         If False, raise an OSError if the data is not locally available
         instead of trying to download the data from the source site.
 
     percent10 : bool, default=True
         Whether to load only 10 percent of the data.
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
     Returns
     -------
     dataset : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : ndarray of shape (494021, 41)
             Each row corresponds to the 41 features in the dataset.
@@ -350,15 +375,15 @@
                 "The cache for fetch_kddcup99 is invalid, please delete "
                 f"{str(kddcup_dir)} and run the fetch_kddcup99 again"
             ) from e
 
     elif download_if_missing:
         _mkdirp(kddcup_dir)
         logger.info("Downloading %s" % archive.url)
-        _fetch_remote(archive, dirname=kddcup_dir)
+        _fetch_remote(archive, dirname=kddcup_dir, n_retries=n_retries, delay=delay)
         DT = np.dtype(dt)
         logger.debug("extracting archive")
         archive_path = join(kddcup_dir, archive.filename)
         file_ = GzipFile(filename=archive_path, mode="r")
         Xy = []
         for line in file_.readlines():
             line = line.decode()
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_lfw.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_lfw.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 """Labeled Faces in the Wild (LFW) dataset
 
 This dataset is a collection of JPEG pictures of famous people collected
 over the internet, all details are available on the official website:
 
     http://vis-www.cs.umass.edu/lfw/
 """
+
 # Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 
 import logging
 from numbers import Integral, Real
 from os import PathLike, listdir, makedirs, remove
 from os.path import exists, isdir, join
 
 import numpy as np
 from joblib import Memory
 
 from ..utils import Bunch
 from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params
+from ..utils.fixes import tarfile_extractall
 from ._base import (
     RemoteFileMetadata,
     _fetch_remote,
     get_data_home,
     load_descr,
 )
 
@@ -68,29 +70,33 @@
 
 #
 # Common private utilities for data fetching from the original LFW website
 # local disk caching, and image decoding.
 #
 
 
-def _check_fetch_lfw(data_home=None, funneled=True, download_if_missing=True):
+def _check_fetch_lfw(
+    data_home=None, funneled=True, download_if_missing=True, n_retries=3, delay=1.0
+):
     """Helper function to download any missing LFW data"""
 
     data_home = get_data_home(data_home=data_home)
     lfw_home = join(data_home, "lfw_home")
 
     if not exists(lfw_home):
         makedirs(lfw_home)
 
     for target in TARGETS:
         target_filepath = join(lfw_home, target.filename)
         if not exists(target_filepath):
             if download_if_missing:
                 logger.info("Downloading LFW metadata: %s", target.url)
-                _fetch_remote(target, dirname=lfw_home)
+                _fetch_remote(
+                    target, dirname=lfw_home, n_retries=n_retries, delay=delay
+                )
             else:
                 raise OSError("%s is missing" % target_filepath)
 
     if funneled:
         data_folder_path = join(lfw_home, "lfw_funneled")
         archive = FUNNELED_ARCHIVE
     else:
@@ -98,22 +104,25 @@
         archive = ARCHIVE
 
     if not exists(data_folder_path):
         archive_path = join(lfw_home, archive.filename)
         if not exists(archive_path):
             if download_if_missing:
                 logger.info("Downloading LFW data (~200MB): %s", archive.url)
-                _fetch_remote(archive, dirname=lfw_home)
+                _fetch_remote(
+                    archive, dirname=lfw_home, n_retries=n_retries, delay=delay
+                )
             else:
                 raise OSError("%s is missing" % archive_path)
 
         import tarfile
 
         logger.debug("Decompressing the data archive to %s", data_folder_path)
-        tarfile.open(archive_path, "r:gz").extractall(path=lfw_home)
+        with tarfile.open(archive_path, "r:gz") as fp:
+            tarfile_extractall(fp, path=lfw_home)
         remove(archive_path)
 
     return lfw_home, data_folder_path
 
 
 def _load_imgs(file_paths, slice_, color, resize):
     """Internally used to load images"""
@@ -238,27 +247,31 @@
         "funneled": ["boolean"],
         "resize": [Interval(Real, 0, None, closed="neither"), None],
         "min_faces_per_person": [Interval(Integral, 0, None, closed="left"), None],
         "color": ["boolean"],
         "slice_": [tuple, Hidden(None)],
         "download_if_missing": ["boolean"],
         "return_X_y": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_lfw_people(
     *,
     data_home=None,
     funneled=True,
     resize=0.5,
     min_faces_per_person=0,
     color=False,
     slice_=(slice(70, 195), slice(78, 172)),
     download_if_missing=True,
     return_X_y=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the Labeled Faces in the Wild (LFW) people dataset \
 (classification).
 
     Download it if necessary.
 
     =================   =======================
@@ -304,14 +317,24 @@
     return_X_y : bool, default=False
         If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch
         object. See below for more information about the `dataset.data` and
         `dataset.target` object.
 
         .. versionadded:: 0.20
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     dataset : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : numpy array of shape (13233, 2914)
             Each row corresponds to a ravelled face image
@@ -334,17 +357,37 @@
     (data, target) : tuple if ``return_X_y`` is True
         A tuple of two ndarray. The first containing a 2D array of
         shape (n_samples, n_features) with each row representing one
         sample and each column representing the features. The second
         ndarray of shape (n_samples,) containing the target samples.
 
         .. versionadded:: 0.20
+
+    Examples
+    --------
+    >>> from sklearn.datasets import fetch_lfw_people
+    >>> lfw_people = fetch_lfw_people()
+    >>> lfw_people.data.shape
+    (13233, 2914)
+    >>> lfw_people.target.shape
+    (13233,)
+    >>> for name in lfw_people.target_names[:5]:
+    ...    print(name)
+    AJ Cook
+    AJ Lamas
+    Aaron Eckhart
+    Aaron Guiel
+    Aaron Patterson
     """
     lfw_home, data_folder_path = _check_fetch_lfw(
-        data_home=data_home, funneled=funneled, download_if_missing=download_if_missing
+        data_home=data_home,
+        funneled=funneled,
+        download_if_missing=download_if_missing,
+        n_retries=n_retries,
+        delay=delay,
     )
     logger.debug("Loading LFW people faces from %s", lfw_home)
 
     # wrap the loader in a memoizing function that will return memmaped data
     # arrays for optimal memory usage
     m = Memory(location=lfw_home, compress=6, verbose=0)
     load_func = m.cache(_fetch_lfw_people)
@@ -433,26 +476,30 @@
         "subset": [StrOptions({"train", "test", "10_folds"})],
         "data_home": [str, PathLike, None],
         "funneled": ["boolean"],
         "resize": [Interval(Real, 0, None, closed="neither"), None],
         "color": ["boolean"],
         "slice_": [tuple, Hidden(None)],
         "download_if_missing": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_lfw_pairs(
     *,
     subset="train",
     data_home=None,
     funneled=True,
     resize=0.5,
     color=False,
     slice_=(slice(70, 195), slice(78, 172)),
     download_if_missing=True,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).
 
     Download it if necessary.
 
     =================   =======================
     Classes                                   2
@@ -501,14 +548,24 @@
         'interesting' part of the jpeg files and avoid use statistical
         correlation from the background.
 
     download_if_missing : bool, default=True
         If False, raise an OSError if the data is not locally available
         instead of trying to download the data from the source site.
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     data : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : ndarray of shape (2200, 5828). Shape depends on ``subset``.
             Each row corresponds to 2 ravel'd face images
@@ -525,17 +582,34 @@
             Labels associated to each pair of images.
             The two label values being different persons or the same person.
         target_names : numpy array of shape (2,)
             Explains the target values of the target array.
             0 corresponds to "Different person", 1 corresponds to "same person".
         DESCR : str
             Description of the Labeled Faces in the Wild (LFW) dataset.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import fetch_lfw_pairs
+    >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')
+    >>> list(lfw_pairs_train.target_names)
+    ['Different persons', 'Same person']
+    >>> lfw_pairs_train.pairs.shape
+    (2200, 2, 62, 47)
+    >>> lfw_pairs_train.data.shape
+    (2200, 5828)
+    >>> lfw_pairs_train.target.shape
+    (2200,)
     """
     lfw_home, data_folder_path = _check_fetch_lfw(
-        data_home=data_home, funneled=funneled, download_if_missing=download_if_missing
+        data_home=data_home,
+        funneled=funneled,
+        download_if_missing=download_if_missing,
+        n_retries=n_retries,
+        delay=delay,
     )
     logger.debug("Loading %s LFW pairs from %s", subset, lfw_home)
 
     # wrap the loader in a memoizing function that will return memmaped data
     # arrays for optimal memory usage
     m = Memory(location=lfw_home, compress=6, verbose=0)
     load_func = m.cache(_fetch_lfw_pairs)
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_olivetti_faces.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_olivetti_faces.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,23 +9,24 @@
 
     https://cs.nyu.edu/~roweis/
 """
 
 # Copyright (c) 2011 David Warde-Farley <wardefar at iro dot umontreal dot ca>
 # License: BSD 3 clause
 
+from numbers import Integral, Real
 from os import PathLike, makedirs, remove
 from os.path import exists
 
 import joblib
 import numpy as np
 from scipy.io import loadmat
 
 from ..utils import Bunch, check_random_state
-from ..utils._param_validation import validate_params
+from ..utils._param_validation import Interval, validate_params
 from . import get_data_home
 from ._base import RemoteFileMetadata, _fetch_remote, _pkl_filepath, load_descr
 
 # The original data can be found at:
 # https://cs.nyu.edu/~roweis/data/olivettifaces.mat
 FACES = RemoteFileMetadata(
     filename="olivettifaces.mat",
@@ -37,24 +38,28 @@
 @validate_params(
     {
         "data_home": [str, PathLike, None],
         "shuffle": ["boolean"],
         "random_state": ["random_state"],
         "download_if_missing": ["boolean"],
         "return_X_y": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_olivetti_faces(
     *,
     data_home=None,
     shuffle=False,
     random_state=0,
     download_if_missing=True,
     return_X_y=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the Olivetti faces data-set from AT&T (classification).
 
     Download it if necessary.
 
     =================   =====================
     Classes                                40
@@ -86,14 +91,24 @@
 
     return_X_y : bool, default=False
         If True, returns `(data, target)` instead of a `Bunch` object. See
         below for more information about the `data` and `target` object.
 
         .. versionadded:: 0.22
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     data : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data: ndarray, shape (400, 4096)
             Each row corresponds to a ravelled
@@ -108,25 +123,38 @@
         DESCR : str
             Description of the modified Olivetti Faces Dataset.
 
     (data, target) : tuple if `return_X_y=True`
         Tuple with the `data` and `target` objects described above.
 
         .. versionadded:: 0.22
+
+    Examples
+    --------
+    >>> from sklearn.datasets import fetch_olivetti_faces
+    >>> olivetti_faces = fetch_olivetti_faces()
+    >>> olivetti_faces.data.shape
+    (400, 4096)
+    >>> olivetti_faces.target.shape
+    (400,)
+    >>> olivetti_faces.images.shape
+    (400, 64, 64)
     """
     data_home = get_data_home(data_home=data_home)
     if not exists(data_home):
         makedirs(data_home)
     filepath = _pkl_filepath(data_home, "olivetti.pkz")
     if not exists(filepath):
         if not download_if_missing:
             raise OSError("Data not found and `download_if_missing` is False")
 
         print("downloading Olivetti faces from %s to %s" % (FACES.url, data_home))
-        mat_path = _fetch_remote(FACES, dirname=data_home)
+        mat_path = _fetch_remote(
+            FACES, dirname=data_home, n_retries=n_retries, delay=delay
+        )
         mfile = loadmat(file_name=mat_path)
         # delete raw .mat data
         remove(mat_path)
 
         faces = mfile["faces"].T.copy()
         joblib.dump(faces, filepath, compress=6)
         del mfile
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_openml.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_openml.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,18 +11,16 @@
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 from urllib.error import HTTPError, URLError
 from urllib.request import Request, urlopen
 from warnings import warn
 
 import numpy as np
 
-from ..utils import (
-    Bunch,
-    check_pandas_support,  # noqa  # noqa
-)
+from ..utils import Bunch
+from ..utils._optional_dependencies import check_pandas_support  # noqa
 from ..utils._param_validation import (
     Integral,
     Interval,
     Real,
     StrOptions,
     validate_params,
 )
@@ -757,15 +755,15 @@
         "data_id": [Interval(Integral, 1, None, closed="left"), None],
         "data_home": [str, os.PathLike, None],
         "target_column": [str, list, None],
         "cache": [bool],
         "return_X_y": [bool],
         "as_frame": [bool, StrOptions({"auto"})],
         "n_retries": [Interval(Integral, 1, None, closed="left")],
-        "delay": [Interval(Real, 0, None, closed="right")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
         "parser": [
             StrOptions({"auto", "pandas", "liac-arff"}),
         ],
         "read_csv_kwargs": [dict, None],
     },
     prefer_skip_nested_validation=True,
 )
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_rcv1.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_rcv1.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,24 +6,25 @@
 """
 
 # Author: Tom Dupre la Tour
 # License: BSD 3 clause
 
 import logging
 from gzip import GzipFile
+from numbers import Integral, Real
 from os import PathLike, makedirs, remove
 from os.path import exists, join
 
 import joblib
 import numpy as np
 import scipy.sparse as sp
 
 from ..utils import Bunch
 from ..utils import shuffle as shuffle_
-from ..utils._param_validation import StrOptions, validate_params
+from ..utils._param_validation import Interval, StrOptions, validate_params
 from . import get_data_home
 from ._base import RemoteFileMetadata, _fetch_remote, _pkl_filepath, load_descr
 from ._svmlight_format_io import load_svmlight_files
 
 # The original vectorized data can be found at:
 #    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt0.dat.gz
 #    http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a13-vector-files/lyrl2004_vectors_test_pt1.dat.gz
@@ -76,25 +77,29 @@
     {
         "data_home": [str, PathLike, None],
         "subset": [StrOptions({"train", "test", "all"})],
         "download_if_missing": ["boolean"],
         "random_state": ["random_state"],
         "shuffle": ["boolean"],
         "return_X_y": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_rcv1(
     *,
     data_home=None,
     subset="all",
     download_if_missing=True,
     random_state=None,
     shuffle=False,
     return_X_y=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the RCV1 multilabel dataset (classification).
 
     Download it if necessary.
 
     Version: RCV1-v2, vectors, full sets, topics multilabels.
 
@@ -136,14 +141,24 @@
     return_X_y : bool, default=False
         If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch
         object. See below for more information about the `dataset.data` and
         `dataset.target` object.
 
         .. versionadded:: 0.20
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     dataset : :class:`~sklearn.utils.Bunch`
         Dictionary-like object. Returned only if `return_X_y` is False.
         `dataset` has the following attributes:
 
         - data : sparse matrix of shape (804414, 47236), dtype=np.float64
@@ -159,14 +174,23 @@
             Description of the RCV1 dataset.
 
     (data, target) : tuple
         A tuple consisting of `dataset.data` and `dataset.target`, as
         described above. Returned only if `return_X_y` is True.
 
         .. versionadded:: 0.20
+
+    Examples
+    --------
+    >>> from sklearn.datasets import fetch_rcv1
+    >>> rcv1 = fetch_rcv1()
+    >>> rcv1.data.shape
+    (804414, 47236)
+    >>> rcv1.target.shape
+    (804414, 103)
     """
     N_SAMPLES = 804414
     N_FEATURES = 47236
     N_CATEGORIES = 103
     N_TRAIN = 23149
 
     data_home = get_data_home(data_home=data_home)
@@ -181,15 +205,17 @@
     topics_path = _pkl_filepath(rcv1_dir, "topics_names.pkl")
 
     # load data (X) and sample_id
     if download_if_missing and (not exists(samples_path) or not exists(sample_id_path)):
         files = []
         for each in XY_METADATA:
             logger.info("Downloading %s" % each.url)
-            file_path = _fetch_remote(each, dirname=rcv1_dir)
+            file_path = _fetch_remote(
+                each, dirname=rcv1_dir, n_retries=n_retries, delay=delay
+            )
             files.append(GzipFile(filename=file_path))
 
         Xy = load_svmlight_files(files, n_features=N_FEATURES)
 
         # Training data is before testing data
         X = sp.vstack([Xy[8], Xy[0], Xy[2], Xy[4], Xy[6]]).tocsr()
         sample_id = np.hstack((Xy[9], Xy[1], Xy[3], Xy[5], Xy[7]))
@@ -207,15 +233,17 @@
         sample_id = joblib.load(sample_id_path)
 
     # load target (y), categories, and sample_id_bis
     if download_if_missing and (
         not exists(sample_topics_path) or not exists(topics_path)
     ):
         logger.info("Downloading %s" % TOPICS_METADATA.url)
-        topics_archive_path = _fetch_remote(TOPICS_METADATA, dirname=rcv1_dir)
+        topics_archive_path = _fetch_remote(
+            TOPICS_METADATA, dirname=rcv1_dir, n_retries=n_retries, delay=delay
+        )
 
         # parse the target file
         n_cat = -1
         n_doc = -1
         doc_previous = -1
         y = np.zeros((N_SAMPLES, N_CATEGORIES), dtype=np.uint8)
         sample_id_bis = np.zeros(N_SAMPLES, dtype=np.int32)
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_samples_generator.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_samples_generator.py`

 * *Files 4% similar despite different names*

```diff
@@ -217,17 +217,15 @@
             " features"
         )
     # Use log2 to avoid overflow errors
     if n_informative < np.log2(n_classes * n_clusters_per_class):
         msg = "n_classes({}) * n_clusters_per_class({}) must be"
         msg += " smaller or equal 2**n_informative({})={}"
         raise ValueError(
-            msg.format(
-                n_classes, n_clusters_per_class, n_informative, 2**n_informative
-            )
+            msg.format(n_classes, n_clusters_per_class, n_informative, 2**n_informative)
         )
 
     if weights is not None:
         if len(weights) not in [n_classes, n_classes - 1]:
             raise ValueError(
                 "Weights specified but incompatible with number of classes."
             )
@@ -545,14 +543,25 @@
     --------
     make_gaussian_quantiles : A generalization of this dataset approach.
 
     References
     ----------
     .. [1] T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical
            Learning Ed. 2", Springer, 2009.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_hastie_10_2
+    >>> X, y = make_hastie_10_2(n_samples=24000, random_state=42)
+    >>> X.shape
+    (24000, 10)
+    >>> y.shape
+    (24000,)
+    >>> list(y[:5])
+    [-1.0, 1.0, -1.0, 1.0, -1.0]
     """
     rs = check_random_state(random_state)
 
     shape = (n_samples, 10)
     X = rs.normal(size=shape).reshape(shape)
     y = ((X**2.0).sum(axis=1) > 9.34).astype(np.float64, copy=False)
     y[y == 0.0] = -1.0
@@ -860,14 +869,23 @@
     Returns
     -------
     X : ndarray of shape (n_samples, 2)
         The generated samples.
 
     y : ndarray of shape (n_samples,)
         The integer labels (0 or 1) for class membership of each sample.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_moons
+    >>> X, y = make_moons(n_samples=200, noise=0.2, random_state=42)
+    >>> X.shape
+    (200, 2)
+    >>> y.shape
+    (200,)
     """
 
     if isinstance(n_samples, numbers.Integral):
         n_samples_out = n_samples // 2
         n_samples_in = n_samples - n_samples_out
     else:
         try:
@@ -1396,14 +1414,28 @@
         for reproducible output across multiple function calls.
         See :term:`Glossary <random_state>`.
 
     Returns
     -------
     X : ndarray of shape (n_samples, n_features)
         The matrix.
+
+    Examples
+    --------
+    >>> from numpy.linalg import svd
+    >>> from sklearn.datasets import make_low_rank_matrix
+    >>> X = make_low_rank_matrix(
+    ...     n_samples=50,
+    ...     n_features=25,
+    ...     effective_rank=5,
+    ...     tail_strength=0.01,
+    ...     random_state=0,
+    ... )
+    >>> X.shape
+    (50, 25)
     """
     generator = check_random_state(random_state)
     n = min(n_samples, n_features)
 
     # Random (ortho normal) vectors
     u, _ = linalg.qr(
         generator.standard_normal(size=(n_samples, n)),
@@ -1430,32 +1462,30 @@
 @validate_params(
     {
         "n_samples": [Interval(Integral, 1, None, closed="left")],
         "n_components": [Interval(Integral, 1, None, closed="left")],
         "n_features": [Interval(Integral, 1, None, closed="left")],
         "n_nonzero_coefs": [Interval(Integral, 1, None, closed="left")],
         "random_state": ["random_state"],
-        "data_transposed": ["boolean", Hidden(StrOptions({"deprecated"}))],
     },
     prefer_skip_nested_validation=True,
 )
 def make_sparse_coded_signal(
     n_samples,
     *,
     n_components,
     n_features,
     n_nonzero_coefs,
     random_state=None,
-    data_transposed="deprecated",
 ):
     """Generate a signal as a sparse combination of dictionary elements.
 
-    Returns a matrix `Y = DX`, such that `D` is of shape `(n_features, n_components)`,
-    `X` is of shape `(n_components, n_samples)` and each column of `X` has exactly
-    `n_nonzero_coefs` non-zero elements.
+    Returns matrices `Y`, `D` and `X` such that `Y = XD` where `X` is of shape
+    `(n_samples, n_components)`, `D` is of shape `(n_components, n_features)`, and
+    each row of `X` has exactly `n_nonzero_coefs` non-zero elements.
 
     Read more in the :ref:`User Guide <sample_generators>`.
 
     Parameters
     ----------
     n_samples : int
         Number of samples to generate.
@@ -1470,41 +1500,42 @@
         Number of active (non-zero) coefficients in each sample.
 
     random_state : int, RandomState instance or None, default=None
         Determines random number generation for dataset creation. Pass an int
         for reproducible output across multiple function calls.
         See :term:`Glossary <random_state>`.
 
-    data_transposed : bool, default=False
-        By default, Y, D and X are not transposed.
-
-        .. versionadded:: 1.1
-
-        .. versionchanged:: 1.3
-            Default value changed from True to False.
-
-        .. deprecated:: 1.3
-            `data_transposed` is deprecated and will be removed in 1.5.
-
     Returns
     -------
-    data : ndarray of shape (n_features, n_samples) or (n_samples, n_features)
-        The encoded signal (Y). The shape is `(n_samples, n_features)` if
-        `data_transposed` is False, otherwise it's `(n_features, n_samples)`.
-
-    dictionary : ndarray of shape (n_features, n_components) or \
-            (n_components, n_features)
-        The dictionary with normalized components (D). The shape is
-        `(n_components, n_features)` if `data_transposed` is False, otherwise it's
-        `(n_features, n_components)`.
+    data : ndarray of shape (n_samples, n_features)
+        The encoded signal (Y).
+
+    dictionary : ndarray of shape (n_components, n_features)
+        The dictionary with normalized components (D).
 
-    code : ndarray of shape (n_components, n_samples) or (n_samples, n_components)
+    code : ndarray of shape (n_samples, n_components)
         The sparse code such that each column of this matrix has exactly
-        n_nonzero_coefs non-zero items (X). The shape is `(n_samples, n_components)`
-        if `data_transposed` is False, otherwise it's `(n_components, n_samples)`.
+        n_nonzero_coefs non-zero items (X).
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_sparse_coded_signal
+    >>> data, dictionary, code = make_sparse_coded_signal(
+    ...     n_samples=50,
+    ...     n_components=100,
+    ...     n_features=10,
+    ...     n_nonzero_coefs=4,
+    ...     random_state=0
+    ... )
+    >>> data.shape
+    (50, 10)
+    >>> dictionary.shape
+    (100, 10)
+    >>> code.shape
+    (50, 100)
     """
     generator = check_random_state(random_state)
 
     # generate dictionary
     D = generator.standard_normal(size=(n_features, n_components))
     D /= np.sqrt(np.sum((D**2), axis=0))
 
@@ -1515,27 +1546,16 @@
         generator.shuffle(idx)
         idx = idx[:n_nonzero_coefs]
         X[idx, i] = generator.standard_normal(size=n_nonzero_coefs)
 
     # encode signal
     Y = np.dot(D, X)
 
-    # TODO(1.5) remove data_transposed
-    # raise warning if data_transposed is not passed explicitly
-    if data_transposed != "deprecated":
-        warnings.warn(
-            "data_transposed was deprecated in version 1.3 and will be removed in 1.5.",
-            FutureWarning,
-        )
-    else:
-        data_transposed = False
-
-    # transpose if needed
-    if not data_transposed:
-        Y, D, X = Y.T, D.T, X.T
+    # Transpose to have shapes consistent with the rest of the API
+    Y, D, X = Y.T, D.T, X.T
 
     return map(np.squeeze, (Y, D, X))
 
 
 @validate_params(
     {
         "n_samples": [Interval(Integral, 1, None, closed="left")],
@@ -1579,14 +1599,23 @@
         The output values.
 
     References
     ----------
     .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,
            "Regularization in regression: comparing Bayesian and frequentist
            methods in a poorly informative situation", 2009.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_sparse_uncorrelated
+    >>> X, y = make_sparse_uncorrelated(random_state=0)
+    >>> X.shape
+    (100, 10)
+    >>> y.shape
+    (100,)
     """
     generator = check_random_state(random_state)
 
     X = generator.normal(loc=0, scale=1, size=(n_samples, n_features))
     y = generator.normal(
         loc=(X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]),
         scale=np.ones(n_samples),
@@ -1839,14 +1868,23 @@
     The algorithm is from Marsland [1].
 
     References
     ----------
     .. [1] S. Marsland, "Machine Learning: An Algorithmic Perspective", 2nd edition,
            Chapter 6, 2014.
            https://homepages.ecs.vuw.ac.nz/~marslast/Code/Ch6/lle.py
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_swiss_roll
+    >>> X, t = make_swiss_roll(noise=0.05, random_state=0)
+    >>> X.shape
+    (100, 3)
+    >>> t.shape
+    (100,)
     """
     generator = check_random_state(random_state)
 
     if not hole:
         t = 1.5 * np.pi * (1 + 2 * generator.uniform(size=n_samples))
         y = 21 * generator.uniform(size=n_samples)
     else:
@@ -1897,16 +1935,25 @@
 
     Returns
     -------
     X : ndarray of shape (n_samples, 3)
         The points.
 
     t : ndarray of shape (n_samples,)
-        The univariate position of the sample according to the main dimension
-        of the points in the manifold.
+        The univariate position of the sample according
+        to the main dimension of the points in the manifold.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_s_curve
+    >>> X, t = make_s_curve(noise=0.05, random_state=0)
+    >>> X.shape
+    (100, 3)
+    >>> t.shape
+    (100,)
     """
     generator = check_random_state(random_state)
 
     t = 3 * np.pi * (generator.uniform(size=(1, n_samples)) - 0.5)
     X = np.empty(shape=(n_samples, 3), dtype=np.float64)
     X[:, 0] = np.sin(t)
     X[:, 1] = 2.0 * generator.uniform(size=n_samples)
@@ -2117,14 +2164,27 @@
     References
     ----------
 
     .. [1] Dhillon, I. S. (2001, August). Co-clustering documents and
         words using bipartite spectral graph partitioning. In Proceedings
         of the seventh ACM SIGKDD international conference on Knowledge
         discovery and data mining (pp. 269-274). ACM.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_biclusters
+    >>> data, rows, cols = make_biclusters(
+    ...     shape=(10, 20), n_clusters=2, random_state=42
+    ... )
+    >>> data.shape
+    (10, 20)
+    >>> rows.shape
+    (2, 10)
+    >>> cols.shape
+    (2, 20)
     """
     generator = check_random_state(random_state)
     n_rows, n_cols = shape
     consts = generator.uniform(minval, maxval, n_clusters)
 
     # row and column clusters of approximately equal sizes
     row_sizes = generator.multinomial(n_rows, np.repeat(1.0 / n_clusters, n_clusters))
@@ -2224,14 +2284,28 @@
         for biclustering.
 
     References
     ----------
     .. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).
         Spectral biclustering of microarray data: coclustering genes
         and conditions. Genome research, 13(4), 703-716.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_checkerboard
+    >>> data, rows, columns = make_checkerboard(shape=(300, 300), n_clusters=10,
+    ...                                         random_state=42)
+    >>> data.shape
+    (300, 300)
+    >>> rows.shape
+    (100, 300)
+    >>> columns.shape
+    (100, 300)
+    >>> print(rows[0][:5], columns[0][:5])
+    [False False False  True False] [False False False False False]
     """
     generator = check_random_state(random_state)
 
     if hasattr(n_clusters, "__len__"):
         n_row_clusters, n_col_clusters = n_clusters
     else:
         n_row_clusters = n_col_clusters = n_clusters
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_species_distributions.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_species_distributions.py`

 * *Files 6% similar despite different names*

```diff
@@ -35,22 +35,23 @@
 # Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #          Jake Vanderplas <vanderplas@astro.washington.edu>
 #
 # License: BSD 3 clause
 
 import logging
 from io import BytesIO
+from numbers import Integral, Real
 from os import PathLike, makedirs, remove
 from os.path import exists
 
 import joblib
 import numpy as np
 
 from ..utils import Bunch
-from ..utils._param_validation import validate_params
+from ..utils._param_validation import Interval, validate_params
 from . import get_data_home
 from ._base import RemoteFileMetadata, _fetch_remote, _pkl_filepath
 
 # The original data can be found at:
 # https://biodiversityinformatics.amnh.org/open_source/maxent/samples.zip
 SAMPLES = RemoteFileMetadata(
     filename="samples.zip",
@@ -132,32 +133,53 @@
     # y coordinates of the grid cells
     ygrid = np.arange(ymin, ymax, batch.grid_size)
 
     return (xgrid, ygrid)
 
 
 @validate_params(
-    {"data_home": [str, PathLike, None], "download_if_missing": ["boolean"]},
+    {
+        "data_home": [str, PathLike, None],
+        "download_if_missing": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
+    },
     prefer_skip_nested_validation=True,
 )
-def fetch_species_distributions(*, data_home=None, download_if_missing=True):
+def fetch_species_distributions(
+    *,
+    data_home=None,
+    download_if_missing=True,
+    n_retries=3,
+    delay=1.0,
+):
     """Loader for species distribution dataset from Phillips et. al. (2006).
 
     Read more in the :ref:`User Guide <species_distribution_dataset>`.
 
     Parameters
     ----------
     data_home : str or path-like, default=None
         Specify another download and cache folder for the datasets. By default
         all scikit-learn data is stored in '~/scikit_learn_data' subfolders.
 
     download_if_missing : bool, default=True
         If False, raise an OSError if the data is not locally available
         instead of trying to download the data from the source site.
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     data : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         coverages : array, shape = [14, 1592, 1212]
             These represent the 14 features measured
@@ -238,28 +260,32 @@
 
     archive_path = _pkl_filepath(data_home, DATA_ARCHIVE_NAME)
 
     if not exists(archive_path):
         if not download_if_missing:
             raise OSError("Data not found and `download_if_missing` is False")
         logger.info("Downloading species data from %s to %s" % (SAMPLES.url, data_home))
-        samples_path = _fetch_remote(SAMPLES, dirname=data_home)
+        samples_path = _fetch_remote(
+            SAMPLES, dirname=data_home, n_retries=n_retries, delay=delay
+        )
         with np.load(samples_path) as X:  # samples.zip is a valid npz
             for f in X.files:
                 fhandle = BytesIO(X[f])
                 if "train" in f:
                     train = _load_csv(fhandle)
                 if "test" in f:
                     test = _load_csv(fhandle)
         remove(samples_path)
 
         logger.info(
             "Downloading coverage data from %s to %s" % (COVERAGES.url, data_home)
         )
-        coverages_path = _fetch_remote(COVERAGES, dirname=data_home)
+        coverages_path = _fetch_remote(
+            COVERAGES, dirname=data_home, n_retries=n_retries, delay=delay
+        )
         with np.load(coverages_path) as X:  # coverages.zip is a valid npz
             coverages = []
             for f in X.files:
                 fhandle = BytesIO(X[f])
                 logger.debug(" - converting {}".format(f))
                 coverages.append(_load_coverage(fhandle))
             coverages = np.asarray(coverages, dtype=dtype)
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_svmlight_format_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/datasets/_svmlight_format_fast.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_svmlight_format_io.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_svmlight_format_io.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,18 +19,19 @@
 from contextlib import closing
 from numbers import Integral
 
 import numpy as np
 import scipy.sparse as sp
 
 from .. import __version__
-from ..utils import IS_PYPY, check_array
+from ..utils import check_array
 from ..utils._param_validation import HasMethods, Interval, StrOptions, validate_params
+from ..utils.fixes import _IS_PYPY
 
-if not IS_PYPY:
+if not _IS_PYPY:
     from ._svmlight_format_fast import (
         _dump_svmlight_file,
         _load_svmlight_file,
     )
 else:
 
     def _load_svmlight_file(*args, **kwargs):
@@ -172,15 +173,15 @@
         format, enforcing the same number of features/columns on all of them.
 
     Examples
     --------
     To use joblib.Memory to cache the svmlight file::
 
         from joblib import Memory
-        from .datasets import load_svmlight_file
+        from sklearn.datasets import load_svmlight_file
         mem = Memory("./mycache")
 
         @mem.cache
         def get_data():
             data = load_svmlight_file("mysvmlightfile")
             return data[0], data[1]
 
@@ -355,14 +356,31 @@
 
     Notes
     -----
     When fitting a model to a matrix X_train and evaluating it against a
     matrix X_test, it is essential that X_train and X_test have the same
     number of features (X_train.shape[1] == X_test.shape[1]). This may not
     be the case if you load the files individually with load_svmlight_file.
+
+    Examples
+    --------
+    To use joblib.Memory to cache the svmlight file::
+
+        from joblib import Memory
+        from sklearn.datasets import load_svmlight_file
+        mem = Memory("./mycache")
+
+        @mem.cache
+        def get_data():
+            data_train, target_train, data_test, target_test = load_svmlight_files(
+                ["svmlight_file_train", "svmlight_file_test"]
+            )
+            return data_train, target_train, data_test, target_test
+
+        X_train, y_train, X_test, y_test = get_data()
     """
     if (offset != 0 or length > 0) and zero_based == "auto":
         # disable heuristic search to avoid getting inconsistent results on
         # different segments of the file
         zero_based = True
 
     if (offset != 0 or length > 0) and n_features is None:
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/_twenty_newsgroups.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/_twenty_newsgroups.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,34 +17,37 @@
     classification and text clustering.
 
 This dataset loader will download the recommended "by date" variant of the
 dataset and which features a point in time split between the train and
 test sets. The compressed dataset size is around 14 Mb compressed. Once
 uncompressed the train set is 52 MB and the test set is 34 MB.
 """
+
 # Copyright (c) 2011 Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
 
 import codecs
 import logging
 import os
 import pickle
 import re
 import shutil
 import tarfile
 from contextlib import suppress
+from numbers import Integral, Real
 
 import joblib
 import numpy as np
 import scipy.sparse as sp
 
 from .. import preprocessing
 from ..feature_extraction.text import CountVectorizer
 from ..utils import Bunch, check_random_state
-from ..utils._param_validation import StrOptions, validate_params
+from ..utils._param_validation import Interval, StrOptions, validate_params
+from ..utils.fixes import tarfile_extractall
 from . import get_data_home, load_files
 from ._base import (
     RemoteFileMetadata,
     _convert_data_dataframe,
     _fetch_remote,
     _pkl_filepath,
     load_descr,
@@ -61,26 +64,29 @@
 )
 
 CACHE_NAME = "20news-bydate.pkz"
 TRAIN_FOLDER = "20news-bydate-train"
 TEST_FOLDER = "20news-bydate-test"
 
 
-def _download_20newsgroups(target_dir, cache_path):
+def _download_20newsgroups(target_dir, cache_path, n_retries, delay):
     """Download the 20 newsgroups data and stored it as a zipped pickle."""
     train_path = os.path.join(target_dir, TRAIN_FOLDER)
     test_path = os.path.join(target_dir, TEST_FOLDER)
 
     os.makedirs(target_dir, exist_ok=True)
 
     logger.info("Downloading dataset from %s (14 MB)", ARCHIVE.url)
-    archive_path = _fetch_remote(ARCHIVE, dirname=target_dir)
+    archive_path = _fetch_remote(
+        ARCHIVE, dirname=target_dir, n_retries=n_retries, delay=delay
+    )
 
     logger.debug("Decompressing %s", archive_path)
-    tarfile.open(archive_path, "r:gz").extractall(path=target_dir)
+    with tarfile.open(archive_path, "r:gz") as fp:
+        tarfile_extractall(fp, path=target_dir)
 
     with suppress(FileNotFoundError):
         os.remove(archive_path)
 
     # Store a zipped pickle
     cache = dict(
         train=load_files(train_path, encoding="latin1"),
@@ -159,27 +165,31 @@
         "subset": [StrOptions({"train", "test", "all"})],
         "categories": ["array-like", None],
         "shuffle": ["boolean"],
         "random_state": ["random_state"],
         "remove": [tuple],
         "download_if_missing": ["boolean"],
         "return_X_y": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_20newsgroups(
     *,
     data_home=None,
     subset="train",
     categories=None,
     shuffle=True,
     random_state=42,
     remove=(),
     download_if_missing=True,
     return_X_y=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load the filenames and data from the 20 newsgroups dataset \
 (classification).
 
     Download it if necessary.
 
     =================   ==========
@@ -235,14 +245,24 @@
 
     return_X_y : bool, default=False
         If True, returns `(data.data, data.target)` instead of a Bunch
         object.
 
         .. versionadded:: 0.22
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     bunch : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data : list of shape (n_samples,)
             The data list to learn.
@@ -258,14 +278,28 @@
     (data, target) : tuple if `return_X_y=True`
         A tuple of two ndarrays. The first contains a 2D array of shape
         (n_samples, n_classes) with each row representing one sample and each
         column representing the features. The second array of shape
         (n_samples,) contains the target samples.
 
         .. versionadded:: 0.22
+
+    Examples
+    --------
+    >>> from sklearn.datasets import fetch_20newsgroups
+    >>> cats = ['alt.atheism', 'sci.space']
+    >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)
+    >>> list(newsgroups_train.target_names)
+    ['alt.atheism', 'sci.space']
+    >>> newsgroups_train.filenames.shape
+    (1073,)
+    >>> newsgroups_train.target.shape
+    (1073,)
+    >>> newsgroups_train.target[:10]
+    array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])
     """
 
     data_home = get_data_home(data_home=data_home)
     cache_path = _pkl_filepath(data_home, CACHE_NAME)
     twenty_home = os.path.join(data_home, "20news_home")
     cache = None
     if os.path.exists(cache_path):
@@ -280,15 +314,18 @@
             print(80 * "_")
             print(e)
 
     if cache is None:
         if download_if_missing:
             logger.info("Downloading 20news dataset. This may take a few minutes.")
             cache = _download_20newsgroups(
-                target_dir=twenty_home, cache_path=cache_path
+                target_dir=twenty_home,
+                cache_path=cache_path,
+                n_retries=n_retries,
+                delay=delay,
             )
         else:
             raise OSError("20Newsgroups dataset not found")
 
     if subset in ("train", "test"):
         data = cache[subset]
     elif subset == "all":
@@ -354,26 +391,30 @@
         "subset": [StrOptions({"train", "test", "all"})],
         "remove": [tuple],
         "data_home": [str, os.PathLike, None],
         "download_if_missing": ["boolean"],
         "return_X_y": ["boolean"],
         "normalize": ["boolean"],
         "as_frame": ["boolean"],
+        "n_retries": [Interval(Integral, 1, None, closed="left")],
+        "delay": [Interval(Real, 0.0, None, closed="neither")],
     },
     prefer_skip_nested_validation=True,
 )
 def fetch_20newsgroups_vectorized(
     *,
     subset="train",
     remove=(),
     data_home=None,
     download_if_missing=True,
     return_X_y=False,
     normalize=True,
     as_frame=False,
+    n_retries=3,
+    delay=1.0,
 ):
     """Load and vectorize the 20 newsgroups dataset (classification).
 
     Download it if necessary.
 
     This is a convenience function; the transformation is done using the
     default settings for
@@ -437,14 +478,24 @@
         If True, the data is a pandas DataFrame including columns with
         appropriate dtypes (numeric, string, or categorical). The target is
         a pandas DataFrame or Series depending on the number of
         `target_columns`.
 
         .. versionadded:: 0.24
 
+    n_retries : int, default=3
+        Number of retries when HTTP errors are encountered.
+
+        .. versionadded:: 1.5
+
+    delay : float, default=1.0
+        Number of seconds between retries.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     bunch : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
 
         data: {sparse matrix, dataframe} of shape (n_samples, n_features)
             The input data matrix. If ``as_frame`` is `True`, ``data`` is
@@ -463,14 +514,23 @@
             .. versionadded:: 0.24
 
     (data, target) : tuple if ``return_X_y`` is True
         `data` and `target` would be of the format defined in the `Bunch`
         description above.
 
         .. versionadded:: 0.20
+
+    Examples
+    --------
+    >>> from sklearn.datasets import fetch_20newsgroups_vectorized
+    >>> newsgroups_vectorized = fetch_20newsgroups_vectorized(subset='test')
+    >>> newsgroups_vectorized.data.shape
+    (7532, 130107)
+    >>> newsgroups_vectorized.target.shape
+    (7532,)
     """
     data_home = get_data_home(data_home=data_home)
     filebase = "20newsgroup_vectorized"
     if remove:
         filebase += "remove-" + "-".join(remove)
     target_file = _pkl_filepath(data_home, filebase + ".pkl")
 
@@ -479,24 +539,28 @@
         data_home=data_home,
         subset="train",
         categories=None,
         shuffle=True,
         random_state=12,
         remove=remove,
         download_if_missing=download_if_missing,
+        n_retries=n_retries,
+        delay=delay,
     )
 
     data_test = fetch_20newsgroups(
         data_home=data_home,
         subset="test",
         categories=None,
         shuffle=True,
         random_state=12,
         remove=remove,
         download_if_missing=download_if_missing,
+        n_retries=n_retries,
+        delay=delay,
     )
 
     if os.path.exists(target_file):
         try:
             X_train, X_test, feature_names = joblib.load(target_file)
         except ValueError as e:
             raise ValueError(
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/boston_house_prices.csv` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/boston_house_prices.csv`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/breast_cancer.csv` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/breast_cancer.csv`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/diabetes_data_raw.csv.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/diabetes_data_raw.csv.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/diabetes_target.csv.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/diabetes_target.csv.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/digits.csv.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/digits.csv.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/iris.csv` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/iris.csv`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/data/wine_data.csv` & `scikit_learn-1.5.0rc1/sklearn/datasets/data/wine_data.csv`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/breast_cancer.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/breast_cancer.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/california_housing.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/california_housing.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/covtype.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/covtype.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/diabetes.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/diabetes.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/digits.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/digits.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/iris.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/iris.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/kddcup99.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/kddcup99.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/lfw.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/lfw.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/linnerud.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/linnerud.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/olivetti_faces.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/olivetti_faces.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/rcv1.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/rcv1.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/species_distributions.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/species_distributions.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/twenty_newsgroups.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/twenty_newsgroups.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/descr/wine_data.rst` & `scikit_learn-1.5.0rc1/sklearn/datasets/descr/wine_data.rst`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/images/README.txt` & `scikit_learn-1.5.0rc1/sklearn/datasets/images/README.txt`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/images/china.jpg` & `scikit_learn-1.5.0rc1/sklearn/datasets/images/china.jpg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/images/flower.jpg` & `scikit_learn-1.5.0rc1/sklearn/datasets/images/flower.jpg`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/api-v1-jd-1.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/api-v1-jd-1.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/api-v1-jdf-1.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/api-v1-jdf-1.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1/data-v1-dl-1.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1/data-v1-dl-1.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jd-1119.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jd-1119.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdf-1119.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdf-1119.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdq-1119.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/api-v1-jdq-1119.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1119/data-v1-dl-54002.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1119/data-v1-dl-54002.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/api-v1-jd-1590.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/api-v1-jd-1590.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdf-1590.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdf-1590.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdq-1590.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/api-v1-jdq-1590.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_1590/data-v1-dl-1595261.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_1590/data-v1-dl-1595261.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jd-2.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jd-2.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jdf-2.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jdf-2.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/api-v1-jdq-2.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/api-v1-jdq-2.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_2/data-v1-dl-1666876.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_2/data-v1-dl-1666876.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-292.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-292.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-40981.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/api-v1-jd-40981.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_292/data-v1-dl-49822.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_292/data-v1-dl-49822.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/api-v1-jd-3.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/api-v1-jd-3.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/api-v1-jdf-3.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/api-v1-jdf-3.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/api-v1-jdq-3.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/api-v1-jdq-3.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_3/data-v1-dl-3.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_3/data-v1-dl-3.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jd-40589.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jd-40589.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdf-40589.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdf-40589.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdq-40589.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/api-v1-jdq-40589.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40589/data-v1-dl-4644182.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40589/data-v1-dl-4644182.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdq-40675.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/api-v1-jdq-40675.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40675/data-v1-dl-4965250.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40675/data-v1-dl-4965250.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/api-v1-jdq-40945.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/api-v1-jdq-40945.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40945/data-v1-dl-16826755.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40945/data-v1-dl-16826755.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jd-40966.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jd-40966.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdf-40966.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdf-40966.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdq-40966.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/api-v1-jdq-40966.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_40966/data-v1-dl-17928620.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_40966/data-v1-dl-17928620.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/api-v1-jd-42074.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/api-v1-jd-42074.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/api-v1-jdq-42074.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/api-v1-jdq-42074.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42074/data-v1-dl-21552912.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42074/data-v1-dl-21552912.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/api-v1-jd-42585.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/api-v1-jd-42585.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_42585/data-v1-dl-21854866.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_42585/data-v1-dl-21854866.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jd-561.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jd-561.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/api-v1-jdq-561.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/api-v1-jdq-561.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_561/data-v1-dl-52739.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_561/data-v1-dl-52739.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jd-61.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jd-61.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/api-v1-jdq-61.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/api-v1-jdq-61.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_61/data-v1-dl-61.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_61/data-v1-dl-61.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/api-v1-jd-62.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/api-v1-jd-62.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/api-v1-jdf-62.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/api-v1-jdf-62.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/api-v1-jdq-62.json.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/api-v1-jdq-62.json.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/data/openml/id_62/data-v1-dl-52352.arff.gz` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/data/openml/id_62/data-v1-dl-52352.arff.gz`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_20news.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_20news.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """Test the 20news downloader, if the data is available,
 or if specifically requested via environment variable
 (e.g. for CI jobs)."""
+
 from functools import partial
 from unittest.mock import patch
 
 import numpy as np
 import pytest
 import scipy.sparse as sp
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_arff_parser.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_arff_parser.py`

 * *Files 5% similar despite different names*

```diff
@@ -79,23 +79,27 @@
 
 
 @pytest.mark.parametrize("parser_func", [_liac_arff_parser, _pandas_arff_parser])
 def test_pandas_arff_parser_strip_single_quotes(parser_func):
     """Check that we properly strip single quotes from the data."""
     pd = pytest.importorskip("pandas")
 
-    arff_file = BytesIO(textwrap.dedent("""
+    arff_file = BytesIO(
+        textwrap.dedent(
+            """
             @relation 'toy'
             @attribute 'cat_single_quote' {'A', 'B', 'C'}
             @attribute 'str_single_quote' string
             @attribute 'str_nested_quote' string
             @attribute 'class' numeric
             @data
             'A','some text','\"expect double quotes\"',0
-            """).encode("utf-8"))
+            """
+        ).encode("utf-8")
+    )
 
     columns_info = {
         "cat_single_quote": {
             "data_type": "nominal",
             "name": "cat_single_quote",
         },
         "str_single_quote": {
@@ -146,23 +150,27 @@
 
 
 @pytest.mark.parametrize("parser_func", [_liac_arff_parser, _pandas_arff_parser])
 def test_pandas_arff_parser_strip_double_quotes(parser_func):
     """Check that we properly strip double quotes from the data."""
     pd = pytest.importorskip("pandas")
 
-    arff_file = BytesIO(textwrap.dedent("""
+    arff_file = BytesIO(
+        textwrap.dedent(
+            """
             @relation 'toy'
             @attribute 'cat_double_quote' {"A", "B", "C"}
             @attribute 'str_double_quote' string
             @attribute 'str_nested_quote' string
             @attribute 'class' numeric
             @data
             "A","some text","\'expect double quotes\'",0
-            """).encode("utf-8"))
+            """
+        ).encode("utf-8")
+    )
 
     columns_info = {
         "cat_double_quote": {
             "data_type": "nominal",
             "name": "cat_double_quote",
         },
         "str_double_quote": {
@@ -213,23 +221,27 @@
         _pandas_arff_parser,
     ],
 )
 def test_pandas_arff_parser_strip_no_quotes(parser_func):
     """Check that we properly parse with no quotes characters."""
     pd = pytest.importorskip("pandas")
 
-    arff_file = BytesIO(textwrap.dedent("""
+    arff_file = BytesIO(
+        textwrap.dedent(
+            """
             @relation 'toy'
             @attribute 'cat_without_quote' {A, B, C}
             @attribute 'str_without_quote' string
             @attribute 'str_internal_quote' string
             @attribute 'class' numeric
             @data
             A,some text,'internal' quote,0
-            """).encode("utf-8"))
+            """
+        ).encode("utf-8")
+    )
 
     columns_info = {
         "cat_without_quote": {
             "data_type": "nominal",
             "name": "cat_without_quote",
         },
         "str_without_quote": {
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,18 @@
+import io
 import os
 import shutil
 import tempfile
 import warnings
 from functools import partial
 from importlib import resources
 from pathlib import Path
 from pickle import dumps, loads
+from unittest.mock import Mock
+from urllib.error import HTTPError
 
 import numpy as np
 import pytest
 
 from sklearn.datasets import (
     clear_data_home,
     get_data_home,
@@ -20,14 +23,16 @@
     load_iris,
     load_linnerud,
     load_sample_image,
     load_sample_images,
     load_wine,
 )
 from sklearn.datasets._base import (
+    RemoteFileMetadata,
+    _fetch_remote,
     load_csv_data,
     load_gzip_compressed_csv_data,
 )
 from sklearn.datasets.tests.test_common import check_as_frame
 from sklearn.preprocessing import scale
 from sklearn.utils import Bunch
 
@@ -359,7 +364,30 @@
     with pytest.raises(ImportError, match=msg):
         from sklearn.datasets import load_boston  # noqa
 
     # other non-existing function should raise the usual import error
     msg = "cannot import name 'non_existing_function' from 'sklearn.datasets'"
     with pytest.raises(ImportError, match=msg):
         from sklearn.datasets import non_existing_function  # noqa
+
+
+def test_fetch_remote_raise_warnings_with_invalid_url(monkeypatch):
+    """Check retry mechanism in _fetch_remote."""
+
+    url = "https://scikit-learn.org/this_file_does_not_exist.tar.gz"
+    invalid_remote_file = RemoteFileMetadata("invalid_file", url, None)
+    urlretrieve_mock = Mock(
+        side_effect=HTTPError(
+            url=url, code=404, msg="Not Found", hdrs=None, fp=io.BytesIO()
+        )
+    )
+    monkeypatch.setattr("sklearn.datasets._base.urlretrieve", urlretrieve_mock)
+
+    with pytest.warns(UserWarning, match="Retry downloading") as record:
+        with pytest.raises(HTTPError, match="HTTP Error 404"):
+            _fetch_remote(invalid_remote_file, n_retries=3, delay=0)
+
+        assert urlretrieve_mock.call_count == 4
+
+        for r in record:
+            assert str(r.message) == f"Retry downloading from url: {url}"
+        assert len(record) == 3
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_california_housing.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_california_housing.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """Test the california_housing loader, if the data is available,
 or if specifically requested via environment variable
 (e.g. for CI jobs)."""
+
 from functools import partial
 
 import pytest
 
 from sklearn.datasets.tests.test_common import check_return_X_y
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_common.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Test loaders for common functionality."""
+
 import inspect
 import os
 
 import numpy as np
 import pytest
 
 import sklearn.datasets
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_covtype.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_covtype.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """Test the covtype loader, if the data is available,
 or if specifically requested via environment variable
 (e.g. for CI jobs)."""
+
 from functools import partial
 
 import pytest
 
 from sklearn.datasets.tests.test_common import check_return_X_y
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_kddcup99.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_kddcup99.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_lfw.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_lfw.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,80 +4,75 @@
 the tests won't run (skipped).
 
 If the test are run, the first execution will be long (typically a bit
 more than a couple of minutes) but as the dataset loader is leveraging
 joblib, successive runs will be fast (less than 200ms).
 """
 
-import os
 import random
-import shutil
-import tempfile
 from functools import partial
 
 import numpy as np
 import pytest
 
 from sklearn.datasets import fetch_lfw_pairs, fetch_lfw_people
 from sklearn.datasets.tests.test_common import check_return_X_y
 from sklearn.utils._testing import assert_array_equal
 
-SCIKIT_LEARN_DATA = None
-SCIKIT_LEARN_EMPTY_DATA = None
-LFW_HOME = None
-
 FAKE_NAMES = [
     "Abdelatif_Smith",
     "Abhati_Kepler",
     "Camara_Alvaro",
     "Chen_Dupont",
     "John_Lee",
     "Lin_Bauman",
     "Onur_Lopez",
 ]
 
 
-def setup_module():
-    """Test fixture run once and common to all tests of this module"""
-    Image = pytest.importorskip("PIL.Image")
+@pytest.fixture(scope="module")
+def mock_empty_data_home(tmp_path_factory):
+    data_dir = tmp_path_factory.mktemp("scikit_learn_empty_test")
 
-    global SCIKIT_LEARN_DATA, SCIKIT_LEARN_EMPTY_DATA, LFW_HOME
+    yield data_dir
 
-    SCIKIT_LEARN_DATA = tempfile.mkdtemp(prefix="scikit_learn_lfw_test_")
-    LFW_HOME = os.path.join(SCIKIT_LEARN_DATA, "lfw_home")
 
-    SCIKIT_LEARN_EMPTY_DATA = tempfile.mkdtemp(prefix="scikit_learn_empty_test_")
+@pytest.fixture(scope="module")
+def mock_data_home(tmp_path_factory):
+    """Test fixture run once and common to all tests of this module"""
+    Image = pytest.importorskip("PIL.Image")
 
-    if not os.path.exists(LFW_HOME):
-        os.makedirs(LFW_HOME)
+    data_dir = tmp_path_factory.mktemp("scikit_learn_lfw_test")
+    lfw_home = data_dir / "lfw_home"
+    lfw_home.mkdir(parents=True, exist_ok=True)
 
     random_state = random.Random(42)
     np_rng = np.random.RandomState(42)
 
     # generate some random jpeg files for each person
     counts = {}
     for name in FAKE_NAMES:
-        folder_name = os.path.join(LFW_HOME, "lfw_funneled", name)
-        if not os.path.exists(folder_name):
-            os.makedirs(folder_name)
+        folder_name = lfw_home / "lfw_funneled" / name
+        folder_name.mkdir(parents=True, exist_ok=True)
 
         n_faces = np_rng.randint(1, 5)
         counts[name] = n_faces
         for i in range(n_faces):
-            file_path = os.path.join(folder_name, name + "_%04d.jpg" % i)
+            file_path = folder_name / (name + "_%04d.jpg" % i)
             uniface = np_rng.randint(0, 255, size=(250, 250, 3))
             img = Image.fromarray(uniface.astype(np.uint8))
             img.save(file_path)
 
     # add some random file pollution to test robustness
-    with open(os.path.join(LFW_HOME, "lfw_funneled", ".test.swp"), "wb") as f:
-        f.write(b"Text file to be ignored by the dataset loader.")
+    (lfw_home / "lfw_funneled" / ".test.swp").write_bytes(
+        b"Text file to be ignored by the dataset loader."
+    )
 
     # generate some pairing metadata files using the same format as LFW
-    with open(os.path.join(LFW_HOME, "pairsDevTrain.txt"), "wb") as f:
+    with open(lfw_home / "pairsDevTrain.txt", "wb") as f:
         f.write(b"10\n")
         more_than_two = [name for name, count in counts.items() if count >= 2]
         for i in range(5):
             name = random_state.choice(more_than_two)
             first, second = random_state.sample(range(counts[name]), 2)
             f.write(("%s\t%d\t%d\n" % (name, first, second)).encode())
 
@@ -88,37 +83,30 @@
             f.write(
                 (
                     "%s\t%d\t%s\t%d\n"
                     % (first_name, first_index, second_name, second_index)
                 ).encode()
             )
 
-    with open(os.path.join(LFW_HOME, "pairsDevTest.txt"), "wb") as f:
-        f.write(b"Fake place holder that won't be tested")
-
-    with open(os.path.join(LFW_HOME, "pairs.txt"), "wb") as f:
-        f.write(b"Fake place holder that won't be tested")
-
+    (lfw_home / "pairsDevTest.txt").write_bytes(
+        b"Fake place holder that won't be tested"
+    )
+    (lfw_home / "pairs.txt").write_bytes(b"Fake place holder that won't be tested")
 
-def teardown_module():
-    """Test fixture (clean up) run once after all tests of this module"""
-    if os.path.isdir(SCIKIT_LEARN_DATA):
-        shutil.rmtree(SCIKIT_LEARN_DATA)
-    if os.path.isdir(SCIKIT_LEARN_EMPTY_DATA):
-        shutil.rmtree(SCIKIT_LEARN_EMPTY_DATA)
+    yield data_dir
 
 
-def test_load_empty_lfw_people():
+def test_load_empty_lfw_people(mock_empty_data_home):
     with pytest.raises(OSError):
-        fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)
+        fetch_lfw_people(data_home=mock_empty_data_home, download_if_missing=False)
 
 
-def test_load_fake_lfw_people():
+def test_load_fake_lfw_people(mock_data_home):
     lfw_people = fetch_lfw_people(
-        data_home=SCIKIT_LEARN_DATA, min_faces_per_person=3, download_if_missing=False
+        data_home=mock_data_home, min_faces_per_person=3, download_if_missing=False
     )
 
     # The data is croped around the center as a rectangular bounding box
     # around the face. Colors are converted to gray levels:
     assert lfw_people.images.shape == (10, 62, 47)
     assert lfw_people.data.shape == (10, 2914)
 
@@ -128,15 +116,15 @@
     # names of the persons can be found using the target_names array
     expected_classes = ["Abdelatif Smith", "Abhati Kepler", "Onur Lopez"]
     assert_array_equal(lfw_people.target_names, expected_classes)
 
     # It is possible to ask for the original data without any croping or color
     # conversion and not limit on the number of picture per person
     lfw_people = fetch_lfw_people(
-        data_home=SCIKIT_LEARN_DATA,
+        data_home=mock_data_home,
         resize=None,
         slice_=None,
         color=True,
         download_if_missing=False,
     )
     assert lfw_people.images.shape == (17, 250, 250, 3)
     assert lfw_people.DESCR.startswith(".. _labeled_faces_in_the_wild_dataset:")
@@ -157,40 +145,40 @@
             "Onur Lopez",
         ],
     )
 
     # test return_X_y option
     fetch_func = partial(
         fetch_lfw_people,
-        data_home=SCIKIT_LEARN_DATA,
+        data_home=mock_data_home,
         resize=None,
         slice_=None,
         color=True,
         download_if_missing=False,
     )
     check_return_X_y(lfw_people, fetch_func)
 
 
-def test_load_fake_lfw_people_too_restrictive():
+def test_load_fake_lfw_people_too_restrictive(mock_data_home):
     with pytest.raises(ValueError):
         fetch_lfw_people(
-            data_home=SCIKIT_LEARN_DATA,
+            data_home=mock_data_home,
             min_faces_per_person=100,
             download_if_missing=False,
         )
 
 
-def test_load_empty_lfw_pairs():
+def test_load_empty_lfw_pairs(mock_empty_data_home):
     with pytest.raises(OSError):
-        fetch_lfw_pairs(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)
+        fetch_lfw_pairs(data_home=mock_empty_data_home, download_if_missing=False)
 
 
-def test_load_fake_lfw_pairs():
+def test_load_fake_lfw_pairs(mock_data_home):
     lfw_pairs_train = fetch_lfw_pairs(
-        data_home=SCIKIT_LEARN_DATA, download_if_missing=False
+        data_home=mock_data_home, download_if_missing=False
     )
 
     # The data is croped around the center as a rectangular bounding box
     # around the face. Colors are converted to gray levels:
     assert lfw_pairs_train.pairs.shape == (10, 2, 62, 47)
 
     # the target is whether the person is the same or not
@@ -199,41 +187,41 @@
     # names of the persons can be found using the target_names array
     expected_classes = ["Different persons", "Same person"]
     assert_array_equal(lfw_pairs_train.target_names, expected_classes)
 
     # It is possible to ask for the original data without any croping or color
     # conversion
     lfw_pairs_train = fetch_lfw_pairs(
-        data_home=SCIKIT_LEARN_DATA,
+        data_home=mock_data_home,
         resize=None,
         slice_=None,
         color=True,
         download_if_missing=False,
     )
     assert lfw_pairs_train.pairs.shape == (10, 2, 250, 250, 3)
 
     # the ids and class names are the same as previously
     assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])
     assert_array_equal(lfw_pairs_train.target_names, expected_classes)
 
     assert lfw_pairs_train.DESCR.startswith(".. _labeled_faces_in_the_wild_dataset:")
 
 
-def test_fetch_lfw_people_internal_cropping():
+def test_fetch_lfw_people_internal_cropping(mock_data_home):
     """Check that we properly crop the images.
 
     Non-regression test for:
     https://github.com/scikit-learn/scikit-learn/issues/24942
     """
     # If cropping was not done properly and we don't resize the images, the images would
     # have their original size (250x250) and the image would not fit in the NumPy array
     # pre-allocated based on `slice_` parameter.
     slice_ = (slice(70, 195), slice(78, 172))
     lfw = fetch_lfw_people(
-        data_home=SCIKIT_LEARN_DATA,
+        data_home=mock_data_home,
         min_faces_per_person=3,
         download_if_missing=False,
         resize=None,
         slice_=slice_,
     )
     assert lfw.images[0].shape == (
         slice_[0].stop - slice_[0].start,
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_olivetti_faces.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_olivetti_faces.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_openml.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_openml.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Test the openml loader."""
+
 import gzip
 import json
 import os
 import re
 from functools import partial
 from importlib import resources
 from io import BytesIO
@@ -17,15 +18,16 @@
 from sklearn.datasets import fetch_openml as fetch_openml_orig
 from sklearn.datasets._openml import (
     _OPENML_PREFIX,
     _get_local_path,
     _open_openml_url,
     _retry_with_clean_cache,
 )
-from sklearn.utils import Bunch, check_pandas_support
+from sklearn.utils import Bunch
+from sklearn.utils._optional_dependencies import check_pandas_support
 from sklearn.utils._testing import (
     SkipTest,
     assert_allclose,
     assert_array_equal,
     fails_if_pypy,
 )
 
@@ -1452,16 +1454,15 @@
 
 @pytest.mark.parametrize("gzip_response", [True, False])
 def test_fetch_openml_cache(monkeypatch, gzip_response, tmpdir):
     def _mock_urlopen_raise(request, *args, **kwargs):
         raise ValueError(
             "This mechanism intends to test correct cache"
             "handling. As such, urlopen should never be "
-            "accessed. URL: %s"
-            % request.get_full_url()
+            "accessed. URL: %s" % request.get_full_url()
         )
 
     data_id = 61
     cache_directory = str(tmpdir.mkdir("scikit_learn_data"))
     _monkey_patch_webbased_functions(monkeypatch, data_id, gzip_response)
     X_fetched, y_fetched = fetch_openml(
         data_id=data_id,
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_rcv1.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_rcv1.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_samples_generator.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_samples_generator.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,15 +29,14 @@
 )
 from sklearn.utils._testing import (
     assert_allclose,
     assert_allclose_dense_sparse,
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
-    ignore_warnings,
 )
 from sklearn.utils.validation import assert_all_finite
 
 
 def test_make_classification():
     weights = [0.1, 0.25]
     X, y = make_classification(
@@ -496,49 +495,14 @@
     assert X.shape == (5, 8), "X shape mismatch"
     for row in X:
         assert len(np.flatnonzero(row)) == 3, "Non-zero coefs mismatch"
     assert_allclose(Y, X @ D)
     assert_allclose(np.sqrt((D**2).sum(axis=1)), np.ones(D.shape[0]))
 
 
-# TODO(1.5): remove
-@ignore_warnings(category=FutureWarning)
-def test_make_sparse_coded_signal_transposed():
-    Y, D, X = make_sparse_coded_signal(
-        n_samples=5,
-        n_components=8,
-        n_features=10,
-        n_nonzero_coefs=3,
-        random_state=0,
-        data_transposed=True,
-    )
-    assert Y.shape == (10, 5), "Y shape mismatch"
-    assert D.shape == (10, 8), "D shape mismatch"
-    assert X.shape == (8, 5), "X shape mismatch"
-    for col in X.T:
-        assert len(np.flatnonzero(col)) == 3, "Non-zero coefs mismatch"
-    assert_allclose(Y, D @ X)
-    assert_allclose(np.sqrt((D**2).sum(axis=0)), np.ones(D.shape[1]))
-
-
-# TODO(1.5): remove
-def test_make_sparse_code_signal_deprecation_warning():
-    """Check the message for future deprecation."""
-    warn_msg = "data_transposed was deprecated in version 1.3"
-    with pytest.warns(FutureWarning, match=warn_msg):
-        make_sparse_coded_signal(
-            n_samples=1,
-            n_components=1,
-            n_features=1,
-            n_nonzero_coefs=1,
-            random_state=0,
-            data_transposed=True,
-        )
-
-
 def test_make_sparse_uncorrelated():
     X, y = make_sparse_uncorrelated(n_samples=5, n_features=10, random_state=0)
 
     assert X.shape == (5, 10), "X shape mismatch"
     assert y.shape == (5,), "y shape mismatch"
```

### Comparing `scikit-learn-1.4.2/sklearn/datasets/tests/test_svmlight_format.py` & `scikit_learn-1.5.0rc1/sklearn/datasets/tests/test_svmlight_format.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 """
 The :mod:`sklearn.decomposition` module includes matrix decomposition
 algorithms, including among others PCA, NMF or ICA. Most of the algorithms of
 this module can be regarded as dimensionality reduction techniques.
 """
 
-
 from ..utils.extmath import randomized_svd
 from ._dict_learning import (
     DictionaryLearning,
     MiniBatchDictionaryLearning,
     SparseCoder,
     dict_learning,
     dict_learning_online,
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_base.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_base.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,19 +8,17 @@
 #
 # License: BSD 3 clause
 
 from abc import ABCMeta, abstractmethod
 
 import numpy as np
 from scipy import linalg
-from scipy.sparse import issparse
 
 from ..base import BaseEstimator, ClassNamePrefixFeaturesOutMixin, TransformerMixin
 from ..utils._array_api import _add_to_diagonal, device, get_namespace
-from ..utils.sparsefuncs import _implicit_column_offset
 from ..utils.validation import check_is_fitted
 
 
 class _BasePCA(
     ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator, metaclass=ABCMeta
 ):
     """Base class for PCA methods.
@@ -134,29 +132,41 @@
 
         Returns
         -------
         X_new : array-like of shape (n_samples, n_components)
             Projection of X in the first principal components, where `n_samples`
             is the number of samples and `n_components` is the number of the components.
         """
-        xp, _ = get_namespace(X)
+        xp, _ = get_namespace(X, self.components_, self.explained_variance_)
 
         check_is_fitted(self)
 
         X = self._validate_data(
-            X, accept_sparse=("csr", "csc"), dtype=[xp.float64, xp.float32], reset=False
+            X, dtype=[xp.float64, xp.float32], accept_sparse=("csr", "csc"), reset=False
         )
-        if self.mean_ is not None:
-            if issparse(X):
-                X = _implicit_column_offset(X, self.mean_)
-            else:
-                X = X - self.mean_
+        return self._transform(X, xp=xp, x_is_centered=False)
+
+    def _transform(self, X, xp, x_is_centered=False):
         X_transformed = X @ self.components_.T
+        if not x_is_centered:
+            # Apply the centering after the projection.
+            # For dense X this avoids copying or mutating the data passed by
+            # the caller.
+            # For sparse X it keeps sparsity and avoids having to wrap X into
+            # a linear operator.
+            X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T
         if self.whiten:
-            X_transformed /= xp.sqrt(self.explained_variance_)
+            # For some solvers (such as "arpack" and "covariance_eigh"), on
+            # rank deficient data, some components can have a variance
+            # arbitrarily close to zero, leading to non-finite results when
+            # whitening. To avoid this problem we clip the variance below.
+            scale = xp.sqrt(self.explained_variance_)
+            min_scale = xp.finfo(scale.dtype).eps
+            scale[scale < min_scale] = min_scale
+            X_transformed /= scale
         return X_transformed
 
     def inverse_transform(self, X):
         """Transform data back to its original space.
 
         In other words, return an input `X_original` whose transform would be X.
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_cdnmf_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_cdnmf_fast.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_dict_learning.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_dict_learning.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-""" Dictionary learning.
-"""
+"""Dictionary learning."""
+
 # Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort
 # License: BSD 3 clause
 
 import itertools
 import sys
 import time
 from numbers import Integral, Real
@@ -660,14 +660,23 @@
 
     if return_n_iter:
         return code, dictionary, errors, ii + 1
     else:
         return code, dictionary, errors
 
 
+@validate_params(
+    {
+        "X": ["array-like"],
+        "return_code": ["boolean"],
+        "method": [StrOptions({"cd", "lars"})],
+        "method_max_iter": [Interval(Integral, 0, None, closed="left")],
+    },
+    prefer_skip_nested_validation=False,
+)
 def dict_learning_online(
     X,
     n_components=2,
     *,
     alpha=1,
     max_iter=100,
     return_code=True,
@@ -700,15 +709,15 @@
     This is accomplished by repeatedly iterating over mini-batches by slicing
     the input data.
 
     Read more in the :ref:`User Guide <DictionaryLearning>`.
 
     Parameters
     ----------
-    X : ndarray of shape (n_samples, n_features)
+    X : array-like of shape (n_samples, n_features)
         Data matrix.
 
     n_components : int or None, default=2
         Number of dictionary atoms to extract. If None, then ``n_components``
         is set to ``n_features``.
 
     alpha : float, default=1
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_factor_analysis.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_factor_analysis.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_fastica.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_fastica.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_incremental_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_incremental_pca.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_kernel_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_kernel_pca.py`

 * *Files 1% similar despite different names*

```diff
@@ -362,17 +362,15 @@
 
         # make sure that the eigenvalues are ok and fix numerical issues
         self.eigenvalues_ = _check_psd_eigenvalues(
             self.eigenvalues_, enable_warnings=False
         )
 
         # flip eigenvectors' sign to enforce deterministic output
-        self.eigenvectors_, _ = svd_flip(
-            self.eigenvectors_, np.zeros_like(self.eigenvectors_).T
-        )
+        self.eigenvectors_, _ = svd_flip(u=self.eigenvectors_, v=None)
 
         # sort eigenvectors in descending order
         indices = self.eigenvalues_.argsort()[::-1]
         self.eigenvalues_ = self.eigenvalues_[indices]
         self.eigenvectors_ = self.eigenvectors_[:, indices]
 
         # remove eigenvectors with a zero eigenvalue (null space) if required
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_lda.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_lda.py`

 * *Files 0% similar despite different names*

```diff
@@ -236,16 +236,15 @@
         Evaluating perplexity in every iteration might increase training time
         up to two-fold.
 
     total_samples : int, default=1e6
         Total number of documents. Only used in the :meth:`partial_fit` method.
 
     perp_tol : float, default=1e-1
-        Perplexity tolerance in batch learning. Only used when
-        ``evaluate_every`` is greater than 0.
+        Perplexity tolerance. Only used when ``evaluate_every`` is greater than 0.
 
     mean_change_tol : float, default=1e-3
         Stopping tolerance for updating document topic distribution in E-step.
 
     max_doc_update_iter : int, default=100
         Max number of iterations for updating document topic distribution in
         the E-step.
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_nmf.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_nmf.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-""" Non-negative matrix factorization.
-"""
+"""Non-negative matrix factorization."""
+
 # Author: Vlad Niculae
 #         Lars Buitinck
 #         Mathieu Blondel <mathieu@mblondel.org>
 #         Tom Dupre la Tour
 # License: BSD 3 clause
 
 import itertools
@@ -28,14 +28,15 @@
 from ..utils import check_array, check_random_state, gen_batches, metadata_routing
 from ..utils._param_validation import (
     Hidden,
     Interval,
     StrOptions,
     validate_params,
 )
+from ..utils.deprecation import _deprecate_Xt_in_inverse_transform
 from ..utils.extmath import randomized_svd, safe_sparse_dot, squared_norm
 from ..utils.validation import (
     check_is_fitted,
     check_non_negative,
 )
 from ._cdnmf_fast import _update_cdnmf_fast
 
@@ -1135,17 +1136,17 @@
     return W, H, n_iter
 
 
 class _BaseNMF(ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator, ABC):
     """Base class for NMF and MiniBatchNMF."""
 
     # This prevents ``set_split_inverse_transform`` to be generated for the
-    # non-standard ``W`` arg on ``inverse_transform``.
-    # TODO: remove when W is removed in v1.5 for inverse_transform
-    __metadata_request__inverse_transform = {"W": metadata_routing.UNUSED}
+    # non-standard ``Xt`` arg on ``inverse_transform``.
+    # TODO(1.7): remove when Xt is removed in v1.7 for inverse_transform
+    __metadata_request__inverse_transform = {"Xt": metadata_routing.UNUSED}
 
     _parameter_constraints: dict = {
         "n_components": [
             Interval(Integral, 1, None, closed="left"),
             None,
             StrOptions({"auto"}),
             Hidden(StrOptions({"warn"})),
@@ -1306,52 +1307,40 @@
             Returns the instance itself.
         """
         # param validation is done in fit_transform
 
         self.fit_transform(X, **params)
         return self
 
-    def inverse_transform(self, Xt=None, W=None):
+    def inverse_transform(self, X=None, *, Xt=None):
         """Transform data back to its original space.
 
         .. versionadded:: 0.18
 
         Parameters
         ----------
-        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)
+        X : {ndarray, sparse matrix} of shape (n_samples, n_components)
             Transformed data matrix.
 
-        W : deprecated
-            Use `Xt` instead.
+        Xt : {ndarray, sparse matrix} of shape (n_samples, n_components)
+            Transformed data matrix.
 
-            .. deprecated:: 1.3
+            .. deprecated:: 1.5
+                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.
 
         Returns
         -------
         X : ndarray of shape (n_samples, n_features)
             Returns a data matrix of the original shape.
         """
-        if Xt is None and W is None:
-            raise TypeError("Missing required positional argument: Xt")
-
-        if W is not None and Xt is not None:
-            raise ValueError("Please provide only `Xt`, and not `W`.")
 
-        if W is not None:
-            warnings.warn(
-                (
-                    "Input argument `W` was renamed to `Xt` in v1.3 and will be removed"
-                    " in v1.5."
-                ),
-                FutureWarning,
-            )
-            Xt = W
+        X = _deprecate_Xt_in_inverse_transform(X, Xt)
 
         check_is_fitted(self)
-        return Xt @ self.components_
+        return X @ self.components_
 
     @property
     def _n_features_out(self):
         """Number of transformed output features."""
         return self.components_.shape[0]
 
     def _more_tags(self):
@@ -1765,16 +1754,15 @@
             )
         else:
             raise ValueError("Invalid solver parameter '%s'." % self.solver)
 
         if n_iter == self.max_iter and self.tol > 0:
             warnings.warn(
                 "Maximum number of iterations %d reached. Increase "
-                "it to improve convergence."
-                % self.max_iter,
+                "it to improve convergence." % self.max_iter,
                 ConvergenceWarning,
             )
 
         return W, H, n_iter
 
     def transform(self, X):
         """Transform the data X according to the fitted NMF model.
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_online_lda_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_online_lda_fast.pyx`

 * *Files 11% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-from cython cimport floating
-
-cimport numpy as cnp
 import numpy as np
 
-cnp.import_array()
 
+from cython cimport floating
 from libc.math cimport exp, fabs, log
 
+from ..utils._typedefs cimport float64_t, intp_t
+
 
 def mean_change(const floating[:] arr_1, const floating[:] arr_2):
     """Calculate the mean difference between two arrays.
 
     Equivalent to np.abs(arr_1 - arr2).mean().
     """
 
-    cdef cnp.float64_t total, diff
-    cdef cnp.npy_intp i, size
+    cdef float64_t total, diff
+    cdef intp_t i, size
 
     size = arr_1.shape[0]
     total = 0.0
     for i in range(size):
         diff = fabs(arr_1[i] - arr_2[i])
         total += diff
 
@@ -37,15 +36,15 @@
 
     Equivalent to
         doc_topic += doc_topic_prior
         out[:] = np.exp(psi(doc_topic) - psi(np.sum(doc_topic)))
     """
 
     cdef floating dt, psi_total, total
-    cdef cnp.npy_intp i, size
+    cdef intp_t i, size
 
     size = doc_topic.shape[0]
 
     total = 0.0
     for i in range(size):
         dt = doc_topic[i] + doc_topic_prior
         doc_topic[i] = dt
@@ -63,15 +62,15 @@
     Equivalent to psi(arr) - psi(np.sum(arr, axis=1))[:, np.newaxis].
 
     Note that unlike _dirichlet_expectation_1d, this function doesn't compute
     the exp and doesn't add in the prior.
     """
     cdef floating row_total, psi_row_total
     cdef floating[:, :] d_exp
-    cdef cnp.npy_intp i, j, n_rows, n_cols
+    cdef intp_t i, j, n_rows, n_cols
 
     n_rows = arr.shape[0]
     n_cols = arr.shape[1]
 
     d_exp = np.empty_like(arr)
     for i in range(n_rows):
         row_total = 0
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_pca.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-""" Principal Component Analysis.
-"""
+"""Principal Component Analysis."""
 
 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #         Olivier Grisel <olivier.grisel@ensta.org>
 #         Mathieu Blondel <mathieu@mblondel.org>
 #         Denis A. Engemann <denis-alexander.engemann@inria.fr>
 #         Michael Eickenberg <michael.eickenberg@inria.fr>
 #         Giorgio Patrini <giorgio.patrini@anu.edu.au>
@@ -126,19 +125,21 @@
     data to project it to a lower dimensional space. The input data is centered
     but not scaled for each feature before applying the SVD.
 
     It uses the LAPACK implementation of the full SVD or a randomized truncated
     SVD by the method of Halko et al. 2009, depending on the shape of the input
     data and the number of components to extract.
 
-    It can also use the scipy.sparse.linalg ARPACK implementation of the
-    truncated SVD.
-
-    Notice that this class does not support sparse input. See
-    :class:`TruncatedSVD` for an alternative with sparse data.
+    With sparse inputs, the ARPACK implementation of the truncated SVD can be
+    used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one
+    may consider :class:`TruncatedSVD` where the data are not centered.
+
+    Notice that this class only supports sparse inputs for some solvers such as
+    "arpack" and "covariance_eigh". See :class:`TruncatedSVD` for an
+    alternative with sparse data.
 
     For a usage example, see
     :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`
 
     Read more in the :ref:`User Guide <PCA>`.
 
     Parameters
@@ -175,34 +176,51 @@
         to ensure uncorrelated outputs with unit component-wise variances.
 
         Whitening will remove some information from the transformed signal
         (the relative variance scales of the components) but can sometime
         improve the predictive accuracy of the downstream estimators by
         making their data respect some hard-wired assumptions.
 
-    svd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'
-        If auto :
-            The solver is selected by a default policy based on `X.shape` and
-            `n_components`: if the input data is larger than 500x500 and the
-            number of components to extract is lower than 80% of the smallest
-            dimension of the data, then the more efficient 'randomized'
-            method is enabled. Otherwise the exact full SVD is computed and
-            optionally truncated afterwards.
-        If full :
-            run exact full SVD calling the standard LAPACK solver via
+    svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},\
+            default='auto'
+        "auto" :
+            The solver is selected by a default 'auto' policy is based on `X.shape` and
+            `n_components`: if the input data has fewer than 1000 features and
+            more than 10 times as many samples, then the "covariance_eigh"
+            solver is used. Otherwise, if the input data is larger than 500x500
+            and the number of components to extract is lower than 80% of the
+            smallest dimension of the data, then the more efficient
+            "randomized" method is selected. Otherwise the exact "full" SVD is
+            computed and optionally truncated afterwards.
+        "full" :
+            Run exact full SVD calling the standard LAPACK solver via
             `scipy.linalg.svd` and select the components by postprocessing
-        If arpack :
-            run SVD truncated to n_components calling ARPACK solver via
+        "covariance_eigh" :
+            Precompute the covariance matrix (on centered data), run a
+            classical eigenvalue decomposition on the covariance matrix
+            typically using LAPACK and select the components by postprocessing.
+            This solver is very efficient for n_samples >> n_features and small
+            n_features. It is, however, not tractable otherwise for large
+            n_features (large memory footprint required to materialize the
+            covariance matrix). Also note that compared to the "full" solver,
+            this solver effectively doubles the condition number and is
+            therefore less numerical stable (e.g. on input data with a large
+            range of singular values).
+        "arpack" :
+            Run SVD truncated to `n_components` calling ARPACK solver via
             `scipy.sparse.linalg.svds`. It requires strictly
-            0 < n_components < min(X.shape)
-        If randomized :
-            run randomized SVD by the method of Halko et al.
+            `0 < n_components < min(X.shape)`
+        "randomized" :
+            Run randomized SVD by the method of Halko et al.
 
         .. versionadded:: 0.18.0
 
+        .. versionchanged:: 1.5
+            Added the 'covariance_eigh' solver.
+
     tol : float, default=0.0
         Tolerance for singular values computed by svd_solver == 'arpack'.
         Must be of range [0.0, infinity).
 
         .. versionadded:: 0.18.0
 
     iterated_power : int or 'auto', default='auto'
@@ -369,15 +387,17 @@
             Interval(Integral, 0, None, closed="left"),
             Interval(RealNotInt, 0, 1, closed="neither"),
             StrOptions({"mle"}),
             None,
         ],
         "copy": ["boolean"],
         "whiten": ["boolean"],
-        "svd_solver": [StrOptions({"auto", "full", "arpack", "randomized"})],
+        "svd_solver": [
+            StrOptions({"auto", "full", "covariance_eigh", "arpack", "randomized"})
+        ],
         "tol": [Interval(Real, 0, None, closed="left")],
         "iterated_power": [
             StrOptions({"auto"}),
             Interval(Integral, 0, None, closed="left"),
         ],
         "n_oversamples": [Interval(Integral, 1, None, closed="left")],
         "power_iteration_normalizer": [StrOptions({"auto", "QR", "LU", "none"})],
@@ -447,118 +467,191 @@
             Transformed values.
 
         Notes
         -----
         This method returns a Fortran-ordered array. To convert it to a
         C-ordered array, use 'np.ascontiguousarray'.
         """
-        U, S, Vt = self._fit(X)
-        U = U[:, : self.n_components_]
-
-        if self.whiten:
-            # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)
-            U *= sqrt(X.shape[0] - 1)
-        else:
-            # X_new = X * V = U * S * Vt * V = U * S
-            U *= S[: self.n_components_]
+        U, S, _, X, x_is_centered, xp = self._fit(X)
+        if U is not None:
+            U = U[:, : self.n_components_]
+
+            if self.whiten:
+                # X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)
+                U *= sqrt(X.shape[0] - 1)
+            else:
+                # X_new = X * V = U * S * Vt * V = U * S
+                U *= S[: self.n_components_]
 
-        return U
+            return U
+        else:  # solver="covariance_eigh" does not compute U at fit time.
+            return self._transform(X, xp, x_is_centered=x_is_centered)
 
     def _fit(self, X):
         """Dispatch to the right submethod depending on the chosen solver."""
         xp, is_array_api_compliant = get_namespace(X)
 
         # Raise an error for sparse input and unsupported svd_solver
-        if issparse(X) and self.svd_solver != "arpack":
+        if issparse(X) and self.svd_solver not in ["auto", "arpack", "covariance_eigh"]:
             raise TypeError(
-                'PCA only support sparse inputs with the "arpack" solver, while '
-                f'"{self.svd_solver}" was passed. See TruncatedSVD for a possible'
-                " alternative."
+                'PCA only support sparse inputs with the "arpack" and'
+                f' "covariance_eigh" solvers, while "{self.svd_solver}" was passed. See'
+                " TruncatedSVD for a possible alternative."
             )
-        # Raise an error for non-Numpy input and arpack solver.
         if self.svd_solver == "arpack" and is_array_api_compliant:
             raise ValueError(
                 "PCA with svd_solver='arpack' is not supported for Array API inputs."
             )
 
+        # Validate the data, without ever forcing a copy as any solver that
+        # supports sparse input data and the `covariance_eigh` solver are
+        # written in a way to avoid the need for any inplace modification of
+        # the input data contrary to the other solvers.
+        # The copy will happen
+        # later, only if needed, once the solver negotiation below is done.
         X = self._validate_data(
             X,
             dtype=[xp.float64, xp.float32],
             accept_sparse=("csr", "csc"),
             ensure_2d=True,
-            copy=self.copy,
+            copy=False,
         )
+        self._fit_svd_solver = self.svd_solver
+        if self._fit_svd_solver == "auto" and issparse(X):
+            self._fit_svd_solver = "arpack"
 
-        # Handle n_components==None
         if self.n_components is None:
-            if self.svd_solver != "arpack":
+            if self._fit_svd_solver != "arpack":
                 n_components = min(X.shape)
             else:
                 n_components = min(X.shape) - 1
         else:
             n_components = self.n_components
 
-        # Handle svd_solver
-        self._fit_svd_solver = self.svd_solver
         if self._fit_svd_solver == "auto":
+            # Tall and skinny problems are best handled by precomputing the
+            # covariance matrix.
+            if X.shape[1] <= 1_000 and X.shape[0] >= 10 * X.shape[1]:
+                self._fit_svd_solver = "covariance_eigh"
             # Small problem or n_components == 'mle', just call full PCA
-            if max(X.shape) <= 500 or n_components == "mle":
+            elif max(X.shape) <= 500 or n_components == "mle":
                 self._fit_svd_solver = "full"
             elif 1 <= n_components < 0.8 * min(X.shape):
                 self._fit_svd_solver = "randomized"
-            # This is also the case of n_components in (0,1)
+            # This is also the case of n_components in (0, 1)
             else:
                 self._fit_svd_solver = "full"
 
         # Call different fits for either full or truncated SVD
-        if self._fit_svd_solver == "full":
-            return self._fit_full(X, n_components)
+        if self._fit_svd_solver in ("full", "covariance_eigh"):
+            return self._fit_full(X, n_components, xp, is_array_api_compliant)
         elif self._fit_svd_solver in ["arpack", "randomized"]:
-            return self._fit_truncated(X, n_components, self._fit_svd_solver)
+            return self._fit_truncated(X, n_components, xp)
 
-    def _fit_full(self, X, n_components):
+    def _fit_full(self, X, n_components, xp, is_array_api_compliant):
         """Fit the model by computing full SVD on X."""
-        xp, is_array_api_compliant = get_namespace(X)
-
         n_samples, n_features = X.shape
 
         if n_components == "mle":
             if n_samples < n_features:
                 raise ValueError(
                     "n_components='mle' is only supported if n_samples >= n_features"
                 )
         elif not 0 <= n_components <= min(n_samples, n_features):
             raise ValueError(
-                "n_components=%r must be between 0 and "
-                "min(n_samples, n_features)=%r with "
-                "svd_solver='full'" % (n_components, min(n_samples, n_features))
+                f"n_components={n_components} must be between 0 and "
+                f"min(n_samples, n_features)={min(n_samples, n_features)} with "
+                f"svd_solver={self._fit_svd_solver!r}"
             )
 
-        # Center data
         self.mean_ = xp.mean(X, axis=0)
-        X -= self.mean_
+        # When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need
+        # to transform it to a 1D array. Note that this is not the case when X
+        # is a scipy sparse array.
+        # TODO: remove the following two lines when scikit-learn only depends
+        # on scipy versions that no longer support scipy.sparse matrices.
+        self.mean_ = xp.reshape(xp.asarray(self.mean_), (-1,))
+
+        if self._fit_svd_solver == "full":
+            X_centered = xp.asarray(X, copy=True) if self.copy else X
+            X_centered -= self.mean_
+            x_is_centered = not self.copy
+
+            if not is_array_api_compliant:
+                # Use scipy.linalg with NumPy/SciPy inputs for the sake of not
+                # introducing unanticipated behavior changes. In the long run we
+                # could instead decide to always use xp.linalg.svd for all inputs,
+                # but that would make this code rely on numpy's SVD instead of
+                # scipy's. It's not 100% clear whether they use the same LAPACK
+                # solver by default though (assuming both are built against the
+                # same BLAS).
+                U, S, Vt = linalg.svd(X_centered, full_matrices=False)
+            else:
+                U, S, Vt = xp.linalg.svd(X_centered, full_matrices=False)
+            explained_variance_ = (S**2) / (n_samples - 1)
 
-        if not is_array_api_compliant:
-            # Use scipy.linalg with NumPy/SciPy inputs for the sake of not
-            # introducing unanticipated behavior changes. In the long run we
-            # could instead decide to always use xp.linalg.svd for all inputs,
-            # but that would make this code rely on numpy's SVD instead of
-            # scipy's. It's not 100% clear whether they use the same LAPACK
-            # solver by default though (assuming both are built against the
-            # same BLAS).
-            U, S, Vt = linalg.svd(X, full_matrices=False)
         else:
-            U, S, Vt = xp.linalg.svd(X, full_matrices=False)
+            assert self._fit_svd_solver == "covariance_eigh"
+            # In the following, we center the covariance matrix C afterwards
+            # (without centering the data X first) to avoid an unnecessary copy
+            # of X. Note that the mean_ attribute is still needed to center
+            # test data in the transform method.
+            #
+            # Note: at the time of writing, `xp.cov` does not exist in the
+            # Array API standard:
+            # https://github.com/data-apis/array-api/issues/43
+            #
+            # Besides, using `numpy.cov`, as of numpy 1.26.0, would not be
+            # memory efficient for our use case when `n_samples >> n_features`:
+            # `numpy.cov` centers a copy of the data before computing the
+            # matrix product instead of subtracting a small `(n_features,
+            # n_features)` square matrix from the gram matrix X.T @ X, as we do
+            # below.
+            x_is_centered = False
+            C = X.T @ X
+            C -= (
+                n_samples
+                * xp.reshape(self.mean_, (-1, 1))
+                * xp.reshape(self.mean_, (1, -1))
+            )
+            C /= n_samples - 1
+            eigenvals, eigenvecs = xp.linalg.eigh(C)
+
+            # When X is a scipy sparse matrix, the following two datastructures
+            # are returned as instances of the soft-deprecated numpy.matrix
+            # class. Note that this problem does not occur when X is a scipy
+            # sparse array (or another other kind of supported array).
+            # TODO: remove the following two lines when scikit-learn only
+            # depends on scipy versions that no longer support scipy.sparse
+            # matrices.
+            eigenvals = xp.reshape(xp.asarray(eigenvals), (-1,))
+            eigenvecs = xp.asarray(eigenvecs)
+
+            eigenvals = xp.flip(eigenvals, axis=0)
+            eigenvecs = xp.flip(eigenvecs, axis=1)
+
+            # The covariance matrix C is positive semi-definite by
+            # construction. However, the eigenvalues returned by xp.linalg.eigh
+            # can be slightly negative due to numerical errors. This would be
+            # an issue for the subsequent sqrt, hence the manual clipping.
+            eigenvals[eigenvals < 0.0] = 0.0
+            explained_variance_ = eigenvals
+
+            # Re-construct SVD of centered X indirectly and make it consistent
+            # with the other solvers.
+            S = xp.sqrt(eigenvals * (n_samples - 1))
+            Vt = eigenvecs.T
+            U = None
+
         # flip eigenvectors' sign to enforce deterministic output
-        U, Vt = svd_flip(U, Vt)
+        U, Vt = svd_flip(U, Vt, u_based_decision=False)
 
         components_ = Vt
 
         # Get variance explained by singular values
-        explained_variance_ = (S**2) / (n_samples - 1)
         total_var = xp.sum(explained_variance_)
         explained_variance_ratio_ = explained_variance_ / total_var
         singular_values_ = xp.asarray(S, copy=True)  # Store the singular values.
 
         # Postprocess the number of components required
         if n_components == "mle":
             n_components = _infer_dimension(explained_variance_, n_samples)
@@ -590,30 +683,41 @@
         # The sigma2 maximum likelihood (cf. eq. 12.46)
         if n_components < min(n_features, n_samples):
             self.noise_variance_ = xp.mean(explained_variance_[n_components:])
         else:
             self.noise_variance_ = 0.0
 
         self.n_samples_ = n_samples
-        self.components_ = components_[:n_components, :]
         self.n_components_ = n_components
-        self.explained_variance_ = explained_variance_[:n_components]
-        self.explained_variance_ratio_ = explained_variance_ratio_[:n_components]
-        self.singular_values_ = singular_values_[:n_components]
+        # Assign a copy of the result of the truncation of the components in
+        # order to:
+        # - release the memory used by the discarded components,
+        # - ensure that the kept components are allocated contiguously in
+        #   memory to make the transform method faster by leveraging cache
+        #   locality.
+        self.components_ = xp.asarray(components_[:n_components, :], copy=True)
+
+        # We do the same for the other arrays for the sake of consistency.
+        self.explained_variance_ = xp.asarray(
+            explained_variance_[:n_components], copy=True
+        )
+        self.explained_variance_ratio_ = xp.asarray(
+            explained_variance_ratio_[:n_components], copy=True
+        )
+        self.singular_values_ = xp.asarray(singular_values_[:n_components], copy=True)
 
-        return U, S, Vt
+        return U, S, Vt, X, x_is_centered, xp
 
-    def _fit_truncated(self, X, n_components, svd_solver):
+    def _fit_truncated(self, X, n_components, xp):
         """Fit the model by computing truncated SVD (by ARPACK or randomized)
         on X.
         """
-        xp, _ = get_namespace(X)
-
         n_samples, n_features = X.shape
 
+        svd_solver = self._fit_svd_solver
         if isinstance(n_components, str):
             raise ValueError(
                 "n_components=%r cannot be a string with svd_solver='%s'"
                 % (n_components, svd_solver)
             )
         elif not 1 <= n_components <= min(n_samples, n_features):
             raise ValueError(
@@ -633,39 +737,43 @@
         random_state = check_random_state(self.random_state)
 
         # Center data
         total_var = None
         if issparse(X):
             self.mean_, var = mean_variance_axis(X, axis=0)
             total_var = var.sum() * n_samples / (n_samples - 1)  # ddof=1
-            X = _implicit_column_offset(X, self.mean_)
+            X_centered = _implicit_column_offset(X, self.mean_)
+            x_is_centered = False
         else:
             self.mean_ = xp.mean(X, axis=0)
-            X -= self.mean_
+            X_centered = xp.asarray(X, copy=True) if self.copy else X
+            X_centered -= self.mean_
+            x_is_centered = not self.copy
 
         if svd_solver == "arpack":
             v0 = _init_arpack_v0(min(X.shape), random_state)
-            U, S, Vt = svds(X, k=n_components, tol=self.tol, v0=v0)
+            U, S, Vt = svds(X_centered, k=n_components, tol=self.tol, v0=v0)
             # svds doesn't abide by scipy.linalg.svd/randomized_svd
             # conventions, so reverse its outputs.
             S = S[::-1]
             # flip eigenvectors' sign to enforce deterministic output
-            U, Vt = svd_flip(U[:, ::-1], Vt[::-1])
+            U, Vt = svd_flip(U[:, ::-1], Vt[::-1], u_based_decision=False)
 
         elif svd_solver == "randomized":
             # sign flipping is done inside
             U, S, Vt = randomized_svd(
-                X,
+                X_centered,
                 n_components=n_components,
                 n_oversamples=self.n_oversamples,
                 n_iter=self.iterated_power,
                 power_iteration_normalizer=self.power_iteration_normalizer,
-                flip_sign=True,
+                flip_sign=False,
                 random_state=random_state,
             )
+            U, Vt = svd_flip(U, Vt, u_based_decision=False)
 
         self.n_samples_ = n_samples
         self.components_ = Vt
         self.n_components_ = n_components
 
         # Get variance explained by singular values
         self.explained_variance_ = (S**2) / (n_samples - 1)
@@ -675,27 +783,27 @@
         #
         # TODO: update this code to either:
         # * Use the array-api variance calculation, unless memory usage suffers
         # * Update sklearn.utils.extmath._incremental_mean_and_var to support array-api
         # See: https://github.com/scikit-learn/scikit-learn/pull/18689#discussion_r1335540991
         if total_var is None:
             N = X.shape[0] - 1
-            X **= 2
-            total_var = xp.sum(X) / N
+            X_centered **= 2
+            total_var = xp.sum(X_centered) / N
 
         self.explained_variance_ratio_ = self.explained_variance_ / total_var
         self.singular_values_ = xp.asarray(S, copy=True)  # Store the singular values.
 
         if self.n_components_ < min(n_features, n_samples):
             self.noise_variance_ = total_var - xp.sum(self.explained_variance_)
             self.noise_variance_ /= min(n_features, n_samples) - n_components
         else:
             self.noise_variance_ = 0.0
 
-        return U, S, Vt
+        return U, S, Vt, X, x_is_centered, xp
 
     def score_samples(self, X):
         """Return the log-likelihood of each sample.
 
         See. "Pattern Recognition and Machine Learning"
         by C. Bishop, 12.2.1 p. 574
         or http://www.miketipping.com/papers/met-mppca.pdf
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_sparse_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_sparse_pca.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Matrix factorization with Sparse PCA."""
+
 # Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort
 # License: BSD 3 clause
 
 from numbers import Integral, Real
 
 import numpy as np
 
@@ -320,15 +321,15 @@
             verbose=self.verbose,
             random_state=random_state,
             code_init=code_init,
             dict_init=dict_init,
             return_n_iter=True,
         )
         # flip eigenvectors' sign to enforce deterministic output
-        code, dictionary = svd_flip(code, dictionary, u_based_decision=False)
+        code, dictionary = svd_flip(code, dictionary, u_based_decision=True)
         self.components_ = code.T
         components_norm = np.linalg.norm(self.components_, axis=1)[:, np.newaxis]
         components_norm[components_norm == 0] = 1
         self.components_ /= components_norm
         self.n_components_ = len(self.components_)
 
         self.error_ = E
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/_truncated_svd.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/_truncated_svd.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Truncated SVD for sparse matrices, aka latent semantic analysis (LSA).
-"""
+"""Truncated SVD for sparse matrices, aka latent semantic analysis (LSA)."""
 
 # Author: Lars Buitinck
 #         Olivier Grisel <olivier.grisel@ensta.org>
 #         Michael Becker <mike@beckerfuffle.com>
 # License: 3-clause BSD.
 
 from numbers import Integral, Real
@@ -231,30 +230,33 @@
 
         if self.algorithm == "arpack":
             v0 = _init_arpack_v0(min(X.shape), random_state)
             U, Sigma, VT = svds(X, k=self.n_components, tol=self.tol, v0=v0)
             # svds doesn't abide by scipy.linalg.svd/randomized_svd
             # conventions, so reverse its outputs.
             Sigma = Sigma[::-1]
-            U, VT = svd_flip(U[:, ::-1], VT[::-1])
+            # u_based_decision=False is needed to be consistent with PCA.
+            U, VT = svd_flip(U[:, ::-1], VT[::-1], u_based_decision=False)
 
         elif self.algorithm == "randomized":
             if self.n_components > X.shape[1]:
                 raise ValueError(
                     f"n_components({self.n_components}) must be <="
                     f" n_features({X.shape[1]})."
                 )
             U, Sigma, VT = randomized_svd(
                 X,
                 self.n_components,
                 n_iter=self.n_iter,
                 n_oversamples=self.n_oversamples,
                 power_iteration_normalizer=self.power_iteration_normalizer,
                 random_state=random_state,
+                flip_sign=False,
             )
+            U, VT = svd_flip(U, VT, u_based_decision=False)
 
         self.components_ = VT
 
         # As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,
         # X @ V is not the same as U @ Sigma
         if self.algorithm == "randomized" or (
             self.algorithm == "arpack" and self.tol > 0
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_dict_learning.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_dict_learning.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_factor_analysis.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_factor_analysis.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_fastica.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_fastica.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Test the fastica algorithm.
 """
+
 import itertools
 import os
 import warnings
 
 import numpy as np
 import pytest
 from scipy import stats
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_incremental_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_incremental_pca.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """Tests for Incremental PCA."""
+
 import warnings
 
 import numpy as np
 import pytest
-from numpy.testing import assert_array_equal
+from numpy.testing import assert_allclose, assert_array_equal
 
 from sklearn import datasets
 from sklearn.decomposition import PCA, IncrementalPCA
 from sklearn.utils._testing import (
     assert_allclose_dense_sparse,
     assert_almost_equal,
     assert_array_almost_equal,
@@ -379,33 +380,46 @@
     X_hat = np.dot(X_pca, pca.components_)
     pca.fit(X_hat)
     ipca.fit(X_hat)
     assert_array_almost_equal(pca.singular_values_, [3.142, 2.718, 1.0], 14)
     assert_array_almost_equal(ipca.singular_values_, [3.142, 2.718, 1.0], 14)
 
 
-def test_whitening():
+def test_whitening(global_random_seed):
     # Test that PCA and IncrementalPCA transforms match to sign flip.
     X = datasets.make_low_rank_matrix(
-        1000, 10, tail_strength=0.0, effective_rank=2, random_state=1999
+        1000, 10, tail_strength=0.0, effective_rank=2, random_state=global_random_seed
     )
-    prec = 3
-    n_samples, n_features = X.shape
+    atol = 1e-3
     for nc in [None, 9]:
         pca = PCA(whiten=True, n_components=nc).fit(X)
         ipca = IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X)
 
+        # Since the data is rank deficient, some components are pure noise. We
+        # should not expect those dimensions to carry any signal and their
+        # values might be arbitrarily changed by implementation details of the
+        # internal SVD solver. We therefore filter them out before comparison.
+        stable_mask = pca.explained_variance_ratio_ > 1e-12
+
         Xt_pca = pca.transform(X)
         Xt_ipca = ipca.transform(X)
-        assert_almost_equal(np.abs(Xt_pca), np.abs(Xt_ipca), decimal=prec)
+        assert_allclose(
+            np.abs(Xt_pca)[:, stable_mask],
+            np.abs(Xt_ipca)[:, stable_mask],
+            atol=atol,
+        )
+
+        # The noisy dimensions are in the null space of the inverse transform,
+        # so they are not influencing the reconstruction. We therefore don't
+        # need to apply the mask here.
         Xinv_ipca = ipca.inverse_transform(Xt_ipca)
         Xinv_pca = pca.inverse_transform(Xt_pca)
-        assert_almost_equal(X, Xinv_ipca, decimal=prec)
-        assert_almost_equal(X, Xinv_pca, decimal=prec)
-        assert_almost_equal(Xinv_pca, Xinv_ipca, decimal=prec)
+        assert_allclose(X, Xinv_ipca, atol=atol)
+        assert_allclose(X, Xinv_pca, atol=atol)
+        assert_allclose(Xinv_pca, Xinv_ipca, atol=atol)
 
 
 def test_incremental_pca_partial_fit_float_division():
     # Test to ensure float division is used in all versions of Python
     # (non-regression test for issue #9489)
 
     rng = np.random.RandomState(0)
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_kernel_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_kernel_pca.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_nmf.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_nmf.py`

 * *Files 1% similar despite different names*

```diff
@@ -929,38 +929,39 @@
     sys.stdout = StringIO()
     try:
         nmf.fit(A)
     finally:
         sys.stdout = old_stdout
 
 
-# TODO(1.5): remove this test
-def test_NMF_inverse_transform_W_deprecation():
-    rng = np.random.mtrand.RandomState(42)
+# TODO(1.7): remove this test
+@pytest.mark.parametrize("Estimator", [NMF, MiniBatchNMF])
+def test_NMF_inverse_transform_Xt_deprecation(Estimator):
+    rng = np.random.RandomState(42)
     A = np.abs(rng.randn(6, 5))
-    est = NMF(
+    est = Estimator(
         n_components=3,
         init="random",
         random_state=0,
         tol=1e-6,
     )
-    Xt = est.fit_transform(A)
+    X = est.fit_transform(A)
 
     with pytest.raises(TypeError, match="Missing required positional argument"):
         est.inverse_transform()
 
-    with pytest.raises(ValueError, match="Please provide only"):
-        est.inverse_transform(Xt=Xt, W=Xt)
+    with pytest.raises(TypeError, match="Cannot use both X and Xt. Use X only"):
+        est.inverse_transform(X=X, Xt=X)
 
     with warnings.catch_warnings(record=True):
         warnings.simplefilter("error")
-        est.inverse_transform(Xt)
+        est.inverse_transform(X)
 
-    with pytest.warns(FutureWarning, match="Input argument `W` was renamed to `Xt`"):
-        est.inverse_transform(W=Xt)
+    with pytest.warns(FutureWarning, match="Xt was renamed X in version 1.5"):
+        est.inverse_transform(Xt=X)
 
 
 @pytest.mark.parametrize("Estimator", [NMF, MiniBatchNMF])
 def test_nmf_n_components_auto(Estimator):
     # Check that n_components is correctly inferred
     # from the provided custom initialization.
     rng = np.random.RandomState(0)
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_online_lda.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_online_lda.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_pca.py`

 * *Files 13% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import numpy as np
 import pytest
 import scipy as sp
 from numpy.testing import assert_array_equal
 
 from sklearn import config_context, datasets
 from sklearn.base import clone
-from sklearn.datasets import load_iris, make_classification
+from sklearn.datasets import load_iris, make_classification, make_low_rank_matrix
 from sklearn.decomposition import PCA
 from sklearn.decomposition._pca import _assess_dimension, _infer_dimension
 from sklearn.utils._array_api import (
     _atol_for_type,
     _convert_to_numpy,
     yield_namespace_device_dtype_combinations,
 )
@@ -21,32 +21,35 @@
 from sklearn.utils.estimator_checks import (
     _get_check_estimator_ids,
     check_array_api_input_and_values,
 )
 from sklearn.utils.fixes import CSC_CONTAINERS, CSR_CONTAINERS
 
 iris = datasets.load_iris()
-PCA_SOLVERS = ["full", "arpack", "randomized", "auto"]
+PCA_SOLVERS = ["full", "covariance_eigh", "arpack", "randomized", "auto"]
 
 # `SPARSE_M` and `SPARSE_N` could be larger, but be aware:
 # * SciPy's generation of random sparse matrix can be costly
 # * A (SPARSE_M, SPARSE_N) dense array is allocated to compare against
 SPARSE_M, SPARSE_N = 1000, 300  # arbitrary
 SPARSE_MAX_COMPONENTS = min(SPARSE_M, SPARSE_N)
 
 
-def _check_fitted_pca_close(pca1, pca2, rtol):
-    assert_allclose(pca1.components_, pca2.components_, rtol=rtol)
-    assert_allclose(pca1.explained_variance_, pca2.explained_variance_, rtol=rtol)
-    assert_allclose(pca1.singular_values_, pca2.singular_values_, rtol=rtol)
-    assert_allclose(pca1.mean_, pca2.mean_, rtol=rtol)
-    assert_allclose(pca1.n_components_, pca2.n_components_, rtol=rtol)
-    assert_allclose(pca1.n_samples_, pca2.n_samples_, rtol=rtol)
-    assert_allclose(pca1.noise_variance_, pca2.noise_variance_, rtol=rtol)
-    assert_allclose(pca1.n_features_in_, pca2.n_features_in_, rtol=rtol)
+def _check_fitted_pca_close(pca1, pca2, rtol=1e-7, atol=1e-12):
+    assert_allclose(pca1.components_, pca2.components_, rtol=rtol, atol=atol)
+    assert_allclose(
+        pca1.explained_variance_, pca2.explained_variance_, rtol=rtol, atol=atol
+    )
+    assert_allclose(pca1.singular_values_, pca2.singular_values_, rtol=rtol, atol=atol)
+    assert_allclose(pca1.mean_, pca2.mean_, rtol=rtol, atol=atol)
+    assert_allclose(pca1.noise_variance_, pca2.noise_variance_, rtol=rtol, atol=atol)
+
+    assert pca1.n_components_ == pca2.n_components_
+    assert pca1.n_samples_ == pca2.n_samples_
+    assert pca1.n_features_in_ == pca2.n_features_in_
 
 
 @pytest.mark.parametrize("svd_solver", PCA_SOLVERS)
 @pytest.mark.parametrize("n_components", range(1, iris.data.shape[1]))
 def test_pca(svd_solver, n_components):
     X = iris.data
     pca = PCA(n_components=n_components, svd_solver=svd_solver)
@@ -66,22 +69,25 @@
     precision = pca.get_precision()
     assert_allclose(np.dot(cov, precision), np.eye(X.shape[1]), atol=1e-12)
 
 
 @pytest.mark.parametrize("density", [0.01, 0.1, 0.30])
 @pytest.mark.parametrize("n_components", [1, 2, 10])
 @pytest.mark.parametrize("sparse_container", CSR_CONTAINERS + CSC_CONTAINERS)
-@pytest.mark.parametrize("svd_solver", ["arpack"])
+@pytest.mark.parametrize("svd_solver", ["arpack", "covariance_eigh"])
 @pytest.mark.parametrize("scale", [1, 10, 100])
 def test_pca_sparse(
     global_random_seed, svd_solver, sparse_container, n_components, density, scale
 ):
-    # Make sure any tolerance changes pass with SKLEARN_TESTS_GLOBAL_RANDOM_SEED="all"
-    rtol = 5e-07
-    transform_rtol = 3e-05
+    """Check that the results are the same for sparse and dense input."""
+
+    # Set atol in addition of the default rtol to account for the very wide range of
+    # result values (1e-8 to 1e0).
+    atol = 1e-12
+    transform_atol = 1e-10
 
     random_state = np.random.default_rng(global_random_seed)
     X = sparse_container(
         sp.sparse.random(
             SPARSE_M,
             SPARSE_N,
             random_state=random_state,
@@ -104,29 +110,29 @@
         n_components=n_components,
         svd_solver=svd_solver,
         random_state=global_random_seed,
     )
     pcad.fit(Xd)
 
     # Fitted attributes equality
-    _check_fitted_pca_close(pca, pcad, rtol=rtol)
+    _check_fitted_pca_close(pca, pcad, atol=atol)
 
     # Test transform
     X2 = sparse_container(
         sp.sparse.random(
             SPARSE_M,
             SPARSE_N,
             random_state=random_state,
             density=density,
         )
     )
     X2d = X2.toarray()
 
-    assert_allclose(pca.transform(X2), pca.transform(X2d), rtol=transform_rtol)
-    assert_allclose(pca.transform(X2), pcad.transform(X2d), rtol=transform_rtol)
+    assert_allclose(pca.transform(X2), pca.transform(X2d), atol=transform_atol)
+    assert_allclose(pca.transform(X2), pcad.transform(X2d), atol=transform_atol)
 
 
 @pytest.mark.parametrize("sparse_container", CSR_CONTAINERS + CSC_CONTAINERS)
 def test_pca_sparse_fit_transform(global_random_seed, sparse_container):
     random_state = np.random.default_rng(global_random_seed)
     X = sparse_container(
         sp.sparse.random(
@@ -149,40 +155,58 @@
     pca_fit_transform = PCA(
         n_components=10, svd_solver="arpack", random_state=global_random_seed
     )
 
     pca_fit.fit(X)
     transformed_X = pca_fit_transform.fit_transform(X)
 
-    _check_fitted_pca_close(pca_fit, pca_fit_transform, rtol=1e-10)
-    assert_allclose(transformed_X, pca_fit_transform.transform(X), rtol=2e-9)
-    assert_allclose(transformed_X, pca_fit.transform(X), rtol=2e-9)
-    assert_allclose(pca_fit.transform(X2), pca_fit_transform.transform(X2), rtol=2e-9)
+    _check_fitted_pca_close(pca_fit, pca_fit_transform)
+    assert_allclose(transformed_X, pca_fit_transform.transform(X))
+    assert_allclose(transformed_X, pca_fit.transform(X))
+    assert_allclose(pca_fit.transform(X2), pca_fit_transform.transform(X2))
 
 
-@pytest.mark.parametrize("svd_solver", ["randomized", "full", "auto"])
+@pytest.mark.parametrize("svd_solver", ["randomized", "full"])
 @pytest.mark.parametrize("sparse_container", CSR_CONTAINERS + CSC_CONTAINERS)
 def test_sparse_pca_solver_error(global_random_seed, svd_solver, sparse_container):
     random_state = np.random.RandomState(global_random_seed)
     X = sparse_container(
         sp.sparse.random(
             SPARSE_M,
             SPARSE_N,
             random_state=random_state,
         )
     )
     pca = PCA(n_components=30, svd_solver=svd_solver)
     error_msg_pattern = (
-        f'PCA only support sparse inputs with the "arpack" solver, while "{svd_solver}"'
-        " was passed"
+        'PCA only support sparse inputs with the "arpack" and "covariance_eigh"'
+        f' solvers, while "{svd_solver}" was passed'
     )
     with pytest.raises(TypeError, match=error_msg_pattern):
         pca.fit(X)
 
 
+@pytest.mark.parametrize("sparse_container", CSR_CONTAINERS + CSC_CONTAINERS)
+def test_sparse_pca_auto_arpack_singluar_values_consistency(
+    global_random_seed, sparse_container
+):
+    """Check that "auto" and "arpack" solvers are equivalent for sparse inputs."""
+    random_state = np.random.RandomState(global_random_seed)
+    X = sparse_container(
+        sp.sparse.random(
+            SPARSE_M,
+            SPARSE_N,
+            random_state=random_state,
+        )
+    )
+    pca_arpack = PCA(n_components=10, svd_solver="arpack").fit(X)
+    pca_auto = PCA(n_components=10, svd_solver="auto").fit(X)
+    assert_allclose(pca_arpack.singular_values_, pca_auto.singular_values_, rtol=5e-3)
+
+
 def test_no_empty_slice_warning():
     # test if we avoid numpy warnings for computing over empty arrays
     n_components = 10
     n_features = n_components + 2  # anything > n_comps triggered it in 0.16
     X = np.random.uniform(-1, 1, size=(n_components, n_features))
     pca = PCA(n_components=n_components)
     with warnings.catch_warnings():
@@ -241,43 +265,162 @@
     assert X_unwhitened.shape == (n_samples, n_components)
 
     # in that case the output components still have varying variances
     assert X_unwhitened.std(axis=0).std() == pytest.approx(74.1, rel=1e-1)
     # we always center, so no test for non-centering.
 
 
-@pytest.mark.parametrize("svd_solver", ["arpack", "randomized"])
-def test_pca_explained_variance_equivalence_solver(svd_solver):
-    rng = np.random.RandomState(0)
-    n_samples, n_features = 100, 80
-    X = rng.randn(n_samples, n_features)
+@pytest.mark.parametrize(
+    "other_svd_solver", sorted(list(set(PCA_SOLVERS) - {"full", "auto"}))
+)
+@pytest.mark.parametrize("data_shape", ["tall", "wide"])
+@pytest.mark.parametrize("rank_deficient", [False, True])
+@pytest.mark.parametrize("whiten", [False, True])
+def test_pca_solver_equivalence(
+    other_svd_solver,
+    data_shape,
+    rank_deficient,
+    whiten,
+    global_random_seed,
+    global_dtype,
+):
+    if data_shape == "tall":
+        n_samples, n_features = 100, 30
+    else:
+        n_samples, n_features = 30, 100
+    n_samples_test = 10
+
+    if rank_deficient:
+        rng = np.random.default_rng(global_random_seed)
+        rank = min(n_samples, n_features) // 2
+        X = rng.standard_normal(
+            size=(n_samples + n_samples_test, rank)
+        ) @ rng.standard_normal(size=(rank, n_features))
+    else:
+        X = make_low_rank_matrix(
+            n_samples=n_samples + n_samples_test,
+            n_features=n_features,
+            tail_strength=0.5,
+            random_state=global_random_seed,
+        )
+        # With a non-zero tail strength, the data is actually full-rank.
+        rank = min(n_samples, n_features)
 
-    pca_full = PCA(n_components=2, svd_solver="full")
-    pca_other = PCA(n_components=2, svd_solver=svd_solver, random_state=0)
+    X = X.astype(global_dtype, copy=False)
+    X_train, X_test = X[:n_samples], X[n_samples:]
 
-    pca_full.fit(X)
-    pca_other.fit(X)
+    if global_dtype == np.float32:
+        tols = dict(atol=1e-2, rtol=1e-5)
+        variance_threshold = 1e-5
+    else:
+        tols = dict(atol=1e-10, rtol=1e-12)
+        variance_threshold = 1e-12
+
+    extra_other_kwargs = {}
+    if other_svd_solver == "randomized":
+        # Only check for a truncated result with a large number of iterations
+        # to make sure that we can recover precise results.
+        n_components = 10
+        extra_other_kwargs = {"iterated_power": 50}
+    elif other_svd_solver == "arpack":
+        # Test all components except the last one which cannot be estimated by
+        # arpack.
+        n_components = np.minimum(n_samples, n_features) - 1
+    else:
+        # Test all components to high precision.
+        n_components = None
 
-    assert_allclose(
-        pca_full.explained_variance_, pca_other.explained_variance_, rtol=5e-2
+    pca_full = PCA(n_components=n_components, svd_solver="full", whiten=whiten)
+    pca_other = PCA(
+        n_components=n_components,
+        svd_solver=other_svd_solver,
+        whiten=whiten,
+        random_state=global_random_seed,
+        **extra_other_kwargs,
     )
+    X_trans_full_train = pca_full.fit_transform(X_train)
+    assert np.isfinite(X_trans_full_train).all()
+    assert X_trans_full_train.dtype == global_dtype
+    X_trans_other_train = pca_other.fit_transform(X_train)
+    assert np.isfinite(X_trans_other_train).all()
+    assert X_trans_other_train.dtype == global_dtype
+
+    assert (pca_full.explained_variance_ >= 0).all()
+    assert_allclose(pca_full.explained_variance_, pca_other.explained_variance_, **tols)
     assert_allclose(
         pca_full.explained_variance_ratio_,
         pca_other.explained_variance_ratio_,
-        rtol=5e-2,
+        **tols,
     )
+    reference_components = pca_full.components_
+    assert np.isfinite(reference_components).all()
+    other_components = pca_other.components_
+    assert np.isfinite(other_components).all()
+
+    # For some choice of n_components and data distribution, some components
+    # might be pure noise, let's ignore them in the comparison:
+    stable = pca_full.explained_variance_ > variance_threshold
+    assert stable.sum() > 1
+    assert_allclose(reference_components[stable], other_components[stable], **tols)
+
+    # As a result the output of fit_transform should be the same:
+    assert_allclose(
+        X_trans_other_train[:, stable], X_trans_full_train[:, stable], **tols
+    )
+
+    # And similarly for the output of transform on new data (except for the
+    # last component that can be underdetermined):
+    X_trans_full_test = pca_full.transform(X_test)
+    assert np.isfinite(X_trans_full_test).all()
+    assert X_trans_full_test.dtype == global_dtype
+    X_trans_other_test = pca_other.transform(X_test)
+    assert np.isfinite(X_trans_other_test).all()
+    assert X_trans_other_test.dtype == global_dtype
+    assert_allclose(X_trans_other_test[:, stable], X_trans_full_test[:, stable], **tols)
+
+    # Check that inverse transform reconstructions for both solvers are
+    # compatible.
+    X_recons_full_test = pca_full.inverse_transform(X_trans_full_test)
+    assert np.isfinite(X_recons_full_test).all()
+    assert X_recons_full_test.dtype == global_dtype
+    X_recons_other_test = pca_other.inverse_transform(X_trans_other_test)
+    assert np.isfinite(X_recons_other_test).all()
+    assert X_recons_other_test.dtype == global_dtype
+
+    if pca_full.components_.shape[0] == pca_full.components_.shape[1]:
+        # In this case, the models should have learned the same invertible
+        # transform. They should therefore both be able to reconstruct the test
+        # data.
+        assert_allclose(X_recons_full_test, X_test, **tols)
+        assert_allclose(X_recons_other_test, X_test, **tols)
+    elif pca_full.components_.shape[0] < rank:
+        # In the absence of noisy components, both models should be able to
+        # reconstruct the same low-rank approximation of the original data.
+        assert pca_full.explained_variance_.min() > variance_threshold
+        assert_allclose(X_recons_full_test, X_recons_other_test, **tols)
+    else:
+        # When n_features > n_samples and n_components is larger than the rank
+        # of the training set, the output of the `inverse_transform` function
+        # is ill-defined. We can only check that we reach the same fixed point
+        # after another round of transform:
+        assert_allclose(
+            pca_full.transform(X_recons_full_test)[:, stable],
+            pca_other.transform(X_recons_other_test)[:, stable],
+            **tols,
+        )
 
 
 @pytest.mark.parametrize(
     "X",
     [
         np.random.RandomState(0).randn(100, 80),
         datasets.make_classification(100, 80, n_informative=78, random_state=0)[0],
+        np.random.RandomState(0).randn(10, 100),
     ],
-    ids=["random-data", "correlated-data"],
+    ids=["random-tall", "correlated-tall", "random-wide"],
 )
 @pytest.mark.parametrize("svd_solver", PCA_SOLVERS)
 def test_pca_explained_variance_empirical(X, svd_solver):
     pca = PCA(n_components=2, svd_solver=svd_solver, random_state=0)
     X_pca = pca.fit_transform(X)
     assert_allclose(pca.explained_variance_, np.var(X_pca, ddof=1, axis=0))
 
@@ -607,31 +750,36 @@
     assert pca.noise_variance_ == 0
     # Non-regression test for gh-12489
     # ensure no divide-by-zero error for n_components == n_samples < n_features
     pca.score(X.T)
 
 
 @pytest.mark.parametrize(
-    "data, n_components, expected_solver",
-    [  # case: n_components in (0,1) => 'full'
-        (np.random.RandomState(0).uniform(size=(1000, 50)), 0.5, "full"),
-        # case: max(X.shape) <= 500 => 'full'
-        (np.random.RandomState(0).uniform(size=(10, 50)), 5, "full"),
+    "n_samples, n_features, n_components, expected_solver",
+    [
+        # case: n_samples < 10 * n_features and max(X.shape) <= 500 => 'full'
+        (10, 50, 5, "full"),
+        # case: n_samples > 10 * n_features and n_features < 500 => 'covariance_eigh'
+        (1000, 50, 50, "covariance_eigh"),
         # case: n_components >= .8 * min(X.shape) => 'full'
-        (np.random.RandomState(0).uniform(size=(1000, 50)), 50, "full"),
+        (1000, 500, 400, "full"),
         # n_components >= 1 and n_components < .8*min(X.shape) => 'randomized'
-        (np.random.RandomState(0).uniform(size=(1000, 50)), 10, "randomized"),
+        (1000, 500, 10, "randomized"),
+        # case: n_components in (0,1) => 'full'
+        (1000, 500, 0.5, "full"),
     ],
 )
-def test_pca_svd_solver_auto(data, n_components, expected_solver):
+def test_pca_svd_solver_auto(n_samples, n_features, n_components, expected_solver):
+    data = np.random.RandomState(0).uniform(size=(n_samples, n_features))
     pca_auto = PCA(n_components=n_components, random_state=0)
     pca_test = PCA(
         n_components=n_components, svd_solver=expected_solver, random_state=0
     )
     pca_auto.fit(data)
+    assert pca_auto._fit_svd_solver == expected_solver
     pca_test.fit(data)
     assert_allclose(pca_auto.components_, pca_test.components_)
 
 
 @pytest.mark.parametrize("svd_solver", PCA_SOLVERS)
 def test_pca_deterministic_output(svd_solver):
     rng = np.random.RandomState(0)
@@ -641,36 +789,41 @@
     for i in range(20):
         pca = PCA(n_components=2, svd_solver=svd_solver, random_state=rng)
         transformed_X[i, :] = pca.fit_transform(X)[0]
     assert_allclose(transformed_X, np.tile(transformed_X[0, :], 20).reshape(20, 2))
 
 
 @pytest.mark.parametrize("svd_solver", PCA_SOLVERS)
-def test_pca_dtype_preservation(svd_solver):
-    check_pca_float_dtype_preservation(svd_solver)
+def test_pca_dtype_preservation(svd_solver, global_random_seed):
+    check_pca_float_dtype_preservation(svd_solver, global_random_seed)
     check_pca_int_dtype_upcast_to_double(svd_solver)
 
 
-def check_pca_float_dtype_preservation(svd_solver):
+def check_pca_float_dtype_preservation(svd_solver, seed):
     # Ensure that PCA does not upscale the dtype when input is float32
-    X_64 = np.random.RandomState(0).rand(1000, 4).astype(np.float64, copy=False)
-    X_32 = X_64.astype(np.float32)
+    X = np.random.RandomState(seed).rand(1000, 4)
+    X_float64 = X.astype(np.float64, copy=False)
+    X_float32 = X.astype(np.float32)
 
-    pca_64 = PCA(n_components=3, svd_solver=svd_solver, random_state=0).fit(X_64)
-    pca_32 = PCA(n_components=3, svd_solver=svd_solver, random_state=0).fit(X_32)
+    pca_64 = PCA(n_components=3, svd_solver=svd_solver, random_state=seed).fit(
+        X_float64
+    )
+    pca_32 = PCA(n_components=3, svd_solver=svd_solver, random_state=seed).fit(
+        X_float32
+    )
 
     assert pca_64.components_.dtype == np.float64
     assert pca_32.components_.dtype == np.float32
-    assert pca_64.transform(X_64).dtype == np.float64
-    assert pca_32.transform(X_32).dtype == np.float32
+    assert pca_64.transform(X_float64).dtype == np.float64
+    assert pca_32.transform(X_float32).dtype == np.float32
 
-    # the rtol is set such that the test passes on all platforms tested on
-    # conda-forge: PR#15775
-    # see: https://github.com/conda-forge/scikit-learn-feedstock/pull/113
-    assert_allclose(pca_64.components_, pca_32.components_, rtol=2e-4)
+    # The atol and rtol are set such that the test passes for all random seeds
+    # on all supported platforms on our CI and conda-forge with the default
+    # random seed.
+    assert_allclose(pca_64.components_, pca_32.components_, rtol=1e-3, atol=1e-3)
 
 
 def check_pca_int_dtype_upcast_to_double(svd_solver):
     # Ensure that all int types will be upcast to float64
     X_i64 = np.random.RandomState(0).randint(0, 1000, (1000, 4))
     X_i64 = X_i64.astype(np.int64, copy=False)
     X_i32 = X_i64.astype(np.int32, copy=False)
@@ -822,32 +975,35 @@
     iris_np = iris.data.astype(dtype_name)
     iris_xp = xp.asarray(iris_np, device=device)
 
     estimator.fit(iris_np)
     precision_np = estimator.get_precision()
     covariance_np = estimator.get_covariance()
 
+    rtol = 2e-4 if iris_np.dtype == "float32" else 2e-7
     with config_context(array_api_dispatch=True):
         estimator_xp = clone(estimator).fit(iris_xp)
         precision_xp = estimator_xp.get_precision()
         assert precision_xp.shape == (4, 4)
         assert precision_xp.dtype == iris_xp.dtype
 
         assert_allclose(
             _convert_to_numpy(precision_xp, xp=xp),
             precision_np,
+            rtol=rtol,
             atol=_atol_for_type(dtype_name),
         )
         covariance_xp = estimator_xp.get_covariance()
         assert covariance_xp.shape == (4, 4)
         assert covariance_xp.dtype == iris_xp.dtype
 
         assert_allclose(
             _convert_to_numpy(covariance_xp, xp=xp),
             covariance_np,
+            rtol=rtol,
             atol=_atol_for_type(dtype_name),
         )
 
 
 @pytest.mark.parametrize(
     "array_namespace, device, dtype_name", yield_namespace_device_dtype_combinations()
 )
@@ -856,15 +1012,18 @@
     [check_array_api_input_and_values, check_array_api_get_precision],
     ids=_get_check_estimator_ids,
 )
 @pytest.mark.parametrize(
     "estimator",
     [
         PCA(n_components=2, svd_solver="full"),
+        PCA(n_components=2, svd_solver="full", whiten=True),
         PCA(n_components=0.1, svd_solver="full", whiten=True),
+        PCA(n_components=2, svd_solver="covariance_eigh"),
+        PCA(n_components=2, svd_solver="covariance_eigh", whiten=True),
         PCA(
             n_components=2,
             svd_solver="randomized",
             power_iteration_normalizer="QR",
             random_state=0,  # how to use global_random_seed here?
         ),
     ],
@@ -953,15 +1112,15 @@
         extra_variance_xp_np = explained_variance_xp_np[min_components:]
         assert all(np.abs(extra_variance_np - reference_variance) < atol)
         assert all(np.abs(extra_variance_xp_np - reference_variance) < atol)
 
 
 def test_array_api_error_and_warnings_on_unsupported_params():
     pytest.importorskip("array_api_compat")
-    xp = pytest.importorskip("numpy.array_api")
+    xp = pytest.importorskip("array_api_strict")
     iris_xp = xp.asarray(iris.data)
 
     pca = PCA(n_components=2, svd_solver="arpack", random_state=0)
     expected_msg = re.escape(
         "PCA with svd_solver='arpack' is not supported for Array API inputs."
     )
     with pytest.raises(ValueError, match=expected_msg):
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_sparse_pca.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_sparse_pca.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 from sklearn.decomposition import PCA, MiniBatchSparsePCA, SparsePCA
 from sklearn.utils import check_random_state
 from sklearn.utils._testing import (
     assert_allclose,
     assert_array_almost_equal,
     if_safe_multiprocessing_with_blas,
 )
+from sklearn.utils.extmath import svd_flip
 
 
 def generate_toy_data(n_components, n_samples, image_size, random_state=None):
     n_features = image_size[0] * image_size[1]
 
     rng = check_random_state(random_state)
     U = rng.randn(n_samples, n_components)
@@ -110,15 +111,18 @@
     rng = np.random.RandomState(0)
     U_init = rng.randn(5, 3)
     V_init = rng.randn(3, 4)
     model = SparsePCA(
         n_components=3, U_init=U_init, V_init=V_init, max_iter=0, random_state=rng
     )
     model.fit(rng.randn(5, 4))
-    assert_allclose(model.components_, V_init / np.linalg.norm(V_init, axis=1)[:, None])
+
+    expected_components = V_init / np.linalg.norm(V_init, axis=1, keepdims=True)
+    expected_components = svd_flip(u=expected_components.T, v=None)[0].T
+    assert_allclose(model.components_, expected_components)
 
 
 def test_mini_batch_correct_shapes():
     rng = np.random.RandomState(0)
     X = rng.randn(12, 10)
     pca = MiniBatchSparsePCA(n_components=8, max_iter=1, random_state=rng)
     U = pca.fit_transform(X)
```

### Comparing `scikit-learn-1.4.2/sklearn/decomposition/tests/test_truncated_svd.py` & `scikit_learn-1.5.0rc1/sklearn/decomposition/tests/test_truncated_svd.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/discriminant_analysis.py` & `scikit_learn-1.5.0rc1/sklearn/discriminant_analysis.py`

 * *Files 1% similar despite different names*

```diff
@@ -189,15 +189,19 @@
     share the same covariance matrix.
 
     The fitted model can also be used to reduce the dimensionality of the input
     by projecting it to the most discriminative directions, using the
     `transform` method.
 
     .. versionadded:: 0.17
-       *LinearDiscriminantAnalysis*.
+
+    For a comparison between
+    :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`
+    and :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`, see
+    :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`.
 
     Read more in the :ref:`User Guide <lda_qda>`.
 
     Parameters
     ----------
     solver : {'svd', 'lsqr', 'eigen'}, default='svd'
         Solver to use, possible values:
@@ -218,24 +222,30 @@
           - None: no shrinkage (default).
           - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.
           - float between 0 and 1: fixed shrinkage parameter.
 
         This should be left to None if `covariance_estimator` is used.
         Note that shrinkage works only with 'lsqr' and 'eigen' solvers.
 
+        For a usage example, see
+        :ref:`sphx_glr_auto_examples_classification_plot_lda.py`.
+
     priors : array-like of shape (n_classes,), default=None
         The class prior probabilities. By default, the class proportions are
         inferred from the training data.
 
     n_components : int, default=None
         Number of components (<= min(n_classes - 1, n_features)) for
         dimensionality reduction. If None, will be set to
         min(n_classes - 1, n_features). This parameter only affects the
         `transform` method.
 
+        For a usage example, see
+        :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`.
+
     store_covariance : bool, default=False
         If True, explicitly compute the weighted within-class covariance
         matrix when solver is 'svd'. The matrix is always computed
         and stored for the other solvers.
 
         .. versionadded:: 0.17
 
@@ -693,15 +703,15 @@
         C : ndarray of shape (n_samples, n_classes)
             Estimated probabilities.
         """
         check_is_fitted(self)
         xp, is_array_api_compliant = get_namespace(X)
         decision = self.decision_function(X)
         if size(self.classes_) == 2:
-            proba = _expit(decision)
+            proba = _expit(decision, xp)
             return xp.stack([1 - proba, proba], axis=1)
         else:
             return softmax(decision)
 
     def predict_log_proba(self, X):
         """Estimate log probability.
 
@@ -761,15 +771,19 @@
     A classifier with a quadratic decision boundary, generated
     by fitting class conditional densities to the data
     and using Bayes' rule.
 
     The model fits a Gaussian density to each class.
 
     .. versionadded:: 0.17
-       *QuadraticDiscriminantAnalysis*
+
+    For a comparison between
+    :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`
+    and :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, see
+    :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`.
 
     Read more in the :ref:`User Guide <lda_qda>`.
 
     Parameters
     ----------
     priors : array-like of shape (n_classes,), default=None
         Class priors. By default, the class proportions are inferred from the
```

### Comparing `scikit-learn-1.4.2/sklearn/dummy.py` & `scikit_learn-1.5.0rc1/sklearn/dummy.py`

 * *Files 5% similar despite different names*

```diff
@@ -106,14 +106,21 @@
     n_classes_ : int or list of int
         Number of label for each output.
 
     class_prior_ : ndarray of shape (n_classes,) or list of such arrays
         Frequency of each class observed in `y`. For multioutput classification
         problems, this is computed independently for each output.
 
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X` has
+        feature names that are all strings.
+
     n_outputs_ : int
         Number of outputs.
 
     sparse_output_ : bool
         True if the array returned from predict is to be in sparse CSC format.
         Is automatically set to True if the input `y` is passed in sparse
         format.
@@ -166,14 +173,16 @@
             Sample weights.
 
         Returns
         -------
         self : object
             Returns the instance itself.
         """
+        self._validate_data(X, cast_to_ndarray=False)
+
         self._strategy = self.strategy
 
         if self._strategy == "uniform" and sp.issparse(y):
             y = y.toarray()
             warnings.warn(
                 (
                     "A local copy of the target data has been converted "
@@ -484,14 +493,21 @@
 
     Attributes
     ----------
     constant_ : ndarray of shape (1, n_outputs)
         Mean or median or quantile of the training targets or constant value
         given by the user.
 
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X` has
+        feature names that are all strings.
+
     n_outputs_ : int
         Number of outputs.
 
     See Also
     --------
     DummyClassifier: Classifier that makes predictions using simple rules.
 
@@ -541,14 +557,16 @@
             Sample weights.
 
         Returns
         -------
         self : object
             Fitted estimator.
         """
+        self._validate_data(X, cast_to_ndarray=False)
+
         y = check_array(y, ensure_2d=False, input_name="y")
         if len(y) == 0:
             raise ValueError("y must not be empty.")
 
         if y.ndim == 1:
             y = np.reshape(y, (-1, 1))
         self.n_outputs_ = y.shape[1]
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 The :mod:`sklearn.ensemble` module includes ensemble-based methods for
 classification, regression and anomaly detection.
 """
+
 from ._bagging import BaggingClassifier, BaggingRegressor
 from ._base import BaseEnsemble
 from ._forest import (
     ExtraTreesClassifier,
     ExtraTreesRegressor,
     RandomForestClassifier,
     RandomForestRegressor,
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_bagging.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_bagging.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,26 +12,42 @@
 from warnings import warn
 
 import numpy as np
 
 from ..base import ClassifierMixin, RegressorMixin, _fit_context
 from ..metrics import accuracy_score, r2_score
 from ..tree import DecisionTreeClassifier, DecisionTreeRegressor
-from ..utils import check_random_state, column_or_1d, indices_to_mask
+from ..utils import (
+    Bunch,
+    _safe_indexing,
+    check_random_state,
+    column_or_1d,
+)
+from ..utils._mask import indices_to_mask
 from ..utils._param_validation import HasMethods, Interval, RealNotInt
 from ..utils._tags import _safe_tags
 from ..utils.metadata_routing import (
-    _raise_for_unsupported_routing,
-    _RoutingNotSupportedMixin,
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    _routing_enabled,
+    get_routing_for_object,
+    process_routing,
 )
 from ..utils.metaestimators import available_if
 from ..utils.multiclass import check_classification_targets
 from ..utils.parallel import Parallel, delayed
 from ..utils.random import sample_without_replacement
-from ..utils.validation import _check_sample_weight, check_is_fitted, has_fit_parameter
+from ..utils.validation import (
+    _check_method_params,
+    _check_sample_weight,
+    _deprecate_positional_args,
+    check_is_fitted,
+    has_fit_parameter,
+)
 from ._base import BaseEnsemble, _partition_estimators
 
 __all__ = ["BaggingClassifier", "BaggingRegressor"]
 
 MAX_INT = np.iinfo(np.int32).max
 
 
@@ -73,38 +89,45 @@
 
 
 def _parallel_build_estimators(
     n_estimators,
     ensemble,
     X,
     y,
-    sample_weight,
     seeds,
     total_n_estimators,
     verbose,
     check_input,
+    fit_params,
 ):
     """Private function used to build a batch of estimators within a job."""
     # Retrieve settings
     n_samples, n_features = X.shape
     max_features = ensemble._max_features
     max_samples = ensemble._max_samples
     bootstrap = ensemble.bootstrap
     bootstrap_features = ensemble.bootstrap_features
-    support_sample_weight = has_fit_parameter(ensemble.estimator_, "sample_weight")
     has_check_input = has_fit_parameter(ensemble.estimator_, "check_input")
     requires_feature_indexing = bootstrap_features or max_features != n_features
 
-    if not support_sample_weight and sample_weight is not None:
-        raise ValueError("The base estimator doesn't support sample weight")
-
     # Build estimators
     estimators = []
     estimators_features = []
 
+    # TODO: (slep6) remove if condition for unrouted sample_weight when metadata
+    # routing can't be disabled.
+    support_sample_weight = has_fit_parameter(ensemble.estimator_, "sample_weight")
+    if not _routing_enabled() and (
+        not support_sample_weight and fit_params.get("sample_weight") is not None
+    ):
+        raise ValueError(
+            "The base estimator doesn't support sample weight, but sample_weight is "
+            "passed to the fit method."
+        )
+
     for i in range(n_estimators):
         if verbose > 1:
             print(
                 "Building estimator %d of %d for this parallel run (total %d)..."
                 % (i + 1, n_estimators, total_n_estimators)
             )
 
@@ -123,33 +146,57 @@
             bootstrap,
             n_features,
             n_samples,
             max_features,
             max_samples,
         )
 
-        # Draw samples, using sample weights, and then fit
-        if support_sample_weight:
-            if sample_weight is None:
-                curr_sample_weight = np.ones((n_samples,))
-            else:
-                curr_sample_weight = sample_weight.copy()
+        fit_params_ = fit_params.copy()
+
+        # TODO(SLEP6): remove if condition for unrouted sample_weight when metadata
+        # routing can't be disabled.
+        # 1. If routing is enabled, we will check if the routing supports sample
+        # weight and use it if it does.
+        # 2. If routing is not enabled, we will check if the base
+        # estimator supports sample_weight and use it if it does.
+
+        # Note: Row sampling can be achieved either through setting sample_weight or
+        # by indexing. The former is more efficient. Therefore, use this method
+        # if possible, otherwise use indexing.
+        if _routing_enabled():
+            request_or_router = get_routing_for_object(ensemble.estimator_)
+            consumes_sample_weight = request_or_router.consumes(
+                "fit", ("sample_weight",)
+            )
+        else:
+            consumes_sample_weight = support_sample_weight
+        if consumes_sample_weight:
+            # Draw sub samples, using sample weights, and then fit
+            curr_sample_weight = _check_sample_weight(
+                fit_params_.pop("sample_weight", None), X
+            ).copy()
 
             if bootstrap:
                 sample_counts = np.bincount(indices, minlength=n_samples)
                 curr_sample_weight *= sample_counts
             else:
                 not_indices_mask = ~indices_to_mask(indices, n_samples)
                 curr_sample_weight[not_indices_mask] = 0
 
+            fit_params_["sample_weight"] = curr_sample_weight
             X_ = X[:, features] if requires_feature_indexing else X
-            estimator_fit(X_, y, sample_weight=curr_sample_weight)
+            estimator_fit(X_, y, **fit_params_)
         else:
-            X_ = X[indices][:, features] if requires_feature_indexing else X[indices]
-            estimator_fit(X_, y[indices])
+            # cannot use sample_weight, so use indexing
+            y_ = _safe_indexing(y, indices)
+            X_ = _safe_indexing(X, indices)
+            fit_params_ = _check_method_params(X, params=fit_params_, indices=indices)
+            if requires_feature_indexing:
+                X_ = X_[:, features]
+            estimator_fit(X_, y_, **fit_params_)
 
         estimators.append(estimator)
         estimators_features.append(features)
 
     return estimators, estimators_features
 
 
@@ -290,19 +337,23 @@
         self.bootstrap_features = bootstrap_features
         self.oob_score = oob_score
         self.warm_start = warm_start
         self.n_jobs = n_jobs
         self.random_state = random_state
         self.verbose = verbose
 
+    # TODO(1.7): remove `sample_weight` from the signature after deprecation
+    # cycle; pop it from `fit_params` before the `_raise_for_params` check and
+    # reinsert later, for backwards compatibility
+    @_deprecate_positional_args(version="1.7")
     @_fit_context(
         # BaseBagging.estimator is not validated yet
         prefer_skip_nested_validation=False
     )
-    def fit(self, X, y, sample_weight=None):
+    def fit(self, X, y, *, sample_weight=None, **fit_params):
         """Build a Bagging ensemble of estimators from the training set (X, y).
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             The training input samples. Sparse matrices are accepted only if
             they are supported by the base estimator.
@@ -312,42 +363,59 @@
             regression).
 
         sample_weight : array-like of shape (n_samples,), default=None
             Sample weights. If None, then samples are equally weighted.
             Note that this is supported only if the base estimator supports
             sample weighting.
 
+        **fit_params : dict
+            Parameters to pass to the underlying estimators.
+
+            .. versionadded:: 1.5
+
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        _raise_for_unsupported_routing(self, "fit", sample_weight=sample_weight)
+        _raise_for_params(fit_params, self, "fit")
+
         # Convert data (X is required to be 2d and indexable)
         X, y = self._validate_data(
             X,
             y,
             accept_sparse=["csr", "csc"],
             dtype=None,
             force_all_finite=False,
             multi_output=True,
         )
-        return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
+
+        if sample_weight is not None:
+            sample_weight = _check_sample_weight(sample_weight, X, dtype=None)
+            fit_params["sample_weight"] = sample_weight
+
+        return self._fit(X, y, max_samples=self.max_samples, **fit_params)
 
     def _parallel_args(self):
         return {}
 
     def _fit(
         self,
         X,
         y,
         max_samples=None,
         max_depth=None,
-        sample_weight=None,
         check_input=True,
+        **fit_params,
     ):
         """Build a Bagging ensemble of estimators from the training
            set (X, y).
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
@@ -361,40 +429,48 @@
         max_samples : int or float, default=None
             Argument to use instead of self.max_samples.
 
         max_depth : int, default=None
             Override value used when constructing base estimator. Only
             supported if the base estimator has a max_depth parameter.
 
-        sample_weight : array-like of shape (n_samples,), default=None
-            Sample weights. If None, then samples are equally weighted.
-            Note that this is supported only if the base estimator supports
-            sample weighting.
-
         check_input : bool, default=True
             Override value used when fitting base estimator. Only supported
             if the base estimator has a check_input parameter for fit function.
+            If the meta-estimator already checks the input, set this value to
+            False to prevent redundant input validation.
+
+        **fit_params : dict, default=None
+            Parameters to pass to the :term:`fit` method of the underlying
+            estimator.
 
         Returns
         -------
         self : object
             Fitted estimator.
         """
         random_state = check_random_state(self.random_state)
 
-        if sample_weight is not None:
-            sample_weight = _check_sample_weight(sample_weight, X, dtype=None)
-
         # Remap output
         n_samples = X.shape[0]
         self._n_samples = n_samples
         y = self._validate_y(y)
 
         # Check parameters
-        self._validate_estimator()
+        self._validate_estimator(self._get_estimator())
+
+        if _routing_enabled():
+            routed_params = process_routing(self, "fit", **fit_params)
+        else:
+            routed_params = Bunch()
+            routed_params.estimator = Bunch(fit=fit_params)
+            if "sample_weight" in fit_params:
+                routed_params.estimator.fit["sample_weight"] = fit_params[
+                    "sample_weight"
+                ]
 
         if max_depth is not None:
             self.estimator_.max_depth = max_depth
 
         # Validate max_samples
         if max_samples is None:
             max_samples = self.max_samples
@@ -470,19 +546,19 @@
             n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args()
         )(
             delayed(_parallel_build_estimators)(
                 n_estimators[i],
                 self,
                 X,
                 y,
-                sample_weight,
                 seeds[starts[i] : starts[i + 1]],
                 total_n_estimators,
                 verbose=self.verbose,
                 check_input=check_input,
+                fit_params=routed_params.estimator.fit,
             )
             for i in range(n_jobs)
         )
 
         # Reduce
         self.estimators_ += list(
             itertools.chain.from_iterable(t[0] for t in all_results)
@@ -533,16 +609,44 @@
 
         Note: the list is re-created at each call to the property in order
         to reduce the object memory footprint by not storing the sampling
         data. Thus fetching the property may be slower than expected.
         """
         return [sample_indices for _, sample_indices in self._get_estimators_indices()]
 
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
+
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__)
+        router.add(
+            estimator=self._get_estimator(),
+            method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+        )
+        return router
+
+    @abstractmethod
+    def _get_estimator(self):
+        """Resolve which estimator to return."""
+
+    def _more_tags(self):
+        return {"allow_nan": _safe_tags(self._get_estimator(), "allow_nan")}
+
 
-class BaggingClassifier(_RoutingNotSupportedMixin, ClassifierMixin, BaseBagging):
+class BaggingClassifier(ClassifierMixin, BaseBagging):
     """A Bagging classifier.
 
     A Bagging classifier is an ensemble meta-estimator that fits base
     classifiers each on random subsets of the original dataset and then
     aggregate their individual predictions (either by voting or by averaging)
     to form a final prediction. Such a meta-estimator can typically be used as
     a way to reduce the variance of a black-box estimator (e.g., a decision
@@ -731,17 +835,19 @@
             oob_score=oob_score,
             warm_start=warm_start,
             n_jobs=n_jobs,
             random_state=random_state,
             verbose=verbose,
         )
 
-    def _validate_estimator(self):
-        """Check the estimator and set the estimator_ attribute."""
-        super()._validate_estimator(default=DecisionTreeClassifier())
+    def _get_estimator(self):
+        """Resolve which estimator to return (default is DecisionTreeClassifier)"""
+        if self.estimator is None:
+            return DecisionTreeClassifier()
+        return self.estimator
 
     def _set_oob_score(self, X, y):
         n_samples = y.shape[0]
         n_classes_ = self.n_classes_
 
         predictions = np.zeros((n_samples, n_classes_))
 
@@ -957,24 +1063,16 @@
         )
 
         # Reduce
         decisions = sum(all_decisions) / self.n_estimators
 
         return decisions
 
-    def _more_tags(self):
-        if self.estimator is None:
-            estimator = DecisionTreeClassifier()
-        else:
-            estimator = self.estimator
-
-        return {"allow_nan": _safe_tags(estimator, "allow_nan")}
-
 
-class BaggingRegressor(_RoutingNotSupportedMixin, RegressorMixin, BaseBagging):
+class BaggingRegressor(RegressorMixin, BaseBagging):
     """A Bagging regressor.
 
     A Bagging regressor is an ensemble meta-estimator that fits base
     regressors each on random subsets of the original dataset and then
     aggregate their individual predictions (either by voting or by averaging)
     to form a final prediction. Such a meta-estimator can typically be used as
     a way to reduce the variance of a black-box estimator (e.g., a decision
@@ -1198,18 +1296,14 @@
         )
 
         # Reduce
         y_hat = sum(all_y_hat) / self.n_estimators
 
         return y_hat
 
-    def _validate_estimator(self):
-        """Check the estimator and set the estimator_ attribute."""
-        super()._validate_estimator(default=DecisionTreeRegressor())
-
     def _set_oob_score(self, X, y):
         n_samples = y.shape[0]
 
         predictions = np.zeros((n_samples,))
         n_predictions = np.zeros((n_samples,))
 
         for estimator, samples, features in zip(
@@ -1230,13 +1324,12 @@
             n_predictions[n_predictions == 0] = 1
 
         predictions /= n_predictions
 
         self.oob_prediction_ = predictions
         self.oob_score_ = r2_score(y, predictions)
 
-    def _more_tags(self):
+    def _get_estimator(self):
+        """Resolve which estimator to return (default is DecisionTreeClassifier)"""
         if self.estimator is None:
-            estimator = DecisionTreeRegressor()
-        else:
-            estimator = self.estimator
-        return {"allow_nan": _safe_tags(estimator, "allow_nan")}
+            return DecisionTreeRegressor()
+        return self.estimator
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_base.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_base.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,38 +6,42 @@
 from abc import ABCMeta, abstractmethod
 from typing import List
 
 import numpy as np
 from joblib import effective_n_jobs
 
 from ..base import BaseEstimator, MetaEstimatorMixin, clone, is_classifier, is_regressor
-from ..utils import Bunch, _print_elapsed_time, check_random_state
+from ..utils import Bunch, check_random_state
 from ..utils._tags import _safe_tags
+from ..utils._user_interface import _print_elapsed_time
+from ..utils.metadata_routing import _routing_enabled
 from ..utils.metaestimators import _BaseComposition
 
 
 def _fit_single_estimator(
-    estimator, X, y, sample_weight=None, message_clsname=None, message=None
+    estimator, X, y, fit_params, message_clsname=None, message=None
 ):
     """Private function used to fit an estimator within a job."""
-    if sample_weight is not None:
+    # TODO(SLEP6): remove if condition for unrouted sample_weight when metadata
+    # routing can't be disabled.
+    if not _routing_enabled() and "sample_weight" in fit_params:
         try:
             with _print_elapsed_time(message_clsname, message):
-                estimator.fit(X, y, sample_weight=sample_weight)
+                estimator.fit(X, y, sample_weight=fit_params["sample_weight"])
         except TypeError as exc:
             if "unexpected keyword argument 'sample_weight'" in str(exc):
                 raise TypeError(
                     "Underlying estimator {} does not support sample weights.".format(
                         estimator.__class__.__name__
                     )
                 ) from exc
             raise
     else:
         with _print_elapsed_time(message_clsname, message):
-            estimator.fit(X, y)
+            estimator.fit(X, y, **fit_params)
     return estimator
 
 
 def _set_random_states(estimator, random_state=None):
     """Set fixed random_state parameters for an estimator.
 
     Finds all parameters ending ``random_state`` and sets them to integers
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_forest.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_forest.py`

 * *Files 0% similar despite different names*

```diff
@@ -851,16 +851,15 @@
         if self.class_weight is not None:
             valid_presets = ("balanced", "balanced_subsample")
             if isinstance(self.class_weight, str):
                 if self.class_weight not in valid_presets:
                     raise ValueError(
                         "Valid presets for class_weight include "
                         '"balanced" and "balanced_subsample".'
-                        'Given "%s".'
-                        % self.class_weight
+                        'Given "%s".' % self.class_weight
                     )
                 if self.warm_start:
                     warn(
                         'class_weight presets "balanced" or '
                         '"balanced_subsample" are '
                         "not recommended for warm_start if the fitted data "
                         "differs from the full dataset. In order to use "
@@ -1131,27 +1130,28 @@
         self.oob_score_ = scoring_function(y, self.oob_prediction_)
 
     def _compute_partial_dependence_recursion(self, grid, target_features):
         """Fast partial dependence computation.
 
         Parameters
         ----------
-        grid : ndarray of shape (n_samples, n_target_features)
+        grid : ndarray of shape (n_samples, n_target_features), dtype=DTYPE
             The grid points on which the partial dependence should be
             evaluated.
-        target_features : ndarray of shape (n_target_features)
+        target_features : ndarray of shape (n_target_features), dtype=np.intp
             The set of target features for which the partial dependence
             should be evaluated.
 
         Returns
         -------
         averaged_predictions : ndarray of shape (n_samples,)
             The value of the partial dependence function on each grid point.
         """
         grid = np.asarray(grid, dtype=DTYPE, order="C")
+        target_features = np.asarray(target_features, dtype=np.intp, order="C")
         averaged_predictions = np.zeros(
             shape=grid.shape[0], dtype=np.float64, order="C"
         )
 
         for tree in self.estimators_:
             # Note: we don't sum in parallel because the GIL isn't released in
             # the fast method.
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_gb.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_gb.py`

 * *Files 0% similar despite different names*

```diff
@@ -736,16 +736,15 @@
                             raise ValueError(msg) from e
                         else:  # regular estimator whose input checking failed
                             raise
                     except ValueError as e:
                         if (
                             "pass parameters to specific steps of "
                             "your pipeline using the "
-                            "stepname__parameter"
-                            in str(e)
+                            "stepname__parameter" in str(e)
                         ):  # pipeline
                             raise ValueError(msg) from e
                         else:  # regular estimator whose input checking failed
                             raise
 
                 raw_predictions = _init_raw_predictions(
                     X_train, self.init_, self._loss, is_classifier(self)
@@ -1038,40 +1037,41 @@
         return avg_feature_importances / np.sum(avg_feature_importances)
 
     def _compute_partial_dependence_recursion(self, grid, target_features):
         """Fast partial dependence computation.
 
         Parameters
         ----------
-        grid : ndarray of shape (n_samples, n_target_features)
+        grid : ndarray of shape (n_samples, n_target_features), dtype=np.float32
             The grid points on which the partial dependence should be
             evaluated.
-        target_features : ndarray of shape (n_target_features,)
+        target_features : ndarray of shape (n_target_features,), dtype=np.intp
             The set of target features for which the partial dependence
             should be evaluated.
 
         Returns
         -------
         averaged_predictions : ndarray of shape \
                 (n_trees_per_iteration_, n_samples)
             The value of the partial dependence function on each grid point.
         """
         if self.init is not None:
             warnings.warn(
                 "Using recursion method with a non-constant init predictor "
                 "will lead to incorrect partial dependence values. "
-                "Got init=%s."
-                % self.init,
+                "Got init=%s." % self.init,
                 UserWarning,
             )
         grid = np.asarray(grid, dtype=DTYPE, order="C")
         n_estimators, n_trees_per_stage = self.estimators_.shape
         averaged_predictions = np.zeros(
             (n_trees_per_stage, grid.shape[0]), dtype=np.float64, order="C"
         )
+        target_features = np.asarray(target_features, dtype=np.intp, order="C")
+
         for stage in range(n_estimators):
             for k in range(n_trees_per_stage):
                 tree = self.estimators_[stage, k].tree_
                 tree.compute_partial_dependence(
                     grid, target_features, averaged_predictions[k]
                 )
         averaged_predictions *= self.learning_rate
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_gradient_boosting.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_gradient_boosting.pyx`

 * *Files 3% similar despite different names*

```diff
@@ -2,20 +2,19 @@
 #
 # License: BSD 3 clause
 
 from libc.stdlib cimport free
 from libc.string cimport memset
 
 import numpy as np
-cimport numpy as cnp
-cnp.import_array()
-
 from scipy.sparse import issparse
 
-from ..utils._typedefs cimport float32_t, float64_t, intp_t, int32_t
+from ..utils._typedefs cimport float32_t, float64_t, intp_t, int32_t, uint8_t
+# Note: _tree uses cimport numpy, cnp.import_array, so we need to include
+# numpy headers, see setup.py.
 from ..tree._tree cimport Node
 from ..tree._tree cimport Tree
 from ..tree._utils cimport safe_realloc
 
 
 # no namespace lookup for numpy dtype and array creation
 from numpy import zeros as np_zeros
@@ -26,15 +25,15 @@
 
 cdef void _predict_regression_tree_inplace_fast_dense(
     const float32_t[:, ::1] X,
     Node* root_node,
     double *value,
     double scale,
     Py_ssize_t k,
-    cnp.float64_t[:, :] out
+    float64_t[:, :] out
 ) noexcept nogil:
     """Predicts output for regression tree and stores it in ``out[i, k]``.
 
     This function operates directly on the data arrays of the tree
     data structures. This is 5x faster than the variant above because
     it allows us to avoid buffer validation.
 
@@ -75,15 +74,15 @@
         out[i, k] += scale * value[node - root_node]
 
 
 def _predict_regression_tree_stages_sparse(
     object[:, :] estimators,
     object X,
     double scale,
-    cnp.float64_t[:, :] out
+    float64_t[:, :] out
 ):
     """Predicts output for regression tree inplace and adds scaled value to ``out[i, k]``.
 
     The function assumes that the ndarray that wraps ``X`` is csr_matrix.
     """
     cdef const float32_t[::1] X_data = X.data
     cdef const int32_t[::1] X_indices = X.indices
@@ -163,15 +162,15 @@
     free(values)
 
 
 def predict_stages(
     object[:, :] estimators,
     object X,
     double scale,
-    cnp.float64_t[:, :] out
+    float64_t[:, :] out
 ):
     """Add predictions of ``estimators`` to ``out``.
 
     Each estimator is scaled by ``scale`` before its prediction
     is added to ``out``.
     """
     cdef Py_ssize_t i
@@ -210,29 +209,29 @@
 
 
 def predict_stage(
     object[:, :] estimators,
     int stage,
     object X,
     double scale,
-    cnp.float64_t[:, :] out
+    float64_t[:, :] out
 ):
     """Add predictions of ``estimators[stage]`` to ``out``.
 
     Each estimator in the stage is scaled by ``scale`` before
     its prediction is added to ``out``.
     """
     return predict_stages(
         estimators=estimators[stage:stage + 1], X=X, scale=scale, out=out
     )
 
 
 def _random_sample_mask(
-    cnp.npy_intp n_total_samples,
-    cnp.npy_intp n_total_in_bag,
+    intp_t n_total_samples,
+    intp_t n_total_in_bag,
     random_state
 ):
     """Create a random sample mask where ``n_total_in_bag`` elements are set.
 
     Parameters
     ----------
     n_total_samples : int
@@ -246,19 +245,19 @@
 
     Returns
     -------
     sample_mask : np.ndarray, shape=[n_total_samples]
         An ndarray where ``n_total_in_bag`` elements are set to ``True``
         the others are ``False``.
     """
-    cdef cnp.float64_t[::1] rand = random_state.uniform(size=n_total_samples)
-    cdef cnp.uint8_t[::1] sample_mask = np_zeros((n_total_samples,), dtype=bool)
+    cdef float64_t[::1] rand = random_state.uniform(size=n_total_samples)
+    cdef uint8_t[::1] sample_mask = np_zeros((n_total_samples,), dtype=bool)
 
-    cdef cnp.npy_intp n_bagged = 0
-    cdef cnp.npy_intp i = 0
+    cdef intp_t n_bagged = 0
+    cdef intp_t i = 0
 
     for i in range(n_total_samples):
         if rand[i] * (n_total_samples - i) < (n_total_in_bag - n_bagged):
             sample_mask[i] = 1
             n_bagged += 1
 
     return sample_mask.base
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_binning.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_binning.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_bitset.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_bitset.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 # Author: Nicolas Hug
 
 from cython.parallel import prange
 from libc.math cimport isnan
 import numpy as np
 
+from ...utils._typedefs cimport intp_t
 from .common cimport X_DTYPE_C
 from .common cimport Y_DTYPE_C
 from .common import Y_DTYPE
 from .common cimport X_BINNED_DTYPE_C
 from .common cimport BITSET_INNER_DTYPE_C
 from .common cimport node_struct
 from ._bitset cimport in_bitset_2d_memoryview
-from sklearn.utils._typedefs cimport intp_t
 
 
 def _predict_from_raw_data(  # raw data = non-binned data
         const node_struct [:] nodes,
         const X_DTYPE_C [:, :] numeric_data,
         const BITSET_INNER_DTYPE_C [:, ::1] raw_left_cat_bitsets,
         const BITSET_INNER_DTYPE_C [:, ::1] known_cat_bitsets,
@@ -144,15 +144,15 @@
                 node_idx = node.right
         node = nodes[node_idx]
 
 
 def _compute_partial_dependence(
     node_struct [:] nodes,
     const X_DTYPE_C [:, ::1] X,
-    int [:] target_features,
+    const intp_t [:] target_features,
     Y_DTYPE_C [:] out
 ):
     """Partial dependence of the response on the ``target_features`` set.
 
     For each sample in ``X`` a tree traversal is performed.
     Each traversal starts from the root with weight 1.0.
 
@@ -169,15 +169,15 @@
     Parameters
     ----------
     nodes : view on array of PREDICTOR_RECORD_DTYPE, shape (n_nodes)
         The array representing the predictor tree.
     X : view on 2d ndarray, shape (n_samples, n_target_features)
         The grid points on which the partial dependence should be
         evaluated.
-    target_features : view on 1d ndarray, shape (n_target_features)
+    target_features : view on 1d ndarray of intp_t, shape (n_target_features)
         The set of target features for which the partial dependence
         should be evaluated.
     out : view on 1d ndarray, shape (n_samples)
         The value of the partial dependence function on each grid
         point.
     """
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/binning.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/binning.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """
 This module contains the BinMapper class.
 
 BinMapper is used for mapping a real-valued dataset into integer-valued bins.
 Bin thresholds are computed with the quantiles so that each bin contains
 approximately the same number of samples.
 """
+
 # Author: Nicolas Hug
 
 import numpy as np
 
 from ...base import BaseEstimator, TransformerMixin
 from ...utils import check_array, check_random_state
 from ...utils._openmp_helpers import _openmp_effective_n_threads
@@ -41,22 +42,23 @@
         A given value x will be mapped into bin value i iff
         bining_thresholds[i - 1] < x <= binning_thresholds[i]
     """
     # ignore missing values when computing bin thresholds
     missing_mask = np.isnan(col_data)
     if missing_mask.any():
         col_data = col_data[~missing_mask]
-    col_data = np.ascontiguousarray(col_data, dtype=X_DTYPE)
-    distinct_values = np.unique(col_data)
+    # The data will be sorted anyway in np.unique and again in percentile, so we do it
+    # here. Sorting also returns a contiguous array.
+    col_data = np.sort(col_data)
+    distinct_values = np.unique(col_data).astype(X_DTYPE)
     if len(distinct_values) <= max_bins:
         midpoints = distinct_values[:-1] + distinct_values[1:]
         midpoints *= 0.5
     else:
-        # We sort again the data in this case. We could compute
-        # approximate midpoint percentiles using the output of
+        # We could compute approximate midpoint percentiles using the output of
         # np.unique(col_data, return_counts) instead but this is more
         # work and the performance benefit will be limited because we
         # work on a fixed-size subsample of the full data.
         percentiles = np.linspace(0, 100, num=max_bins + 1)
         percentiles = percentiles[1:-1]
         midpoints = percentile(col_data, percentiles, method="midpoint").astype(X_DTYPE)
         assert midpoints.shape[0] == max_bins - 1
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/common.pxd` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/common.pxd`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,18 @@
-cimport numpy as cnp
-from sklearn.utils._typedefs cimport intp_t
+from ...utils._typedefs cimport float32_t, float64_t, intp_t, uint8_t, uint32_t
 
-cnp.import_array()
 
-
-ctypedef cnp.npy_float64 X_DTYPE_C
-ctypedef cnp.npy_uint8 X_BINNED_DTYPE_C
-ctypedef cnp.npy_float64 Y_DTYPE_C
-ctypedef cnp.npy_float32 G_H_DTYPE_C
-ctypedef cnp.npy_uint32 BITSET_INNER_DTYPE_C
+ctypedef float64_t X_DTYPE_C
+ctypedef uint8_t X_BINNED_DTYPE_C
+ctypedef float64_t Y_DTYPE_C
+ctypedef float32_t G_H_DTYPE_C
+ctypedef uint32_t BITSET_INNER_DTYPE_C
 ctypedef BITSET_INNER_DTYPE_C[8] BITSET_DTYPE_C
 
+
 cdef packed struct hist_struct:
     # Same as histogram dtype but we need a struct to declare views. It needs
     # to be packed since by default numpy dtypes aren't aligned
     Y_DTYPE_C sum_gradients
     Y_DTYPE_C sum_hessians
     unsigned int count
 
@@ -34,11 +32,12 @@
     unsigned char is_leaf
     X_BINNED_DTYPE_C bin_threshold
     unsigned char is_categorical
     # The index of the corresponding bitsets in the Predictor's bitset arrays.
     # Only used if is_categorical is True
     unsigned int bitset_idx
 
+
 cpdef enum MonotonicConstraint:
     NO_CST = 0
     POS = 1
     NEG = -1
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/common.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/common.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,16 @@
     is_classifier,
 )
 from ...compose import ColumnTransformer
 from ...metrics import check_scoring
 from ...metrics._scorer import _SCORERS
 from ...model_selection import train_test_split
 from ...preprocessing import FunctionTransformer, LabelEncoder, OrdinalEncoder
-from ...utils import check_random_state, compute_sample_weight, is_scalar_nan, resample
+from ...utils import check_random_state, compute_sample_weight, resample
+from ...utils._missing import is_scalar_nan
 from ...utils._openmp_helpers import _openmp_effective_n_threads
 from ...utils._param_validation import Hidden, Interval, RealNotInt, StrOptions
 from ...utils.multiclass import check_classification_targets
 from ...utils.validation import (
     _check_monotonic_cst,
     _check_sample_weight,
     _check_y,
@@ -578,14 +579,25 @@
         if not self.warm_start or not self._is_fitted():
             self._random_seed = rng.randint(np.iinfo(np.uint32).max, dtype="u8")
             feature_subsample_seed = rng.randint(np.iinfo(np.uint32).max, dtype="u8")
             self._feature_subsample_rng = np.random.default_rng(feature_subsample_seed)
 
         self._validate_parameters()
         monotonic_cst = _check_monotonic_cst(self, self.monotonic_cst)
+        # _preprocess_X places the categorical features at the beginning,
+        # change the order of monotonic_cst accordingly
+        if self.is_categorical_ is not None:
+            monotonic_cst_remapped = np.concatenate(
+                (
+                    monotonic_cst[self.is_categorical_],
+                    monotonic_cst[~self.is_categorical_],
+                )
+            )
+        else:
+            monotonic_cst_remapped = monotonic_cst
 
         # used for validation in predict
         n_samples, self._n_features = X.shape
 
         # Encode constraints into a list of sets of features indices (integers).
         interaction_cst = self._check_interaction_cst(self._n_features)
 
@@ -890,15 +902,15 @@
                     X_binned=X_binned_train,
                     gradients=g_view[:, k],
                     hessians=h_view[:, k],
                     n_bins=n_bins,
                     n_bins_non_missing=self._bin_mapper.n_bins_non_missing_,
                     has_missing_values=has_missing_values,
                     is_categorical=self._is_categorical_remapped,
-                    monotonic_cst=monotonic_cst,
+                    monotonic_cst=monotonic_cst_remapped,
                     interaction_cst=interaction_cst,
                     max_leaf_nodes=self.max_leaf_nodes,
                     max_depth=self.max_depth,
                     min_samples_leaf=self.min_samples_leaf,
                     l2_regularization=self.l2_regularization,
                     feature_fraction_per_split=self.max_features,
                     rng=self._feature_subsample_rng,
@@ -1355,18 +1367,18 @@
             yield raw_predictions.copy()
 
     def _compute_partial_dependence_recursion(self, grid, target_features):
         """Fast partial dependence computation.
 
         Parameters
         ----------
-        grid : ndarray, shape (n_samples, n_target_features)
+        grid : ndarray, shape (n_samples, n_target_features), dtype=np.float32
             The grid points on which the partial dependence should be
             evaluated.
-        target_features : ndarray, shape (n_target_features)
+        target_features : ndarray, shape (n_target_features), dtype=np.intp
             The set of target features for which the partial dependence
             should be evaluated.
 
         Returns
         -------
         averaged_predictions : ndarray, shape \
                 (n_trees_per_iteration, n_samples)
@@ -1381,14 +1393,15 @@
                 "time.".format(self.__class__.__name__)
             )
 
         grid = np.asarray(grid, dtype=X_DTYPE, order="C")
         averaged_predictions = np.zeros(
             (self.n_trees_per_iteration_, grid.shape[0]), dtype=Y_DTYPE
         )
+        target_features = np.asarray(target_features, dtype=np.intp, order="C")
 
         for predictors_of_ith_iteration in self._predictors:
             for k, predictor in enumerate(predictors_of_ith_iteration):
                 predictor.compute_partial_dependence(
                     grid, target_features, averaged_predictions[k]
                 )
         # Note that the learning rate is already accounted for in the leaves
@@ -1424,14 +1437,16 @@
     This estimator has native support for missing values (NaNs). During
     training, the tree grower learns at each split point whether samples
     with missing values should go to the left or right child, based on the
     potential gain. When predicting, samples with missing values are
     assigned to the left or right child consequently. If no missing values
     were encountered for a given feature during training, then samples with
     missing values are mapped to whichever child has the most samples.
+    See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for a
+    usecase example of this feature.
 
     This implementation is inspired by
     `LightGBM <https://github.com/Microsoft/LightGBM>`_.
 
     Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.
 
     .. versionadded:: 0.21
@@ -1475,15 +1490,16 @@
         edges to go from the root to the deepest leaf.
         Depth isn't constrained by default.
     min_samples_leaf : int, default=20
         The minimum number of samples per leaf. For small datasets with less
         than a few hundred samples, it is recommended to lower this value
         since only very shallow trees would be built.
     l2_regularization : float, default=0
-        The L2 regularization parameter. Use ``0`` for no regularization (default).
+        The L2 regularization parameter penalizing leaves with small hessians.
+        Use ``0`` for no regularization (default).
     max_features : float, default=1.0
         Proportion of randomly chosen features in each and every node split.
         This is a form of regularization, smaller values make the trees weaker
         learners and might prevent overfitting.
         If interaction constraints from `interaction_cst` are present, only allowed
         features are taken into account for the subsampling.
 
@@ -1851,15 +1867,16 @@
         edges to go from the root to the deepest leaf.
         Depth isn't constrained by default.
     min_samples_leaf : int, default=20
         The minimum number of samples per leaf. For small datasets with less
         than a few hundred samples, it is recommended to lower this value
         since only very shallow trees would be built.
     l2_regularization : float, default=0
-        The L2 regularization parameter. Use ``0`` for no regularization (default).
+        The L2 regularization parameter penalizing leaves with small hessians.
+        Use ``0`` for no regularization (default).
     max_features : float, default=1.0
         Proportion of randomly chosen features in each and every node split.
         This is a form of regularization, smaller values make the trees weaker
         learners and might prevent overfitting.
         If interaction constraints from `interaction_cst` are present, only allowed
         features are taken into account for the subsampling.
 
@@ -2135,15 +2152,21 @@
 
         Returns
         -------
         y : ndarray, shape (n_samples,)
             The predicted classes.
         """
         # TODO: This could be done in parallel
-        encoded_classes = np.argmax(self.predict_proba(X), axis=1)
+        raw_predictions = self._raw_predict(X)
+        if raw_predictions.shape[1] == 1:
+            # np.argmax([0.5, 0.5]) is 0, not 1. Therefore "> 0" not ">= 0" to be
+            # consistent with the multiclass case.
+            encoded_classes = (raw_predictions.ravel() > 0).astype(int)
+        else:
+            encoded_classes = np.argmax(raw_predictions, axis=1)
         return self.classes_[encoded_classes]
 
     def staged_predict(self, X):
         """Predict classes at each iteration.
 
         This method allows monitoring (i.e. determine error on testing set)
         after each stage.
@@ -2156,16 +2179,20 @@
             The input samples.
 
         Yields
         ------
         y : generator of ndarray of shape (n_samples,)
             The predicted classes of the input samples, for each iteration.
         """
-        for proba in self.staged_predict_proba(X):
-            encoded_classes = np.argmax(proba, axis=1)
+        for raw_predictions in self._staged_raw_predict(X):
+            if raw_predictions.shape[1] == 1:
+                # np.argmax([0, 0]) is 0, not 1, therefor "> 0" not ">= 0"
+                encoded_classes = (raw_predictions.ravel() > 0).astype(int)
+            else:
+                encoded_classes = np.argmax(raw_predictions, axis=1)
             yield self.classes_.take(encoded_classes, axis=0)
 
     def predict_proba(self, X):
         """Predict class probabilities for X.
 
         Parameters
         ----------
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/grower.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/grower.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,50 +1,52 @@
 """
 This module contains the TreeGrower class.
 
 TreeGrower builds a regression tree fitting a Newton-Raphson step, based on
 the gradients and hessians of the training data.
 """
+
 # Author: Nicolas Hug
 
 import numbers
 from heapq import heappop, heappush
 from timeit import default_timer as time
 
 import numpy as np
 
 from sklearn.utils._openmp_helpers import _openmp_effective_n_threads
 
+from ...utils.arrayfuncs import sum_parallel
 from ._bitset import set_raw_bitset_from_binned_bitset
 from .common import (
     PREDICTOR_RECORD_DTYPE,
     X_BITSET_INNER_DTYPE,
-    Y_DTYPE,
     MonotonicConstraint,
 )
 from .histogram import HistogramBuilder
 from .predictor import TreePredictor
 from .splitting import Splitter
-from .utils import sum_parallel
-
-EPS = np.finfo(Y_DTYPE).eps  # to avoid zero division errors
 
 
 class TreeNode:
     """Tree Node class used in TreeGrower.
 
     This isn't used for prediction purposes, only for training (see
     TreePredictor).
 
     Parameters
     ----------
     depth : int
         The depth of the node, i.e. its distance from the root.
     sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint32
         The indices of the samples at the node.
+    partition_start : int
+        start position of the node's sample_indices in splitter.partition.
+    partition_stop : int
+        stop position of the node's sample_indices in splitter.partition.
     sum_gradients : float
         The sum of the gradients of the samples at the node.
     sum_hessians : float
         The sum of the hessians of the samples at the node.
 
     Attributes
     ----------
@@ -77,41 +79,49 @@
         Indices of the interaction sets that have to be applied on splits of
         child nodes. The fewer sets the stronger the constraint as fewer sets
         contain fewer features.
     children_lower_bound : float
     children_upper_bound : float
     """
 
-    split_info = None
-    left_child = None
-    right_child = None
-    histograms = None
-
-    # start and stop indices of the node in the splitter.partition
-    # array. Concretely,
-    # self.sample_indices = view(self.splitter.partition[start:stop])
-    # Please see the comments about splitter.partition and
-    # splitter.split_indices for more info about this design.
-    # These 2 attributes are only used in _update_raw_prediction, because we
-    # need to iterate over the leaves and I don't know how to efficiently
-    # store the sample_indices views because they're all of different sizes.
-    partition_start = 0
-    partition_stop = 0
-
-    def __init__(self, depth, sample_indices, sum_gradients, sum_hessians, value=None):
+    def __init__(
+        self,
+        *,
+        depth,
+        sample_indices,
+        partition_start,
+        partition_stop,
+        sum_gradients,
+        sum_hessians,
+        value=None,
+    ):
         self.depth = depth
         self.sample_indices = sample_indices
         self.n_samples = sample_indices.shape[0]
         self.sum_gradients = sum_gradients
         self.sum_hessians = sum_hessians
         self.value = value
         self.is_leaf = False
         self.allowed_features = None
         self.interaction_cst_indices = None
         self.set_children_bounds(float("-inf"), float("+inf"))
+        self.split_info = None
+        self.left_child = None
+        self.right_child = None
+        self.histograms = None
+        # start and stop indices of the node in the splitter.partition
+        # array. Concretely,
+        # self.sample_indices = view(self.splitter.partition[start:stop])
+        # Please see the comments about splitter.partition and
+        # splitter.split_indices for more info about this design.
+        # These 2 attributes are only used in _update_raw_prediction, because we
+        # need to iterate over the leaves and I don't know how to efficiently
+        # store the sample_indices views because they're all of different sizes.
+        self.partition_start = partition_start
+        self.partition_stop = partition_stop
 
     def set_children_bounds(self, lower, upper):
         """Set children values bounds to respect monotonic constraints."""
 
         # These are bounds for the node's *children* values, not the node's
         # value. The bounds are used in the splitter when considering potential
         # left and right child.
@@ -188,15 +198,16 @@
           - 0: no constraint
           - -1: monotonic decrease
 
         Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.
     interaction_cst : list of sets of integers, default=None
         List of interaction constraints.
     l2_regularization : float, default=0.
-        The L2 regularization parameter.
+        The L2 regularization parameter penalizing leaves with small hessians.
+        Use ``0`` for no regularization (default).
     feature_fraction_per_split : float, default=1
         Proportion of randomly chosen features in each and every node split.
         This is a form of regularization, smaller values make the trees weaker
         learners and might prevent overfitting.
     rng : Generator
         Numpy random Generator used for feature subsampling.
     shrinkage : float, default=1.
@@ -337,15 +348,15 @@
         self.n_threads = n_threads
         self.splittable_nodes = []
         self.finalized_leaves = []
         self.total_find_split_time = 0.0  # time spent finding the best splits
         self.total_compute_hist_time = 0.0  # time spent computing histograms
         self.total_apply_split_time = 0.0  # time spent splitting nodes
         self.n_categorical_splits = 0
-        self._intilialize_root(gradients, hessians, hessians_are_constant)
+        self._initialize_root(gradients, hessians)
         self.n_nodes = 1
 
     def _validate_parameters(
         self,
         X_binned,
         min_gain_to_split,
         min_hessian_to_split,
@@ -385,34 +396,33 @@
         would be shrunk but its sibling would potentially not be (if it's a
         non-leaf), which would lead to a wrong computation of the 'middle'
         value needed to enforce the monotonic constraints.
         """
         for leaf in self.finalized_leaves:
             leaf.value *= self.shrinkage
 
-    def _intilialize_root(self, gradients, hessians, hessians_are_constant):
+    def _initialize_root(self, gradients, hessians):
         """Initialize root node and finalize it if needed."""
         n_samples = self.X_binned.shape[0]
         depth = 0
         sum_gradients = sum_parallel(gradients, self.n_threads)
         if self.histogram_builder.hessians_are_constant:
             sum_hessians = hessians[0] * n_samples
         else:
             sum_hessians = sum_parallel(hessians, self.n_threads)
         self.root = TreeNode(
             depth=depth,
             sample_indices=self.splitter.partition,
+            partition_start=0,
+            partition_stop=n_samples,
             sum_gradients=sum_gradients,
             sum_hessians=sum_hessians,
             value=0,
         )
 
-        self.root.partition_start = 0
-        self.root.partition_stop = n_samples
-
         if self.root.n_samples < 2 * self.min_samples_leaf:
             # Do not even bother computing any splitting statistics.
             self._finalize_leaf(self.root)
             return
         if sum_hessians < self.splitter.min_hessian_to_split:
             self._finalize_leaf(self.root)
             return
@@ -481,37 +491,35 @@
         self.total_apply_split_time += time() - tic
 
         depth = node.depth + 1
         n_leaf_nodes = len(self.finalized_leaves) + len(self.splittable_nodes)
         n_leaf_nodes += 2
 
         left_child_node = TreeNode(
-            depth,
-            sample_indices_left,
-            node.split_info.sum_gradient_left,
-            node.split_info.sum_hessian_left,
+            depth=depth,
+            sample_indices=sample_indices_left,
+            partition_start=node.partition_start,
+            partition_stop=node.partition_start + right_child_pos,
+            sum_gradients=node.split_info.sum_gradient_left,
+            sum_hessians=node.split_info.sum_hessian_left,
             value=node.split_info.value_left,
         )
         right_child_node = TreeNode(
-            depth,
-            sample_indices_right,
-            node.split_info.sum_gradient_right,
-            node.split_info.sum_hessian_right,
+            depth=depth,
+            sample_indices=sample_indices_right,
+            partition_start=left_child_node.partition_stop,
+            partition_stop=node.partition_stop,
+            sum_gradients=node.split_info.sum_gradient_right,
+            sum_hessians=node.split_info.sum_hessian_right,
             value=node.split_info.value_right,
         )
 
         node.right_child = right_child_node
         node.left_child = left_child_node
 
-        # set start and stop indices
-        left_child_node.partition_start = node.partition_start
-        left_child_node.partition_stop = node.partition_start + right_child_pos
-        right_child_node.partition_start = left_child_node.partition_stop
-        right_child_node.partition_stop = node.partition_stop
-
         # set interaction constraints (the indices of the constraints sets)
         if self.interaction_cst is not None:
             # Calculate allowed_features and interaction_cst_indices only once. Child
             # nodes inherit them before they get split.
             (
                 left_child_node.allowed_features,
                 left_child_node.interaction_cst_indices,
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/histogram.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/histogram.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """This module contains routines for building histograms."""
 
 # Author: Nicolas Hug
 
 cimport cython
 from cython.parallel import prange
+from libc.string cimport memset
 
 import numpy as np
 
 from .common import HISTOGRAM_DTYPE
 from .common cimport hist_struct
 from .common cimport X_BINNED_DTYPE_C
 from .common cimport G_H_DTYPE_C
@@ -192,20 +193,17 @@
             unsigned int root_node = X_binned.shape[0] == n_samples
             G_H_DTYPE_C [::1] ordered_gradients = \
                 self.ordered_gradients[:n_samples]
             G_H_DTYPE_C [::1] ordered_hessians = \
                 self.ordered_hessians[:n_samples]
             unsigned char hessians_are_constant = \
                 self.hessians_are_constant
-            unsigned int bin_idx = 0
 
-        for bin_idx in range(self.n_bins):
-            histograms[feature_idx, bin_idx].sum_gradients = 0.
-            histograms[feature_idx, bin_idx].sum_hessians = 0.
-            histograms[feature_idx, bin_idx].count = 0
+        # Set histograms to zero.
+        memset(&histograms[feature_idx, 0], 0, self.n_bins * sizeof(hist_struct))
 
         if root_node:
             if hessians_are_constant:
                 _build_histogram_root_no_hessian(feature_idx, X_binned,
                                                  ordered_gradients,
                                                  histograms)
             else:
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/meson.build` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/meson.build`

 * *Files 14% similar despite different names*

```diff
@@ -2,20 +2,19 @@
   '_gradient_boosting': {'sources': ['_gradient_boosting.pyx']},
   'histogram': {'sources': ['histogram.pyx']},
   'splitting': {'sources': ['splitting.pyx']},
   '_binning': {'sources': ['_binning.pyx']},
   '_predictor': {'sources': ['_predictor.pyx']},
   '_bitset': {'sources': ['_bitset.pyx']},
   'common': {'sources': ['common.pyx']},
-  'utils': {'sources': ['utils.pyx']},
 }
 
 foreach ext_name, ext_dict : hist_gradient_boosting_extension_metadata
   py.extension_module(
     ext_name,
     ext_dict.get('sources'),
-    dependencies: [np_dep, openmp_dep],
+    dependencies: [openmp_dep],
     cython_args: cython_args,
     subdir: 'sklearn/ensemble/_hist_gradient_boosting',
     install: true
   )
 endforeach
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/predictor.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/predictor.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This module contains the TreePredictor class which is used for prediction.
 """
+
 # Author: Nicolas Hug
 
 import numpy as np
 
 from ._predictor import (
     _compute_partial_dependence,
     _predict_from_binned_data,
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/splitting.pyx` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/splitting.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -5,32 +5,30 @@
 - Apply a split to a node, i.e. split the indices of the samples at the node
   into the newly created left and right children.
 """
 # Author: Nicolas Hug
 
 cimport cython
 from cython.parallel import prange
-cimport numpy as cnp
 import numpy as np
 from libc.math cimport INFINITY, ceil
 from libc.stdlib cimport malloc, free, qsort
 from libc.string cimport memcpy
 
+from ...utils._typedefs cimport uint8_t
 from .common cimport X_BINNED_DTYPE_C
 from .common cimport Y_DTYPE_C
 from .common cimport hist_struct
 from .common cimport BITSET_INNER_DTYPE_C
 from .common cimport BITSET_DTYPE_C
 from .common cimport MonotonicConstraint
 from ._bitset cimport init_bitset
 from ._bitset cimport set_bitset
 from ._bitset cimport in_bitset
 
-cnp.import_array()
-
 
 cdef struct split_info_struct:
     # Same as the SplitInfo class, but we need a C struct to use it in the
     # nogil sections and to use in arrays.
     Y_DTYPE_C gain
     int feature_idx
     unsigned int bin_idx
@@ -486,15 +484,15 @@
             split_info_struct * split_infos
             const unsigned char [::1] has_missing_values = self.has_missing_values
             const unsigned char [::1] is_categorical = self.is_categorical
             const signed char [::1] monotonic_cst = self.monotonic_cst
             int n_threads = self.n_threads
             bint has_interaction_cst = False
             Y_DTYPE_C feature_fraction_per_split = self.feature_fraction_per_split
-            cnp.npy_bool [:] subsample_mask
+            uint8_t [:] subsample_mask  # same as npy_bool
             int n_subsampled_features
 
         has_interaction_cst = allowed_features is not None
         if has_interaction_cst:
             n_allowed_features = allowed_features.shape[0]
         else:
             n_allowed_features = self.n_features
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,17 +31,18 @@
 from sklearn.ensemble._hist_gradient_boosting.grower import TreeGrower
 from sklearn.ensemble._hist_gradient_boosting.predictor import TreePredictor
 from sklearn.exceptions import NotFittedError
 from sklearn.metrics import get_scorer, mean_gamma_deviance, mean_poisson_deviance
 from sklearn.model_selection import cross_val_score, train_test_split
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler, OneHotEncoder
-from sklearn.utils import _IS_32BIT, shuffle
+from sklearn.utils import shuffle
 from sklearn.utils._openmp_helpers import _openmp_effective_n_threads
 from sklearn.utils._testing import _convert_container
+from sklearn.utils.fixes import _IS_32BIT
 
 n_threads = _openmp_effective_n_threads()
 
 X_classification, y_classification = make_classification(random_state=0)
 X_regression, y_regression = make_regression(random_state=0)
 X_multi_classification, y_multi_classification = make_classification(
     n_classes=3, n_informative=3, random_state=0
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py`

 * *Files 4% similar despite different names*

```diff
@@ -202,39 +202,50 @@
     assert_children_values_monotonic(predictor, monotonic_cst)
     assert_children_values_bounded(grower, monotonic_cst)
     assert_leaves_values_monotonic(predictor, monotonic_cst)
 
 
 @pytest.mark.parametrize("use_feature_names", (True, False))
 def test_predictions(global_random_seed, use_feature_names):
-    # Train a model with a POS constraint on the first feature and a NEG
-    # constraint on the second feature, and make sure the constraints are
-    # respected by checking the predictions.
+    # Train a model with a POS constraint on the first non-categorical feature
+    # and a NEG constraint on the second non-categorical feature, and make sure
+    # the constraints are respected by checking the predictions.
     # test adapted from lightgbm's test_monotone_constraint(), itself inspired
     # by https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html
 
     rng = np.random.RandomState(global_random_seed)
 
     n_samples = 1000
     f_0 = rng.rand(n_samples)  # positive correlation with y
-    f_1 = rng.rand(n_samples)  # negative correslation with y
-    X = np.c_[f_0, f_1]
-    columns_name = ["f_0", "f_1"]
+    f_1 = rng.rand(n_samples)  # negative correlation with y
+
+    # extra categorical features, no correlation with y,
+    # to check the correctness of monotonicity constraint remapping, see issue #28898
+    f_a = rng.randint(low=0, high=9, size=n_samples)
+    f_b = rng.randint(low=0, high=9, size=n_samples)
+    f_c = rng.randint(low=0, high=9, size=n_samples)
+
+    X = np.c_[f_a, f_0, f_b, f_1, f_c]
+    columns_name = ["f_a", "f_0", "f_b", "f_1", "f_c"]
     constructor_name = "dataframe" if use_feature_names else "array"
     X = _convert_container(X, constructor_name, columns_name=columns_name)
 
     noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)
     y = 5 * f_0 + np.sin(10 * np.pi * f_0) - 5 * f_1 - np.cos(10 * np.pi * f_1) + noise
 
     if use_feature_names:
         monotonic_cst = {"f_0": +1, "f_1": -1}
+        categorical_features = ["f_a", "f_b", "f_c"]
     else:
-        monotonic_cst = [+1, -1]
+        monotonic_cst = [0, +1, 0, -1, 0]
+        categorical_features = [0, 2, 4]
 
-    gbdt = HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)
+    gbdt = HistGradientBoostingRegressor(
+        monotonic_cst=monotonic_cst, categorical_features=categorical_features
+    )
     gbdt.fit(X, y)
 
     linspace = np.linspace(0, 1, 100)
     sin = np.sin(linspace)
     constant = np.full_like(linspace, fill_value=0.5)
 
     # We now assert the predictions properly respect the constraints, on each
@@ -243,34 +254,34 @@
     # equal" type of constraints:
     # a constraint on the first feature only means that
     # x0 < x0' => f(x0, x1) < f(x0', x1)
     # while x1 stays constant.
     # The constraint does not guanrantee that
     # x0 < x0' => f(x0, x1) < f(x0', x1')
 
-    # First feature (POS)
+    # First non-categorical feature (POS)
     # assert pred is all increasing when f_0 is all increasing
-    X = np.c_[linspace, constant]
+    X = np.c_[constant, linspace, constant, constant, constant]
     X = _convert_container(X, constructor_name, columns_name=columns_name)
     pred = gbdt.predict(X)
     assert is_increasing(pred)
     # assert pred actually follows the variations of f_0
-    X = np.c_[sin, constant]
+    X = np.c_[constant, sin, constant, constant, constant]
     X = _convert_container(X, constructor_name, columns_name=columns_name)
     pred = gbdt.predict(X)
     assert np.all((np.diff(pred) >= 0) == (np.diff(sin) >= 0))
 
-    # Second feature (NEG)
+    # Second non-categorical feature (NEG)
     # assert pred is all decreasing when f_1 is all increasing
-    X = np.c_[constant, linspace]
+    X = np.c_[constant, constant, constant, linspace, constant]
     X = _convert_container(X, constructor_name, columns_name=columns_name)
     pred = gbdt.predict(X)
     assert is_decreasing(pred)
     # assert pred actually follows the inverse variations of f_1
-    X = np.c_[constant, sin]
+    X = np.c_[constant, constant, constant, sin, constant]
     X = _convert_container(X, constructor_name, columns_name=columns_name)
     pred = gbdt.predict(X)
     assert ((np.diff(pred) <= 0) == (np.diff(sin) >= 0)).all()
 
 
 def test_input_error():
     X = [[1, 2], [2, 3], [3, 4]]
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_iforest.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_iforest.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 from ..base import OutlierMixin, _fit_context
 from ..tree import ExtraTreeRegressor
 from ..tree._tree import DTYPE as tree_dtype
 from ..utils import (
     check_array,
     check_random_state,
     gen_batches,
-    get_chunk_n_rows,
 )
+from ..utils._chunking import get_chunk_n_rows
 from ..utils._param_validation import Interval, RealNotInt, StrOptions
 from ..utils.validation import _num_samples, check_is_fitted
 from ._bagging import BaseBagging
 
 __all__ = ["IsolationForest"]
 
 
@@ -228,31 +228,37 @@
         bootstrap=False,
         n_jobs=None,
         random_state=None,
         verbose=0,
         warm_start=False,
     ):
         super().__init__(
-            estimator=ExtraTreeRegressor(
-                max_features=1, splitter="random", random_state=random_state
-            ),
+            estimator=None,
             # here above max_features has no links with self.max_features
             bootstrap=bootstrap,
             bootstrap_features=False,
             n_estimators=n_estimators,
             max_samples=max_samples,
             max_features=max_features,
             warm_start=warm_start,
             n_jobs=n_jobs,
             random_state=random_state,
             verbose=verbose,
         )
 
         self.contamination = contamination
 
+    def _get_estimator(self):
+        return ExtraTreeRegressor(
+            # here max_features has no links with self.max_features
+            max_features=1,
+            splitter="random",
+            random_state=self.random_state,
+        )
+
     def _set_oob_score(self, X, y):
         raise NotImplementedError("OOB score not supported by iforest")
 
     def _parallel_args(self):
         # ExtraTreeRegressor releases the GIL, so it's more efficient to use
         # a thread-based backend rather than a process-based backend so as
         # to avoid suffering from communication overhead and extra memory
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_stacking.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_stacking.py`

 * *Files 2% similar despite different names*

```diff
@@ -197,28 +197,36 @@
         self : object
         """
         # all_estimators contains all estimators, the one to be fitted and the
         # 'drop' string.
         names, all_estimators = self._validate_estimators()
         self._validate_final_estimator()
 
+        # FIXME: when adding support for metadata routing in Stacking*.
+        # This is a hotfix to make StackingClassifier and StackingRegressor
+        # pass the tests despite not supporting metadata routing but sharing
+        # the same base class with VotingClassifier and VotingRegressor.
+        fit_params = dict()
+        if sample_weight is not None:
+            fit_params["sample_weight"] = sample_weight
+
         stack_method = [self.stack_method] * len(all_estimators)
 
         if self.cv == "prefit":
             self.estimators_ = []
             for estimator in all_estimators:
                 if estimator != "drop":
                     check_is_fitted(estimator)
                     self.estimators_.append(estimator)
         else:
             # Fit the base estimators on the whole training data. Those
             # base estimators will be used in transform, predict, and
             # predict_proba. They are exposed publicly.
             self.estimators_ = Parallel(n_jobs=self.n_jobs)(
-                delayed(_fit_single_estimator)(clone(est), X, y, sample_weight)
+                delayed(_fit_single_estimator)(clone(est), X, y, fit_params)
                 for est in all_estimators
                 if est != "drop"
             )
 
         self.named_estimators_ = Bunch()
         est_fitted_idx = 0
         for name_est, org_est in zip(names, all_estimators):
@@ -249,17 +257,14 @@
             # To ensure that the data provided to each estimator are the same,
             # we need to set the random state of the cv if there is one and we
             # need to take a copy.
             cv = check_cv(self.cv, y=y, classifier=is_classifier(self))
             if hasattr(cv, "random_state") and cv.random_state is None:
                 cv.random_state = np.random.RandomState()
 
-            fit_params = (
-                {"sample_weight": sample_weight} if sample_weight is not None else None
-            )
             predictions = Parallel(n_jobs=self.n_jobs)(
                 delayed(cross_val_predict)(
                     clone(est),
                     X,
                     y,
                     cv=deepcopy(cv),
                     method=meth,
@@ -276,17 +281,15 @@
         self.stack_method_ = [
             meth
             for (meth, est) in zip(self.stack_method_, all_estimators)
             if est != "drop"
         ]
 
         X_meta = self._concatenate_predictions(X, predictions)
-        _fit_single_estimator(
-            self.final_estimator_, X_meta, y, sample_weight=sample_weight
-        )
+        _fit_single_estimator(self.final_estimator_, X_meta, y, fit_params=fit_params)
 
         return self
 
     @property
     def n_features_in_(self):
         """Number of features seen during :term:`fit`."""
         try:
@@ -549,15 +552,15 @@
     >>> from sklearn.preprocessing import StandardScaler
     >>> from sklearn.pipeline import make_pipeline
     >>> from sklearn.ensemble import StackingClassifier
     >>> X, y = load_iris(return_X_y=True)
     >>> estimators = [
     ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
     ...     ('svr', make_pipeline(StandardScaler(),
-    ...                           LinearSVC(dual="auto", random_state=42)))
+    ...                           LinearSVC(random_state=42)))
     ... ]
     >>> clf = StackingClassifier(
     ...     estimators=estimators, final_estimator=LogisticRegression()
     ... )
     >>> from sklearn.model_selection import train_test_split
     >>> X_train, X_test, y_train, y_test = train_test_split(
     ...     X, y, stratify=y, random_state=42
@@ -893,15 +896,15 @@
     >>> from sklearn.linear_model import RidgeCV
     >>> from sklearn.svm import LinearSVR
     >>> from sklearn.ensemble import RandomForestRegressor
     >>> from sklearn.ensemble import StackingRegressor
     >>> X, y = load_diabetes(return_X_y=True)
     >>> estimators = [
     ...     ('lr', RidgeCV()),
-    ...     ('svr', LinearSVR(dual="auto", random_state=42))
+    ...     ('svr', LinearSVR(random_state=42))
     ... ]
     >>> reg = StackingRegressor(
     ...     estimators=estimators,
     ...     final_estimator=RandomForestRegressor(n_estimators=10,
     ...                                           random_state=42)
     ... )
     >>> from sklearn.model_selection import train_test_split
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_voting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_voting.py`

 * *Files 12% similar despite different names*

```diff
@@ -27,22 +27,26 @@
 )
 from ..exceptions import NotFittedError
 from ..preprocessing import LabelEncoder
 from ..utils import Bunch
 from ..utils._estimator_html_repr import _VisualBlock
 from ..utils._param_validation import StrOptions
 from ..utils.metadata_routing import (
-    _raise_for_unsupported_routing,
-    _RoutingNotSupportedMixin,
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    _routing_enabled,
+    process_routing,
 )
 from ..utils.metaestimators import available_if
 from ..utils.multiclass import type_of_target
 from ..utils.parallel import Parallel, delayed
 from ..utils.validation import (
     _check_feature_names_in,
+    _deprecate_positional_args,
     check_is_fitted,
     column_or_1d,
 )
 from ._base import _BaseHeterogeneousEnsemble, _fit_single_estimator
 
 
 class _BaseVoting(TransformerMixin, _BaseHeterogeneousEnsemble):
@@ -72,34 +76,45 @@
         return [w for est, w in zip(self.estimators, self.weights) if est[1] != "drop"]
 
     def _predict(self, X):
         """Collect results from clf.predict calls."""
         return np.asarray([est.predict(X) for est in self.estimators_]).T
 
     @abstractmethod
-    def fit(self, X, y, sample_weight=None):
+    def fit(self, X, y, **fit_params):
         """Get common fit operations."""
         names, clfs = self._validate_estimators()
 
         if self.weights is not None and len(self.weights) != len(self.estimators):
             raise ValueError(
                 "Number of `estimators` and weights must be equal; got"
                 f" {len(self.weights)} weights, {len(self.estimators)} estimators"
             )
 
+        if _routing_enabled():
+            routed_params = process_routing(self, "fit", **fit_params)
+        else:
+            routed_params = Bunch()
+            for name in names:
+                routed_params[name] = Bunch(fit={})
+                if "sample_weight" in fit_params:
+                    routed_params[name].fit["sample_weight"] = fit_params[
+                        "sample_weight"
+                    ]
+
         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
             delayed(_fit_single_estimator)(
                 clone(clf),
                 X,
                 y,
-                sample_weight=sample_weight,
+                fit_params=routed_params[name]["fit"],
                 message_clsname="Voting",
-                message=self._log_message(names[idx], idx + 1, len(clfs)),
+                message=self._log_message(name, idx + 1, len(clfs)),
             )
-            for idx, clf in enumerate(clfs)
+            for idx, (name, clf) in enumerate(zip(names, clfs))
             if clf != "drop"
         )
 
         self.named_estimators_ = Bunch()
 
         # Uses 'drop' as placeholder for dropped estimators
         est_iter = iter(self.estimators_)
@@ -152,16 +167,40 @@
 
         return self.estimators_[0].n_features_in_
 
     def _sk_visual_block_(self):
         names, estimators = zip(*self.estimators)
         return _VisualBlock("parallel", estimators, names=names)
 
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
+
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__)
+
+        # `self.estimators` is a list of (name, est) tuples
+        for name, estimator in self.estimators:
+            router.add(
+                **{name: estimator},
+                method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+            )
+        return router
+
 
-class VotingClassifier(_RoutingNotSupportedMixin, ClassifierMixin, _BaseVoting):
+class VotingClassifier(ClassifierMixin, _BaseVoting):
     """Soft Voting/Majority Rule classifier for unfitted estimators.
 
     Read more in the :ref:`User Guide <voting_classifier>`.
 
     .. versionadded:: 0.17
 
     Parameters
@@ -244,15 +283,15 @@
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn.linear_model import LogisticRegression
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier
-    >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)
+    >>> clf1 = LogisticRegression(random_state=1)
     >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
     >>> clf3 = GaussianNB()
     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
     >>> y = np.array([1, 1, 1, 2, 2, 2])
     >>> eclf1 = VotingClassifier(estimators=[
     ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
     >>> eclf1 = eclf1.fit(X, y)
@@ -313,15 +352,19 @@
         self.flatten_transform = flatten_transform
         self.verbose = verbose
 
     @_fit_context(
         # estimators in VotingClassifier.estimators are not validated yet
         prefer_skip_nested_validation=False
     )
-    def fit(self, X, y, sample_weight=None):
+    # TODO(1.7): remove `sample_weight` from the signature after deprecation
+    # cycle; pop it from `fit_params` before the `_raise_for_params` check and
+    # reinsert later, for backwards compatibility
+    @_deprecate_positional_args(version="1.7")
+    def fit(self, X, y, *, sample_weight=None, **fit_params):
         """Fit the estimators.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Training vectors, where `n_samples` is the number of samples and
             `n_features` is the number of features.
@@ -332,20 +375,31 @@
         sample_weight : array-like of shape (n_samples,), default=None
             Sample weights. If None, then samples are equally weighted.
             Note that this is supported only if all underlying estimators
             support sample weights.
 
             .. versionadded:: 0.18
 
+        **fit_params : dict
+            Parameters to pass to the underlying estimators.
+
+            .. versionadded:: 1.5
+
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Returns the instance itself.
         """
-        _raise_for_unsupported_routing(self, "fit", sample_weight=sample_weight)
+        _raise_for_params(fit_params, self, "fit")
         y_type = type_of_target(y, input_name="y")
         if y_type in ("unknown", "continuous"):
             # raise a specific ValueError for non-classification tasks
             raise ValueError(
                 f"Unknown label type: {y_type}. Maybe you are trying to fit a "
                 "classifier, which expects discrete classes on a "
                 "regression target with continuous values."
@@ -359,15 +413,18 @@
                 "supported."
             )
 
         self.le_ = LabelEncoder().fit(y)
         self.classes_ = self.le_.classes_
         transformed_y = self.le_.transform(y)
 
-        return super().fit(X, transformed_y, sample_weight)
+        if sample_weight is not None:
+            fit_params["sample_weight"] = sample_weight
+
+        return super().fit(X, transformed_y, **fit_params)
 
     def predict(self, X):
         """Predict class labels for X.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
@@ -491,15 +548,15 @@
         n_classes = len(self.classes_)
         names_out = [
             f"{class_name}_{name}{i}" for name in active_names for i in range(n_classes)
         ]
         return np.asarray(names_out, dtype=object)
 
 
-class VotingRegressor(_RoutingNotSupportedMixin, RegressorMixin, _BaseVoting):
+class VotingRegressor(RegressorMixin, _BaseVoting):
     """Prediction voting regressor for unfitted estimators.
 
     A voting regressor is an ensemble meta-estimator that fits several base
     regressors, each on the whole dataset. Then it averages the individual
     predictions to form a final prediction.
 
     Read more in the :ref:`User Guide <voting_regressor>`.
@@ -592,15 +649,19 @@
         self.n_jobs = n_jobs
         self.verbose = verbose
 
     @_fit_context(
         # estimators in VotingRegressor.estimators are not validated yet
         prefer_skip_nested_validation=False
     )
-    def fit(self, X, y, sample_weight=None):
+    # TODO(1.7): remove `sample_weight` from the signature after deprecation cycle;
+    # pop it from `fit_params` before the `_raise_for_params` check and reinsert later,
+    # for backwards compatibility
+    @_deprecate_positional_args(version="1.7")
+    def fit(self, X, y, *, sample_weight=None, **fit_params):
         """Fit the estimators.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Training vectors, where `n_samples` is the number of samples and
             `n_features` is the number of features.
@@ -609,22 +670,35 @@
             Target values.
 
         sample_weight : array-like of shape (n_samples,), default=None
             Sample weights. If None, then samples are equally weighted.
             Note that this is supported only if all underlying estimators
             support sample weights.
 
+        **fit_params : dict
+            Parameters to pass to the underlying estimators.
+
+            .. versionadded:: 1.5
+
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        _raise_for_unsupported_routing(self, "fit", sample_weight=sample_weight)
+        _raise_for_params(fit_params, self, "fit")
         y = column_or_1d(y, warn=True)
-        return super().fit(X, y, sample_weight)
+        if sample_weight is not None:
+            fit_params["sample_weight"] = sample_weight
+        return super().fit(X, y, **fit_params)
 
     def predict(self, X):
         """Predict regression target for X.
 
         The predicted regression target of an input sample is computed as the
         mean predicted regression targets of the estimators in the ensemble.
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/_weight_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/_weight_boosting.py`

 * *Files 0% similar despite different names*

```diff
@@ -474,14 +474,18 @@
     >>> clf = AdaBoostClassifier(n_estimators=100, algorithm="SAMME", random_state=0)
     >>> clf.fit(X, y)
     AdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)
     >>> clf.predict([[0, 0, 0, 0]])
     array([1])
     >>> clf.score(X, y)
     0.96...
+
+    For a detailed example of using AdaBoost to fit a sequence of DecisionTrees
+    as weaklearners, please refer to
+    :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py`.
     """
 
     # TODO(1.6): Modify _parameter_constraints for "algorithm" to only check
     # for "SAMME"
     _parameter_constraints: dict = {
         **BaseWeightBoosting._parameter_constraints,
         "algorithm": [
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_bagging.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_bagging.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,22 +6,27 @@
 # License: BSD 3 clause
 from itertools import cycle, product
 
 import joblib
 import numpy as np
 import pytest
 
+import sklearn
 from sklearn.base import BaseEstimator
 from sklearn.datasets import load_diabetes, load_iris, make_hastie_10_2
 from sklearn.dummy import DummyClassifier, DummyRegressor
 from sklearn.ensemble import (
+    AdaBoostClassifier,
+    AdaBoostRegressor,
     BaggingClassifier,
     BaggingRegressor,
     HistGradientBoostingClassifier,
     HistGradientBoostingRegressor,
+    RandomForestClassifier,
+    RandomForestRegressor,
 )
 from sklearn.feature_selection import SelectKBest
 from sklearn.linear_model import LogisticRegression, Perceptron
 from sklearn.model_selection import GridSearchCV, ParameterGrid, train_test_split
 from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import FunctionTransformer, scale
@@ -932,7 +937,40 @@
         (BaggingClassifier(LogisticRegression()), False),
         (BaggingRegressor(SVR()), False),
     ],
 )
 def test_bagging_allow_nan_tag(bagging, expected_allow_nan):
     """Check that bagging inherits allow_nan tag."""
     assert bagging._get_tags()["allow_nan"] == expected_allow_nan
+
+
+@pytest.mark.parametrize(
+    "model",
+    [
+        BaggingClassifier(
+            estimator=RandomForestClassifier(n_estimators=1), n_estimators=1
+        ),
+        BaggingRegressor(
+            estimator=RandomForestRegressor(n_estimators=1), n_estimators=1
+        ),
+    ],
+)
+def test_bagging_with_metadata_routing(model):
+    """Make sure that metadata routing works with non-default estimator."""
+    with sklearn.config_context(enable_metadata_routing=True):
+        model.fit(iris.data, iris.target)
+
+
+@pytest.mark.parametrize(
+    "model",
+    [
+        BaggingClassifier(
+            estimator=AdaBoostClassifier(n_estimators=1, algorithm="SAMME"),
+            n_estimators=1,
+        ),
+        BaggingRegressor(estimator=AdaBoostRegressor(n_estimators=1), n_estimators=1),
+    ],
+)
+def test_bagging_without_support_metadata_routing(model):
+    """Make sure that we still can use an estimator that does not implement the
+    metadata routing."""
+    model.fit(iris.data, iris.target)
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_base.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,47 +30,47 @@
     "X, y, estimator",
     [
         (
             *make_classification(n_samples=10),
             StackingClassifier(
                 estimators=[
                     ("lr", LogisticRegression()),
-                    ("svm", LinearSVC(dual="auto")),
+                    ("svm", LinearSVC()),
                     ("rf", RandomForestClassifier(n_estimators=5, max_depth=3)),
                 ],
                 cv=2,
             ),
         ),
         (
             *make_classification(n_samples=10),
             VotingClassifier(
                 estimators=[
                     ("lr", LogisticRegression()),
-                    ("svm", LinearSVC(dual="auto")),
+                    ("svm", LinearSVC()),
                     ("rf", RandomForestClassifier(n_estimators=5, max_depth=3)),
                 ]
             ),
         ),
         (
             *make_regression(n_samples=10),
             StackingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto")),
+                    ("svm", LinearSVR()),
                     ("rf", RandomForestRegressor(n_estimators=5, max_depth=3)),
                 ],
                 cv=2,
             ),
         ),
         (
             *make_regression(n_samples=10),
             VotingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto")),
+                    ("svm", LinearSVR()),
                     ("rf", RandomForestRegressor(n_estimators=5, max_depth=3)),
                 ]
             ),
         ),
     ],
     ids=[
         "stacking-classifier",
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_forest.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_forest.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_gradient_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_gradient_boosting.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Testing for the gradient boosting module (sklearn.ensemble.gradient_boosting).
 """
+
 import re
 import warnings
 
 import numpy as np
 import pytest
 from numpy.testing import assert_allclose
 
@@ -18,15 +19,15 @@
 from sklearn.exceptions import DataConversionWarning, NotFittedError
 from sklearn.linear_model import LinearRegression
 from sklearn.metrics import mean_squared_error
 from sklearn.model_selection import train_test_split
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import scale
 from sklearn.svm import NuSVR
-from sklearn.utils import check_random_state, tosequence
+from sklearn.utils import check_random_state
 from sklearn.utils._mocking import NoSampleWeightWrapper
 from sklearn.utils._param_validation import InvalidParameterError
 from sklearn.utils._testing import (
     assert_array_almost_equal,
     assert_array_equal,
     skip_if_32bit,
 )
@@ -525,18 +526,18 @@
     assert_allclose(y_quantile, y_ae)
 
 
 def test_symbol_labels():
     # Test with non-integer class labels.
     clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
 
-    symbol_y = tosequence(map(str, y))
+    symbol_y = list(map(str, y))
 
     clf.fit(X, symbol_y)
-    assert_array_equal(clf.predict(T), tosequence(map(str, true_result)))
+    assert_array_equal(clf.predict(T), list(map(str, true_result)))
     assert 100 == len(clf.estimators_)
 
 
 def test_float_class_labels():
     # Test with float class labels.
     clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_iforest.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_iforest.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_stacking.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_stacking.py`

 * *Files 3% similar despite different names*

```diff
@@ -65,15 +65,15 @@
 @pytest.mark.parametrize("passthrough", [False, True])
 def test_stacking_classifier_iris(cv, final_estimator, passthrough):
     # prescale the data to avoid convergence warning without using a pipeline
     # for later assert
     X_train, X_test, y_train, y_test = train_test_split(
         scale(X_iris), y_iris, stratify=y_iris, random_state=42
     )
-    estimators = [("lr", LogisticRegression()), ("svc", LinearSVC(dual="auto"))]
+    estimators = [("lr", LogisticRegression()), ("svc", LinearSVC())]
     clf = StackingClassifier(
         estimators=estimators,
         final_estimator=final_estimator,
         cv=cv,
         passthrough=passthrough,
     )
     clf.fit(X_train, y_train)
@@ -117,32 +117,32 @@
     clf = StackingClassifier(estimators=estimators, cv=3)
 
     clf.fit(X_train, y_train)
     X_trans = clf.transform(X_test)
     assert X_trans.shape[1] == 2
 
     # LinearSVC does not implement 'predict_proba' and will not drop one column
-    estimators = [("lr", LogisticRegression()), ("svc", LinearSVC(dual="auto"))]
+    estimators = [("lr", LogisticRegression()), ("svc", LinearSVC())]
     clf.set_params(estimators=estimators)
 
     clf.fit(X_train, y_train)
     X_trans = clf.transform(X_test)
     assert X_trans.shape[1] == 2
 
 
 def test_stacking_classifier_drop_estimator():
     # prescale the data to avoid convergence warning without using a pipeline
     # for later assert
     X_train, X_test, y_train, _ = train_test_split(
         scale(X_iris), y_iris, stratify=y_iris, random_state=42
     )
-    estimators = [("lr", "drop"), ("svc", LinearSVC(dual="auto", random_state=0))]
+    estimators = [("lr", "drop"), ("svc", LinearSVC(random_state=0))]
     rf = RandomForestClassifier(n_estimators=10, random_state=42)
     clf = StackingClassifier(
-        estimators=[("svc", LinearSVC(dual="auto", random_state=0))],
+        estimators=[("svc", LinearSVC(random_state=0))],
         final_estimator=rf,
         cv=5,
     )
     clf_drop = StackingClassifier(estimators=estimators, final_estimator=rf, cv=5)
 
     clf.fit(X_train, y_train)
     clf_drop.fit(X_train, y_train)
@@ -153,18 +153,18 @@
 
 def test_stacking_regressor_drop_estimator():
     # prescale the data to avoid convergence warning without using a pipeline
     # for later assert
     X_train, X_test, y_train, _ = train_test_split(
         scale(X_diabetes), y_diabetes, random_state=42
     )
-    estimators = [("lr", "drop"), ("svr", LinearSVR(dual="auto", random_state=0))]
+    estimators = [("lr", "drop"), ("svr", LinearSVR(random_state=0))]
     rf = RandomForestRegressor(n_estimators=10, random_state=42)
     reg = StackingRegressor(
-        estimators=[("svr", LinearSVR(dual="auto", random_state=0))],
+        estimators=[("svr", LinearSVR(random_state=0))],
         final_estimator=rf,
         cv=5,
     )
     reg_drop = StackingRegressor(estimators=estimators, final_estimator=rf, cv=5)
 
     reg.fit(X_train, y_train)
     reg_drop.fit(X_train, y_train)
@@ -184,15 +184,15 @@
 @pytest.mark.parametrize("passthrough", [False, True])
 def test_stacking_regressor_diabetes(cv, final_estimator, predict_params, passthrough):
     # prescale the data to avoid convergence warning without using a pipeline
     # for later assert
     X_train, X_test, y_train, _ = train_test_split(
         scale(X_diabetes), y_diabetes, random_state=42
     )
-    estimators = [("lr", LinearRegression()), ("svr", LinearSVR(dual="auto"))]
+    estimators = [("lr", LinearRegression()), ("svr", LinearSVR())]
     reg = StackingRegressor(
         estimators=estimators,
         final_estimator=final_estimator,
         cv=cv,
         passthrough=passthrough,
     )
     reg.fit(X_train, y_train)
@@ -222,15 +222,15 @@
     "sparse_container", COO_CONTAINERS + CSC_CONTAINERS + CSR_CONTAINERS
 )
 def test_stacking_regressor_sparse_passthrough(sparse_container):
     # Check passthrough behavior on a sparse X matrix
     X_train, X_test, y_train, _ = train_test_split(
         sparse_container(scale(X_diabetes)), y_diabetes, random_state=42
     )
-    estimators = [("lr", LinearRegression()), ("svr", LinearSVR(dual="auto"))]
+    estimators = [("lr", LinearRegression()), ("svr", LinearSVR())]
     rf = RandomForestRegressor(n_estimators=10, random_state=42)
     clf = StackingRegressor(
         estimators=estimators, final_estimator=rf, cv=5, passthrough=True
     )
     clf.fit(X_train, y_train)
     X_trans = clf.transform(X_test)
     assert_allclose_dense_sparse(X_test, X_trans[:, -10:])
@@ -242,15 +242,15 @@
     "sparse_container", COO_CONTAINERS + CSC_CONTAINERS + CSR_CONTAINERS
 )
 def test_stacking_classifier_sparse_passthrough(sparse_container):
     # Check passthrough behavior on a sparse X matrix
     X_train, X_test, y_train, _ = train_test_split(
         sparse_container(scale(X_iris)), y_iris, random_state=42
     )
-    estimators = [("lr", LogisticRegression()), ("svc", LinearSVC(dual="auto"))]
+    estimators = [("lr", LogisticRegression()), ("svc", LinearSVC())]
     rf = RandomForestClassifier(n_estimators=10, random_state=42)
     clf = StackingClassifier(
         estimators=estimators, final_estimator=rf, cv=5, passthrough=True
     )
     clf.fit(X_train, y_train)
     X_trans = clf.transform(X_test)
     assert_allclose_dense_sparse(X_test, X_trans[:, -4:])
@@ -315,15 +315,15 @@
             "does not support sample weight",
         ),
         (
             y_iris,
             {
                 "estimators": [
                     ("lr", LogisticRegression()),
-                    ("cor", LinearSVC(dual="auto", max_iter=50_000)),
+                    ("cor", LinearSVC(max_iter=50_000)),
                 ],
                 "final_estimator": NoWeightClassifier(),
             },
             TypeError,
             "does not support sample weight",
         ),
     ],
@@ -345,15 +345,15 @@
             "does not support sample weight",
         ),
         (
             y_diabetes,
             {
                 "estimators": [
                     ("lr", LinearRegression()),
-                    ("cor", LinearSVR(dual="auto")),
+                    ("cor", LinearSVR()),
                 ],
                 "final_estimator": NoWeightRegressor(),
             },
             TypeError,
             "does not support sample weight",
         ),
     ],
@@ -367,25 +367,25 @@
 @pytest.mark.parametrize(
     "estimator, X, y",
     [
         (
             StackingClassifier(
                 estimators=[
                     ("lr", LogisticRegression(random_state=0)),
-                    ("svm", LinearSVC(dual="auto", random_state=0)),
+                    ("svm", LinearSVC(random_state=0)),
                 ]
             ),
             X_iris[:100],
             y_iris[:100],
         ),  # keep only classes 0 and 1
         (
             StackingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto", random_state=0)),
+                    ("svm", LinearSVR(random_state=0)),
                 ]
             ),
             X_diabetes,
             y_diabetes,
         ),
     ],
     ids=["StackingClassifier", "StackingRegressor"],
@@ -411,41 +411,41 @@
 
 
 def test_stacking_classifier_stratify_default():
     # check that we stratify the classes for the default CV
     clf = StackingClassifier(
         estimators=[
             ("lr", LogisticRegression(max_iter=10_000)),
-            ("svm", LinearSVC(dual="auto", max_iter=10_000)),
+            ("svm", LinearSVC(max_iter=10_000)),
         ]
     )
     # since iris is not shuffled, a simple k-fold would not contain the
     # 3 classes during training
     clf.fit(X_iris, y_iris)
 
 
 @pytest.mark.parametrize(
     "stacker, X, y",
     [
         (
             StackingClassifier(
                 estimators=[
                     ("lr", LogisticRegression()),
-                    ("svm", LinearSVC(dual="auto", random_state=42)),
+                    ("svm", LinearSVC(random_state=42)),
                 ],
                 final_estimator=LogisticRegression(),
                 cv=KFold(shuffle=True, random_state=42),
             ),
             *load_breast_cancer(return_X_y=True),
         ),
         (
             StackingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto", random_state=42)),
+                    ("svm", LinearSVR(random_state=42)),
                 ],
                 final_estimator=LinearRegression(),
                 cv=KFold(shuffle=True, random_state=42),
             ),
             X_diabetes,
             y_diabetes,
         ),
@@ -494,25 +494,25 @@
 @pytest.mark.parametrize(
     "stacker, X, y",
     [
         (
             StackingClassifier(
                 estimators=[
                     ("lr", LogisticRegression()),
-                    ("svm", LinearSVC(dual="auto", random_state=42)),
+                    ("svm", LinearSVC(random_state=42)),
                 ],
                 final_estimator=LogisticRegression(),
             ),
             *load_breast_cancer(return_X_y=True),
         ),
         (
             StackingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto", random_state=42)),
+                    ("svm", LinearSVR(random_state=42)),
                 ],
                 final_estimator=LinearRegression(),
             ),
             X_diabetes,
             y_diabetes,
         ),
     ],
@@ -610,15 +610,15 @@
             X_iris,
             y_iris,
         ),
         (
             StackingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto")),
+                    ("svm", LinearSVR()),
                 ],
                 cv="prefit",
             ),
             X_diabetes,
             y_diabetes,
         ),
     ],
@@ -776,15 +776,15 @@
 @pytest.mark.parametrize(
     "stacker, feature_names, X, y, expected_names",
     [
         (
             StackingClassifier(
                 estimators=[
                     ("lr", LogisticRegression(random_state=0)),
-                    ("svm", LinearSVC(dual="auto", random_state=0)),
+                    ("svm", LinearSVC(random_state=0)),
                 ]
             ),
             iris.feature_names,
             X_iris,
             y_iris,
             [
                 "stackingclassifier_lr0",
@@ -796,30 +796,30 @@
             ],
         ),
         (
             StackingClassifier(
                 estimators=[
                     ("lr", LogisticRegression(random_state=0)),
                     ("other", "drop"),
-                    ("svm", LinearSVC(dual="auto", random_state=0)),
+                    ("svm", LinearSVC(random_state=0)),
                 ]
             ),
             iris.feature_names,
             X_iris[:100],
             y_iris[:100],  # keep only classes 0 and 1
             [
                 "stackingclassifier_lr",
                 "stackingclassifier_svm",
             ],
         ),
         (
             StackingRegressor(
                 estimators=[
                     ("lr", LinearRegression()),
-                    ("svm", LinearSVR(dual="auto", random_state=0)),
+                    ("svm", LinearSVR(random_state=0)),
                 ]
             ),
             diabetes.feature_names,
             X_diabetes,
             y_diabetes,
             [
                 "stackingregressor_lr",
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_voting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_voting.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,20 +19,26 @@
 from sklearn.linear_model import LinearRegression, LogisticRegression
 from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.naive_bayes import GaussianNB
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.preprocessing import StandardScaler
 from sklearn.svm import SVC
+from sklearn.tests.metadata_routing_common import (
+    ConsumingClassifier,
+    ConsumingRegressor,
+    _Registry,
+    check_recorded_metadata,
+)
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
 from sklearn.utils._testing import (
-    _convert_container,
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
+    ignore_warnings,
 )
 
 # Load datasets
 iris = datasets.load_iris()
 X, y = iris.data[:, 1:3], iris.target
 # Scaled to solve ConvergenceWarning throw by Logistic Regression
 X_scaled = StandardScaler().fit_transform(X)
@@ -251,27 +257,27 @@
         )
         eclf.fit(X, y).predict_proba(X)
 
     assert isinstance(exec_info.value.__cause__, AttributeError)
     assert inner_msg in str(exec_info.value.__cause__)
 
 
-@pytest.mark.parametrize("container_type", ["list", "array", "dataframe"])
-def test_multilabel(container_type):
+def test_multilabel():
     """Check if error is raised for multilabel classification."""
     X, y = make_multilabel_classification(
         n_classes=2, n_labels=1, allow_unlabeled=False, random_state=123
     )
-    y = _convert_container(y, container_type)
     clf = OneVsRestClassifier(SVC(kernel="linear"))
 
     eclf = VotingClassifier(estimators=[("ovr", clf)], voting="hard")
-    err_msg = "only supports binary or multiclass classification"
-    with pytest.raises(NotImplementedError, match=err_msg):
+
+    try:
         eclf.fit(X, y)
+    except NotImplementedError:
+        return
 
 
 def test_gridsearch():
     """Check GridSearch support."""
     clf1 = LogisticRegression(random_state=1)
     clf2 = RandomForestClassifier(random_state=1, n_estimators=3)
     clf3 = GaussianNB()
@@ -304,14 +310,15 @@
         estimators=[("lr", clf1), ("rf", clf2), ("gnb", clf3)], voting="soft", n_jobs=2
     ).fit(X, y)
 
     assert_array_equal(eclf1.predict(X), eclf2.predict(X))
     assert_array_almost_equal(eclf1.predict_proba(X), eclf2.predict_proba(X))
 
 
+@ignore_warnings(category=FutureWarning)
 def test_sample_weight(global_random_seed):
     """Tests sample_weight parameter of VotingClassifier"""
     clf1 = LogisticRegression(random_state=global_random_seed)
     clf2 = RandomForestClassifier(n_estimators=10, random_state=global_random_seed)
     clf3 = SVC(probability=True, random_state=global_random_seed)
     eclf1 = VotingClassifier(
         estimators=[("lr", clf1), ("rf", clf2), ("svc", clf3)], voting="soft"
@@ -678,7 +685,104 @@
 
     msg = (
         "get_feature_names_out is not supported when `voting='soft'` and "
         "`flatten_transform=False`"
     )
     with pytest.raises(ValueError, match=msg):
         voting.get_feature_names_out()
+
+
+# Metadata Routing Tests
+# ======================
+
+
+@pytest.mark.parametrize(
+    "Estimator, Child",
+    [(VotingClassifier, ConsumingClassifier), (VotingRegressor, ConsumingRegressor)],
+)
+def test_routing_passed_metadata_not_supported(Estimator, Child):
+    """Test that the right error message is raised when metadata is passed while
+    not supported when `enable_metadata_routing=False`."""
+
+    X = np.array([[0, 1], [2, 2], [4, 6]])
+    y = [1, 2, 3]
+
+    with pytest.raises(
+        ValueError, match="is only supported if enable_metadata_routing=True"
+    ):
+        Estimator(["clf", Child()]).fit(X, y, sample_weight=[1, 1, 1], metadata="a")
+
+
+@pytest.mark.usefixtures("enable_slep006")
+@pytest.mark.parametrize(
+    "Estimator, Child",
+    [(VotingClassifier, ConsumingClassifier), (VotingRegressor, ConsumingRegressor)],
+)
+def test_get_metadata_routing_without_fit(Estimator, Child):
+    # Test that metadata_routing() doesn't raise when called before fit.
+    est = Estimator([("sub_est", Child())])
+    est.get_metadata_routing()
+
+
+@pytest.mark.usefixtures("enable_slep006")
+@pytest.mark.parametrize(
+    "Estimator, Child",
+    [(VotingClassifier, ConsumingClassifier), (VotingRegressor, ConsumingRegressor)],
+)
+@pytest.mark.parametrize("prop", ["sample_weight", "metadata"])
+def test_metadata_routing_for_voting_estimators(Estimator, Child, prop):
+    """Test that metadata is routed correctly for Voting*."""
+    X = np.array([[0, 1], [2, 2], [4, 6]])
+    y = [1, 2, 3]
+    sample_weight, metadata = [1, 1, 1], "a"
+
+    est = Estimator(
+        [
+            (
+                "sub_est1",
+                Child(registry=_Registry()).set_fit_request(**{prop: True}),
+            ),
+            (
+                "sub_est2",
+                Child(registry=_Registry()).set_fit_request(**{prop: True}),
+            ),
+        ]
+    )
+
+    est.fit(X, y, **{prop: sample_weight if prop == "sample_weight" else metadata})
+
+    for estimator in est.estimators:
+        if prop == "sample_weight":
+            kwargs = {prop: sample_weight}
+        else:
+            kwargs = {prop: metadata}
+        # access sub-estimator in (name, est) with estimator[1]
+        registry = estimator[1].registry
+        assert len(registry)
+        for sub_est in registry:
+            check_recorded_metadata(obj=sub_est, method="fit", **kwargs)
+
+
+@pytest.mark.usefixtures("enable_slep006")
+@pytest.mark.parametrize(
+    "Estimator, Child",
+    [(VotingClassifier, ConsumingClassifier), (VotingRegressor, ConsumingRegressor)],
+)
+def test_metadata_routing_error_for_voting_estimators(Estimator, Child):
+    """Test that the right error is raised when metadata is not requested."""
+    X = np.array([[0, 1], [2, 2], [4, 6]])
+    y = [1, 2, 3]
+    sample_weight, metadata = [1, 1, 1], "a"
+
+    est = Estimator([("sub_est", Child())])
+
+    error_message = (
+        "[sample_weight, metadata] are passed but are not explicitly set as requested"
+        f" or not requested for {Child.__name__}.fit"
+    )
+
+    with pytest.raises(ValueError, match=re.escape(error_message)):
+        est.fit(X, y, sample_weight=sample_weight, metadata=metadata)
+
+
+# End of Metadata Routing Tests
+# =============================
```

### Comparing `scikit-learn-1.4.2/sklearn/ensemble/tests/test_weight_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/ensemble/tests/test_weight_boosting.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/exceptions.py` & `scikit_learn-1.5.0rc1/sklearn/exceptions.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/experimental/enable_halving_search_cv.py` & `scikit_learn-1.5.0rc1/sklearn/experimental/enable_halving_search_cv.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/experimental/enable_hist_gradient_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/experimental/enable_hist_gradient_boosting.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 
 It used to enable the use of
 :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
 :class:`~sklearn.ensemble.HistGradientBoostingRegressor` when they were still
 :term:`experimental`, but these estimators are now stable and can be imported
 normally from `sklearn.ensemble`.
 """
+
 # Don't remove this file, we don't want to break users code just because the
 # feature isn't experimental anymore.
 
 
 import warnings
 
 warnings.warn(
```

### Comparing `scikit-learn-1.4.2/sklearn/experimental/enable_iterative_imputer.py` & `scikit_learn-1.5.0rc1/sklearn/experimental/enable_iterative_imputer.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py` & `scikit_learn-1.5.0rc1/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Tests for making sure experimental imports work as expected."""
 
 import textwrap
 
 import pytest
 
-from sklearn.utils import _IS_WASM
 from sklearn.utils._testing import assert_run_python_script_without_output
+from sklearn.utils.fixes import _IS_WASM
 
 
 @pytest.mark.xfail(_IS_WASM, reason="cannot start subprocess")
 def test_import_raises_warning():
     code = """
     import pytest
     with pytest.warns(UserWarning, match="it is not needed to import"):
```

### Comparing `scikit-learn-1.4.2/sklearn/experimental/tests/test_enable_iterative_imputer.py` & `scikit_learn-1.5.0rc1/sklearn/experimental/tests/test_enable_iterative_imputer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Tests for making sure experimental imports work as expected."""
 
 import textwrap
 
 import pytest
 
-from sklearn.utils import _IS_WASM
 from sklearn.utils._testing import assert_run_python_script_without_output
+from sklearn.utils.fixes import _IS_WASM
 
 
 @pytest.mark.xfail(_IS_WASM, reason="cannot start subprocess")
 def test_imports_strategies():
     # Make sure different import strategies work or fail as expected.
 
     # Since Python caches the imported modules, we need to run a child process
```

### Comparing `scikit-learn-1.4.2/sklearn/experimental/tests/test_enable_successive_halving.py` & `scikit_learn-1.5.0rc1/sklearn/experimental/tests/test_enable_successive_halving.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Tests for making sure experimental imports work as expected."""
 
 import textwrap
 
 import pytest
 
-from sklearn.utils import _IS_WASM
 from sklearn.utils._testing import assert_run_python_script_without_output
+from sklearn.utils.fixes import _IS_WASM
 
 
 @pytest.mark.xfail(_IS_WASM, reason="cannot start subprocess")
 def test_imports_strategies():
     # Make sure different import strategies work or fail as expected.
 
     # Since Python caches the imported modules, we need to run a child process
```

### Comparing `scikit-learn-1.4.2/sklearn/externals/_arff.py` & `scikit_learn-1.5.0rc1/sklearn/externals/_arff.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/externals/_packaging/_structures.py` & `scikit_learn-1.5.0rc1/sklearn/externals/_packaging/_structures.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/externals/_packaging/version.py` & `scikit_learn-1.5.0rc1/sklearn/externals/_packaging/version.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/externals/_scipy/sparse/csgraph/_laplacian.py` & `scikit_learn-1.5.0rc1/sklearn/externals/_scipy/sparse/csgraph/_laplacian.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/_dict_vectorizer.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/_dict_vectorizer.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/_hash.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/_hash.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/_hashing_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/_hashing_fast.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/_stop_words.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/_stop_words.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/image.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/image.py`

 * *Files 3% similar despite different names*

```diff
@@ -171,22 +171,24 @@
         dtype of img.
 
     Returns
     -------
     graph : ndarray or a sparse matrix class
         The computed adjacency matrix.
 
-    Notes
-    -----
-    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was
-    handled by returning a dense np.matrix instance.  Going forward, np.ndarray
-    returns an np.ndarray, as expected.
-
-    For compatibility, user code relying on this method should wrap its
-    calls in ``np.asarray`` to avoid type issues.
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.feature_extraction.image import img_to_graph
+    >>> img = np.array([[0, 0], [0, 1]])
+    >>> img_to_graph(img, return_as=np.ndarray)
+    array([[0, 0, 0, 0],
+           [0, 0, 0, 1],
+           [0, 0, 0, 1],
+           [0, 1, 1, 1]])
     """
     img = np.atleast_3d(img)
     n_x, n_y, n_z = img.shape
     return _to_graph(n_x, n_y, n_z, mask, img, return_as, dtype)
 
 
 @validate_params(
@@ -225,23 +227,14 @@
         The data of the returned sparse matrix. By default it is int.
 
     Returns
     -------
     graph : np.ndarray or a sparse matrix class
         The computed adjacency matrix.
 
-    Notes
-    -----
-    For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was
-    handled by returning a dense np.matrix instance.  Going forward, np.ndarray
-    returns an np.ndarray, as expected.
-
-    For compatibility, user code relying on this method should wrap its
-    calls in ``np.asarray`` to avoid type issues.
-
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn.feature_extraction.image import grid_to_graph
     >>> shape_img = (4, 4, 1)
     >>> mask = np.zeros(shape=shape_img, dtype=bool)
     >>> mask[[1, 2], [1, 2], :] = True
@@ -489,14 +482,31 @@
         (image_height, image_width, n_channels)
         The size of the image that will be reconstructed.
 
     Returns
     -------
     image : ndarray of shape image_size
         The reconstructed image.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_sample_image
+    >>> from sklearn.feature_extraction import image
+    >>> one_image = load_sample_image("china.jpg")
+    >>> print('Image shape: {}'.format(one_image.shape))
+    Image shape: (427, 640, 3)
+    >>> image_patches = image.extract_patches_2d(image=one_image, patch_size=(10, 10))
+    >>> print('Patches shape: {}'.format(image_patches.shape))
+    Patches shape: (263758, 10, 10, 3)
+    >>> image_reconstructed = image.reconstruct_from_patches_2d(
+    ...     patches=image_patches,
+    ...     image_size=one_image.shape
+    ... )
+    >>> print(f"Reconstructed shape: {image_reconstructed.shape}")
+    Reconstructed shape: (427, 640, 3)
     """
     i_h, i_w = image_size[:2]
     p_h, p_w = patches.shape[1:3]
     img = np.zeros(image_size)
     # compute the dimensions of the patches array
     n_h = i_h - p_h + 1
     n_w = i_w - p_w + 1
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_dict_vectorizer.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_dict_vectorizer.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_feature_hasher.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_feature_hasher.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_image.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_image.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/tests/test_text.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/tests/test_text.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,22 +22,21 @@
     strip_accents_ascii,
     strip_accents_unicode,
     strip_tags,
 )
 from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
 from sklearn.pipeline import Pipeline
 from sklearn.svm import LinearSVC
-from sklearn.utils import _IS_WASM, IS_PYPY
 from sklearn.utils._testing import (
     assert_allclose_dense_sparse,
     assert_almost_equal,
     fails_if_pypy,
     skip_if_32bit,
 )
-from sklearn.utils.fixes import CSC_CONTAINERS, CSR_CONTAINERS
+from sklearn.utils.fixes import _IS_PYPY, _IS_WASM, CSC_CONTAINERS, CSR_CONTAINERS
 
 JUNK_FOOD_DOCS = (
     "the pizza pizza beer copyright",
     "the pizza burger beer copyright",
     "the the pizza beer beer copyright",
     "the burger beer beer copyright",
     "the coke burger coke copyright",
@@ -753,29 +752,19 @@
     for idx, name in enumerate(feature_names):
         assert idx == cv.vocabulary_.get(name)
 
 
 @pytest.mark.parametrize("Vectorizer", (CountVectorizer, TfidfVectorizer))
 def test_vectorizer_max_features(Vectorizer):
     expected_vocabulary = {"burger", "beer", "salad", "pizza"}
-    expected_stop_words = {
-        "celeri",
-        "tomato",
-        "copyright",
-        "coke",
-        "sparkling",
-        "water",
-        "the",
-    }
 
     # test bounded number of extracted features
     vectorizer = Vectorizer(max_df=0.6, max_features=4)
     vectorizer.fit(ALL_FOOD_DOCS)
     assert set(vectorizer.vocabulary_) == expected_vocabulary
-    assert vectorizer.stop_words_ == expected_stop_words
 
 
 def test_count_vectorizer_max_features():
     # Regression test: max_features didn't work correctly in 0.14.
 
     cv_1 = CountVectorizer(max_features=1)
     cv_3 = CountVectorizer(max_features=3)
@@ -802,52 +791,42 @@
 
 def test_vectorizer_max_df():
     test_data = ["abc", "dea", "eat"]
     vect = CountVectorizer(analyzer="char", max_df=1.0)
     vect.fit(test_data)
     assert "a" in vect.vocabulary_.keys()
     assert len(vect.vocabulary_.keys()) == 6
-    assert len(vect.stop_words_) == 0
 
     vect.max_df = 0.5  # 0.5 * 3 documents -> max_doc_count == 1.5
     vect.fit(test_data)
     assert "a" not in vect.vocabulary_.keys()  # {ae} ignored
     assert len(vect.vocabulary_.keys()) == 4  # {bcdt} remain
-    assert "a" in vect.stop_words_
-    assert len(vect.stop_words_) == 2
 
     vect.max_df = 1
     vect.fit(test_data)
     assert "a" not in vect.vocabulary_.keys()  # {ae} ignored
     assert len(vect.vocabulary_.keys()) == 4  # {bcdt} remain
-    assert "a" in vect.stop_words_
-    assert len(vect.stop_words_) == 2
 
 
 def test_vectorizer_min_df():
     test_data = ["abc", "dea", "eat"]
     vect = CountVectorizer(analyzer="char", min_df=1)
     vect.fit(test_data)
     assert "a" in vect.vocabulary_.keys()
     assert len(vect.vocabulary_.keys()) == 6
-    assert len(vect.stop_words_) == 0
 
     vect.min_df = 2
     vect.fit(test_data)
     assert "c" not in vect.vocabulary_.keys()  # {bcdt} ignored
     assert len(vect.vocabulary_.keys()) == 2  # {ae} remain
-    assert "c" in vect.stop_words_
-    assert len(vect.stop_words_) == 4
 
     vect.min_df = 0.8  # 0.8 * 3 documents -> min_doc_count == 2.4
     vect.fit(test_data)
     assert "c" not in vect.vocabulary_.keys()  # {bcdet} ignored
     assert len(vect.vocabulary_.keys()) == 1  # {a} remains
-    assert "c" in vect.stop_words_
-    assert len(vect.stop_words_) == 5
 
 
 def test_count_binary_occurrences():
     # by default multiple occurrences are counted as longs
     test_data = ["aaabc", "abbde"]
     vect = CountVectorizer(analyzer="char", max_df=1.0)
     X = vect.fit_transform(test_data).toarray()
@@ -933,15 +912,15 @@
     target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
 
     # split the dataset for model development and final evaluation
     train_data, test_data, target_train, target_test = train_test_split(
         data, target, test_size=0.2, random_state=0
     )
 
-    pipeline = Pipeline([("vect", CountVectorizer()), ("svc", LinearSVC(dual="auto"))])
+    pipeline = Pipeline([("vect", CountVectorizer()), ("svc", LinearSVC())])
 
     parameters = {
         "vect__ngram_range": [(1, 1), (1, 2)],
         "svc__loss": ("hinge", "squared_hinge"),
     }
 
     # find the best parameters for both the feature extraction and the
@@ -969,15 +948,15 @@
     target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
 
     # split the dataset for model development and final evaluation
     train_data, test_data, target_train, target_test = train_test_split(
         data, target, test_size=0.1, random_state=0
     )
 
-    pipeline = Pipeline([("vect", TfidfVectorizer()), ("svc", LinearSVC(dual="auto"))])
+    pipeline = Pipeline([("vect", TfidfVectorizer()), ("svc", LinearSVC())])
 
     parameters = {
         "vect__ngram_range": [(1, 1), (1, 2)],
         "vect__norm": ("l1", "l2"),
         "svc__loss": ("hinge", "squared_hinge"),
     }
 
@@ -1003,15 +982,15 @@
 def test_vectorizer_pipeline_cross_validation():
     # raw documents
     data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
 
     # label junk food as -1, the others as +1
     target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
 
-    pipeline = Pipeline([("vect", TfidfVectorizer()), ("svc", LinearSVC(dual="auto"))])
+    pipeline = Pipeline([("vect", TfidfVectorizer()), ("svc", LinearSVC())])
 
     cv_scores = cross_val_score(pipeline, data, target, cv=3)
     assert_array_equal(cv_scores, [1.0, 1.0, 1.0])
 
 
 @fails_if_pypy
 def test_vectorizer_unicode():
@@ -1065,15 +1044,15 @@
     ]
 
     for orig in instances:
         s = pickle.dumps(orig)
         copy = pickle.loads(s)
         assert type(copy) == orig.__class__
         assert copy.get_params() == orig.get_params()
-        if IS_PYPY and isinstance(orig, HashingVectorizer):
+        if _IS_PYPY and isinstance(orig, HashingVectorizer):
             continue
         else:
             assert_allclose_dense_sparse(
                 copy.fit_transform(JUNK_FOOD_DOCS),
                 orig.fit_transform(JUNK_FOOD_DOCS),
             )
 
@@ -1152,36 +1131,14 @@
         cv.fit(ALL_FOOD_DOCS)
         unpickled_cv.fit(ALL_FOOD_DOCS)
         assert_array_equal(
             cv.get_feature_names_out(), unpickled_cv.get_feature_names_out()
         )
 
 
-def test_stop_words_removal():
-    # Ensure that deleting the stop_words_ attribute doesn't affect transform
-
-    fitted_vectorizers = (
-        TfidfVectorizer().fit(JUNK_FOOD_DOCS),
-        CountVectorizer(preprocessor=strip_tags).fit(JUNK_FOOD_DOCS),
-        CountVectorizer(strip_accents=strip_eacute).fit(JUNK_FOOD_DOCS),
-    )
-
-    for vect in fitted_vectorizers:
-        vect_transform = vect.transform(JUNK_FOOD_DOCS).toarray()
-
-        vect.stop_words_ = None
-        stop_None_transform = vect.transform(JUNK_FOOD_DOCS).toarray()
-
-        delattr(vect, "stop_words_")
-        stop_del_transform = vect.transform(JUNK_FOOD_DOCS).toarray()
-
-        assert_array_equal(stop_None_transform, vect_transform)
-        assert_array_equal(stop_del_transform, vect_transform)
-
-
 def test_pickling_transformer():
     X = CountVectorizer().fit_transform(JUNK_FOOD_DOCS)
     orig = TfidfTransformer().fit(X)
     s = pickle.dumps(orig)
     copy = pickle.loads(s)
     assert type(copy) == orig.__class__
     assert_array_equal(copy.fit_transform(X).toarray(), orig.fit_transform(X).toarray())
@@ -1343,15 +1300,15 @@
     # vectorizers could be initialized with invalid ngram range
     # test for raising error message
     invalid_range = vec.ngram_range
     message = re.escape(
         f"Invalid value for ngram_range={invalid_range} "
         "lower boundary larger than the upper boundary."
     )
-    if isinstance(vec, HashingVectorizer) and IS_PYPY:
+    if isinstance(vec, HashingVectorizer) and _IS_PYPY:
         pytest.xfail(reason="HashingVectorizer is not supported on PyPy")
 
     with pytest.raises(ValueError, match=message):
         vec.fit(["good news everyone"])
 
     with pytest.raises(ValueError, match=message):
         vec.fit_transform(["good news everyone"])
@@ -1458,15 +1415,15 @@
     "input_type, err_type, err_msg",
     [
         ("filename", FileNotFoundError, ""),
         ("file", AttributeError, "'str' object has no attribute 'read'"),
     ],
 )
 def test_callable_analyzer_error(Estimator, input_type, err_type, err_msg):
-    if issubclass(Estimator, HashingVectorizer) and IS_PYPY:
+    if issubclass(Estimator, HashingVectorizer) and _IS_PYPY:
         pytest.xfail("HashingVectorizer is not supported on PyPy")
     data = ["this is text, not file or filename"]
     with pytest.raises(err_type, match=err_msg):
         Estimator(analyzer=lambda x: x.split(), input=input_type).fit_transform(data)
 
 
 @pytest.mark.parametrize(
@@ -1491,15 +1448,15 @@
     "Estimator", [CountVectorizer, TfidfVectorizer, HashingVectorizer]
 )
 def test_callable_analyzer_reraise_error(tmpdir, Estimator):
     # check if a custom exception from the analyzer is shown to the user
     def analyzer(doc):
         raise Exception("testing")
 
-    if issubclass(Estimator, HashingVectorizer) and IS_PYPY:
+    if issubclass(Estimator, HashingVectorizer) and _IS_PYPY:
         pytest.xfail("HashingVectorizer is not supported on PyPy")
 
     f = tmpdir.join("file.txt")
     f.write("sample content\n")
 
     with pytest.raises(Exception, match="testing"):
         Estimator(analyzer=analyzer, input="file").fit_transform([f])
@@ -1649,7 +1606,28 @@
 @pytest.mark.parametrize(
     "Estimator", [CountVectorizer, TfidfVectorizer, TfidfTransformer, HashingVectorizer]
 )
 def test_vectorizers_do_not_have_set_output(Estimator):
     """Check that vectorizers do not define set_output."""
     est = Estimator()
     assert not hasattr(est, "set_output")
+
+
+@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
+def test_tfidf_transformer_copy(csr_container):
+    """Check the behaviour of TfidfTransformer.transform with the copy parameter."""
+    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)
+    X_csr = csr_container(X)
+
+    # keep a copy of the original matrix for later comparison
+    X_csr_original = X_csr.copy()
+
+    transformer = TfidfTransformer().fit(X_csr)
+
+    X_transform = transformer.transform(X_csr, copy=True)
+    assert_allclose_dense_sparse(X_csr, X_csr_original)
+    assert X_transform is not X_csr
+
+    X_transform = transformer.transform(X_csr, copy=False)
+    assert X_transform is X_csr
+    with pytest.raises(AssertionError):
+        assert_allclose_dense_sparse(X_csr, X_csr_original)
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_extraction/text.py` & `scikit_learn-1.5.0rc1/sklearn/feature_extraction/text.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,16 +23,16 @@
 
 import numpy as np
 import scipy.sparse as sp
 
 from ..base import BaseEstimator, OneToOneFeatureMixin, TransformerMixin, _fit_context
 from ..exceptions import NotFittedError
 from ..preprocessing import normalize
-from ..utils import _IS_32BIT
 from ..utils._param_validation import HasMethods, Interval, RealNotInt, StrOptions
+from ..utils.fixes import _IS_32BIT
 from ..utils.validation import FLOAT_DTYPES, check_array, check_is_fitted
 from ._hash import FeatureHasher
 from ._stop_words import ENGLISH_STOP_WORDS
 
 __all__ = [
     "HashingVectorizer",
     "CountVectorizer",
@@ -405,16 +405,15 @@
             self._stop_words_id = id(self.stop_words)
 
             if inconsistent:
                 warnings.warn(
                     "Your stop_words may be inconsistent with "
                     "your preprocessing. Tokenizing the stop "
                     "words generated tokens %r not in "
-                    "stop_words."
-                    % sorted(inconsistent)
+                    "stop_words." % sorted(inconsistent)
                 )
             return not inconsistent
         except Exception:
             # Failed to check stop words consistency (e.g. because a custom
             # preprocessor or tokenizer was used)
             self._stop_words_id = id(self.stop_words)
             return "error"
@@ -512,16 +511,15 @@
 
     def _validate_ngram_range(self):
         """Check validity of ngram_range parameter"""
         min_n, max_m = self.ngram_range
         if min_n > max_m:
             raise ValueError(
                 "Invalid value for ngram_range=%s "
-                "lower boundary larger than the upper boundary."
-                % str(self.ngram_range)
+                "lower boundary larger than the upper boundary." % str(self.ngram_range)
             )
 
     def _warn_for_unused_params(self):
         if self.tokenizer is not None and self.token_pattern is not None:
             warnings.warn(
                 "The parameter 'token_pattern' will not be used"
                 " since 'tokenizer' is not None'"
@@ -601,14 +599,18 @@
     - no IDF weighting as this would render the transformer stateful.
 
     The hash function employed is the signed 32-bit version of Murmurhash3.
 
     For an efficiency comparison of the different feature extractors, see
     :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.
 
+    For an example of document clustering and comparison with
+    :class:`~sklearn.feature_extraction.text.TfidfVectorizer`, see
+    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.
+
     Read more in the :ref:`User Guide <text_feature_extraction>`.
 
     Parameters
     ----------
     input : {'filename', 'file', 'content'}, default='content'
         - If `'filename'`, the sequence passed as an argument to fit is
           expected to be a list of filenames that need reading to fetch
@@ -1077,37 +1079,22 @@
     vocabulary_ : dict
         A mapping of terms to feature indices.
 
     fixed_vocabulary_ : bool
         True if a fixed vocabulary of term to indices mapping
         is provided by the user.
 
-    stop_words_ : set
-        Terms that were ignored because they either:
-
-          - occurred in too many documents (`max_df`)
-          - occurred in too few documents (`min_df`)
-          - were cut off by feature selection (`max_features`).
-
-        This is only available if no vocabulary was given.
-
     See Also
     --------
     HashingVectorizer : Convert a collection of text documents to a
         matrix of token counts.
 
     TfidfVectorizer : Convert a collection of raw documents to a matrix
         of TF-IDF features.
 
-    Notes
-    -----
-    The ``stop_words_`` attribute can get large and increase the model size
-    when pickling. This attribute is provided only for introspection and can
-    be safely removed using delattr or set to None before pickling.
-
     Examples
     --------
     >>> from sklearn.feature_extraction.text import CountVectorizer
     >>> corpus = [
     ...     'This is the first document.',
     ...     'This document is the second document.',
     ...     'And this is the third one.',
@@ -1238,27 +1225,25 @@
             tfs = np.asarray(X.sum(axis=0)).ravel()
             mask_inds = (-tfs[mask]).argsort()[:limit]
             new_mask = np.zeros(len(dfs), dtype=bool)
             new_mask[np.where(mask)[0][mask_inds]] = True
             mask = new_mask
 
         new_indices = np.cumsum(mask) - 1  # maps old indices to new
-        removed_terms = set()
         for term, old_index in list(vocabulary.items()):
             if mask[old_index]:
                 vocabulary[term] = new_indices[old_index]
             else:
                 del vocabulary[term]
-                removed_terms.add(term)
         kept_indices = np.where(mask)[0]
         if len(kept_indices) == 0:
             raise ValueError(
                 "After pruning, no terms remain. Try a lower min_df or a higher max_df."
             )
-        return X[:, kept_indices], removed_terms
+        return X[:, kept_indices]
 
     def _count_vocab(self, raw_documents, fixed_vocab):
         """Create sparse feature matrix, and vocabulary where fixed_vocab=False"""
         if fixed_vocab:
             vocabulary = self.vocabulary_
         else:
             # Add a new value when a new vocabulary item is seen
@@ -1395,15 +1380,15 @@
             n_doc = X.shape[0]
             max_doc_count = max_df if isinstance(max_df, Integral) else max_df * n_doc
             min_doc_count = min_df if isinstance(min_df, Integral) else min_df * n_doc
             if max_doc_count < min_doc_count:
                 raise ValueError("max_df corresponds to < documents than min_df")
             if max_features is not None:
                 X = self._sort_features(X, vocabulary)
-            X, self.stop_words_ = self._limit_features(
+            X = self._limit_features(
                 X, vocabulary, max_doc_count, min_doc_count, max_features
             )
             if max_features is None:
                 X = self._sort_features(X, vocabulary)
             self.vocabulary_ = vocabulary
 
         return X
@@ -1662,114 +1647,99 @@
         # _document_frequency uses np.bincount which works on arrays of
         # dtype NPY_INTP which is int32 for 32bit platforms. See #20923
         X = self._validate_data(
             X, accept_sparse=("csr", "csc"), accept_large_sparse=not _IS_32BIT
         )
         if not sp.issparse(X):
             X = sp.csr_matrix(X)
-        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64
+        dtype = X.dtype if X.dtype in (np.float64, np.float32) else np.float64
 
         if self.use_idf:
-            n_samples, n_features = X.shape
+            n_samples, _ = X.shape
             df = _document_frequency(X)
             df = df.astype(dtype, copy=False)
 
             # perform idf smoothing if required
-            df += int(self.smooth_idf)
+            df += float(self.smooth_idf)
             n_samples += int(self.smooth_idf)
 
             # log+1 instead of log makes sure terms with zero idf don't get
             # suppressed entirely.
-            idf = np.log(n_samples / df) + 1
-            self._idf_diag = sp.diags(
-                idf,
-                offsets=0,
-                shape=(n_features, n_features),
-                format="csr",
-                dtype=dtype,
-            )
+            # `np.log` preserves the dtype of `df` and thus `dtype`.
+            self.idf_ = np.log(n_samples / df) + 1.0
 
         return self
 
     def transform(self, X, copy=True):
         """Transform a count matrix to a tf or tf-idf representation.
 
         Parameters
         ----------
         X : sparse matrix of (n_samples, n_features)
             A matrix of term/token counts.
 
         copy : bool, default=True
             Whether to copy X and operate on the copy or perform in-place
-            operations.
+            operations. `copy=False` will only be effective with CSR sparse matrix.
 
         Returns
         -------
         vectors : sparse matrix of shape (n_samples, n_features)
             Tf-idf-weighted document-term matrix.
         """
+        check_is_fitted(self)
         X = self._validate_data(
-            X, accept_sparse="csr", dtype=FLOAT_DTYPES, copy=copy, reset=False
+            X,
+            accept_sparse="csr",
+            dtype=[np.float64, np.float32],
+            copy=copy,
+            reset=False,
         )
         if not sp.issparse(X):
-            X = sp.csr_matrix(X, dtype=np.float64)
+            X = sp.csr_matrix(X, dtype=X.dtype)
 
         if self.sublinear_tf:
             np.log(X.data, X.data)
-            X.data += 1
-
-        if self.use_idf:
-            # idf_ being a property, the automatic attributes detection
-            # does not work as usual and we need to specify the attribute
-            # name:
-            check_is_fitted(self, attributes=["idf_"], msg="idf vector is not fitted")
+            X.data += 1.0
 
-            X = X @ self._idf_diag
+        if hasattr(self, "idf_"):
+            # the columns of X (CSR matrix) can be accessed with `X.indices `and
+            # multiplied with the corresponding `idf` value
+            X.data *= self.idf_[X.indices]
 
         if self.norm is not None:
             X = normalize(X, norm=self.norm, copy=False)
 
         return X
 
-    @property
-    def idf_(self):
-        """Inverse document frequency vector, only defined if `use_idf=True`.
-
-        Returns
-        -------
-        ndarray of shape (n_features,)
-        """
-        # if _idf_diag is not set, this will raise an attribute error,
-        # which means hasattr(self, "idf_") is False
-        return np.ravel(self._idf_diag.sum(axis=0))
-
-    @idf_.setter
-    def idf_(self, value):
-        value = np.asarray(value, dtype=np.float64)
-        n_features = value.shape[0]
-        self._idf_diag = sp.spdiags(
-            value, diags=0, m=n_features, n=n_features, format="csr"
-        )
-
     def _more_tags(self):
-        return {"X_types": ["2darray", "sparse"]}
+        return {
+            "X_types": ["2darray", "sparse"],
+            # FIXME: np.float16 could be preserved if _inplace_csr_row_normalize_l2
+            # accepted it.
+            "preserves_dtype": [np.float64, np.float32],
+        }
 
 
 class TfidfVectorizer(CountVectorizer):
     r"""Convert a collection of raw documents to a matrix of TF-IDF features.
 
     Equivalent to :class:`CountVectorizer` followed by
     :class:`TfidfTransformer`.
 
     For an example of usage, see
     :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.
 
     For an efficiency comparison of the different feature extractors, see
     :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.
 
+    For an example of document clustering and comparison with
+    :class:`~sklearn.feature_extraction.text.HashingVectorizer`, see
+    :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.
+
     Read more in the :ref:`User Guide <text_feature_extraction>`.
 
     Parameters
     ----------
     input : {'filename', 'file', 'content'}, default='content'
         - If `'filename'`, the sequence passed as an argument to fit is
           expected to be a list of filenames that need reading to fetch
@@ -1928,36 +1898,21 @@
         True if a fixed vocabulary of term to indices mapping
         is provided by the user.
 
     idf_ : array of shape (n_features,)
         The inverse document frequency (IDF) vector; only defined
         if ``use_idf`` is True.
 
-    stop_words_ : set
-        Terms that were ignored because they either:
-
-          - occurred in too many documents (`max_df`)
-          - occurred in too few documents (`min_df`)
-          - were cut off by feature selection (`max_features`).
-
-        This is only available if no vocabulary was given.
-
     See Also
     --------
     CountVectorizer : Transforms text into a sparse matrix of n-gram counts.
 
     TfidfTransformer : Performs the TF-IDF transformation from a provided
         matrix of counts.
 
-    Notes
-    -----
-    The ``stop_words_`` attribute can get large and increase the model size
-    when pickling. This attribute is provided only for introspection and can
-    be safely removed using delattr or set to None before pickling.
-
     Examples
     --------
     >>> from sklearn.feature_extraction.text import TfidfVectorizer
     >>> corpus = [
     ...     'This is the first document.',
     ...     'This document is the second document.',
     ...     'And this is the third one.',
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_base.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,23 +7,18 @@
 from abc import ABCMeta, abstractmethod
 from operator import attrgetter
 
 import numpy as np
 from scipy.sparse import csc_matrix, issparse
 
 from ..base import TransformerMixin
-from ..utils import (
-    _is_pandas_df,
-    _safe_indexing,
-    check_array,
-    safe_sqr,
-)
+from ..utils import _safe_indexing, check_array, safe_sqr
 from ..utils._set_output import _get_output_config
 from ..utils._tags import _safe_tags
-from ..utils.validation import _check_feature_names_in, check_is_fitted
+from ..utils.validation import _check_feature_names_in, _is_pandas_df, check_is_fitted
 
 
 class SelectorMixin(TransformerMixin, metaclass=ABCMeta):
     """
     Transformer mixin that performs feature selection given a support mask
 
     This mixin provides a feature selector implementation with `transform` and
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_from_model.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_from_model.py`

 * *Files 0% similar despite different names*

```diff
@@ -345,20 +345,20 @@
         y : array-like of shape (n_samples,), default=None
             The target values (integers that correspond to classes in
             classification, real numbers in regression).
 
         **fit_params : dict
             - If `enable_metadata_routing=False` (default):
 
-                Parameters directly passed to the `partial_fit` method of the
+                Parameters directly passed to the `fit` method of the
                 sub-estimator. They are ignored if `prefit=True`.
 
             - If `enable_metadata_routing=True`:
 
-                Parameters safely routed to the `partial_fit` method of the
+                Parameters safely routed to the `fit` method of the
                 sub-estimator. They are ignored if `prefit=True`.
 
                 .. versionchanged:: 1.4
                     See :ref:`Metadata Routing User Guide <metadata_routing>` for
                     more details.
 
         Returns
@@ -509,14 +509,14 @@
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
         router = MetadataRouter(owner=self.__class__.__name__).add(
             estimator=self.estimator,
             method_mapping=MethodMapping()
-            .add(callee="partial_fit", caller="partial_fit")
-            .add(callee="fit", caller="fit"),
+            .add(caller="partial_fit", callee="partial_fit")
+            .add(caller="fit", callee="fit"),
         )
         return router
 
     def _more_tags(self):
         return {"allow_nan": _safe_tags(self.estimator, key="allow_nan")}
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_mutual_info.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_mutual_info.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 
 from ..metrics.cluster import mutual_info_score
 from ..neighbors import KDTree, NearestNeighbors
 from ..preprocessing import scale
 from ..utils import check_random_state
 from ..utils._param_validation import Interval, StrOptions, validate_params
 from ..utils.multiclass import check_classification_targets
+from ..utils.parallel import Parallel, delayed
 from ..utils.validation import check_array, check_X_y
 
 
 def _compute_mi_cc(x, y, n_neighbors):
     """Compute mutual information between two continuous variables.
 
     Parameters
@@ -197,19 +198,21 @@
         for i in columns:
             yield X[:, i]
 
 
 def _estimate_mi(
     X,
     y,
+    *,
     discrete_features="auto",
     discrete_target=False,
     n_neighbors=3,
     copy=True,
     random_state=None,
+    n_jobs=None,
 ):
     """Estimate mutual information between the features and the target.
 
     Parameters
     ----------
     X : array-like or sparse matrix, shape (n_samples, n_features)
         Feature matrix.
@@ -238,14 +241,24 @@
 
     random_state : int, RandomState instance or None, default=None
         Determines random number generation for adding small noise to
         continuous variables in order to remove repeated values.
         Pass an int for reproducible results across multiple function calls.
         See :term:`Glossary <random_state>`.
 
+    n_jobs : int, default=None
+        The number of jobs to use for computing the mutual information.
+        The parallelization is done on the columns of `X`.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+        .. versionadded:: 1.5
+
+
     Returns
     -------
     mi : ndarray, shape (n_features,)
         Estimated mutual information between each feature and the target in
         nat units. A negative value will be replaced by 0.
 
     References
@@ -297,35 +310,43 @@
         y = scale(y, with_mean=False)
         y += (
             1e-10
             * np.maximum(1, np.mean(np.abs(y)))
             * rng.standard_normal(size=n_samples)
         )
 
-    mi = [
-        _compute_mi(x, y, discrete_feature, discrete_target, n_neighbors)
+    mi = Parallel(n_jobs=n_jobs)(
+        delayed(_compute_mi)(x, y, discrete_feature, discrete_target, n_neighbors)
         for x, discrete_feature in zip(_iterate_columns(X), discrete_mask)
-    ]
+    )
 
     return np.array(mi)
 
 
 @validate_params(
     {
         "X": ["array-like", "sparse matrix"],
         "y": ["array-like"],
         "discrete_features": [StrOptions({"auto"}), "boolean", "array-like"],
         "n_neighbors": [Interval(Integral, 1, None, closed="left")],
         "copy": ["boolean"],
         "random_state": ["random_state"],
+        "n_jobs": [Integral, None],
     },
     prefer_skip_nested_validation=True,
 )
 def mutual_info_regression(
-    X, y, *, discrete_features="auto", n_neighbors=3, copy=True, random_state=None
+    X,
+    y,
+    *,
+    discrete_features="auto",
+    n_neighbors=3,
+    copy=True,
+    random_state=None,
+    n_jobs=None,
 ):
     """Estimate mutual information for a continuous target variable.
 
     Mutual information (MI) [1]_ between two random variables is a non-negative
     value, which measures the dependency between the variables. It is equal
     to zero if and only if two random variables are independent, and higher
     values mean higher dependency.
@@ -363,14 +384,24 @@
 
     random_state : int, RandomState instance or None, default=None
         Determines random number generation for adding small noise to
         continuous variables in order to remove repeated values.
         Pass an int for reproducible results across multiple function calls.
         See :term:`Glossary <random_state>`.
 
+    n_jobs : int, default=None
+        The number of jobs to use for computing the mutual information.
+        The parallelization is done on the columns of `X`.
+
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     mi : ndarray, shape (n_features,)
         Estimated mutual information between each feature and the target in
         nat units.
 
     Notes
@@ -403,30 +434,47 @@
     >>> from sklearn.feature_selection import mutual_info_regression
     >>> X, y = make_regression(
     ...     n_samples=50, n_features=3, n_informative=1, noise=1e-4, random_state=42
     ... )
     >>> mutual_info_regression(X, y)
     array([0.1..., 2.6...  , 0.0...])
     """
-    return _estimate_mi(X, y, discrete_features, False, n_neighbors, copy, random_state)
+    return _estimate_mi(
+        X,
+        y,
+        discrete_features=discrete_features,
+        discrete_target=False,
+        n_neighbors=n_neighbors,
+        copy=copy,
+        random_state=random_state,
+        n_jobs=n_jobs,
+    )
 
 
 @validate_params(
     {
         "X": ["array-like", "sparse matrix"],
         "y": ["array-like"],
         "discrete_features": [StrOptions({"auto"}), "boolean", "array-like"],
         "n_neighbors": [Interval(Integral, 1, None, closed="left")],
         "copy": ["boolean"],
         "random_state": ["random_state"],
+        "n_jobs": [Integral, None],
     },
     prefer_skip_nested_validation=True,
 )
 def mutual_info_classif(
-    X, y, *, discrete_features="auto", n_neighbors=3, copy=True, random_state=None
+    X,
+    y,
+    *,
+    discrete_features="auto",
+    n_neighbors=3,
+    copy=True,
+    random_state=None,
+    n_jobs=None,
 ):
     """Estimate mutual information for a discrete target variable.
 
     Mutual information (MI) [1]_ between two random variables is a non-negative
     value, which measures the dependency between the variables. It is equal
     to zero if and only if two random variables are independent, and higher
     values mean higher dependency.
@@ -464,14 +512,23 @@
 
     random_state : int, RandomState instance or None, default=None
         Determines random number generation for adding small noise to
         continuous variables in order to remove repeated values.
         Pass an int for reproducible results across multiple function calls.
         See :term:`Glossary <random_state>`.
 
+    n_jobs : int, default=None
+        The number of jobs to use for computing the mutual information.
+        The parallelization is done on the columns of `X`.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     mi : ndarray, shape (n_features,)
         Estimated mutual information between each feature and the target in
         nat units.
 
     Notes
@@ -507,8 +564,17 @@
     ...     shuffle=False, random_state=42
     ... )
     >>> mutual_info_classif(X, y)
     array([0.58..., 0.10..., 0.19..., 0.09... , 0.        ,
            0.     , 0.     , 0.     , 0.      , 0.        ])
     """
     check_classification_targets(y)
-    return _estimate_mi(X, y, discrete_features, True, n_neighbors, copy, random_state)
+    return _estimate_mi(
+        X,
+        y,
+        discrete_features=discrete_features,
+        discrete_target=True,
+        n_neighbors=n_neighbors,
+        copy=copy,
+        random_state=random_state,
+        n_jobs=n_jobs,
+    )
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_rfe.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_rfe.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 #          Vincent Michel <vincent.michel@inria.fr>
 #          Gilles Louppe <g.louppe@gmail.com>
 #
 # License: BSD 3 clause
 
 """Recursive feature elimination for feature ranking"""
 
+import warnings
 from numbers import Integral
 
 import numpy as np
 from joblib import effective_n_jobs
 
 from ..base import BaseEstimator, MetaEstimatorMixin, _fit_context, clone, is_classifier
 from ..metrics import check_scoring
@@ -24,30 +25,33 @@
 from ..utils.parallel import Parallel, delayed
 from ..utils.validation import check_is_fitted
 from ._base import SelectorMixin, _get_feature_importances
 
 
 def _rfe_single_fit(rfe, estimator, X, y, train, test, scorer):
     """
-    Return the score for a fit across one fold.
+    Return the score and n_features per step for a fit across one fold.
     """
     X_train, y_train = _safe_split(estimator, X, y, train)
     X_test, y_test = _safe_split(estimator, X, y, test, train)
-    return rfe._fit(
+
+    rfe._fit(
         X_train,
         y_train,
         lambda estimator, features: _score(
             # TODO(SLEP6): pass score_params here
             estimator,
             X_test[:, features],
             y_test,
             scorer,
             score_params=None,
         ),
-    ).scores_
+    )
+
+    return rfe.step_scores_, rfe.step_n_features_
 
 
 def _estimator_has(attr):
     """Check if we can delegate a method to the underlying estimator.
 
     First, we check the fitted `estimator_` if available, otherwise we check the
     unfitted `estimator`. We raise the original `AttributeError` if `attr` does
@@ -260,18 +264,17 @@
         self : object
             Fitted estimator.
         """
         _raise_for_unsupported_routing(self, "fit", **fit_params)
         return self._fit(X, y, **fit_params)
 
     def _fit(self, X, y, step_score=None, **fit_params):
-        # Parameter step_score controls the calculation of self.scores_
-        # step_score is not exposed to users
-        # and is used when implementing RFECV
-        # self.scores_ will not be calculated when calling _fit through fit
+        # Parameter step_score controls the calculation of self.step_scores_
+        # step_score is not exposed to users and is used when implementing RFECV
+        # self.step_scores_ will not be calculated when calling _fit through fit
 
         X, y = self._validate_data(
             X,
             y,
             accept_sparse="csc",
             ensure_min_features=2,
             force_all_finite=False,
@@ -280,27 +283,36 @@
 
         # Initialization
         n_features = X.shape[1]
         if self.n_features_to_select is None:
             n_features_to_select = n_features // 2
         elif isinstance(self.n_features_to_select, Integral):  # int
             n_features_to_select = self.n_features_to_select
+            if n_features_to_select > n_features:
+                warnings.warn(
+                    (
+                        f"Found {n_features_to_select=} > {n_features=}. There will be"
+                        " no feature selection and all features will be kept."
+                    ),
+                    UserWarning,
+                )
         else:  # float
             n_features_to_select = int(n_features * self.n_features_to_select)
 
         if 0.0 < self.step < 1.0:
             step = int(max(1, self.step * n_features))
         else:
             step = int(self.step)
 
         support_ = np.ones(n_features, dtype=bool)
         ranking_ = np.ones(n_features, dtype=int)
 
         if step_score:
-            self.scores_ = []
+            self.step_n_features_ = []
+            self.step_scores_ = []
 
         # Elimination
         while np.sum(support_) > n_features_to_select:
             # Remaining features
             features = np.arange(n_features)[support_]
 
             # Rank the remaining features
@@ -324,26 +336,28 @@
             # Eliminate the worse features
             threshold = min(step, np.sum(support_) - n_features_to_select)
 
             # Compute step score on the previous selection iteration
             # because 'estimator' must use features
             # that have not been eliminated yet
             if step_score:
-                self.scores_.append(step_score(estimator, features))
+                self.step_n_features_.append(len(features))
+                self.step_scores_.append(step_score(estimator, features))
             support_[features[ranks][:threshold]] = False
             ranking_[np.logical_not(support_)] += 1
 
         # Set final attributes
         features = np.arange(n_features)[support_]
         self.estimator_ = clone(self.estimator)
         self.estimator_.fit(X[:, features], y, **fit_params)
 
         # Compute step score when only n_features_to_select features left
         if step_score:
-            self.scores_.append(step_score(self.estimator_, features))
+            self.step_n_features_.append(len(features))
+            self.step_scores_.append(step_score(self.estimator_, features))
         self.n_features_ = support_.sum()
         self.support_ = support_
         self.ranking_ = ranking_
 
         return self
 
     @available_if(_estimator_has("predict"))
@@ -562,25 +576,32 @@
     classes_ : ndarray of shape (n_classes,)
         The classes labels. Only available when `estimator` is a classifier.
 
     estimator_ : ``Estimator`` instance
         The fitted estimator used to select features.
 
     cv_results_ : dict of ndarrays
-        A dict with keys:
+        All arrays (values of the dictionary) are sorted in ascending order
+        by the number of features used (i.e., the first element of the array
+        represents the models that used the least number of features, while the
+        last element represents the models that used all available features).
+        This dictionary contains the following keys:
 
         split(k)_test_score : ndarray of shape (n_subsets_of_features,)
             The cross-validation scores across (k)th fold.
 
         mean_test_score : ndarray of shape (n_subsets_of_features,)
             Mean of scores over the folds.
 
         std_test_score : ndarray of shape (n_subsets_of_features,)
             Standard deviation of scores over the folds.
 
+        n_features : ndarray of shape (n_subsets_of_features,)
+            Number of features used at each step.
+
         .. versionadded:: 1.0
 
     n_features_ : int
         The number of selected features with cross-validation.
 
     n_features_in_ : int
         Number of features seen during :term:`fit`. Only defined if the
@@ -710,26 +731,30 @@
             force_all_finite=False,
             multi_output=True,
         )
 
         # Initialization
         cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))
         scorer = check_scoring(self.estimator, scoring=self.scoring)
-        n_features = X.shape[1]
-
-        if 0.0 < self.step < 1.0:
-            step = int(max(1, self.step * n_features))
-        else:
-            step = int(self.step)
 
         # Build an RFE object, which will evaluate and score each possible
         # feature count, down to self.min_features_to_select
+        n_features = X.shape[1]
+        if self.min_features_to_select > n_features:
+            warnings.warn(
+                (
+                    f"Found min_features_to_select={self.min_features_to_select} > "
+                    f"{n_features=}. There will be no feature selection and all "
+                    "features will be kept."
+                ),
+                UserWarning,
+            )
         rfe = RFE(
             estimator=self.estimator,
-            n_features_to_select=self.min_features_to_select,
+            n_features_to_select=min(self.min_features_to_select, n_features),
             importance_getter=self.importance_getter,
             step=self.step,
             verbose=self.verbose,
         )
 
         # Determine the number of subsets of features by fitting across
         # the train folds and choosing the "features_to_select" parameter
@@ -745,26 +770,26 @@
 
         if effective_n_jobs(self.n_jobs) == 1:
             parallel, func = list, _rfe_single_fit
         else:
             parallel = Parallel(n_jobs=self.n_jobs)
             func = delayed(_rfe_single_fit)
 
-        scores = parallel(
+        scores_features = parallel(
             func(rfe, self.estimator, X, y, train, test, scorer)
             for train, test in cv.split(X, y, groups)
         )
+        scores, step_n_features = zip(*scores_features)
 
+        step_n_features_rev = np.array(step_n_features[0])[::-1]
         scores = np.array(scores)
-        scores_sum = np.sum(scores, axis=0)
-        scores_sum_rev = scores_sum[::-1]
-        argmax_idx = len(scores_sum) - np.argmax(scores_sum_rev) - 1
-        n_features_to_select = max(
-            n_features - (argmax_idx * step), self.min_features_to_select
-        )
+
+        # Reverse order such that lowest number of features is selected in case of tie.
+        scores_sum_rev = np.sum(scores, axis=0)[::-1]
+        n_features_to_select = step_n_features_rev[np.argmax(scores_sum_rev)]
 
         # Re-execute an elimination with best_k over the whole set
         rfe = RFE(
             estimator=self.estimator,
             n_features_to_select=n_features_to_select,
             step=self.step,
             importance_getter=self.importance_getter,
@@ -778,15 +803,14 @@
         self.n_features_ = rfe.n_features_
         self.ranking_ = rfe.ranking_
         self.estimator_ = clone(self.estimator)
         self.estimator_.fit(self._transform(X), y)
 
         # reverse to stay consistent with before
         scores_rev = scores[:, ::-1]
-        self.cv_results_ = {}
-        self.cv_results_["mean_test_score"] = np.mean(scores_rev, axis=0)
-        self.cv_results_["std_test_score"] = np.std(scores_rev, axis=0)
-
-        for i in range(scores.shape[0]):
-            self.cv_results_[f"split{i}_test_score"] = scores_rev[i]
-
+        self.cv_results_ = {
+            "mean_test_score": np.mean(scores_rev, axis=0),
+            "std_test_score": np.std(scores_rev, axis=0),
+            **{f"split{i}_test_score": scores_rev[i] for i in range(scores.shape[0])},
+            "n_features": step_n_features_rev,
+        }
         return self
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_sequential.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_sequential.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Sequential feature selection
 """
+
 from numbers import Integral, Real
 
 import numpy as np
 
 from ..base import BaseEstimator, MetaEstimatorMixin, _fit_context, clone, is_classifier
 from ..metrics import get_scorer_names
 from ..model_selection import check_cv, cross_val_score
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_univariate_selection.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_univariate_selection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/_variance_threshold.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/_variance_threshold.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_base.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_chi2.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_chi2.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_feature_select.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_feature_select.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Todo: cross-check the F-value with stats model
 """
+
 import itertools
 import warnings
 
 import numpy as np
 import pytest
 from numpy.testing import assert_allclose
 from scipy import sparse, stats
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_from_model.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_from_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -404,15 +404,15 @@
 
     # check that if est doesn't have partial_fit, neither does SelectFromModel
     transformer = SelectFromModel(estimator=RandomForestClassifier())
     assert not hasattr(transformer, "partial_fit")
 
 
 def test_calling_fit_reinitializes():
-    est = LinearSVC(dual="auto", random_state=0)
+    est = LinearSVC(random_state=0)
     transformer = SelectFromModel(estimator=est)
     transformer.fit(data, y)
     transformer.set_params(estimator__C=100)
     transformer.fit(data, y)
     assert transformer.estimator_.C == 100
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_mutual_info.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_mutual_info.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import numpy as np
 import pytest
 
+from sklearn.datasets import make_classification, make_regression
 from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
 from sklearn.feature_selection._mutual_info import _compute_mi
 from sklearn.utils import check_random_state
 from sklearn.utils._testing import (
     assert_allclose,
     assert_array_equal,
 )
@@ -248,7 +249,22 @@
     X = rng.randint(100, size=(100, 10))
     X_float = X.astype(np.float64, copy=True)
     y = rng.randint(100, size=100)
 
     expected = mutual_info_regression(X_float, y, random_state=global_random_seed)
     result = mutual_info_regression(X, y, random_state=global_random_seed)
     assert_allclose(result, expected)
+
+
+@pytest.mark.parametrize(
+    "mutual_info_func, data_generator",
+    [
+        (mutual_info_regression, make_regression),
+        (mutual_info_classif, make_classification),
+    ],
+)
+def test_mutual_info_n_jobs(global_random_seed, mutual_info_func, data_generator):
+    """Check that results are consistent with different `n_jobs`."""
+    X, y = data_generator(random_state=global_random_seed)
+    single_job = mutual_info_func(X, y, random_state=global_random_seed, n_jobs=1)
+    multi_job = mutual_info_func(X, y, random_state=global_random_seed, n_jobs=2)
+    assert_allclose(single_job, multi_job)
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_rfe.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_rfe.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 import numpy as np
 import pytest
 from numpy.testing import assert_allclose, assert_array_almost_equal, assert_array_equal
 
 from sklearn.base import BaseEstimator, ClassifierMixin
 from sklearn.compose import TransformedTargetRegressor
 from sklearn.cross_decomposition import CCA, PLSCanonical, PLSRegression
-from sklearn.datasets import load_iris, make_friedman1
+from sklearn.datasets import load_iris, make_classification, make_friedman1
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.feature_selection import RFE, RFECV
 from sklearn.impute import SimpleImputer
 from sklearn.linear_model import LinearRegression, LogisticRegression
 from sklearn.metrics import get_scorer, make_scorer, zero_one_loss
 from sklearn.model_selection import GroupKFold, cross_val_score
 from sklearn.pipeline import make_pipeline
@@ -460,15 +460,15 @@
     "importance_getter", [attrgetter("regressor_.coef_"), "regressor_.coef_"]
 )
 @pytest.mark.parametrize("selector, expected_n_features", [(RFE, 5), (RFECV, 4)])
 def test_rfe_wrapped_estimator(importance_getter, selector, expected_n_features):
     # Non-regression test for
     # https://github.com/scikit-learn/scikit-learn/issues/15312
     X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
-    estimator = LinearSVR(dual="auto", random_state=0)
+    estimator = LinearSVR(random_state=0)
 
     log_estimator = TransformedTargetRegressor(
         regressor=estimator, func=np.log, inverse_func=np.exp
     )
 
     selector = selector(log_estimator, importance_getter=importance_getter)
     sel = selector.fit(X, y)
@@ -482,15 +482,15 @@
         ("random", AttributeError),
         (lambda x: x.importance, AttributeError),
     ],
 )
 @pytest.mark.parametrize("Selector", [RFE, RFECV])
 def test_rfe_importance_getter_validation(importance_getter, err_type, Selector):
     X, y = make_friedman1(n_samples=50, n_features=10, random_state=42)
-    estimator = LinearSVR(dual="auto")
+    estimator = LinearSVR()
     log_estimator = TransformedTargetRegressor(
         regressor=estimator, func=np.log, inverse_func=np.exp
     )
 
     with pytest.raises(err_type):
         model = Selector(log_estimator, importance_getter=importance_getter)
         model.fit(X, y)
@@ -533,25 +533,61 @@
     generator = check_random_state(global_random_seed)
     iris = load_iris()
     X = np.c_[iris.data, generator.normal(size=(len(iris.data), 6))]
     y = iris.target
 
     rfecv = RFECV(estimator=SVC(kernel="linear"))
     rfecv.fit(X, y)
-    n_split_keys = len(rfecv.cv_results_) - 2
-    split_keys = [f"split{i}_test_score" for i in range(n_split_keys)]
-
+    split_keys = [key for key in rfecv.cv_results_.keys() if "split" in key]
     cv_scores = np.asarray([rfecv.cv_results_[key] for key in split_keys])
     expected_mean = np.mean(cv_scores, axis=0)
     expected_std = np.std(cv_scores, axis=0)
 
     assert_allclose(rfecv.cv_results_["mean_test_score"], expected_mean)
     assert_allclose(rfecv.cv_results_["std_test_score"], expected_std)
 
 
+@pytest.mark.parametrize(
+    ["min_features_to_select", "n_features", "step", "cv_results_n_features"],
+    [
+        [1, 4, 1, np.array([1, 2, 3, 4])],
+        [1, 5, 1, np.array([1, 2, 3, 4, 5])],
+        [1, 4, 2, np.array([1, 2, 4])],
+        [1, 5, 2, np.array([1, 3, 5])],
+        [1, 4, 3, np.array([1, 4])],
+        [1, 5, 3, np.array([1, 2, 5])],
+        [1, 4, 4, np.array([1, 4])],
+        [1, 5, 4, np.array([1, 5])],
+        [4, 4, 2, np.array([4])],
+        [4, 5, 1, np.array([4, 5])],
+        [4, 5, 2, np.array([4, 5])],
+    ],
+)
+def test_rfecv_cv_results_n_features(
+    min_features_to_select,
+    n_features,
+    step,
+    cv_results_n_features,
+):
+    X, y = make_classification(
+        n_samples=20, n_features=n_features, n_informative=n_features, n_redundant=0
+    )
+    rfecv = RFECV(
+        estimator=SVC(kernel="linear"),
+        step=step,
+        min_features_to_select=min_features_to_select,
+    )
+    rfecv.fit(X, y)
+    assert_array_equal(rfecv.cv_results_["n_features"], cv_results_n_features)
+    assert all(
+        len(value) == len(rfecv.cv_results_["n_features"])
+        for value in rfecv.cv_results_.values()
+    )
+
+
 @pytest.mark.parametrize("ClsRFE", [RFE, RFECV])
 def test_multioutput(ClsRFE):
     X = np.random.normal(size=(10, 3))
     y = np.random.randint(2, size=(10, 2))
     clf = RandomForestClassifier(n_estimators=5)
     rfe_test = ClsRFE(clf)
     rfe_test.fit(X, y)
@@ -609,7 +645,24 @@
 
     outer_msg = "This 'RFE' has no attribute 'decision_function'"
     inner_msg = "'LinearRegression' object has no attribute 'decision_function'"
     with pytest.raises(AttributeError, match=outer_msg) as exec_info:
         rfe.fit(iris.data, iris.target).decision_function(iris.data)
     assert isinstance(exec_info.value.__cause__, AttributeError)
     assert inner_msg in str(exec_info.value.__cause__)
+
+
+@pytest.mark.parametrize(
+    "ClsRFE, param", [(RFE, "n_features_to_select"), (RFECV, "min_features_to_select")]
+)
+def test_rfe_n_features_to_select_warning(ClsRFE, param):
+    """Check if the correct warning is raised when trying to initialize a RFE
+    object with a n_features_to_select attribute larger than the number of
+    features present in the X variable that is passed to the fit method
+    """
+    X, y = make_classification(n_features=20, random_state=0)
+
+    with pytest.warns(UserWarning, match=f"{param}=21 > n_features=20"):
+        # Create RFE/RFECV with n_features_to_select/min_features_to_select
+        # larger than the number of features present in the X variable
+        clsrfe = ClsRFE(estimator=LogisticRegression(), **{param: 21})
+        clsrfe.fit(X, y)
```

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_sequential.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_sequential.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/feature_selection/tests/test_variance_threshold.py` & `scikit_learn-1.5.0rc1/sklearn/feature_selection/tests/test_variance_threshold.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/_gpc.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/_gpc.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/_gpr.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/_gpr.py`

 * *Files 0% similar despite different names*

```diff
@@ -452,17 +452,15 @@
             )
 
             if return_cov:
                 # Alg 2.1, page 19, line 6 -> K(X_test, X_test) - v^T. v
                 y_cov = self.kernel_(X) - V.T @ V
 
                 # undo normalisation
-                y_cov = np.outer(y_cov, self._y_train_std**2).reshape(
-                    *y_cov.shape, -1
-                )
+                y_cov = np.outer(y_cov, self._y_train_std**2).reshape(*y_cov.shape, -1)
                 # if y_cov has shape (n_samples, n_samples, 1), reshape to
                 # (n_samples, n_samples)
                 if y_cov.shape[2] == 1:
                     y_cov = np.squeeze(y_cov, axis=2)
 
                 return y_mean, y_cov
             elif return_std:
@@ -479,17 +477,15 @@
                     warnings.warn(
                         "Predicted variances smaller than 0. "
                         "Setting those variances to 0."
                     )
                     y_var[y_var_negative] = 0.0
 
                 # undo normalisation
-                y_var = np.outer(y_var, self._y_train_std**2).reshape(
-                    *y_var.shape, -1
-                )
+                y_var = np.outer(y_var, self._y_train_std**2).reshape(*y_var.shape, -1)
 
                 # if y_var has shape (n_samples, 1), reshape to (n_samples,)
                 if y_var.shape[1] == 1:
                     y_var = np.squeeze(y_var, axis=1)
 
                 return y_mean, np.sqrt(y_var)
             else:
```

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/kernels.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/kernels.py`

 * *Files 0% similar despite different names*

```diff
@@ -1746,17 +1746,15 @@
             if self.hyperparameter_length_scale.fixed:
                 # Hyperparameter l kept fixed
                 K_gradient = np.empty((X.shape[0], X.shape[0], 0))
                 return K, K_gradient
 
             # We need to recompute the pairwise dimension-wise distances
             if self.anisotropic:
-                D = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 / (
-                    length_scale**2
-                )
+                D = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 / (length_scale**2)
             else:
                 D = squareform(dists**2)[:, :, np.newaxis]
 
             if self.nu == 0.5:
                 denominator = np.sqrt(D.sum(axis=2))[:, :, np.newaxis]
                 divide_result = np.zeros_like(D)
                 np.divide(
```

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/tests/_mini_sequence_kernel.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/_mini_sequence_kernel.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/tests/test_gpc.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/test_gpc.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""Testing for Gaussian process classification """
+"""Testing for Gaussian process classification"""
 
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD 3 clause
 
 import warnings
 
 import numpy as np
@@ -214,27 +214,25 @@
         warnings.simplefilter("always")
         gpc_sum.fit(X, y)
 
         assert len(record) == 2
 
         assert issubclass(record[0].category, ConvergenceWarning)
         assert (
-            record[0].message.args[0]
-            == "The optimal value found for "
+            record[0].message.args[0] == "The optimal value found for "
             "dimension 0 of parameter "
             "k1__noise_level is close to the "
             "specified upper bound 0.001. "
             "Increasing the bound and calling "
             "fit again may find a better value."
         )
 
         assert issubclass(record[1].category, ConvergenceWarning)
         assert (
-            record[1].message.args[0]
-            == "The optimal value found for "
+            record[1].message.args[0] == "The optimal value found for "
             "dimension 0 of parameter "
             "k2__length_scale is close to the "
             "specified lower bound 1000.0. "
             "Decreasing the bound and calling "
             "fit again may find a better value."
         )
 
@@ -246,27 +244,25 @@
         warnings.simplefilter("always")
         gpc_dims.fit(X_tile, y)
 
         assert len(record) == 2
 
         assert issubclass(record[0].category, ConvergenceWarning)
         assert (
-            record[0].message.args[0]
-            == "The optimal value found for "
+            record[0].message.args[0] == "The optimal value found for "
             "dimension 0 of parameter "
             "length_scale is close to the "
             "specified upper bound 100.0. "
             "Increasing the bound and calling "
             "fit again may find a better value."
         )
 
         assert issubclass(record[1].category, ConvergenceWarning)
         assert (
-            record[1].message.args[0]
-            == "The optimal value found for "
+            record[1].message.args[0] == "The optimal value found for "
             "dimension 1 of parameter "
             "length_scale is close to the "
             "specified upper bound 100.0. "
             "Increasing the bound and calling "
             "fit again may find a better value."
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/tests/test_gpr.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/test_gpr.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""Testing for Gaussian process regression """
+"""Testing for Gaussian process regression"""
 
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # Modified by: Pete Green <p.l.green@liverpool.ac.uk>
 # License: BSD 3 clause
 
 import re
 import sys
@@ -489,27 +489,25 @@
         warnings.simplefilter("always")
         gpr_sum.fit(X, y)
 
         assert len(record) == 2
 
         assert issubclass(record[0].category, ConvergenceWarning)
         assert (
-            record[0].message.args[0]
-            == "The optimal value found for "
+            record[0].message.args[0] == "The optimal value found for "
             "dimension 0 of parameter "
             "k1__noise_level is close to the "
             "specified upper bound 0.001. "
             "Increasing the bound and calling "
             "fit again may find a better value."
         )
 
         assert issubclass(record[1].category, ConvergenceWarning)
         assert (
-            record[1].message.args[0]
-            == "The optimal value found for "
+            record[1].message.args[0] == "The optimal value found for "
             "dimension 0 of parameter "
             "k2__length_scale is close to the "
             "specified lower bound 1000.0. "
             "Decreasing the bound and calling "
             "fit again may find a better value."
         )
 
@@ -521,27 +519,25 @@
         warnings.simplefilter("always")
         gpr_dims.fit(X_tile, y)
 
         assert len(record) == 2
 
         assert issubclass(record[0].category, ConvergenceWarning)
         assert (
-            record[0].message.args[0]
-            == "The optimal value found for "
+            record[0].message.args[0] == "The optimal value found for "
             "dimension 0 of parameter "
             "length_scale is close to the "
             "specified lower bound 10.0. "
             "Decreasing the bound and calling "
             "fit again may find a better value."
         )
 
         assert issubclass(record[1].category, ConvergenceWarning)
         assert (
-            record[1].message.args[0]
-            == "The optimal value found for "
+            record[1].message.args[0] == "The optimal value found for "
             "dimension 1 of parameter "
             "length_scale is close to the "
             "specified lower bound 10.0. "
             "Decreasing the bound and calling "
             "fit again may find a better value."
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/gaussian_process/tests/test_kernels.py` & `scikit_learn-1.5.0rc1/sklearn/gaussian_process/tests/test_kernels.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/impute/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/impute/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Transformers for missing value imputation"""
+
 import typing
 
 from ._base import MissingIndicator, SimpleImputer
 from ._knn import KNNImputer
 
 if typing.TYPE_CHECKING:
     # Avoid errors in type checkers (e.g. mypy) for experimental estimators.
```

### Comparing `scikit-learn-1.4.2/sklearn/impute/_base.py` & `scikit_learn-1.5.0rc1/sklearn/impute/_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,30 +2,31 @@
 #          Sergey Feldman <sergeyfeldman@gmail.com>
 # License: BSD 3 clause
 
 import numbers
 import warnings
 from collections import Counter
 from functools import partial
+from typing import Callable
 
 import numpy as np
 import numpy.ma as ma
 from scipy import sparse as sp
 
 from ..base import BaseEstimator, TransformerMixin, _fit_context
-from ..utils import _is_pandas_na, is_scalar_nan
 from ..utils._mask import _get_mask
+from ..utils._missing import is_pandas_na, is_scalar_nan
 from ..utils._param_validation import MissingValues, StrOptions
 from ..utils.fixes import _mode
 from ..utils.sparsefuncs import _get_median
 from ..utils.validation import FLOAT_DTYPES, _check_feature_names_in, check_is_fitted
 
 
 def _check_inputs_dtype(X, missing_values):
-    if _is_pandas_na(missing_values):
+    if is_pandas_na(missing_values):
         # Allow using `pd.NA` as missing values to impute numerical arrays.
         return
     if X.dtype.kind in ("f", "i", "u") and not isinstance(missing_values, numbers.Real):
         raise ValueError(
             "'X' and 'missing_values' types are expected to be"
             " both numerical. Got X.dtype={} and "
             " type(missing_values)={}.".format(X.dtype, type(missing_values))
@@ -159,30 +160,36 @@
     ----------
     missing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan
         The placeholder for the missing values. All occurrences of
         `missing_values` will be imputed. For pandas' dataframes with
         nullable integer dtypes with missing values, `missing_values`
         can be set to either `np.nan` or `pd.NA`.
 
-    strategy : str, default='mean'
+    strategy : str or Callable, default='mean'
         The imputation strategy.
 
         - If "mean", then replace missing values using the mean along
           each column. Can only be used with numeric data.
         - If "median", then replace missing values using the median along
           each column. Can only be used with numeric data.
         - If "most_frequent", then replace missing using the most frequent
           value along each column. Can be used with strings or numeric data.
           If there is more than one such value, only the smallest is returned.
         - If "constant", then replace missing values with fill_value. Can be
           used with strings or numeric data.
+        - If an instance of Callable, then replace missing values using the
+          scalar statistic returned by running the callable over a dense 1d
+          array containing non-missing values of each column.
 
         .. versionadded:: 0.20
            strategy="constant" for fixed value imputation.
 
+        .. versionadded:: 1.5
+           strategy=callable for custom value imputation.
+
     fill_value : str or numerical value, default=None
         When strategy == "constant", `fill_value` is used to replace all
         occurrences of missing_values. For string or object data types,
         `fill_value` must be a string.
         If `None`, `fill_value` will be 0 when imputing numerical
         data and "missing_value" for strings or object data types.
 
@@ -266,15 +273,18 @@
 
     For a more detailed example see
     :ref:`sphx_glr_auto_examples_impute_plot_missing_values.py`.
     """
 
     _parameter_constraints: dict = {
         **_BaseImputer._parameter_constraints,
-        "strategy": [StrOptions({"mean", "median", "most_frequent", "constant"})],
+        "strategy": [
+            StrOptions({"mean", "median", "most_frequent", "constant"}),
+            callable,
+        ],
         "fill_value": "no_validation",  # any object is valid
         "copy": ["boolean"],
     }
 
     def __init__(
         self,
         *,
@@ -309,15 +319,15 @@
         else:
             dtype = FLOAT_DTYPES
 
         if not in_fit and self._fit_dtype.kind == "O":
             # Use object dtype if fitted on object dtypes
             dtype = self._fit_dtype
 
-        if _is_pandas_na(self.missing_values) or is_scalar_nan(self.missing_values):
+        if is_pandas_na(self.missing_values) or is_scalar_nan(self.missing_values):
             force_all_finite = "allow-nan"
         else:
             force_all_finite = True
 
         try:
             X = self._validate_data(
                 X,
@@ -464,14 +474,17 @@
 
                     elif strategy == "median":
                         statistics[i] = _get_median(column, n_zeros)
 
                     elif strategy == "most_frequent":
                         statistics[i] = _most_frequent(column, 0, n_zeros)
 
+                    elif isinstance(strategy, Callable):
+                        statistics[i] = self.strategy(column)
+
         super()._fit_indicator(missing_mask)
 
         return statistics
 
     def _dense_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on dense data."""
         missing_mask = _get_mask(X, missing_values)
@@ -526,14 +539,21 @@
 
         # Constant
         elif strategy == "constant":
             # for constant strategy, self.statistcs_ is used to store
             # fill_value in each column
             return np.full(X.shape[1], fill_value, dtype=X.dtype)
 
+        # Custom
+        elif isinstance(strategy, Callable):
+            statistics = np.empty(masked_X.shape[1])
+            for i in range(masked_X.shape[1]):
+                statistics[i] = self.strategy(masked_X[:, i].compressed())
+            return statistics
+
     def transform(self, X):
         """Impute all missing values in `X`.
 
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input data to complete.
@@ -677,17 +697,16 @@
                 original_idx += 1
 
         X_original[full_mask] = self.missing_values
         return X_original
 
     def _more_tags(self):
         return {
-            "allow_nan": _is_pandas_na(self.missing_values) or is_scalar_nan(
-                self.missing_values
-            )
+            "allow_nan": is_pandas_na(self.missing_values)
+            or is_scalar_nan(self.missing_values)
         }
 
     def get_feature_names_out(self, input_features=None):
         """Get output feature names for transformation.
 
         Parameters
         ----------
```

### Comparing `scikit-learn-1.4.2/sklearn/impute/_iterative.py` & `scikit_learn-1.5.0rc1/sklearn/impute/_iterative.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,24 +5,25 @@
 
 import numpy as np
 from scipy import stats
 
 from ..base import _fit_context, clone
 from ..exceptions import ConvergenceWarning
 from ..preprocessing import normalize
-from ..utils import (
-    _safe_assign,
-    _safe_indexing,
-    check_array,
-    check_random_state,
-    is_scalar_nan,
-)
+from ..utils import _safe_indexing, check_array, check_random_state
+from ..utils._indexing import _safe_assign
 from ..utils._mask import _get_mask
+from ..utils._missing import is_scalar_nan
 from ..utils._param_validation import HasMethods, Interval, StrOptions
-from ..utils.metadata_routing import _RoutingNotSupportedMixin
+from ..utils.metadata_routing import (
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    process_routing,
+)
 from ..utils.validation import FLOAT_DTYPES, _check_feature_names_in, check_is_fitted
 from ._base import SimpleImputer, _BaseImputer, _check_inputs_dtype
 
 _ImputerTriplet = namedtuple(
     "_ImputerTriplet", ["feat_idx", "neighbor_feat_idx", "estimator"]
 )
 
@@ -43,15 +44,15 @@
     """
     if hasattr(X1, "mask"):  # pandas dataframes
         X1.mask(cond=cond, other=X2, inplace=True)
     else:  # ndarrays
         X1[cond] = X2[cond]
 
 
-class IterativeImputer(_RoutingNotSupportedMixin, _BaseImputer):
+class IterativeImputer(_BaseImputer):
     """Multivariate imputer that estimates each feature from all the others.
 
     A strategy for imputing missing values by modeling each feature with
     missing values as a function of other features in a round-robin fashion.
 
     Read more in the :ref:`User Guide <iterative_imputer>`.
 
@@ -345,14 +346,15 @@
         self,
         X_filled,
         mask_missing_values,
         feat_idx,
         neighbor_feat_idx,
         estimator=None,
         fit_mode=True,
+        params=None,
     ):
         """Impute a single feature from the others provided.
 
         This function predicts the missing values of one of the features using
         the current estimates of all the other features. The `estimator` must
         support `return_std=True` in its `predict` method for this function
         to work.
@@ -376,14 +378,17 @@
             If `sample_posterior=True`, the estimator must support
             `return_std` in its `predict` method.
             If None, it will be cloned from self._estimator.
 
         fit_mode : boolean, default=True
             Whether to fit and predict with the estimator or just predict.
 
+        params : dict
+            Additional params routed to the individual estimator.
+
         Returns
         -------
         X_filled : ndarray
             Input data with `X_filled[missing_row_mask, feat_idx]` updated.
 
         estimator : estimator with sklearn API
             The fitted estimator used to impute
@@ -406,15 +411,15 @@
                 axis=0,
             )
             y_train = _safe_indexing(
                 _safe_indexing(X_filled, feat_idx, axis=1),
                 ~missing_row_mask,
                 axis=0,
             )
-            estimator.fit(X_train, y_train)
+            estimator.fit(X_train, y_train, **params)
 
         # if no missing values, don't predict
         if np.sum(missing_row_mask) == 0:
             return X_filled, estimator
 
         # get posterior samples if there is at least one missing value
         X_test = _safe_indexing(
@@ -681,31 +686,49 @@
             )
         return limit
 
     @_fit_context(
         # IterativeImputer.estimator is not validated yet
         prefer_skip_nested_validation=False
     )
-    def fit_transform(self, X, y=None):
+    def fit_transform(self, X, y=None, **params):
         """Fit the imputer on `X` and return the transformed `X`.
 
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
             Input data, where `n_samples` is the number of samples and
             `n_features` is the number of features.
 
         y : Ignored
             Not used, present for API consistency by convention.
 
+        **params : dict
+            Parameters routed to the `fit` method of the sub-estimator via the
+            metadata routing API.
+
+            .. versionadded:: 1.5
+              Only available if
+              `sklearn.set_config(enable_metadata_routing=True)` is set. See
+              :ref:`Metadata Routing User Guide <metadata_routing>` for more
+              details.
+
         Returns
         -------
         Xt : array-like, shape (n_samples, n_features)
             The imputed input data.
         """
+        _raise_for_params(params, self, "fit")
+
+        routed_params = process_routing(
+            self,
+            "fit",
+            **params,
+        )
+
         self.random_state_ = getattr(
             self, "random_state_", check_random_state(self.random_state)
         )
 
         if self.estimator is None:
             from ..linear_model import BayesianRidge
 
@@ -724,15 +747,15 @@
         super()._fit_indicator(complete_mask)
         X_indicator = super()._transform_indicator(complete_mask)
 
         if self.max_iter == 0 or np.all(mask_missing_values):
             self.n_iter_ = 0
             return super()._concatenate_indicator(Xt, X_indicator)
 
-        # Edge case: a single feature. We return the initial ...
+        # Edge case: a single feature, we return the initial imputation.
         if Xt.shape[1] == 1:
             self.n_iter_ = 0
             return super()._concatenate_indicator(Xt, X_indicator)
 
         self._min_value = self._validate_limit(self.min_value, "min", X.shape[1])
         self._max_value = self._validate_limit(self.max_value, "max", X.shape[1])
 
@@ -766,14 +789,15 @@
                 Xt, estimator = self._impute_one_feature(
                     Xt,
                     mask_missing_values,
                     feat_idx,
                     neighbor_feat_idx,
                     estimator=None,
                     fit_mode=True,
+                    params=routed_params.estimator.fit,
                 )
                 estimator_triplet = _ImputerTriplet(
                     feat_idx, neighbor_feat_idx, estimator
                 )
                 self.imputation_sequence_.append(estimator_triplet)
 
             if self.verbose > 1:
@@ -856,32 +880,42 @@
                     )
                 i_rnd += 1
 
         _assign_where(Xt, X, cond=~mask_missing_values)
 
         return super()._concatenate_indicator(Xt, X_indicator)
 
-    def fit(self, X, y=None):
+    def fit(self, X, y=None, **fit_params):
         """Fit the imputer on `X` and return self.
 
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
             Input data, where `n_samples` is the number of samples and
             `n_features` is the number of features.
 
         y : Ignored
             Not used, present for API consistency by convention.
 
+        **fit_params : dict
+            Parameters routed to the `fit` method of the sub-estimator via the
+            metadata routing API.
+
+            .. versionadded:: 1.5
+              Only available if
+              `sklearn.set_config(enable_metadata_routing=True)` is set. See
+              :ref:`Metadata Routing User Guide <metadata_routing>` for more
+              details.
+
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        self.fit_transform(X)
+        self.fit_transform(X, **fit_params)
         return self
 
     def get_feature_names_out(self, input_features=None):
         """Get output feature names for transformation.
 
         Parameters
         ----------
@@ -900,7 +934,27 @@
         feature_names_out : ndarray of str objects
             Transformed feature names.
         """
         check_is_fitted(self, "n_features_in_")
         input_features = _check_feature_names_in(self, input_features)
         names = self.initial_imputer_.get_feature_names_out(input_features)
         return self._concatenate_indicator_feature_names_out(names, input_features)
+
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
+
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__).add(
+            estimator=self.estimator,
+            method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+        )
+        return router
```

### Comparing `scikit-learn-1.4.2/sklearn/impute/_knn.py` & `scikit_learn-1.5.0rc1/sklearn/impute/_knn.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,16 +6,16 @@
 
 import numpy as np
 
 from ..base import _fit_context
 from ..metrics import pairwise_distances_chunked
 from ..metrics.pairwise import _NAN_METRICS
 from ..neighbors._base import _get_weights
-from ..utils import is_scalar_nan
 from ..utils._mask import _get_mask
+from ..utils._missing import is_scalar_nan
 from ..utils._param_validation import Hidden, Interval, StrOptions
 from ..utils.validation import FLOAT_DTYPES, _check_feature_names_in, check_is_fitted
 from ._base import _BaseImputer
 
 
 class KNNImputer(_BaseImputer):
     """Imputation for completing missing values using k-Nearest Neighbors.
```

### Comparing `scikit-learn-1.4.2/sklearn/impute/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/impute/tests/test_base.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/impute/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/impute/tests/test_common.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/impute/tests/test_impute.py` & `scikit_learn-1.5.0rc1/sklearn/impute/tests/test_impute.py`

 * *Files 1% similar despite different names*

```diff
@@ -1711,14 +1711,48 @@
                 X_imputed[:, 0].toarray() if array_type == "sparse" else X_imputed[:, 0]
             )
             assert_array_equal(constant_feature, 0)
         else:
             assert X_imputed.shape == (X.shape[0], X.shape[1] - 1)
 
 
+@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
+def test_imputation_custom(csc_container):
+    X = np.array(
+        [
+            [1.1, 1.1, 1.1],
+            [3.9, 1.2, np.nan],
+            [np.nan, 1.3, np.nan],
+            [0.1, 1.4, 1.4],
+            [4.9, 1.5, 1.5],
+            [np.nan, 1.6, 1.6],
+        ]
+    )
+
+    X_true = np.array(
+        [
+            [1.1, 1.1, 1.1],
+            [3.9, 1.2, 1.1],
+            [0.1, 1.3, 1.1],
+            [0.1, 1.4, 1.4],
+            [4.9, 1.5, 1.5],
+            [0.1, 1.6, 1.6],
+        ]
+    )
+
+    imputer = SimpleImputer(missing_values=np.nan, strategy=np.min)
+    X_trans = imputer.fit_transform(X)
+    assert_array_equal(X_trans, X_true)
+
+    # Sparse matrix
+    imputer = SimpleImputer(missing_values=np.nan, strategy=np.min)
+    X_trans = imputer.fit_transform(csc_container(X))
+    assert_array_equal(X_trans.toarray(), X_true)
+
+
 def test_simple_imputer_constant_fill_value_casting():
     """Check that we raise a proper error message when we cannot cast the fill value
     to the input data type. Otherwise, check that the casting is done properly.
 
     Non-regression test for:
     https://github.com/scikit-learn/scikit-learn/issues/28309
     """
```

### Comparing `scikit-learn-1.4.2/sklearn/impute/tests/test_knn.py` & `scikit_learn-1.5.0rc1/sklearn/impute/tests/test_knn.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_partial_dependence.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_partial_dependence.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,23 +15,17 @@
 from ..ensemble import RandomForestRegressor
 from ..ensemble._gb import BaseGradientBoosting
 from ..ensemble._hist_gradient_boosting.gradient_boosting import (
     BaseHistGradientBoosting,
 )
 from ..exceptions import NotFittedError
 from ..tree import DecisionTreeRegressor
-from ..utils import (
-    Bunch,
-    _determine_key_type,
-    _get_column_indices,
-    _safe_assign,
-    _safe_indexing,
-    check_array,
-    check_matplotlib_support,  # noqa
-)
+from ..utils import Bunch, _safe_indexing, check_array
+from ..utils._indexing import _determine_key_type, _get_column_indices, _safe_assign
+from ..utils._optional_dependencies import check_matplotlib_support  # noqa
 from ..utils._param_validation import (
     HasMethods,
     Integral,
     Interval,
     StrOptions,
     validate_params,
 )
@@ -530,22 +524,14 @@
         average : ndarray of shape (n_outputs, len(values[0]), \
                 len(values[1]), ...)
             The predictions for all the points in the grid, averaged
             over all samples in X (or over the training data if
             `method` is 'recursion').
             Only available when `kind='average'` or `kind='both'`.
 
-        values : seq of 1d ndarrays
-            The values with which the grid has been created.
-
-            .. deprecated:: 1.3
-                The key `values` has been deprecated in 1.3 and will be removed
-                in 1.5 in favor of `grid_values`. See `grid_values` for details
-                about the `values` attribute.
-
         grid_values : seq of 1d ndarrays
             The values with which the grid has been created. The generated
             grid is a cartesian product of the arrays in `grid_values` where
             `len(grid_values) == len(features)`. The size of each array
             `grid_values[j]` is either `grid_resolution`, or the number of
             unique values in `X[:, j]`, whichever is smaller.
 
@@ -656,15 +642,15 @@
         # _get_column_indices() supports negative indexing. Here, we limit
         # the indexing to be positive. The upper bound will be checked
         # by _get_column_indices()
         if np.any(np.less(features, 0)):
             raise ValueError("all features must be in [0, {}]".format(X.shape[1] - 1))
 
     features_indices = np.asarray(
-        _get_column_indices(X, features), dtype=np.int32, order="C"
+        _get_column_indices(X, features), dtype=np.intp, order="C"
     ).ravel()
 
     feature_names = _check_feature_names(X, feature_names)
 
     n_features = X.shape[1]
     if categorical_features is None:
         is_categorical = [False] * len(features_indices)
@@ -718,23 +704,15 @@
         )
 
     # reshape averaged_predictions to
     # (n_outputs, n_values_feature_0, n_values_feature_1, ...)
     averaged_predictions = averaged_predictions.reshape(
         -1, *[val.shape[0] for val in values]
     )
-    pdp_results = Bunch()
-
-    msg = (
-        "Key: 'values', is deprecated in 1.3 and will be removed in 1.5. "
-        "Please use 'grid_values' instead."
-    )
-    pdp_results._set_deprecated(
-        values, new_key="grid_values", deprecated_key="values", warning_message=msg
-    )
+    pdp_results = Bunch(grid_values=values)
 
     if kind == "average":
         pdp_results["average"] = averaged_predictions
     elif kind == "individual":
         pdp_results["individual"] = predictions
     else:  # kind='both'
         pdp_results["average"] = averaged_predictions
```

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_pd_utils.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_pd_utils.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_permutation_importance.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_permutation_importance.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 
 import numbers
 
 import numpy as np
 
 from ..ensemble._bagging import _generate_indices
 from ..metrics import check_scoring, get_scorer_names
-from ..metrics._scorer import _check_multimetric_scoring, _MultimetricScorer
 from ..model_selection._validation import _aggregate_score_dicts
 from ..utils import Bunch, _safe_indexing, check_array, check_random_state
 from ..utils._param_validation import (
     HasMethods,
     Integral,
     Interval,
     RealNotInt,
@@ -275,22 +274,15 @@
     random_seed = random_state.randint(np.iinfo(np.int32).max + 1)
 
     if not isinstance(max_samples, numbers.Integral):
         max_samples = int(max_samples * X.shape[0])
     elif max_samples > X.shape[0]:
         raise ValueError("max_samples must be <= n_samples")
 
-    if callable(scoring):
-        scorer = scoring
-    elif scoring is None or isinstance(scoring, str):
-        scorer = check_scoring(estimator, scoring=scoring)
-    else:
-        scorers_dict = _check_multimetric_scoring(estimator, scoring)
-        scorer = _MultimetricScorer(scorers=scorers_dict)
-
+    scorer = check_scoring(estimator, scoring=scoring)
     baseline_score = _weights_scorer(scorer, estimator, X, y, sample_weight)
 
     scores = Parallel(n_jobs=n_jobs)(
         delayed(_calculate_permutation_scores)(
             estimator,
             X,
             y,
```

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_plot/decision_boundary.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_plot/decision_boundary.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 import numpy as np
 
 from ...base import is_regressor
 from ...preprocessing import LabelEncoder
-from ...utils import _safe_indexing, check_matplotlib_support
+from ...utils import _safe_indexing
+from ...utils._optional_dependencies import check_matplotlib_support
 from ...utils._response import _get_response_values
+from ...utils._set_output import _get_adapter_from_container
 from ...utils.validation import (
     _is_arraylike_not_scalar,
+    _is_pandas_df,
+    _is_polars_df,
     _num_features,
     check_is_fitted,
 )
 
 
 def _check_boundary_response_method(estimator, response_method, class_of_interest):
     """Validate the response methods to be used with the fitted estimator.
@@ -340,21 +344,23 @@
         x0_min, x0_max = x0.min() - eps, x0.max() + eps
         x1_min, x1_max = x1.min() - eps, x1.max() + eps
 
         xx0, xx1 = np.meshgrid(
             np.linspace(x0_min, x0_max, grid_resolution),
             np.linspace(x1_min, x1_max, grid_resolution),
         )
-        if hasattr(X, "iloc"):
-            # we need to preserve the feature names and therefore get an empty dataframe
-            X_grid = X.iloc[[], :].copy()
-            X_grid.iloc[:, 0] = xx0.ravel()
-            X_grid.iloc[:, 1] = xx1.ravel()
-        else:
-            X_grid = np.c_[xx0.ravel(), xx1.ravel()]
+
+        X_grid = np.c_[xx0.ravel(), xx1.ravel()]
+        if _is_pandas_df(X) or _is_polars_df(X):
+            adapter = _get_adapter_from_container(X)
+            X_grid = adapter.create_container(
+                X_grid,
+                X_grid,
+                columns=X.columns,
+            )
 
         prediction_method = _check_boundary_response_method(
             estimator, response_method, class_of_interest
         )
         try:
             response, _, response_method_used = _get_response_values(
                 estimator,
```

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_plot/partial_dependence.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_plot/partial_dependence.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,18 +7,18 @@
 from scipy.stats.mstats import mquantiles
 
 from ...base import is_regressor
 from ...utils import (
     Bunch,
     _safe_indexing,
     check_array,
-    check_matplotlib_support,  # noqa
     check_random_state,
 )
 from ...utils._encode import _unique
+from ...utils._optional_dependencies import check_matplotlib_support  # noqa
 from ...utils.parallel import Parallel, delayed
 from .. import partial_dependence
 from .._pd_utils import _check_feature_names, _get_feature_index
 
 
 class PartialDependenceDisplay:
     """Partial Dependence Plot (PDP).
```

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_plot/tests/test_boundary_decision_display.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_plot/tests/test_boundary_decision_display.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 from sklearn.ensemble import IsolationForest
 from sklearn.inspection import DecisionBoundaryDisplay
 from sklearn.inspection._plot.decision_boundary import _check_boundary_response_method
 from sklearn.linear_model import LogisticRegression
 from sklearn.preprocessing import scale
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
 from sklearn.utils._testing import (
+    _convert_container,
     assert_allclose,
     assert_array_equal,
 )
 
 # TODO: Remove when https://github.com/numpy/numpy/issues/14397 is resolved
 pytestmark = pytest.mark.filterwarnings(
     "ignore:In future, it will be an error for 'np.bool_':DeprecationWarning:"
@@ -464,23 +465,26 @@
         log_reg,
         X,
         grid_resolution=5,
         response_method="predict",
     )
 
 
-def test_dataframe_support(pyplot):
+@pytest.mark.parametrize("constructor_name", ["pandas", "polars"])
+def test_dataframe_support(pyplot, constructor_name):
     """Check that passing a dataframe at fit and to the Display does not
     raise warnings.
 
     Non-regression test for:
-    https://github.com/scikit-learn/scikit-learn/issues/23311
+    * https://github.com/scikit-learn/scikit-learn/issues/23311
+    * https://github.com/scikit-learn/scikit-learn/issues/28717
     """
-    pd = pytest.importorskip("pandas")
-    df = pd.DataFrame(X, columns=["col_x", "col_y"])
+    df = _convert_container(
+        X, constructor_name=constructor_name, columns_name=["col_x", "col_y"]
+    )
     estimator = LogisticRegression().fit(df, y)
 
     with warnings.catch_warnings():
         # no warnings linked to feature names validation should be raised
         warnings.simplefilter("error", UserWarning)
         DecisionBoundaryDisplay.from_estimator(estimator, df, response_method="predict")
```

### Comparing `scikit-learn-1.4.2/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/inspection/tests/test_partial_dependence.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/tests/test_partial_dependence.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 """
 Testing for the partial dependence module.
 """
-import warnings
 
 import numpy as np
 import pytest
 
 import sklearn
 from sklearn.base import BaseEstimator, ClassifierMixin, clone, is_regressor
 from sklearn.cluster import KMeans
@@ -33,16 +32,16 @@
     PolynomialFeatures,
     RobustScaler,
     StandardScaler,
     scale,
 )
 from sklearn.tree import DecisionTreeRegressor
 from sklearn.tree.tests.test_tree import assert_is_subtree
-from sklearn.utils import _IS_32BIT
 from sklearn.utils._testing import assert_allclose, assert_array_equal
+from sklearn.utils.fixes import _IS_32BIT
 from sklearn.utils.validation import check_random_state
 
 # toy sample
 X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
 y = [-1, -1, -1, 1, 1, 1]
 
 
@@ -268,15 +267,15 @@
     # The 'init' estimator for GBDT (here the average prediction) isn't taken
     # into account with the recursion method, for technical reasons. We set
     # the mean to 0 to that this 'bug' doesn't have any effect.
     y = y - y.mean()
     est.fit(X, y)
 
     # target feature will be set to .5 and then to 123
-    features = np.array([target_feature], dtype=np.int32)
+    features = np.array([target_feature], dtype=np.intp)
     grid = np.array([[0.5], [123]])
 
     if method == "brute":
         pdp, predictions = _partial_dependence_brute(
             est, grid, features, X, response_method="auto"
         )
     else:
@@ -352,15 +351,15 @@
         # cannot be equal either. See
         # https://github.com/scikit-learn/scikit-learn/issues/8853
         assert _IS_32BIT, "this should only fail on 32 bit platforms"
         return
 
     grid = rng.randn(50).reshape(-1, 1)
     for f in range(n_features):
-        features = np.array([f], dtype=np.int32)
+        features = np.array([f], dtype=np.intp)
 
         pdp_forest = _partial_dependence_recursion(forest, grid, features)
         pdp_gbdt = _partial_dependence_recursion(gbdt, grid, features)
         pdp_tree = _partial_dependence_recursion(tree, grid, features)
 
         np.testing.assert_allclose(pdp_gbdt, pdp_tree)
         np.testing.assert_allclose(pdp_forest, pdp_tree)
@@ -910,42 +909,14 @@
 
     with pytest.raises(ValueError, match="'recursion' method can only be applied when"):
         partial_dependence(
             est, X, features=[0], method="recursion", sample_weight=sample_weight
         )
 
 
-# TODO(1.5): Remove when bunch values is deprecated in 1.5
-def test_partial_dependence_bunch_values_deprecated():
-    """Test that deprecation warning is raised when values is accessed."""
-
-    est = LogisticRegression()
-    (X, y), _ = binary_classification_data
-    est.fit(X, y)
-
-    pdp_avg = partial_dependence(est, X=X, features=[1, 2], kind="average")
-
-    msg = (
-        "Key: 'values', is deprecated in 1.3 and will be "
-        "removed in 1.5. Please use 'grid_values' instead"
-    )
-
-    with warnings.catch_warnings():
-        # Does not raise warnings with "grid_values"
-        warnings.simplefilter("error", FutureWarning)
-        grid_values = pdp_avg["grid_values"]
-
-    with pytest.warns(FutureWarning, match=msg):
-        # Warns for "values"
-        values = pdp_avg["values"]
-
-    # "values" and "grid_values" are the same object
-    assert values is grid_values
-
-
 def test_mixed_type_categorical():
     """Check that we raise a proper error when a column has mixed types and
     the sorting of `np.unique` will fail."""
     X = np.array(["A", "B", "C", np.nan], dtype=object).reshape(-1, 1)
     y = np.array([0, 1, 0, 1])
 
     from sklearn.preprocessing import OrdinalEncoder
```

### Comparing `scikit-learn-1.4.2/sklearn/inspection/tests/test_pd_utils.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/tests/test_pd_utils.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/inspection/tests/test_permutation_importance.py` & `scikit_learn-1.5.0rc1/sklearn/inspection/tests/test_permutation_importance.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import numpy as np
 import pytest
+from joblib import parallel_backend
 from numpy.testing import assert_allclose
 
 from sklearn.compose import ColumnTransformer
 from sklearn.datasets import (
     load_diabetes,
     load_iris,
     make_classification,
@@ -18,15 +19,14 @@
     get_scorer,
     mean_squared_error,
     r2_score,
 )
 from sklearn.model_selection import train_test_split
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler, scale
-from sklearn.utils import parallel_backend
 from sklearn.utils._testing import _convert_container
 
 
 @pytest.mark.parametrize("n_jobs", [1, 2])
 @pytest.mark.parametrize("max_samples", [0.5, 1.0])
 @pytest.mark.parametrize("sample_weight", [None, "ones"])
 def test_permutation_importance_correlated_feature_regression(
@@ -433,17 +433,15 @@
     x1_x2_imp_ratio_w_ones = pi.importances_mean[0] / pi.importances_mean[1]
     assert x1_x2_imp_ratio_w_ones == pytest.approx(x1_x2_imp_ratio_w_none, 0.01)
 
     # When the ratio between the weights of the first half of the samples and
     # the second half of the samples approaches to infinity, the ratio of
     # the two features importance should equal to 2 on expectation (when using
     # mean absolutes error as the loss function).
-    w = np.hstack(
-        [np.repeat(10.0**10, n_half_samples), np.repeat(1.0, n_half_samples)]
-    )
+    w = np.hstack([np.repeat(10.0**10, n_half_samples), np.repeat(1.0, n_half_samples)])
     lr.fit(x, y, w)
     pi = permutation_importance(
         lr,
         x,
         y,
         random_state=1,
         scoring="neg_mean_absolute_error",
```

### Comparing `scikit-learn-1.4.2/sklearn/isotonic.py` & `scikit_learn-1.5.0rc1/sklearn/isotonic.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/kernel_approximation.py` & `scikit_learn-1.5.0rc1/sklearn/kernel_approximation.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 from .base import (
     BaseEstimator,
     ClassNamePrefixFeaturesOutMixin,
     TransformerMixin,
     _fit_context,
 )
 from .metrics.pairwise import KERNEL_PARAMS, PAIRWISE_KERNEL_FUNCTIONS, pairwise_kernels
-from .utils import check_random_state, deprecated
+from .utils import check_random_state
 from .utils._param_validation import Interval, StrOptions
 from .utils.extmath import safe_sparse_dot
 from .utils.validation import (
     _check_feature_names_in,
     check_is_fitted,
     check_non_negative,
 )
@@ -596,21 +596,14 @@
         Gives the number of (complex) sampling points.
 
     sample_interval : float, default=None
         Sampling interval. Must be specified when sample_steps not in {1,2,3}.
 
     Attributes
     ----------
-    sample_interval_ : float
-        Stored sampling interval. Specified as a parameter if `sample_steps`
-        not in {1,2,3}.
-
-        .. deprecated:: 1.3
-           `sample_interval_` serves internal purposes only and will be removed in 1.5.
-
     n_features_in_ : int
         Number of features seen during :term:`fit`.
 
         .. versionadded:: 0.24
 
     feature_names_in_ : ndarray of shape (`n_features_in_`,)
         Names of features seen during :term:`fit`. Defined only when `X`
@@ -689,45 +682,22 @@
         -------
         self : object
             Returns the transformer.
         """
         X = self._validate_data(X, accept_sparse="csr")
         check_non_negative(X, "X in AdditiveChi2Sampler.fit")
 
-        # TODO(1.5): remove the setting of _sample_interval from fit
-        if self.sample_interval is None:
-            # See figure 2 c) of "Efficient additive kernels via explicit feature maps"
-            # <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>
-            # A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,
-            # 2011
-            if self.sample_steps == 1:
-                self._sample_interval = 0.8
-            elif self.sample_steps == 2:
-                self._sample_interval = 0.5
-            elif self.sample_steps == 3:
-                self._sample_interval = 0.4
-            else:
-                raise ValueError(
-                    "If sample_steps is not in [1, 2, 3],"
-                    " you need to provide sample_interval"
-                )
-        else:
-            self._sample_interval = self.sample_interval
+        if self.sample_interval is None and self.sample_steps not in (1, 2, 3):
+            raise ValueError(
+                "If sample_steps is not in [1, 2, 3],"
+                " you need to provide sample_interval"
+            )
 
         return self
 
-    # TODO(1.5): remove
-    @deprecated(  # type: ignore
-        "The ``sample_interval_`` attribute was deprecated in version 1.3 and "
-        "will be removed 1.5."
-    )
-    @property
-    def sample_interval_(self):
-        return self._sample_interval
-
     def transform(self, X):
         """Apply approximate feature map to X.
 
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
             Training data, where `n_samples` is the number of samples
@@ -740,37 +710,32 @@
             Whether the return value is an array or sparse matrix depends on
             the type of the input X.
         """
         X = self._validate_data(X, accept_sparse="csr", reset=False)
         check_non_negative(X, "X in AdditiveChi2Sampler.transform")
         sparse = sp.issparse(X)
 
-        if hasattr(self, "_sample_interval"):
-            # TODO(1.5): remove this branch
-            sample_interval = self._sample_interval
-
-        else:
-            if self.sample_interval is None:
-                # See figure 2 c) of "Efficient additive kernels via explicit feature maps" # noqa
-                # <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>
-                # A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, # noqa
-                # 2011
-                if self.sample_steps == 1:
-                    sample_interval = 0.8
-                elif self.sample_steps == 2:
-                    sample_interval = 0.5
-                elif self.sample_steps == 3:
-                    sample_interval = 0.4
-                else:
-                    raise ValueError(
-                        "If sample_steps is not in [1, 2, 3],"
-                        " you need to provide sample_interval"
-                    )
+        if self.sample_interval is None:
+            # See figure 2 c) of "Efficient additive kernels via explicit feature maps" # noqa
+            # <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>
+            # A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, # noqa
+            # 2011
+            if self.sample_steps == 1:
+                sample_interval = 0.8
+            elif self.sample_steps == 2:
+                sample_interval = 0.5
+            elif self.sample_steps == 3:
+                sample_interval = 0.4
             else:
-                sample_interval = self.sample_interval
+                raise ValueError(
+                    "If sample_steps is not in [1, 2, 3],"
+                    " you need to provide sample_interval"
+                )
+        else:
+            sample_interval = self.sample_interval
 
         # zeroth component
         # 1/cosh = sech
         # cosh(0) = 1.0
         transf = self._transform_sparse if sparse else self._transform_dense
         return transf(X, self.sample_steps, sample_interval)
 
@@ -964,21 +929,21 @@
 
     Examples
     --------
     >>> from sklearn import datasets, svm
     >>> from sklearn.kernel_approximation import Nystroem
     >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)
     >>> data = X / 16.
-    >>> clf = svm.LinearSVC(dual="auto")
+    >>> clf = svm.LinearSVC()
     >>> feature_map_nystroem = Nystroem(gamma=.2,
     ...                                 random_state=1,
     ...                                 n_components=300)
     >>> data_transformed = feature_map_nystroem.fit_transform(data)
     >>> clf.fit(data_transformed, y)
-    LinearSVC(dual='auto')
+    LinearSVC()
     >>> clf.score(data_transformed, y)
     0.9987...
     """
 
     _parameter_constraints: dict = {
         "kernel": [
             StrOptions(set(PAIRWISE_KERNEL_FUNCTIONS.keys()) | {"precomputed"}),
```

### Comparing `scikit-learn-1.4.2/sklearn/kernel_ridge.py` & `scikit_learn-1.5.0rc1/sklearn/kernel_ridge.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_base.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_base.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,25 +29,32 @@
     BaseEstimator,
     ClassifierMixin,
     MultiOutputMixin,
     RegressorMixin,
     _fit_context,
 )
 from ..utils import check_array, check_random_state
-from ..utils._array_api import get_namespace
+from ..utils._array_api import (
+    _asarray_with_order,
+    _average,
+    get_namespace,
+    get_namespace_and_device,
+    indexing_dtype,
+    supported_float_dtypes,
+)
 from ..utils._seq_dataset import (
     ArrayDataset32,
     ArrayDataset64,
     CSRDataset32,
     CSRDataset64,
 )
 from ..utils.extmath import safe_sparse_dot
 from ..utils.parallel import Parallel, delayed
 from ..utils.sparsefuncs import mean_variance_axis
-from ..utils.validation import FLOAT_DTYPES, _check_sample_weight, check_is_fitted
+from ..utils.validation import _check_sample_weight, check_is_fitted
 
 # TODO: bayesian_ridge_regression and bayesian_regression_ard
 # should be squashed into its respective objects.
 
 SPARSE_INTERCEPT_DECAY = 0.01
 # For sparse data intercept updates are scaled by this decay factor to avoid
 # intercept oscillation.
@@ -151,51 +158,59 @@
     X_offset : ndarray of shape (n_features,)
         The mean per column of input X.
     y_offset : float or ndarray of shape (n_features,)
     X_scale : ndarray of shape (n_features,)
         Always an array of ones. TODO: refactor the code base to make it
         possible to remove this unused variable.
     """
+    xp, _, device_ = get_namespace_and_device(X, y, sample_weight)
+    n_samples, n_features = X.shape
+    X_is_sparse = sp.issparse(X)
+
     if isinstance(sample_weight, numbers.Number):
         sample_weight = None
     if sample_weight is not None:
-        sample_weight = np.asarray(sample_weight)
+        sample_weight = xp.asarray(sample_weight)
 
     if check_input:
-        X = check_array(X, copy=copy, accept_sparse=["csr", "csc"], dtype=FLOAT_DTYPES)
+        X = check_array(
+            X, copy=copy, accept_sparse=["csr", "csc"], dtype=supported_float_dtypes(xp)
+        )
         y = check_array(y, dtype=X.dtype, copy=copy_y, ensure_2d=False)
     else:
-        y = y.astype(X.dtype, copy=copy_y)
+        y = xp.astype(y, X.dtype, copy=copy_y)
         if copy:
-            if sp.issparse(X):
+            if X_is_sparse:
                 X = X.copy()
             else:
-                X = X.copy(order="K")
+                X = _asarray_with_order(X, order="K", copy=True, xp=xp)
+
+    dtype_ = X.dtype
 
     if fit_intercept:
-        if sp.issparse(X):
+        if X_is_sparse:
             X_offset, X_var = mean_variance_axis(X, axis=0, weights=sample_weight)
         else:
-            X_offset = np.average(X, axis=0, weights=sample_weight)
+            X_offset = _average(X, axis=0, weights=sample_weight, xp=xp)
 
-            X_offset = X_offset.astype(X.dtype, copy=False)
+            X_offset = xp.astype(X_offset, X.dtype, copy=False)
             X -= X_offset
 
-        y_offset = np.average(y, axis=0, weights=sample_weight)
+        y_offset = _average(y, axis=0, weights=sample_weight, xp=xp)
         y -= y_offset
     else:
-        X_offset = np.zeros(X.shape[1], dtype=X.dtype)
+        X_offset = xp.zeros(n_features, dtype=X.dtype, device=device_)
         if y.ndim == 1:
-            y_offset = X.dtype.type(0)
+            y_offset = xp.asarray(0.0, dtype=dtype_, device=device_)
         else:
-            y_offset = np.zeros(y.shape[1], dtype=X.dtype)
+            y_offset = xp.zeros(y.shape[1], dtype=dtype_, device=device_)
 
     # XXX: X_scale is no longer needed. It is an historic artifact from the
     # time where linear model exposed the normalize parameter.
-    X_scale = np.ones(X.shape[1], dtype=X.dtype)
+    X_scale = xp.ones(n_features, dtype=X.dtype, device=device_)
     return X, y, X_offset, y_offset, X_scale
 
 
 # TODO: _rescale_data should be factored into _preprocess_data.
 # Currently, the fact that sag implements its own way to deal with
 # sample_weight makes the refactoring tricky.
 
@@ -220,58 +235,63 @@
     -------
     X_rescaled : {array-like, sparse matrix}
 
     y_rescaled : {array-like, sparse matrix}
     """
     # Assume that _validate_data and _check_sample_weight have been called by
     # the caller.
+    xp, _ = get_namespace(X, y, sample_weight)
     n_samples = X.shape[0]
-    sample_weight_sqrt = np.sqrt(sample_weight)
+    sample_weight_sqrt = xp.sqrt(sample_weight)
 
     if sp.issparse(X) or sp.issparse(y):
         sw_matrix = sparse.dia_matrix(
             (sample_weight_sqrt, 0), shape=(n_samples, n_samples)
         )
 
     if sp.issparse(X):
         X = safe_sparse_dot(sw_matrix, X)
     else:
         if inplace:
-            X *= sample_weight_sqrt[:, np.newaxis]
+            X *= sample_weight_sqrt[:, None]
         else:
-            X = X * sample_weight_sqrt[:, np.newaxis]
+            X = X * sample_weight_sqrt[:, None]
 
     if sp.issparse(y):
         y = safe_sparse_dot(sw_matrix, y)
     else:
         if inplace:
             if y.ndim == 1:
                 y *= sample_weight_sqrt
             else:
-                y *= sample_weight_sqrt[:, np.newaxis]
+                y *= sample_weight_sqrt[:, None]
         else:
             if y.ndim == 1:
                 y = y * sample_weight_sqrt
             else:
-                y = y * sample_weight_sqrt[:, np.newaxis]
+                y = y * sample_weight_sqrt[:, None]
     return X, y, sample_weight_sqrt
 
 
 class LinearModel(BaseEstimator, metaclass=ABCMeta):
     """Base class for Linear Models"""
 
     @abstractmethod
     def fit(self, X, y):
         """Fit model."""
 
     def _decision_function(self, X):
         check_is_fitted(self)
 
         X = self._validate_data(X, accept_sparse=["csr", "csc", "coo"], reset=False)
-        return safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_
+        coef_ = self.coef_
+        if coef_.ndim == 1:
+            return X @ coef_ + self.intercept_
+        else:
+            return X @ coef_.T + self.intercept_
 
     def predict(self, X):
         """
         Predict using the linear model.
 
         Parameters
         ----------
@@ -283,19 +303,30 @@
         C : array, shape (n_samples,)
             Returns predicted values.
         """
         return self._decision_function(X)
 
     def _set_intercept(self, X_offset, y_offset, X_scale):
         """Set the intercept_"""
+
+        xp, _ = get_namespace(X_offset, y_offset, X_scale)
+
         if self.fit_intercept:
             # We always want coef_.dtype=X.dtype. For instance, X.dtype can differ from
             # coef_.dtype if warm_start=True.
-            self.coef_ = np.divide(self.coef_, X_scale, dtype=X_scale.dtype)
-            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)
+            coef_ = xp.astype(self.coef_, X_scale.dtype, copy=False)
+            coef_ = self.coef_ = xp.divide(coef_, X_scale)
+
+            if coef_.ndim == 1:
+                intercept_ = y_offset - X_offset @ coef_
+            else:
+                intercept_ = y_offset - X_offset @ coef_.T
+
+            self.intercept_ = intercept_
+
         else:
             self.intercept_ = 0.0
 
     def _more_tags(self):
         return {"requires_y": True}
 
 
@@ -346,15 +377,15 @@
         -------
         y_pred : ndarray of shape (n_samples,)
             Vector containing the class labels for each sample.
         """
         xp, _ = get_namespace(X)
         scores = self.decision_function(X)
         if len(scores.shape) == 1:
-            indices = xp.astype(scores > 0, int)
+            indices = xp.astype(scores > 0, indexing_dtype(xp))
         else:
             indices = xp.argmax(scores, axis=1)
 
         return xp.take(self.classes_, indices, axis=0)
 
     def _predict_proba_lr(self, X):
         """Probability estimation for OvR logistic regression.
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_bayes.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_bayes.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,92 +1,49 @@
 """
 Various bayesian regression
 """
 
 # Authors: V. Michel, F. Pedregosa, A. Gramfort
 # License: BSD 3 clause
 
-import warnings
 from math import log
 from numbers import Integral, Real
 
 import numpy as np
 from scipy import linalg
 from scipy.linalg import pinvh
 
 from ..base import RegressorMixin, _fit_context
 from ..utils import _safe_indexing
-from ..utils._param_validation import Hidden, Interval, StrOptions
+from ..utils._param_validation import Interval
 from ..utils.extmath import fast_logdet
 from ..utils.validation import _check_sample_weight
 from ._base import LinearModel, _preprocess_data, _rescale_data
 
-
-# TODO(1.5) Remove
-def _deprecate_n_iter(n_iter, max_iter):
-    """Deprecates n_iter in favour of max_iter. Checks if the n_iter has been
-    used instead of max_iter and generates a deprecation warning if True.
-
-    Parameters
-    ----------
-    n_iter : int,
-        Value of n_iter attribute passed by the estimator.
-
-    max_iter : int, default=None
-        Value of max_iter attribute passed by the estimator.
-        If `None`, it corresponds to `max_iter=300`.
-
-    Returns
-    -------
-    max_iter : int,
-        Value of max_iter which shall further be used by the estimator.
-
-    Notes
-    -----
-    This function should be completely removed in 1.5.
-    """
-    if n_iter != "deprecated":
-        if max_iter is not None:
-            raise ValueError(
-                "Both `n_iter` and `max_iter` attributes were set. Attribute"
-                " `n_iter` was deprecated in version 1.3 and will be removed in"
-                " 1.5. To avoid this error, only set the `max_iter` attribute."
-            )
-        warnings.warn(
-            (
-                "'n_iter' was renamed to 'max_iter' in version 1.3 and "
-                "will be removed in 1.5"
-            ),
-            FutureWarning,
-        )
-        max_iter = n_iter
-    elif max_iter is None:
-        max_iter = 300
-    return max_iter
-
-
 ###############################################################################
 # BayesianRidge regression
 
 
 class BayesianRidge(RegressorMixin, LinearModel):
     """Bayesian ridge regression.
 
     Fit a Bayesian ridge model. See the Notes section for details on this
     implementation and the optimization of the regularization parameters
     lambda (precision of the weights) and alpha (precision of the noise).
 
     Read more in the :ref:`User Guide <bayesian_regression>`.
+    For an intuitive visualization of how the sinusoid is approximated by
+    a polynomial using different pairs of initial values, see
+    :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.
 
     Parameters
     ----------
-    max_iter : int, default=None
+    max_iter : int, default=300
         Maximum number of iterations over the complete dataset before
-        stopping independently of any early stopping criterion. If `None`, it
-        corresponds to `max_iter=300`.
+        stopping independently of any early stopping criterion.
 
         .. versionchanged:: 1.3
 
     tol : float, default=1e-3
         Stop the algorithm if w has converged.
 
     alpha_1 : float, default=1e-6
@@ -130,21 +87,14 @@
 
     copy_X : bool, default=True
         If True, X will be copied; else, it may be overwritten.
 
     verbose : bool, default=False
         Verbose mode when fitting the model.
 
-    n_iter : int
-        Maximum number of iterations. Should be greater than or equal to 1.
-
-        .. deprecated:: 1.3
-           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
-           `max_iter` instead.
-
     Attributes
     ----------
     coef_ : array-like of shape (n_features,)
         Coefficients of the regression model (mean of distribution)
 
     intercept_ : float
         Independent term in decision function. Set to 0.0 if
@@ -216,62 +166,56 @@
     >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
     BayesianRidge()
     >>> clf.predict([[1, 1]])
     array([1.])
     """
 
     _parameter_constraints: dict = {
-        "max_iter": [Interval(Integral, 1, None, closed="left"), None],
+        "max_iter": [Interval(Integral, 1, None, closed="left")],
         "tol": [Interval(Real, 0, None, closed="neither")],
         "alpha_1": [Interval(Real, 0, None, closed="left")],
         "alpha_2": [Interval(Real, 0, None, closed="left")],
         "lambda_1": [Interval(Real, 0, None, closed="left")],
         "lambda_2": [Interval(Real, 0, None, closed="left")],
         "alpha_init": [None, Interval(Real, 0, None, closed="left")],
         "lambda_init": [None, Interval(Real, 0, None, closed="left")],
         "compute_score": ["boolean"],
         "fit_intercept": ["boolean"],
         "copy_X": ["boolean"],
         "verbose": ["verbose"],
-        "n_iter": [
-            Interval(Integral, 1, None, closed="left"),
-            Hidden(StrOptions({"deprecated"})),
-        ],
     }
 
     def __init__(
         self,
         *,
-        max_iter=None,  # TODO(1.5): Set to 300
+        max_iter=300,
         tol=1.0e-3,
         alpha_1=1.0e-6,
         alpha_2=1.0e-6,
         lambda_1=1.0e-6,
         lambda_2=1.0e-6,
         alpha_init=None,
         lambda_init=None,
         compute_score=False,
         fit_intercept=True,
         copy_X=True,
         verbose=False,
-        n_iter="deprecated",  # TODO(1.5): Remove
     ):
         self.max_iter = max_iter
         self.tol = tol
         self.alpha_1 = alpha_1
         self.alpha_2 = alpha_2
         self.lambda_1 = lambda_1
         self.lambda_2 = lambda_2
         self.alpha_init = alpha_init
         self.lambda_init = lambda_init
         self.compute_score = compute_score
         self.fit_intercept = fit_intercept
         self.copy_X = copy_X
         self.verbose = verbose
-        self.n_iter = n_iter
 
     @_fit_context(prefer_skip_nested_validation=True)
     def fit(self, X, y, sample_weight=None):
         """Fit the model.
 
         Parameters
         ----------
@@ -287,16 +231,14 @@
                parameter *sample_weight* support to BayesianRidge.
 
         Returns
         -------
         self : object
             Returns the instance itself.
         """
-        max_iter = _deprecate_n_iter(self.n_iter, self.max_iter)
-
         X, y = self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True)
         dtype = X.dtype
 
         if sample_weight is not None:
             sample_weight = _check_sample_weight(sample_weight, X, dtype=dtype)
 
         X, y, X_offset_, y_offset_, X_scale_ = _preprocess_data(
@@ -340,15 +282,15 @@
         coef_old_ = None
 
         XT_y = np.dot(X.T, y)
         U, S, Vh = linalg.svd(X, full_matrices=False)
         eigen_vals_ = S**2
 
         # Convergence loop of the bayesian ridge regression
-        for iter_ in range(max_iter):
+        for iter_ in range(self.max_iter):
             # update posterior mean coef_ based on alpha_ and lambda_ and
             # compute corresponding rmse
             coef_, rmse_ = self._update_coef_(
                 X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_
             )
             if self.compute_score:
                 # compute the log marginal likelihood
@@ -495,16 +437,16 @@
     weights) and alpha (precision of the distribution of the noise).
     The estimation is done by an iterative procedures (Evidence Maximization)
 
     Read more in the :ref:`User Guide <bayesian_regression>`.
 
     Parameters
     ----------
-    max_iter : int, default=None
-        Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.
+    max_iter : int, default=300
+        Maximum number of iterations.
 
         .. versionchanged:: 1.3
 
     tol : float, default=1e-3
         Stop the algorithm if w has converged.
 
     alpha_1 : float, default=1e-6
@@ -537,21 +479,14 @@
 
     copy_X : bool, default=True
         If True, X will be copied; else, it may be overwritten.
 
     verbose : bool, default=False
         Verbose mode when fitting the model.
 
-    n_iter : int
-        Maximum number of iterations.
-
-        .. deprecated:: 1.3
-           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use
-           `max_iter` instead.
-
     Attributes
     ----------
     coef_ : array-like of shape (n_features,)
         Coefficients of the regression model (mean of distribution)
 
     alpha_ : float
        estimated precision of the noise.
@@ -621,59 +556,53 @@
     >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
     ARDRegression()
     >>> clf.predict([[1, 1]])
     array([1.])
     """
 
     _parameter_constraints: dict = {
-        "max_iter": [Interval(Integral, 1, None, closed="left"), None],
+        "max_iter": [Interval(Integral, 1, None, closed="left")],
         "tol": [Interval(Real, 0, None, closed="left")],
         "alpha_1": [Interval(Real, 0, None, closed="left")],
         "alpha_2": [Interval(Real, 0, None, closed="left")],
         "lambda_1": [Interval(Real, 0, None, closed="left")],
         "lambda_2": [Interval(Real, 0, None, closed="left")],
         "compute_score": ["boolean"],
         "threshold_lambda": [Interval(Real, 0, None, closed="left")],
         "fit_intercept": ["boolean"],
         "copy_X": ["boolean"],
         "verbose": ["verbose"],
-        "n_iter": [
-            Interval(Integral, 1, None, closed="left"),
-            Hidden(StrOptions({"deprecated"})),
-        ],
     }
 
     def __init__(
         self,
         *,
-        max_iter=None,  # TODO(1.5): Set to 300
+        max_iter=300,
         tol=1.0e-3,
         alpha_1=1.0e-6,
         alpha_2=1.0e-6,
         lambda_1=1.0e-6,
         lambda_2=1.0e-6,
         compute_score=False,
         threshold_lambda=1.0e4,
         fit_intercept=True,
         copy_X=True,
         verbose=False,
-        n_iter="deprecated",  # TODO(1.5): Remove
     ):
         self.max_iter = max_iter
         self.tol = tol
         self.fit_intercept = fit_intercept
         self.alpha_1 = alpha_1
         self.alpha_2 = alpha_2
         self.lambda_1 = lambda_1
         self.lambda_2 = lambda_2
         self.compute_score = compute_score
         self.threshold_lambda = threshold_lambda
         self.copy_X = copy_X
         self.verbose = verbose
-        self.n_iter = n_iter
 
     @_fit_context(prefer_skip_nested_validation=True)
     def fit(self, X, y):
         """Fit the model according to the given training data and parameters.
 
         Iterative procedure to maximize the evidence
 
@@ -686,16 +615,14 @@
             Target values (integers). Will be cast to X's dtype if necessary.
 
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        max_iter = _deprecate_n_iter(self.n_iter, self.max_iter)
-
         X, y = self._validate_data(
             X, y, dtype=[np.float64, np.float32], y_numeric=True, ensure_min_samples=2
         )
         dtype = X.dtype
 
         n_samples, n_features = X.shape
         coef_ = np.zeros(n_features, dtype=dtype)
@@ -735,15 +662,15 @@
 
         update_sigma = (
             self._update_sigma
             if n_samples >= n_features
             else self._update_sigma_woodbury
         )
         # Iterative procedure of ARDRegression
-        for iter_ in range(max_iter):
+        for iter_ in range(self.max_iter):
             sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)
             coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)
 
             # Update alpha and lambda
             rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)
             gamma_ = 1.0 - lambda_[keep_lambda] * np.diag(sigma_)
             lambda_[keep_lambda] = (gamma_ + 2.0 * lambda_1) / (
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_cd_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_cd_fast.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -3,31 +3,28 @@
 #         Olivier Grisel <olivier.grisel@ensta.org>
 #         Alexis Mignon <alexis.mignon@gmail.com>
 #         Manoj Kumar <manojkumarsivaraj334@gmail.com>
 #
 # License: BSD 3 clause
 
 from libc.math cimport fabs
-cimport numpy as cnp
 import numpy as np
 
 from cython cimport floating
 import warnings
 from ..exceptions import ConvergenceWarning
 
 from ..utils._cython_blas cimport (
     _axpy, _dot, _asum, _gemv, _nrm2, _copy, _scal
 )
 from ..utils._cython_blas cimport ColMajor, Trans, NoTrans
 from ..utils._typedefs cimport uint32_t
 from ..utils._random cimport our_rand_r
 
 
-cnp.import_array()
-
 # The following two functions are shamelessly copied from the tree code.
 
 cdef enum:
     # Max value for our rand_r replacement (near the bottom).
     # We don't use RAND_MAX because it's different across platforms and
     # particularly tiny on Windows/MSVC.
     # It corresponds to the maximum representable value for
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_coordinate_descent.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_coordinate_descent.py`

 * *Files 0% similar despite different names*

```diff
@@ -524,14 +524,33 @@
     ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.
 
     Notes
     -----
     For an example, see
     :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py
     <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.
+
+    Examples
+    --------
+    >>> from sklearn.linear_model import enet_path
+    >>> from sklearn.datasets import make_regression
+    >>> X, y, true_coef = make_regression(
+    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0
+    ... )
+    >>> true_coef
+    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])
+    >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)
+    >>> alphas.shape
+    (3,)
+    >>> estimated_coef
+     array([[ 0.        ,  0.78...,  0.56...],
+            [ 0.        ,  1.12...,  0.61...],
+            [-0.        , -2.12..., -1.12...],
+            [ 0.        , 23.04..., 88.93...],
+            [ 0.        , 10.63..., 41.56...]])
     """
     X_offset_param = params.pop("X_offset", None)
     X_scale_param = params.pop("X_scale", None)
     sample_weight = params.pop("sample_weight", None)
     tol = params.pop("tol", 1e-4)
     max_iter = params.pop("max_iter", 1000)
     random_state = params.pop("random_state", None)
@@ -902,17 +921,20 @@
 
     @_fit_context(prefer_skip_nested_validation=True)
     def fit(self, X, y, sample_weight=None, check_input=True):
         """Fit model with coordinate descent.
 
         Parameters
         ----------
-        X : {ndarray, sparse matrix} of (n_samples, n_features)
+        X : {ndarray, sparse matrix, sparse array} of (n_samples, n_features)
             Data.
 
+            Note that large sparse matrices and arrays requiring `int64`
+            indices are not accepted.
+
         y : ndarray of shape (n_samples,) or (n_samples, n_targets)
             Target. Will be cast to X's dtype if necessary.
 
         sample_weight : float or array-like of shape (n_samples,), default=None
             Sample weights. Internally, the `sample_weight` vector will be
             rescaled to sum to `n_samples`.
 
@@ -954,14 +976,15 @@
             X_copied = self.copy_X and self.fit_intercept
             X, y = self._validate_data(
                 X,
                 y,
                 accept_sparse="csc",
                 order="F",
                 dtype=[np.float64, np.float32],
+                accept_large_sparse=False,
                 copy=X_copied,
                 multi_output=True,
                 y_numeric=True,
             )
             y = check_array(
                 y, order="F", copy=False, dtype=X.dtype.type, ensure_2d=False
             )
@@ -1528,15 +1551,16 @@
         Fit is on grid of alphas and best alpha estimated by cross-validation.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Training data. Pass directly as Fortran-contiguous data
             to avoid unnecessary memory duplication. If y is mono-output,
-            X can be sparse.
+            X can be sparse. Note that large sparse matrices and arrays
+            requiring `int64` indices are not accepted.
 
         y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Target values.
 
         sample_weight : float or array-like of shape (n_samples,), \
                 default=None
             Sample weights used for fitting and evaluation of the weighted
@@ -1578,15 +1602,18 @@
             # by the model fitting itself
 
             # Need to validate separately here.
             # We can't pass multi_output=True because that would allow y to be
             # csr. We also want to allow y to be 64 or 32 but check_X_y only
             # allows to convert for 64.
             check_X_params = dict(
-                accept_sparse="csc", dtype=[np.float64, np.float32], copy=False
+                accept_sparse="csc",
+                dtype=[np.float64, np.float32],
+                copy=False,
+                accept_large_sparse=False,
             )
             X, y = self._validate_data(
                 X, y, validate_separately=(check_X_params, check_y_params)
             )
             if sparse.issparse(X):
                 if hasattr(reference_to_old_X, "data") and not np.may_share_memory(
                     reference_to_old_X.data, X.data
@@ -1829,15 +1856,15 @@
             routing information.
         """
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
             .add(
                 splitter=check_cv(self.cv),
-                method_mapping=MethodMapping().add(callee="split", caller="fit"),
+                method_mapping=MethodMapping().add(caller="fit", callee="split"),
             )
         )
         return router
 
 
 class LassoCV(RegressorMixin, LinearModelCV):
     """Lasso linear model with iterative fitting along a regularization path.
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_glm/_newton_solver.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/_newton_solver.py`

 * *Files 0% similar despite different names*

```diff
@@ -226,15 +226,15 @@
 
         # np.sum(np.abs(self.gradient_old))
         sum_abs_grad_old = -1
 
         is_verbose = self.verbose >= 2
         if is_verbose:
             print("  Backtracking Line Search")
-            print(f"    eps=10 * finfo.eps={eps}")
+            print(f"    eps=16 * finfo.eps={eps}")
 
         for i in range(21):  # until and including t = beta**20 ~ 1e-6
             self.coef = self.coef_old + t * self.coef_newton
             raw = self.raw_prediction + t * raw_prediction_newton
             self.loss_value, self.gradient = self.linear_loss.loss_gradient(
                 coef=self.coef,
                 X=X,
@@ -498,16 +498,15 @@
             warnings.warn(
                 f"The inner solver of {self.__class__.__name__} stumbled upon a "
                 "singular or very ill-conditioned Hessian matrix at iteration "
                 f"#{self.iteration}. It will now resort to lbfgs instead.\n"
                 "Further options are to use another solver or to avoid such situation "
                 "in the first place. Possible remedies are removing collinear features"
                 " of X or increasing the penalization strengths.\n"
-                "The original Linear Algebra message was:\n"
-                + str(e),
+                "The original Linear Algebra message was:\n" + str(e),
                 scipy.linalg.LinAlgWarning,
             )
             # Possible causes:
             # 1. hess_pointwise is negative. But this is already taken care in
             #    LinearModelLoss.gradient_hessian.
             # 2. X is singular or ill-conditioned
             #    This might be the most probable cause.
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_glm/glm.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/glm.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_glm/tests/test_glm.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_glm/tests/test_glm.py`

 * *Files 0% similar despite different names*

```diff
@@ -1103,10 +1103,9 @@
     with warnings.catch_warnings():
         warnings.simplefilter("ignore", ConvergenceWarning)
         sol.solve(X, y, None)
     captured = capsys.readouterr()
     if verbose >= 1:
         assert (
             "The inner solver detected a pointwise Hessian with many negative values"
-            " and resorts to lbfgs instead."
-            in captured.out
+            " and resorts to lbfgs instead." in captured.out
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_huber.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_huber.py`

 * *Files 0% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 from numbers import Integral, Real
 
 import numpy as np
 from scipy import optimize
 
 from ..base import BaseEstimator, RegressorMixin, _fit_context
-from ..utils import axis0_safe_slice
+from ..utils._mask import axis0_safe_slice
 from ..utils._param_validation import Interval
 from ..utils.extmath import safe_sparse_dot
 from ..utils.optimize import _check_optimize_result
 from ..utils.validation import _check_sample_weight
 from ._base import LinearModel
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_least_angle.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_least_angle.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 Least Angle Regression algorithm. See the documentation on the
 Generalized Linear Model for a complete discussion.
 """
+
 # Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #         Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #         Gael Varoquaux
 #
 # License: BSD 3 clause
 
 import sys
@@ -94,16 +95,15 @@
     X : None or ndarray of shape (n_samples, n_features)
         Input data. Note that if X is `None` then the Gram matrix must be
         specified, i.e., cannot be `None` or `False`.
 
     y : None or ndarray of shape (n_samples,)
         Input targets.
 
-    Xy : array-like of shape (n_features,) or (n_features, n_targets), \
-            default=None
+    Xy : array-like of shape (n_features,), default=None
         `Xy = X.T @ y` that can be precomputed. It is useful
         only when the Gram matrix is precomputed.
 
     Gram : None, 'auto', bool, ndarray of shape (n_features, n_features), \
             default=None
         Precomputed Gram matrix `X.T @ X`, if `'auto'`, the Gram
         matrix is precomputed from the given X, if there are more samples
@@ -186,14 +186,33 @@
            http://statweb.stanford.edu/~tibs/ftp/lars.pdf
 
     .. [2] `Wikipedia entry on the Least-angle regression
            <https://en.wikipedia.org/wiki/Least-angle_regression>`_
 
     .. [3] `Wikipedia entry on the Lasso
            <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_
+
+    Examples
+    --------
+    >>> from sklearn.linear_model import lars_path
+    >>> from sklearn.datasets import make_regression
+    >>> X, y, true_coef = make_regression(
+    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0
+    ... )
+    >>> true_coef
+    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])
+    >>> alphas, _, estimated_coef = lars_path(X, y)
+    >>> alphas.shape
+    (3,)
+    >>> estimated_coef
+    array([[ 0.     ,  0.     ,  0.     ],
+           [ 0.     ,  0.     ,  0.     ],
+           [ 0.     ,  0.     ,  0.     ],
+           [ 0.     , 46.96..., 97.99...],
+           [ 0.     ,  0.     , 45.70...]])
     """
     if X is None and Gram is not None:
         raise ValueError(
             "X cannot be None if Gram is not None"
             "Use lars_path_gram to avoid passing X and y."
         )
     return _lars_path_solver(
@@ -251,22 +270,22 @@
 ):
     """The lars_path in the sufficient stats mode [1].
 
     The optimization objective for the case method='lasso' is::
 
     (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
 
-    in the case of method='lars', the objective function is only known in
+    in the case of method='lar', the objective function is only known in
     the form of an implicit equation (see discussion in [1])
 
     Read more in the :ref:`User Guide <least_angle_regression>`.
 
     Parameters
     ----------
-    Xy : ndarray of shape (n_features,) or (n_features, n_targets)
+    Xy : ndarray of shape (n_features,)
         `Xy = X.T @ y`.
 
     Gram : ndarray of shape (n_features, n_features)
         `Gram = X.T @ X`.
 
     n_samples : int
         Equivalent size of sample.
@@ -348,14 +367,33 @@
            http://statweb.stanford.edu/~tibs/ftp/lars.pdf
 
     .. [2] `Wikipedia entry on the Least-angle regression
            <https://en.wikipedia.org/wiki/Least-angle_regression>`_
 
     .. [3] `Wikipedia entry on the Lasso
            <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_
+
+    Examples
+    --------
+    >>> from sklearn.linear_model import lars_path_gram
+    >>> from sklearn.datasets import make_regression
+    >>> X, y, true_coef = make_regression(
+    ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0
+    ... )
+    >>> true_coef
+    array([ 0.        ,  0.        ,  0.        , 97.9..., 45.7...])
+    >>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)
+    >>> alphas.shape
+    (3,)
+    >>> estimated_coef
+    array([[ 0.     ,  0.     ,  0.     ],
+           [ 0.     ,  0.     ,  0.     ],
+           [ 0.     ,  0.     ,  0.     ],
+           [ 0.     , 46.96..., 97.99...],
+           [ 0.     ,  0.     , 45.70...]])
     """
     return _lars_path_solver(
         X=None,
         y=None,
         Xy=Xy,
         Gram=Gram,
         n_samples=n_samples,
@@ -391,30 +429,29 @@
 ):
     """Compute Least Angle Regression or Lasso path using LARS algorithm [1]
 
     The optimization objective for the case method='lasso' is::
 
     (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
 
-    in the case of method='lars', the objective function is only known in
+    in the case of method='lar', the objective function is only known in
     the form of an implicit equation (see discussion in [1])
 
     Read more in the :ref:`User Guide <least_angle_regression>`.
 
     Parameters
     ----------
     X : None or ndarray of shape (n_samples, n_features)
         Input data. Note that if X is None then Gram must be specified,
         i.e., cannot be None or False.
 
     y : None or ndarray of shape (n_samples,)
         Input targets.
 
-    Xy : array-like of shape (n_features,) or (n_features, n_targets), \
-            default=None
+    Xy : array-like of shape (n_features,), default=None
         `Xy = np.dot(X.T, y)` that can be precomputed. It is useful
         only when the Gram matrix is precomputed.
 
     Gram : None, 'auto' or array-like of shape (n_features, n_features), \
             default=None
         Precomputed Gram matrix `(X' * X)`, if ``'auto'``, the Gram
         matrix is precomputed from the given X, if there are more samples
@@ -1697,16 +1734,15 @@
             routed_params = Bunch(splitter=Bunch(split={}))
 
         # As we use cross-validation, the Gram matrix is not precomputed here
         Gram = self.precompute
         if hasattr(Gram, "__array__"):
             warnings.warn(
                 'Parameter "precompute" cannot be an array in '
-                '%s. Automatically switch to "auto" instead.'
-                % self.__class__.__name__
+                '%s. Automatically switch to "auto" instead.' % self.__class__.__name__
             )
             Gram = "auto"
 
         cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
             delayed(_lars_path_residues)(
                 X[train],
                 y[train],
@@ -1781,15 +1817,15 @@
         -------
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
         router = MetadataRouter(owner=self.__class__.__name__).add(
             splitter=check_cv(self.cv),
-            method_mapping=MethodMapping().add(callee="split", caller="fit"),
+            method_mapping=MethodMapping().add(caller="fit", callee="split"),
         )
         return router
 
 
 class LassoLarsCV(LarsCV):
     """Cross-validated Lasso, using the LARS algorithm.
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_linear_loss.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_linear_loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Loss functions for linear models with raw_prediction = X @ coef
 """
+
 import numpy as np
 from scipy import sparse
 
 from ..utils.extmath import squared_norm
 
 
 class LinearModelLoss:
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_logistic.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_logistic.py`

 * *Files 5% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 from ..utils import (
     Bunch,
     check_array,
     check_consistent_length,
     check_random_state,
     compute_class_weight,
 )
-from ..utils._param_validation import Interval, StrOptions
+from ..utils._param_validation import Hidden, Interval, StrOptions
 from ..utils.extmath import row_norms, softmax
 from ..utils.metadata_routing import (
     MetadataRouter,
     MethodMapping,
     _raise_for_params,
     _routing_enabled,
     process_routing,
@@ -473,15 +473,22 @@
                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,
             )
             w0, loss = opt_res.x, opt_res.fun
         elif solver == "newton-cg":
             l2_reg_strength = 1.0 / (C * sw_sum)
             args = (X, target, sample_weight, l2_reg_strength, n_threads)
             w0, n_iter_i = _newton_cg(
-                hess, func, grad, w0, args=args, maxiter=max_iter, tol=tol
+                grad_hess=hess,
+                func=func,
+                grad=grad,
+                x0=w0,
+                args=args,
+                maxiter=max_iter,
+                tol=tol,
+                verbose=verbose,
             )
         elif solver == "newton-cholesky":
             l2_reg_strength = 1.0 / (C * sw_sum)
             sol = NewtonCholeskySolver(
                 coef=w0,
                 linear_loss=loss,
                 l2_reg_strength=l2_reg_strength,
@@ -894,36 +901,41 @@
 
     solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \
             default='lbfgs'
 
         Algorithm to use in the optimization problem. Default is 'lbfgs'.
         To choose a solver, you might want to consider the following aspects:
 
-            - For small datasets, 'liblinear' is a good choice, whereas 'sag'
-              and 'saga' are faster for large ones;
-            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and
-              'lbfgs' handle multinomial loss;
-            - 'liblinear' is limited to one-versus-rest schemes.
-            - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,
-              especially with one-hot encoded categorical features with rare
-              categories. Note that it is limited to binary classification and the
-              one-versus-rest reduction for multiclass classification. Be aware that
-              the memory usage of this solver has a quadratic dependency on
-              `n_features` because it explicitly computes the Hessian matrix.
+        - For small datasets, 'liblinear' is a good choice, whereas 'sag'
+          and 'saga' are faster for large ones;
+        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and
+          'lbfgs' handle multinomial loss;
+        - 'liblinear' and 'newton-cholesky' can only handle binary classification
+          by default. To apply a one-versus-rest scheme for the multiclass setting
+          one can wrapt it with the `OneVsRestClassifier`.
+        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,
+          especially with one-hot encoded categorical features with rare
+          categories. Be aware that the memory usage of this solver has a quadratic
+          dependency on `n_features` because it explicitly computes the Hessian
+          matrix.
 
         .. warning::
-           The choice of the algorithm depends on the penalty chosen.
-           Supported penalties by solver:
+           The choice of the algorithm depends on the penalty chosen and on
+           (multinomial) multiclass support:
 
-           - 'lbfgs'           -   ['l2', None]
-           - 'liblinear'       -   ['l1', 'l2']
-           - 'newton-cg'       -   ['l2', None]
-           - 'newton-cholesky' -   ['l2', None]
-           - 'sag'             -   ['l2', None]
-           - 'saga'            -   ['elasticnet', 'l1', 'l2', None]
+           ================= ============================== ======================
+           solver            penalty                        multinomial multiclass
+           ================= ============================== ======================
+           'lbfgs'           'l2', None                     yes
+           'liblinear'       'l1', 'l2'                     no
+           'newton-cg'       'l2', None                     yes
+           'newton-cholesky' 'l2', None                     no
+           'sag'             'l2', None                     yes
+           'saga'            'elasticnet', 'l1', 'l2', None yes
+           ================= ============================== ======================
 
         .. note::
            'sag' and 'saga' fast convergence is only guaranteed on features
            with approximately the same scale. You can preprocess the data with
            a scaler from :mod:`sklearn.preprocessing`.
 
         .. seealso::
@@ -952,14 +964,21 @@
         'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
         and otherwise selects 'multinomial'.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
         .. versionchanged:: 0.22
             Default changed from 'ovr' to 'auto' in 0.22.
+        .. deprecated:: 1.5
+           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.
+           From then on, the recommended 'multinomial' will always be used for
+           `n_classes >= 3`.
+           Solvers that do not support 'multinomial' will raise an error.
+           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you
+           still want to use OvR.
 
     verbose : int, default=0
         For the liblinear and lbfgs solvers set verbose to any positive
         number for verbosity.
 
     warm_start : bool, default=False
         When set to True, reuse the solution of the previous call to fit as
@@ -1093,19 +1112,22 @@
         "random_state": ["random_state"],
         "solver": [
             StrOptions(
                 {"lbfgs", "liblinear", "newton-cg", "newton-cholesky", "sag", "saga"}
             )
         ],
         "max_iter": [Interval(Integral, 0, None, closed="left")],
-        "multi_class": [StrOptions({"auto", "ovr", "multinomial"})],
         "verbose": ["verbose"],
         "warm_start": ["boolean"],
         "n_jobs": [None, Integral],
         "l1_ratio": [Interval(Real, 0, 1, closed="both"), None],
+        "multi_class": [
+            StrOptions({"auto", "ovr", "multinomial"}),
+            Hidden(StrOptions({"deprecated"})),
+        ],
     }
 
     def __init__(
         self,
         penalty="l2",
         *,
         dual=False,
@@ -1113,15 +1135,15 @@
         C=1.0,
         fit_intercept=True,
         intercept_scaling=1,
         class_weight=None,
         random_state=None,
         solver="lbfgs",
         max_iter=100,
-        multi_class="auto",
+        multi_class="deprecated",
         verbose=0,
         warm_start=False,
         n_jobs=None,
         l1_ratio=None,
     ):
         self.penalty = penalty
         self.dual = dual
@@ -1205,15 +1227,48 @@
             dtype=_dtype,
             order="C",
             accept_large_sparse=solver not in ["liblinear", "sag", "saga"],
         )
         check_classification_targets(y)
         self.classes_ = np.unique(y)
 
-        multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))
+        # TODO(1.7) remove multi_class
+        multi_class = self.multi_class
+        if self.multi_class == "multinomial" and len(self.classes_) == 2:
+            warnings.warn(
+                (
+                    "'multi_class' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. From then on, binary problems will be fit as proper binary "
+                    " logistic regression models (as if multi_class='ovr' were set)."
+                    " Leave it to its default value to avoid this warning."
+                ),
+                FutureWarning,
+            )
+        elif self.multi_class in ("multinomial", "auto"):
+            warnings.warn(
+                (
+                    "'multi_class' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. From then on, it will always use 'multinomial'."
+                    " Leave it to its default value to avoid this warning."
+                ),
+                FutureWarning,
+            )
+        elif self.multi_class == "ovr":
+            warnings.warn(
+                (
+                    "'multi_class' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead."
+                    " Leave it to its default value to avoid this warning."
+                ),
+                FutureWarning,
+            )
+        else:
+            # Set to old default value.
+            multi_class = "auto"
+        multi_class = _check_multi_class(multi_class, solver, len(self.classes_))
 
         if solver == "liblinear":
             if effective_n_jobs(self.n_jobs) != 1:
                 warnings.warn(
                     "'n_jobs' > 1 does not have any effect when"
                     " 'solver' is set to 'liblinear'. Got 'n_jobs'"
                     " = {}.".format(effective_n_jobs(self.n_jobs))
@@ -1242,16 +1297,15 @@
 
         n_classes = len(self.classes_)
         classes_ = self.classes_
         if n_classes < 2:
             raise ValueError(
                 "This solver needs samples of at least 2 classes"
                 " in the data, but the data contains only one"
-                " class: %r"
-                % classes_[0]
+                " class: %r" % classes_[0]
             )
 
         if len(self.classes_) == 2:
             n_classes = 1
             classes_ = classes_[1:]
 
         if self.warm_start:
@@ -1363,15 +1417,15 @@
         T : array-like of shape (n_samples, n_classes)
             Returns the probability of the sample for each class in the model,
             where classes are ordered as they are in ``self.classes_``.
         """
         check_is_fitted(self)
 
         ovr = self.multi_class in ["ovr", "warn"] or (
-            self.multi_class == "auto"
+            self.multi_class in ["auto", "deprecated"]
             and (
                 self.classes_.size <= 2
                 or self.solver in ("liblinear", "newton-cholesky")
             )
         )
         if ovr:
             return super()._predict_proba_lr(X)
@@ -1409,15 +1463,15 @@
 
 class LogisticRegressionCV(LogisticRegression, LinearClassifierMixin, BaseEstimator):
     """Logistic Regression CV (aka logit, MaxEnt) classifier.
 
     See glossary entry for :term:`cross-validation estimator`.
 
     This class implements logistic regression using liblinear, newton-cg, sag
-    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
+    or lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
     regularization with primal formulation. The liblinear solver supports both
     L1 and L2 regularization, with a dual formulation only for the L2 penalty.
     Elastic-Net penalty is only supported by the saga solver.
 
     For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter
     is selected by the cross-validator
     :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed
@@ -1475,38 +1529,43 @@
 
     solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \
             default='lbfgs'
 
         Algorithm to use in the optimization problem. Default is 'lbfgs'.
         To choose a solver, you might want to consider the following aspects:
 
-            - For small datasets, 'liblinear' is a good choice, whereas 'sag'
-              and 'saga' are faster for large ones;
-            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and
-              'lbfgs' handle multinomial loss;
-            - 'liblinear' might be slower in :class:`LogisticRegressionCV`
-              because it does not handle warm-starting. 'liblinear' is
-              limited to one-versus-rest schemes.
-            - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,
-              especially with one-hot encoded categorical features with rare
-              categories. Note that it is limited to binary classification and the
-              one-versus-rest reduction for multiclass classification. Be aware that
-              the memory usage of this solver has a quadratic dependency on
-              `n_features` because it explicitly computes the Hessian matrix.
+        - For small datasets, 'liblinear' is a good choice, whereas 'sag'
+          and 'saga' are faster for large ones;
+        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and
+          'lbfgs' handle multinomial loss;
+        - 'liblinear' might be slower in :class:`LogisticRegressionCV`
+          because it does not handle warm-starting.
+        - 'liblinear' and 'newton-cholesky' can only handle binary classification
+          by default. To apply a one-versus-rest scheme for the multiclass setting
+          one can wrapt it with the `OneVsRestClassifier`.
+        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,
+          especially with one-hot encoded categorical features with rare
+          categories. Be aware that the memory usage of this solver has a quadratic
+          dependency on `n_features` because it explicitly computes the Hessian
+          matrix.
 
         .. warning::
-           The choice of the algorithm depends on the penalty chosen.
-           Supported penalties by solver:
+           The choice of the algorithm depends on the penalty chosen and on
+           (multinomial) multiclass support:
 
-           - 'lbfgs'           -   ['l2']
-           - 'liblinear'       -   ['l1', 'l2']
-           - 'newton-cg'       -   ['l2']
-           - 'newton-cholesky' -   ['l2']
-           - 'sag'             -   ['l2']
-           - 'saga'            -   ['elasticnet', 'l1', 'l2']
+           ================= ============================== ======================
+           solver            penalty                        multinomial multiclass
+           ================= ============================== ======================
+           'lbfgs'           'l2'                           yes
+           'liblinear'       'l1', 'l2'                     no
+           'newton-cg'       'l2'                           yes
+           'newton-cholesky' 'l2',                          no
+           'sag'             'l2',                          yes
+           'saga'            'elasticnet', 'l1', 'l2'       yes
+           ================= ============================== ======================
 
         .. note::
            'sag' and 'saga' fast convergence is only guaranteed on features
            with approximately the same scale. You can preprocess the data with
            a scaler from :mod:`sklearn.preprocessing`.
 
         .. versionadded:: 0.17
@@ -1574,14 +1633,21 @@
         'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
         and otherwise selects 'multinomial'.
 
         .. versionadded:: 0.18
            Stochastic Average Gradient descent solver for 'multinomial' case.
         .. versionchanged:: 0.22
             Default changed from 'ovr' to 'auto' in 0.22.
+        .. deprecated:: 1.5
+           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.
+           From then on, the recommended 'multinomial' will always be used for
+           `n_classes >= 3`.
+           Solvers that do not support 'multinomial' will raise an error.
+           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you
+           still want to use OvR.
 
     random_state : int, RandomState instance, default=None
         Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.
         Note that this only applies to the solver and not the cross-validation
         generator. See :term:`Glossary <random_state>` for details.
 
     l1_ratios : list of float, default=None
@@ -1715,15 +1781,15 @@
         tol=1e-4,
         max_iter=100,
         class_weight=None,
         n_jobs=None,
         verbose=0,
         refit=True,
         intercept_scaling=1.0,
-        multi_class="auto",
+        multi_class="deprecated",
         random_state=None,
         l1_ratios=None,
     ):
         self.Cs = Cs
         self.fit_intercept = fit_intercept
         self.cv = cv
         self.dual = dual
@@ -1783,16 +1849,15 @@
                         or l1_ratio > 1
                     )
                     for l1_ratio in self.l1_ratios
                 )
             ):
                 raise ValueError(
                     "l1_ratios must be a list of numbers between "
-                    "0 and 1; got (l1_ratios=%r)"
-                    % self.l1_ratios
+                    "0 and 1; got (l1_ratios=%r)" % self.l1_ratios
                 )
             l1_ratios_ = self.l1_ratios
         else:
             if self.l1_ratios is not None:
                 warnings.warn(
                     "l1_ratios parameter is only used when penalty "
                     "is 'elasticnet'. Got (penalty={})".format(self.penalty)
@@ -1820,15 +1885,48 @@
                 label_encoder.transform([cls])[0]: v for cls, v in class_weight.items()
             }
 
         # The original class labels
         classes = self.classes_ = label_encoder.classes_
         encoded_labels = label_encoder.transform(label_encoder.classes_)
 
-        multi_class = _check_multi_class(self.multi_class, solver, len(classes))
+        # TODO(1.7) remove multi_class
+        multi_class = self.multi_class
+        if self.multi_class == "multinomial" and len(self.classes_) == 2:
+            warnings.warn(
+                (
+                    "'multi_class' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. From then on, binary problems will be fit as proper binary "
+                    " logistic regression models (as if multi_class='ovr' were set)."
+                    " Leave it to its default value to avoid this warning."
+                ),
+                FutureWarning,
+            )
+        elif self.multi_class in ("multinomial", "auto"):
+            warnings.warn(
+                (
+                    "'multi_class' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. From then on, it will always use 'multinomial'."
+                    " Leave it to its default value to avoid this warning."
+                ),
+                FutureWarning,
+            )
+        elif self.multi_class == "ovr":
+            warnings.warn(
+                (
+                    "'multi_class' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. Use OneVsRestClassifier(LogisticRegressionCV(..)) instead."
+                    " Leave it to its default value to avoid this warning."
+                ),
+                FutureWarning,
+            )
+        else:
+            # Set to old default value.
+            multi_class = "auto"
+        multi_class = _check_multi_class(multi_class, solver, len(classes))
 
         if solver in ["sag", "saga"]:
             max_squared_sum = row_norms(X, squared=True).max()
         else:
             max_squared_sum = None
 
         if _routing_enabled():
@@ -1852,16 +1950,15 @@
         # Use the label encoded classes
         n_classes = len(encoded_labels)
 
         if n_classes < 2:
             raise ValueError(
                 "This solver needs samples of at least 2 classes"
                 " in the data, but the data contains only one"
-                " class: %r"
-                % classes[0]
+                " class: %r" % classes[0]
             )
 
         if n_classes == 2:
             # OvR in case of binary problems is as good as fitting
             # the higher label
             n_classes = 1
             encoded_labels = encoded_labels[1:]
@@ -2158,21 +2255,21 @@
         """
 
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
             .add(
                 splitter=self.cv,
-                method_mapping=MethodMapping().add(callee="split", caller="fit"),
+                method_mapping=MethodMapping().add(caller="fit", callee="split"),
             )
             .add(
                 scorer=self._get_scorer(),
                 method_mapping=MethodMapping()
-                .add(callee="score", caller="score")
-                .add(callee="score", caller="fit"),
+                .add(caller="score", callee="score")
+                .add(caller="fit", callee="score"),
             )
         )
         return router
 
     def _more_tags(self):
         return {
             "_xfail_checks": {
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_omp.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_omp.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Orthogonal matching pursuit algorithms
-"""
+"""Orthogonal matching pursuit algorithms"""
 
 # Author: Vlad Niculae
 #
 # License: BSD 3 clause
 
 import warnings
 from math import sqrt
@@ -384,14 +383,25 @@
     Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
     (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)
 
     This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
     M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
     Matching Pursuit Technical Report - CS Technion, April 2008.
     https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_regression
+    >>> from sklearn.linear_model import orthogonal_mp
+    >>> X, y = make_regression(noise=4, random_state=0)
+    >>> coef = orthogonal_mp(X, y)
+    >>> coef.shape
+    (100,)
+    >>> X[:1,] @ coef
+    array([-78.68...])
     """
     X = check_array(X, order="F", copy=copy_X)
     copy_X = False
     if y.ndim == 1:
         y = y.reshape(-1, 1)
     y = check_array(y)
     if y.shape[1] > 1:  # subsequent targets will be affected
@@ -551,14 +561,25 @@
     Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
     (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)
 
     This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
     M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
     Matching Pursuit Technical Report - CS Technion, April 2008.
     https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_regression
+    >>> from sklearn.linear_model import orthogonal_mp_gram
+    >>> X, y = make_regression(noise=4, random_state=0)
+    >>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)
+    >>> coef.shape
+    (100,)
+    >>> X[:1,] @ coef
+    array([-78.68...])
     """
     Gram = check_array(Gram, order="F", copy=copy_Gram)
     Xy = np.asarray(Xy)
     if Xy.ndim > 1 and Xy.shape[1] > 1:
         # or subsequent target will be affected
         copy_Gram = True
     if Xy.ndim == 1:
@@ -625,16 +646,17 @@
     """Orthogonal Matching Pursuit model (OMP).
 
     Read more in the :ref:`User Guide <omp>`.
 
     Parameters
     ----------
     n_nonzero_coefs : int, default=None
-        Desired number of non-zero entries in the solution. If None (by
-        default) this value is set to 10% of n_features.
+        Desired number of non-zero entries in the solution. Ignored if `tol` is set.
+        When `None` and `tol` is also `None`, this value is either set to 10% of
+        `n_features` or 1, whichever is greater.
 
     tol : float, default=None
         Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.
 
     fit_intercept : bool, default=True
         Whether to calculate the intercept for this model. If set
         to false, no intercept will be used in calculations
@@ -653,17 +675,17 @@
 
     intercept_ : float or ndarray of shape (n_targets,)
         Independent term in decision function.
 
     n_iter_ : int or array-like
         Number of active features across every target.
 
-    n_nonzero_coefs_ : int
-        The number of non-zero coefficients in the solution. If
-        `n_nonzero_coefs` is None and `tol` is None this value is either set
+    n_nonzero_coefs_ : int or None
+        The number of non-zero coefficients in the solution or `None` when `tol` is
+        set. If `n_nonzero_coefs` is None and `tol` is None this value is either set
         to 10% of `n_features` or 1, whichever is greater.
 
     n_features_in_ : int
         Number of features seen during :term:`fit`.
 
         .. versionadded:: 0.24
 
@@ -757,14 +779,16 @@
         if y.ndim == 1:
             y = y[:, np.newaxis]
 
         if self.n_nonzero_coefs is None and self.tol is None:
             # default for n_nonzero_coefs is 0.1 * n_features
             # but at least one.
             self.n_nonzero_coefs_ = max(int(0.1 * n_features), 1)
+        elif self.tol is not None:
+            self.n_nonzero_coefs_ = None
         else:
             self.n_nonzero_coefs_ = self.n_nonzero_coefs
 
         if Gram is False:
             coef_, self.n_iter_ = orthogonal_mp(
                 X,
                 y,
@@ -1088,10 +1112,10 @@
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
 
         router = MetadataRouter(owner=self.__class__.__name__).add(
             splitter=self.cv,
-            method_mapping=MethodMapping().add(callee="split", caller="fit"),
+            method_mapping=MethodMapping().add(caller="fit", callee="split"),
         )
         return router
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_passive_aggressive.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_passive_aggressive.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_perceptron.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_perceptron.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_quantile.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_quantile.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_ransac.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_ransac.py`

 * *Files 21% similar despite different names*

```diff
@@ -13,27 +13,37 @@
     MultiOutputMixin,
     RegressorMixin,
     _fit_context,
     clone,
 )
 from ..exceptions import ConvergenceWarning
 from ..utils import check_consistent_length, check_random_state
+from ..utils._bunch import Bunch
 from ..utils._param_validation import (
     HasMethods,
     Interval,
     Options,
     RealNotInt,
     StrOptions,
 )
 from ..utils.metadata_routing import (
-    _raise_for_unsupported_routing,
-    _RoutingNotSupportedMixin,
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    _routing_enabled,
+    process_routing,
 )
 from ..utils.random import sample_without_replacement
-from ..utils.validation import _check_sample_weight, check_is_fitted, has_fit_parameter
+from ..utils.validation import (
+    _check_method_params,
+    _check_sample_weight,
+    _deprecate_positional_args,
+    check_is_fitted,
+    has_fit_parameter,
+)
 from ._base import LinearRegression
 
 _EPSILON = np.spacing(1)
 
 
 def _dynamic_max_trials(n_inliers, n_samples, min_samples, probability):
     """Determine number trials such that at least one outlier-free subset is
@@ -66,15 +76,14 @@
         return 0
     if denom == 1:
         return float("inf")
     return abs(float(np.ceil(np.log(nom) / np.log(denom))))
 
 
 class RANSACRegressor(
-    _RoutingNotSupportedMixin,
     MetaEstimatorMixin,
     RegressorMixin,
     MultiOutputMixin,
     BaseEstimator,
 ):
     """RANSAC (RANdom SAmple Consensus) algorithm.
 
@@ -302,15 +311,19 @@
         self.random_state = random_state
         self.loss = loss
 
     @_fit_context(
         # RansacRegressor.estimator is not validated yet
         prefer_skip_nested_validation=False
     )
-    def fit(self, X, y, sample_weight=None):
+    # TODO(1.7): remove `sample_weight` from the signature after deprecation
+    # cycle; for backwards compatibility: pop it from `fit_params` before the
+    # `_raise_for_params` check and reinsert it after the check
+    @_deprecate_positional_args(version="1.7")
+    def fit(self, X, y, *, sample_weight=None, **fit_params):
         """Fit estimator using RANSAC algorithm.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Training data.
 
@@ -320,30 +333,41 @@
         sample_weight : array-like of shape (n_samples,), default=None
             Individual weights for each sample
             raises error if sample_weight is passed and estimator
             fit method does not support it.
 
             .. versionadded:: 0.18
 
+        **fit_params : dict
+            Parameters routed to the `fit` method of the sub-estimator via the
+            metadata routing API.
+
+            .. versionadded:: 1.5
+
+                Only available if
+                `sklearn.set_config(enable_metadata_routing=True)` is set. See
+                :ref:`Metadata Routing User Guide <metadata_routing>` for more
+                details.
+
         Returns
         -------
         self : object
             Fitted `RANSACRegressor` estimator.
 
         Raises
         ------
         ValueError
             If no valid consensus set could be found. This occurs if
             `is_data_valid` and `is_model_valid` return False for all
             `max_trials` randomly chosen sub-samples.
         """
-        _raise_for_unsupported_routing(self, "fit", sample_weight=sample_weight)
         # Need to validate separately here. We can't pass multi_output=True
         # because that would allow y to be csr. Delay expensive finiteness
         # check to the estimator's own input validation.
+        _raise_for_params(fit_params, self, "fit")
         check_X_params = dict(accept_sparse="csr", force_all_finite=False)
         check_y_params = dict(ensure_2d=False)
         X, y = self._validate_data(
             X, y, validate_separately=(check_X_params, check_y_params)
         )
         check_consistent_length(X, y)
 
@@ -400,20 +424,30 @@
         except ValueError:
             pass
 
         estimator_fit_has_sample_weight = has_fit_parameter(estimator, "sample_weight")
         estimator_name = type(estimator).__name__
         if sample_weight is not None and not estimator_fit_has_sample_weight:
             raise ValueError(
-                "%s does not support sample_weight. Samples"
+                "%s does not support sample_weight. Sample"
                 " weights are only used for the calibration"
                 " itself." % estimator_name
             )
+
         if sample_weight is not None:
-            sample_weight = _check_sample_weight(sample_weight, X)
+            fit_params["sample_weight"] = sample_weight
+
+        if _routing_enabled():
+            routed_params = process_routing(self, "fit", **fit_params)
+        else:
+            routed_params = Bunch()
+            routed_params.estimator = Bunch(fit={}, predict={}, score={})
+            if sample_weight is not None:
+                sample_weight = _check_sample_weight(sample_weight, X)
+                routed_params.estimator.fit = {"sample_weight": sample_weight}
 
         n_inliers_best = 1
         score_best = -np.inf
         inlier_mask_best = None
         X_inlier_best = None
         y_inlier_best = None
         inlier_best_idxs_subset = None
@@ -447,21 +481,21 @@
             # check if random sample set is valid
             if self.is_data_valid is not None and not self.is_data_valid(
                 X_subset, y_subset
             ):
                 self.n_skips_invalid_data_ += 1
                 continue
 
+            # cut `fit_params` down to `subset_idxs`
+            fit_params_subset = _check_method_params(
+                X, params=routed_params.estimator.fit, indices=subset_idxs
+            )
+
             # fit model for current random sample set
-            if sample_weight is None:
-                estimator.fit(X_subset, y_subset)
-            else:
-                estimator.fit(
-                    X_subset, y_subset, sample_weight=sample_weight[subset_idxs]
-                )
+            estimator.fit(X_subset, y_subset, **fit_params_subset)
 
             # check if estimated model is valid
             if self.is_model_valid is not None and not self.is_model_valid(
                 estimator, X_subset, y_subset
             ):
                 self.n_skips_invalid_model_ += 1
                 continue
@@ -480,16 +514,25 @@
                 continue
 
             # extract inlier data set
             inlier_idxs_subset = sample_idxs[inlier_mask_subset]
             X_inlier_subset = X[inlier_idxs_subset]
             y_inlier_subset = y[inlier_idxs_subset]
 
+            # cut `fit_params` down to `inlier_idxs_subset`
+            score_params_inlier_subset = _check_method_params(
+                X, params=routed_params.estimator.score, indices=inlier_idxs_subset
+            )
+
             # score of inlier data set
-            score_subset = estimator.score(X_inlier_subset, y_inlier_subset)
+            score_subset = estimator.score(
+                X_inlier_subset,
+                y_inlier_subset,
+                **score_params_inlier_subset,
+            )
 
             # same number of inliers but worse score -> skip current random
             # sample
             if n_inliers_subset == n_inliers_best and score_subset < score_best:
                 continue
 
             # save current random sample as best sample
@@ -545,77 +588,137 @@
                         " `max_skips`. See estimator attributes for"
                         " diagnostics (n_skips*)."
                     ),
                     ConvergenceWarning,
                 )
 
         # estimate final model using all inliers
-        if sample_weight is None:
-            estimator.fit(X_inlier_best, y_inlier_best)
-        else:
-            estimator.fit(
-                X_inlier_best,
-                y_inlier_best,
-                sample_weight=sample_weight[inlier_best_idxs_subset],
-            )
+        fit_params_best_idxs_subset = _check_method_params(
+            X, params=routed_params.estimator.fit, indices=inlier_best_idxs_subset
+        )
+
+        estimator.fit(X_inlier_best, y_inlier_best, **fit_params_best_idxs_subset)
 
         self.estimator_ = estimator
         self.inlier_mask_ = inlier_mask_best
         return self
 
-    def predict(self, X):
+    def predict(self, X, **params):
         """Predict using the estimated model.
 
         This is a wrapper for `estimator_.predict(X)`.
 
         Parameters
         ----------
         X : {array-like or sparse matrix} of shape (n_samples, n_features)
             Input data.
 
+        **params : dict
+            Parameters routed to the `predict` method of the sub-estimator via
+            the metadata routing API.
+
+            .. versionadded:: 1.5
+
+                Only available if
+                `sklearn.set_config(enable_metadata_routing=True)` is set. See
+                :ref:`Metadata Routing User Guide <metadata_routing>` for more
+                details.
+
         Returns
         -------
         y : array, shape = [n_samples] or [n_samples, n_targets]
             Returns predicted values.
         """
         check_is_fitted(self)
         X = self._validate_data(
             X,
             force_all_finite=False,
             accept_sparse=True,
             reset=False,
         )
-        return self.estimator_.predict(X)
 
-    def score(self, X, y):
+        _raise_for_params(params, self, "predict")
+
+        if _routing_enabled():
+            predict_params = process_routing(self, "predict", **params).estimator[
+                "predict"
+            ]
+        else:
+            predict_params = {}
+
+        return self.estimator_.predict(X, **predict_params)
+
+    def score(self, X, y, **params):
         """Return the score of the prediction.
 
         This is a wrapper for `estimator_.score(X, y)`.
 
         Parameters
         ----------
         X : (array-like or sparse matrix} of shape (n_samples, n_features)
             Training data.
 
         y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Target values.
 
+        **params : dict
+            Parameters routed to the `score` method of the sub-estimator via
+            the metadata routing API.
+
+            .. versionadded:: 1.5
+
+                Only available if
+                `sklearn.set_config(enable_metadata_routing=True)` is set. See
+                :ref:`Metadata Routing User Guide <metadata_routing>` for more
+                details.
+
         Returns
         -------
         z : float
             Score of the prediction.
         """
         check_is_fitted(self)
         X = self._validate_data(
             X,
             force_all_finite=False,
             accept_sparse=True,
             reset=False,
         )
-        return self.estimator_.score(X, y)
+
+        _raise_for_params(params, self, "score")
+        if _routing_enabled():
+            score_params = process_routing(self, "score", **params).estimator["score"]
+        else:
+            score_params = {}
+
+        return self.estimator_.score(X, y, **score_params)
+
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
+
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__).add(
+            estimator=self.estimator,
+            method_mapping=MethodMapping()
+            .add(caller="fit", callee="fit")
+            .add(caller="fit", callee="score")
+            .add(caller="score", callee="score")
+            .add(caller="predict", callee="predict"),
+        )
+        return router
 
     def _more_tags(self):
         return {
             "_xfail_checks": {
                 "check_sample_weights_invariance": (
                     "zero sample_weight is not equivalent to removing samples"
                 ),
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_ridge.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_ridge.py`

 * *Files 8% similar despite different names*

```diff
@@ -21,26 +21,38 @@
 
 from ..base import MultiOutputMixin, RegressorMixin, _fit_context, is_classifier
 from ..exceptions import ConvergenceWarning
 from ..metrics import check_scoring, get_scorer_names
 from ..model_selection import GridSearchCV
 from ..preprocessing import LabelBinarizer
 from ..utils import (
+    Bunch,
     check_array,
     check_consistent_length,
     check_scalar,
     column_or_1d,
     compute_sample_weight,
+    deprecated,
 )
-from ..utils._param_validation import Interval, StrOptions, validate_params
+from ..utils._array_api import (
+    _is_numpy_namespace,
+    _ravel,
+    device,
+    get_namespace,
+    get_namespace_and_device,
+)
+from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params
 from ..utils.extmath import row_norms, safe_sparse_dot
 from ..utils.fixes import _sparse_linalg_cg
 from ..utils.metadata_routing import (
-    _raise_for_unsupported_routing,
-    _RoutingNotSupportedMixin,
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    _routing_enabled,
+    process_routing,
 )
 from ..utils.sparsefuncs import mean_variance_axis
 from ..utils.validation import _check_sample_weight, check_is_fitted
 from ._base import LinearClassifierMixin, LinearModel, _preprocess_data, _rescale_data
 from ._sag import sag_solver
 
 
@@ -269,23 +281,24 @@
 
         if has_sw:
             dual_coefs *= sw[np.newaxis, :]
 
         return dual_coefs.T
 
 
-def _solve_svd(X, y, alpha):
-    U, s, Vt = linalg.svd(X, full_matrices=False)
+def _solve_svd(X, y, alpha, xp=None):
+    xp, _ = get_namespace(X, xp=xp)
+    U, s, Vt = xp.linalg.svd(X, full_matrices=False)
     idx = s > 1e-15  # same default value as scipy.linalg.pinv
-    s_nnz = s[idx][:, np.newaxis]
-    UTy = np.dot(U.T, y)
-    d = np.zeros((s.size, alpha.size), dtype=X.dtype)
+    s_nnz = s[idx][:, None]
+    UTy = U.T @ y
+    d = xp.zeros((s.shape[0], alpha.shape[0]), dtype=X.dtype, device=device(X))
     d[idx] = s_nnz / (s_nnz**2 + alpha)
     d_UT_y = d * UTy
-    return np.dot(Vt.T, d_UT_y).T
+    return (Vt.T @ d_UT_y).T
 
 
 def _solve_lbfgs(
     X,
     y,
     alpha,
     positive=True,
@@ -592,36 +605,37 @@
     max_iter=None,
     tol=1e-4,
     verbose=0,
     positive=False,
     random_state=None,
     return_n_iter=False,
     return_intercept=False,
+    return_solver=False,
     X_scale=None,
     X_offset=None,
     check_input=True,
     fit_intercept=False,
 ):
+    xp, is_array_api_compliant, device_ = get_namespace_and_device(
+        X, y, sample_weight, X_scale, X_offset
+    )
+    is_numpy_namespace = _is_numpy_namespace(xp)
+    X_is_sparse = sparse.issparse(X)
+
     has_sw = sample_weight is not None
 
-    if solver == "auto":
-        if positive:
-            solver = "lbfgs"
-        elif return_intercept:
-            # sag supports fitting intercept directly
-            solver = "sag"
-        elif not sparse.issparse(X):
-            solver = "cholesky"
-        else:
-            solver = "sparse_cg"
+    solver = resolve_solver(solver, positive, return_intercept, X_is_sparse, xp)
 
-    if solver not in ("sparse_cg", "cholesky", "svd", "lsqr", "sag", "saga", "lbfgs"):
+    if is_numpy_namespace and not X_is_sparse:
+        X = np.asarray(X)
+
+    if not is_numpy_namespace and solver != "svd":
         raise ValueError(
-            "Known solvers are 'sparse_cg', 'cholesky', 'svd'"
-            " 'lsqr', 'sag', 'saga' or 'lbfgs'. Got %s." % solver
+            f"Array API dispatch to namespace {xp.__name__} only supports "
+            f"solver 'svd'. Got '{solver}'."
         )
 
     if positive and solver != "lbfgs":
         raise ValueError(
             "When positive=True, only 'lbfgs' solver can be used. "
             f"Please change solver {solver} to 'lbfgs' "
             "or set positive=False."
@@ -637,28 +651,28 @@
         raise ValueError(
             "In Ridge, only 'sag' solver can directly fit the "
             "intercept. Please change solver to 'sag' or set "
             "return_intercept=False."
         )
 
     if check_input:
-        _dtype = [np.float64, np.float32]
-        _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X), solver)
+        _dtype = [xp.float64, xp.float32]
+        _accept_sparse = _get_valid_accept_sparse(X_is_sparse, solver)
         X = check_array(X, accept_sparse=_accept_sparse, dtype=_dtype, order="C")
         y = check_array(y, dtype=X.dtype, ensure_2d=False, order=None)
     check_consistent_length(X, y)
 
     n_samples, n_features = X.shape
 
     if y.ndim > 2:
         raise ValueError("Target y has the wrong shape %s" % str(y.shape))
 
     ravel = False
     if y.ndim == 1:
-        y = y.reshape(-1, 1)
+        y = xp.reshape(y, (-1, 1))
         ravel = True
 
     n_samples_, n_targets = y.shape
 
     if n_samples != n_samples_:
         raise ValueError(
             "Number of samples in X and y does not correspond: %d != %d"
@@ -671,33 +685,35 @@
         if solver not in ["sag", "saga"]:
             # SAG supports sample_weight directly. For other solvers,
             # we implement sample_weight via a simple rescaling.
             X, y, sample_weight_sqrt = _rescale_data(X, y, sample_weight)
 
     # Some callers of this method might pass alpha as single
     # element array which already has been validated.
-    if alpha is not None and not isinstance(alpha, np.ndarray):
+    if alpha is not None and not isinstance(alpha, type(xp.asarray([0.0]))):
         alpha = check_scalar(
             alpha,
             "alpha",
             target_type=numbers.Real,
             min_val=0.0,
             include_boundaries="left",
         )
 
     # There should be either 1 or n_targets penalties
-    alpha = np.asarray(alpha, dtype=X.dtype).ravel()
-    if alpha.size not in [1, n_targets]:
+    alpha = _ravel(xp.asarray(alpha, device=device_, dtype=X.dtype), xp=xp)
+    if alpha.shape[0] not in [1, n_targets]:
         raise ValueError(
             "Number of targets and number of penalties do not correspond: %d != %d"
-            % (alpha.size, n_targets)
+            % (alpha.shape[0], n_targets)
         )
 
-    if alpha.size == 1 and n_targets > 1:
-        alpha = np.repeat(alpha, n_targets)
+    if alpha.shape[0] == 1 and n_targets > 1:
+        alpha = xp.full(
+            shape=(n_targets,), fill_value=alpha[0], dtype=alpha.dtype, device=device_
+        )
 
     n_iter = None
     if solver == "sparse_cg":
         coef = _solve_sparse_cg(
             X,
             y,
             alpha,
@@ -771,15 +787,14 @@
                 intercept[i] = coef_[-1]
             else:
                 coef[i] = coef_
             n_iter[i] = n_iter_
 
         if intercept.shape[0] == 1:
             intercept = intercept[0]
-        coef = np.asarray(coef)
 
     elif solver == "lbfgs":
         coef = _solve_lbfgs(
             X,
             y,
             alpha,
             positive=positive,
@@ -787,30 +802,79 @@
             max_iter=max_iter,
             X_offset=X_offset,
             X_scale=X_scale,
             sample_weight_sqrt=sample_weight_sqrt if has_sw else None,
         )
 
     if solver == "svd":
-        if sparse.issparse(X):
+        if X_is_sparse:
             raise TypeError("SVD solver does not support sparse inputs currently")
-        coef = _solve_svd(X, y, alpha)
+        coef = _solve_svd(X, y, alpha, xp)
 
     if ravel:
-        # When y was passed as a 1d-array, we flatten the coefficients.
-        coef = coef.ravel()
+        coef = _ravel(coef)
+
+    coef = xp.asarray(coef)
 
     if return_n_iter and return_intercept:
-        return coef, n_iter, intercept
+        res = coef, n_iter, intercept
     elif return_intercept:
-        return coef, intercept
+        res = coef, intercept
     elif return_n_iter:
-        return coef, n_iter
+        res = coef, n_iter
     else:
-        return coef
+        res = coef
+
+    return (*res, solver) if return_solver else res
+
+
+def resolve_solver(solver, positive, return_intercept, is_sparse, xp):
+    if solver != "auto":
+        return solver
+
+    is_numpy_namespace = _is_numpy_namespace(xp)
+
+    auto_solver_np = resolve_solver_for_numpy(positive, return_intercept, is_sparse)
+    if is_numpy_namespace:
+        return auto_solver_np
+
+    if positive:
+        raise ValueError(
+            "The solvers that support positive fitting do not support "
+            f"Array API dispatch to namespace {xp.__name__}. Please "
+            "either disable Array API dispatch, or use a numpy-like "
+            "namespace, or set `positive=False`."
+        )
+
+    # At the moment, Array API dispatch only supports the "svd" solver.
+    solver = "svd"
+    if solver != auto_solver_np:
+        warnings.warn(
+            f"Using Array API dispatch to namespace {xp.__name__} with "
+            f"`solver='auto'` will result in using the solver '{solver}'. "
+            "The results may differ from those when using a Numpy array, "
+            f"because in that case the preferred solver would be {auto_solver_np}. "
+            f"Set `solver='{solver}'` to suppress this warning."
+        )
+
+    return solver
+
+
+def resolve_solver_for_numpy(positive, return_intercept, is_sparse):
+    if positive:
+        return "lbfgs"
+
+    if return_intercept:
+        # sag supports fitting intercept directly
+        return "sag"
+
+    if not is_sparse:
+        return "cholesky"
+
+    return "sparse_cg"
 
 
 class _BaseRidge(LinearModel, metaclass=ABCMeta):
     _parameter_constraints: dict = {
         "alpha": [Interval(Real, 0, None, closed="left"), np.ndarray],
         "fit_intercept": ["boolean"],
         "copy_X": ["boolean"],
@@ -844,14 +908,16 @@
         self.max_iter = max_iter
         self.tol = tol
         self.solver = solver
         self.positive = positive
         self.random_state = random_state
 
     def fit(self, X, y, sample_weight=None):
+        xp, is_array_api_compliant = get_namespace(X, y, sample_weight)
+
         if self.solver == "lbfgs" and not self.positive:
             raise ValueError(
                 "'lbfgs' solver can be used only when positive=True. "
                 "Please use another solver."
             )
 
         if self.positive:
@@ -895,51 +961,53 @@
             y,
             fit_intercept=self.fit_intercept,
             copy=self.copy_X,
             sample_weight=sample_weight,
         )
 
         if solver == "sag" and sparse.issparse(X) and self.fit_intercept:
-            self.coef_, self.n_iter_, self.intercept_ = _ridge_regression(
+            self.coef_, self.n_iter_, self.intercept_, self.solver_ = _ridge_regression(
                 X,
                 y,
                 alpha=self.alpha,
                 sample_weight=sample_weight,
                 max_iter=self.max_iter,
                 tol=self.tol,
                 solver="sag",
                 positive=self.positive,
                 random_state=self.random_state,
                 return_n_iter=True,
                 return_intercept=True,
+                return_solver=True,
                 check_input=False,
             )
             # add the offset which was subtracted by _preprocess_data
             self.intercept_ += y_offset
 
         else:
             if sparse.issparse(X) and self.fit_intercept:
                 # required to fit intercept with sparse_cg and lbfgs solver
                 params = {"X_offset": X_offset, "X_scale": X_scale}
             else:
                 # for dense matrices or when intercept is set to 0
                 params = {}
 
-            self.coef_, self.n_iter_ = _ridge_regression(
+            self.coef_, self.n_iter_, self.solver_ = _ridge_regression(
                 X,
                 y,
                 alpha=self.alpha,
                 sample_weight=sample_weight,
                 max_iter=self.max_iter,
                 tol=self.tol,
                 solver=solver,
                 positive=self.positive,
                 random_state=self.random_state,
                 return_n_iter=True,
                 return_intercept=False,
+                return_solver=True,
                 check_input=False,
                 fit_intercept=self.fit_intercept,
                 **params,
             )
             self._set_intercept(X_offset, y_offset, X_scale)
 
         return self
@@ -1087,14 +1155,20 @@
 
     feature_names_in_ : ndarray of shape (`n_features_in_`,)
         Names of features seen during :term:`fit`. Defined only when `X`
         has feature names that are all strings.
 
         .. versionadded:: 1.0
 
+    solver_ : str
+        The solver that was used at fit time by the computational
+        routines.
+
+        .. versionadded:: 1.5
+
     See Also
     --------
     RidgeClassifier : Ridge classifier.
     RidgeCV : Ridge regression with built-in cross validation.
     :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression
         combines ridge regression with the kernel trick.
 
@@ -1160,24 +1234,28 @@
 
         Returns
         -------
         self : object
             Fitted estimator.
         """
         _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X), self.solver)
+        xp, _ = get_namespace(X, y, sample_weight)
         X, y = self._validate_data(
             X,
             y,
             accept_sparse=_accept_sparse,
-            dtype=[np.float64, np.float32],
+            dtype=[xp.float64, xp.float32],
             multi_output=True,
             y_numeric=True,
         )
         return super().fit(X, y, sample_weight=sample_weight)
 
+    def _more_tags(self):
+        return {"array_api_support": True}
+
 
 class _RidgeClassifierMixin(LinearClassifierMixin):
     def _prepare_data(self, X, y, sample_weight, solver):
         """Validate `X` and `y` and binarize `y`.
 
         Parameters
         ----------
@@ -1395,14 +1473,20 @@
 
     feature_names_in_ : ndarray of shape (`n_features_in_`,)
         Names of features seen during :term:`fit`. Defined only when `X`
         has feature names that are all strings.
 
         .. versionadded:: 1.0
 
+    solver_ : str
+        The solver that was used at fit time by the computational
+        routines.
+
+        .. versionadded:: 1.5
+
     See Also
     --------
     Ridge : Ridge regression.
     RidgeClassifierCV :  Ridge classifier with built-in cross validation.
 
     Notes
     -----
@@ -1644,24 +1728,24 @@
         self,
         alphas=(0.1, 1.0, 10.0),
         *,
         fit_intercept=True,
         scoring=None,
         copy_X=True,
         gcv_mode=None,
-        store_cv_values=False,
+        store_cv_results=False,
         is_clf=False,
         alpha_per_target=False,
     ):
         self.alphas = alphas
         self.fit_intercept = fit_intercept
         self.scoring = scoring
         self.copy_X = copy_X
         self.gcv_mode = gcv_mode
-        self.store_cv_values = store_cv_values
+        self.store_cv_results = store_cv_results
         self.is_clf = is_clf
         self.alpha_per_target = alpha_per_target
 
     @staticmethod
     def _decomp_diag(v_prime, Q):
         # compute diagonal of the matrix: dot(Q, dot(diag(v_prime), Q^T))
         return (v_prime * Q**2).sum(axis=-1)
@@ -1966,15 +2050,15 @@
         c = np.dot(U, self._diag_dot(w, UT_y)) + (alpha**-1) * y
         G_inverse_diag = self._decomp_diag(w, U) + (alpha**-1)
         if len(y.shape) != 1:
             # handle case where y is 2-d
             G_inverse_diag = G_inverse_diag[:, np.newaxis]
         return G_inverse_diag, c
 
-    def fit(self, X, y, sample_weight=None):
+    def fit(self, X, y, sample_weight=None, score_params=None):
         """Fit Ridge regression model with gcv.
 
         Parameters
         ----------
         X : {ndarray, sparse matrix} of shape (n_samples, n_features)
             Training data. Will be cast to float64 if necessary.
 
@@ -1983,14 +2067,21 @@
 
         sample_weight : float or ndarray of shape (n_samples,), default=None
             Individual weights for each sample. If given a float, every sample
             will have the same weight. Note that the scale of `sample_weight`
             has an impact on the loss; i.e. multiplying all weights by `k`
             is equivalent to setting `alpha / k`.
 
+        score_params : dict, default=None
+            Parameters to be passed to the underlying scorer.
+
+            .. versionadded:: 1.5
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
         """
         X, y = self._validate_data(
             X,
             y,
@@ -2036,58 +2127,44 @@
         if sample_weight is not None:
             X, y, sqrt_sw = _rescale_data(X, y, sample_weight)
         else:
             sqrt_sw = np.ones(n_samples, dtype=X.dtype)
 
         X_mean, *decomposition = decompose(X, y, sqrt_sw)
 
-        scorer = check_scoring(self, scoring=self.scoring, allow_none=True)
-        error = scorer is None
+        scorer = self._get_scorer()
 
         n_y = 1 if len(y.shape) == 1 else y.shape[1]
         n_alphas = 1 if np.ndim(self.alphas) == 0 else len(self.alphas)
 
-        if self.store_cv_values:
-            self.cv_values_ = np.empty((n_samples * n_y, n_alphas), dtype=X.dtype)
+        if self.store_cv_results:
+            self.cv_results_ = np.empty((n_samples * n_y, n_alphas), dtype=X.dtype)
 
         best_coef, best_score, best_alpha = None, None, None
 
         for i, alpha in enumerate(np.atleast_1d(self.alphas)):
             G_inverse_diag, c = solve(float(alpha), y, sqrt_sw, X_mean, *decomposition)
-            if error:
+            if scorer is None:
                 squared_errors = (c / G_inverse_diag) ** 2
-                if self.alpha_per_target:
-                    alpha_score = -squared_errors.mean(axis=0)
-                else:
-                    alpha_score = -squared_errors.mean()
-                if self.store_cv_values:
-                    self.cv_values_[:, i] = squared_errors.ravel()
+                alpha_score = self._score_without_scorer(squared_errors=squared_errors)
+                if self.store_cv_results:
+                    self.cv_results_[:, i] = squared_errors.ravel()
             else:
                 predictions = y - (c / G_inverse_diag)
-                if self.store_cv_values:
-                    self.cv_values_[:, i] = predictions.ravel()
+                if self.store_cv_results:
+                    self.cv_results_[:, i] = predictions.ravel()
 
-                if self.is_clf:
-                    identity_estimator = _IdentityClassifier(classes=np.arange(n_y))
-                    alpha_score = scorer(
-                        identity_estimator, predictions, y.argmax(axis=1)
-                    )
-                else:
-                    identity_estimator = _IdentityRegressor()
-                    if self.alpha_per_target:
-                        alpha_score = np.array(
-                            [
-                                scorer(identity_estimator, predictions[:, j], y[:, j])
-                                for j in range(n_y)
-                            ]
-                        )
-                    else:
-                        alpha_score = scorer(
-                            identity_estimator, predictions.ravel(), y.ravel()
-                        )
+                score_params = score_params or {}
+                alpha_score = self._score(
+                    predictions=predictions,
+                    y=y,
+                    n_y=n_y,
+                    scorer=scorer,
+                    score_params=score_params,
+                )
 
             # Keep track of the best model
             if best_score is None:
                 # initialize
                 if self.alpha_per_target and n_y > 1:
                     best_coef = c
                     best_score = np.atleast_1d(alpha_score)
@@ -2113,55 +2190,106 @@
 
         if sparse.issparse(X):
             X_offset = X_mean * X_scale
         else:
             X_offset += X_mean * X_scale
         self._set_intercept(X_offset, y_offset, X_scale)
 
-        if self.store_cv_values:
+        if self.store_cv_results:
             if len(y.shape) == 1:
-                cv_values_shape = n_samples, n_alphas
+                cv_results_shape = n_samples, n_alphas
             else:
-                cv_values_shape = n_samples, n_y, n_alphas
-            self.cv_values_ = self.cv_values_.reshape(cv_values_shape)
+                cv_results_shape = n_samples, n_y, n_alphas
+            self.cv_results_ = self.cv_results_.reshape(cv_results_shape)
 
         return self
 
+    def _get_scorer(self):
+        return check_scoring(self, scoring=self.scoring, allow_none=True)
+
+    def _score_without_scorer(self, squared_errors):
+        """Performs scoring using squared errors when the scorer is None."""
+        if self.alpha_per_target:
+            _score = -squared_errors.mean(axis=0)
+        else:
+            _score = -squared_errors.mean()
+
+        return _score
+
+    def _score(self, *, predictions, y, n_y, scorer, score_params):
+        """Performs scoring with the specified scorer using the
+        predictions and the true y values.
+        """
+        if self.is_clf:
+            identity_estimator = _IdentityClassifier(classes=np.arange(n_y))
+            _score = scorer(
+                identity_estimator,
+                predictions,
+                y.argmax(axis=1),
+                **score_params,
+            )
+        else:
+            identity_estimator = _IdentityRegressor()
+            if self.alpha_per_target:
+                _score = np.array(
+                    [
+                        scorer(
+                            identity_estimator,
+                            predictions[:, j],
+                            y[:, j],
+                            **score_params,
+                        )
+                        for j in range(n_y)
+                    ]
+                )
+            else:
+                _score = scorer(
+                    identity_estimator,
+                    predictions.ravel(),
+                    y.ravel(),
+                    **score_params,
+                )
+
+        return _score
+
 
 class _BaseRidgeCV(LinearModel):
     _parameter_constraints: dict = {
         "alphas": ["array-like", Interval(Real, 0, None, closed="neither")],
         "fit_intercept": ["boolean"],
         "scoring": [StrOptions(set(get_scorer_names())), callable, None],
         "cv": ["cv_object"],
         "gcv_mode": [StrOptions({"auto", "svd", "eigen"}), None],
-        "store_cv_values": ["boolean"],
+        "store_cv_results": ["boolean", Hidden(None)],
         "alpha_per_target": ["boolean"],
+        "store_cv_values": ["boolean", Hidden(StrOptions({"deprecated"}))],
     }
 
     def __init__(
         self,
         alphas=(0.1, 1.0, 10.0),
         *,
         fit_intercept=True,
         scoring=None,
         cv=None,
         gcv_mode=None,
-        store_cv_values=False,
+        store_cv_results=None,
         alpha_per_target=False,
+        store_cv_values="deprecated",
     ):
         self.alphas = alphas
         self.fit_intercept = fit_intercept
         self.scoring = scoring
         self.cv = cv
         self.gcv_mode = gcv_mode
-        self.store_cv_values = store_cv_values
+        self.store_cv_results = store_cv_results
         self.alpha_per_target = alpha_per_target
+        self.store_cv_values = store_cv_values
 
-    def fit(self, X, y, sample_weight=None):
+    def fit(self, X, y, sample_weight=None, **params):
         """Fit Ridge regression model with cv.
 
         Parameters
         ----------
         X : ndarray of shape (n_samples, n_features)
             Training data. If using GCV, will be cast to float64
             if necessary.
@@ -2169,95 +2297,196 @@
         y : ndarray of shape (n_samples,) or (n_samples, n_targets)
             Target values. Will be cast to X's dtype if necessary.
 
         sample_weight : float or ndarray of shape (n_samples,), default=None
             Individual weights for each sample. If given a float, every sample
             will have the same weight.
 
+        **params : dict, default=None
+            Extra parameters for the underlying scorer.
+
+            .. versionadded:: 1.5
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Fitted estimator.
 
         Notes
         -----
         When sample_weight is provided, the selected hyperparameter may depend
         on whether we use leave-one-out cross-validation (cv=None or cv='auto')
         or another form of cross-validation, because only leave-one-out
         cross-validation takes the sample weights into account when computing
         the validation score.
         """
+        _raise_for_params(params, self, "fit")
         cv = self.cv
 
-        check_scalar_alpha = partial(
-            check_scalar,
-            target_type=numbers.Real,
-            min_val=0.0,
-            include_boundaries="neither",
-        )
+        # TODO(1.7): Remove in 1.7
+        # Also change `store_cv_results` default back to False
+        if self.store_cv_values != "deprecated":
+            if self.store_cv_results is not None:
+                raise ValueError(
+                    "Both 'store_cv_values' and 'store_cv_results' were set. "
+                    "'store_cv_values' is deprecated in version 1.5 and will be "
+                    "removed in 1.7. To avoid this error, only set 'store_cv_results'."
+                )
+            warnings.warn(
+                (
+                    "'store_cv_values' is deprecated in version 1.5 and will be "
+                    "removed in 1.7. Use 'store_cv_results' instead."
+                ),
+                FutureWarning,
+            )
+            self._store_cv_results = self.store_cv_values
+        elif self.store_cv_results is None:
+            self._store_cv_results = False
+        else:
+            self._store_cv_results = self.store_cv_results
+
+        # `_RidgeGCV` does not work for alpha = 0
+        if cv is None:
+            check_scalar_alpha = partial(
+                check_scalar,
+                target_type=numbers.Real,
+                min_val=0.0,
+                include_boundaries="neither",
+            )
+        else:
+            check_scalar_alpha = partial(
+                check_scalar,
+                target_type=numbers.Real,
+                min_val=0.0,
+                include_boundaries="left",
+            )
 
         if isinstance(self.alphas, (np.ndarray, list, tuple)):
             n_alphas = 1 if np.ndim(self.alphas) == 0 else len(self.alphas)
             if n_alphas != 1:
                 for index, alpha in enumerate(self.alphas):
                     alpha = check_scalar_alpha(alpha, f"alphas[{index}]")
             else:
                 self.alphas[0] = check_scalar_alpha(self.alphas[0], "alphas")
         alphas = np.asarray(self.alphas)
 
+        if sample_weight is not None:
+            params["sample_weight"] = sample_weight
+
         if cv is None:
+            if _routing_enabled():
+                routed_params = process_routing(
+                    self,
+                    "fit",
+                    **params,
+                )
+            else:
+                routed_params = Bunch(scorer=Bunch(score={}))
+                if sample_weight is not None:
+                    routed_params.scorer.score["sample_weight"] = sample_weight
+
             estimator = _RidgeGCV(
                 alphas,
                 fit_intercept=self.fit_intercept,
                 scoring=self.scoring,
                 gcv_mode=self.gcv_mode,
-                store_cv_values=self.store_cv_values,
+                store_cv_results=self._store_cv_results,
                 is_clf=is_classifier(self),
                 alpha_per_target=self.alpha_per_target,
             )
-            estimator.fit(X, y, sample_weight=sample_weight)
+            estimator.fit(
+                X,
+                y,
+                sample_weight=sample_weight,
+                score_params=routed_params.scorer.score,
+            )
             self.alpha_ = estimator.alpha_
             self.best_score_ = estimator.best_score_
-            if self.store_cv_values:
-                self.cv_values_ = estimator.cv_values_
+            if self._store_cv_results:
+                self.cv_results_ = estimator.cv_results_
         else:
-            if self.store_cv_values:
-                raise ValueError("cv!=None and store_cv_values=True are incompatible")
+            if self._store_cv_results:
+                raise ValueError("cv!=None and store_cv_results=True are incompatible")
             if self.alpha_per_target:
                 raise ValueError("cv!=None and alpha_per_target=True are incompatible")
 
             parameters = {"alpha": alphas}
             solver = "sparse_cg" if sparse.issparse(X) else "auto"
             model = RidgeClassifier if is_classifier(self) else Ridge
-            gs = GridSearchCV(
-                model(
-                    fit_intercept=self.fit_intercept,
-                    solver=solver,
-                ),
+            estimator = model(
+                fit_intercept=self.fit_intercept,
+                solver=solver,
+            )
+            if _routing_enabled():
+                estimator.set_fit_request(sample_weight=True)
+
+            grid_search = GridSearchCV(
+                estimator,
                 parameters,
                 cv=cv,
                 scoring=self.scoring,
             )
-            gs.fit(X, y, sample_weight=sample_weight)
-            estimator = gs.best_estimator_
-            self.alpha_ = gs.best_estimator_.alpha
-            self.best_score_ = gs.best_score_
+
+            grid_search.fit(X, y, **params)
+            estimator = grid_search.best_estimator_
+            self.alpha_ = grid_search.best_estimator_.alpha
+            self.best_score_ = grid_search.best_score_
 
         self.coef_ = estimator.coef_
         self.intercept_ = estimator.intercept_
         self.n_features_in_ = estimator.n_features_in_
         if hasattr(estimator, "feature_names_in_"):
             self.feature_names_in_ = estimator.feature_names_in_
 
         return self
 
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
 
-class RidgeCV(
-    _RoutingNotSupportedMixin, MultiOutputMixin, RegressorMixin, _BaseRidgeCV
-):
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = (
+            MetadataRouter(owner=self.__class__.__name__)
+            .add_self_request(self)
+            .add(
+                scorer=self._get_scorer(),
+                method_mapping=MethodMapping().add(callee="score", caller="fit"),
+            )
+        )
+        return router
+
+    def _get_scorer(self):
+        return check_scoring(self, scoring=self.scoring, allow_none=True)
+
+    # TODO(1.7): Remove
+    # mypy error: Decorated property not supported
+    @deprecated(  # type: ignore
+        "Attribute `cv_values_` is deprecated in version 1.5 and will be removed "
+        "in 1.7. Use `cv_results_` instead."
+    )
+    @property
+    def cv_values_(self):
+        return self.cv_results_
+
+
+class RidgeCV(MultiOutputMixin, RegressorMixin, _BaseRidgeCV):
     """Ridge regression with built-in cross-validation.
 
     See glossary entry for :term:`cross-validation estimator`.
 
     By default, it performs efficient Leave-One-Out Cross-Validation.
 
     Read more in the :ref:`User Guide <ridge_regression>`.
@@ -2268,28 +2497,26 @@
         Array of alpha values to try.
         Regularization strength; must be a positive float. Regularization
         improves the conditioning of the problem and reduces the variance of
         the estimates. Larger values specify stronger regularization.
         Alpha corresponds to ``1 / (2C)`` in other linear models such as
         :class:`~sklearn.linear_model.LogisticRegression` or
         :class:`~sklearn.svm.LinearSVC`.
-        If using Leave-One-Out cross-validation, alphas must be positive.
+        If using Leave-One-Out cross-validation, alphas must be strictly positive.
 
     fit_intercept : bool, default=True
         Whether to calculate the intercept for this model. If set
         to false, no intercept will be used in calculations
         (i.e. data is expected to be centered).
 
     scoring : str, callable, default=None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
-        If None, the negative mean squared error if cv is 'auto' or None
-        (i.e. when using leave-one-out cross-validation), and r2 score
-        otherwise.
+        A string (see :ref:`scoring_parameter`) or a scorer callable object /
+        function with signature ``scorer(estimator, X, y)``. If None, the
+        negative mean squared error if cv is 'auto' or None (i.e. when using
+        leave-one-out cross-validation), and r2 score otherwise.
 
     cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
 
         - None, to use the efficient Leave-One-Out cross-validation
         - integer, to specify the number of folds.
@@ -2311,39 +2538,55 @@
             'svd' : force use of singular value decomposition of X when X is
                 dense, eigenvalue decomposition of X^T.X when X is sparse.
             'eigen' : force computation via eigendecomposition of X.X^T
 
         The 'auto' mode is the default and is intended to pick the cheaper
         option of the two depending on the shape of the training data.
 
-    store_cv_values : bool, default=False
+    store_cv_results : bool, default=False
         Flag indicating if the cross-validation values corresponding to
         each alpha should be stored in the ``cv_values_`` attribute (see
         below). This flag is only compatible with ``cv=None`` (i.e. using
         Leave-One-Out Cross-Validation).
 
+        .. versionchanged:: 1.5
+            Parameter name changed from `store_cv_values` to `store_cv_results`.
+
     alpha_per_target : bool, default=False
         Flag indicating whether to optimize the alpha value (picked from the
         `alphas` parameter list) for each target separately (for multi-output
         settings: multiple prediction targets). When set to `True`, after
         fitting, the `alpha_` attribute will contain a value for each target.
         When set to `False`, a single alpha is used for all targets.
 
         .. versionadded:: 0.24
 
+    store_cv_values : bool
+        Flag indicating if the cross-validation values corresponding to
+        each alpha should be stored in the ``cv_values_`` attribute (see
+        below). This flag is only compatible with ``cv=None`` (i.e. using
+        Leave-One-Out Cross-Validation).
+
+        .. deprecated:: 1.5
+            `store_cv_values` is deprecated in version 1.5 in favor of
+            `store_cv_results` and will be removed in version 1.7.
+
     Attributes
     ----------
-    cv_values_ : ndarray of shape (n_samples, n_alphas) or \
+    cv_results_ : ndarray of shape (n_samples, n_alphas) or \
             shape (n_samples, n_targets, n_alphas), optional
         Cross-validation values for each alpha (only available if
-        ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been
+        ``store_cv_results=True`` and ``cv=None``). After ``fit()`` has been
         called, this attribute will contain the mean squared errors if
         `scoring is None` otherwise it will contain standardized per point
         prediction values.
 
+        .. versionchanged:: 1.5
+            `cv_values_` changed to `cv_results_`.
+
     coef_ : ndarray of shape (n_features) or (n_targets, n_features)
         Weight vector(s).
 
     intercept_ : float or ndarray of shape (n_targets,)
         Independent term in decision function. Set to 0.0 if
         ``fit_intercept = False``.
 
@@ -2381,15 +2624,15 @@
     >>> X, y = load_diabetes(return_X_y=True)
     >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)
     >>> clf.score(X, y)
     0.5166...
     """
 
     @_fit_context(prefer_skip_nested_validation=True)
-    def fit(self, X, y, sample_weight=None):
+    def fit(self, X, y, sample_weight=None, **params):
         """Fit Ridge regression model with cv.
 
         Parameters
         ----------
         X : ndarray of shape (n_samples, n_features)
             Training data. If using GCV, will be cast to float64
             if necessary.
@@ -2397,33 +2640,42 @@
         y : ndarray of shape (n_samples,) or (n_samples, n_targets)
             Target values. Will be cast to X's dtype if necessary.
 
         sample_weight : float or ndarray of shape (n_samples,), default=None
             Individual weights for each sample. If given a float, every sample
             will have the same weight.
 
+        **params : dict, default=None
+            Parameters to be passed to the underlying scorer.
+
+            .. versionadded:: 1.5
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Fitted estimator.
 
         Notes
         -----
         When sample_weight is provided, the selected hyperparameter may depend
         on whether we use leave-one-out cross-validation (cv=None or cv='auto')
         or another form of cross-validation, because only leave-one-out
         cross-validation takes the sample weights into account when computing
         the validation score.
         """
-        _raise_for_unsupported_routing(self, "fit", sample_weight=sample_weight)
-        super().fit(X, y, sample_weight=sample_weight)
+        super().fit(X, y, sample_weight=sample_weight, **params)
         return self
 
 
-class RidgeClassifierCV(_RoutingNotSupportedMixin, _RidgeClassifierMixin, _BaseRidgeCV):
+class RidgeClassifierCV(_RidgeClassifierMixin, _BaseRidgeCV):
     """Ridge classifier with built-in cross-validation.
 
     See glossary entry for :term:`cross-validation estimator`.
 
     By default, it performs Leave-One-Out Cross-Validation. Currently,
     only the n_features > n_samples case is handled efficiently.
 
@@ -2435,24 +2687,24 @@
         Array of alpha values to try.
         Regularization strength; must be a positive float. Regularization
         improves the conditioning of the problem and reduces the variance of
         the estimates. Larger values specify stronger regularization.
         Alpha corresponds to ``1 / (2C)`` in other linear models such as
         :class:`~sklearn.linear_model.LogisticRegression` or
         :class:`~sklearn.svm.LinearSVC`.
+        If using Leave-One-Out cross-validation, alphas must be strictly positive.
 
     fit_intercept : bool, default=True
         Whether to calculate the intercept for this model. If set
         to false, no intercept will be used in calculations
         (i.e. data is expected to be centered).
 
     scoring : str, callable, default=None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
+        A string (see :ref:`scoring_parameter`) or a scorer callable object /
+        function with signature ``scorer(estimator, X, y)``.
 
     cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
 
         - None, to use the efficient Leave-One-Out cross-validation
         - integer, to specify the number of folds.
@@ -2466,28 +2718,44 @@
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one.
 
         The "balanced" mode uses the values of y to automatically adjust
         weights inversely proportional to class frequencies in the input data
         as ``n_samples / (n_classes * np.bincount(y))``.
 
-    store_cv_values : bool, default=False
+    store_cv_results : bool, default=False
+        Flag indicating if the cross-validation results corresponding to
+        each alpha should be stored in the ``cv_results_`` attribute (see
+        below). This flag is only compatible with ``cv=None`` (i.e. using
+        Leave-One-Out Cross-Validation).
+
+        .. versionchanged:: 1.5
+            Parameter name changed from `store_cv_values` to `store_cv_results`.
+
+    store_cv_values : bool
         Flag indicating if the cross-validation values corresponding to
         each alpha should be stored in the ``cv_values_`` attribute (see
         below). This flag is only compatible with ``cv=None`` (i.e. using
         Leave-One-Out Cross-Validation).
 
+        .. deprecated:: 1.5
+            `store_cv_values` is deprecated in version 1.5 in favor of
+            `store_cv_results` and will be removed in version 1.7.
+
     Attributes
     ----------
-    cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional
-        Cross-validation values for each alpha (only if ``store_cv_values=True`` and
+    cv_results_ : ndarray of shape (n_samples, n_targets, n_alphas), optional
+        Cross-validation results for each alpha (only if ``store_cv_results=True`` and
         ``cv=None``). After ``fit()`` has been called, this attribute will
         contain the mean squared errors if `scoring is None` otherwise it
         will contain standardized per point prediction values.
 
+        .. versionchanged:: 1.5
+            `cv_values_` changed to `cv_results_`.
+
     coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)
         Coefficient of the features in the decision function.
 
         ``coef_`` is of shape (1, n_features) when the given problem is binary.
 
     intercept_ : float or ndarray of shape (n_targets,)
         Independent term in decision function. Set to 0.0 if
@@ -2548,27 +2816,29 @@
         self,
         alphas=(0.1, 1.0, 10.0),
         *,
         fit_intercept=True,
         scoring=None,
         cv=None,
         class_weight=None,
-        store_cv_values=False,
+        store_cv_results=None,
+        store_cv_values="deprecated",
     ):
         super().__init__(
             alphas=alphas,
             fit_intercept=fit_intercept,
             scoring=scoring,
             cv=cv,
+            store_cv_results=store_cv_results,
             store_cv_values=store_cv_values,
         )
         self.class_weight = class_weight
 
     @_fit_context(prefer_skip_nested_validation=True)
-    def fit(self, X, y, sample_weight=None):
+    def fit(self, X, y, sample_weight=None, **params):
         """Fit Ridge classifier with cv.
 
         Parameters
         ----------
         X : ndarray of shape (n_samples, n_features)
             Training vectors, where `n_samples` is the number of samples
             and `n_features` is the number of features. When using GCV,
@@ -2577,32 +2847,41 @@
         y : ndarray of shape (n_samples,)
             Target values. Will be cast to X's dtype if necessary.
 
         sample_weight : float or ndarray of shape (n_samples,), default=None
             Individual weights for each sample. If given a float, every sample
             will have the same weight.
 
+        **params : dict, default=None
+            Parameters to be passed to the underlying scorer.
+
+            .. versionadded:: 1.5
+                Only available if `enable_metadata_routing=True`,
+                which can be set by using
+                ``sklearn.set_config(enable_metadata_routing=True)``.
+                See :ref:`Metadata Routing User Guide <metadata_routing>` for
+                more details.
+
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        _raise_for_unsupported_routing(self, "fit", sample_weight=sample_weight)
         # `RidgeClassifier` does not accept "sag" or "saga" solver and thus support
         # csr, csc, and coo sparse matrices. By using solver="eigen" we force to accept
         # all sparse format.
         X, y, sample_weight, Y = self._prepare_data(X, y, sample_weight, solver="eigen")
 
         # If cv is None, gcv mode will be used and we used the binarized Y
         # since y will not be binarized in _RidgeGCV estimator.
         # If cv is not None, a GridSearchCV with some RidgeClassifier
         # estimators are used where y will be binarized. Thus, we pass y
         # instead of the binarized Y.
         target = Y if self.cv is None else y
-        super().fit(X, target, sample_weight=sample_weight)
+        super().fit(X, target, sample_weight=sample_weight, **params)
         return self
 
     def _more_tags(self):
         return {
             "multilabel": True,
             "_xfail_checks": {
                 "check_sample_weights_invariance": (
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_sag.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_sag.py`

 * *Files 2% similar despite different names*

```diff
@@ -216,18 +216,17 @@
     >>> y = rng.randn(n_samples)
     >>> clf = linear_model.Ridge(solver='sag')
     >>> clf.fit(X, y)
     Ridge(solver='sag')
 
     >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
     >>> y = np.array([1, 1, 2, 2])
-    >>> clf = linear_model.LogisticRegression(
-    ...     solver='sag', multi_class='multinomial')
+    >>> clf = linear_model.LogisticRegression(solver='sag')
     >>> clf.fit(X, y)
-    LogisticRegression(multi_class='multinomial', solver='sag')
+    LogisticRegression(solver='sag')
 
     References
     ----------
     Schmidt, M., Roux, N. L., & Bach, F. (2013).
     Minimizing finite sums with the stochastic average gradient
     https://hal.inria.fr/hal-00860051/document
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_sag_fast.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_sag_fast.pyx.tp`

 * *Files 3% similar despite different names*

```diff
@@ -23,35 +23,27 @@
 dtypes = [('64', 'double', 'np.float64'),
           ('32', 'float', 'np.float32')]
 
 }}
 """SAG and SAGA implementation"""
 
 import numpy as np
-from libc.math cimport fabs, exp, log
+from libc.math cimport exp, fabs, isfinite, log
 from libc.time cimport time, time_t
 
 from ._sgd_fast cimport LossFunction
 from ._sgd_fast cimport Log, SquaredLoss
 
 from ..utils._seq_dataset cimport SequentialDataset32, SequentialDataset64
 
 from libc.stdio cimport printf
 
 
 {{for name_suffix, c_type, np_type in dtypes}}
 
-cdef extern from "_sgd_fast_helpers.h":
-    bint skl_isfinite{{name_suffix}}({{c_type}}) nogil
-
-
-{{endfor}}
-
-{{for name_suffix, c_type, np_type in dtypes}}
-
 cdef inline {{c_type}} fmax{{name_suffix}}({{c_type}} x, {{c_type}} y) noexcept nogil:
     if x > y:
         return x
     return y
 
 {{endfor}}
 
@@ -81,35 +73,35 @@
 
 {{endfor}}
 
 
 {{for name_suffix, c_type, np_type in dtypes}}
 
 cdef class MultinomialLogLoss{{name_suffix}}:
-    cdef {{c_type}} _loss(self, {{c_type}}* prediction, {{c_type}} y, int n_classes,
+    cdef {{c_type}} _loss(self, {{c_type}} y, {{c_type}}* prediction, int n_classes,
                       {{c_type}} sample_weight) noexcept nogil:
         r"""Multinomial Logistic regression loss.
 
         The multinomial logistic loss for one sample is:
         loss = - sw \sum_c \delta_{y,c} (prediction[c] - logsumexp(prediction))
              = sw (logsumexp(prediction) - prediction[y])
 
         where:
             prediction = dot(x_sample, weights) + intercept
             \delta_{y,c} = 1 if (y == c) else 0
             sw = sample_weight
 
         Parameters
         ----------
-        prediction : pointer to a np.ndarray[{{c_type}}] of shape (n_classes,)
-            Prediction of the multinomial classifier, for current sample.
-
         y : {{c_type}}, between 0 and n_classes - 1
             Indice of the correct class for current sample (i.e. label encoded).
 
+        prediction : pointer to a np.ndarray[{{c_type}}] of shape (n_classes,)
+            Prediction of the multinomial classifier, for current sample.
+
         n_classes : integer
             Total number of classes.
 
         sample_weight : {{c_type}}
             Weight of current sample.
 
         Returns
@@ -125,15 +117,15 @@
         cdef {{c_type}} logsumexp_prediction = _logsumexp{{name_suffix}}(prediction, n_classes)
         cdef {{c_type}} loss
 
         # y is the indice of the correct class of current sample.
         loss = (logsumexp_prediction - prediction[int(y)]) * sample_weight
         return loss
 
-    cdef void dloss(self, {{c_type}}* prediction, {{c_type}} y, int n_classes,
+    cdef void dloss(self, {{c_type}} y, {{c_type}}* prediction, int n_classes,
                      {{c_type}} sample_weight, {{c_type}}* gradient_ptr) noexcept nogil:
         r"""Multinomial Logistic regression gradient of the loss.
 
         The gradient of the multinomial logistic loss with respect to a class c,
         and for one sample is:
         grad_c = - sw * (p[c] - \delta_{y,c})
 
@@ -410,17 +402,17 @@
                     intercept=&intercept_array[0],
                     prediction=&prediction[0],
                     n_classes=n_classes
                 )
 
                 # compute the gradient for this sample, given the prediction
                 if multinomial:
-                    multiloss.dloss(&prediction[0], y, n_classes, sample_weight, &gradient[0])
+                    multiloss.dloss(y, &prediction[0], n_classes, sample_weight, &gradient[0])
                 else:
-                    gradient[0] = loss.dloss(prediction[0], y) * sample_weight
+                    gradient[0] = loss.dloss(y, prediction[0]) * sample_weight
 
                 # L2 regularization by simply rescaling the weights
                 wscale *= wscale_update
 
                 # make the updates to the sum of gradients
                 for j in range(xnnz):
                     feature_ind = x_ind_ptr[j]
@@ -454,15 +446,15 @@
                                  num_seen * intercept_decay) + gradient_correction
                         else:
                             intercept_array[class_ind] -= \
                                 (step_size * intercept_sum_gradient_init[class_ind] /
                                  num_seen * intercept_decay)
 
                         # check to see that the intercept is not inf or NaN
-                        if not skl_isfinite{{name_suffix}}(intercept_array[class_ind]):
+                        if not isfinite(intercept_array[class_ind]):
                             status = -1
                             break
                     # Break from the n_samples outer loop if an error happened
                     # in the fit_intercept n_classes inner loop
                     if status == -1:
                         break
 
@@ -664,15 +656,15 @@
                 cum_sum_prox -= cumulative_sums_prox[feature_hist[feature_ind] - 1]
         if not prox:
             for class_ind in range(n_classes):
                 idx = f_idx + class_ind
                 weights[idx] -= cum_sum * sum_gradient[idx]
                 if reset:
                     weights[idx] *= wscale
-                    if not skl_isfinite{{name_suffix}}(weights[idx]):
+                    if not isfinite(weights[idx]):
                         # returning here does not require the gil as the return
                         # type is a C integer
                         return -1
         else:
             for class_ind in range(n_classes):
                 idx = f_idx + class_ind
                 if fabs(sum_gradient[idx] * cum_sum) < cum_sum_prox:
@@ -700,15 +692,15 @@
                         weights[idx] -= sum_gradient[idx] * grad_step
                         weights[idx] = _soft_thresholding{{name_suffix}}(weights[idx],
                                                           prox_step)
 
                 if reset:
                     weights[idx] *= wscale
                     # check to see that the weight is not inf or NaN
-                    if not skl_isfinite{{name_suffix}}(weights[idx]):
+                    if not isfinite(weights[idx]):
                         return -1
         if reset:
             feature_hist[feature_ind] = sample_itr % n_samples
         else:
             feature_hist[feature_ind] = sample_itr
 
     if reset:
@@ -831,18 +823,18 @@
                 wscale,
                 &intercept_array[0],
                 &prediction[0],
                 n_classes
             )
 
             # compute the gradient for this sample, given the prediction
-            multiloss.dloss(&prediction[0], y, n_classes, sample_weight, &gradient[0])
+            multiloss.dloss(y, &prediction[0], n_classes, sample_weight, &gradient[0])
 
             # compute the loss for this sample, given the prediction
-            sum_loss += multiloss._loss(&prediction[0], y, n_classes, sample_weight)
+            sum_loss += multiloss._loss(y, &prediction[0], n_classes, sample_weight)
 
             # update the sum of the gradient
             for j in range(xnnz):
                 feature_ind = x_ind_ptr[j]
                 val = x_data_ptr[j]
                 for class_ind in range(n_classes):
                     sum_gradient[feature_ind * n_classes + class_ind] += gradient[class_ind] * val
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_sgd_fast.pxd` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_sgd_fast.pxd`

 * *Files 14% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 # License: BSD 3 clause
 """Helper to load LossFunction from sgd_fast.pyx to sag_fast.pyx"""
 
 cdef class LossFunction:
-    cdef double loss(self, double p, double y) noexcept nogil
-    cdef double dloss(self, double p, double y) noexcept nogil
+    cdef double loss(self, double y, double p) noexcept nogil
+    cdef double dloss(self, double y, double p) noexcept nogil
 
 
 cdef class Regression(LossFunction):
-    cdef double loss(self, double p, double y) noexcept nogil
-    cdef double dloss(self, double p, double y) noexcept nogil
+    cdef double loss(self, double y, double p) noexcept nogil
+    cdef double dloss(self, double y, double p) noexcept nogil
 
 
 cdef class Classification(LossFunction):
-    cdef double loss(self, double p, double y) noexcept nogil
-    cdef double dloss(self, double p, double y) noexcept nogil
+    cdef double loss(self, double y, double p) noexcept nogil
+    cdef double dloss(self, double y, double p) noexcept nogil
 
 
 cdef class Log(Classification):
-    cdef double loss(self, double p, double y) noexcept nogil
-    cdef double dloss(self, double p, double y) noexcept nogil
+    cdef double loss(self, double y, double p) noexcept nogil
+    cdef double dloss(self, double y, double p) noexcept nogil
 
 
 cdef class SquaredLoss(Regression):
-    cdef double loss(self, double p, double y) noexcept nogil
-    cdef double dloss(self, double p, double y) noexcept nogil
+    cdef double loss(self, double y, double p) noexcept nogil
+    cdef double dloss(self, double y, double p) noexcept nogil
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_sgd_fast.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_sgd_fast.pyx.tp`

 * *Files 3% similar despite different names*

```diff
@@ -22,28 +22,24 @@
     ("64", "double", "np.float64"),
     ("32", "float", "np.float32"),
 ]
 
 }}
 """SGD implementation"""
 
-from cython cimport floating
 import numpy as np
 from time import time
 
-from libc.math cimport exp, log, pow, fabs, INFINITY
-cimport numpy as cnp
-cdef extern from "_sgd_fast_helpers.h":
-    bint skl_isfinite32(float) nogil
-    bint skl_isfinite64(double) nogil
+from cython cimport floating
+from libc.math cimport exp, fabs, isfinite, log, pow, INFINITY
 
+from ..utils._typedefs cimport uint32_t
 from ..utils._weight_vector cimport WeightVector32, WeightVector64
 from ..utils._seq_dataset cimport SequentialDataset32, SequentialDataset64
 
-cnp.import_array()
 
 cdef extern from *:
     """
     /* Penalty constants */
     #define NO_PENALTY 0
     #define L1 1
     #define L2 2
@@ -73,23 +69,23 @@
 # ----------------------------------------
 # Extension Types for Loss Functions
 # ----------------------------------------
 
 cdef class LossFunction:
     """Base class for convex loss functions"""
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         """Evaluate the loss function.
 
         Parameters
         ----------
-        p : double
-            The prediction, `p = w^T x + intercept`.
         y : double
             The true value (aka target).
+        p : double
+            The prediction, `p = w^T x + intercept`.
 
         Returns
         -------
         double
             The loss evaluated at `p` and `y`.
         """
         return 0.
@@ -107,15 +103,15 @@
             The true value (aka target).
 
         Returns
         -------
         double
             The derivative of the loss function with regards to `p`.
         """
-        return self.dloss(p, y)
+        return self.dloss(y, p)
 
     def py_loss(self, double p, double y):
         """Python version of `loss` for testing.
 
         Pytest needs a python function and can't use cdef functions.
 
         Parameters
@@ -126,73 +122,73 @@
             The true value (aka target).
 
         Returns
         -------
         double
             The loss evaluated at `p` and `y`.
         """
-        return self.loss(p, y)
+        return self.loss(y, p)
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         """Evaluate the derivative of the loss function with respect to
         the prediction `p`.
 
         Parameters
         ----------
-        p : double
-            The prediction, `p = w^T x`.
         y : double
             The true value (aka target).
+        p : double
+            The prediction, `p = w^T x`.
 
         Returns
         -------
         double
             The derivative of the loss function with regards to `p`.
         """
         return 0.
 
 
 cdef class Regression(LossFunction):
     """Base class for loss functions for regression"""
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         return 0.
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         return 0.
 
 
 cdef class Classification(LossFunction):
     """Base class for loss functions for classification"""
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         return 0.
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         return 0.
 
 
 cdef class ModifiedHuber(Classification):
     """Modified Huber loss for binary classification with y in {-1, 1}
 
     This is equivalent to quadratically smoothed SVM with gamma = 2.
 
     See T. Zhang 'Solving Large Scale Linear Prediction Problems Using
     Stochastic Gradient Descent', ICML'04.
     """
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double z = p * y
         if z >= 1.0:
             return 0.0
         elif z >= -1.0:
             return (1.0 - z) * (1.0 - z)
         else:
             return -4.0 * z
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         cdef double z = p * y
         if z >= 1.0:
             return 0.0
         elif z >= -1.0:
             return 2.0 * (1.0 - z) * -y
         else:
             return -4.0 * y
@@ -213,21 +209,21 @@
     """
 
     cdef double threshold
 
     def __init__(self, double threshold=1.0):
         self.threshold = threshold
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double z = p * y
         if z <= self.threshold:
             return self.threshold - z
         return 0.0
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         cdef double z = p * y
         if z <= self.threshold:
             return -y
         return 0.0
 
     def __reduce__(self):
         return Hinge, (self.threshold,)
@@ -245,61 +241,61 @@
     """
 
     cdef double threshold
 
     def __init__(self, double threshold=1.0):
         self.threshold = threshold
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double z = self.threshold - p * y
         if z > 0:
             return z * z
         return 0.0
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         cdef double z = self.threshold - p * y
         if z > 0:
             return -2 * y * z
         return 0.0
 
     def __reduce__(self):
         return SquaredHinge, (self.threshold,)
 
 
 cdef class Log(Classification):
     """Logistic regression loss for binary classification with y in {-1, 1}"""
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double z = p * y
         # approximately equal and saves the computation of the log
         if z > 18:
             return exp(-z)
         if z < -18:
             return -z
         return log(1.0 + exp(-z))
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         cdef double z = p * y
         # approximately equal and saves the computation of the log
         if z > 18.0:
             return exp(-z) * -y
         if z < -18.0:
             return -y
         return -y / (exp(z) + 1.0)
 
     def __reduce__(self):
         return Log, ()
 
 
 cdef class SquaredLoss(Regression):
     """Squared loss traditional used in linear regression."""
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         return 0.5 * (p - y) * (p - y)
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         return p - y
 
     def __reduce__(self):
         return SquaredLoss, ()
 
 
 cdef class Huber(Regression):
@@ -312,23 +308,23 @@
     """
 
     cdef double c
 
     def __init__(self, double c):
         self.c = c
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double r = p - y
         cdef double abs_r = fabs(r)
         if abs_r <= self.c:
             return 0.5 * r * r
         else:
             return self.c * abs_r - (0.5 * self.c * self.c)
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         cdef double r = p - y
         cdef double abs_r = fabs(r)
         if abs_r <= self.c:
             return r
         elif r > 0.0:
             return self.c
         else:
@@ -345,19 +341,19 @@
     """
 
     cdef double epsilon
 
     def __init__(self, double epsilon):
         self.epsilon = epsilon
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double ret = fabs(y - p) - self.epsilon
         return ret if ret > 0 else 0
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         if y - p > self.epsilon:
             return -1
         elif p - y > self.epsilon:
             return 1
         else:
             return 0
 
@@ -372,19 +368,19 @@
     """
 
     cdef double epsilon
 
     def __init__(self, double epsilon):
         self.epsilon = epsilon
 
-    cdef double loss(self, double p, double y) noexcept nogil:
+    cdef double loss(self, double y, double p) noexcept nogil:
         cdef double ret = fabs(y - p) - self.epsilon
         return ret * ret if ret > 0 else 0
 
-    cdef double dloss(self, double p, double y) noexcept nogil:
+    cdef double dloss(self, double y, double p) noexcept nogil:
         cdef double z
         z = y - p
         if z > self.epsilon:
             return -2 * (z - self.epsilon)
         elif z < -self.epsilon:
             return 2 * (-z - self.epsilon)
         else:
@@ -411,15 +407,15 @@
     validation_score_cb,
     int n_iter_no_change,
     unsigned int max_iter,
     double tol,
     int fit_intercept,
     int verbose,
     bint shuffle,
-    cnp.uint32_t seed,
+    uint32_t seed,
     double weight_pos,
     double weight_neg,
     int learning_rate,
     double eta0,
     double power_t,
     bint one_class,
     double t=1.0,
@@ -472,15 +468,15 @@
         Print verbose output; 0 for quite.
     shuffle : boolean
         Whether to shuffle the training data before each epoch.
     weight_pos : float
         The weight of the positive class.
     weight_neg : float
         The weight of the negative class.
-    seed : cnp.uint32_t
+    seed : uint32_t
         Seed of the pseudorandom number generator used to shuffle the data.
     learning_rate : int
         The learning rate:
         (1) constant, eta = eta0
         (2) optimal, eta = 1.0/(alpha * t).
         (3) inverse scaling, eta = eta0 / pow(t, power_t)
         (4) adaptive decrease
@@ -565,15 +561,15 @@
         l1_ratio = 1.0
 
     eta = eta0
 
     if learning_rate == OPTIMAL:
         typw = np.sqrt(1.0 / np.sqrt(alpha))
         # computing eta0, the initial learning rate
-        initial_eta0 = typw / max(1.0, loss.dloss(-typw, 1.0))
+        initial_eta0 = typw / max(1.0, loss.dloss(1.0, -typw))
         # initialize t such that eta at first sample equals eta0
         optimal_init = 1.0 / (initial_eta0 * alpha)
 
     t_start = time()
     with nogil:
         for epoch in range(max_iter):
             sumloss = 0
@@ -594,31 +590,31 @@
                 p = w.dot(x_data_ptr, x_ind_ptr, xnnz) + intercept
                 if learning_rate == OPTIMAL:
                     eta = 1.0 / (alpha * (optimal_init + t - 1))
                 elif learning_rate == INVSCALING:
                     eta = eta0 / pow(t, power_t)
 
                 if verbose or not early_stopping:
-                    sumloss += loss.loss(p, y)
+                    sumloss += loss.loss(y, p)
 
                 if y > 0.0:
                     class_weight = weight_pos
                 else:
                     class_weight = weight_neg
 
                 if learning_rate == PA1:
                     update = sqnorm(x_data_ptr, x_ind_ptr, xnnz)
                     if update == 0:
                         continue
-                    update = min(C, loss.loss(p, y) / update)
+                    update = min(C, loss.loss(y, p) / update)
                 elif learning_rate == PA2:
                     update = sqnorm(x_data_ptr, x_ind_ptr, xnnz)
-                    update = loss.loss(p, y) / (update + 0.5 / C)
+                    update = loss.loss(y, p) / (update + 0.5 / C)
                 else:
-                    dloss = loss.dloss(p, y)
+                    dloss = loss.dloss(y, p)
                     # clip dloss with large values to avoid numerical
                     # instabilities
                     if dloss < -MAX_DLOSS:
                         dloss = -MAX_DLOSS
                     elif dloss > MAX_DLOSS:
                         dloss = MAX_DLOSS
                     update = -eta * dloss
@@ -671,16 +667,15 @@
                           "Avg. loss: %f"
                           % (w.norm(), np.nonzero(weights)[0].shape[0],
                              intercept, count, sumloss / train_count))
                     print("Total training time: %.2f seconds."
                           % (time() - t_start))
 
             # floating-point under-/overflow check.
-            if (not skl_isfinite(intercept)
-                or any_nonfinite(&weights[0], n_features)):
+            if (not isfinite(intercept) or any_nonfinite(weights)):
                 infinity = True
                 break
 
             # evaluate the score on the validation set
             if early_stopping:
                 with gil:
                     score = validation_score_cb(weights.base, intercept)
@@ -725,24 +720,17 @@
         average_intercept,
         epoch + 1
     )
 
 {{endfor}}
 
 
-cdef inline bint skl_isfinite(floating w) noexcept nogil:
-    if floating is float:
-        return skl_isfinite32(w)
-    else:
-        return skl_isfinite64(w)
-
-
-cdef inline bint any_nonfinite(const floating *w, int n) noexcept nogil:
-    for i in range(n):
-        if not skl_isfinite(w[i]):
+cdef inline bint any_nonfinite(const floating[::1] w) noexcept nogil:
+    for i in range(w.shape[0]):
+        if not isfinite(w[i]):
             return True
     return 0
 
 
 cdef inline double sqnorm(
     floating * x_data_ptr,
     int * x_ind_ptr,
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_stochastic_gradient.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_stochastic_gradient.py`

 * *Files 2% similar despite different names*

```diff
@@ -86,15 +86,15 @@
         "fit_intercept": ["boolean"],
         "max_iter": [Interval(Integral, 1, None, closed="left")],
         "tol": [Interval(Real, 0, None, closed="left"), None],
         "shuffle": ["boolean"],
         "verbose": ["verbose"],
         "random_state": ["random_state"],
         "warm_start": ["boolean"],
-        "average": [Interval(Integral, 0, None, closed="left"), bool, np.bool_],
+        "average": [Interval(Integral, 0, None, closed="left"), "boolean"],
     }
 
     def __init__(
         self,
         loss,
         *,
         penalty="l2",
@@ -599,14 +599,25 @@
             accept_sparse="csr",
             dtype=[np.float64, np.float32],
             order="C",
             accept_large_sparse=False,
             reset=first_call,
         )
 
+        if first_call:
+            # TODO(1.7) remove 0 from average parameter constraint
+            if not isinstance(self.average, (bool, np.bool_)) and self.average == 0:
+                warnings.warn(
+                    (
+                        "Passing average=0 to disable averaging is deprecated and will"
+                        " be removed in 1.7. Please use average=False instead."
+                    ),
+                    FutureWarning,
+                )
+
         n_samples, n_features = X.shape
 
         _check_partial_fit_first_call(self, classes)
 
         n_classes = self.classes_.shape[0]
 
         # Allocate datastructures from input arguments
@@ -674,14 +685,24 @@
         intercept_init=None,
         sample_weight=None,
     ):
         if hasattr(self, "classes_"):
             # delete the attribute otherwise _partial_fit thinks it's not the first call
             delattr(self, "classes_")
 
+        # TODO(1.7) remove 0 from average parameter constraint
+        if not isinstance(self.average, (bool, np.bool_)) and self.average == 0:
+            warnings.warn(
+                (
+                    "Passing average=0 to disable averaging is deprecated and will be "
+                    "removed in 1.7. Please use average=False instead."
+                ),
+                FutureWarning,
+            )
+
         # labels can be encoded as float, int, or string literals
         # np.unique sorts in asc order; largest class id is positive class
         y = self._validate_data(y=y)
         classes = np.unique(y)
 
         if self.warm_start and hasattr(self, "coef_"):
             if coef_init is None:
@@ -1333,16 +1354,15 @@
 
             return prob
 
         else:
             raise NotImplementedError(
                 "predict_(log_)proba only supported when"
                 " loss='log_loss' or loss='modified_huber' "
-                "(%r given)"
-                % self.loss
+                "(%r given)" % self.loss
             )
 
     @available_if(_check_proba)
     def predict_log_proba(self, X):
         """Log of probability estimates.
 
         This method is only available for log loss and modified Huber loss.
@@ -1461,14 +1481,25 @@
             order="C",
             dtype=[np.float64, np.float32],
             accept_large_sparse=False,
             reset=first_call,
         )
         y = y.astype(X.dtype, copy=False)
 
+        if first_call:
+            # TODO(1.7) remove 0 from average parameter constraint
+            if not isinstance(self.average, (bool, np.bool_)) and self.average == 0:
+                warnings.warn(
+                    (
+                        "Passing average=0 to disable averaging is deprecated and will"
+                        " be removed in 1.7. Please use average=False instead."
+                    ),
+                    FutureWarning,
+                )
+
         n_samples, n_features = X.shape
 
         sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
 
         # Allocate datastructures from input arguments
         if first_call:
             self._allocate_parameter_mem(
@@ -1538,14 +1569,24 @@
         C,
         loss,
         learning_rate,
         coef_init=None,
         intercept_init=None,
         sample_weight=None,
     ):
+        # TODO(1.7) remove 0 from average parameter constraint
+        if not isinstance(self.average, (bool, np.bool_)) and self.average == 0:
+            warnings.warn(
+                (
+                    "Passing average=0 to disable averaging is deprecated and will be "
+                    "removed in 1.7. Please use average=False instead."
+                ),
+                FutureWarning,
+            )
+
         if self.warm_start and getattr(self, "coef_", None) is not None:
             if coef_init is None:
                 coef_init = self.coef_
             if intercept_init is None:
                 intercept_init = self.intercept_
         else:
             self.coef_ = None
@@ -2354,14 +2395,25 @@
             accept_sparse="csr",
             dtype=[np.float64, np.float32],
             order="C",
             accept_large_sparse=False,
             reset=first_call,
         )
 
+        if first_call:
+            # TODO(1.7) remove 0 from average parameter constraint
+            if not isinstance(self.average, (bool, np.bool_)) and self.average == 0:
+                warnings.warn(
+                    (
+                        "Passing average=0 to disable averaging is deprecated and will"
+                        " be removed in 1.7. Please use average=False instead."
+                    ),
+                    FutureWarning,
+                )
+
         n_features = X.shape[1]
 
         # Allocate datastructures from input arguments
         sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
 
         # We use intercept = 1 - offset where intercept is the intercept of
         # the SGD implementation and offset is the offset of the One-Class SVM
@@ -2444,14 +2496,24 @@
         C,
         loss,
         learning_rate,
         coef_init=None,
         offset_init=None,
         sample_weight=None,
     ):
+        # TODO(1.7) remove 0 from average parameter constraint
+        if not isinstance(self.average, (bool, np.bool_)) and self.average == 0:
+            warnings.warn(
+                (
+                    "Passing average=0 to disable averaging is deprecated and will be "
+                    "removed in 1.7. Please use average=False instead."
+                ),
+                FutureWarning,
+            )
+
         if self.warm_start and hasattr(self, "coef_"):
             if coef_init is None:
                 coef_init = self.coef_
             if offset_init is None:
                 offset_init = self.offset_
         else:
             self.coef_ = None
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/_theil_sen.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/_theil_sen.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/meson.build` & `scikit_learn-1.5.0rc1/sklearn/linear_model/meson.build`

 * *Files 15% similar despite different names*

```diff
@@ -2,16 +2,15 @@
 linear_model_cython_tree = [
   fs.copyfile('__init__.py'),
   fs.copyfile('_sgd_fast.pxd'),
 ]
 
 py.extension_module(
   '_cd_fast',
-  '_cd_fast.pyx',
-  dependencies: [np_dep],
+  ['_cd_fast.pyx', utils_cython_tree],
   cython_args: cython_args,
   subdir: 'sklearn/linear_model',
   install: true
 )
 
 name_list = ['_sgd_fast', '_sag_fast']
 
@@ -20,14 +19,13 @@
     name + '_pyx',
     output: name + '.pyx',
     input: name + '.pyx.tp',
     command: [py, tempita, '@INPUT@', '-o', '@OUTDIR@']
   )
   py.extension_module(
     name,
-    [pyx, linear_model_cython_tree],
-    dependencies: [np_dep],
+    [pyx, linear_model_cython_tree, utils_cython_tree],
     cython_args: cython_args,
     subdir: 'sklearn/linear_model',
     install: true
 )
 endforeach
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_base.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_bayes.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_bayes.py`

 * *Files 12% similar despite different names*

```diff
@@ -293,37 +293,7 @@
 def test_dtype_correctness(Estimator):
     X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])
     y = np.array([1, 2, 3, 2, 0, 4, 5]).T
     model = Estimator()
     coef_32 = model.fit(X.astype(np.float32), y).coef_
     coef_64 = model.fit(X.astype(np.float64), y).coef_
     np.testing.assert_allclose(coef_32, coef_64, rtol=1e-4)
-
-
-# TODO(1.5) remove
-@pytest.mark.parametrize("Estimator", [BayesianRidge, ARDRegression])
-def test_bayesian_ridge_ard_n_iter_deprecated(Estimator):
-    """Check the deprecation warning of `n_iter`."""
-    depr_msg = (
-        "'n_iter' was renamed to 'max_iter' in version 1.3 and will be removed in 1.5"
-    )
-    X, y = diabetes.data, diabetes.target
-    model = Estimator(n_iter=5)
-
-    with pytest.warns(FutureWarning, match=depr_msg):
-        model.fit(X, y)
-
-
-# TODO(1.5) remove
-@pytest.mark.parametrize("Estimator", [BayesianRidge, ARDRegression])
-def test_bayesian_ridge_ard_max_iter_and_n_iter_both_set(Estimator):
-    """Check that a ValueError is raised when both `max_iter` and `n_iter` are set."""
-    err_msg = (
-        "Both `n_iter` and `max_iter` attributes were set. Attribute"
-        " `n_iter` was deprecated in version 1.3 and will be removed in"
-        " 1.5. To avoid this error, only set the `max_iter` attribute."
-    )
-    X, y = diabetes.data, diabetes.target
-    model = Estimator(n_iter=5, max_iter=5)
-
-    with pytest.raises(ValueError, match=err_msg):
-        model.fit(X, y)
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_common.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_coordinate_descent.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_coordinate_descent.py`

 * *Files 0% similar despite different names*

```diff
@@ -1599,15 +1599,15 @@
     class:`MultiTaskLassoCV` if `sample_weight` is passed and the
     CV splitter does not support `sample_weight` an error is raised.
     On the other hand if the splitter does support `sample_weight`
     while `sample_weight` is passed there is no error and process
     completes smoothly as before.
     """
 
-    class CVSplitter(BaseCrossValidator, GroupsConsumerMixin):
+    class CVSplitter(GroupsConsumerMixin, BaseCrossValidator):
         def get_n_splits(self, X=None, y=None, groups=None, metadata=None):
             pass  # pragma: nocover
 
     class CVSplitterSampleWeight(CVSplitter):
         def split(self, X, y=None, groups=None, sample_weight=None):
             split_index = len(X) // 2
             train_indices = list(range(0, split_index))
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_huber.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_huber.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_least_angle.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_least_angle.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_linear_loss.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_linear_loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """
 Tests for LinearModelLoss
 
 Note that correctness of losses (which compose LinearModelLoss) is already well
 covered in the _loss module.
 """
+
 import numpy as np
 import pytest
 from numpy.testing import assert_allclose
 from scipy import linalg, optimize
 
 from sklearn._loss.loss import (
     HalfBinomialLoss,
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_logistic.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_logistic.py`

 * *Files 6% similar despite different names*

```diff
@@ -31,19 +31,20 @@
 from sklearn.metrics import get_scorer, log_loss
 from sklearn.model_selection import (
     GridSearchCV,
     StratifiedKFold,
     cross_val_score,
     train_test_split,
 )
+from sklearn.multiclass import OneVsRestClassifier
 from sklearn.preprocessing import LabelEncoder, StandardScaler, scale
 from sklearn.svm import l1_min_c
-from sklearn.utils import _IS_32BIT, compute_class_weight, shuffle
+from sklearn.utils import compute_class_weight, shuffle
 from sklearn.utils._testing import ignore_warnings, skip_if_no_parallel
-from sklearn.utils.fixes import COO_CONTAINERS, CSR_CONTAINERS
+from sklearn.utils.fixes import _IS_32BIT, COO_CONTAINERS, CSR_CONTAINERS
 
 pytestmark = pytest.mark.filterwarnings(
     "error::sklearn.exceptions.ConvergenceWarning:sklearn.*"
 )
 # Fixing random_state helps prevent ConvergenceWarnings
 LogisticRegression = partial(LogisticRegressionDefault, random_state=0)
 LogisticRegressionCV = partial(LogisticRegressionCVDefault, random_state=0)
@@ -140,22 +141,22 @@
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_predict_3_classes(csr_container):
     check_predictions(LogisticRegression(C=10), X, Y2)
     check_predictions(LogisticRegression(C=10), csr_container(X), Y2)
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize(
     "clf",
     [
         LogisticRegression(C=len(iris.data), solver="liblinear", multi_class="ovr"),
-        LogisticRegression(C=len(iris.data), solver="lbfgs", multi_class="multinomial"),
-        LogisticRegression(
-            C=len(iris.data), solver="newton-cg", multi_class="multinomial"
-        ),
+        LogisticRegression(C=len(iris.data), solver="lbfgs"),
+        LogisticRegression(C=len(iris.data), solver="newton-cg"),
         LogisticRegression(
             C=len(iris.data), solver="sag", tol=1e-2, multi_class="ovr", random_state=42
         ),
         LogisticRegression(
             C=len(iris.data),
             solver="saga",
             tol=1e-2,
@@ -191,14 +192,16 @@
     probabilities = clf.predict_proba(iris.data)
     assert_allclose(probabilities.sum(axis=1), np.ones(n_samples))
 
     pred = iris.target_names[probabilities.argmax(axis=1)]
     assert np.mean(pred == target) > 0.95
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize("LR", [LogisticRegression, LogisticRegressionCV])
 def test_check_solver_option(LR):
     X, y = iris.data, iris.target
 
     # only 'liblinear' and 'newton-cholesky' solver
     for solver in ["liblinear", "newton-cholesky"]:
         msg = f"Solver {solver} does not support a multinomial backend."
@@ -241,14 +244,16 @@
     # Check that an informative error message is raised when penalty="elasticnet"
     # but l1_ratio is not specified.
     model = LR(penalty="elasticnet", solver="saga")
     with pytest.raises(ValueError, match=r".*l1_ratio.*"):
         model.fit(np.array([[1, 2], [3, 4]]), np.array([0, 1]))
 
 
+# TODO(1.7): remove whole test with deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize("solver", ["lbfgs", "newton-cg", "sag", "saga"])
 def test_multinomial_binary(solver):
     # Test multinomial LR on a binary problem.
     target = (iris.target > 0).astype(np.intp)
     target = np.array(["setosa", "not-setosa"])[target]
 
     clf = LogisticRegression(
@@ -264,14 +269,18 @@
         solver=solver, multi_class="multinomial", random_state=42, fit_intercept=False
     )
     mlr.fit(iris.data, target)
     pred = clf.classes_[np.argmax(clf.predict_log_proba(iris.data), axis=1)]
     assert np.mean(pred == target) > 0.9
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+# Maybe even remove this whole test as correctness of multinomial loss is tested
+# elsewhere.
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 def test_multinomial_binary_probabilities(global_random_seed):
     # Test multinomial LR gives expected probabilities based on the
     # decision function, for a binary problem.
     X, y = make_classification(random_state=global_random_seed)
     clf = LogisticRegression(
         multi_class="multinomial",
         solver="saga",
@@ -369,24 +378,22 @@
             X,
             y,
             Cs=Cs,
             fit_intercept=False,
             tol=1e-5,
             solver=solver,
             max_iter=1000,
-            multi_class="ovr",
             random_state=0,
         )
         for i, C in enumerate(Cs):
             lr = LogisticRegression(
                 C=C,
                 fit_intercept=False,
                 tol=1e-5,
                 solver=solver,
-                multi_class="ovr",
                 random_state=0,
                 max_iter=1000,
             )
             lr.fit(X, y)
             lr_coef = lr.coef_.ravel()
             assert_array_almost_equal(
                 lr_coef, coefs[i], decimal=4, err_msg="with solver = %s" % solver
@@ -399,22 +406,20 @@
             X,
             y,
             Cs=Cs,
             tol=1e-6,
             solver=solver,
             intercept_scaling=10000.0,
             random_state=0,
-            multi_class="ovr",
         )
         lr = LogisticRegression(
             C=Cs[0],
             tol=1e-6,
             intercept_scaling=10000.0,
             random_state=0,
-            multi_class="ovr",
             solver=solver,
         )
         lr.fit(X, y)
         lr_coef = np.concatenate([lr.coef_.ravel(), lr.intercept_])
         assert_array_almost_equal(
             lr_coef, coefs[0], decimal=4, err_msg="with solver = %s" % solver
         )
@@ -446,31 +451,28 @@
     # random_state is relevant for liblinear solver only if dual=True
     X, y = make_classification(n_samples=20, random_state=0)
     lr1 = LogisticRegression(
         random_state=0,
         dual=True,
         tol=1e-3,
         solver="liblinear",
-        multi_class="ovr",
     )
     lr1.fit(X, y)
     lr2 = LogisticRegression(
         random_state=0,
         dual=True,
         tol=1e-3,
         solver="liblinear",
-        multi_class="ovr",
     )
     lr2.fit(X, y)
     lr3 = LogisticRegression(
         random_state=8,
         dual=True,
         tol=1e-3,
         solver="liblinear",
-        multi_class="ovr",
     )
     lr3.fit(X, y)
 
     # same result for same random state
     assert_array_almost_equal(lr1.coef_, lr2.coef_)
     # different results for different random states
     msg = "Arrays are not almost equal to 6 decimals"
@@ -483,20 +485,18 @@
     n_samples, n_features = 50, 5
     rng = np.random.RandomState(0)
     X_ref = rng.randn(n_samples, n_features)
     y = np.sign(X_ref.dot(5 * rng.randn(n_features)))
     X_ref -= X_ref.mean()
     X_ref /= X_ref.std()
     lr_cv = LogisticRegressionCV(
-        Cs=[1.0], fit_intercept=False, solver="liblinear", multi_class="ovr", cv=3
+        Cs=[1.0], fit_intercept=False, solver="liblinear", cv=3
     )
     lr_cv.fit(X_ref, y)
-    lr = LogisticRegression(
-        C=1.0, fit_intercept=False, solver="liblinear", multi_class="ovr"
-    )
+    lr = LogisticRegression(C=1.0, fit_intercept=False, solver="liblinear")
     lr.fit(X_ref, y)
     assert_array_almost_equal(lr.coef_, lr_cv.coef_)
 
     assert_array_equal(lr_cv.coef_.shape, (1, n_features))
     assert_array_equal(lr_cv.classes_, [-1, 1])
     assert len(lr_cv.classes_) == 2
 
@@ -526,15 +526,15 @@
     # test that LogisticRegressionCV uses the right score to compute its
     # cross-validation scores when using a multinomial scoring
     # see https://github.com/scikit-learn/scikit-learn/issues/8720
     X, y = make_classification(
         n_samples=100, random_state=0, n_classes=3, n_informative=6
     )
     train, test = np.arange(80), np.arange(80, 100)
-    lr = LogisticRegression(C=1.0, multi_class="multinomial")
+    lr = LogisticRegression(C=1.0)
     # we use lbfgs to support multinomial
     params = lr.get_params()
     # we store the params to set them further in _log_reg_scoring_path
     for key in ["C", "n_jobs", "warm_start"]:
         del params[key]
     lr.fit(X[train], y[train])
     for averaging in multiclass_agg_list:
@@ -547,15 +547,15 @@
                 test,
                 Cs=[1.0],
                 scoring=scorer,
                 pos_class=None,
                 max_squared_sum=None,
                 sample_weight=None,
                 score_params=None,
-                **params,
+                **(params | {"multi_class": "multinomial"}),
             )[2][0],
             scorer(lr, X[test], y[test]),
         )
 
 
 def test_multinomial_logistic_regression_string_inputs():
     # Test with string labels for LogisticRegression(CV)
@@ -567,18 +567,18 @@
         n_informative=3,
         random_state=0,
     )
     y_str = LabelEncoder().fit(["bar", "baz", "foo"]).inverse_transform(y)
     # For numerical labels, let y values be taken from set (-1, 0, 1)
     y = np.array(y) - 1
     # Test for string labels
-    lr = LogisticRegression(multi_class="multinomial")
-    lr_cv = LogisticRegressionCV(multi_class="multinomial", Cs=3)
-    lr_str = LogisticRegression(multi_class="multinomial")
-    lr_cv_str = LogisticRegressionCV(multi_class="multinomial", Cs=3)
+    lr = LogisticRegression()
+    lr_cv = LogisticRegressionCV(Cs=3)
+    lr_str = LogisticRegression()
+    lr_cv_str = LogisticRegressionCV(Cs=3)
 
     lr.fit(X_ref, y)
     lr_cv.fit(X_ref, y)
     lr_str.fit(X_ref, y_str)
     lr_cv_str.fit(X_ref, y_str)
 
     assert_array_almost_equal(lr.coef_, lr_str.coef_)
@@ -588,17 +588,17 @@
     assert sorted(lr_cv_str.classes_) == ["bar", "baz", "foo"]
 
     # The predictions should be in original labels
     assert sorted(np.unique(lr_str.predict(X_ref))) == ["bar", "baz", "foo"]
     assert sorted(np.unique(lr_cv_str.predict(X_ref))) == ["bar", "baz", "foo"]
 
     # Make sure class weights can be given with string labels
-    lr_cv_str = LogisticRegression(
-        class_weight={"bar": 1, "baz": 2, "foo": 0}, multi_class="multinomial"
-    ).fit(X_ref, y_str)
+    lr_cv_str = LogisticRegression(class_weight={"bar": 1, "baz": 2, "foo": 0}).fit(
+        X_ref, y_str
+    )
     assert sorted(np.unique(lr_cv_str.predict(X_ref))) == ["bar", "baz"]
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_logistic_cv_sparse(csr_container):
     X, y = make_classification(n_samples=50, n_features=5, random_state=0)
     X[X < 1.0] = 0.0
@@ -609,14 +609,17 @@
     clfs = LogisticRegressionCV()
     clfs.fit(csr, y)
     assert_array_almost_equal(clfs.coef_, clf.coef_)
     assert_array_almost_equal(clfs.intercept_, clf.intercept_)
     assert clfs.C_ == clf.C_
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+# Best remove this whole test.
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 def test_ovr_multinomial_iris():
     # Test that OvR and multinomial are correct using the iris dataset.
     train, target = iris.data, iris.target
     n_samples, n_features = train.shape
 
     # The cv indices from stratified kfold (where stratification is done based
     # on the fine-grained iris classes, i.e, before the classes 0 and 1 are
@@ -651,15 +654,14 @@
     assert scores.shape == (3, n_cv, 10)
 
     # Test that for the iris data multinomial gives a better accuracy than OvR
     for solver in ["lbfgs", "newton-cg", "sag", "saga"]:
         max_iter = 500 if solver in ["sag", "saga"] else 30
         clf_multi = LogisticRegressionCV(
             solver=solver,
-            multi_class="multinomial",
             max_iter=max_iter,
             random_state=42,
             tol=1e-3 if solver in ["sag", "saga"] else 1e-2,
             cv=2,
         )
         if solver == "lbfgs":
             # lbfgs requires scaling to avoid convergence warnings
@@ -680,15 +682,15 @@
         assert scores.shape == (3, n_cv, 10)
 
 
 def test_logistic_regression_solvers():
     """Test solvers converge to the same result."""
     X, y = make_classification(n_features=10, n_informative=5, random_state=0)
 
-    params = dict(fit_intercept=False, random_state=42, multi_class="ovr")
+    params = dict(fit_intercept=False, random_state=42)
 
     regressors = {
         solver: LogisticRegression(solver=solver, **params).fit(X, y)
         for solver in SOLVERS
     }
 
     for solver_1, solver_2 in itertools.combinations(regressors, r=2):
@@ -698,26 +700,26 @@
 
 
 def test_logistic_regression_solvers_multiclass():
     """Test solvers converge to the same result for multiclass problems."""
     X, y = make_classification(
         n_samples=20, n_features=20, n_informative=10, n_classes=3, random_state=0
     )
-    tol = 1e-7
-    params = dict(fit_intercept=False, tol=tol, random_state=42, multi_class="ovr")
+    tol = 1e-8
+    params = dict(fit_intercept=False, tol=tol, random_state=42)
 
     # Override max iteration count for specific solvers to allow for
     # proper convergence.
-    solver_max_iter = {"sag": 1000, "saga": 10000}
+    solver_max_iter = {"sag": 10_000, "saga": 10_000}
 
     regressors = {
         solver: LogisticRegression(
             solver=solver, max_iter=solver_max_iter.get(solver, 100), **params
         ).fit(X, y)
-        for solver in SOLVERS
+        for solver in set(SOLVERS) - set(["liblinear", "newton-cholesky"])
     }
 
     for solver_1, solver_2 in itertools.combinations(regressors, r=2):
         assert_allclose(
             regressors[solver_1].coef_,
             regressors[solver_2].coef_,
             rtol=5e-3 if solver_2 == "saga" else 1e-3,
@@ -741,15 +743,14 @@
         n_redundant=0,
         n_classes=n_classes,
         random_state=global_random_seed,
     )
     params = dict(
         Cs=1,
         fit_intercept=False,
-        multi_class="ovr",
         class_weight=class_weight,
         tol=1e-8,
     )
     clf_lbfgs = LogisticRegressionCV(solver="lbfgs", **params)
 
     # XXX: lbfgs' line search can fail and cause a ConvergenceWarning for some
     # 10% of the random seeds, but only on specific platforms (in particular
@@ -757,15 +758,15 @@
     # parameter of the solver does not help. However this lack of proper
     # convergence does not seem to prevent the assertion to pass, so we ignore
     # the warning for now.
     # See: https://github.com/scikit-learn/scikit-learn/pull/27649
     with ignore_warnings(category=ConvergenceWarning):
         clf_lbfgs.fit(X, y)
 
-    for solver in set(SOLVERS) - set(["lbfgs"]):
+    for solver in set(SOLVERS) - set(["lbfgs", "liblinear", "newton-cholesky"]):
         clf = LogisticRegressionCV(solver=solver, **params)
         if solver in ("sag", "saga"):
             clf.set_params(
                 tol=1e-18, max_iter=10000, random_state=global_random_seed + 1
             )
         clf.fit(X, y)
 
@@ -777,15 +778,15 @@
 def test_logistic_regression_sample_weights():
     X, y = make_classification(
         n_samples=20, n_features=5, n_informative=3, n_classes=2, random_state=0
     )
     sample_weight = y + 1
 
     for LR in [LogisticRegression, LogisticRegressionCV]:
-        kw = {"random_state": 42, "fit_intercept": False, "multi_class": "ovr"}
+        kw = {"random_state": 42, "fit_intercept": False}
         if LR is LogisticRegressionCV:
             kw.update({"Cs": 3, "cv": 3})
 
         # Test that passing sample_weight as ones is the same as
         # not passing them at all (default None)
         for solver in ["lbfgs", "liblinear"]:
             clf_sw_none = LR(solver=solver, **kw)
@@ -794,15 +795,15 @@
             clf_sw_ones.fit(X, y, sample_weight=np.ones(y.shape[0]))
             assert_allclose(clf_sw_none.coef_, clf_sw_ones.coef_, rtol=1e-4)
 
         # Test that sample weights work the same with the lbfgs,
         # newton-cg, newton-cholesky and 'sag' solvers
         clf_sw_lbfgs = LR(**kw, tol=1e-5)
         clf_sw_lbfgs.fit(X, y, sample_weight=sample_weight)
-        for solver in set(SOLVERS) - set(("lbfgs", "saga")):
+        for solver in set(SOLVERS) - set(["lbfgs"]):
             clf_sw = LR(solver=solver, tol=1e-10 if solver == "sag" else 1e-5, **kw)
             # ignore convergence warning due to small dataset with sag
             with ignore_warnings():
                 clf_sw.fit(X, y, sample_weight=sample_weight)
             assert_allclose(clf_sw_lbfgs.coef_, clf_sw.coef_, rtol=1e-4)
 
         # Test that passing class_weight as [1,2] is the same as
@@ -820,90 +821,88 @@
     clf_cw = LogisticRegression(
         solver="liblinear",
         fit_intercept=False,
         class_weight={0: 1, 1: 2},
         penalty="l1",
         tol=1e-5,
         random_state=42,
-        multi_class="ovr",
     )
     clf_cw.fit(X, y)
     clf_sw = LogisticRegression(
         solver="liblinear",
         fit_intercept=False,
         penalty="l1",
         tol=1e-5,
         random_state=42,
-        multi_class="ovr",
     )
     clf_sw.fit(X, y, sample_weight)
     assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)
 
     clf_cw = LogisticRegression(
         solver="liblinear",
         fit_intercept=False,
         class_weight={0: 1, 1: 2},
         penalty="l2",
         dual=True,
         random_state=42,
-        multi_class="ovr",
     )
     clf_cw.fit(X, y)
     clf_sw = LogisticRegression(
         solver="liblinear",
         fit_intercept=False,
         penalty="l2",
         dual=True,
         random_state=42,
-        multi_class="ovr",
     )
     clf_sw.fit(X, y, sample_weight)
     assert_array_almost_equal(clf_cw.coef_, clf_sw.coef_, decimal=4)
 
 
 def _compute_class_weight_dictionary(y):
     # helper for returning a dictionary instead of an array
     classes = np.unique(y)
     class_weight = compute_class_weight("balanced", classes=classes, y=y)
     class_weight_dict = dict(zip(classes, class_weight))
     return class_weight_dict
 
 
-def test_logistic_regression_class_weights():
+@pytest.mark.parametrize("csr_container", [lambda x: x] + CSR_CONTAINERS)
+def test_logistic_regression_class_weights(csr_container):
     # Scale data to avoid convergence warnings with the lbfgs solver
     X_iris = scale(iris.data)
     # Multinomial case: remove 90% of class 0
     X = X_iris[45:, :]
+    X = csr_container(X)
     y = iris.target[45:]
-    solvers = ("lbfgs", "newton-cg")
     class_weight_dict = _compute_class_weight_dictionary(y)
 
-    for solver in solvers:
-        clf1 = LogisticRegression(
-            solver=solver, multi_class="multinomial", class_weight="balanced"
-        )
-        clf2 = LogisticRegression(
-            solver=solver, multi_class="multinomial", class_weight=class_weight_dict
-        )
+    for solver in set(SOLVERS) - set(["liblinear", "newton-cholesky"]):
+        params = dict(solver=solver, max_iter=1000)
+        clf1 = LogisticRegression(class_weight="balanced", **params)
+        clf2 = LogisticRegression(class_weight=class_weight_dict, **params)
         clf1.fit(X, y)
         clf2.fit(X, y)
-        assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=4)
+        assert len(clf1.classes_) == 3
+        assert_allclose(clf1.coef_, clf2.coef_, rtol=1e-4)
+        # Same as appropriate sample_weight.
+        sw = np.ones(X.shape[0])
+        for c in clf1.classes_:
+            sw[y == c] *= class_weight_dict[c]
+        clf3 = LogisticRegression(**params).fit(X, y, sample_weight=sw)
+        assert_allclose(clf3.coef_, clf2.coef_, rtol=1e-4)
 
     # Binary case: remove 90% of class 0 and 100% of class 2
     X = X_iris[45:100, :]
     y = iris.target[45:100]
     class_weight_dict = _compute_class_weight_dictionary(y)
 
-    for solver in set(SOLVERS) - set(("sag", "saga")):
-        clf1 = LogisticRegression(
-            solver=solver, multi_class="ovr", class_weight="balanced"
-        )
-        clf2 = LogisticRegression(
-            solver=solver, multi_class="ovr", class_weight=class_weight_dict
-        )
+    for solver in SOLVERS:
+        params = dict(solver=solver, max_iter=1000)
+        clf1 = LogisticRegression(class_weight="balanced", **params)
+        clf2 = LogisticRegression(class_weight=class_weight_dict, **params)
         clf1.fit(X, y)
         clf2.fit(X, y)
         assert_array_almost_equal(clf1.coef_, clf2.coef_, decimal=6)
 
 
 def test_logistic_regression_multinomial():
     # Tests for the multinomial option in logistic regression
@@ -918,33 +917,29 @@
         random_state=0,
     )
 
     X = StandardScaler(with_mean=False).fit_transform(X)
 
     # 'lbfgs' is used as a referenced
     solver = "lbfgs"
-    ref_i = LogisticRegression(solver=solver, multi_class="multinomial", tol=1e-6)
-    ref_w = LogisticRegression(
-        solver=solver, multi_class="multinomial", fit_intercept=False, tol=1e-6
-    )
+    ref_i = LogisticRegression(solver=solver, tol=1e-6)
+    ref_w = LogisticRegression(solver=solver, fit_intercept=False, tol=1e-6)
     ref_i.fit(X, y)
     ref_w.fit(X, y)
     assert ref_i.coef_.shape == (n_classes, n_features)
     assert ref_w.coef_.shape == (n_classes, n_features)
     for solver in ["sag", "saga", "newton-cg"]:
         clf_i = LogisticRegression(
             solver=solver,
-            multi_class="multinomial",
             random_state=42,
             max_iter=2000,
             tol=1e-7,
         )
         clf_w = LogisticRegression(
             solver=solver,
-            multi_class="multinomial",
             random_state=42,
             max_iter=2000,
             tol=1e-7,
             fit_intercept=False,
         )
         clf_i.fit(X, y)
         clf_w.fit(X, y)
@@ -957,42 +952,42 @@
         assert_allclose(ref_i.intercept_, clf_i.intercept_, rtol=1e-3)
 
     # Test that the path give almost the same results. However since in this
     # case we take the average of the coefs after fitting across all the
     # folds, it need not be exactly the same.
     for solver in ["lbfgs", "newton-cg", "sag", "saga"]:
         clf_path = LogisticRegressionCV(
-            solver=solver, max_iter=2000, tol=1e-6, multi_class="multinomial", Cs=[1.0]
+            solver=solver, max_iter=2000, tol=1e-6, Cs=[1.0]
         )
         clf_path.fit(X, y)
         assert_allclose(clf_path.coef_, ref_i.coef_, rtol=1e-2)
         assert_allclose(clf_path.intercept_, ref_i.intercept_, rtol=1e-2)
 
 
 def test_liblinear_decision_function_zero():
     # Test negative prediction when decision_function values are zero.
     # Liblinear predicts the positive class when decision_function values
     # are zero. This is a test to verify that we do not do the same.
     # See Issue: https://github.com/scikit-learn/scikit-learn/issues/3600
     # and the PR https://github.com/scikit-learn/scikit-learn/pull/3623
     X, y = make_classification(n_samples=5, n_features=5, random_state=0)
-    clf = LogisticRegression(fit_intercept=False, solver="liblinear", multi_class="ovr")
+    clf = LogisticRegression(fit_intercept=False, solver="liblinear")
     clf.fit(X, y)
 
     # Dummy data such that the decision function becomes zero.
     X = np.zeros((5, 5))
     assert_array_equal(clf.predict(X), np.zeros(5))
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_liblinear_logregcv_sparse(csr_container):
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
     X, y = make_classification(n_samples=10, n_features=5, random_state=0)
-    clf = LogisticRegressionCV(solver="liblinear", multi_class="ovr")
+    clf = LogisticRegressionCV(solver="liblinear")
     clf.fit(csr_container(X), y)
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_saga_sparse(csr_container):
     # Test LogRegCV with solver='liblinear' works for sparse matrices
 
@@ -1020,25 +1015,23 @@
     X_constant = np.ones(shape=(n_samples, 2))
     X = np.concatenate((X, X_noise, X_constant), axis=1)
     lr_liblinear = LogisticRegression(
         penalty="l1",
         C=1.0,
         solver="liblinear",
         fit_intercept=False,
-        multi_class="ovr",
         tol=1e-10,
     )
     lr_liblinear.fit(X, y)
 
     lr_saga = LogisticRegression(
         penalty="l1",
         C=1.0,
         solver="saga",
         fit_intercept=False,
-        multi_class="ovr",
         max_iter=1000,
         tol=1e-10,
     )
     lr_saga.fit(X, y)
     assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
 
     # Noise and constant features should be regularized to zero by the l1
@@ -1062,25 +1055,23 @@
     X = csr_container(X)
 
     lr_liblinear = LogisticRegression(
         penalty="l1",
         C=1.0,
         solver="liblinear",
         fit_intercept=False,
-        multi_class="ovr",
         tol=1e-10,
     )
     lr_liblinear.fit(X, y)
 
     lr_saga = LogisticRegression(
         penalty="l1",
         C=1.0,
         solver="saga",
         fit_intercept=False,
-        multi_class="ovr",
         max_iter=1000,
         tol=1e-10,
     )
     lr_saga.fit(X, y)
     assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)
     # Noise and constant features should be regularized to zero by the l1
     # penalty
@@ -1089,15 +1080,14 @@
 
     # Check that solving on the sparse and dense data yield the same results
     lr_saga_dense = LogisticRegression(
         penalty="l1",
         C=1.0,
         solver="saga",
         fit_intercept=False,
-        multi_class="ovr",
         max_iter=1000,
         tol=1e-10,
     )
     lr_saga_dense.fit(X.toarray(), y)
     assert_array_almost_equal(lr_saga.coef_, lr_saga_dense.coef_)
 
 
@@ -1130,18 +1120,18 @@
 def test_logreg_predict_proba_multinomial():
     X, y = make_classification(
         n_samples=10, n_features=20, random_state=0, n_classes=3, n_informative=10
     )
 
     # Predicted probabilities using the true-entropy loss should give a
     # smaller loss than those using the ovr method.
-    clf_multi = LogisticRegression(multi_class="multinomial", solver="lbfgs")
+    clf_multi = LogisticRegression(solver="lbfgs")
     clf_multi.fit(X, y)
     clf_multi_loss = log_loss(y, clf_multi.predict_proba(X))
-    clf_ovr = LogisticRegression(multi_class="ovr", solver="lbfgs")
+    clf_ovr = OneVsRestClassifier(LogisticRegression(solver="lbfgs"))
     clf_ovr.fit(X, y)
     clf_ovr_loss = log_loss(y, clf_ovr.predict_proba(X))
     assert clf_ovr_loss > clf_multi_loss
 
     # Predicted probabilities using the soft-max function should give a
     # smaller loss than those using the logistic function.
     clf_multi_loss = log_loss(y, clf_multi.predict_proba(X))
@@ -1152,15 +1142,15 @@
 @pytest.mark.parametrize("max_iter", np.arange(1, 5))
 @pytest.mark.parametrize("multi_class", ["ovr", "multinomial"])
 @pytest.mark.parametrize(
     "solver, message",
     [
         (
             "newton-cg",
-            "newton-cg failed to converge. Increase the number of iterations.",
+            "newton-cg failed to converge.* Increase the number of iterations.",
         ),
         (
             "liblinear",
             "Liblinear failed to converge, increase the number of iterations.",
         ),
         ("sag", "The max_iter was reached which means the coef_ did not converge"),
         ("saga", "The max_iter was reached which means the coef_ did not converge"),
@@ -1187,14 +1177,16 @@
     )
     with pytest.warns(ConvergenceWarning, match=message):
         lr.fit(X, y_bin)
 
     assert lr.n_iter_[0] == max_iter
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize("solver", SOLVERS)
 def test_n_iter(solver):
     # Test that self.n_iter_ has the correct format.
     X, y = iris.data, iris.target
     if solver == "lbfgs":
         # lbfgs requires scaling to avoid convergence warnings
         X = scale(X)
@@ -1237,47 +1229,42 @@
     clf.set_params(multi_class="multinomial").fit(X, y)
     assert clf.n_iter_.shape == (1,)
 
     clf_cv.set_params(multi_class="multinomial").fit(X, y)
     assert clf_cv.n_iter_.shape == (1, n_cv_fold, n_Cs)
 
 
-@pytest.mark.parametrize("solver", sorted(set(SOLVERS) - set(["liblinear"])))
+@pytest.mark.parametrize(
+    "solver", sorted(set(SOLVERS) - set(["liblinear", "newton-cholesky"]))
+)
 @pytest.mark.parametrize("warm_start", (True, False))
 @pytest.mark.parametrize("fit_intercept", (True, False))
-@pytest.mark.parametrize("multi_class", ["ovr", "multinomial"])
-def test_warm_start(solver, warm_start, fit_intercept, multi_class):
+def test_warm_start(solver, warm_start, fit_intercept):
     # A 1-iteration second fit on same data should give almost same result
     # with warm starting, and quite different result without warm starting.
     # Warm starting does not work with liblinear solver.
     X, y = iris.data, iris.target
 
-    if solver == "newton-cholesky" and multi_class == "multinomial":
-        # solver does only support OvR
-        return
-
     clf = LogisticRegression(
         tol=1e-4,
-        multi_class=multi_class,
         warm_start=warm_start,
         solver=solver,
         random_state=42,
         fit_intercept=fit_intercept,
     )
     with ignore_warnings(category=ConvergenceWarning):
         clf.fit(X, y)
         coef_1 = clf.coef_
 
         clf.max_iter = 1
         clf.fit(X, y)
     cum_diff = np.sum(np.abs(coef_1 - clf.coef_))
     msg = (
-        "Warm starting issue with %s solver in %s mode "
-        "with fit_intercept=%s and warm_start=%s"
-        % (solver, multi_class, str(fit_intercept), str(warm_start))
+        f"Warm starting issue with solver {solver}"
+        f"with {fit_intercept=} and {warm_start=}"
     )
     if warm_start:
         assert 2.0 > cum_diff, msg
     else:
         assert cum_diff > 2.0, msg
 
 
@@ -1300,39 +1287,39 @@
         for penalty in ["l1", "l2"]:
             n_samples = X.shape[0]
             # alpha=1e-3 is time consuming
             for alpha in np.logspace(-1, 1, 3):
                 saga = LogisticRegression(
                     C=1.0 / (n_samples * alpha),
                     solver="saga",
-                    multi_class="ovr",
                     max_iter=200,
                     fit_intercept=False,
                     penalty=penalty,
                     random_state=0,
                     tol=1e-6,
                 )
 
                 liblinear = LogisticRegression(
                     C=1.0 / (n_samples * alpha),
                     solver="liblinear",
-                    multi_class="ovr",
                     max_iter=200,
                     fit_intercept=False,
                     penalty=penalty,
                     random_state=0,
                     tol=1e-6,
                 )
 
                 saga.fit(X, y)
                 liblinear.fit(X, y)
                 # Convergence for alpha=1e-3 is very slow
                 assert_array_almost_equal(saga.coef_, liblinear.coef_, 3)
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize("multi_class", ["ovr", "multinomial"])
 @pytest.mark.parametrize(
     "solver", ["liblinear", "newton-cg", "newton-cholesky", "saga"]
 )
 @pytest.mark.parametrize("fit_intercept", [False, True])
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_dtype_match(solver, multi_class, fit_intercept, csr_container):
@@ -1410,20 +1397,16 @@
 def test_warm_start_converge_LR():
     # Test to see that the logistic regression converges on warm start,
     # with multi_class='multinomial'. Non-regressive test for #10836
 
     rng = np.random.RandomState(0)
     X = np.concatenate((rng.randn(100, 2) + [1, 1], rng.randn(100, 2)))
     y = np.array([1] * 100 + [-1] * 100)
-    lr_no_ws = LogisticRegression(
-        multi_class="multinomial", solver="sag", warm_start=False, random_state=0
-    )
-    lr_ws = LogisticRegression(
-        multi_class="multinomial", solver="sag", warm_start=True, random_state=0
-    )
+    lr_no_ws = LogisticRegression(solver="sag", warm_start=False, random_state=0)
+    lr_ws = LogisticRegression(solver="sag", warm_start=True, random_state=0)
 
     lr_no_ws_loss = log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))
     for i in range(5):
         lr_ws.fit(X, y)
     lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))
     assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)
 
@@ -1548,60 +1531,56 @@
         obj += l1_ratio * np.sum(np.abs(coef))
         obj += (1.0 - l1_ratio) * 0.5 * np.dot(coef, coef)
         return obj
 
     assert enet_objective(lr_enet) < enet_objective(lr_l2)
 
 
-@pytest.mark.parametrize("multi_class", ("ovr", "multinomial"))
-def test_LogisticRegressionCV_GridSearchCV_elastic_net(multi_class):
+@pytest.mark.parametrize("n_classes", (2, 3))
+def test_LogisticRegressionCV_GridSearchCV_elastic_net(n_classes):
     # make sure LogisticRegressionCV gives same best params (l1 and C) as
     # GridSearchCV when penalty is elasticnet
 
-    if multi_class == "ovr":
-        # This is actually binary classification, ovr multiclass is treated in
-        # test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr
-        X, y = make_classification(random_state=0)
-    else:
-        X, y = make_classification(
-            n_samples=100, n_classes=3, n_informative=3, random_state=0
-        )
+    X, y = make_classification(
+        n_samples=100, n_classes=n_classes, n_informative=3, random_state=0
+    )
 
     cv = StratifiedKFold(5)
 
     l1_ratios = np.linspace(0, 1, 3)
     Cs = np.logspace(-4, 4, 3)
 
     lrcv = LogisticRegressionCV(
         penalty="elasticnet",
         Cs=Cs,
         solver="saga",
         cv=cv,
         l1_ratios=l1_ratios,
         random_state=0,
-        multi_class=multi_class,
         tol=1e-2,
     )
     lrcv.fit(X, y)
 
     param_grid = {"C": Cs, "l1_ratio": l1_ratios}
     lr = LogisticRegression(
         penalty="elasticnet",
         solver="saga",
         random_state=0,
-        multi_class=multi_class,
         tol=1e-2,
     )
     gs = GridSearchCV(lr, param_grid, cv=cv)
     gs.fit(X, y)
 
     assert gs.best_params_["l1_ratio"] == lrcv.l1_ratio_[0]
     assert gs.best_params_["C"] == lrcv.C_[0]
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+# Maybe remove whole test after removal of the deprecated multi_class.
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 def test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr():
     # make sure LogisticRegressionCV gives same best params (l1 and C) as
     # GridSearchCV when penalty is elasticnet and multiclass is ovr. We can't
     # compare best_params like in the previous test because
     # LogisticRegressionCV with multi_class='ovr' will have one C and one
     # l1_param for each class, while LogisticRegression will share the
     # parameters over the *n_classes* classifiers.
@@ -1639,14 +1618,16 @@
     gs.fit(X_train, y_train)
 
     # Check that predictions are 80% the same
     assert (lrcv.predict(X_train) == gs.predict(X_train)).mean() >= 0.8
     assert (lrcv.predict(X_test) == gs.predict(X_test)).mean() >= 0.8
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize("penalty", ("l2", "elasticnet"))
 @pytest.mark.parametrize("multi_class", ("ovr", "multinomial", "auto"))
 def test_LogisticRegressionCV_no_refit(penalty, multi_class):
     # Test LogisticRegressionCV attribute shapes when refit is False
 
     n_classes = 3
     n_features = 20
@@ -1676,14 +1657,18 @@
     )
     lrcv.fit(X, y)
     assert lrcv.C_.shape == (n_classes,)
     assert lrcv.l1_ratio_.shape == (n_classes,)
     assert lrcv.coef_.shape == (n_classes, n_features)
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+# Remove multi_class an change first element of the expected n_iter_.shape from
+# n_classes to 1 (according to the docstring).
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 def test_LogisticRegressionCV_elasticnet_attribute_shapes():
     # Make sure the shapes of scores_ and coefs_paths_ attributes are correct
     # when using elasticnet (added one dimension for l1_ratios)
 
     n_classes = 3
     n_features = 20
     X, y = make_classification(
@@ -1802,14 +1787,16 @@
         assert_array_almost_equal(coefs[0], coefs[1], decimal=1)
     with pytest.raises(AssertionError):
         assert_array_almost_equal(coefs[0], coefs[2], decimal=1)
     with pytest.raises(AssertionError):
         assert_array_almost_equal(coefs[1], coefs[2], decimal=1)
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize(
     "est",
     [
         LogisticRegression(random_state=0, max_iter=500),
         LogisticRegressionCV(random_state=0, cv=3, Cs=3, tol=1e-3, max_iter=500),
     ],
     ids=lambda x: x.__class__.__name__,
@@ -2001,27 +1988,28 @@
     # Test logistic regression with the iris dataset
     n_samples, n_features = iris.data.shape
     target = iris.target_names[iris.target]
 
     clf = LogisticRegression(
         C=len(iris.data),
         solver="lbfgs",
-        multi_class="multinomial",
         fit_intercept=fit_intercept,
     )
     # Scaling X to ease convergence.
     X_scaled = scale(iris.data)
     clf.fit(X_scaled, target)
 
     # axis=0 is sum over classes
     assert_allclose(clf.coef_.sum(axis=0), 0, atol=1e-10)
     if fit_intercept:
         clf.intercept_.sum(axis=0) == pytest.approx(0, abs=1e-15)
 
 
+# TODO(1.7): remove filterwarnings after the deprecation of multi_class
+@pytest.mark.filterwarnings("ignore:.*'multi_class' was deprecated.*:FutureWarning")
 @pytest.mark.parametrize("multi_class", ["ovr", "multinomial", "auto"])
 @pytest.mark.parametrize("class_weight", [{0: 1.0, 1: 10.0, 2: 1.0}, "balanced"])
 def test_sample_weight_not_modified(multi_class, class_weight):
     X, y = load_iris(return_X_y=True)
     n_features = len(X)
     W = np.ones(n_features)
     W[: n_features // 2] = 2
@@ -2188,7 +2176,32 @@
         params = {"extra_param": 1.0}
 
         with pytest.raises(ValueError, match=msg):
             lr_cv.fit(X, y, **params)
 
         with pytest.raises(ValueError, match=msg):
             lr_cv.score(X, y, **params)
+
+
+# TODO(1.7): remove
+def test_multi_class_deprecated():
+    """Check `multi_class` parameter deprecated."""
+    X, y = make_classification(n_classes=3, n_samples=50, n_informative=6)
+    lr = LogisticRegression(multi_class="ovr")
+    msg = "'multi_class' was deprecated"
+    with pytest.warns(FutureWarning, match=msg):
+        lr.fit(X, y)
+
+    lrCV = LogisticRegressionCV(multi_class="ovr")
+    with pytest.warns(FutureWarning, match=msg):
+        lrCV.fit(X, y)
+
+    # Special warning for "binary multinomial"
+    X, y = make_classification(n_classes=2, n_samples=50, n_informative=6)
+    lr = LogisticRegression(multi_class="multinomial")
+    msg = "'multi_class' was deprecated.*binary problems"
+    with pytest.warns(FutureWarning, match=msg):
+        lr.fit(X, y)
+
+    lrCV = LogisticRegressionCV(multi_class="multinomial")
+    with pytest.warns(FutureWarning, match=msg):
+        lrCV.fit(X, y)
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_omp.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_omp.py`

 * *Files 3% similar despite different names*

```diff
@@ -153,14 +153,25 @@
 
     omp.fit(X, y)
     assert omp.coef_.shape == (n_targets, n_features)
     assert omp.intercept_ == 0
     assert np.count_nonzero(omp.coef_) <= n_targets * n_nonzero_coefs
 
 
+def test_estimator_n_nonzero_coefs():
+    """Check `n_nonzero_coefs_` correct when `tol` is and isn't set."""
+    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)
+    omp.fit(X, y[:, 0])
+    assert omp.n_nonzero_coefs_ == n_nonzero_coefs
+
+    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, tol=0.5)
+    omp.fit(X, y[:, 0])
+    assert omp.n_nonzero_coefs_ is None
+
+
 def test_identical_regressors():
     newX = X.copy()
     newX[:, 1] = newX[:, 0]
     gamma = np.zeros(n_features)
     gamma[0] = gamma[1] = 1.0
     newy = np.dot(newX, gamma)
     warning_message = (
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_passive_aggressive.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_passive_aggressive.py`

 * *Files 3% similar despite different names*

```diff
@@ -262,7 +262,17 @@
     assert_array_almost_equal(reg1.w, reg2.coef_.ravel(), decimal=2)
 
 
 def test_regressor_undefined_methods():
     reg = PassiveAggressiveRegressor(max_iter=100)
     with pytest.raises(AttributeError):
         reg.transform(X)
+
+
+# TODO(1.7): remove
+@pytest.mark.parametrize(
+    "Estimator", [PassiveAggressiveClassifier, PassiveAggressiveRegressor]
+)
+def test_passive_aggressive_deprecated_average(Estimator):
+    est = Estimator(average=0)
+    with pytest.warns(FutureWarning, match="average=0"):
+        est.fit(X, y)
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_perceptron.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_perceptron.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_quantile.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_quantile.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_ransac.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_ransac.py`

 * *Files 1% similar despite different names*

```diff
@@ -457,15 +457,15 @@
     assert _dynamic_max_trials(1, 100, 10, 1) == float("inf")
 
 
 def test_ransac_fit_sample_weight():
     ransac_estimator = RANSACRegressor(random_state=0)
     n_samples = y.shape[0]
     weights = np.ones(n_samples)
-    ransac_estimator.fit(X, y, weights)
+    ransac_estimator.fit(X, y, sample_weight=weights)
     # sanity check
     assert ransac_estimator.inlier_mask_.shape[0] == n_samples
 
     ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_).astype(np.bool_)
     ref_inlier_mask[outliers] = False
     # check that mask is correct
     assert_array_equal(ransac_estimator.inlier_mask_, ref_inlier_mask)
@@ -494,34 +494,34 @@
     )
     ransac_estimator.fit(X_flat, y_flat)
     ref_coef_ = ransac_estimator.estimator_.coef_
 
     sample_weight = np.append(sample_weight, outlier_weight)
     X_ = np.append(X_, outlier_X, axis=0)
     y_ = np.append(y_, outlier_y)
-    ransac_estimator.fit(X_, y_, sample_weight)
+    ransac_estimator.fit(X_, y_, sample_weight=sample_weight)
 
     assert_allclose(ransac_estimator.estimator_.coef_, ref_coef_)
 
     # check that if estimator.fit doesn't support
     # sample_weight, raises error
     estimator = OrthogonalMatchingPursuit()
     ransac_estimator = RANSACRegressor(estimator, min_samples=10)
 
     err_msg = f"{estimator.__class__.__name__} does not support sample_weight."
     with pytest.raises(ValueError, match=err_msg):
-        ransac_estimator.fit(X, y, weights)
+        ransac_estimator.fit(X, y, sample_weight=weights)
 
 
 def test_ransac_final_model_fit_sample_weight():
     X, y = make_regression(n_samples=1000, random_state=10)
     rng = check_random_state(42)
     sample_weight = rng.randint(1, 4, size=y.shape[0])
     sample_weight = sample_weight / sample_weight.sum()
-    ransac = RANSACRegressor(estimator=LinearRegression(), random_state=0)
+    ransac = RANSACRegressor(random_state=0)
     ransac.fit(X, y, sample_weight=sample_weight)
 
     final_model = LinearRegression()
     mask_samples = ransac.inlier_mask_
     final_model.fit(
         X[mask_samples], y[mask_samples], sample_weight=sample_weight[mask_samples]
     )
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_ridge.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_ridge.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 import warnings
 from itertools import product
 
 import numpy as np
 import pytest
 from scipy import linalg
 
-from sklearn import datasets
+from sklearn import config_context, datasets
+from sklearn.base import clone
 from sklearn.datasets import (
     make_classification,
     make_low_rank_matrix,
     make_multilabel_classification,
     make_regression,
 )
 from sklearn.exceptions import ConvergenceWarning
@@ -35,23 +36,36 @@
     GridSearchCV,
     GroupKFold,
     KFold,
     LeaveOneOut,
     cross_val_predict,
 )
 from sklearn.preprocessing import minmax_scale
-from sklearn.utils import _IS_32BIT, check_random_state
+from sklearn.utils import check_random_state
+from sklearn.utils._array_api import (
+    _NUMPY_NAMESPACE_NAMES,
+    _atol_for_type,
+    _convert_to_numpy,
+    yield_namespace_device_dtype_combinations,
+    yield_namespaces,
+)
 from sklearn.utils._testing import (
     assert_allclose,
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
     ignore_warnings,
 )
+from sklearn.utils.estimator_checks import (
+    _array_api_for_tests,
+    _get_check_estimator_ids,
+    check_array_api_input_and_values,
+)
 from sklearn.utils.fixes import (
+    _IS_32BIT,
     COO_CONTAINERS,
     CSC_CONTAINERS,
     CSR_CONTAINERS,
     DOK_CONTAINERS,
     LIL_CONTAINERS,
 )
 
@@ -67,15 +81,15 @@
 ind = ind[:200]
 X_diabetes, y_diabetes = X_diabetes[ind], y_diabetes[ind]
 
 iris = datasets.load_iris()
 X_iris, y_iris = iris.data, iris.target
 
 
-def _accuracy_callable(y_test, y_pred):
+def _accuracy_callable(y_test, y_pred, **kwargs):
     return np.mean(y_test == y_pred)
 
 
 def _mean_squared_error_callable(y_test, y_pred):
     return ((y_test - y_pred) ** 2).mean()
 
 
@@ -189,14 +203,16 @@
 
     # Same with sample_weight.
     model = Ridge(**params).fit(X, y, sample_weight=np.ones(X.shape[0]))
     assert model.intercept_ == pytest.approx(intercept)
     assert_allclose(model.coef_, coef)
     assert model.score(X, y) == pytest.approx(R2_Ridge)
 
+    assert model.solver_ == solver
+
 
 @pytest.mark.parametrize("solver", SOLVERS)
 @pytest.mark.parametrize("fit_intercept", [True, False])
 def test_ridge_regression_hstacked_X(
     solver, fit_intercept, ols_ridge_dataset, global_random_seed
 ):
     """Test that Ridge converges for all solvers to correct solution on hstacked data.
@@ -900,23 +916,23 @@
         np.sum(kfold_errors[indices == i], axis=0) for i in np.arange(X.shape[0])
     ]
     kfold_errors = np.asarray(kfold_errors)
 
     X_gcv = X_container(X)
     gcv_ridge = RidgeCV(
         alphas=alphas,
-        store_cv_values=True,
+        store_cv_results=True,
         gcv_mode=gcv_mode,
         fit_intercept=fit_intercept,
     )
     gcv_ridge.fit(X_gcv, y, sample_weight=sample_weight)
     if len(y_shape) == 2:
-        gcv_errors = gcv_ridge.cv_values_[:, :, alphas.index(kfold.alpha_)]
+        gcv_errors = gcv_ridge.cv_results_[:, :, alphas.index(kfold.alpha_)]
     else:
-        gcv_errors = gcv_ridge.cv_values_[:, alphas.index(kfold.alpha_)]
+        gcv_errors = gcv_ridge.cv_results_[:, alphas.index(kfold.alpha_)]
 
     assert kfold.alpha_ == pytest.approx(gcv_ridge.alpha_)
     assert_allclose(gcv_errors, kfold_errors, rtol=1e-3)
     assert_allclose(gcv_ridge.coef_, kfold.coef_, rtol=1e-3)
     assert_allclose(gcv_ridge.intercept_, kfold.intercept_, rtol=1e-3)
 
 
@@ -1014,34 +1030,34 @@
     assert len(ridge_cv.coef_.shape) == 1
     assert type(ridge_cv.intercept_) == np.float64
 
 
 @pytest.mark.parametrize(
     "ridge, make_dataset",
     [
-        (RidgeCV(store_cv_values=False), make_regression),
-        (RidgeClassifierCV(store_cv_values=False), make_classification),
+        (RidgeCV(store_cv_results=False), make_regression),
+        (RidgeClassifierCV(store_cv_results=False), make_classification),
     ],
 )
-def test_ridge_gcv_cv_values_not_stored(ridge, make_dataset):
-    # Check that `cv_values_` is not stored when store_cv_values is False
+def test_ridge_gcv_cv_results_not_stored(ridge, make_dataset):
+    # Check that `cv_results_` is not stored when store_cv_results is False
     X, y = make_dataset(n_samples=6, random_state=42)
     ridge.fit(X, y)
-    assert not hasattr(ridge, "cv_values_")
+    assert not hasattr(ridge, "cv_results_")
 
 
 @pytest.mark.parametrize(
     "ridge, make_dataset",
     [(RidgeCV(), make_regression), (RidgeClassifierCV(), make_classification)],
 )
 @pytest.mark.parametrize("cv", [None, 3])
 def test_ridge_best_score(ridge, make_dataset, cv):
     # check that the best_score_ is store
     X, y = make_dataset(n_samples=6, random_state=42)
-    ridge.set_params(store_cv_values=False, cv=cv)
+    ridge.set_params(store_cv_results=False, cv=cv)
     ridge.fit(X, y)
     assert hasattr(ridge, "best_score_")
     assert isinstance(ridge.best_score_, float)
 
 
 def test_ridge_cv_individual_penalties():
     # Tests the ridge_cv object optimizing individual penalties for each target
@@ -1070,35 +1086,35 @@
 
     # The resulting regression weights should incorporate the different
     # alpha values.
     assert_array_almost_equal(
         Ridge(alpha=ridge_cv.alpha_).fit(X, y).coef_, ridge_cv.coef_
     )
 
-    # Test shape of alpha_ and cv_values_
-    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=True, store_cv_values=True).fit(
+    # Test shape of alpha_ and cv_results_
+    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=True, store_cv_results=True).fit(
         X, y
     )
     assert ridge_cv.alpha_.shape == (n_targets,)
     assert ridge_cv.best_score_.shape == (n_targets,)
-    assert ridge_cv.cv_values_.shape == (n_samples, len(alphas), n_targets)
+    assert ridge_cv.cv_results_.shape == (n_samples, len(alphas), n_targets)
 
     # Test edge case of there being only one alpha value
-    ridge_cv = RidgeCV(alphas=1, alpha_per_target=True, store_cv_values=True).fit(X, y)
+    ridge_cv = RidgeCV(alphas=1, alpha_per_target=True, store_cv_results=True).fit(X, y)
     assert ridge_cv.alpha_.shape == (n_targets,)
     assert ridge_cv.best_score_.shape == (n_targets,)
-    assert ridge_cv.cv_values_.shape == (n_samples, n_targets, 1)
+    assert ridge_cv.cv_results_.shape == (n_samples, n_targets, 1)
 
     # Test edge case of there being only one target
-    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=True, store_cv_values=True).fit(
+    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=True, store_cv_results=True).fit(
         X, y[:, 0]
     )
     assert np.isscalar(ridge_cv.alpha_)
     assert np.isscalar(ridge_cv.best_score_)
-    assert ridge_cv.cv_values_.shape == (n_samples, len(alphas))
+    assert ridge_cv.cv_results_.shape == (n_samples, len(alphas))
 
     # Try with a custom scoring function
     ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=True, scoring="r2").fit(X, y)
     assert_array_equal(optimal_alphas, ridge_cv.alpha_)
     assert_array_almost_equal(
         Ridge(alpha=ridge_cv.alpha_).fit(X, y).coef_, ridge_cv.coef_
     )
@@ -1170,15 +1186,15 @@
 
 @pytest.mark.parametrize("cv", [None, KFold(5)])
 @pytest.mark.parametrize("sparse_container", [None] + CSR_CONTAINERS)
 def test_ridge_regression_custom_scoring(sparse_container, cv):
     # check that custom scoring is working as expected
     # check the tie breaking strategy (keep the first alpha tried)
 
-    def _dummy_score(y_test, y_pred):
+    def _dummy_score(y_test, y_pred, **kwargs):
         return 0.42
 
     X = X_iris if sparse_container is None else sparse_container(X_iris)
     alphas = np.logspace(-2, 2, num=5)
     clf = RidgeClassifierCV(alphas=alphas, scoring=make_scorer(_dummy_score), cv=cv)
     clf.fit(X, y_iris)
     assert clf.best_score_ == pytest.approx(0.42)
@@ -1196,14 +1212,138 @@
     ridge2 = Ridge(tol=1e-3, fit_intercept=False)
     ridge2.fit(X, y_diabetes)
     score2 = ridge2.score(X, y_diabetes)
 
     assert score >= score2
 
 
+def check_array_api_attributes(name, estimator, array_namespace, device, dtype_name):
+    xp = _array_api_for_tests(array_namespace, device)
+
+    X_iris_np = X_iris.astype(dtype_name)
+    y_iris_np = y_iris.astype(dtype_name)
+
+    X_iris_xp = xp.asarray(X_iris_np, device=device)
+    y_iris_xp = xp.asarray(y_iris_np, device=device)
+
+    estimator.fit(X_iris_np, y_iris_np)
+    coef_np = estimator.coef_
+    intercept_np = estimator.intercept_
+
+    with config_context(array_api_dispatch=True):
+        estimator_xp = clone(estimator).fit(X_iris_xp, y_iris_xp)
+        coef_xp = estimator_xp.coef_
+        assert coef_xp.shape == (4,)
+        assert coef_xp.dtype == X_iris_xp.dtype
+
+        assert_allclose(
+            _convert_to_numpy(coef_xp, xp=xp),
+            coef_np,
+            atol=_atol_for_type(dtype_name),
+        )
+        intercept_xp = estimator_xp.intercept_
+        assert intercept_xp.shape == ()
+        assert intercept_xp.dtype == X_iris_xp.dtype
+
+        assert_allclose(
+            _convert_to_numpy(intercept_xp, xp=xp),
+            intercept_np,
+            atol=_atol_for_type(dtype_name),
+        )
+
+
+@pytest.mark.parametrize(
+    "array_namespace, device, dtype_name", yield_namespace_device_dtype_combinations()
+)
+@pytest.mark.parametrize(
+    "check",
+    [check_array_api_input_and_values, check_array_api_attributes],
+    ids=_get_check_estimator_ids,
+)
+@pytest.mark.parametrize(
+    "estimator",
+    [Ridge(solver="svd")],
+    ids=_get_check_estimator_ids,
+)
+def test_ridge_array_api_compliance(
+    estimator, check, array_namespace, device, dtype_name
+):
+    name = estimator.__class__.__name__
+    check(name, estimator, array_namespace, device=device, dtype_name=dtype_name)
+
+
+@pytest.mark.parametrize(
+    "array_namespace", yield_namespaces(include_numpy_namespaces=False)
+)
+def test_array_api_error_and_warnings_for_solver_parameter(array_namespace):
+    xp = _array_api_for_tests(array_namespace, device=None)
+
+    X_iris_xp = xp.asarray(X_iris[:5])
+    y_iris_xp = xp.asarray(y_iris[:5])
+
+    available_solvers = Ridge._parameter_constraints["solver"][0].options
+    for solver in available_solvers - {"auto", "svd"}:
+        ridge = Ridge(solver=solver, positive=solver == "lbfgs")
+        expected_msg = (
+            f"Array API dispatch to namespace {xp.__name__} only supports "
+            f"solver 'svd'. Got '{solver}'."
+        )
+
+        with pytest.raises(ValueError, match=expected_msg):
+            with config_context(array_api_dispatch=True):
+                ridge.fit(X_iris_xp, y_iris_xp)
+
+    ridge = Ridge(solver="auto", positive=True)
+    expected_msg = (
+        "The solvers that support positive fitting do not support "
+        f"Array API dispatch to namespace {xp.__name__}. Please "
+        "either disable Array API dispatch, or use a numpy-like "
+        "namespace, or set `positive=False`."
+    )
+
+    with pytest.raises(ValueError, match=expected_msg):
+        with config_context(array_api_dispatch=True):
+            ridge.fit(X_iris_xp, y_iris_xp)
+
+    ridge = Ridge()
+    expected_msg = (
+        f"Using Array API dispatch to namespace {xp.__name__} with `solver='auto'` "
+        "will result in using the solver 'svd'. The results may differ from those "
+        "when using a Numpy array, because in that case the preferred solver would "
+        "be cholesky. Set `solver='svd'` to suppress this warning."
+    )
+    with pytest.warns(UserWarning, match=expected_msg):
+        with config_context(array_api_dispatch=True):
+            ridge.fit(X_iris_xp, y_iris_xp)
+
+
+@pytest.mark.parametrize("array_namespace", sorted(_NUMPY_NAMESPACE_NAMES))
+def test_array_api_numpy_namespace_no_warning(array_namespace):
+    xp = _array_api_for_tests(array_namespace, device=None)
+
+    X_iris_xp = xp.asarray(X_iris[:5])
+    y_iris_xp = xp.asarray(y_iris[:5])
+
+    ridge = Ridge()
+    expected_msg = (
+        "Results might be different than when Array API dispatch is "
+        "disabled, or when a numpy-like namespace is used"
+    )
+
+    with warnings.catch_warnings():
+        warnings.filterwarnings("error", message=expected_msg, category=UserWarning)
+        with config_context(array_api_dispatch=True):
+            ridge.fit(X_iris_xp, y_iris_xp)
+
+    # All numpy namespaces are compatible with all solver, in particular
+    # solvers that support `positive=True` (like 'lbfgs') should work.
+    with config_context(array_api_dispatch=True):
+        Ridge(solver="auto", positive=True).fit(X_iris_xp, y_iris_xp)
+
+
 @pytest.mark.parametrize(
     "test_func",
     (
         _test_ridge_loo,
         _test_ridge_cv,
         _test_ridge_diabetes,
         _test_multi_ridge_diabetes,
@@ -1300,70 +1440,70 @@
 
     assert_array_equal(reg.predict([[-0.2, 2]]), np.array([-1]))
 
 
 @pytest.mark.parametrize(
     "scoring", [None, "neg_mean_squared_error", _mean_squared_error_callable]
 )
-def test_ridgecv_store_cv_values(scoring):
+def test_ridgecv_store_cv_results(scoring):
     rng = np.random.RandomState(42)
 
     n_samples = 8
     n_features = 5
     x = rng.randn(n_samples, n_features)
     alphas = [1e-1, 1e0, 1e1]
     n_alphas = len(alphas)
 
     scoring_ = make_scorer(scoring) if callable(scoring) else scoring
 
-    r = RidgeCV(alphas=alphas, cv=None, store_cv_values=True, scoring=scoring_)
+    r = RidgeCV(alphas=alphas, cv=None, store_cv_results=True, scoring=scoring_)
 
     # with len(y.shape) == 1
     y = rng.randn(n_samples)
     r.fit(x, y)
-    assert r.cv_values_.shape == (n_samples, n_alphas)
+    assert r.cv_results_.shape == (n_samples, n_alphas)
 
     # with len(y.shape) == 2
     n_targets = 3
     y = rng.randn(n_samples, n_targets)
     r.fit(x, y)
-    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)
+    assert r.cv_results_.shape == (n_samples, n_targets, n_alphas)
 
-    r = RidgeCV(cv=3, store_cv_values=True, scoring=scoring)
-    with pytest.raises(ValueError, match="cv!=None and store_cv_values"):
+    r = RidgeCV(cv=3, store_cv_results=True, scoring=scoring)
+    with pytest.raises(ValueError, match="cv!=None and store_cv_results"):
         r.fit(x, y)
 
 
 @pytest.mark.parametrize("scoring", [None, "accuracy", _accuracy_callable])
-def test_ridge_classifier_cv_store_cv_values(scoring):
+def test_ridge_classifier_cv_store_cv_results(scoring):
     x = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])
     y = np.array([1, 1, 1, -1, -1])
 
     n_samples = x.shape[0]
     alphas = [1e-1, 1e0, 1e1]
     n_alphas = len(alphas)
 
     scoring_ = make_scorer(scoring) if callable(scoring) else scoring
 
     r = RidgeClassifierCV(
-        alphas=alphas, cv=None, store_cv_values=True, scoring=scoring_
+        alphas=alphas, cv=None, store_cv_results=True, scoring=scoring_
     )
 
     # with len(y.shape) == 1
     n_targets = 1
     r.fit(x, y)
-    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)
+    assert r.cv_results_.shape == (n_samples, n_targets, n_alphas)
 
     # with len(y.shape) == 2
     y = np.array(
         [[1, 1, 1, -1, -1], [1, -1, 1, -1, 1], [-1, -1, 1, -1, -1]]
     ).transpose()
     n_targets = y.shape[1]
     r.fit(x, y)
-    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)
+    assert r.cv_results_.shape == (n_samples, n_targets, n_alphas)
 
 
 @pytest.mark.parametrize("Estimator", [RidgeCV, RidgeClassifierCV])
 def test_ridgecv_alphas_conversion(Estimator):
     rng = np.random.RandomState(0)
     alphas = (0.1, 1.0, 10.0)
 
@@ -1379,14 +1519,36 @@
         ridge_est.alphas is alphas
     ), f"`alphas` was mutated in `{Estimator.__name__}.__init__`"
 
     ridge_est.fit(X, y)
     assert_array_equal(ridge_est.alphas, np.asarray(alphas))
 
 
+@pytest.mark.parametrize("cv", [None, 3])
+@pytest.mark.parametrize("Estimator", [RidgeCV, RidgeClassifierCV])
+def test_ridgecv_alphas_zero(cv, Estimator):
+    """Check alpha=0.0 raises error only when `cv=None`."""
+    rng = np.random.RandomState(0)
+    alphas = (0.0, 1.0, 10.0)
+
+    n_samples, n_features = 5, 5
+    if Estimator is RidgeCV:
+        y = rng.randn(n_samples)
+    else:
+        y = rng.randint(0, 2, n_samples)
+    X = rng.randn(n_samples, n_features)
+
+    ridge_est = Estimator(alphas=alphas, cv=cv)
+    if cv is None:
+        with pytest.raises(ValueError, match=r"alphas\[0\] == 0.0, must be > 0.0."):
+            ridge_est.fit(X, y)
+    else:
+        ridge_est.fit(X, y)
+
+
 def test_ridgecv_sample_weight():
     rng = np.random.RandomState(0)
     alphas = (0.1, 1.0, 10.0)
 
     # There are different algorithms for n_samples > n_features
     # and the opposite, so test them both.
     for n_samples, n_features in ((6, 5), (5, 10)):
@@ -2058,7 +2220,50 @@
         X = sparse_container(X)
         X2 = sparse_container(X2)
     reg1 = Ridge(**params).fit(X, y, sample_weight=sample_weight_1)
     reg2 = Ridge(**params).fit(X2, y2, sample_weight=sample_weight_2)
     assert_allclose(reg1.coef_, reg2.coef_)
     if fit_intercept:
         assert_allclose(reg1.intercept_, reg2.intercept_)
+
+
+# TODO(1.7): Remove
+def test_ridge_store_cv_values_deprecated():
+    """Check `store_cv_values` parameter deprecated."""
+    X, y = make_regression(n_samples=6, random_state=42)
+    ridge = RidgeCV(store_cv_values=True)
+    msg = "'store_cv_values' is deprecated"
+    with pytest.warns(FutureWarning, match=msg):
+        ridge.fit(X, y)
+
+    # Error when both set
+    ridge = RidgeCV(store_cv_results=True, store_cv_values=True)
+    msg = "Both 'store_cv_values' and 'store_cv_results' were"
+    with pytest.raises(ValueError, match=msg):
+        ridge.fit(X, y)
+
+
+def test_ridge_cv_values_deprecated():
+    """Check `cv_values_` deprecated."""
+    X, y = make_regression(n_samples=6, random_state=42)
+    ridge = RidgeCV(store_cv_results=True)
+    msg = "Attribute `cv_values_` is deprecated"
+    with pytest.warns(FutureWarning, match=msg):
+        ridge.fit(X, y)
+        ridge.cv_values_
+
+
+# Metadata Routing Tests
+# ======================
+
+
+@pytest.mark.usefixtures("enable_slep006")
+@pytest.mark.parametrize("metaestimator", [RidgeCV, RidgeClassifierCV])
+def test_metadata_routing_with_default_scoring(metaestimator):
+    """Test that `RidgeCV` or `RidgeClassifierCV` with default `scoring`
+    argument (`None`), don't enter into `RecursionError` when metadata is routed.
+    """
+    metaestimator().get_metadata_routing()
+
+
+# End of Metadata Routing Tests
+# =============================
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_sag.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_sag.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 from sklearn.base import clone
 from sklearn.datasets import load_iris, make_blobs, make_classification
 from sklearn.linear_model import LogisticRegression, Ridge
 from sklearn.linear_model._base import make_dataset
 from sklearn.linear_model._linear_loss import LinearModelLoss
 from sklearn.linear_model._sag import get_auto_step_size
 from sklearn.linear_model._sag_fast import _multinomial_grad_loss_all_samples
+from sklearn.multiclass import OneVsRestClassifier
 from sklearn.preprocessing import LabelBinarizer, LabelEncoder
 from sklearn.utils import check_random_state, compute_class_weight
 from sklearn.utils._testing import (
     assert_allclose,
     assert_almost_equal,
     assert_array_almost_equal,
 )
@@ -268,15 +269,14 @@
         clf = LogisticRegression(
             solver=solver,
             fit_intercept=fit_intercept,
             tol=1e-11,
             C=1.0 / alpha / n_samples,
             max_iter=n_iter,
             random_state=10,
-            multi_class="ovr",
         )
         clf.fit(X, y)
 
         weights, intercept = sag_sparse(
             X,
             y,
             step_size,
@@ -367,24 +367,22 @@
     clf1 = LogisticRegression(
         solver="sag",
         fit_intercept=False,
         tol=0.0000001,
         C=1.0 / alpha / n_samples,
         max_iter=max_iter,
         random_state=10,
-        multi_class="ovr",
     )
     clf2 = clone(clf1)
     clf3 = LogisticRegression(
         fit_intercept=False,
         tol=0.0000001,
         C=1.0 / alpha / n_samples,
         max_iter=max_iter,
         random_state=10,
-        multi_class="ovr",
     )
 
     clf1.fit(X, y)
     clf2.fit(csr_container(X), y)
     clf3.fit(X, y)
 
     pobj1 = get_pobj(clf1.coef_, alpha, X, y, log_loss)
@@ -615,15 +613,14 @@
     clf1 = LogisticRegression(
         solver="sag",
         C=1.0 / alpha / n_samples,
         max_iter=n_iter,
         tol=tol,
         random_state=77,
         fit_intercept=fit_intercept,
-        multi_class="ovr",
     )
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
     clf2.fit(csr_container(X), y)
 
     spweights, spintercept = sag_sparse(
@@ -655,29 +652,30 @@
 
 @pytest.mark.filterwarnings("ignore:The max_iter was reached")
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_sag_multiclass_computed_correctly(csr_container):
     """tests if the multiclass classifier is computed correctly"""
     alpha = 0.1
     n_samples = 20
-    tol = 0.00001
-    max_iter = 40
+    tol = 1e-5
+    max_iter = 70
     fit_intercept = True
     X, y = make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1)
     step_size = get_step_size(X, alpha, fit_intercept, classification=True)
     classes = np.unique(y)
 
-    clf1 = LogisticRegression(
-        solver="sag",
-        C=1.0 / alpha / n_samples,
-        max_iter=max_iter,
-        tol=tol,
-        random_state=77,
-        fit_intercept=fit_intercept,
-        multi_class="ovr",
+    clf1 = OneVsRestClassifier(
+        LogisticRegression(
+            solver="sag",
+            C=1.0 / alpha / n_samples,
+            max_iter=max_iter,
+            tol=tol,
+            random_state=77,
+            fit_intercept=fit_intercept,
+        )
     )
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
     clf2.fit(csr_container(X), y)
 
     coef1 = []
@@ -715,19 +713,20 @@
 
     coef1 = np.vstack(coef1)
     intercept1 = np.array(intercept1)
     coef2 = np.vstack(coef2)
     intercept2 = np.array(intercept2)
 
     for i, cl in enumerate(classes):
-        assert_array_almost_equal(clf1.coef_[i].ravel(), coef1[i].ravel(), decimal=2)
-        assert_almost_equal(clf1.intercept_[i], intercept1[i], decimal=1)
+        assert_allclose(clf1.estimators_[i].coef_.ravel(), coef1[i], rtol=1e-2)
+        assert_allclose(clf1.estimators_[i].intercept_, intercept1[i], rtol=1e-1)
 
-        assert_array_almost_equal(clf2.coef_[i].ravel(), coef2[i].ravel(), decimal=2)
-        assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
+        assert_allclose(clf2.estimators_[i].coef_.ravel(), coef2[i], rtol=1e-2)
+        # Note the very crude accuracy, i.e. high rtol.
+        assert_allclose(clf2.estimators_[i].intercept_, intercept2[i], rtol=5e-1)
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_classifier_results(csr_container):
     """tests if classifier results match target"""
     alpha = 0.1
     n_features = 20
@@ -776,15 +775,14 @@
     clf1 = LogisticRegression(
         solver="sag",
         C=1.0 / alpha / n_samples,
         max_iter=n_iter,
         tol=tol,
         random_state=77,
         fit_intercept=fit_intercept,
-        multi_class="ovr",
         class_weight=class_weight,
     )
     clf2 = clone(clf1)
 
     clf1.fit(X, y)
     clf2.fit(csr_container(X), y)
 
@@ -816,91 +814,14 @@
     assert_array_almost_equal(clf1.coef_.ravel(), spweights.ravel(), decimal=2)
     assert_almost_equal(clf1.intercept_, spintercept, decimal=1)
 
     assert_array_almost_equal(clf2.coef_.ravel(), spweights2.ravel(), decimal=2)
     assert_almost_equal(clf2.intercept_, spintercept2, decimal=1)
 
 
-@pytest.mark.filterwarnings("ignore:The max_iter was reached")
-@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
-def test_multiclass_classifier_class_weight(csr_container):
-    """tests multiclass with classweights for each class"""
-    alpha = 0.1
-    n_samples = 20
-    tol = 0.00001
-    max_iter = 50
-    class_weight = {0: 0.45, 1: 0.55, 2: 0.75}
-    fit_intercept = True
-    X, y = make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1)
-    step_size = get_step_size(X, alpha, fit_intercept, classification=True)
-    classes = np.unique(y)
-
-    clf1 = LogisticRegression(
-        solver="sag",
-        C=1.0 / alpha / n_samples,
-        max_iter=max_iter,
-        tol=tol,
-        random_state=77,
-        fit_intercept=fit_intercept,
-        multi_class="ovr",
-        class_weight=class_weight,
-    )
-    clf2 = clone(clf1)
-    clf1.fit(X, y)
-    clf2.fit(csr_container(X), y)
-
-    le = LabelEncoder()
-    class_weight_ = compute_class_weight(class_weight, classes=np.unique(y), y=y)
-    sample_weight = class_weight_[le.fit_transform(y)]
-
-    coef1 = []
-    intercept1 = []
-    coef2 = []
-    intercept2 = []
-    for cl in classes:
-        y_encoded = np.ones(n_samples)
-        y_encoded[y != cl] = -1
-
-        spweights1, spintercept1 = sag_sparse(
-            X,
-            y_encoded,
-            step_size,
-            alpha,
-            n_iter=max_iter,
-            dloss=log_dloss,
-            sample_weight=sample_weight,
-        )
-        spweights2, spintercept2 = sag_sparse(
-            X,
-            y_encoded,
-            step_size,
-            alpha,
-            n_iter=max_iter,
-            dloss=log_dloss,
-            sample_weight=sample_weight,
-            sparse=True,
-        )
-        coef1.append(spweights1)
-        intercept1.append(spintercept1)
-        coef2.append(spweights2)
-        intercept2.append(spintercept2)
-
-    coef1 = np.vstack(coef1)
-    intercept1 = np.array(intercept1)
-    coef2 = np.vstack(coef2)
-    intercept2 = np.array(intercept2)
-
-    for i, cl in enumerate(classes):
-        assert_array_almost_equal(clf1.coef_[i].ravel(), coef1[i].ravel(), decimal=2)
-        assert_almost_equal(clf1.intercept_[i], intercept1[i], decimal=1)
-
-        assert_array_almost_equal(clf2.coef_[i].ravel(), coef2[i].ravel(), decimal=2)
-        assert_almost_equal(clf2.intercept_[i], intercept2[i], decimal=1)
-
-
 def test_classifier_single_class():
     """tests if ValueError is thrown with only one class"""
     X = [[1, 2], [3, 4]]
     y = [1, 1]
 
     msg = "This solver needs samples of at least 2 classes in the data"
     with pytest.raises(ValueError, match=msg):
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_sgd.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_sgd.py`

 * *Files 1% similar despite different names*

```diff
@@ -356,15 +356,15 @@
         loss="squared_error",
         eta0=eta0,
         alpha=alpha,
         max_iter=2,
         shuffle=False,
     )
     clf2 = klass(
-        average=0,
+        average=False,
         learning_rate="constant",
         loss="squared_error",
         eta0=eta0,
         alpha=alpha,
         max_iter=1,
         shuffle=False,
     )
@@ -1542,15 +1542,20 @@
 
     # 2 passes over the training set but average only at second pass
     clf1 = klass(
         average=7, learning_rate="constant", eta0=eta0, nu=nu, max_iter=2, shuffle=False
     )
     # 1 pass over the training set with no averaging
     clf2 = klass(
-        average=0, learning_rate="constant", eta0=eta0, nu=nu, max_iter=1, shuffle=False
+        average=False,
+        learning_rate="constant",
+        eta0=eta0,
+        nu=nu,
+        max_iter=1,
+        shuffle=False,
     )
 
     clf1.fit(X)
     clf2.fit(X)
 
     # Start from clf2 solution, compute averaging using asgd function and
     # compare with clf1 solution
@@ -2218,7 +2223,15 @@
     # `loss_function_`.
     X = np.array([[1, 2], [3, 4]])
     y = np.array([1, 0])
     est = Estimator().fit(X, y)
 
     with pytest.warns(FutureWarning, match="`loss_function_` was deprecated"):
         est.loss_function_
+
+
+# TODO(1.7): remove
+@pytest.mark.parametrize("Estimator", [SGDClassifier, SGDRegressor, SGDOneClassSVM])
+def test_passive_aggressive_deprecated_average(Estimator):
+    est = Estimator(average=0)
+    with pytest.warns(FutureWarning, match="average=0"):
+        est.fit(X, Y)
```

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_sparse_coordinate_descent.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_sparse_coordinate_descent.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/linear_model/tests/test_theil_sen.py` & `scikit_learn-1.5.0rc1/sklearn/linear_model/tests/test_theil_sen.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_barnes_hut_tsne.pyx` & `scikit_learn-1.5.0rc1/sklearn/manifold/_barnes_hut_tsne.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -6,57 +6,47 @@
 
 
 import numpy as np
 cimport numpy as cnp
 from libc.stdio cimport printf
 from libc.math cimport log
 from libc.stdlib cimport malloc, free
+from libc.time cimport clock, clock_t
 from cython.parallel cimport prange, parallel
 
 from ..neighbors._quad_tree cimport _QuadTree
 
 cnp.import_array()
 
 
 cdef char* EMPTY_STRING = ""
 
-cdef extern from "math.h":
-    float fabsf(float x) nogil
-
 # Smallest strictly positive value that can be represented by floating
 # point numbers for different precision levels. This is useful to avoid
 # taking the log of zero when computing the KL divergence.
 cdef float FLOAT32_TINY = np.finfo(np.float32).tiny
 
 # Useful to void division by zero or divergence to +inf.
 cdef float FLOAT64_EPS = np.finfo(np.float64).eps
 
 # This is effectively an ifdef statement in Cython
 # It allows us to write printf debugging lines
 # and remove them at compile time
 cdef enum:
     DEBUGFLAG = 0
 
-cdef extern from "time.h":
-    # Declare only what is necessary from `tm` structure.
-    ctypedef long clock_t
-    clock_t clock() nogil
-    double CLOCKS_PER_SEC
-
-
 cdef float compute_gradient(float[:] val_P,
                             float[:, :] pos_reference,
                             cnp.int64_t[:] neighbors,
                             cnp.int64_t[:] indptr,
                             float[:, :] tot_force,
                             _QuadTree qt,
                             float theta,
                             int dof,
                             long start,
-                            long stop,
                             bint compute_error,
                             int num_threads) noexcept nogil:
     # Having created the tree, calculate the gradient
     # in two components, the positive and negative forces
     cdef:
         long i, coord
         int ax
@@ -72,15 +62,15 @@
                n_samples * n_dimensions * 2)
     cdef float* neg_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)
     cdef float* pos_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)
 
     if take_timing:
         t1 = clock()
     sQ = compute_gradient_negative(pos_reference, neg_f, qt, dof, theta, start,
-                                   stop, num_threads)
+                                   num_threads)
     if take_timing:
         t2 = clock()
         printf("[t-SNE] Computing negative gradient: %e ticks\n", ((float) (t2 - t1)))
 
     if take_timing:
         t1 = clock()
     error = compute_gradient_positive(val_P, pos_reference, neighbors, indptr,
@@ -171,24 +161,22 @@
 
 cdef double compute_gradient_negative(float[:, :] pos_reference,
                                       float* neg_f,
                                       _QuadTree qt,
                                       int dof,
                                       float theta,
                                       long start,
-                                      long stop,
                                       int num_threads) noexcept nogil:
-    if stop == -1:
-        stop = pos_reference.shape[0]
     cdef:
         int ax
         int n_dimensions = qt.n_dimensions
         int offset = n_dimensions + 2
         long i, j, idx
-        long n = stop - start
+        long n_samples = pos_reference.shape[0]
+        long n = n_samples - start
         long dta = 0
         long dtb = 0
         float size, dist2s, mult
         float exponent = (dof + 1.0) / 2.0
         float float_dof = (float) (dof)
         double qijZ, sum_Q = 0.0
         float* force
@@ -200,15 +188,15 @@
     with nogil, parallel(num_threads=num_threads):
         # Define thread-local buffers
         summary = <float*> malloc(sizeof(float) * n * offset)
         pos = <float *> malloc(sizeof(float) * n_dimensions)
         force = <float *> malloc(sizeof(float) * n_dimensions)
         neg_force = <float *> malloc(sizeof(float) * n_dimensions)
 
-        for i in prange(start, stop, schedule='static'):
+        for i in prange(start, n_samples, schedule='static'):
             # Clear the arrays
             for ax in range(n_dimensions):
                 force[ax] = 0.0
                 neg_force[ax] = 0.0
                 pos[ax] = pos_reference[i, ax]
 
             # Find which nodes are summarizing and collect their centers of mass
@@ -288,15 +276,15 @@
     if verbose > 10:
         # XXX: format hack to workaround lack of `const char *` type
         # in the generated C code that triggers error with gcc 4.9
         # and -Werror=format-security
         printf("[t-SNE] Computing gradient\n%s", EMPTY_STRING)
 
     C = compute_gradient(val_P, pos_output, neighbors, indptr, forces,
-                         qt, theta, dof, skip_num_points, -1, compute_error,
+                         qt, theta, dof, skip_num_points, compute_error,
                          num_threads)
 
     if verbose > 10:
         # XXX: format hack to workaround lack of `const char *` type
         # in the generated C code
         # and -Werror=format-security
         printf("[t-SNE] Checking tree consistency\n%s", EMPTY_STRING)
```

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_isomap.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/_isomap.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_locally_linear.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/_locally_linear.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     TransformerMixin,
     _fit_context,
     _UnstableArchMixin,
 )
 from ..neighbors import NearestNeighbors
 from ..utils import check_array, check_random_state
 from ..utils._arpack import _init_arpack_v0
-from ..utils._param_validation import Interval, StrOptions
+from ..utils._param_validation import Interval, StrOptions, validate_params
 from ..utils.extmath import stable_cumsum
 from ..utils.validation import FLOAT_DTYPES, check_is_fitted
 
 
 def barycenter_weights(X, Y, indices, reg=1e-3):
     """Compute barycenter weights of X from Y along the first axis
 
@@ -194,141 +194,29 @@
         )
         index = np.argsort(np.abs(eigen_values))
         return eigen_vectors[:, index], np.sum(eigen_values)
     else:
         raise ValueError("Unrecognized eigen_solver '%s'" % eigen_solver)
 
 
-def locally_linear_embedding(
+def _locally_linear_embedding(
     X,
     *,
     n_neighbors,
     n_components,
     reg=1e-3,
     eigen_solver="auto",
     tol=1e-6,
     max_iter=100,
     method="standard",
     hessian_tol=1e-4,
     modified_tol=1e-12,
     random_state=None,
     n_jobs=None,
 ):
-    """Perform a Locally Linear Embedding analysis on the data.
-
-    Read more in the :ref:`User Guide <locally_linear_embedding>`.
-
-    Parameters
-    ----------
-    X : {array-like, NearestNeighbors}
-        Sample data, shape = (n_samples, n_features), in the form of a
-        numpy array or a NearestNeighbors object.
-
-    n_neighbors : int
-        Number of neighbors to consider for each point.
-
-    n_components : int
-        Number of coordinates for the manifold.
-
-    reg : float, default=1e-3
-        Regularization constant, multiplies the trace of the local covariance
-        matrix of the distances.
-
-    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'
-        auto : algorithm will attempt to choose the best method for input data
-
-        arpack : use arnoldi iteration in shift-invert mode.
-                    For this method, M may be a dense matrix, sparse matrix,
-                    or general linear operator.
-                    Warning: ARPACK can be unstable for some problems.  It is
-                    best to try several random seeds in order to check results.
-
-        dense  : use standard dense matrix operations for the eigenvalue
-                    decomposition.  For this method, M must be an array
-                    or matrix type.  This method should be avoided for
-                    large problems.
-
-    tol : float, default=1e-6
-        Tolerance for 'arpack' method
-        Not used if eigen_solver=='dense'.
-
-    max_iter : int, default=100
-        Maximum number of iterations for the arpack solver.
-
-    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'
-        standard : use the standard locally linear embedding algorithm.
-                   see reference [1]_
-        hessian  : use the Hessian eigenmap method.  This method requires
-                   n_neighbors > n_components * (1 + (n_components + 1) / 2.
-                   see reference [2]_
-        modified : use the modified locally linear embedding algorithm.
-                   see reference [3]_
-        ltsa     : use local tangent space alignment algorithm
-                   see reference [4]_
-
-    hessian_tol : float, default=1e-4
-        Tolerance for Hessian eigenmapping method.
-        Only used if method == 'hessian'.
-
-    modified_tol : float, default=1e-12
-        Tolerance for modified LLE method.
-        Only used if method == 'modified'.
-
-    random_state : int, RandomState instance, default=None
-        Determines the random number generator when ``solver`` == 'arpack'.
-        Pass an int for reproducible results across multiple function calls.
-        See :term:`Glossary <random_state>`.
-
-    n_jobs : int or None, default=None
-        The number of parallel jobs to run for neighbors search.
-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
-        for more details.
-
-    Returns
-    -------
-    Y : array-like, shape [n_samples, n_components]
-        Embedding vectors.
-
-    squared_error : float
-        Reconstruction error for the embedding vectors. Equivalent to
-        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.
-
-    References
-    ----------
-
-    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction
-        by locally linear embedding.  Science 290:2323 (2000).
-    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally
-        linear embedding techniques for high-dimensional data.
-        Proc Natl Acad Sci U S A.  100:5591 (2003).
-    .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear
-        Embedding Using Multiple Weights.
-        <https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3>`_
-    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear
-        dimensionality reduction via tangent space alignment.
-        Journal of Shanghai Univ.  8:406 (2004)
-
-    Examples
-    --------
-    >>> from sklearn.datasets import load_digits
-    >>> from sklearn.manifold import locally_linear_embedding
-    >>> X, _ = load_digits(return_X_y=True)
-    >>> X.shape
-    (1797, 64)
-    >>> embedding, _ = locally_linear_embedding(X[:100],n_neighbors=5, n_components=2)
-    >>> embedding.shape
-    (100, 2)
-    """
-    if eigen_solver not in ("auto", "arpack", "dense"):
-        raise ValueError("unrecognized eigen_solver '%s'" % eigen_solver)
-
-    if method not in ("standard", "hessian", "modified", "ltsa"):
-        raise ValueError("unrecognized method '%s'" % method)
-
     nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs)
     nbrs.fit(X)
     X = nbrs._fit_X
 
     N, d_in = X.shape
 
     if n_components > d_in:
@@ -337,17 +225,14 @@
         )
     if n_neighbors >= N:
         raise ValueError(
             "Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d"
             % (N, n_neighbors)
         )
 
-    if n_neighbors <= 0:
-        raise ValueError("n_neighbors must be positive")
-
     M_sparse = eigen_solver != "dense"
 
     if method == "standard":
         W = barycenter_kneighbors_graph(
             nbrs, n_neighbors=n_neighbors, reg=reg, n_jobs=n_jobs
         )
 
@@ -557,14 +442,168 @@
         eigen_solver=eigen_solver,
         tol=tol,
         max_iter=max_iter,
         random_state=random_state,
     )
 
 
+@validate_params(
+    {
+        "X": ["array-like", NearestNeighbors],
+        "n_neighbors": [Interval(Integral, 1, None, closed="left")],
+        "n_components": [Interval(Integral, 1, None, closed="left")],
+        "reg": [Interval(Real, 0, None, closed="left")],
+        "eigen_solver": [StrOptions({"auto", "arpack", "dense"})],
+        "tol": [Interval(Real, 0, None, closed="left")],
+        "max_iter": [Interval(Integral, 1, None, closed="left")],
+        "method": [StrOptions({"standard", "hessian", "modified", "ltsa"})],
+        "hessian_tol": [Interval(Real, 0, None, closed="left")],
+        "modified_tol": [Interval(Real, 0, None, closed="left")],
+        "random_state": ["random_state"],
+        "n_jobs": [None, Integral],
+    },
+    prefer_skip_nested_validation=True,
+)
+def locally_linear_embedding(
+    X,
+    *,
+    n_neighbors,
+    n_components,
+    reg=1e-3,
+    eigen_solver="auto",
+    tol=1e-6,
+    max_iter=100,
+    method="standard",
+    hessian_tol=1e-4,
+    modified_tol=1e-12,
+    random_state=None,
+    n_jobs=None,
+):
+    """Perform a Locally Linear Embedding analysis on the data.
+
+    Read more in the :ref:`User Guide <locally_linear_embedding>`.
+
+    Parameters
+    ----------
+    X : {array-like, NearestNeighbors}
+        Sample data, shape = (n_samples, n_features), in the form of a
+        numpy array or a NearestNeighbors object.
+
+    n_neighbors : int
+        Number of neighbors to consider for each point.
+
+    n_components : int
+        Number of coordinates for the manifold.
+
+    reg : float, default=1e-3
+        Regularization constant, multiplies the trace of the local covariance
+        matrix of the distances.
+
+    eigen_solver : {'auto', 'arpack', 'dense'}, default='auto'
+        auto : algorithm will attempt to choose the best method for input data
+
+        arpack : use arnoldi iteration in shift-invert mode.
+                    For this method, M may be a dense matrix, sparse matrix,
+                    or general linear operator.
+                    Warning: ARPACK can be unstable for some problems.  It is
+                    best to try several random seeds in order to check results.
+
+        dense  : use standard dense matrix operations for the eigenvalue
+                    decomposition.  For this method, M must be an array
+                    or matrix type.  This method should be avoided for
+                    large problems.
+
+    tol : float, default=1e-6
+        Tolerance for 'arpack' method
+        Not used if eigen_solver=='dense'.
+
+    max_iter : int, default=100
+        Maximum number of iterations for the arpack solver.
+
+    method : {'standard', 'hessian', 'modified', 'ltsa'}, default='standard'
+        standard : use the standard locally linear embedding algorithm.
+                   see reference [1]_
+        hessian  : use the Hessian eigenmap method.  This method requires
+                   n_neighbors > n_components * (1 + (n_components + 1) / 2.
+                   see reference [2]_
+        modified : use the modified locally linear embedding algorithm.
+                   see reference [3]_
+        ltsa     : use local tangent space alignment algorithm
+                   see reference [4]_
+
+    hessian_tol : float, default=1e-4
+        Tolerance for Hessian eigenmapping method.
+        Only used if method == 'hessian'.
+
+    modified_tol : float, default=1e-12
+        Tolerance for modified LLE method.
+        Only used if method == 'modified'.
+
+    random_state : int, RandomState instance, default=None
+        Determines the random number generator when ``solver`` == 'arpack'.
+        Pass an int for reproducible results across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    n_jobs : int or None, default=None
+        The number of parallel jobs to run for neighbors search.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+    Returns
+    -------
+    Y : ndarray of shape (n_samples, n_components)
+        Embedding vectors.
+
+    squared_error : float
+        Reconstruction error for the embedding vectors. Equivalent to
+        ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.
+
+    References
+    ----------
+
+    .. [1] Roweis, S. & Saul, L. Nonlinear dimensionality reduction
+        by locally linear embedding.  Science 290:2323 (2000).
+    .. [2] Donoho, D. & Grimes, C. Hessian eigenmaps: Locally
+        linear embedding techniques for high-dimensional data.
+        Proc Natl Acad Sci U S A.  100:5591 (2003).
+    .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear
+        Embedding Using Multiple Weights.
+        <https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3>`_
+    .. [4] Zhang, Z. & Zha, H. Principal manifolds and nonlinear
+        dimensionality reduction via tangent space alignment.
+        Journal of Shanghai Univ.  8:406 (2004)
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import locally_linear_embedding
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding, _ = locally_linear_embedding(X[:100],n_neighbors=5, n_components=2)
+    >>> embedding.shape
+    (100, 2)
+    """
+    return _locally_linear_embedding(
+        X=X,
+        n_neighbors=n_neighbors,
+        n_components=n_components,
+        reg=reg,
+        eigen_solver=eigen_solver,
+        tol=tol,
+        max_iter=max_iter,
+        method=method,
+        hessian_tol=hessian_tol,
+        modified_tol=modified_tol,
+        random_state=random_state,
+        n_jobs=n_jobs,
+    )
+
+
 class LocallyLinearEmbedding(
     ClassNamePrefixFeaturesOutMixin,
     TransformerMixin,
     _UnstableArchMixin,
     BaseEstimator,
 ):
     """Locally Linear Embedding.
@@ -749,15 +788,15 @@
             algorithm=self.neighbors_algorithm,
             n_jobs=self.n_jobs,
         )
 
         random_state = check_random_state(self.random_state)
         X = self._validate_data(X, dtype=float)
         self.nbrs_.fit(X)
-        self.embedding_, self.reconstruction_error_ = locally_linear_embedding(
+        self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(
             X=self.nbrs_,
             n_neighbors=self.n_neighbors,
             n_components=self.n_components,
             eigen_solver=self.eigen_solver,
             tol=self.tol,
             max_iter=self.max_iter,
             method=self.method,
```

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_mds.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/_mds.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_spectral_embedding.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/_spectral_embedding.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 from ..neighbors import NearestNeighbors, kneighbors_graph
 from ..utils import (
     check_array,
     check_random_state,
     check_symmetric,
 )
 from ..utils._arpack import _init_arpack_v0
-from ..utils._param_validation import Interval, StrOptions
+from ..utils._param_validation import Interval, StrOptions, validate_params
 from ..utils.extmath import _deterministic_vector_sign_flip
 from ..utils.fixes import laplacian as csgraph_laplacian
 from ..utils.fixes import parse_version, sp_version
 
 
 def _graph_connected_component(graph, node_id):
     """Find the largest graph connected components that contains one
@@ -148,14 +148,26 @@
         else:
             # csr has the fastest matvec and is thus best suited to
             # arpack
             laplacian = laplacian.tocsr()
     return laplacian
 
 
+@validate_params(
+    {
+        "adjacency": ["array-like", "sparse matrix"],
+        "n_components": [Interval(Integral, 1, None, closed="left")],
+        "eigen_solver": [StrOptions({"arpack", "lobpcg", "amg"}), None],
+        "random_state": ["random_state"],
+        "eigen_tol": [Interval(Real, 0, None, closed="left"), StrOptions({"auto"})],
+        "norm_laplacian": ["boolean"],
+        "drop_first": ["boolean"],
+    },
+    prefer_skip_nested_validation=True,
+)
 def spectral_embedding(
     adjacency,
     *,
     n_components=8,
     eigen_solver=None,
     random_state=None,
     eigen_tol="auto",
@@ -268,33 +280,49 @@
     ... )
     >>> # make the matrix symmetric
     >>> affinity_matrix = 0.5 * (affinity_matrix + affinity_matrix.T)
     >>> embedding = spectral_embedding(affinity_matrix, n_components=2, random_state=42)
     >>> embedding.shape
     (100, 2)
     """
+    random_state = check_random_state(random_state)
+
+    return _spectral_embedding(
+        adjacency,
+        n_components=n_components,
+        eigen_solver=eigen_solver,
+        random_state=random_state,
+        eigen_tol=eigen_tol,
+        norm_laplacian=norm_laplacian,
+        drop_first=drop_first,
+    )
+
+
+def _spectral_embedding(
+    adjacency,
+    *,
+    n_components=8,
+    eigen_solver=None,
+    random_state=None,
+    eigen_tol="auto",
+    norm_laplacian=True,
+    drop_first=True,
+):
     adjacency = check_symmetric(adjacency)
 
     if eigen_solver == "amg":
         try:
             from pyamg import smoothed_aggregation_solver
         except ImportError as e:
             raise ValueError(
                 "The eigen_solver was set to 'amg', but pyamg is not available."
             ) from e
 
     if eigen_solver is None:
         eigen_solver = "arpack"
-    elif eigen_solver not in ("arpack", "lobpcg", "amg"):
-        raise ValueError(
-            "Unknown value for eigen_solver: '%s'."
-            "Should be 'amg', 'arpack', or 'lobpcg'" % eigen_solver
-        )
-
-    random_state = check_random_state(random_state)
 
     n_nodes = adjacency.shape[0]
     # Whether to drop the first eigenvector
     if drop_first:
         n_components = n_components + 1
 
     if not _graph_is_connected(adjacency):
@@ -618,15 +646,16 @@
         self.eigen_solver = eigen_solver
         self.eigen_tol = eigen_tol
         self.n_neighbors = n_neighbors
         self.n_jobs = n_jobs
 
     def _more_tags(self):
         return {
-            "pairwise": self.affinity in [
+            "pairwise": self.affinity
+            in [
                 "precomputed",
                 "precomputed_nearest_neighbors",
             ]
         }
 
     def _get_affinity_matrix(self, X, Y=None):
         """Calculate the affinity matrix from data
@@ -710,15 +739,15 @@
             Returns the instance itself.
         """
         X = self._validate_data(X, accept_sparse="csr", ensure_min_samples=2)
 
         random_state = check_random_state(self.random_state)
 
         affinity_matrix = self._get_affinity_matrix(X)
-        self.embedding_ = spectral_embedding(
+        self.embedding_ = _spectral_embedding(
             affinity_matrix,
             n_components=self.n_components,
             eigen_solver=self.eigen_solver,
             eigen_tol=self.eigen_tol,
             random_state=random_state,
         )
         return self
```

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_t_sne.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/_t_sne.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 # License: BSD 3 clause (C) 2014
 
 # This is the exact and Barnes-Hut t-SNE implementation. There are other
 # modifications of the algorithm:
 # * Fast Optimization for t-SNE:
 #   https://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/papers/vandermaaten.pdf
 
+import warnings
 from numbers import Integral, Real
 from time import time
 
 import numpy as np
 from scipy import linalg
 from scipy.sparse import csr_matrix, issparse
 from scipy.spatial.distance import pdist, squareform
@@ -23,15 +24,15 @@
     _fit_context,
 )
 from ..decomposition import PCA
 from ..metrics.pairwise import _VALID_METRICS, pairwise_distances
 from ..neighbors import NearestNeighbors
 from ..utils import check_random_state
 from ..utils._openmp_helpers import _openmp_effective_n_threads
-from ..utils._param_validation import Interval, StrOptions, validate_params
+from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params
 from ..utils.validation import _num_samples, check_non_negative
 
 # mypy error: Module 'sklearn.manifold' has no attribute '_utils'
 # mypy error: Module 'sklearn.manifold' has no attribute '_barnes_hut_tsne'
 from . import _barnes_hut_tsne, _utils  # type: ignore
 
 MACHINE_EPSILON = np.finfo(np.double).eps
@@ -300,15 +301,15 @@
     return error, grad
 
 
 def _gradient_descent(
     objective,
     p0,
     it,
-    n_iter,
+    max_iter,
     n_iter_check=1,
     n_iter_without_progress=300,
     momentum=0.8,
     learning_rate=200.0,
     min_gain=0.01,
     min_grad_norm=1e-7,
     verbose=0,
@@ -328,15 +329,15 @@
     p0 : array-like of shape (n_params,)
         Initial parameter vector.
 
     it : int
         Current number of iterations (this function will be called more than
         once during the optimization).
 
-    n_iter : int
+    max_iter : int
         Maximum number of gradient descent iterations.
 
     n_iter_check : int, default=1
         Number of iterations before evaluating the global error. If the error
         is sufficiently low, we abort the optimization.
 
     n_iter_without_progress : int, default=300
@@ -390,18 +391,18 @@
     update = np.zeros_like(p)
     gains = np.ones_like(p)
     error = np.finfo(float).max
     best_error = np.finfo(float).max
     best_iter = i = it
 
     tic = time()
-    for i in range(it, n_iter):
+    for i in range(it, max_iter):
         check_convergence = (i + 1) % n_iter_check == 0
         # only compute the error when needed
-        kwargs["compute_error"] = check_convergence or i == n_iter - 1
+        kwargs["compute_error"] = check_convergence or i == max_iter - 1
 
         error, grad = objective(p, *args, **kwargs)
 
         inc = update * grad < 0.0
         dec = np.invert(inc)
         gains[inc] += 0.2
         gains[dec] *= 0.8
@@ -613,18 +614,21 @@
         those other implementations. The 'auto' option sets the learning_rate
         to `max(N / early_exaggeration / 4, 50)` where N is the sample size,
         following [4] and [5].
 
         .. versionchanged:: 1.2
            The default value changed to `"auto"`.
 
-    n_iter : int, default=1000
+    max_iter : int, default=1000
         Maximum number of iterations for the optimization. Should be at
         least 250.
 
+        .. versionchanged:: 1.5
+            Parameter name changed from `n_iter` to `max_iter`.
+
     n_iter_without_progress : int, default=300
         Maximum number of iterations without progress before we abort the
         optimization, used after 250 initial iterations with early
         exaggeration. Note that progress is only checked every 50 iterations so
         this value is rounded to the next multiple of 50.
 
         .. versionadded:: 0.17
@@ -696,14 +700,22 @@
         (``metric="euclidean"`` and ``method="exact"``).
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
 
         .. versionadded:: 0.22
 
+    n_iter : int
+        Maximum number of iterations for the optimization. Should be at
+        least 250.
+
+        .. deprecated:: 1.5
+            `n_iter` was deprecated in version 1.5 and will be removed in 1.7.
+            Please use `max_iter` instead.
+
     Attributes
     ----------
     embedding_ : array-like of shape (n_samples, n_components)
         Stores the embedding vectors.
 
     kl_divergence_ : float
         Kullback-Leibler divergence after optimization.
@@ -780,70 +792,76 @@
         "n_components": [Interval(Integral, 1, None, closed="left")],
         "perplexity": [Interval(Real, 0, None, closed="neither")],
         "early_exaggeration": [Interval(Real, 1, None, closed="left")],
         "learning_rate": [
             StrOptions({"auto"}),
             Interval(Real, 0, None, closed="neither"),
         ],
-        "n_iter": [Interval(Integral, 250, None, closed="left")],
+        "max_iter": [Interval(Integral, 250, None, closed="left"), None],
         "n_iter_without_progress": [Interval(Integral, -1, None, closed="left")],
         "min_grad_norm": [Interval(Real, 0, None, closed="left")],
         "metric": [StrOptions(set(_VALID_METRICS) | {"precomputed"}), callable],
         "metric_params": [dict, None],
         "init": [
             StrOptions({"pca", "random"}),
             np.ndarray,
         ],
         "verbose": ["verbose"],
         "random_state": ["random_state"],
         "method": [StrOptions({"barnes_hut", "exact"})],
         "angle": [Interval(Real, 0, 1, closed="both")],
         "n_jobs": [None, Integral],
+        "n_iter": [
+            Interval(Integral, 250, None, closed="left"),
+            Hidden(StrOptions({"deprecated"})),
+        ],
     }
 
     # Control the number of exploration iterations with early_exaggeration on
-    _EXPLORATION_N_ITER = 250
+    _EXPLORATION_MAX_ITER = 250
 
     # Control the number of iterations between progress checks
     _N_ITER_CHECK = 50
 
     def __init__(
         self,
         n_components=2,
         *,
         perplexity=30.0,
         early_exaggeration=12.0,
         learning_rate="auto",
-        n_iter=1000,
+        max_iter=None,  # TODO(1.7): set to 1000
         n_iter_without_progress=300,
         min_grad_norm=1e-7,
         metric="euclidean",
         metric_params=None,
         init="pca",
         verbose=0,
         random_state=None,
         method="barnes_hut",
         angle=0.5,
         n_jobs=None,
+        n_iter="deprecated",
     ):
         self.n_components = n_components
         self.perplexity = perplexity
         self.early_exaggeration = early_exaggeration
         self.learning_rate = learning_rate
-        self.n_iter = n_iter
+        self.max_iter = max_iter
         self.n_iter_without_progress = n_iter_without_progress
         self.min_grad_norm = min_grad_norm
         self.metric = metric
         self.metric_params = metric_params
         self.init = init
         self.verbose = verbose
         self.random_state = random_state
         self.method = method
         self.angle = angle
         self.n_jobs = n_jobs
+        self.n_iter = n_iter
 
     def _check_params_vs_input(self, X):
         if self.perplexity >= X.shape[0]:
             raise ValueError("perplexity must be less than n_samples")
 
     def _fit(self, X, skip_num_points=0):
         """Private function to fit the model using X as training data."""
@@ -1053,16 +1071,16 @@
             "it": 0,
             "n_iter_check": self._N_ITER_CHECK,
             "min_grad_norm": self.min_grad_norm,
             "learning_rate": self.learning_rate_,
             "verbose": self.verbose,
             "kwargs": dict(skip_num_points=skip_num_points),
             "args": [P, degrees_of_freedom, n_samples, self.n_components],
-            "n_iter_without_progress": self._EXPLORATION_N_ITER,
-            "n_iter": self._EXPLORATION_N_ITER,
+            "n_iter_without_progress": self._EXPLORATION_MAX_ITER,
+            "max_iter": self._EXPLORATION_MAX_ITER,
             "momentum": 0.5,
         }
         if self.method == "barnes_hut":
             obj_func = _kl_divergence_bh
             opt_args["kwargs"]["angle"] = self.angle
             # Repeat verbose argument for _kl_divergence_bh
             opt_args["kwargs"]["verbose"] = self.verbose
@@ -1081,17 +1099,17 @@
                 "[t-SNE] KL divergence after %d iterations with early exaggeration: %f"
                 % (it + 1, kl_divergence)
             )
 
         # Learning schedule (part 2): disable early exaggeration and finish
         # optimization with a higher momentum at 0.8
         P /= self.early_exaggeration
-        remaining = self.n_iter - self._EXPLORATION_N_ITER
-        if it < self._EXPLORATION_N_ITER or remaining > 0:
-            opt_args["n_iter"] = self.n_iter
+        remaining = self._max_iter - self._EXPLORATION_MAX_ITER
+        if it < self._EXPLORATION_MAX_ITER or remaining > 0:
+            opt_args["max_iter"] = self._max_iter
             opt_args["it"] = it + 1
             opt_args["momentum"] = 0.8
             opt_args["n_iter_without_progress"] = self.n_iter_without_progress
             params, kl_divergence, it = _gradient_descent(obj_func, params, **opt_args)
 
         # Save the final number of iterations
         self.n_iter_ = it
@@ -1128,14 +1146,36 @@
             Ignored.
 
         Returns
         -------
         X_new : ndarray of shape (n_samples, n_components)
             Embedding of the training data in low-dimensional space.
         """
+        # TODO(1.7): remove
+        # Also make sure to change `max_iter` default back to 1000 and deprecate None
+        if self.n_iter != "deprecated":
+            if self.max_iter is not None:
+                raise ValueError(
+                    "Both 'n_iter' and 'max_iter' attributes were set. Attribute"
+                    " 'n_iter' was deprecated in version 1.5 and will be removed in"
+                    " 1.7. To avoid this error, only set the 'max_iter' attribute."
+                )
+            warnings.warn(
+                (
+                    "'n_iter' was renamed to 'max_iter' in version 1.5 and "
+                    "will be removed in 1.7."
+                ),
+                FutureWarning,
+            )
+            self._max_iter = self.n_iter
+        elif self.max_iter is None:
+            self._max_iter = 1000
+        else:
+            self._max_iter = self.max_iter
+
         self._check_params_vs_input(X)
         embedding = self._fit(X)
         self.embedding_ = embedding
         return self.embedding_
 
     @_fit_context(
         # TSNE.metric is not validated yet
```

### Comparing `scikit-learn-1.4.2/sklearn/manifold/_utils.pyx` & `scikit_learn-1.5.0rc1/sklearn/manifold/_utils.pyx`

 * *Files 9% similar despite different names*

```diff
@@ -1,25 +1,22 @@
-from libc cimport math
 import numpy as np
-cimport numpy as cnp
-
-cnp.import_array()
 
+from libc cimport math
+from libc.math cimport INFINITY
 
-cdef extern from "numpy/npy_math.h":
-    float NPY_INFINITY
+from ..utils._typedefs cimport float32_t, float64_t
 
 
 cdef float EPSILON_DBL = 1e-8
 cdef float PERPLEXITY_TOLERANCE = 1e-5
 
 
 # TODO: have this function support float32 and float64 and preserve inputs' dtypes.
 def _binary_search_perplexity(
-        const cnp.float32_t[:, :] sqdistances,
+        const float32_t[:, :] sqdistances,
         float desired_perplexity,
         int verbose):
     """Binary search for sigmas of conditional Gaussians.
 
     This approximation reduces the computational complexity from O(N^2) to
     O(uN).
 
@@ -61,20 +58,20 @@
     cdef double entropy
     cdef double sum_Pi
     cdef double sum_disti_Pi
     cdef long i, j, l
 
     # This array is later used as a 32bit array. It has multiple intermediate
     # floating point additions that benefit from the extra precision
-    cdef cnp.float64_t[:, :] P = np.zeros(
+    cdef float64_t[:, :] P = np.zeros(
         (n_samples, n_neighbors), dtype=np.float64)
 
     for i in range(n_samples):
-        beta_min = -NPY_INFINITY
-        beta_max = NPY_INFINITY
+        beta_min = -INFINITY
+        beta_max = INFINITY
         beta = 1.0
 
         # Binary search of precision for i-th conditional distribution
         for l in range(n_steps):
             # Compute current entropy and corresponding probabilities
             # computed just over the nearest neighbors or over all data
             # if we're not using neighbors
@@ -96,21 +93,21 @@
             entropy_diff = entropy - desired_entropy
 
             if math.fabs(entropy_diff) <= PERPLEXITY_TOLERANCE:
                 break
 
             if entropy_diff > 0.0:
                 beta_min = beta
-                if beta_max == NPY_INFINITY:
+                if beta_max == INFINITY:
                     beta *= 2.0
                 else:
                     beta = (beta + beta_max) / 2.0
             else:
                 beta_max = beta
-                if beta_min == -NPY_INFINITY:
+                if beta_min == -INFINITY:
                     beta /= 2.0
                 else:
                     beta = (beta + beta_min) / 2.0
 
         beta_sum += beta
 
         if verbose and ((i + 1) % 1000 == 0 or i + 1 == n_samples):
```

### Comparing `scikit-learn-1.4.2/sklearn/manifold/tests/test_isomap.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_isomap.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/tests/test_locally_linear.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_locally_linear.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/tests/test_mds.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_mds.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/tests/test_spectral_embedding.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_spectral_embedding.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/manifold/tests/test_t_sne.py` & `scikit_learn-1.5.0rc1/sklearn/manifold/tests/test_t_sne.py`

 * *Files 9% similar despite different names*

```diff
@@ -69,15 +69,15 @@
     old_stdout = sys.stdout
     sys.stdout = StringIO()
     try:
         _, error, it = _gradient_descent(
             ObjectiveSmallGradient(),
             np.zeros(1),
             0,
-            n_iter=100,
+            max_iter=100,
             n_iter_without_progress=100,
             momentum=0.0,
             learning_rate=0.0,
             min_gain=0.0,
             min_grad_norm=1e-5,
             verbose=2,
         )
@@ -93,15 +93,15 @@
     old_stdout = sys.stdout
     sys.stdout = StringIO()
     try:
         _, error, it = _gradient_descent(
             flat_function,
             np.zeros(1),
             0,
-            n_iter=100,
+            max_iter=100,
             n_iter_without_progress=10,
             momentum=0.0,
             learning_rate=0.0,
             min_gain=0.0,
             min_grad_norm=0.0,
             verbose=2,
         )
@@ -117,15 +117,15 @@
     old_stdout = sys.stdout
     sys.stdout = StringIO()
     try:
         _, error, it = _gradient_descent(
             ObjectiveSmallGradient(),
             np.zeros(1),
             0,
-            n_iter=11,
+            max_iter=11,
             n_iter_without_progress=100,
             momentum=0.0,
             learning_rate=0.0,
             min_gain=0.0,
             min_grad_norm=0.0,
             verbose=2,
         )
@@ -304,34 +304,34 @@
     n_components = 2
     X = random_state.randn(50, n_components).astype(np.float32)
     tsne = TSNE(
         n_components=n_components,
         init=init,
         random_state=0,
         method=method,
-        n_iter=700,
+        max_iter=700,
         learning_rate="auto",
     )
     X_embedded = tsne.fit_transform(X)
     t = trustworthiness(X, X_embedded, n_neighbors=1)
     assert t > 0.85
 
 
 def test_optimization_minimizes_kl_divergence():
     """t-SNE should give a lower KL divergence with more iterations."""
     random_state = check_random_state(0)
     X, _ = make_blobs(n_features=3, random_state=random_state)
     kl_divergences = []
-    for n_iter in [250, 300, 350]:
+    for max_iter in [250, 300, 350]:
         tsne = TSNE(
             n_components=2,
             init="random",
             perplexity=10,
             learning_rate=100.0,
-            n_iter=n_iter,
+            max_iter=max_iter,
             random_state=0,
         )
         tsne.fit_transform(X)
         kl_divergences.append(tsne.kl_divergence_)
     assert kl_divergences[1] <= kl_divergences[0]
     assert kl_divergences[2] <= kl_divergences[1]
 
@@ -349,15 +349,15 @@
     tsne = TSNE(
         n_components=2,
         init="random",
         perplexity=10,
         learning_rate=100.0,
         random_state=0,
         method=method,
-        n_iter=750,
+        max_iter=750,
     )
     X_embedded = tsne.fit_transform(X_csr)
     assert_allclose(trustworthiness(X_csr, X_embedded, n_neighbors=1), 1.0, rtol=1.1e-1)
 
 
 def test_preserve_trustworthiness_approximately_with_precomputed_distances():
     # Nearest neighbors should be preserved approximately.
@@ -369,15 +369,15 @@
             n_components=2,
             perplexity=2,
             learning_rate=100.0,
             early_exaggeration=2.0,
             metric="precomputed",
             random_state=i,
             verbose=0,
-            n_iter=500,
+            max_iter=500,
             init="random",
         )
         X_embedded = tsne.fit_transform(D)
         t = trustworthiness(D, X_embedded, n_neighbors=1, metric="precomputed")
         assert t > 0.95
 
 
@@ -529,53 +529,53 @@
             n_components=n_components,
             perplexity=1,
             learning_rate=100.0,
             init="pca",
             random_state=0,
             method=method,
             early_exaggeration=1.0,
-            n_iter=250,
+            max_iter=250,
         )
         X_embedded1 = tsne.fit_transform(X)
         tsne = TSNE(
             n_components=n_components,
             perplexity=1,
             learning_rate=100.0,
             init="pca",
             random_state=0,
             method=method,
             early_exaggeration=10.0,
-            n_iter=250,
+            max_iter=250,
         )
         X_embedded2 = tsne.fit_transform(X)
 
         assert not np.allclose(X_embedded1, X_embedded2)
 
 
-def test_n_iter_used():
-    # check that the ``n_iter`` parameter has an effect
+def test_max_iter_used():
+    # check that the ``max_iter`` parameter has an effect
     random_state = check_random_state(0)
     n_components = 2
     methods = ["exact", "barnes_hut"]
     X = random_state.randn(25, n_components).astype(np.float32)
     for method in methods:
-        for n_iter in [251, 500]:
+        for max_iter in [251, 500]:
             tsne = TSNE(
                 n_components=n_components,
                 perplexity=1,
                 learning_rate=0.5,
                 init="random",
                 random_state=0,
                 method=method,
                 early_exaggeration=1.0,
-                n_iter=n_iter,
+                max_iter=max_iter,
             )
             tsne.fit_transform(X)
 
-            assert tsne.n_iter_ == n_iter - 1
+            assert tsne.n_iter_ == max_iter - 1
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_answer_gradient_two_points(csr_container):
     # Test the tree with only a single set of children.
     #
     # These tests & answers have been checked against the reference
@@ -728,40 +728,40 @@
     tsne = TSNE(
         n_components=2,
         perplexity=2,
         learning_rate=100.0,
         random_state=0,
         method=method,
         verbose=0,
-        n_iter=300,
+        max_iter=300,
         init="random",
     )
     X_embedded = tsne.fit_transform(X)
     effective_type = X_embedded.dtype
 
     # tsne cython code is only single precision, so the output will
     # always be single precision, irrespectively of the input dtype
     assert effective_type == np.float32
 
 
 @pytest.mark.parametrize("method", ["barnes_hut", "exact"])
 def test_kl_divergence_not_nan(method):
     # Ensure kl_divergence_ is computed at last iteration
-    # even though n_iter % n_iter_check != 0, i.e. 1003 % 50 != 0
+    # even though max_iter % n_iter_check != 0, i.e. 1003 % 50 != 0
     random_state = check_random_state(0)
 
     X = random_state.randn(50, 2)
     tsne = TSNE(
         n_components=2,
         perplexity=2,
         learning_rate=100.0,
         random_state=0,
         method=method,
         verbose=0,
-        n_iter=503,
+        max_iter=503,
         init="random",
     )
     tsne.fit_transform(X)
 
     assert not np.isnan(tsne.kl_divergence_)
 
 
@@ -815,19 +815,19 @@
     for method in ["barnes_hut", "exact"]:
         tsne = TSNE(
             n_iter_without_progress=-1,
             verbose=2,
             learning_rate=1e8,
             random_state=0,
             method=method,
-            n_iter=351,
+            max_iter=351,
             init="random",
         )
         tsne._N_ITER_CHECK = 1
-        tsne._EXPLORATION_N_ITER = 0
+        tsne._EXPLORATION_MAX_ITER = 0
 
         old_stdout = sys.stdout
         sys.stdout = StringIO()
         try:
             tsne.fit_transform(X)
         finally:
             out = sys.stdout.getvalue()
@@ -882,15 +882,19 @@
 
 
 def test_accessible_kl_divergence():
     # Ensures that the accessible kl_divergence matches the computed value
     random_state = check_random_state(0)
     X = random_state.randn(50, 2)
     tsne = TSNE(
-        n_iter_without_progress=2, verbose=2, random_state=0, method="exact", n_iter=500
+        n_iter_without_progress=2,
+        verbose=2,
+        random_state=0,
+        method="exact",
+        max_iter=500,
     )
 
     old_stdout = sys.stdout
     sys.stdout = StringIO()
     try:
         tsne.fit_transform(X)
     finally:
@@ -919,22 +923,22 @@
     Also, t-SNE is not assured to converge to the right solution because bad
     initialization can lead to convergence to bad local minimum (the
     optimization problem is non-convex). To avoid breaking the test too often,
     we re-run t-SNE from the final point when the convergence is not good
     enough.
     """
     seeds = range(3)
-    n_iter = 500
+    max_iter = 500
     for seed in seeds:
         tsne = TSNE(
             n_components=2,
             init="random",
             random_state=seed,
             perplexity=50,
-            n_iter=n_iter,
+            max_iter=max_iter,
             method=method,
             learning_rate="auto",
         )
         Y = tsne.fit_transform(X_2d_grid)
 
         try_name = "{}_{}".format(method, seed)
         try:
@@ -967,32 +971,32 @@
 def test_bh_match_exact():
     # check that the ``barnes_hut`` method match the exact one when
     # ``angle = 0`` and ``perplexity > n_samples / 3``
     random_state = check_random_state(0)
     n_features = 10
     X = random_state.randn(30, n_features).astype(np.float32)
     X_embeddeds = {}
-    n_iter = {}
+    max_iter = {}
     for method in ["exact", "barnes_hut"]:
         tsne = TSNE(
             n_components=2,
             method=method,
             learning_rate=1.0,
             init="random",
             random_state=0,
-            n_iter=251,
+            max_iter=251,
             perplexity=29.5,
             angle=0,
         )
         # Kill the early_exaggeration
-        tsne._EXPLORATION_N_ITER = 0
+        tsne._EXPLORATION_MAX_ITER = 0
         X_embeddeds[method] = tsne.fit_transform(X)
-        n_iter[method] = tsne.n_iter_
+        max_iter[method] = tsne.n_iter_
 
-    assert n_iter["exact"] == n_iter["barnes_hut"]
+    assert max_iter["exact"] == max_iter["barnes_hut"]
     assert_allclose(X_embeddeds["exact"], X_embeddeds["barnes_hut"], rtol=1e-4)
 
 
 def test_gradient_bh_multithread_match_sequential():
     # check that the bh gradient with different num_threads gives the same
     # results
 
@@ -1073,24 +1077,24 @@
     n_components_embedding = 2
     X = random_state.randn(50, n_components_original).astype(np.float32)
     X_transformed_tsne = TSNE(
         metric=metric,
         method=method,
         n_components=n_components_embedding,
         random_state=0,
-        n_iter=300,
+        max_iter=300,
         init="random",
         learning_rate="auto",
     ).fit_transform(X)
     X_transformed_tsne_precomputed = TSNE(
         metric="precomputed",
         method=method,
         n_components=n_components_embedding,
         random_state=0,
-        n_iter=300,
+        max_iter=300,
         init="random",
         learning_rate="auto",
     ).fit_transform(dist_func(X))
     assert_array_equal(X_transformed_tsne, X_transformed_tsne_precomputed)
 
 
 @pytest.mark.parametrize("method", ["exact", "barnes_hut"])
@@ -1126,15 +1130,15 @@
 def test_tsne_with_mahalanobis_distance():
     """Make sure that method_parameters works with mahalanobis distance."""
     random_state = check_random_state(0)
     n_samples, n_features = 300, 10
     X = random_state.randn(n_samples, n_features)
     default_params = {
         "perplexity": 40,
-        "n_iter": 250,
+        "max_iter": 250,
         "learning_rate": "auto",
         "init": "random",
         "n_components": 3,
         "random_state": 0,
     }
 
     tsne = TSNE(metric="mahalanobis", **default_params)
@@ -1175,7 +1179,29 @@
 
     Non-regression test for gh-25365.
     """
     pytest.importorskip("pandas")
     with config_context(transform_output="pandas"):
         arr = np.arange(35 * 4).reshape(35, 4)
         TSNE(n_components=2).fit_transform(arr)
+
+
+# TODO(1.7): remove
+def test_tnse_n_iter_deprecated():
+    """Check `n_iter` parameter deprecated."""
+    random_state = check_random_state(0)
+    X = random_state.randn(40, 100)
+    tsne = TSNE(n_iter=250)
+    msg = "'n_iter' was renamed to 'max_iter'"
+    with pytest.warns(FutureWarning, match=msg):
+        tsne.fit_transform(X)
+
+
+# TODO(1.7): remove
+def test_tnse_n_iter_max_iter_both_set():
+    """Check error raised when `n_iter` and `max_iter` both set."""
+    random_state = check_random_state(0)
+    X = random_state.randn(40, 100)
+    tsne = TSNE(n_iter=250, max_iter=500)
+    msg = "Both 'n_iter' and 'max_iter' attributes were set"
+    with pytest.raises(ValueError, match=msg):
+        tsne.fit_transform(X)
```

### Comparing `scikit-learn-1.4.2/sklearn/meson.build` & `scikit_learn-1.5.0rc1/sklearn/meson.build`

 * *Files 14% similar despite different names*

```diff
@@ -56,14 +56,35 @@
 
 inc_np = include_directories(incdir_numpy)
 np_dep = declare_dependency(include_directories: inc_np)
 
 openmp_dep = dependency('OpenMP', language: 'c', required: false)
 
 if not openmp_dep.found()
+  warn_about_missing_openmp = true
+  # On Apple Clang avoid a misleading warning if compiler variables are set.
+  # See https://github.com/scikit-learn/scikit-learn/issues/28710 for more
+  # details. This may be removed if the OpenMP detection on Apple Clang improves,
+  # see https://github.com/mesonbuild/meson/issues/7435#issuecomment-2047585466.
+  if host_machine.system() == 'darwin' and cc.get_id() == 'clang'
+    compiler_env_vars_with_openmp = run_command(py,
+      [
+        '-c',
+        '''
+import os
+
+compiler_env_vars_to_check = ["CPPFLAGS", "CFLAGS", "CXXFLAGS"]
+
+compiler_env_vars_with_openmp = [
+    var for var in compiler_env_vars_to_check if "-fopenmp" in os.getenv(var, "")]
+print(compiler_env_vars_with_openmp)
+'''], check: true).stdout().strip()
+      warn_about_missing_openmp = compiler_env_vars_with_openmp == '[]'
+  endif
+  if warn_about_missing_openmp
     warning(
 '''
                 ***********
                 * WARNING *
                 ***********
 
 It seems that scikit-learn cannot be built with OpenMP.
@@ -80,14 +101,21 @@
 - The build will continue with OpenMP-based parallelism
   disabled. Note however that some estimators will run in
   sequential mode instead of leveraging thread-based
   parallelism.
 
                     ***
 ''')
+  else
+    warning(
+'''It looks like compiler environment variables were set to enable OpenMP support.
+Check the output of "import sklearn; sklearn.show_versions()" after the build
+to make sure that scikit-learn was actually built with OpenMP support.
+''')
+  endif
 endif
 
 # For now, we keep supporting SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES variable
 # (see how it is done in sklearn/_build_utils/__init__.py when building with
 # setuptools). Accessing environment variables in meson.build is discouraged,
 # so once we drop setuptools this functionality should be behind a meson option
 # or buildtype
@@ -123,29 +151,30 @@
     output: '_built_with_meson.py',
     command: [
         py, '-c', 'with open("sklearn/_built_with_meson.py", "w") as f: f.write("")'
     ],
     install: true,
     install_dir: py.get_install_dir() / 'sklearn'
 )
-# endif
 
 extensions = ['_isotonic']
 
 py.extension_module(
   '_isotonic',
   '_isotonic.pyx',
   cython_args: cython_args,
   install: true,
   subdir: 'sklearn',
 )
 
 # Need for Cython cimports across subpackages to work, i.e. avoid errors like
 # relative cimport from non-package directory is not allowed
-fs.copyfile('__init__.py')
+sklearn_root_cython_tree = [
+  fs.copyfile('__init__.py')
+]
 
 sklearn_dir = py.get_install_dir() / 'sklearn'
 
 # Subpackages are mostly in alphabetical order except to handle Cython
 # dependencies across subpackages
 subdir('__check_build')
 subdir('_loss')
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 """
 The :mod:`sklearn.metrics` module includes score functions, performance metrics
 and pairwise metrics and distance computations.
 """
 
-
 from . import cluster
 from ._classification import (
     accuracy_score,
     balanced_accuracy_score,
     brier_score_loss,
     class_likelihood_ratios,
     classification_report,
     cohen_kappa_score,
     confusion_matrix,
+    d2_log_loss_score,
     f1_score,
     fbeta_score,
     hamming_loss,
     hinge_loss,
     jaccard_score,
     log_loss,
     matthews_corrcoef,
@@ -110,14 +110,15 @@
     "completeness_score",
     "ConfusionMatrixDisplay",
     "confusion_matrix",
     "consensus_score",
     "coverage_error",
     "d2_tweedie_score",
     "d2_absolute_error_score",
+    "d2_log_loss_score",
     "d2_pinball_score",
     "dcg_score",
     "davies_bouldin_score",
     "DetCurveDisplay",
     "det_curve",
     "DistanceMetric",
     "euclidean_distances",
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_base.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 Common code for all metrics.
 
 """
+
 # Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #          Mathieu Blondel <mathieu@mblondel.org>
 #          Olivier Grisel <olivier.grisel@ensta.org>
 #          Arnaud Joly <a.joly@ulg.ac.be>
 #          Jochen Wersdorfer <jochen@wersdoerfer.de>
 #          Lars Buitinck
 #          Joel Nothman <joel.nothman@gmail.com>
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_classification.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_classification.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,20 +34,34 @@
 from ..preprocessing import LabelBinarizer, LabelEncoder
 from ..utils import (
     assert_all_finite,
     check_array,
     check_consistent_length,
     column_or_1d,
 )
-from ..utils._array_api import _union1d, _weighted_sum, get_namespace
-from ..utils._param_validation import Interval, Options, StrOptions, validate_params
+from ..utils._array_api import (
+    _average,
+    _union1d,
+    get_namespace,
+)
+from ..utils._param_validation import (
+    Hidden,
+    Interval,
+    Options,
+    StrOptions,
+    validate_params,
+)
 from ..utils.extmath import _nanaverage
 from ..utils.multiclass import type_of_target, unique_labels
 from ..utils.sparsefuncs import count_nonzero
-from ..utils.validation import _check_pos_label_consistency, _num_samples
+from ..utils.validation import (
+    _check_pos_label_consistency,
+    _check_sample_weight,
+    _num_samples,
+)
 
 
 def _check_zero_division(zero_division):
     if isinstance(zero_division, str) and zero_division == "warn":
         return np.float64(0.0)
     elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:
         return np.float64(zero_division)
@@ -165,15 +179,15 @@
         Otherwise, return the fraction of correctly classified samples.
 
     sample_weight : array-like of shape (n_samples,), default=None
         Sample weights.
 
     Returns
     -------
-    score : float
+    score : float or int
         If ``normalize == True``, return the fraction of correctly
         classified samples (float), else returns the number of correctly
         classified samples (int).
 
         The best performance is 1 with ``normalize == True`` and the number
         of samples with ``normalize == False``.
 
@@ -183,19 +197,14 @@
         imbalanced datasets.
     jaccard_score : Compute the Jaccard similarity coefficient score.
     hamming_loss : Compute the average Hamming loss or Hamming distance between
         two sets of samples.
     zero_one_loss : Compute the Zero-one classification loss. By default, the
         function will return the percentage of imperfectly predicted subsets.
 
-    Notes
-    -----
-    In binary classification, this function is equal to the `jaccard_score`
-    function.
-
     Examples
     --------
     >>> from sklearn.metrics import accuracy_score
     >>> y_pred = [0, 2, 1, 3]
     >>> y_true = [0, 1, 2, 3]
     >>> accuracy_score(y_true, y_pred)
     0.5
@@ -214,15 +223,15 @@
     check_consistent_length(y_true, y_pred, sample_weight)
     if y_type.startswith("multilabel"):
         differing_labels = count_nonzero(y_true - y_pred, axis=1)
         score = differing_labels == 0
     else:
         score = y_true == y_pred
 
-    return _weighted_sum(score, sample_weight, normalize)
+    return float(_average(score, weights=sample_weight, normalize=normalize))
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
         "y_pred": ["array-like"],
         "labels": ["array-like", None],
@@ -574,16 +583,15 @@
                     "multilabel targets. "
                     "Got %d > %d" % (np.max(labels), np.max(present_labels))
                 )
             if np.min(labels) < 0:
                 raise ValueError(
                     "All labels must be in [0, n labels) for "
                     "multilabel targets. "
-                    "Got %d < 0"
-                    % np.min(labels)
+                    "Got %d < 0" % np.min(labels)
                 )
 
         if n_labels is not None:
             y_true = y_true[:, labels[:n_labels]]
             y_pred = y_pred[:, labels[:n_labels]]
 
         # calculate weighted counts
@@ -2608,15 +2616,15 @@
         labels_given = False
     else:
         labels = np.asarray(labels)
         labels_given = True
 
     # labelled micro average
     micro_is_accuracy = (y_type == "multiclass" or y_type == "binary") and (
-        not labels_given or (set(labels) == set(unique_labels(y_true, y_pred)))
+        not labels_given or (set(labels) >= set(unique_labels(y_true, y_pred)))
     )
 
     if target_names is not None and len(labels) != len(target_names):
         if labels_given:
             warnings.warn(
                 "labels size, {0}, does not match size of target_names, {1}".format(
                     len(labels), len(target_names)
@@ -2799,33 +2807,30 @@
         weight_average = np.mean(sample_weight)
 
     if y_type.startswith("multilabel"):
         n_differences = count_nonzero(y_true - y_pred, sample_weight=sample_weight)
         return n_differences / (y_true.shape[0] * y_true.shape[1] * weight_average)
 
     elif y_type in ["binary", "multiclass"]:
-        return _weighted_sum(y_true != y_pred, sample_weight, normalize=True)
+        return float(_average(y_true != y_pred, weights=sample_weight, normalize=True))
     else:
         raise ValueError("{0} is not supported".format(y_type))
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
         "y_pred": ["array-like"],
-        "eps": [StrOptions({"auto"}), Interval(Real, 0, 1, closed="both")],
         "normalize": ["boolean"],
         "sample_weight": ["array-like", None],
         "labels": ["array-like", None],
     },
     prefer_skip_nested_validation=True,
 )
-def log_loss(
-    y_true, y_pred, *, eps="auto", normalize=True, sample_weight=None, labels=None
-):
+def log_loss(y_true, y_pred, *, normalize=True, sample_weight=None, labels=None):
     r"""Log loss, aka logistic loss or cross-entropy loss.
 
     This is the loss function used in (multinomial) logistic regression
     and extensions of it such as neural networks, defined as the negative
     log-likelihood of a logistic model that returns ``y_pred`` probabilities
     for its training data ``y_true``.
     The log loss is only defined for two or more labels.
@@ -2847,27 +2852,16 @@
         Predicted probabilities, as returned by a classifier's
         predict_proba method. If ``y_pred.shape = (n_samples,)``
         the probabilities provided are assumed to be that of the
         positive class. The labels in ``y_pred`` are assumed to be
         ordered alphabetically, as done by
         :class:`~sklearn.preprocessing.LabelBinarizer`.
 
-    eps : float or "auto", default="auto"
-        Log loss is undefined for p=0 or p=1, so probabilities are
-        clipped to `max(eps, min(1 - eps, p))`. The default will depend on the
-        data type of `y_pred` and is set to `np.finfo(y_pred.dtype).eps`.
-
-        .. versionadded:: 1.2
-
-        .. versionchanged:: 1.2
-           The default value changed from `1e-15` to `"auto"` that is
-           equivalent to `np.finfo(y_pred.dtype).eps`.
-
-        .. deprecated:: 1.3
-           `eps` is deprecated in 1.3 and will be removed in 1.5.
+        `y_pred` values are clipped to `[eps, 1-eps]` where `eps` is the machine
+        precision for `y_pred`'s dtype.
 
     normalize : bool, default=True
         If true, return the mean loss per sample.
         Otherwise, return the sum of the per-sample losses.
 
     sample_weight : array-like of shape (n_samples,), default=None
         Sample weights.
@@ -2899,26 +2893,14 @@
     >>> log_loss(["spam", "ham", "ham", "spam"],
     ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])
     0.21616...
     """
     y_pred = check_array(
         y_pred, ensure_2d=False, dtype=[np.float64, np.float32, np.float16]
     )
-    if eps == "auto":
-        eps = np.finfo(y_pred.dtype).eps
-    else:
-        # TODO: Remove user defined eps in 1.5
-        warnings.warn(
-            (
-                "Setting the eps parameter is deprecated and will "
-                "be removed in 1.5. Instead eps will always have"
-                "a default value of `np.finfo(y_pred.dtype).eps`."
-            ),
-            FutureWarning,
-        )
 
     check_consistent_length(y_pred, y_true, sample_weight)
     lb = LabelBinarizer()
 
     if labels is not None:
         lb.fit(labels)
     else:
@@ -2941,24 +2923,34 @@
     transformed_labels = lb.transform(y_true)
 
     if transformed_labels.shape[1] == 1:
         transformed_labels = np.append(
             1 - transformed_labels, transformed_labels, axis=1
         )
 
-    # Clipping
-    y_pred = np.clip(y_pred, eps, 1 - eps)
-
     # If y_pred is of single dimension, assume y_true to be binary
     # and then check.
     if y_pred.ndim == 1:
         y_pred = y_pred[:, np.newaxis]
     if y_pred.shape[1] == 1:
         y_pred = np.append(1 - y_pred, y_pred, axis=1)
 
+    eps = np.finfo(y_pred.dtype).eps
+
+    # Make sure y_pred is normalized
+    y_pred_sum = y_pred.sum(axis=1)
+    if not np.allclose(y_pred_sum, 1, rtol=np.sqrt(eps)):
+        warnings.warn(
+            "The y_pred values do not sum to one. Make sure to pass probabilities.",
+            UserWarning,
+        )
+
+    # Clipping
+    y_pred = np.clip(y_pred, eps, 1 - eps)
+
     # Check if dimensions are consistent.
     transformed_labels = check_array(transformed_labels)
     if len(lb.classes_) != y_pred.shape[1]:
         if labels is None:
             raise ValueError(
                 "y_true and y_pred contain different number of "
                 "classes {0}, {1}. Please provide the true "
@@ -2971,28 +2963,17 @@
         else:
             raise ValueError(
                 "The number of classes in labels is different "
                 "from that in y_pred. Classes found in "
                 "labels: {0}".format(lb.classes_)
             )
 
-    # Renormalize
-    y_pred_sum = y_pred.sum(axis=1)
-    if not np.isclose(y_pred_sum, 1, rtol=1e-15, atol=5 * eps).all():
-        warnings.warn(
-            (
-                "The y_pred values do not sum to one. Starting from 1.5 this"
-                "will result in an error."
-            ),
-            UserWarning,
-        )
-    y_pred = y_pred / y_pred_sum[:, np.newaxis]
     loss = -xlogy(transformed_labels, y_pred).sum(axis=1)
 
-    return _weighted_sum(loss, sample_weight, normalize)
+    return float(_average(loss, weights=sample_weight, normalize=normalize))
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
         "pred_decision": ["array-like"],
         "labels": ["array-like", None],
@@ -3053,32 +3034,32 @@
 
     Examples
     --------
     >>> from sklearn import svm
     >>> from sklearn.metrics import hinge_loss
     >>> X = [[0], [1]]
     >>> y = [-1, 1]
-    >>> est = svm.LinearSVC(dual="auto", random_state=0)
+    >>> est = svm.LinearSVC(random_state=0)
     >>> est.fit(X, y)
-    LinearSVC(dual='auto', random_state=0)
+    LinearSVC(random_state=0)
     >>> pred_decision = est.decision_function([[-2], [3], [0.5]])
     >>> pred_decision
     array([-2.18...,  2.36...,  0.09...])
     >>> hinge_loss([-1, 1, 1], pred_decision)
     0.30...
 
     In the multiclass case:
 
     >>> import numpy as np
     >>> X = np.array([[0], [1], [2], [3]])
     >>> Y = np.array([0, 1, 2, 3])
     >>> labels = np.array([0, 1, 2, 3])
-    >>> est = svm.LinearSVC(dual="auto")
+    >>> est = svm.LinearSVC()
     >>> est.fit(X, Y)
-    LinearSVC(dual='auto')
+    LinearSVC()
     >>> pred_decision = est.decision_function([[-1], [2], [3]])
     >>> y_true = [0, 2, 3]
     >>> hinge_loss(y_true, pred_decision, labels=labels)
     0.56...
     """
     check_consistent_length(y_true, pred_decision, sample_weight)
     pred_decision = check_array(pred_decision, ensure_2d=False)
@@ -3142,21 +3123,24 @@
     np.clip(losses, 0, None, out=losses)
     return np.average(losses, weights=sample_weight)
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
-        "y_prob": ["array-like"],
+        "y_proba": ["array-like", Hidden(None)],
         "sample_weight": ["array-like", None],
         "pos_label": [Real, str, "boolean", None],
+        "y_prob": ["array-like", Hidden(StrOptions({"deprecated"}))],
     },
     prefer_skip_nested_validation=True,
 )
-def brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None):
+def brier_score_loss(
+    y_true, y_proba=None, *, sample_weight=None, pos_label=None, y_prob="deprecated"
+):
     """Compute the Brier score loss.
 
     The smaller the Brier score loss, the better, hence the naming with "loss".
     The Brier score measures the mean squared difference between the predicted
     probability and the actual outcome. The Brier score always
     takes on a value between zero and one, since this is the largest
     possible difference between a predicted probability (which must be
@@ -3176,15 +3160,15 @@
     Read more in the :ref:`User Guide <brier_score_loss>`.
 
     Parameters
     ----------
     y_true : array-like of shape (n_samples,)
         True targets.
 
-    y_prob : array-like of shape (n_samples,)
+    y_proba : array-like of shape (n_samples,)
         Probabilities of the positive class.
 
     sample_weight : array-like of shape (n_samples,), default=None
         Sample weights.
 
     pos_label : int, float, bool or str, default=None
         Label of the positive class. `pos_label` will be inferred in the
@@ -3192,14 +3176,21 @@
 
         * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;
         * else if `y_true` contains string, an error will be raised and
           `pos_label` should be explicitly specified;
         * otherwise, `pos_label` defaults to the greater label,
           i.e. `np.unique(y_true)[-1]`.
 
+    y_prob : array-like of shape (n_samples,)
+        Probabilities of the positive class.
+
+        .. deprecated:: 1.5
+            `y_prob` is deprecated and will be removed in 1.7. Use
+            `y_proba` instead.
+
     Returns
     -------
     score : float
         Brier score loss.
 
     References
     ----------
@@ -3218,37 +3209,148 @@
     >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)
     0.037...
     >>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
     0.037...
     >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)
     0.0
     """
+    # TODO(1.7): remove in 1.7 and reset y_proba to be required
+    # Note: validate params will raise an error if y_prob is not array-like,
+    # or "deprecated"
+    if y_proba is not None and not isinstance(y_prob, str):
+        raise ValueError(
+            "`y_prob` and `y_proba` cannot be both specified. Please use `y_proba` only"
+            " as `y_prob` is deprecated in v1.5 and will be removed in v1.7."
+        )
+    if y_proba is None:
+        warnings.warn(
+            (
+                "y_prob was deprecated in version 1.5 and will be removed in 1.7."
+                "Please use ``y_proba`` instead."
+            ),
+            FutureWarning,
+        )
+        y_proba = y_prob
+
     y_true = column_or_1d(y_true)
-    y_prob = column_or_1d(y_prob)
+    y_proba = column_or_1d(y_proba)
     assert_all_finite(y_true)
-    assert_all_finite(y_prob)
-    check_consistent_length(y_true, y_prob, sample_weight)
+    assert_all_finite(y_proba)
+    check_consistent_length(y_true, y_proba, sample_weight)
 
     y_type = type_of_target(y_true, input_name="y_true")
     if y_type != "binary":
         raise ValueError(
             "Only binary classification is supported. The type of the target "
             f"is {y_type}."
         )
 
-    if y_prob.max() > 1:
-        raise ValueError("y_prob contains values greater than 1.")
-    if y_prob.min() < 0:
-        raise ValueError("y_prob contains values less than 0.")
+    if y_proba.max() > 1:
+        raise ValueError("y_proba contains values greater than 1.")
+    if y_proba.min() < 0:
+        raise ValueError("y_proba contains values less than 0.")
 
     try:
         pos_label = _check_pos_label_consistency(pos_label, y_true)
     except ValueError:
         classes = np.unique(y_true)
         if classes.dtype.kind not in ("O", "U", "S"):
             # for backward compatibility, if classes are not string then
             # `pos_label` will correspond to the greater label
             pos_label = classes[-1]
         else:
             raise
     y_true = np.array(y_true == pos_label, int)
-    return np.average((y_true - y_prob) ** 2, weights=sample_weight)
+    return np.average((y_true - y_proba) ** 2, weights=sample_weight)
+
+
+@validate_params(
+    {
+        "y_true": ["array-like"],
+        "y_pred": ["array-like"],
+        "sample_weight": ["array-like", None],
+        "labels": ["array-like", None],
+    },
+    prefer_skip_nested_validation=True,
+)
+def d2_log_loss_score(y_true, y_pred, *, sample_weight=None, labels=None):
+    """
+    :math:`D^2` score function, fraction of log loss explained.
+
+    Best possible score is 1.0 and it can be negative (because the model can be
+    arbitrarily worse). A model that always uses the empirical mean of `y_true` as
+    constant prediction, disregarding the input features, gets a D^2 score of 0.0.
+
+    Read more in the :ref:`User Guide <d2_score>`.
+
+    .. versionadded:: 1.5
+
+    Parameters
+    ----------
+    y_true : array-like or label indicator matrix
+        The actuals labels for the n_samples samples.
+
+    y_pred : array-like of shape (n_samples, n_classes) or (n_samples,)
+        Predicted probabilities, as returned by a classifier's
+        predict_proba method. If ``y_pred.shape = (n_samples,)``
+        the probabilities provided are assumed to be that of the
+        positive class. The labels in ``y_pred`` are assumed to be
+        ordered alphabetically, as done by
+        :class:`~sklearn.preprocessing.LabelBinarizer`.
+
+    sample_weight : array-like of shape (n_samples,), default=None
+        Sample weights.
+
+    labels : array-like, default=None
+        If not provided, labels will be inferred from y_true. If ``labels``
+        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are
+        assumed to be binary and are inferred from ``y_true``.
+
+    Returns
+    -------
+    d2 : float or ndarray of floats
+        The D^2 score.
+
+    Notes
+    -----
+    This is not a symmetric function.
+
+    Like R^2, D^2 score may be negative (it need not actually be the square of
+    a quantity D).
+
+    This metric is not well-defined for a single sample and will return a NaN
+    value if n_samples is less than two.
+    """
+    y_pred = check_array(y_pred, ensure_2d=False, dtype="numeric")
+    check_consistent_length(y_pred, y_true, sample_weight)
+    if _num_samples(y_pred) < 2:
+        msg = "D^2 score is not well-defined with less than two samples."
+        warnings.warn(msg, UndefinedMetricWarning)
+        return float("nan")
+
+    # log loss of the fitted model
+    numerator = log_loss(
+        y_true=y_true,
+        y_pred=y_pred,
+        normalize=False,
+        sample_weight=sample_weight,
+        labels=labels,
+    )
+
+    # Proportion of labels in the dataset
+    weights = _check_sample_weight(sample_weight, y_true)
+
+    _, y_value_indices = np.unique(y_true, return_inverse=True)
+    counts = np.bincount(y_value_indices, weights=weights)
+    y_prob = counts / weights.sum()
+    y_pred_null = np.tile(y_prob, (len(y_true), 1))
+
+    # log loss of the null model
+    denominator = log_loss(
+        y_true=y_true,
+        y_pred=y_pred_null,
+        normalize=False,
+        sample_weight=sample_weight,
+        labels=labels,
+    )
+
+    return 1 - (numerator / denominator)
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_dist_metrics.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_dist_metrics.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_dist_metrics.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_dist_metrics.pyx.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx.tp`

 * *Files 1% similar despite different names*

```diff
@@ -8,16 +8,17 @@
 from ...utils._typedefs cimport intp_t, float64_t
 
 import numpy as np
 import warnings
 
 from numbers import Integral
 from scipy.sparse import issparse
-from ...utils import check_array, check_scalar, _in_unstable_openblas_configuration
-from ...utils.fixes import threadpool_limits
+from ...utils import check_array, check_scalar
+from ...utils.fixes import _in_unstable_openblas_configuration
+from ... import _threadpool_controller
 
 {{for name_suffix in ['64', '32']}}
 
 from ._base cimport (
     BaseDistancesReduction{{name_suffix}},
     _sqeuclidean_row_norms{{name_suffix}},
 )
@@ -51,49 +52,55 @@
         This allows decoupling the API entirely from the implementation details
         whilst maintaining RAII: all temporarily allocated datastructures necessary
         for the concrete implementation are therefore freed when this classmethod
         returns.
 
         No instance should directly be created outside of this class method.
         """
-        if metric in ("euclidean", "sqeuclidean"):
-            # Specialized implementation of ArgKmin for the Euclidean distance
-            # for the dense-dense and sparse-sparse cases.
-            # This implementation computes the distances by chunk using
-            # a decomposition of the Squared Euclidean distance.
-            # This specialisation has an improved arithmetic intensity for both
-            # the dense and sparse settings, allowing in most case speed-ups of
-            # several orders of magnitude compared to the generic ArgKmin
-            # implementation.
-            # For more information see MiddleTermComputer.
-            use_squared_distances = metric == "sqeuclidean"
-            pda = EuclideanArgKmin{{name_suffix}}(
-                X=X, Y=Y, k=k,
-                use_squared_distances=use_squared_distances,
-                chunk_size=chunk_size,
-                strategy=strategy,
-                metric_kwargs=metric_kwargs,
-            )
-        else:
-            # Fall back on a generic implementation that handles most scipy
-            # metrics by computing the distances between 2 vectors at a time.
-            pda = ArgKmin{{name_suffix}}(
-                datasets_pair=DatasetsPair{{name_suffix}}.get_for(X, Y, metric, metric_kwargs),
-                k=k,
-                chunk_size=chunk_size,
-                strategy=strategy,
-            )
-
         # Limit the number of threads in second level of nested parallelism for BLAS
-        # to avoid threads over-subscription (in GEMM for instance).
-        with threadpool_limits(limits=1, user_api="blas"):
-            if pda.execute_in_parallel_on_Y:
-                pda._parallel_on_Y()
-            else:
-                pda._parallel_on_X()
+        # to avoid threads over-subscription (in DOT or GEMM for instance).
+        with _threadpool_controller.limit(limits=1, user_api='blas'):
+          if metric in ("euclidean", "sqeuclidean"):
+              # Specialized implementation of ArgKmin for the Euclidean distance
+              # for the dense-dense and sparse-sparse cases.
+              # This implementation computes the distances by chunk using
+              # a decomposition of the Squared Euclidean distance.
+              # This specialisation has an improved arithmetic intensity for both
+              # the dense and sparse settings, allowing in most case speed-ups of
+              # several orders of magnitude compared to the generic ArgKmin
+              # implementation.
+              # Note that squared norms of X and Y are precomputed in the
+              # constructor of this class by issuing BLAS calls that may use
+              # multithreading (depending on the BLAS implementation), hence calling
+              # the constructor needs to be protected under the threadpool_limits
+              # context, along with the main calls to _parallel_on_Y and
+              # _parallel_on_X.
+              # For more information see MiddleTermComputer.
+              use_squared_distances = metric == "sqeuclidean"
+              pda = EuclideanArgKmin{{name_suffix}}(
+                  X=X, Y=Y, k=k,
+                  use_squared_distances=use_squared_distances,
+                  chunk_size=chunk_size,
+                  strategy=strategy,
+                  metric_kwargs=metric_kwargs,
+              )
+          else:
+              # Fall back on a generic implementation that handles most scipy
+              # metrics by computing the distances between 2 vectors at a time.
+              pda = ArgKmin{{name_suffix}}(
+                  datasets_pair=DatasetsPair{{name_suffix}}.get_for(X, Y, metric, metric_kwargs),
+                  k=k,
+                  chunk_size=chunk_size,
+                  strategy=strategy,
+              )
+
+          if pda.execute_in_parallel_on_Y:
+              pda._parallel_on_Y()
+          else:
+              pda._parallel_on_X()
 
         return pda._finalize_results(return_distance)
 
     def __init__(
         self,
         DatasetsPair{{name_suffix}} datasets_pair,
         chunk_size=None,
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.pyx.tp`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 from cython cimport floating, integral
 from cython.parallel cimport parallel, prange
 from libcpp.map cimport map as cpp_map, pair as cpp_pair
 from libc.stdlib cimport free
 
 from ...utils._typedefs cimport intp_t, float64_t
+from ... import _threadpool_controller
 
 import numpy as np
 from scipy.sparse import issparse
-from sklearn.utils.fixes import threadpool_limits
 from ._classmode cimport WeightingStrategy
 
 {{for name_suffix in ["32", "64"]}}
 from ._argkmin cimport ArgKmin{{name_suffix}}
 from ._datasets_pair cimport DatasetsPair{{name_suffix}}
 
 cdef class ArgKminClassMode{{name_suffix}}(ArgKmin{{name_suffix}}):
@@ -62,15 +62,15 @@
             weights=weights,
             Y_labels=Y_labels,
             unique_Y_labels=unique_Y_labels,
         )
 
         # Limit the number of threads in second level of nested parallelism for BLAS
         # to avoid threads over-subscription (in GEMM for instance).
-        with threadpool_limits(limits=1, user_api="blas"):
+        with _threadpool_controller.limit(limits=1, user_api="blas"):
             if pda.execute_in_parallel_on_Y:
                 pda._parallel_on_Y()
             else:
                 pda._parallel_on_X()
 
         return pda._finalize_results()
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_base.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_base.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_base.pyx.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_datasets_pair.pyx.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_middle_term_computer.pyx.tp`

 * *Files 2% similar despite different names*

```diff
@@ -10,32 +10,26 @@
     #
     ('64', False, 'float64_t', 'np.float64'),
     ('32', True, 'float32_t', 'np.float32')
 ]
 
 }}
 from libcpp.vector cimport vector
+from libcpp.algorithm cimport fill
 
 from ...utils._cython_blas cimport (
   BLAS_Order,
   BLAS_Trans,
   NoTrans,
   RowMajor,
   Trans,
   _gemm,
 )
 from ...utils._typedefs cimport float64_t, float32_t, int32_t, intp_t
 
-# TODO: change for `libcpp.algorithm.fill` once Cython 3 is used
-# Introduction in Cython:
-#
-# https://github.com/cython/cython/blob/05059e2a9b89bf6738a7750b905057e5b1e3fe2e/Cython/Includes/libcpp/algorithm.pxd#L50 #noqa
-cdef extern from "<algorithm>" namespace "std" nogil:
-    void fill[Iter, T](Iter first, Iter last, const T& value) except + #noqa
-
 import numpy as np
 from scipy.sparse import issparse, csr_matrix
 
 
 cdef void _middle_term_sparse_sparse_64(
     const float64_t[:] X_data,
     const int32_t[:] X_indices,
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors.pyx.tp`

 * *Files 4% similar despite different names*

```diff
@@ -1,34 +1,30 @@
 cimport numpy as cnp
 import numpy as np
 import warnings
 
 from libcpp.memory cimport shared_ptr, make_shared
 from libcpp.vector cimport vector
+from libcpp.algorithm cimport move
 from cython cimport final
 from cython.operator cimport dereference as deref
 from cython.parallel cimport parallel, prange
 
 from ...utils._sorting cimport simultaneous_sort
 from ...utils._typedefs cimport intp_t, float64_t
 from ...utils._vector_sentinel cimport vector_to_nd_array
 
 from numbers import Real
 from scipy.sparse import issparse
-from ...utils import check_array, check_scalar, _in_unstable_openblas_configuration
-from ...utils.fixes import threadpool_limits
+from ...utils import check_array, check_scalar
+from ...utils.fixes import _in_unstable_openblas_configuration
+from ... import _threadpool_controller
 
 cnp.import_array()
 
-# TODO: change for `libcpp.algorithm.move` once Cython 3 is used
-# Introduction in Cython:
-# https://github.com/cython/cython/blob/05059e2a9b89bf6738a7750b905057e5b1e3fe2e/Cython/Includes/libcpp/algorithm.pxd#L47 #noqa
-cdef extern from "<algorithm>" namespace "std" nogil:
-    OutputIt move[InputIt, OutputIt](InputIt first, InputIt last, OutputIt d_first) except + #noqa
-
 ######################
 
 cdef cnp.ndarray[object, ndim=1] coerce_vectors_to_nd_arrays(
     shared_ptr[vector_vector_double_intp_t] vecs
 ):
     """Coerce a std::vector of std::vector to a ndarray of ndarray."""
     cdef:
@@ -110,15 +106,15 @@
                 chunk_size=chunk_size,
                 strategy=strategy,
                 sort_results=sort_results,
             )
 
         # Limit the number of threads in second level of nested parallelism for BLAS
         # to avoid threads over-subscription (in GEMM for instance).
-        with threadpool_limits(limits=1, user_api="blas"):
+        with _threadpool_controller.limit(limits=1, user_api="blas"):
             if pda.execute_in_parallel_on_Y:
                 pda._parallel_on_Y()
             else:
                 pda._parallel_on_X()
 
         return pda._finalize_results(return_distance)
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/_radius_neighbors_classmode.pyx.tp`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from cython.operator cimport dereference as deref
 from cython.parallel cimport parallel, prange
 from ._classmode cimport WeightingStrategy
 from ...utils._typedefs cimport intp_t, float64_t
 
 import numpy as np
 from scipy.sparse import issparse
-from ...utils.fixes import threadpool_limits
+from ... import _threadpool_controller
 
 
 {{for name_suffix in ["32", "64"]}}
 from ._radius_neighbors cimport RadiusNeighbors{{name_suffix}}
 from ._datasets_pair cimport DatasetsPair{{name_suffix}}
 
 cdef class RadiusNeighborsClassMode{{name_suffix}}(RadiusNeighbors{{name_suffix}}):
@@ -56,15 +56,15 @@
             Y_labels=Y_labels,
             unique_Y_labels=unique_Y_labels,
             outlier_label=outlier_label,
         )
 
         # Limit the number of threads in second level of nested parallelism for BLAS
         # to avoid threads over-subscription (in GEMM for instance).
-        with threadpool_limits(limits=1, user_api="blas"):
+        with _threadpool_controller.limit(limits=1, user_api="blas"):
             if pda.execute_in_parallel_on_Y:
                 pda._parallel_on_Y()
             else:
                 pda._parallel_on_X()
 
         return pda._finalize_results()
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_distances_reduction/meson.build` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_distances_reduction/meson.build`

 * *Files 3% similar despite different names*

```diff
@@ -144,14 +144,18 @@
   '_argkmin_classmode',
   [_argkmin_classmode_pyx, _classmode_pxd,
    _argkmin_pxd, _pairwise_distances_reduction_cython_tree,
    _datasets_pair_pxd, _base_pxd, _middle_term_computer_pxd, utils_cython_tree],
   dependencies: [np_dep],
   override_options: ['cython_language=cpp'],
   cython_args: cython_args,
+  # XXX: for some reason -fno-sized-deallocation is needed otherwise there is
+  # an error with undefined symbol _ZdlPv at import time in manylinux wheels.
+  # See https://github.com/scikit-learn/scikit-learn/issues/28596 for more details.
+  cpp_args: ['-fno-sized-deallocation'],
   subdir: 'sklearn/metrics/_pairwise_distances_reduction',
   install: true
 )
 
 _radius_neighbors_classmode_pyx = custom_target(
   '_radius_neighbors_classmode_pyx',
   output: '_radius_neighbors_classmode.pyx',
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_pairwise_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/metrics/_pairwise_fast.pyx`

 * *Files 6% similar despite different names*

```diff
@@ -1,30 +1,29 @@
 # Author: Andreas Mueller <amueller@ais.uni-bonn.de>
 #         Lars Buitinck
 #         Paolo Toccaceli
 #
 # License: BSD 3 clause
 
-cimport numpy as cnp
 from cython cimport floating
 from cython.parallel cimport prange
 from libc.math cimport fabs
 
-from ..utils._openmp_helpers import _openmp_effective_n_threads
+from ..utils._typedefs cimport intp_t
 
-cnp.import_array()
+from ..utils._openmp_helpers import _openmp_effective_n_threads
 
 
 def _chi2_kernel_fast(floating[:, :] X,
                       floating[:, :] Y,
                       floating[:, :] result):
-    cdef cnp.npy_intp i, j, k
-    cdef cnp.npy_intp n_samples_X = X.shape[0]
-    cdef cnp.npy_intp n_samples_Y = Y.shape[0]
-    cdef cnp.npy_intp n_features = X.shape[1]
+    cdef intp_t i, j, k
+    cdef intp_t n_samples_X = X.shape[0]
+    cdef intp_t n_samples_Y = Y.shape[0]
+    cdef intp_t n_features = X.shape[1]
     cdef double res, nom, denom
 
     with nogil:
         for i in range(n_samples_X):
             for j in range(n_samples_Y):
                 res = 0
                 for k in range(n_features):
@@ -48,15 +47,15 @@
 
     Usage:
     >>> D = np.zeros(X.shape[0], Y.shape[0])
     >>> _sparse_manhattan(X.data, X.indices, X.indptr,
     ...                   Y.data, Y.indices, Y.indptr,
     ...                   D)
     """
-    cdef cnp.npy_intp px, py, i, j, ix, iy
+    cdef intp_t px, py, i, j, ix, iy
     cdef double d = 0.0
 
     cdef int m = D.shape[0]
     cdef int n = D.shape[1]
 
     cdef int X_indptr_end = 0
     cdef int Y_indptr_end = 0
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/confusion_matrix.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/confusion_matrix.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from itertools import product
 
 import numpy as np
 
 from ...base import is_classifier
-from ...utils import check_matplotlib_support
+from ...utils._optional_dependencies import check_matplotlib_support
 from ...utils.multiclass import unique_labels
 from .. import confusion_matrix
 
 
 class ConfusionMatrixDisplay:
     """Confusion Matrix visualization.
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/det_curve.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/det_curve.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/precision_recall_curve.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/precision_recall_curve.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/regression.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/regression.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import numbers
 
 import numpy as np
 
-from ...utils import _safe_indexing, check_matplotlib_support, check_random_state
+from ...utils import _safe_indexing, check_random_state
+from ...utils._optional_dependencies import check_matplotlib_support
 
 
 class PredictionErrorDisplay:
     """Visualization of the prediction error of a regression model.
 
     This tool can display "residuals vs predicted" or "actual vs predicted"
     using scatter plots to qualitatively assess the behavior of a regressor,
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/roc_curve.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/roc_curve.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_common_curve_display.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_common_curve_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_confusion_matrix_display.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_confusion_matrix_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_det_curve_display.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_det_curve_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_precision_recall_display.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_precision_recall_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_predict_error_display.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_predict_error_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_plot/tests/test_roc_curve_display.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_plot/tests/test_roc_curve_display.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_ranking.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_ranking.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,15 +32,15 @@
 from ..utils import (
     assert_all_finite,
     check_array,
     check_consistent_length,
     column_or_1d,
 )
 from ..utils._encode import _encode, _unique
-from ..utils._param_validation import Interval, StrOptions, validate_params
+from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params
 from ..utils.extmath import stable_cumsum
 from ..utils.fixes import trapezoid
 from ..utils.multiclass import type_of_target
 from ..utils.sparsefuncs import count_nonzero
 from ..utils.validation import _check_pos_label_consistency, _check_sample_weight
 from ._base import _average_binary_score, _average_multiclass_ovo_score
 
@@ -861,23 +861,33 @@
         fps = 1 + threshold_idxs - tps
     return fps, tps, y_score[threshold_idxs]
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
-        "probas_pred": ["array-like"],
+        "y_score": ["array-like", Hidden(None)],
         "pos_label": [Real, str, "boolean", None],
         "sample_weight": ["array-like", None],
         "drop_intermediate": ["boolean"],
+        "probas_pred": [
+            "array-like",
+            Hidden(StrOptions({"deprecated"})),
+        ],
     },
     prefer_skip_nested_validation=True,
 )
 def precision_recall_curve(
-    y_true, probas_pred, *, pos_label=None, sample_weight=None, drop_intermediate=False
+    y_true,
+    y_score=None,
+    *,
+    pos_label=None,
+    sample_weight=None,
+    drop_intermediate=False,
+    probas_pred="deprecated",
 ):
     """Compute precision-recall pairs for different probability thresholds.
 
     Note: this implementation is restricted to the binary classification task.
 
     The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of
     true positives and ``fp`` the number of false positives. The precision is
@@ -899,15 +909,15 @@
 
     Parameters
     ----------
     y_true : array-like of shape (n_samples,)
         True binary labels. If labels are not either {-1, 1} or {0, 1}, then
         pos_label should be explicitly given.
 
-    probas_pred : array-like of shape (n_samples,)
+    y_score : array-like of shape (n_samples,)
         Target scores, can either be probability estimates of the positive
         class, or non-thresholded measure of decisions (as returned by
         `decision_function` on some classifiers).
 
     pos_label : int, float, bool or str, default=None
         The label of the positive class.
         When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},
@@ -919,14 +929,23 @@
     drop_intermediate : bool, default=False
         Whether to drop some suboptimal thresholds which would not appear
         on a plotted precision-recall curve. This is useful in order to create
         lighter precision-recall curves.
 
         .. versionadded:: 1.3
 
+    probas_pred : array-like of shape (n_samples,)
+        Target scores, can either be probability estimates of the positive
+        class, or non-thresholded measure of decisions (as returned by
+        `decision_function` on some classifiers).
+
+        .. deprecated:: 1.5
+            `probas_pred` is deprecated and will be removed in 1.7. Use
+            `y_score` instead.
+
     Returns
     -------
     precision : ndarray of shape (n_thresholds + 1,)
         Precision values such that element i is the precision of
         predictions with score >= thresholds[i] and the last element is 1.
 
     recall : ndarray of shape (n_thresholds + 1,)
@@ -958,16 +977,34 @@
     >>> precision
     array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])
     >>> recall
     array([1. , 1. , 0.5, 0.5, 0. ])
     >>> thresholds
     array([0.1 , 0.35, 0.4 , 0.8 ])
     """
+    # TODO(1.7): remove in 1.7 and reset y_score to be required
+    # Note: validate params will raise an error if probas_pred is not array-like,
+    # or "deprecated"
+    if y_score is not None and not isinstance(probas_pred, str):
+        raise ValueError(
+            "`probas_pred` and `y_score` cannot be both specified. Please use `y_score`"
+            " only as `probas_pred` is deprecated in v1.5 and will be removed in v1.7."
+        )
+    if y_score is None:
+        warnings.warn(
+            (
+                "probas_pred was deprecated in version 1.5 and will be removed in 1.7."
+                "Please use ``y_score`` instead."
+            ),
+            FutureWarning,
+        )
+        y_score = probas_pred
+
     fps, tps, thresholds = _binary_clf_curve(
-        y_true, probas_pred, pos_label=pos_label, sample_weight=sample_weight
+        y_true, y_score, pos_label=pos_label, sample_weight=sample_weight
     )
 
     if drop_intermediate and len(fps) > 2:
         # Drop thresholds corresponding to points where true positives (tps)
         # do not change from the previous or subsequent point. This will keep
         # only the first and last point for each tps value. All points
         # with the same tps value have the same recall and thus x coordinate.
@@ -1631,15 +1668,15 @@
     European conference on information retrieval (pp. 414-421). Springer,
     Berlin, Heidelberg.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn.metrics import dcg_score
-    >>> # we have groud-truth relevance of some answers to a query:
+    >>> # we have ground-truth relevance of some answers to a query:
     >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])
     >>> # we predict scores for the answers
     >>> scores = np.asarray([[.1, .2, .3, 4, 70]])
     >>> dcg_score(true_relevance, scores)
     9.49...
     >>> # we can set k to truncate the sum; only top k answers contribute
     >>> dcg_score(true_relevance, scores, k=2)
@@ -1791,15 +1828,15 @@
     European conference on information retrieval (pp. 414-421). Springer,
     Berlin, Heidelberg.
 
     Examples
     --------
     >>> import numpy as np
     >>> from sklearn.metrics import ndcg_score
-    >>> # we have groud-truth relevance of some answers to a query:
+    >>> # we have ground-truth relevance of some answers to a query:
     >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])
     >>> # we predict some scores (relevance) for the answers
     >>> scores = np.asarray([[.1, .2, .3, 4, 70]])
     >>> ndcg_score(true_relevance, scores)
     0.69...
     >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])
     >>> ndcg_score(true_relevance, scores)
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_regression.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_regression.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,14 +30,21 @@
 import warnings
 from numbers import Real
 
 import numpy as np
 from scipy.special import xlogy
 
 from ..exceptions import UndefinedMetricWarning
+from ..utils._array_api import (
+    _average,
+    _find_matching_floating_dtype,
+    get_namespace,
+    get_namespace_and_device,
+    size,
+)
 from ..utils._param_validation import Hidden, Interval, StrOptions, validate_params
 from ..utils.stats import _weighted_percentile
 from ..utils.validation import (
     _check_sample_weight,
     _num_samples,
     check_array,
     check_consistent_length,
@@ -61,15 +68,15 @@
     "mean_gamma_deviance",
     "d2_tweedie_score",
     "d2_pinball_score",
     "d2_absolute_error_score",
 ]
 
 
-def _check_reg_targets(y_true, y_pred, multioutput, dtype="numeric"):
+def _check_reg_targets(y_true, y_pred, multioutput, dtype="numeric", xp=None):
     """Check that y_true and y_pred belong to the same regression task.
 
     Parameters
     ----------
     y_true : array-like
 
     y_pred : array-like
@@ -95,23 +102,25 @@
 
     multioutput : array-like of shape (n_outputs) or string in ['raw_values',
         uniform_average', 'variance_weighted'] or None
         Custom output weights if ``multioutput`` is array-like or
         just the corresponding argument if ``multioutput`` is a
         correct keyword.
     """
+    xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)
+
     check_consistent_length(y_true, y_pred)
     y_true = check_array(y_true, ensure_2d=False, dtype=dtype)
     y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)
 
     if y_true.ndim == 1:
-        y_true = y_true.reshape((-1, 1))
+        y_true = xp.reshape(y_true, (-1, 1))
 
     if y_pred.ndim == 1:
-        y_pred = y_pred.reshape((-1, 1))
+        y_pred = xp.reshape(y_pred, (-1, 1))
 
     if y_true.shape[1] != y_pred.shape[1]:
         raise ValueError(
             "y_true and y_pred have different number of output ({0}!={1})".format(
                 y_true.shape[1], y_pred.shape[1]
             )
         )
@@ -851,55 +860,61 @@
             # pass None as weights to np.average: uniform mean
             multioutput = None
 
     return np.average(output_errors, weights=multioutput)
 
 
 def _assemble_r2_explained_variance(
-    numerator, denominator, n_outputs, multioutput, force_finite
+    numerator, denominator, n_outputs, multioutput, force_finite, xp, device
 ):
     """Common part used by explained variance score and :math:`R^2` score."""
+    dtype = numerator.dtype
 
     nonzero_denominator = denominator != 0
 
     if not force_finite:
         # Standard formula, that may lead to NaN or -Inf
         output_scores = 1 - (numerator / denominator)
     else:
         nonzero_numerator = numerator != 0
         # Default = Zero Numerator = perfect predictions. Set to 1.0
         # (note: even if denominator is zero, thus avoiding NaN scores)
-        output_scores = np.ones([n_outputs])
+        output_scores = xp.ones([n_outputs], device=device, dtype=dtype)
         # Non-zero Numerator and Non-zero Denominator: use the formula
         valid_score = nonzero_denominator & nonzero_numerator
+
         output_scores[valid_score] = 1 - (
             numerator[valid_score] / denominator[valid_score]
         )
+
         # Non-zero Numerator and Zero Denominator:
         # arbitrary set to 0.0 to avoid -inf scores
         output_scores[nonzero_numerator & ~nonzero_denominator] = 0.0
 
     if isinstance(multioutput, str):
         if multioutput == "raw_values":
             # return scores individually
             return output_scores
         elif multioutput == "uniform_average":
             # Passing None as weights to np.average results is uniform mean
             avg_weights = None
         elif multioutput == "variance_weighted":
             avg_weights = denominator
-            if not np.any(nonzero_denominator):
+            if not xp.any(nonzero_denominator):
                 # All weights are zero, np.average would raise a ZeroDiv error.
                 # This only happens when all y are constant (or 1-element long)
                 # Since weights are all equal, fall back to uniform weights.
                 avg_weights = None
     else:
         avg_weights = multioutput
 
-    return np.average(output_scores, weights=avg_weights)
+    result = _average(output_scores, weights=avg_weights)
+    if size(result) == 1:
+        return float(result)
+    return result
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
         "y_pred": ["array-like"],
         "sample_weight": ["array-like", None],
@@ -1029,14 +1044,17 @@
 
     return _assemble_r2_explained_variance(
         numerator=numerator,
         denominator=denominator,
         n_outputs=y_true.shape[1],
         multioutput=multioutput,
         force_finite=force_finite,
+        xp=get_namespace(y_true)[0],
+        # TODO: update once Array API support is added to explained_variance_score.
+        device=None,
     )
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
         "y_pred": ["array-like"],
@@ -1173,41 +1191,50 @@
     >>> y_true = [-2, -2, -2]
     >>> y_pred = [-2, -2, -2 + 1e-8]
     >>> r2_score(y_true, y_pred)
     0.0
     >>> r2_score(y_true, y_pred, force_finite=False)
     -inf
     """
-    y_type, y_true, y_pred, multioutput = _check_reg_targets(
-        y_true, y_pred, multioutput
+    xp, _, device_ = get_namespace_and_device(
+        y_true, y_pred, sample_weight, multioutput
+    )
+
+    dtype = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)
+
+    _, y_true, y_pred, multioutput = _check_reg_targets(
+        y_true, y_pred, multioutput, dtype=dtype, xp=xp
     )
     check_consistent_length(y_true, y_pred, sample_weight)
 
     if _num_samples(y_pred) < 2:
         msg = "R^2 score is not well-defined with less than two samples."
         warnings.warn(msg, UndefinedMetricWarning)
         return float("nan")
 
     if sample_weight is not None:
-        sample_weight = column_or_1d(sample_weight)
-        weight = sample_weight[:, np.newaxis]
+        sample_weight = column_or_1d(sample_weight, dtype=dtype)
+        weight = sample_weight[:, None]
     else:
         weight = 1.0
 
-    numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)
-    denominator = (
-        weight * (y_true - np.average(y_true, axis=0, weights=sample_weight)) ** 2
-    ).sum(axis=0, dtype=np.float64)
+    numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)
+    denominator = xp.sum(
+        weight * (y_true - _average(y_true, axis=0, weights=sample_weight, xp=xp)) ** 2,
+        axis=0,
+    )
 
     return _assemble_r2_explained_variance(
         numerator=numerator,
         denominator=denominator,
         n_outputs=y_true.shape[1],
         multioutput=multioutput,
         force_finite=force_finite,
+        xp=xp,
+        device=device_,
     )
 
 
 @validate_params(
     {
         "y_true": ["array-like"],
         "y_pred": ["array-like"],
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/_scorer.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/_scorer.py`

 * *Files 2% similar despite different names*

```diff
@@ -28,27 +28,29 @@
 from ..base import is_regressor
 from ..utils import Bunch
 from ..utils._param_validation import HasMethods, Hidden, StrOptions, validate_params
 from ..utils._response import _get_response_values
 from ..utils.metadata_routing import (
     MetadataRequest,
     MetadataRouter,
+    MethodMapping,
     _MetadataRequester,
     _raise_for_params,
     _routing_enabled,
     get_routing_for_object,
     process_routing,
 )
 from ..utils.validation import _check_response_method
 from . import (
     accuracy_score,
     average_precision_score,
     balanced_accuracy_score,
     brier_score_loss,
     class_likelihood_ratios,
+    d2_absolute_error_score,
     explained_variance_score,
     f1_score,
     jaccard_score,
     log_loss,
     matthews_corrcoef,
     max_error,
     mean_absolute_error,
@@ -143,14 +145,18 @@
             except Exception as e:
                 if self._raise_exc:
                     raise e
                 else:
                     scores[name] = format_exc()
         return scores
 
+    def __repr__(self):
+        scorers = ", ".join([f'"{s}"' for s in self._scorers])
+        return f"MultiMetricScorer({scorers})"
+
     def _use_cache(self, estimator):
         """Return True if using a cache is beneficial, thus when a response method will
         be called several time.
         """
         if len(self._scorers) == 1:  # Only one scorer
             return False
 
@@ -179,19 +185,39 @@
         Returns
         -------
         routing : MetadataRouter
             A :class:`~utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
         return MetadataRouter(owner=self.__class__.__name__).add(
-            **self._scorers, method_mapping="score"
+            **self._scorers,
+            method_mapping=MethodMapping().add(caller="score", callee="score"),
         )
 
 
 class _BaseScorer(_MetadataRequester):
+    """Base scorer that is used as `scorer(estimator, X, y_true)`.
+
+    Parameters
+    ----------
+    score_func : callable
+        The score function to use. It will be called as
+        `score_func(y_true, y_pred, **kwargs)`.
+
+    sign : int
+        Either 1 or -1 to returns the score with `sign * score_func(estimator, X, y)`.
+        Thus, `sign` defined if higher scores are better or worse.
+
+    kwargs : dict
+        Additional parameters to pass to the score function.
+
+    response_method : str
+        The method to call on the estimator to get the response values.
+    """
+
     def __init__(self, score_func, sign, kwargs, response_method="predict"):
         self._score_func = score_func
         self._sign = sign
         self._kwargs = kwargs
         self._response_method = response_method
 
     def _get_pos_label(self):
@@ -402,43 +428,79 @@
                 "to get valid options." % scoring
             )
     else:
         scorer = scoring
     return scorer
 
 
-class _PassthroughScorer:
+class _PassthroughScorer(_MetadataRequester):
+    # Passes scoring of estimator's `score` method back to estimator if scoring
+    # is `None`.
+
     def __init__(self, estimator):
         self._estimator = estimator
 
+        requests = MetadataRequest(owner=self.__class__.__name__)
+        try:
+            requests.score = copy.deepcopy(estimator._metadata_request.score)
+        except AttributeError:
+            try:
+                requests.score = copy.deepcopy(estimator._get_default_requests().score)
+            except AttributeError:
+                pass
+
+        self._metadata_request = requests
+
     def __call__(self, estimator, *args, **kwargs):
         """Method that wraps estimator.score"""
         return estimator.score(*args, **kwargs)
 
+    def __repr__(self):
+        return f"{self._estimator.__class__}.score"
+
     def get_metadata_routing(self):
         """Get requested data properties.
 
         Please check :ref:`User Guide <metadata_routing>` on how the routing
         mechanism works.
 
         .. versionadded:: 1.3
 
         Returns
         -------
         routing : MetadataRouter
             A :class:`~utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
-        # This scorer doesn't do any validation or routing, it only exposes the
-        # requests of the given estimator. This object behaves as a consumer
-        # rather than a router. Ideally it only exposes the score requests to
-        # the parent object; however, that requires computing the routing for
-        # meta-estimators, which would be more time consuming than simply
-        # returning the child object's requests.
-        return get_routing_for_object(self._estimator)
+        return get_routing_for_object(self._metadata_request)
+
+    def set_score_request(self, **kwargs):
+        """Set requested parameters by the scorer.
+
+        Please see :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Parameters
+        ----------
+        kwargs : dict
+            Arguments should be of the form ``param_name=alias``, and `alias`
+            can be one of ``{True, False, None, str}``.
+        """
+        if not _routing_enabled():
+            raise RuntimeError(
+                "This method is only available when metadata routing is enabled."
+                " You can enable it using"
+                " sklearn.set_config(enable_metadata_routing=True)."
+            )
+
+        for param, alias in kwargs.items():
+            self._metadata_request.score.add_request(param=param, alias=alias)
+        return self
 
 
 def _check_multimetric_scoring(estimator, scoring):
     """Check the scoring parameter in cases when multiple metrics are allowed.
 
     In addition, multimetric scoring leverages a caching mechanism to not call the same
     estimator response method multiple times. Hence, the scorer is modified to only use
@@ -719,14 +781,15 @@
 neg_mean_poisson_deviance_scorer = make_scorer(
     mean_poisson_deviance, greater_is_better=False
 )
 
 neg_mean_gamma_deviance_scorer = make_scorer(
     mean_gamma_deviance, greater_is_better=False
 )
+d2_absolute_error_scorer = make_scorer(d2_absolute_error_score)
 
 # Standard Classification Scores
 accuracy_scorer = make_scorer(accuracy_score)
 balanced_accuracy_scorer = make_scorer(balanced_accuracy_score)
 matthews_corrcoef_scorer = make_scorer(matthews_corrcoef)
 
 
@@ -811,14 +874,15 @@
     neg_mean_absolute_percentage_error=neg_mean_absolute_percentage_error_scorer,
     neg_mean_squared_error=neg_mean_squared_error_scorer,
     neg_mean_squared_log_error=neg_mean_squared_log_error_scorer,
     neg_root_mean_squared_error=neg_root_mean_squared_error_scorer,
     neg_root_mean_squared_log_error=neg_root_mean_squared_log_error_scorer,
     neg_mean_poisson_deviance=neg_mean_poisson_deviance_scorer,
     neg_mean_gamma_deviance=neg_mean_gamma_deviance_scorer,
+    d2_absolute_error_score=d2_absolute_error_scorer,
     accuracy=accuracy_scorer,
     top_k_accuracy=top_k_accuracy_scorer,
     roc_auc=roc_auc_scorer,
     roc_auc_ovr=roc_auc_ovr_scorer,
     roc_auc_ovo=roc_auc_ovo_scorer,
     roc_auc_ovr_weighted=roc_auc_ovr_weighted_scorer,
     roc_auc_ovo_weighted=roc_auc_ovo_weighted_scorer,
@@ -876,34 +940,52 @@
     for average in ["macro", "micro", "samples", "weighted"]:
         qualified_name = "{0}_{1}".format(name, average)
         _SCORERS[qualified_name] = make_scorer(metric, pos_label=None, average=average)
 
 
 @validate_params(
     {
-        "estimator": [HasMethods("fit")],
-        "scoring": [StrOptions(set(get_scorer_names())), callable, None],
+        "estimator": [HasMethods("fit"), None],
+        "scoring": [
+            StrOptions(set(get_scorer_names())),
+            callable,
+            list,
+            set,
+            tuple,
+            dict,
+            None,
+        ],
         "allow_none": ["boolean"],
     },
     prefer_skip_nested_validation=True,
 )
-def check_scoring(estimator, scoring=None, *, allow_none=False):
+def check_scoring(estimator=None, scoring=None, *, allow_none=False):
     """Determine scorer from user options.
 
     A TypeError will be thrown if the estimator cannot be scored.
 
     Parameters
     ----------
-    estimator : estimator object implementing 'fit'
-        The object to use to fit the data.
+    estimator : estimator object implementing 'fit' or None, default=None
+        The object to use to fit the data. If `None`, then this function may error
+        depending on `allow_none`.
+
+    scoring : str, callable, list, tuple, or dict, default=None
+        Scorer to use. If `scoring` represents a single score, one can use:
+
+        - a single string (see :ref:`scoring_parameter`);
+        - a callable (see :ref:`scoring`) that returns a single value.
+
+        If `scoring` represents multiple scores, one can use:
+
+        - a list or tuple of unique strings;
+        - a callable returning a dictionary where the keys are the metric
+          names and the values are the metric scorers;
+        - a dictionary with metric names as keys and callables a values.
 
-    scoring : str or callable, default=None
-        A string (see model evaluation documentation) or
-        a scorer callable object / function with signature
-        ``scorer(estimator, X, y)``.
         If None, the provided estimator object's `score` method is used.
 
     allow_none : bool, default=False
         If no scoring is specified and the estimator has no score function, we
         can either return None or raise an exception.
 
     Returns
@@ -938,14 +1020,17 @@
                 "scoring value %r looks like it is a metric "
                 "function rather than a scorer. A scorer should "
                 "require an estimator as its first parameter. "
                 "Please use `make_scorer` to convert a metric "
                 "to a scorer." % scoring
             )
         return get_scorer(scoring)
+    if isinstance(scoring, (list, tuple, set, dict)):
+        scorers = _check_multimetric_scoring(estimator, scoring=scoring)
+        return _MultimetricScorer(scorers=scorers)
     if scoring is None:
         if hasattr(estimator, "score"):
             return _PassthroughScorer(estimator)
         elif allow_none:
             return None
         else:
             raise TypeError(
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """
 The :mod:`sklearn.metrics.cluster` submodule contains evaluation metrics for
 cluster analysis results. There are two forms of evaluation:
 
 - supervised, which uses a ground truth class values for each sample.
 - unsupervised, which does not and measures the 'quality' of the model itself.
 """
+
 from ._bicluster import consensus_score
 from ._supervised import (
     adjusted_mutual_info_score,
     adjusted_rand_score,
     completeness_score,
     contingency_matrix,
     entropy,
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/_bicluster.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_bicluster.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_expected_mutual_info_fast.pyx`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 # Authors: Robert Layton <robertlayton@gmail.com>
 #           Corey Lynch <coreylynch9@gmail.com>
 # License: BSD 3 clause
 
 from libc.math cimport exp, lgamma
-from scipy.special import gammaln
-import numpy as np
-cimport numpy as cnp
 
-cnp.import_array()
+from ...utils._typedefs cimport float64_t, int64_t
+
+import numpy as np
+from scipy.special import gammaln
 
 
-def expected_mutual_information(contingency, cnp.int64_t n_samples):
+def expected_mutual_information(contingency, int64_t n_samples):
     """Calculate the expected mutual information for two labelings."""
     cdef:
-        cnp.float64_t emi = 0
-        cnp.int64_t n_rows, n_cols
-        cnp.float64_t term2, term3, gln
-        cnp.int64_t[::1] a_view, b_view
-        cnp.float64_t[::1] term1
-        cnp.float64_t[::1] gln_a, gln_b, gln_Na, gln_Nb, gln_Nnij, log_Nnij
-        cnp.float64_t[::1] log_a, log_b
+        float64_t emi = 0
+        int64_t n_rows, n_cols
+        float64_t term2, term3, gln
+        int64_t[::1] a_view, b_view
+        float64_t[::1] term1
+        float64_t[::1] gln_a, gln_b, gln_Na, gln_Nb, gln_Nnij, log_Nnij
+        float64_t[::1] log_a, log_b
         Py_ssize_t i, j, nij
-        cnp.int64_t start, end
+        int64_t start, end
 
     n_rows, n_cols = contingency.shape
     a = np.ravel(contingency.sum(axis=1).astype(np.int64, copy=False))
     b = np.ravel(contingency.sum(axis=0).astype(np.int64, copy=False))
     a_view = a
     b_view = b
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/_supervised.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_supervised.py`

 * *Files 0% similar despite different names*

```diff
@@ -490,15 +490,15 @@
 
     Parameters
     ----------
     labels_true : array-like of shape (n_samples,)
         Ground truth class labels to be used as a reference.
 
     labels_pred : array-like of shape (n_samples,)
-        Gluster labels to evaluate.
+        Cluster labels to evaluate.
 
     beta : float, default=1.0
         Ratio of weight attributed to ``homogeneity`` vs ``completeness``.
         If ``beta`` is greater than 1, ``completeness`` is weighted more
         strongly in the calculation. If ``beta`` is less than 1,
         ``homogeneity`` is weighted more strongly.
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/_unsupervised.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/_unsupervised.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 from numbers import Integral
 
 import numpy as np
 from scipy.sparse import issparse
 
 from ...preprocessing import LabelEncoder
 from ...utils import _safe_indexing, check_random_state, check_X_y
+from ...utils._array_api import _atol_for_type
 from ...utils._param_validation import (
     Interval,
     StrOptions,
     validate_params,
 )
 from ..pairwise import _VALID_METRICS, pairwise_distances, pairwise_distances_chunked
 
@@ -280,15 +281,16 @@
     # Check for non-zero diagonal entries in precomputed distance matrix
     if metric == "precomputed":
         error_msg = ValueError(
             "The precomputed distance matrix contains non-zero "
             "elements on the diagonal. Use np.fill_diagonal(X, 0)."
         )
         if X.dtype.kind == "f":
-            atol = np.finfo(X.dtype).eps * 100
+            atol = _atol_for_type(X.dtype)
+
             if np.any(np.abs(X.diagonal()) > atol):
                 raise error_msg
         elif np.any(X.diagonal() != 0):  # integral dtype
             raise error_msg
 
     le = LabelEncoder()
     labels = le.fit_transform(labels)
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_bicluster.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_bicluster.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_common.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_supervised.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_supervised.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/cluster/tests/test_unsupervised.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/cluster/tests/test_unsupervised.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/meson.build` & `scikit_learn-1.5.0rc1/sklearn/metrics/meson.build`

 * *Files 20% similar despite different names*

```diff
@@ -1,44 +1,46 @@
 # Metrics is cimported from other subpackages so this is needed for the cimport
 # to work
 metrics_cython_tree = [
   fs.copyfile('__init__.py')
 ]
+# Some metrics code cimports code from utils, we may as well copy all the necessary files
+metrics_cython_tree += utils_cython_tree
 
 _dist_metrics_pxd = custom_target(
   '_dist_metrics_pxd',
   output: '_dist_metrics.pxd',
   input: '_dist_metrics.pxd.tp',
   command: [py, tempita, '@INPUT@', '-o', '@OUTDIR@'],
   # Need to install the generated pxd because it is needed in other subpackages
   # Cython code, e.g. sklearn.cluster
   install_dir: sklearn_dir / 'metrics',
   install: true,
 )
+metrics_cython_tree += [_dist_metrics_pxd]
 
 _dist_metrics_pyx = custom_target(
   '_dist_metrics_pyx',
   output: '_dist_metrics.pyx',
   input: '_dist_metrics.pyx.tp',
   command: [py, tempita, '@INPUT@', '-o', '@OUTDIR@']
 )
 
 _dist_metrics = py.extension_module(
   '_dist_metrics',
-  [_dist_metrics_pyx, _dist_metrics_pxd, utils_cython_tree, metrics_cython_tree],
+  [_dist_metrics_pyx, metrics_cython_tree],
   dependencies: [np_dep],
   cython_args: cython_args,
   subdir: 'sklearn/metrics',
   install: true
 )
 
 py.extension_module(
   '_pairwise_fast',
-  ['_pairwise_fast.pyx', utils_cython_tree, metrics_cython_tree],
-  dependencies: [np_dep],
+  ['_pairwise_fast.pyx', metrics_cython_tree],
   cython_args: cython_args,
   subdir: 'sklearn/metrics',
   install: true
 )
 
 subdir('_pairwise_distances_reduction')
 subdir('cluster')
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/pairwise.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/pairwise.py`

 * *Files 1% similar despite different names*

```diff
@@ -20,18 +20,18 @@
 from .. import config_context
 from ..exceptions import DataConversionWarning
 from ..preprocessing import normalize
 from ..utils import (
     check_array,
     gen_batches,
     gen_even_slices,
-    get_chunk_n_rows,
-    is_scalar_nan,
 )
+from ..utils._chunking import get_chunk_n_rows
 from ..utils._mask import _get_mask
+from ..utils._missing import is_scalar_nan
 from ..utils._param_validation import (
     Hidden,
     Interval,
     MissingValues,
     Options,
     StrOptions,
     validate_params,
@@ -70,17 +70,18 @@
 
 
 def check_pairwise_arrays(
     X,
     Y,
     *,
     precomputed=False,
-    dtype=None,
+    dtype="infer_float",
     accept_sparse="csr",
     force_all_finite=True,
+    ensure_2d=True,
     copy=False,
 ):
     """Set X and Y appropriately and checks inputs.
 
     If Y is None, it is set as a pointer to X (i.e. not a copy).
     If Y is given, this does not happen.
     All distance metrics should use this function first to assert that the
@@ -98,17 +99,18 @@
 
     Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
 
     precomputed : bool, default=False
         True if X is to be treated as precomputed distances to the samples in
         Y.
 
-    dtype : str, type, list of type, default=None
-        Data type required for X and Y. If None, the dtype will be an
-        appropriate float type selected by _return_float_dtype.
+    dtype : str, type, list of type or None default="infer_float"
+        Data type required for X and Y. If "infer_float", the dtype will be an
+        appropriate float type selected by _return_float_dtype. If None, the
+        dtype of the input is preserved.
 
         .. versionadded:: 0.18
 
     accept_sparse : str, bool or list/tuple of str, default='csr'
         String[s] representing allowed sparse matrix formats, such as 'csc',
         'csr', etc. If the input is sparse but not in the allowed format,
         it will be converted to the first listed format. True allows the input
@@ -126,14 +128,21 @@
 
         .. versionadded:: 0.22
            ``force_all_finite`` accepts the string ``'allow-nan'``.
 
         .. versionchanged:: 0.23
            Accepts `pd.NA` and converts it into `np.nan`.
 
+    ensure_2d : bool, default=True
+        Whether to raise an error when the input arrays are not 2-dimensional. Setting
+        this to `False` is necessary when using a custom metric with certain
+        non-numerical inputs (e.g. a list of strings).
+
+        .. versionadded:: 1.5
+
     copy : bool, default=False
         Whether a forced copy will be triggered. If copy=False, a copy might
         be triggered by a conversion.
 
         .. versionadded:: 0.22
 
     Returns
@@ -144,52 +153,57 @@
     safe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
         An array equal to Y if Y was not None, guaranteed to be a numpy array.
         If Y was None, safe_Y will be a pointer to X.
     """
     X, Y, dtype_float = _return_float_dtype(X, Y)
 
     estimator = "check_pairwise_arrays"
-    if dtype is None:
+    if dtype == "infer_float":
         dtype = dtype_float
 
     if Y is X or Y is None:
         X = Y = check_array(
             X,
             accept_sparse=accept_sparse,
             dtype=dtype,
             copy=copy,
             force_all_finite=force_all_finite,
             estimator=estimator,
+            ensure_2d=ensure_2d,
         )
     else:
         X = check_array(
             X,
             accept_sparse=accept_sparse,
             dtype=dtype,
             copy=copy,
             force_all_finite=force_all_finite,
             estimator=estimator,
+            ensure_2d=ensure_2d,
         )
         Y = check_array(
             Y,
             accept_sparse=accept_sparse,
             dtype=dtype,
             copy=copy,
             force_all_finite=force_all_finite,
             estimator=estimator,
+            ensure_2d=ensure_2d,
         )
 
     if precomputed:
         if X.shape[1] != Y.shape[0]:
             raise ValueError(
                 "Precomputed metric requires shape "
                 "(n_queries, n_indexed). Got (%d, %d) "
                 "for %d indexed." % (X.shape[0], X.shape[1], Y.shape[0])
             )
-    elif X.shape[1] != Y.shape[1]:
+    elif ensure_2d and X.shape[1] != Y.shape[1]:
+        # Only check the number of features if 2d arrays are enforced. Otherwise,
+        # validation is left to the user for custom metrics.
         raise ValueError(
             "Incompatible dimension for X and Y matrices: "
             "X.shape[1] == %d while Y.shape[1] == %d" % (X.shape[1], Y.shape[1])
         )
 
     return X, Y
 
@@ -888,17 +902,14 @@
     --------
     >>> from sklearn.metrics.pairwise import pairwise_distances_argmin
     >>> X = [[0, 0, 0], [1, 1, 1]]
     >>> Y = [[1, 0, 0], [1, 1, 0]]
     >>> pairwise_distances_argmin(X, Y)
     array([0, 1])
     """
-    if metric_kwargs is None:
-        metric_kwargs = {}
-
     X, Y = check_pairwise_arrays(X, Y)
 
     if axis == 0:
         X, Y = Y, X
 
     if metric_kwargs is None:
         metric_kwargs = {}
@@ -1108,15 +1119,15 @@
     S = cosine_similarity(X, Y)
     S *= -1
     S += 1
     np.clip(S, 0, 2, out=S)
     if X is Y or Y is None:
         # Ensure that distances between vectors and themselves are set to 0.0.
         # This may not be the case due to floating point rounding errors.
-        S[np.diag_indices_from(S)] = 0.0
+        np.fill_diagonal(S, 0.0)
     return S
 
 
 # Paired distances
 @validate_params(
     {"X": ["array-like", "sparse matrix"], "Y": ["array-like", "sparse matrix"]},
     prefer_skip_nested_validation=True,
@@ -1636,15 +1647,15 @@
         ``False``, the output is sparse if both input arrays are sparse.
 
         .. versionadded:: 0.17
            parameter ``dense_output`` for dense output.
 
     Returns
     -------
-    similarities : ndarray of shape (n_samples_X, n_samples_Y)
+    similarities : ndarray or sparse matrix of shape (n_samples_X, n_samples_Y)
         Returns the cosine similarity between samples in X and Y.
 
     Examples
     --------
     >>> from sklearn.metrics.pairwise import cosine_similarity
     >>> X = [[0, 0, 0], [1, 1, 1]]
     >>> Y = [[1, 0, 0], [1, 1, 0]]
@@ -1884,15 +1895,21 @@
         np.fill_diagonal(ret, 0)
 
     return ret
 
 
 def _pairwise_callable(X, Y, metric, force_all_finite=True, **kwds):
     """Handle the callable case for pairwise_{distances,kernels}."""
-    X, Y = check_pairwise_arrays(X, Y, force_all_finite=force_all_finite)
+    X, Y = check_pairwise_arrays(
+        X,
+        Y,
+        dtype=None,
+        force_all_finite=force_all_finite,
+        ensure_2d=False,
+    )
 
     if X is Y:
         # Only calculate metric for upper triangle
         out = np.zeros((X.shape[0], Y.shape[0]), dtype="float")
         iterator = itertools.combinations(range(X.shape[0]), 2)
         for i, j in iterator:
             # scipy has not yet implemented 1D sparse slices; once implemented this can
@@ -2162,21 +2179,30 @@
         "metric": [StrOptions(set(_VALID_METRICS) | {"precomputed"}), callable],
         "n_jobs": [Integral, None],
         "force_all_finite": ["boolean", StrOptions({"allow-nan"})],
     },
     prefer_skip_nested_validation=True,
 )
 def pairwise_distances(
-    X, Y=None, metric="euclidean", *, n_jobs=None, force_all_finite=True, **kwds
+    X,
+    Y=None,
+    metric="euclidean",
+    *,
+    n_jobs=None,
+    force_all_finite=True,
+    **kwds,
 ):
     """Compute the distance matrix from a vector array X and optional Y.
 
     This method takes either a vector array or a distance matrix, and returns
-    a distance matrix. If the input is a vector array, the distances are
-    computed. If the input is a distances matrix, it is returned instead.
+    a distance matrix.
+    If the input is a vector array, the distances are computed.
+    If the input is a distances matrix, it is returned instead.
+    If the input is a collection of non-numeric data (e.g. a list of strings or a
+    boolean array), a custom metric must be passed.
 
     This method provides a safe way to take a distance matrix as input, while
     preserving compatibility with many other algorithms that take a vector
     array.
 
     If Y is given (default is None), then the returned matrix is the pairwise
     distance between the arrays from both X and Y.
@@ -2238,14 +2264,18 @@
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
 
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
 
+        The "euclidean" and "cosine" metrics rely heavily on BLAS which is already
+        multithreaded. So, increasing `n_jobs` would likely cause oversubscription
+        and quickly degrade performance.
+
     force_all_finite : bool or 'allow-nan', default=True
         Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored
         for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The
         possibilities are:
 
         - True: Force all values of array to be finite.
         - False: accepts np.inf, np.nan, pd.NA in array.
@@ -2300,21 +2330,24 @@
         )
         check_non_negative(X, whom=whom)
         return X
     elif metric in PAIRWISE_DISTANCE_FUNCTIONS:
         func = PAIRWISE_DISTANCE_FUNCTIONS[metric]
     elif callable(metric):
         func = partial(
-            _pairwise_callable, metric=metric, force_all_finite=force_all_finite, **kwds
+            _pairwise_callable,
+            metric=metric,
+            force_all_finite=force_all_finite,
+            **kwds,
         )
     else:
         if issparse(X) or issparse(Y):
             raise TypeError("scipy distance metrics do not support sparse matrices.")
 
-        dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None
+        dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else "infer_float"
 
         if dtype == bool and (X.dtype != bool or (Y is not None and Y.dtype != bool)):
             msg = "Data was converted to boolean for metric %s" % metric
             warnings.warn(msg, DataConversionWarning)
 
         X, Y = check_pairwise_arrays(
             X, Y, dtype=dtype, force_all_finite=force_all_finite
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_classification.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_classification.py`

 * *Files 4% similar despite different names*

```diff
@@ -31,15 +31,15 @@
     matthews_corrcoef,
     multilabel_confusion_matrix,
     precision_recall_fscore_support,
     precision_score,
     recall_score,
     zero_one_loss,
 )
-from sklearn.metrics._classification import _check_targets
+from sklearn.metrics._classification import _check_targets, d2_log_loss_score
 from sklearn.model_selection import cross_val_score
 from sklearn.preprocessing import LabelBinarizer, label_binarize
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.utils._mocking import MockDataFrame
 from sklearn.utils._testing import (
     assert_allclose,
     assert_almost_equal,
@@ -84,24 +84,24 @@
 
     # add noisy features to make the problem harder and avoid perfect results
     rng = np.random.RandomState(0)
     X = np.c_[X, rng.randn(n_samples, 200 * n_features)]
 
     # run classifier, get class probabilities and label predictions
     clf = svm.SVC(kernel="linear", probability=True, random_state=0)
-    probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])
+    y_pred_proba = clf.fit(X[:half], y[:half]).predict_proba(X[half:])
 
     if binary:
         # only interested in probabilities of the positive case
         # XXX: do we really want a special API for the binary case?
-        probas_pred = probas_pred[:, 1]
+        y_pred_proba = y_pred_proba[:, 1]
 
     y_pred = clf.predict(X[half:])
     y_true = y[half:]
-    return y_true, y_pred, probas_pred
+    return y_true, y_pred, y_pred_proba
 
 
 ###############################################################################
 # Tests
 
 
 def test_classification_report_dictionary_output():
@@ -211,14 +211,37 @@
             for item in record:
                 msg = "Use `zero_division` parameter to control this behavior."
                 assert msg in str(item.message)
         else:
             assert not record
 
 
+@pytest.mark.parametrize(
+    "labels, show_micro_avg", [([0], True), ([0, 1], False), ([0, 1, 2], False)]
+)
+def test_classification_report_labels_subset_superset(labels, show_micro_avg):
+    """Check the behaviour of passing `labels` as a superset or subset of the labels.
+    WHen a superset, we expect to show the "accuracy" in the report while it should be
+    the micro-averaging if this is a subset.
+
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/27927
+    """
+
+    y_true, y_pred = [0, 1], [0, 1]
+
+    report = classification_report(y_true, y_pred, labels=labels, output_dict=True)
+    if show_micro_avg:
+        assert "micro avg" in report
+        assert "accuracy" not in report
+    else:  # accuracy should be shown
+        assert "accuracy" in report
+        assert "micro avg" not in report
+
+
 def test_multilabel_accuracy_score_subset_accuracy():
     # Dense label indicator matrix format
     y1 = np.array([[0, 1, 1], [1, 0, 1]])
     y2 = np.array([[0, 0, 1], [1, 0, 1]])
 
     assert accuracy_score(y1, y2) == 0.5
     assert accuracy_score(y1, y1) == 1
@@ -2190,28 +2213,26 @@
             np.array([[0, 0], [0, 0]]),
             np.array([[1, 1], [1, 1]]),
             average="micro",
             zero_division=zero_division,
         )
         if zero_division == "warn":
             assert (
-                str(record.pop().message)
-                == "Recall is ill-defined and "
+                str(record.pop().message) == "Recall is ill-defined and "
                 "being set to 0.0 due to no true samples."
                 " Use `zero_division` parameter to control"
                 " this behavior."
             )
         else:
             assert len(record) == 0
 
         recall_score([0, 0], [0, 0])
         if zero_division == "warn":
             assert (
-                str(record.pop().message)
-                == "Recall is ill-defined and "
+                str(record.pop().message) == "Recall is ill-defined and "
                 "being set to 0.0 due to no true samples."
                 " Use `zero_division` parameter to control"
                 " this behavior."
             )
 
 
 @pytest.mark.parametrize("zero_division", ["warn", 0, 1, np.nan])
@@ -2222,28 +2243,26 @@
             np.array([[1, 1], [1, 1]]),
             np.array([[0, 0], [0, 0]]),
             average="micro",
             zero_division=zero_division,
         )
         if zero_division == "warn":
             assert (
-                str(record.pop().message)
-                == "Precision is ill-defined and "
+                str(record.pop().message) == "Precision is ill-defined and "
                 "being set to 0.0 due to no predicted samples."
                 " Use `zero_division` parameter to control"
                 " this behavior."
             )
         else:
             assert len(record) == 0
 
         precision_score([0, 0], [0, 0])
         if zero_division == "warn":
             assert (
-                str(record.pop().message)
-                == "Precision is ill-defined and "
+                str(record.pop().message) == "Precision is ill-defined and "
                 "being set to 0.0 due to no predicted samples."
                 " Use `zero_division` parameter to control"
                 " this behavior."
             )
 
     assert_no_warnings(
         precision_score,
@@ -2280,16 +2299,15 @@
                 np.array([[0, 0], [0, 0]]),
                 np.array([[0, 0], [0, 0]]),
                 average="micro",
                 zero_division=zero_division,
             )
             if zero_division == "warn":
                 assert (
-                    str(record.pop().message)
-                    == "F-score is ill-defined and "
+                    str(record.pop().message) == "F-score is ill-defined and "
                     "being set to 0.0 due to no true nor predicted "
                     "samples. Use `zero_division` parameter to "
                     "control this behavior."
                 )
             else:
                 assert len(record) == 0
 
@@ -2602,135 +2620,125 @@
     # binary case with symbolic labels ("no" < "yes")
     y_true = ["no", "no", "no", "yes", "yes", "yes"]
     y_pred = np.array(
         [[0.5, 0.5], [0.1, 0.9], [0.01, 0.99], [0.9, 0.1], [0.75, 0.25], [0.001, 0.999]]
     )
     loss = log_loss(y_true, y_pred)
     loss_true = -np.mean(bernoulli.logpmf(np.array(y_true) == "yes", y_pred[:, 1]))
-    assert_almost_equal(loss, loss_true)
+    assert_allclose(loss, loss_true)
 
     # multiclass case; adapted from http://bit.ly/RJJHWA
     y_true = [1, 0, 2]
     y_pred = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]
     loss = log_loss(y_true, y_pred, normalize=True)
-    assert_almost_equal(loss, 0.6904911)
+    assert_allclose(loss, 0.6904911)
 
     # check that we got all the shapes and axes right
     # by doubling the length of y_true and y_pred
     y_true *= 2
     y_pred *= 2
     loss = log_loss(y_true, y_pred, normalize=False)
-    assert_almost_equal(loss, 0.6904911 * 6, decimal=6)
-
-    user_warning_msg = "y_pred values do not sum to one"
-    # check eps and handling of absolute zero and one probabilities
-    y_pred = np.asarray(y_pred) > 0.5
-    with pytest.warns(FutureWarning):
-        loss = log_loss(y_true, y_pred, normalize=True, eps=0.1)
-    with pytest.warns(UserWarning, match=user_warning_msg):
-        assert_almost_equal(loss, log_loss(y_true, np.clip(y_pred, 0.1, 0.9)))
-
-    # binary case: check correct boundary values for eps = 0
-    with pytest.warns(FutureWarning):
-        assert log_loss([0, 1], [0, 1], eps=0) == 0
-    with pytest.warns(FutureWarning):
-        assert log_loss([0, 1], [0, 0], eps=0) == np.inf
-    with pytest.warns(FutureWarning):
-        assert log_loss([0, 1], [1, 1], eps=0) == np.inf
-
-    # multiclass case: check correct boundary values for eps = 0
-    with pytest.warns(FutureWarning):
-        assert log_loss([0, 1, 2], [[1, 0, 0], [0, 1, 0], [0, 0, 1]], eps=0) == 0
-    with pytest.warns(FutureWarning):
-        assert (
-            log_loss([0, 1, 2], [[0, 0.5, 0.5], [0, 1, 0], [0, 0, 1]], eps=0) == np.inf
-        )
+    assert_allclose(loss, 0.6904911 * 6)
 
     # raise error if number of classes are not equal.
     y_true = [1, 0, 2]
-    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1]]
+    y_pred = [[0.3, 0.7], [0.6, 0.4], [0.4, 0.6]]
     with pytest.raises(ValueError):
         log_loss(y_true, y_pred)
 
     # case when y_true is a string array object
     y_true = ["ham", "spam", "spam", "ham"]
-    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]]
-    with pytest.warns(UserWarning, match=user_warning_msg):
-        loss = log_loss(y_true, y_pred)
-    assert_almost_equal(loss, 1.0383217, decimal=6)
+    y_pred = [[0.3, 0.7], [0.6, 0.4], [0.4, 0.6], [0.7, 0.3]]
+    loss = log_loss(y_true, y_pred)
+    assert_allclose(loss, 0.7469410)
 
     # test labels option
 
     y_true = [2, 2]
-    y_pred = [[0.2, 0.7], [0.6, 0.5]]
+    y_pred = [[0.2, 0.8], [0.6, 0.4]]
     y_score = np.array([[0.1, 0.9], [0.1, 0.9]])
     error_str = (
         r"y_true contains only one label \(2\). Please provide "
         r"the true labels explicitly through the labels argument."
     )
     with pytest.raises(ValueError, match=error_str):
         log_loss(y_true, y_pred)
 
-    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.2, 0.3]]
-    error_str = "Found input variables with inconsistent numbers of samples: [3, 2]"
-    (ValueError, error_str, log_loss, y_true, y_pred)
+    y_pred = [[0.2, 0.8], [0.6, 0.4], [0.7, 0.3]]
+    error_str = r"Found input variables with inconsistent numbers of samples: \[3, 2\]"
+    with pytest.raises(ValueError, match=error_str):
+        log_loss(y_true, y_pred)
 
     # works when the labels argument is used
 
     true_log_loss = -np.mean(np.log(y_score[:, 1]))
     calculated_log_loss = log_loss(y_true, y_score, labels=[1, 2])
-    assert_almost_equal(calculated_log_loss, true_log_loss)
+    assert_allclose(calculated_log_loss, true_log_loss)
 
     # ensure labels work when len(np.unique(y_true)) != y_pred.shape[1]
     y_true = [1, 2, 2]
-    y_score2 = [[0.2, 0.7, 0.3], [0.6, 0.5, 0.3], [0.3, 0.9, 0.1]]
-    with pytest.warns(UserWarning, match=user_warning_msg):
-        loss = log_loss(y_true, y_score2, labels=[1, 2, 3])
-    assert_almost_equal(loss, 1.0630345, decimal=6)
+    y_score2 = [[0.7, 0.1, 0.2], [0.2, 0.7, 0.1], [0.1, 0.7, 0.2]]
+    loss = log_loss(y_true, y_score2, labels=[1, 2, 3])
+    assert_allclose(loss, -np.log(0.7))
+
 
+@pytest.mark.parametrize("dtype", [np.float64, np.float32, np.float16])
+def test_log_loss_eps(dtype):
+    """Check the behaviour internal eps that changes depending on the input dtype.
 
-def test_log_loss_eps_auto(global_dtype):
-    """Check the behaviour of `eps="auto"` that changes depending on the input
-    array dtype.
     Non-regression test for:
     https://github.com/scikit-learn/scikit-learn/issues/24315
     """
-    y_true = np.array([0, 1], dtype=global_dtype)
-    y_pred = y_true.copy()
+    y_true = np.array([0, 1], dtype=dtype)
+    y_pred = np.array([1, 0], dtype=dtype)
 
-    loss = log_loss(y_true, y_pred, eps="auto")
+    loss = log_loss(y_true, y_pred)
     assert np.isfinite(loss)
 
 
-def test_log_loss_eps_auto_float16():
-    """Check the behaviour of `eps="auto"` for np.float16"""
-    y_true = np.array([0, 1], dtype=np.float16)
-    y_pred = y_true.copy()
+@pytest.mark.parametrize("dtype", [np.float64, np.float32, np.float16])
+def test_log_loss_not_probabilities_warning(dtype):
+    """Check that log_loss raises a warning when y_pred values don't sum to 1."""
+    y_true = np.array([0, 1, 1, 0])
+    y_pred = np.array([[0.2, 0.7], [0.6, 0.3], [0.4, 0.7], [0.8, 0.3]], dtype=dtype)
 
-    loss = log_loss(y_true, y_pred, eps="auto")
-    assert np.isfinite(loss)
+    with pytest.warns(UserWarning, match="The y_pred values do not sum to one."):
+        log_loss(y_true, y_pred)
+
+
+@pytest.mark.parametrize(
+    "y_true, y_pred",
+    [
+        ([0, 1, 0], [0, 1, 0]),
+        ([0, 1, 0], [[1, 0], [0, 1], [1, 0]]),
+        ([0, 1, 2], [[1, 0, 0], [0, 1, 0], [0, 0, 1]]),
+    ],
+)
+def test_log_loss_perfect_predictions(y_true, y_pred):
+    """Check that log_loss returns 0 for perfect predictions."""
+    # Because of the clipping, the result is not exactly 0
+    assert log_loss(y_true, y_pred) == pytest.approx(0)
 
 
 def test_log_loss_pandas_input():
     # case when input is a pandas series and dataframe gh-5715
     y_tr = np.array(["ham", "spam", "spam", "ham"])
-    y_pr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])
+    y_pr = np.array([[0.3, 0.7], [0.6, 0.4], [0.4, 0.6], [0.7, 0.3]])
     types = [(MockDataFrame, MockDataFrame)]
     try:
         from pandas import DataFrame, Series
 
         types.append((Series, DataFrame))
     except ImportError:
         pass
     for TrueInputType, PredInputType in types:
         # y_pred dataframe, y_true series
         y_true, y_pred = TrueInputType(y_tr), PredInputType(y_pr)
-        with pytest.warns(UserWarning, match="y_pred values do not sum to one"):
-            loss = log_loss(y_true, y_pred)
-        assert_almost_equal(loss, 1.0383217, decimal=6)
+        loss = log_loss(y_true, y_pred)
+        assert_allclose(loss, 0.7469410)
 
 
 def test_brier_score_loss():
     # Check brier_score_loss function
     y_true = np.array([0, 1, 1, 0, 1, 1])
     y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1.0, 0.95])
     true_score = linalg.norm(y_true - y_pred) ** 2 / len(y_true)
@@ -2860,7 +2868,228 @@
 
     Non-regression test for:
     https://github.com/scikit-learn/scikit-learn/issues/27563
     """
     X, y = datasets.make_classification(random_state=0)
     classifier = DecisionTreeClassifier(max_depth=3, random_state=0).fit(X, y)
     cross_val_score(classifier, X, y, scoring=scoring, n_jobs=2, error_score="raise")
+
+
+# TODO(1.7): remove
+def test_brier_score_loss_deprecation_warning():
+    """Check the message for future deprecation."""
+    # Check brier_score_loss function
+    y_true = np.array([0, 1, 1, 0, 1, 1])
+    y_pred = np.array([0.1, 0.8, 0.9, 0.3, 1.0, 0.95])
+
+    warn_msg = "y_prob was deprecated in version 1.5"
+    with pytest.warns(FutureWarning, match=warn_msg):
+        brier_score_loss(
+            y_true,
+            y_prob=y_pred,
+        )
+
+    error_msg = "`y_prob` and `y_proba` cannot be both specified"
+    with pytest.raises(ValueError, match=error_msg):
+        brier_score_loss(
+            y_true,
+            y_prob=y_pred,
+            y_proba=y_pred,
+        )
+
+
+def test_d2_log_loss_score():
+    y_true = [0, 0, 0, 1, 1, 1]
+    y_true_string = ["no", "no", "no", "yes", "yes", "yes"]
+    y_pred = np.array(
+        [
+            [0.5, 0.5],
+            [0.9, 0.1],
+            [0.4, 0.6],
+            [0.6, 0.4],
+            [0.35, 0.65],
+            [0.01, 0.99],
+        ]
+    )
+    y_pred_null = np.array(
+        [
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+        ]
+    )
+    d2_score = d2_log_loss_score(y_true=y_true, y_pred=y_pred)
+    log_likelihood = log_loss(y_true=y_true, y_pred=y_pred, normalize=False)
+    log_likelihood_null = log_loss(y_true=y_true, y_pred=y_pred_null, normalize=False)
+    d2_score_true = 1 - log_likelihood / log_likelihood_null
+    assert d2_score == pytest.approx(d2_score_true)
+
+    # check that using sample weight also gives the correct d2 score
+    sample_weight = np.array([2, 1, 3, 4, 3, 1])
+    y_pred_null[:, 0] = sample_weight[:3].sum() / sample_weight.sum()
+    y_pred_null[:, 1] = sample_weight[3:].sum() / sample_weight.sum()
+    d2_score = d2_log_loss_score(
+        y_true=y_true, y_pred=y_pred, sample_weight=sample_weight
+    )
+    log_likelihood = log_loss(
+        y_true=y_true,
+        y_pred=y_pred,
+        sample_weight=sample_weight,
+        normalize=False,
+    )
+    log_likelihood_null = log_loss(
+        y_true=y_true,
+        y_pred=y_pred_null,
+        sample_weight=sample_weight,
+        normalize=False,
+    )
+    d2_score_true = 1 - log_likelihood / log_likelihood_null
+    assert d2_score == pytest.approx(d2_score_true)
+
+    # check if good predictions give a relatively higher value for the d2 score
+    y_pred = np.array(
+        [
+            [0.9, 0.1],
+            [0.8, 0.2],
+            [0.9, 0.1],
+            [0.1, 0.9],
+            [0.2, 0.8],
+            [0.1, 0.9],
+        ]
+    )
+    d2_score = d2_log_loss_score(y_true, y_pred)
+    assert 0.5 < d2_score < 1.0
+    # check that a similar value is obtained for string labels
+    d2_score_string = d2_log_loss_score(y_true_string, y_pred)
+    assert d2_score_string == pytest.approx(d2_score)
+
+    # check if poor predictions gives a relatively low value for the d2 score
+    y_pred = np.array(
+        [
+            [0.5, 0.5],
+            [0.1, 0.9],
+            [0.1, 0.9],
+            [0.9, 0.1],
+            [0.75, 0.25],
+            [0.1, 0.9],
+        ]
+    )
+    d2_score = d2_log_loss_score(y_true, y_pred)
+    assert d2_score < 0
+    # check that a similar value is obtained for string labels
+    d2_score_string = d2_log_loss_score(y_true_string, y_pred)
+    assert d2_score_string == pytest.approx(d2_score)
+
+    # check if simply using the average of the classes as the predictions
+    # gives a d2 score of 0
+    y_true = [0, 0, 0, 1, 1, 1]
+    y_pred = np.array(
+        [
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+            [0.5, 0.5],
+        ]
+    )
+    d2_score = d2_log_loss_score(y_true, y_pred)
+    assert d2_score == 0
+    d2_score_string = d2_log_loss_score(y_true_string, y_pred)
+    assert d2_score_string == 0
+
+    # check if simply using the average of the classes as the predictions
+    # gives a d2 score of 0 when the positive class has a higher proportion
+    y_true = [0, 1, 1, 1]
+    y_true_string = ["no", "yes", "yes", "yes"]
+    y_pred = np.array([[0.25, 0.75], [0.25, 0.75], [0.25, 0.75], [0.25, 0.75]])
+    d2_score = d2_log_loss_score(y_true, y_pred)
+    assert d2_score == 0
+    d2_score_string = d2_log_loss_score(y_true_string, y_pred)
+    assert d2_score_string == 0
+    sample_weight = [2, 2, 2, 2]
+    d2_score_with_sample_weight = d2_log_loss_score(
+        y_true, y_pred, sample_weight=sample_weight
+    )
+    assert d2_score_with_sample_weight == 0
+
+    # check that the d2 scores seem correct when more than 2
+    # labels are specified
+    y_true = ["high", "high", "low", "neutral"]
+    sample_weight = [1.4, 0.6, 0.8, 0.2]
+
+    y_pred = np.array(
+        [
+            [0.8, 0.1, 0.1],
+            [0.8, 0.1, 0.1],
+            [0.1, 0.8, 0.1],
+            [0.1, 0.1, 0.8],
+        ]
+    )
+    d2_score = d2_log_loss_score(y_true, y_pred)
+    assert 0.5 < d2_score < 1.0
+    d2_score = d2_log_loss_score(y_true, y_pred, sample_weight=sample_weight)
+    assert 0.5 < d2_score < 1.0
+
+    y_pred = np.array(
+        [
+            [0.2, 0.5, 0.3],
+            [0.1, 0.7, 0.2],
+            [0.1, 0.1, 0.8],
+            [0.2, 0.7, 0.1],
+        ]
+    )
+    d2_score = d2_log_loss_score(y_true, y_pred)
+    assert d2_score < 0
+    d2_score = d2_log_loss_score(y_true, y_pred, sample_weight=sample_weight)
+    assert d2_score < 0
+
+
+def test_d2_log_loss_score_raises():
+    """Test that d2_log_loss raises error on invalid input."""
+    y_true = [0, 1, 2]
+    y_pred = [[0.2, 0.8], [0.5, 0.5], [0.4, 0.6]]
+    err = "contain different number of classes"
+    with pytest.raises(ValueError, match=err):
+        d2_log_loss_score(y_true, y_pred)
+
+    # check error if the number of classes in labels do not match the number
+    # of classes in y_pred.
+    y_true = ["a", "b", "c"]
+    y_pred = [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]
+    labels = [0, 1, 2]
+    err = "number of classes in labels is different"
+    with pytest.raises(ValueError, match=err):
+        d2_log_loss_score(y_true, y_pred, labels=labels)
+
+    # check error if y_true and y_pred do not have equal lengths
+    y_true = [0, 1, 2]
+    y_pred = [[0.5, 0.5, 0.5], [0.6, 0.3, 0.1]]
+    err = "inconsistent numbers of samples"
+    with pytest.raises(ValueError, match=err):
+        d2_log_loss_score(y_true, y_pred)
+
+    # check warning for samples < 2
+    y_true = [1]
+    y_pred = [[0.5, 0.5]]
+    err = "score is not well-defined"
+    with pytest.warns(UndefinedMetricWarning, match=err):
+        d2_log_loss_score(y_true, y_pred)
+
+    # check error when y_true only has 1 label
+    y_true = [1, 1, 1]
+    y_pred = [[0.5, 0.5], [0.5, 0.5], [0.5, 5]]
+    err = "y_true contains only one label"
+    with pytest.raises(ValueError, match=err):
+        d2_log_loss_score(y_true, y_pred)
+
+    # check error when y_true only has 1 label and labels also has
+    # only 1 label
+    y_true = [1, 1, 1]
+    labels = [1]
+    y_pred = [[0.5, 0.5], [0.5, 0.5], [0.5, 5]]
+    err = "The labels array needs to contain at least two"
+    with pytest.raises(ValueError, match=err):
+        d2_log_loss_score(y_true, y_pred, labels=labels)
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,14 +51,15 @@
     zero_one_loss,
 )
 from sklearn.metrics._base import _average_binary_score
 from sklearn.preprocessing import LabelBinarizer
 from sklearn.utils import shuffle
 from sklearn.utils._array_api import (
     _atol_for_type,
+    _convert_to_numpy,
     yield_namespace_device_dtype_combinations,
 )
 from sklearn.utils._testing import (
     _array_api_for_tests,
     assert_allclose,
     assert_almost_equal,
     assert_array_equal,
@@ -632,15 +633,18 @@
 @ignore_warnings
 def test_sample_order_invariance_multilabel_and_multioutput():
     random_state = check_random_state(0)
 
     # Generate some data
     y_true = random_state.randint(0, 2, size=(20, 25))
     y_pred = random_state.randint(0, 2, size=(20, 25))
-    y_score = random_state.normal(size=y_true.shape)
+    y_score = random_state.uniform(size=y_true.shape)
+
+    # Some metrics (e.g. log_loss) require y_score to be probabilities (sum to 1)
+    y_score /= y_score.sum(axis=1, keepdims=True)
 
     y_true_shuffle, y_pred_shuffle, y_score_shuffle = shuffle(
         y_true, y_pred, y_score, random_state=0
     )
 
     for name in MULTILABELS_METRICS:
         metric = ALL_METRICS[name]
@@ -1561,15 +1565,18 @@
         n_features=1, n_classes=10, random_state=0, n_samples=50, allow_unlabeled=False
     )
     _, yb = make_multilabel_classification(
         n_features=1, n_classes=10, random_state=1, n_samples=50, allow_unlabeled=False
     )
     y_true = np.vstack([ya, yb])
     y_pred = np.vstack([ya, ya])
-    y_score = random_state.randint(1, 4, size=y_true.shape)
+    y_score = random_state.uniform(size=y_true.shape)
+
+    # Some metrics (e.g. log_loss) require y_score to be probabilities (sum to 1)
+    y_score /= y_score.sum(axis=1, keepdims=True)
 
     metric = ALL_METRICS[name]
     if name in THRESHOLDED_METRICS:
         check_sample_weight_invariance(name, metric, y_true, y_score)
     else:
         check_sample_weight_invariance(name, metric, y_true, y_pred)
 
@@ -1624,15 +1631,18 @@
 @pytest.mark.parametrize(
     "name", sorted(THRESHOLDED_MULTILABEL_METRICS | MULTIOUTPUT_METRICS)
 )
 def test_thresholded_multilabel_multioutput_permutations_invariance(name):
     random_state = check_random_state(0)
     n_samples, n_classes = 20, 4
     y_true = random_state.randint(0, 2, size=(n_samples, n_classes))
-    y_score = random_state.normal(size=y_true.shape)
+    y_score = random_state.uniform(size=y_true.shape)
+
+    # Some metrics (e.g. log_loss) require y_score to be probabilities (sum to 1)
+    y_score /= y_score.sum(axis=1, keepdims=True)
 
     # Makes sure all samples have at least one label. This works around errors
     # when running metrics where average="sample"
     y_true[y_true.sum(1) == 4, 0] = 0
     y_true[y_true.sum(1) == 0, 0] = 1
 
     metric = ALL_METRICS[name]
@@ -1745,15 +1755,15 @@
     if sample_weight is not None:
         sample_weight = xp.asarray(sample_weight, device=device)
 
     with config_context(array_api_dispatch=True):
         metric_xp = metric(y_true_xp, y_pred_xp, sample_weight=sample_weight)
 
         assert_allclose(
-            metric_xp,
+            _convert_to_numpy(xp.asarray(metric_xp), xp),
             metric_np,
             atol=_atol_for_type(dtype_name),
         )
 
 
 def check_array_api_binary_classification_metric(
     metric, array_namespace, device, dtype_name
@@ -1809,23 +1819,51 @@
         dtype_name,
         y_true_np=y_true_np,
         y_pred_np=y_pred_np,
         sample_weight=sample_weight,
     )
 
 
+def check_array_api_regression_metric(metric, array_namespace, device, dtype_name):
+    y_true_np = np.array([[1, 3], [1, 2]], dtype=dtype_name)
+    y_pred_np = np.array([[1, 4], [1, 1]], dtype=dtype_name)
+
+    check_array_api_metric(
+        metric,
+        array_namespace,
+        device,
+        dtype_name,
+        y_true_np=y_true_np,
+        y_pred_np=y_pred_np,
+        sample_weight=None,
+    )
+
+    sample_weight = np.array([0.1, 2.0], dtype=dtype_name)
+
+    check_array_api_metric(
+        metric,
+        array_namespace,
+        device,
+        dtype_name,
+        y_true_np=y_true_np,
+        y_pred_np=y_pred_np,
+        sample_weight=sample_weight,
+    )
+
+
 array_api_metric_checkers = {
     accuracy_score: [
         check_array_api_binary_classification_metric,
         check_array_api_multiclass_classification_metric,
     ],
     zero_one_loss: [
         check_array_api_binary_classification_metric,
         check_array_api_multiclass_classification_metric,
     ],
+    r2_score: [check_array_api_regression_metric],
 }
 
 
 def yield_metric_checker_combinations(metric_checkers=array_api_metric_checkers):
     for metric, checkers in metric_checkers.items():
         for checker in checkers:
             yield metric, checker
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_dist_metrics.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_dist_metrics.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_pairwise.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_pairwise.py`

 * *Files 1% similar despite different names*

```diff
@@ -1604,14 +1604,62 @@
             params = {"VI": np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T}
 
     dist = pairwise_distances(X, Y, metric=metric, **params)
 
     assert_allclose(dist, expected_dist)
 
 
+@pytest.mark.parametrize(
+    "X,Y,expected_distance",
+    [
+        (
+            ["a", "ab", "abc"],
+            None,
+            [[0.0, 1.0, 2.0], [1.0, 0.0, 1.0], [2.0, 1.0, 0.0]],
+        ),
+        (
+            ["a", "ab", "abc"],
+            ["a", "ab"],
+            [[0.0, 1.0], [1.0, 0.0], [2.0, 1.0]],
+        ),
+    ],
+)
+def test_pairwise_dist_custom_metric_for_string(X, Y, expected_distance):
+    """Check pairwise_distances with lists of strings as input."""
+
+    def dummy_string_similarity(x, y):
+        return np.abs(len(x) - len(y))
+
+    actual_distance = pairwise_distances(X=X, Y=Y, metric=dummy_string_similarity)
+    assert_allclose(actual_distance, expected_distance)
+
+
+def test_pairwise_dist_custom_metric_for_bool():
+    """Check that pairwise_distances does not convert boolean input to float
+    when using a custom metric.
+    """
+
+    def dummy_bool_dist(v1, v2):
+        # dummy distance func using `&` and thus relying on the input data being boolean
+        return 1 - (v1 & v2).sum() / (v1 | v2).sum()
+
+    X = np.array([[1, 0, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1]], dtype=bool)
+
+    expected_distance = np.array(
+        [
+            [0.0, 0.5, 0.75],
+            [0.5, 0.0, 0.5],
+            [0.75, 0.5, 0.0],
+        ]
+    )
+
+    actual_distance = pairwise_distances(X=X, metric=dummy_bool_dist)
+    assert_allclose(actual_distance, expected_distance)
+
+
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_sparse_manhattan_readonly_dataset(csr_container):
     # Non-regression test for: https://github.com/scikit-learn/scikit-learn/issues/7981
     matrices1 = [csr_container(np.ones((5, 5)))]
     matrices2 = [csr_container(np.ones((5, 5)))]
     # Joblib memory maps datasets which makes them read-only.
     # The following call was reporting as failing in #7981, but this must pass.
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_pairwise_distances_reduction.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_pairwise_distances_reduction.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 import itertools
 import re
 import warnings
 from functools import partial
 
 import numpy as np
 import pytest
-import threadpoolctl
 from scipy.spatial.distance import cdist
 
+from sklearn import _threadpool_controller
 from sklearn.metrics import euclidean_distances, pairwise_distances
 from sklearn.metrics._pairwise_distances_reduction import (
     ArgKmin,
     ArgKminClassMode,
     BaseDistancesReductionDispatcher,
     RadiusNeighbors,
     RadiusNeighborsClassMode,
@@ -1196,15 +1196,15 @@
         Y,
         parameter,
         chunk_size=25,  # make sure we use multiple threads
         return_distance=True,
         **compute_parameters,
     )
 
-    with threadpoolctl.threadpool_limits(limits=1, user_api="openmp"):
+    with _threadpool_controller.limit(limits=1, user_api="openmp"):
         dist, indices = Dispatcher.compute(
             X,
             Y,
             parameter,
             chunk_size=25,
             return_distance=True,
             **compute_parameters,
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_ranking.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_ranking.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,18 +25,20 @@
     top_k_accuracy_score,
 )
 from sklearn.metrics._ranking import _dcg_sample_scores, _ndcg_sample_scores
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import label_binarize
 from sklearn.random_projection import _sparse_random_matrix
 from sklearn.utils._testing import (
+    _convert_container,
     assert_allclose,
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
+    ignore_warnings,
 )
 from sklearn.utils.extmath import softmax
 from sklearn.utils.fixes import CSR_CONTAINERS
 from sklearn.utils.validation import (
     check_array,
     check_consistent_length,
     check_random_state,
@@ -860,34 +862,40 @@
     )
     with pytest.raises(ValueError, match=msg):
         curve_func(np.array(["a", "b"], dtype="<U1"), [0.0, 1.0])
 
     with pytest.raises(ValueError, match=msg):
         curve_func(np.array(["a", "b"], dtype=object), [0.0, 1.0])
 
-    # The error message is slightly different for bytes-encoded
-    # class labels, but otherwise the behavior is the same:
-    msg = (
-        "y_true takes value in {b'a', b'b'} and pos_label is "
-        "not specified: either make y_true take "
-        "value in {0, 1} or {-1, 1} or pass pos_label "
-        "explicitly."
-    )
-    with pytest.raises(ValueError, match=msg):
-        curve_func(np.array([b"a", b"b"], dtype="<S1"), [0.0, 1.0])
-
     # Check that it is possible to use floating point class labels
     # that are interpreted similarly to integer class labels:
     y_pred = [0.0, 1.0, 0.2, 0.42]
     int_curve = curve_func([0, 1, 1, 0], y_pred)
     float_curve = curve_func([0.0, 1.0, 1.0, 0.0], y_pred)
     for int_curve_part, float_curve_part in zip(int_curve, float_curve):
         np.testing.assert_allclose(int_curve_part, float_curve_part)
 
 
+# TODO(1.7): Update test to check for error when bytes support is removed.
+@ignore_warnings(category=FutureWarning)
+@pytest.mark.parametrize("curve_func", [precision_recall_curve, roc_curve])
+@pytest.mark.parametrize("labels_type", ["list", "array"])
+def test_binary_clf_curve_implicit_bytes_pos_label(curve_func, labels_type):
+    # Check that using bytes class labels raises an informative
+    # error for any supported string dtype:
+    labels = _convert_container([b"a", b"b"], labels_type)
+    msg = (
+        "y_true takes value in {b'a', b'b'} and pos_label is not "
+        "specified: either make y_true take value in {0, 1} or "
+        "{-1, 1} or pass pos_label explicitly."
+    )
+    with pytest.raises(ValueError, match=msg):
+        curve_func(labels, [0.0, 1.0])
+
+
 @pytest.mark.parametrize("curve_func", CURVE_FUNCS)
 def test_binary_clf_curve_zero_sample_weight(curve_func):
     y_true = [0, 0, 1, 1, 1]
     y_score = [0.1, 0.2, 0.3, 0.4, 0.5]
     sample_weight = [1, 1, 1, 0.5, 0]
 
     result_1 = curve_func(y_true, y_score, sample_weight=sample_weight)
@@ -2238,7 +2246,29 @@
     https://github.com/scikit-learn/scikit-learn/issues/26193
     """
     rng = np.random.RandomState(global_random_seed)
     y_true = rng.randint(0, 2, size=10)
     y_score = rng.rand(10)
     _, _, thresholds = roc_curve(y_true, y_score)
     assert np.isinf(thresholds[0])
+
+
+# TODO(1.7): remove
+def test_precision_recall_curve_deprecation_warning():
+    """Check the message for future deprecation."""
+    # Check precision_recall_curve function
+    y_true, _, y_score = make_prediction(binary=True)
+
+    warn_msg = "probas_pred was deprecated in version 1.5"
+    with pytest.warns(FutureWarning, match=warn_msg):
+        precision_recall_curve(
+            y_true,
+            probas_pred=y_score,
+        )
+
+    error_msg = "`probas_pred` and `y_score` cannot be both specified"
+    with pytest.raises(ValueError, match=error_msg):
+        precision_recall_curve(
+            y_true,
+            probas_pred=y_score,
+            y_score=y_score,
+        )
```

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_regression.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_regression.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/metrics/tests/test_score_objects.py` & `scikit_learn-1.5.0rc1/sklearn/metrics/tests/test_score_objects.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,12 +1,9 @@
 import numbers
-import os
 import pickle
-import shutil
-import tempfile
 from copy import deepcopy
 from functools import partial
 from unittest.mock import Mock
 
 import joblib
 import numpy as np
 import pytest
@@ -52,26 +49,26 @@
 )
 from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
 from sklearn.multiclass import OneVsRestClassifier
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.pipeline import make_pipeline
 from sklearn.svm import LinearSVC
 from sklearn.tests.metadata_routing_common import (
-    assert_request_equal,
     assert_request_is_empty,
 )
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
 from sklearn.utils._testing import (
     assert_almost_equal,
     assert_array_equal,
     ignore_warnings,
 )
-from sklearn.utils.metadata_routing import MetadataRouter
+from sklearn.utils.metadata_routing import MetadataRouter, MethodMapping
 
 REGRESSION_SCORERS = [
+    "d2_absolute_error_score",
     "explained_variance",
     "r2",
     "neg_mean_absolute_error",
     "neg_mean_squared_error",
     "neg_mean_absolute_percentage_error",
     "neg_mean_squared_log_error",
     "neg_median_absolute_error",
@@ -162,36 +159,25 @@
         [(name, sensible_regr) for name in REGRESSION_SCORERS]
         + [(name, sensible_clf) for name in CLF_SCORERS]
         + [(name, sensible_clf) for name in CLUSTER_SCORERS]
         + [(name, sensible_ml_clf) for name in MULTILABEL_ONLY_SCORERS]
     )
 
 
-X_mm, y_mm, y_ml_mm = None, None, None
-ESTIMATORS = None
-TEMP_FOLDER = None
-
-
-def setup_module():
-    # Create some memory mapped data
-    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS
-    TEMP_FOLDER = tempfile.mkdtemp(prefix="sklearn_test_score_objects_")
+@pytest.fixture(scope="module")
+def memmap_data_and_estimators(tmp_path_factory):
+    temp_folder = tmp_path_factory.mktemp("sklearn_test_score_objects")
     X, y = make_classification(n_samples=30, n_features=5, random_state=0)
     _, y_ml = make_multilabel_classification(n_samples=X.shape[0], random_state=0)
-    filename = os.path.join(TEMP_FOLDER, "test_data.pkl")
+    filename = temp_folder / "test_data.pkl"
     joblib.dump((X, y, y_ml), filename)
     X_mm, y_mm, y_ml_mm = joblib.load(filename, mmap_mode="r")
-    ESTIMATORS = _make_estimators(X_mm, y_mm, y_ml_mm)
-
+    estimators = _make_estimators(X_mm, y_mm, y_ml_mm)
 
-def teardown_module():
-    global X_mm, y_mm, y_ml_mm, TEMP_FOLDER, ESTIMATORS
-    # GC closes the mmap file descriptors
-    X_mm, y_mm, y_ml_mm, ESTIMATORS = None, None, None, None
-    shutil.rmtree(TEMP_FOLDER)
+    yield X_mm, y_mm, y_ml_mm, estimators
 
 
 class EstimatorWithFit(BaseEstimator):
     """Dummy estimator to test scoring validators"""
 
     def fit(self, X, y):
         return self
@@ -286,15 +272,15 @@
     ],
 )
 def test_check_scoring_and_check_multimetric_scoring(scoring):
     check_scoring_validator_for_single_metric_usecases(check_scoring)
     # To make sure the check_scoring is correctly applied to the constituent
     # scorers
 
-    estimator = LinearSVC(dual="auto", random_state=0)
+    estimator = LinearSVC(random_state=0)
     estimator.fit([[1], [2], [3]], [1, 1, 0])
 
     scorers = _check_multimetric_scoring(estimator, scoring)
     assert isinstance(scorers, dict)
     assert sorted(scorers.keys()) == sorted(list(scoring))
     assert all([isinstance(scorer, _Scorer) for scorer in list(scorers.values())])
     assert all(scorer._response_method == "predict" for scorer in scorers.values())
@@ -347,20 +333,20 @@
         _check_multimetric_scoring(estimator, scoring=scoring)
 
 
 def test_check_scoring_gridsearchcv():
     # test that check_scoring works on GridSearchCV and pipeline.
     # slightly redundant non-regression test.
 
-    grid = GridSearchCV(LinearSVC(dual="auto"), param_grid={"C": [0.1, 1]}, cv=3)
+    grid = GridSearchCV(LinearSVC(), param_grid={"C": [0.1, 1]}, cv=3)
     scorer = check_scoring(grid, scoring="f1")
     assert isinstance(scorer, _Scorer)
     assert scorer._response_method == "predict"
 
-    pipe = make_pipeline(LinearSVC(dual="auto"))
+    pipe = make_pipeline(LinearSVC())
     scorer = check_scoring(pipe, scoring="f1")
     assert isinstance(scorer, _Scorer)
     assert scorer._response_method == "predict"
 
     # check that cross_val_score definitely calls the scorer
     # and doesn't make any assumptions about the estimator apart from having a
     # fit.
@@ -394,15 +380,15 @@
     ],
 )
 def test_classification_binary_scores(scorer_name, metric):
     # check consistency between score and scorer for scores supporting
     # binary classification.
     X, y = make_blobs(random_state=0, centers=2)
     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     clf.fit(X_train, y_train)
 
     score = get_scorer(scorer_name)(clf, X_test, y_test)
     expected_score = metric(y_test, clf.predict(X_test))
     assert_almost_equal(score, expected_score)
 
 
@@ -444,15 +430,15 @@
     assert score == pytest.approx(expected_score)
 
 
 def test_custom_scorer_pickling():
     # test that custom scorer can be pickled
     X, y = make_blobs(random_state=0, centers=2)
     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     clf.fit(X_train, y_train)
 
     scorer = make_scorer(fbeta_score, beta=2)
     score1 = scorer(clf, X_test, y_test)
     unpickled_scorer = pickle.loads(pickle.dumps(scorer))
     score2 = unpickled_scorer(clf, X_test, y_test)
     assert score1 == pytest.approx(score2)
@@ -542,15 +528,15 @@
     clf = OneVsRestClassifier(DecisionTreeClassifier())
     clf.fit(X_train, y_train)
     score1 = get_scorer("roc_auc")(clf, X_test, y_test)
     score2 = roc_auc_score(y_test, clf.predict_proba(X_test))
     assert_almost_equal(score1, score2)
 
     # Multilabel decision function
-    clf = OneVsRestClassifier(LinearSVC(dual="auto", random_state=0))
+    clf = OneVsRestClassifier(LinearSVC(random_state=0))
     clf.fit(X_train, y_train)
     score1 = get_scorer("roc_auc")(clf, X_test, y_test)
     score2 = roc_auc_score(y_test, clf.decision_function(X_test))
     assert_almost_equal(score1, score2)
 
 
 def test_supervised_cluster_scorers():
@@ -684,28 +670,29 @@
             assert "sample_weight" in str(e), (
                 f"scorer {name} raises unhelpful exception when called "
                 f"with sample weights: {str(e)}"
             )
 
 
 @pytest.mark.parametrize("name", get_scorer_names())
-def test_scorer_memmap_input(name):
+def test_scorer_memmap_input(name, memmap_data_and_estimators):
     # Non-regression test for #6147: some score functions would
     # return singleton memmap when computed on memmap data instead of scalar
     # float values.
+    X_mm, y_mm, y_ml_mm, estimators = memmap_data_and_estimators
 
     if name in REQUIRE_POSITIVE_Y_SCORERS:
         y_mm_1 = _require_positive_y(y_mm)
         y_ml_mm_1 = _require_positive_y(y_ml_mm)
     else:
         y_mm_1, y_ml_mm_1 = y_mm, y_ml_mm
 
     # UndefinedMetricWarning for P / R scores
     with ignore_warnings():
-        scorer, estimator = get_scorer(name), ESTIMATORS[name]
+        scorer, estimator = get_scorer(name), estimators[name]
         if name in MULTILABEL_ONLY_SCORERS:
             score = scorer(estimator, X_mm, y_ml_mm_1)
         else:
             score = scorer(estimator, X_mm, y_mm_1)
         assert isinstance(score, numbers.Number), name
 
 
@@ -935,15 +922,15 @@
     ],
 )
 def test_multiclass_roc_proba_scorer(scorer_name, metric):
     scorer = get_scorer(scorer_name)
     X, y = make_classification(
         n_classes=3, n_informative=3, n_samples=20, random_state=0
     )
-    lr = LogisticRegression(multi_class="multinomial").fit(X, y)
+    lr = LogisticRegression().fit(X, y)
     y_proba = lr.predict_proba(X)
     expected_score = metric(y, y_proba)
 
     assert scorer(lr, X, y) == pytest.approx(expected_score)
 
 
 def test_multiclass_roc_proba_scorer_label():
@@ -952,15 +939,15 @@
         multi_class="ovo",
         labels=[0, 1, 2],
         response_method="predict_proba",
     )
     X, y = make_classification(
         n_classes=3, n_informative=3, n_samples=20, random_state=0
     )
-    lr = LogisticRegression(multi_class="multinomial").fit(X, y)
+    lr = LogisticRegression().fit(X, y)
     y_proba = lr.predict_proba(X)
 
     y_binary = y == 0
     expected_score = roc_auc_score(
         y_binary, y_proba, multi_class="ovo", labels=[0, 1, 2]
     )
 
@@ -1242,26 +1229,28 @@
     assert (
         weighted_scorer.get_metadata_routing().score.requests["sample_weight"] is True
     )
 
     # make sure putting the scorer in a router doesn't request anything by
     # default
     router = MetadataRouter(owner="test").add(
-        method_mapping="score", scorer=get_scorer(name)
+        scorer=get_scorer(name),
+        method_mapping=MethodMapping().add(caller="score", callee="score"),
     )
     # make sure `sample_weight` is refused if passed.
     with pytest.raises(TypeError, match="got unexpected argument"):
         router.validate_metadata(params={"sample_weight": 1}, method="score")
     # make sure `sample_weight` is not routed even if passed.
     routed_params = router.route_params(params={"sample_weight": 1}, caller="score")
     assert not routed_params.scorer.score
 
     # make sure putting weighted_scorer in a router requests sample_weight
     router = MetadataRouter(owner="test").add(
-        scorer=weighted_scorer, method_mapping="score"
+        scorer=weighted_scorer,
+        method_mapping=MethodMapping().add(caller="score", callee="score"),
     )
     router.validate_metadata(params={"sample_weight": 1}, method="score")
     routed_params = router.route_params(params={"sample_weight": 1}, caller="score")
     assert list(routed_params.scorer.score.keys()) == ["sample_weight"]
 
 
 @pytest.mark.usefixtures("enable_slep006")
@@ -1284,31 +1273,47 @@
         scorer.set_score_request(labels=True)
 
     with pytest.warns(UserWarning, match="There is an overlap"):
         scorer(lr, X, y, labels=lr.classes_)
 
 
 @pytest.mark.usefixtures("enable_slep006")
-def test_PassthroughScorer_metadata_request():
-    """Test that _PassthroughScorer properly routes metadata.
+def test_PassthroughScorer_set_score_request():
+    """Test that _PassthroughScorer.set_score_request adds the correct metadata request
+    on itself and doesn't change its estimator's routing."""
+    est = LogisticRegression().set_score_request(sample_weight="estimator_weights")
+    # make a `_PassthroughScorer` with `check_scoring`:
+    scorer = check_scoring(est, None)
+    assert (
+        scorer.get_metadata_routing().score.requests["sample_weight"]
+        == "estimator_weights"
+    )
 
-    _PassthroughScorer should behave like a consumer, mirroring whatever is the
-    underlying score method.
-    """
-    scorer = _PassthroughScorer(
-        estimator=LinearSVC()
-        .set_score_request(sample_weight="alias")
-        .set_fit_request(sample_weight=True)
-    )
-    # Test that _PassthroughScorer doesn't change estimator's routing.
-    assert_request_equal(
-        scorer.get_metadata_routing(),
-        {"fit": {"sample_weight": True}, "score": {"sample_weight": "alias"}},
+    scorer.set_score_request(sample_weight="scorer_weights")
+    assert (
+        scorer.get_metadata_routing().score.requests["sample_weight"]
+        == "scorer_weights"
     )
 
+    # making sure changing the passthrough object doesn't affect the estimator.
+    assert (
+        est.get_metadata_routing().score.requests["sample_weight"]
+        == "estimator_weights"
+    )
+
+
+def test_PassthroughScorer_set_score_request_raises_without_routing_enabled():
+    """Test that _PassthroughScorer.set_score_request raises if metadata routing is
+    disabled."""
+    scorer = check_scoring(LogisticRegression(), None)
+    msg = "This method is only available when metadata routing is enabled."
+
+    with pytest.raises(RuntimeError, match=msg):
+        scorer.set_score_request(sample_weight="my_weights")
+
 
 @pytest.mark.usefixtures("enable_slep006")
 def test_multimetric_scoring_metadata_routing():
     # Test that _MultimetricScorer properly routes metadata.
     def score1(y_true, y_pred):
         return 1
 
@@ -1488,14 +1493,74 @@
     roc_auc_scorer = make_scorer(roc_auc_score, **new_params)
 
     assert deprecated_roc_auc_scorer(classifier, X, y) == pytest.approx(
         roc_auc_scorer(classifier, X, y)
     )
 
 
+@pytest.mark.parametrize("pass_estimator", [True, False])
+def test_get_scorer_multimetric(pass_estimator):
+    """Check that check_scoring is compatible with multi-metric configurations."""
+    X, y = make_classification(n_samples=150, n_features=10, random_state=0)
+    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
+    clf = LogisticRegression(random_state=0)
+
+    if pass_estimator:
+        check_scoring_ = check_scoring
+    else:
+        check_scoring_ = partial(check_scoring, clf)
+
+    clf.fit(X_train, y_train)
+
+    y_pred = clf.predict(X_test)
+    y_proba = clf.predict_proba(X_test)
+
+    expected_results = {
+        "r2": r2_score(y_test, y_pred),
+        "roc_auc": roc_auc_score(y_test, y_proba[:, 1]),
+        "accuracy": accuracy_score(y_test, y_pred),
+    }
+
+    for container in [set, list, tuple]:
+        scoring = check_scoring_(scoring=container(["r2", "roc_auc", "accuracy"]))
+        result = scoring(clf, X_test, y_test)
+
+        assert result.keys() == expected_results.keys()
+        for name in result:
+            assert result[name] == pytest.approx(expected_results[name])
+
+    def double_accuracy(y_true, y_pred):
+        return 2 * accuracy_score(y_true, y_pred)
+
+    custom_scorer = make_scorer(double_accuracy, response_method="predict")
+
+    # dict with different names
+    dict_scoring = check_scoring_(
+        scoring={
+            "my_r2": "r2",
+            "my_roc_auc": "roc_auc",
+            "double_accuracy": custom_scorer,
+        }
+    )
+    dict_result = dict_scoring(clf, X_test, y_test)
+    assert len(dict_result) == 3
+    assert dict_result["my_r2"] == pytest.approx(expected_results["r2"])
+    assert dict_result["my_roc_auc"] == pytest.approx(expected_results["roc_auc"])
+    assert dict_result["double_accuracy"] == pytest.approx(
+        2 * expected_results["accuracy"]
+    )
+
+
+def test_multimetric_scorer_repr():
+    """Check repr for multimetric scorer"""
+    multi_metric_scorer = check_scoring(scoring=["accuracy", "r2"])
+
+    assert str(multi_metric_scorer) == 'MultiMetricScorer("accuracy", "r2")'
+
+
 @pytest.mark.parametrize("enable_metadata_routing", [True, False])
 def test_metadata_routing_multimetric_metadata_routing(enable_metadata_routing):
     """Test multimetric scorer works with and without metadata routing enabled when
     there is no actual metadata to pass.
 
     Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/28256
     """
```

### Comparing `scikit-learn-1.4.2/sklearn/mixture/_base.py` & `scikit_learn-1.5.0rc1/sklearn/mixture/_base.py`

 * *Files 2% similar despite different names*

```diff
@@ -236,44 +236,47 @@
 
             lower_bound = -np.inf if do_init else self.lower_bound_
 
             if self.max_iter == 0:
                 best_params = self._get_parameters()
                 best_n_iter = 0
             else:
+                converged = False
                 for n_iter in range(1, self.max_iter + 1):
                     prev_lower_bound = lower_bound
 
                     log_prob_norm, log_resp = self._e_step(X)
                     self._m_step(X, log_resp)
                     lower_bound = self._compute_lower_bound(log_resp, log_prob_norm)
 
                     change = lower_bound - prev_lower_bound
                     self._print_verbose_msg_iter_end(n_iter, change)
 
                     if abs(change) < self.tol:
-                        self.converged_ = True
+                        converged = True
                         break
 
-                self._print_verbose_msg_init_end(lower_bound)
+                self._print_verbose_msg_init_end(lower_bound, converged)
 
                 if lower_bound > max_lower_bound or max_lower_bound == -np.inf:
                     max_lower_bound = lower_bound
                     best_params = self._get_parameters()
                     best_n_iter = n_iter
+                    self.converged_ = converged
 
         # Should only warn about convergence if max_iter > 0, otherwise
         # the user is assumed to have used 0-iters initialization
         # to get the initial means.
         if not self.converged_ and self.max_iter > 0:
             warnings.warn(
-                "Initialization %d did not converge. "
-                "Try different init parameters, "
-                "or increase max_iter, tol "
-                "or check for degenerate data." % (init + 1),
+                (
+                    "Best performing initialization did not converge. "
+                    "Try different init parameters, or increase max_iter, "
+                    "tol, or check for degenerate data."
+                ),
                 ConvergenceWarning,
             )
 
         self._set_parameters(best_params)
         self.n_iter_ = best_n_iter
         self.lower_bound_ = max_lower_bound
 
@@ -545,16 +548,18 @@
                 cur_time = time()
                 print(
                     "  Iteration %d\t time lapse %.5fs\t ll change %.5f"
                     % (n_iter, cur_time - self._iter_prev_time, diff_ll)
                 )
                 self._iter_prev_time = cur_time
 
-    def _print_verbose_msg_init_end(self, ll):
+    def _print_verbose_msg_init_end(self, lb, init_has_converged):
         """Print verbose message on the end of iteration."""
+        converged_msg = "converged" if init_has_converged else "did not converge"
         if self.verbose == 1:
-            print("Initialization converged: %s" % self.converged_)
+            print(f"Initialization {converged_msg}.")
         elif self.verbose >= 2:
+            t = time() - self._init_prev_time
             print(
-                "Initialization converged: %s\t time lapse %.5fs\t ll %.5f"
-                % (self.converged_, time() - self._init_prev_time, ll)
+                f"Initialization {converged_msg}. time lapse {t:.5f}s\t lower bound"
+                f" {lb:.5f}."
             )
```

### Comparing `scikit-learn-1.4.2/sklearn/mixture/_bayesian_mixture.py` & `scikit_learn-1.5.0rc1/sklearn/mixture/_bayesian_mixture.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Bayesian Gaussian Mixture Model."""
+
 # Author: Wei Xue <xuewei4d@gmail.com>
 #         Thierry Guillemot <thierry.guillemot.work@gmail.com>
 # License: BSD 3 clause
 
 import math
 from numbers import Real
 
@@ -241,15 +242,15 @@
 
             (n_components,)                        if 'spherical',
             (n_features, n_features)               if 'tied',
             (n_components, n_features)             if 'diag',
             (n_components, n_features, n_features) if 'full'
 
     converged_ : bool
-        True when convergence was reached in fit(), False otherwise.
+        True when convergence of the best fit of inference was reached, False otherwise.
 
     n_iter_ : int
         Number of step used by the best fit of inference to reach the
         convergence.
 
     lower_bound_ : float
         Lower bound value on the model evidence (of the training data) of the
```

### Comparing `scikit-learn-1.4.2/sklearn/mixture/_gaussian_mixture.py` & `scikit_learn-1.5.0rc1/sklearn/mixture/_gaussian_mixture.py`

 * *Files 0% similar despite different names*

```diff
@@ -648,15 +648,15 @@
 
             (n_components,)                        if 'spherical',
             (n_features, n_features)               if 'tied',
             (n_components, n_features)             if 'diag',
             (n_components, n_features, n_features) if 'full'
 
     converged_ : bool
-        True when convergence was reached in fit(), False otherwise.
+        True when convergence of the best fit of EM was reached, False otherwise.
 
     n_iter_ : int
         Number of step used by the best fit of EM to reach the convergence.
 
     lower_bound_ : float
         Lower bound value on the log-likelihood (of the training data with
         respect to the model) of the best fit of EM.
```

### Comparing `scikit-learn-1.4.2/sklearn/mixture/tests/test_bayesian_mixture.py` & `scikit_learn-1.5.0rc1/sklearn/mixture/tests/test_bayesian_mixture.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/mixture/tests/test_gaussian_mixture.py` & `scikit_learn-1.5.0rc1/sklearn/mixture/tests/test_gaussian_mixture.py`

 * *Files 0% similar despite different names*

```diff
@@ -679,17 +679,17 @@
             n_init=1,
             max_iter=max_iter,
             reg_covar=0,
             random_state=rng,
             covariance_type=covar_type,
         )
         msg = (
-            f"Initialization {max_iter} did not converge. Try different init "
-            "parameters, or increase max_iter, tol or check for degenerate"
-            " data."
+            "Best performing initialization did not converge. "
+            "Try different init parameters, or increase max_iter, "
+            "tol, or check for degenerate data."
         )
         with pytest.warns(ConvergenceWarning, match=msg):
             g.fit(X)
 
 
 def test_multiple_init():
     # Test that multiple inits does not much worse than a single one
```

### Comparing `scikit-learn-1.4.2/sklearn/mixture/tests/test_mixture.py` & `scikit_learn-1.5.0rc1/sklearn/mixture/tests/test_mixture.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,13 @@
 import typing
 
+from ._classification_threshold import (
+    FixedThresholdClassifier,
+    TunedThresholdClassifierCV,
+)
 from ._plot import LearningCurveDisplay, ValidationCurveDisplay
 from ._search import GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV
 from ._split import (
     BaseCrossValidator,
     BaseShuffleSplit,
     GroupKFold,
     GroupShuffleSplit,
@@ -59,14 +63,16 @@
     "ParameterSampler",
     "PredefinedSplit",
     "RandomizedSearchCV",
     "ShuffleSplit",
     "StratifiedKFold",
     "StratifiedGroupKFold",
     "StratifiedShuffleSplit",
+    "FixedThresholdClassifier",
+    "TunedThresholdClassifierCV",
     "check_cv",
     "cross_val_predict",
     "cross_val_score",
     "cross_validate",
     "learning_curve",
     "LearningCurveDisplay",
     "permutation_test_score",
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/_plot.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/_plot.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,23 @@
-import warnings
-
 import numpy as np
 
-from ..utils import check_matplotlib_support
+from ..utils._optional_dependencies import check_matplotlib_support
 from ..utils._plotting import _interval_max_min_ratio, _validate_score_name
 from ._validation import learning_curve, validation_curve
 
 
 class _BaseCurveDisplay:
     def _plot_curve(
         self,
         x_data,
         *,
         ax=None,
         negate_score=False,
         score_name=None,
         score_type="test",
-        log_scale="deprecated",
         std_display_style="fill_between",
         line_kw=None,
         fill_between_kw=None,
         errorbar_kw=None,
     ):
         check_matplotlib_support(f"{self.__class__.__name__}.plot")
 
@@ -104,33 +101,22 @@
                     )
                 )
 
         score_name = self.score_name if score_name is None else score_name
 
         ax.legend()
 
-        # TODO(1.5): to be removed
-        if log_scale != "deprecated":
-            warnings.warn(
-                (
-                    "The `log_scale` parameter is deprecated as of version 1.3 "
-                    "and will be removed in 1.5. You can use display.ax_.set_xscale "
-                    "and display.ax_.set_yscale instead."
-                ),
-                FutureWarning,
-            )
-            xscale = "log" if log_scale else "linear"
+        # We found that a ratio, smaller or bigger than 5, between the largest and
+        # smallest gap of the x values is a good indicator to choose between linear
+        # and log scale.
+        if _interval_max_min_ratio(x_data) > 5:
+            xscale = "symlog" if x_data.min() <= 0 else "log"
         else:
-            # We found that a ratio, smaller or bigger than 5, between the largest and
-            # smallest gap of the x values is a good indicator to choose between linear
-            # and log scale.
-            if _interval_max_min_ratio(x_data) > 5:
-                xscale = "symlog" if x_data.min() <= 0 else "log"
-            else:
-                xscale = "linear"
+            xscale = "linear"
+
         ax.set_xscale(xscale)
         ax.set_ylabel(f"{score_name}")
 
         self.ax_ = ax
         self.figure_ = ax.figure
 
 
@@ -222,15 +208,14 @@
     def plot(
         self,
         ax=None,
         *,
         negate_score=False,
         score_name=None,
         score_type="both",
-        log_scale="deprecated",
         std_display_style="fill_between",
         line_kw=None,
         fill_between_kw=None,
         errorbar_kw=None,
     ):
         """Plot visualization.
 
@@ -255,21 +240,14 @@
             replace it by `"Negative"` if `negate_score` is
             `False` or just remove it otherwise.
 
         score_type : {"test", "train", "both"}, default="both"
             The type of score to plot. Can be one of `"test"`, `"train"`, or
             `"both"`.
 
-        log_scale : bool, default="deprecated"
-            Whether or not to use a logarithmic scale for the x-axis.
-
-            .. deprecated:: 1.3
-               `log_scale` is deprecated in 1.3 and will be removed in 1.5.
-               Use `display.ax_.set_xscale` and `display.ax_.set_yscale` instead.
-
         std_display_style : {"errorbar", "fill_between"} or None, default="fill_between"
             The style used to display the score standard deviation around the
             mean score. If None, no standard deviation representation is
             displayed.
 
         line_kw : dict, default=None
             Additional keyword arguments passed to the `plt.plot` used to draw
@@ -290,15 +268,14 @@
         """
         self._plot_curve(
             self.train_sizes,
             ax=ax,
             negate_score=negate_score,
             score_name=score_name,
             score_type=score_type,
-            log_scale=log_scale,
             std_display_style=std_display_style,
             line_kw=line_kw,
             fill_between_kw=fill_between_kw,
             errorbar_kw=errorbar_kw,
         )
         self.ax_.set_xlabel("Number of samples in the training set")
         return self
@@ -322,15 +299,14 @@
         random_state=None,
         error_score=np.nan,
         fit_params=None,
         ax=None,
         negate_score=False,
         score_name=None,
         score_type="both",
-        log_scale="deprecated",
         std_display_style="fill_between",
         line_kw=None,
         fill_between_kw=None,
         errorbar_kw=None,
     ):
         """Create a learning curve display from an estimator.
 
@@ -447,21 +423,14 @@
             replace it by `"Negative"` if `negate_score` is
             `False` or just remove it otherwise.
 
         score_type : {"test", "train", "both"}, default="both"
             The type of score to plot. Can be one of `"test"`, `"train"`, or
             `"both"`.
 
-        log_scale : bool, default="deprecated"
-            Whether or not to use a logarithmic scale for the x-axis.
-
-            .. deprecated:: 1.3
-               `log_scale` is deprecated in 1.3 and will be removed in 1.5.
-               Use `display.ax_.xscale` and `display.ax_.yscale` instead.
-
         std_display_style : {"errorbar", "fill_between"} or None, default="fill_between"
             The style used to display the score standard deviation around the
             mean score. If `None`, no representation of the standard deviation
             is displayed.
 
         line_kw : dict, default=None
             Additional keyword arguments passed to the `plt.plot` used to draw
@@ -521,15 +490,14 @@
             test_scores=test_scores,
             score_name=score_name,
         )
         return viz.plot(
             ax=ax,
             negate_score=negate_score,
             score_type=score_type,
-            log_scale=log_scale,
             std_display_style=std_display_style,
             line_kw=line_kw,
             fill_between_kw=fill_between_kw,
             errorbar_kw=errorbar_kw,
         )
 
 
@@ -690,15 +658,14 @@
         """
         self._plot_curve(
             self.param_range,
             ax=ax,
             negate_score=negate_score,
             score_name=score_name,
             score_type=score_type,
-            log_scale="deprecated",
             std_display_style=std_display_style,
             line_kw=line_kw,
             fill_between_kw=fill_between_kw,
             errorbar_kw=errorbar_kw,
         )
         self.ax_.set_xlabel(f"{self.param_name}")
         return self
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/_search.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/_search.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,16 +29,18 @@
 from ..metrics import check_scoring
 from ..metrics._scorer import (
     _check_multimetric_scoring,
     _MultimetricScorer,
     get_scorer_names,
 )
 from ..utils import Bunch, check_random_state
+from ..utils._estimator_html_repr import _VisualBlock
 from ..utils._param_validation import HasMethods, Interval, StrOptions
 from ..utils._tags import _safe_tags
+from ..utils.deprecation import _deprecate_Xt_in_inverse_transform
 from ..utils.metadata_routing import (
     MetadataRouter,
     MethodMapping,
     _raise_for_params,
     _routing_enabled,
     process_routing,
 )
@@ -480,16 +482,15 @@
             score_params = process_routing(self, "score", **params).scorer["score"]
         else:
             score_params = dict()
 
         if self.scorer_ is None:
             raise ValueError(
                 "No score function explicitly defined, "
-                "and the estimator doesn't provide one %s"
-                % self.best_estimator_
+                "and the estimator doesn't provide one %s" % self.best_estimator_
             )
         if isinstance(self.scorer_, dict):
             if self.multimetric_:
                 scorer = self.scorer_[self.refit]
             else:
                 scorer = self.scorer_
             return scorer(self.best_estimator_, X, y, **score_params)
@@ -633,34 +634,42 @@
             `X` transformed in the new space based on the estimator with
             the best found parameters.
         """
         check_is_fitted(self)
         return self.best_estimator_.transform(X)
 
     @available_if(_estimator_has("inverse_transform"))
-    def inverse_transform(self, Xt):
+    def inverse_transform(self, X=None, Xt=None):
         """Call inverse_transform on the estimator with the best found params.
 
         Only available if the underlying estimator implements
         ``inverse_transform`` and ``refit=True``.
 
         Parameters
         ----------
+        X : indexable, length n_samples
+            Must fulfill the input assumptions of the
+            underlying estimator.
+
         Xt : indexable, length n_samples
             Must fulfill the input assumptions of the
             underlying estimator.
 
+            .. deprecated:: 1.5
+                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.
+
         Returns
         -------
         X : {ndarray, sparse matrix} of shape (n_samples, n_features)
             Result of the `inverse_transform` function for `Xt` based on the
             estimator with the best found parameters.
         """
+        X = _deprecate_Xt_in_inverse_transform(X, Xt)
         check_is_fitted(self)
-        return self.best_estimator_.inverse_transform(Xt)
+        return self.best_estimator_.inverse_transform(X)
 
     @property
     def n_features_in_(self):
         """Number of features seen during :term:`fit`.
 
         Only available when `refit=True`.
         """
@@ -777,44 +786,36 @@
                 raise TypeError("best_index_ returned is not an integer")
             if best_index < 0 or best_index >= len(results["params"]):
                 raise IndexError("best_index_ index out of range")
         else:
             best_index = results[f"rank_test_{refit_metric}"].argmin()
         return best_index
 
-    def _get_scorers(self, convert_multimetric):
+    def _get_scorers(self):
         """Get the scorer(s) to be used.
 
         This is used in ``fit`` and ``get_metadata_routing``.
 
-        Parameters
-        ----------
-        convert_multimetric : bool
-            Whether to convert a dict of scorers to a _MultimetricScorer. This
-            is used in ``get_metadata_routing`` to include the routing info for
-            multiple scorers.
-
         Returns
         -------
         scorers, refit_metric
         """
         refit_metric = "score"
 
         if callable(self.scoring):
             scorers = self.scoring
         elif self.scoring is None or isinstance(self.scoring, str):
             scorers = check_scoring(self.estimator, self.scoring)
         else:
             scorers = _check_multimetric_scoring(self.estimator, self.scoring)
             self._check_refit_for_multimetric(scorers)
             refit_metric = self.refit
-            if convert_multimetric and isinstance(scorers, dict):
-                scorers = _MultimetricScorer(
-                    scorers=scorers, raise_exc=(self.error_score == "raise")
-                )
+            scorers = _MultimetricScorer(
+                scorers=scorers, raise_exc=(self.error_score == "raise")
+            )
 
         return scorers, refit_metric
 
     def _get_routed_params_for_fit(self, params):
         """Get the parameters to be used for routing.
 
         This is a method instead of a snippet in ``fit`` since it's used twice,
@@ -862,18 +863,15 @@
 
         Returns
         -------
         self : object
             Instance of fitted estimator.
         """
         estimator = self.estimator
-        # Here we keep a dict of scorers as is, and only convert to a
-        # _MultimetricScorer at a later stage. Issue:
-        # https://github.com/scikit-learn/scikit-learn/issues/27001
-        scorers, refit_metric = self._get_scorers(convert_multimetric=False)
+        scorers, refit_metric = self._get_scorers()
 
         X, y = indexable(X, y)
         params = _check_method_params(X, params=params)
 
         routed_params = self._get_routed_params_for_fit(params)
 
         cv_orig = check_cv(self.cv, y, classifier=is_classifier(estimator))
@@ -1011,15 +1009,18 @@
             refit_end_time = time.time()
             self.refit_time_ = refit_end_time - refit_start_time
 
             if hasattr(self.best_estimator_, "feature_names_in_"):
                 self.feature_names_in_ = self.best_estimator_.feature_names_in_
 
         # Store the only scorer not as a dict for single metric evaluation
-        self.scorer_ = scorers
+        if isinstance(scorers, _MultimetricScorer):
+            self.scorer_ = scorers._scorers
+        else:
+            self.scorer_ = scorers
 
         self.cv_results_ = results
         self.n_splits_ = n_splits
 
         return self
 
     def _format_results(self, candidate_params, n_splits, out, more_results=None):
@@ -1077,35 +1078,37 @@
                     rank_result = rankdata(-array_means, method="min").astype(
                         np.int32, copy=False
                     )
                 results["rank_%s" % key_name] = rank_result
 
         _store("fit_time", out["fit_time"])
         _store("score_time", out["score_time"])
-        # Use one MaskedArray and mask all the places where the param is not
-        # applicable for that candidate. Use defaultdict as each candidate may
-        # not contain all the params
-        param_results = defaultdict(
-            partial(
-                MaskedArray,
-                np.empty(
-                    n_candidates,
-                ),
-                mask=True,
-                dtype=object,
-            )
-        )
+        param_results = defaultdict(dict)
         for cand_idx, params in enumerate(candidate_params):
             for name, value in params.items():
-                # An all masked empty array gets created for the key
-                # `"param_%s" % name` at the first occurrence of `name`.
-                # Setting the value at an index also unmasks that index
                 param_results["param_%s" % name][cand_idx] = value
+        for key, param_result in param_results.items():
+            param_list = list(param_result.values())
+            try:
+                arr_dtype = np.result_type(*param_list)
+            except TypeError:
+                arr_dtype = object
+            if len(param_list) == n_candidates and arr_dtype != object:
+                # Exclude `object` else the numpy constructor might infer a list of
+                # tuples to be a 2d array.
+                results[key] = MaskedArray(param_list, mask=False, dtype=arr_dtype)
+            else:
+                # Use one MaskedArray and mask all the places where the param is not
+                # applicable for that candidate (which may not contain all the params).
+                ma = MaskedArray(np.empty(n_candidates), mask=True, dtype=arr_dtype)
+                for index, value in param_result.items():
+                    # Setting the value at an index unmasks that index
+                    ma[index] = value
+                results[key] = ma
 
-        results.update(param_results)
         # Store a list of param dicts at the key 'params'
         results["params"] = candidate_params
 
         test_scores_dict = _normalize_score_results(out["test_scores"])
         if self.return_train_score:
             train_scores_dict = _normalize_score_results(out["train_scores"])
 
@@ -1143,27 +1146,40 @@
         """
         router = MetadataRouter(owner=self.__class__.__name__)
         router.add(
             estimator=self.estimator,
             method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
 
-        scorer, _ = self._get_scorers(convert_multimetric=True)
+        scorer, _ = self._get_scorers()
         router.add(
             scorer=scorer,
             method_mapping=MethodMapping()
             .add(caller="score", callee="score")
             .add(caller="fit", callee="score"),
         )
         router.add(
             splitter=self.cv,
             method_mapping=MethodMapping().add(caller="fit", callee="split"),
         )
         return router
 
+    def _sk_visual_block_(self):
+        if hasattr(self, "best_estimator_"):
+            key, estimator = "best_estimator_", self.best_estimator_
+        else:
+            key, estimator = "estimator", self.estimator
+
+        return _VisualBlock(
+            "parallel",
+            [estimator],
+            names=[f"{key}: {estimator.__class__.__name__}"],
+            name_details=[str(estimator)],
+        )
+
 
 class GridSearchCV(BaseSearchCV):
     """Exhaustive search over specified parameter values for an estimator.
 
     Important members are fit, predict.
 
     GridSearchCV implements a "fit" and a "score" method.
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/_search_successive_halving.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/_search_successive_halving.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/_split.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/_split.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,21 +20,26 @@
 from itertools import chain, combinations
 from math import ceil, floor
 
 import numpy as np
 from scipy.special import comb
 
 from ..utils import (
-    _approximate_mode,
     _safe_indexing,
     check_random_state,
     indexable,
     metadata_routing,
 )
+from ..utils._array_api import (
+    _convert_to_numpy,
+    ensure_common_namespace_device,
+    get_namespace,
+)
 from ..utils._param_validation import Interval, RealNotInt, validate_params
+from ..utils.extmath import _approximate_mode
 from ..utils.metadata_routing import _MetadataRequester
 from ..utils.multiclass import type_of_target
 from ..utils.validation import _num_samples, check_array, column_or_1d
 
 __all__ = [
     "BaseCrossValidator",
     "KFold",
@@ -52,14 +57,48 @@
     "StratifiedShuffleSplit",
     "PredefinedSplit",
     "train_test_split",
     "check_cv",
 ]
 
 
+class _UnsupportedGroupCVMixin:
+    """Mixin for splitters that do not support Groups."""
+
+    def split(self, X, y=None, groups=None):
+        """Generate indices to split data into training and test set.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
+            The target variable for supervised learning problems.
+
+        groups : object
+            Always ignored, exists for compatibility.
+
+        Yields
+        ------
+        train : ndarray
+            The training set indices for that split.
+
+        test : ndarray
+            The testing set indices for that split.
+        """
+        if groups is not None:
+            warnings.warn(
+                f"The groups parameter is ignored by {self.__class__.__name__}",
+                UserWarning,
+            )
+        return super().split(X, y, groups=groups)
+
+
 class GroupsConsumerMixin(_MetadataRequester):
     """A Mixin to ``groups`` by default.
 
     This Mixin makes the object to request ``groups`` by default as ``True``.
 
     .. versionadded:: 1.3
     """
@@ -130,15 +169,15 @@
     def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator."""
 
     def __repr__(self):
         return _build_repr(self)
 
 
-class LeaveOneOut(BaseCrossValidator):
+class LeaveOneOut(_UnsupportedGroupCVMixin, BaseCrossValidator):
     """Leave-One-Out cross-validator.
 
     Provides train/test indices to split data in train/test sets. Each
     sample is used once as a test set (singleton) while the remaining
     samples form the training set.
 
     Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and
@@ -209,15 +248,15 @@
             Returns the number of splitting iterations in the cross-validator.
         """
         if X is None:
             raise ValueError("The 'X' parameter should not be None.")
         return _num_samples(X)
 
 
-class LeavePOut(BaseCrossValidator):
+class LeavePOut(_UnsupportedGroupCVMixin, BaseCrossValidator):
     """Leave-P-Out cross-validator.
 
     Provides train/test indices to split data in train/test sets. This results
     in testing on all distinct samples of size p, while the remaining n - p
     samples form the training set in each iteration.
 
     Note: ``LeavePOut(p)`` is NOT equivalent to
@@ -395,15 +434,15 @@
         -------
         n_splits : int
             Returns the number of splitting iterations in the cross-validator.
         """
         return self.n_splits
 
 
-class KFold(_BaseKFold):
+class KFold(_UnsupportedGroupCVMixin, _BaseKFold):
     """K-Fold cross-validator.
 
     Provides train/test indices to split data in train/test sets. Split
     dataset into k consecutive folds (without shuffling by default).
 
     Each fold is then used once as a validation while the k - 1 remaining
     folds form the training set.
@@ -498,15 +537,15 @@
 class GroupKFold(GroupsConsumerMixin, _BaseKFold):
     """K-fold iterator variant with non-overlapping groups.
 
     Each group will appear exactly once in the test set across all folds (the
     number of distinct groups has to be at least equal to the number of folds).
 
     The folds are approximately balanced in the sense that the number of
-    distinct groups is approximately the same in each fold.
+    samples is approximately the same in each test fold.
 
     Read more in the :ref:`User Guide <group_k_fold>`.
 
     For visualisation of cross-validation behaviour and
     comparison between common scikit-learn split methods
     refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`
 
@@ -801,14 +840,19 @@
 
         Notes
         -----
         Randomized CV splitters may return different results for each call of
         split. You can make the results identical by setting `random_state`
         to an integer.
         """
+        if groups is not None:
+            warnings.warn(
+                f"The groups parameter is ignored by {self.__class__.__name__}",
+                UserWarning,
+            )
         y = check_array(y, input_name="y", ensure_2d=False, dtype=None)
         return super().split(X, y, groups)
 
 
 class StratifiedGroupKFold(GroupsConsumerMixin, _BaseKFold):
     """Stratified K-Fold iterator variant with non-overlapping groups.
 
@@ -1159,15 +1203,39 @@
         ------
         train : ndarray
             The training set indices for that split.
 
         test : ndarray
             The testing set indices for that split.
         """
-        X, y, groups = indexable(X, y, groups)
+        if groups is not None:
+            warnings.warn(
+                f"The groups parameter is ignored by {self.__class__.__name__}",
+                UserWarning,
+            )
+        return self._split(X)
+
+    def _split(self, X):
+        """Generate indices to split data into training and test set.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        Yields
+        ------
+        train : ndarray
+            The training set indices for that split.
+
+        test : ndarray
+            The testing set indices for that split.
+        """
+        (X,) = indexable(X)
         n_samples = _num_samples(X)
         n_splits = self.n_splits
         n_folds = n_splits + 1
         gap = self.gap
         test_size = (
             self.test_size if self.test_size is not None else n_samples // n_folds
         )
@@ -1556,15 +1624,15 @@
         cv = self.cv(random_state=rng, shuffle=True, **self.cvargs)
         return cv.get_n_splits(X, y, groups) * self.n_repeats
 
     def __repr__(self):
         return _build_repr(self)
 
 
-class RepeatedKFold(_RepeatedSplits):
+class RepeatedKFold(_UnsupportedGroupCVMixin, _RepeatedSplits):
     """Repeated K-Fold cross validator.
 
     Repeats K-Fold n times with different randomization in each repetition.
 
     Read more in the :ref:`User Guide <repeated_k_fold>`.
 
     Parameters
@@ -1622,15 +1690,15 @@
 
     def __init__(self, *, n_splits=5, n_repeats=10, random_state=None):
         super().__init__(
             KFold, n_repeats=n_repeats, random_state=random_state, n_splits=n_splits
         )
 
 
-class RepeatedStratifiedKFold(_RepeatedSplits):
+class RepeatedStratifiedKFold(_UnsupportedGroupCVMixin, _RepeatedSplits):
     """Repeated Stratified K-Fold cross validator.
 
     Repeats Stratified K-Fold n times with different randomization in each
     repetition.
 
     Read more in the :ref:`User Guide <repeated_k_fold>`.
 
@@ -1694,15 +1762,39 @@
             n_repeats=n_repeats,
             random_state=random_state,
             n_splits=n_splits,
         )
 
 
 class BaseShuffleSplit(_MetadataRequester, metaclass=ABCMeta):
-    """Base class for ShuffleSplit and StratifiedShuffleSplit."""
+    """Base class for *ShuffleSplit.
+
+    Parameters
+    ----------
+    n_splits : int, default=10
+        Number of re-shuffling & splitting iterations.
+
+    test_size : float or int, default=None
+        If float, should be between 0.0 and 1.0 and represent the proportion
+        of the dataset to include in the test split. If int, represents the
+        absolute number of test samples. If None, the value is set to the
+        complement of the train size. If ``train_size`` is also None, it will
+        be set to 0.1.
+
+    train_size : float or int, default=None
+        If float, should be between 0.0 and 1.0 and represent the
+        proportion of the dataset to include in the train split. If
+        int, represents the absolute number of train samples. If None,
+        the value is automatically set to the complement of the test size.
+
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of the training and testing indices produced.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+    """
 
     # This indicates that by default CV splitters don't have a "groups" kwarg,
     # unless indicated by inheriting from ``GroupsConsumerMixin``.
     # This also prevents ``set_split_request`` to be generated for splitters
     # which don't support ``groups``.
     __metadata_request__split = {"groups": metadata_routing.UNUSED}
 
@@ -1745,17 +1837,31 @@
         split. You can make the results identical by setting `random_state`
         to an integer.
         """
         X, y, groups = indexable(X, y, groups)
         for train, test in self._iter_indices(X, y, groups):
             yield train, test
 
-    @abstractmethod
     def _iter_indices(self, X, y=None, groups=None):
         """Generate (train, test) indices"""
+        n_samples = _num_samples(X)
+        n_train, n_test = _validate_shuffle_split(
+            n_samples,
+            self.test_size,
+            self.train_size,
+            default_test_size=self._default_test_size,
+        )
+
+        rng = check_random_state(self.random_state)
+        for i in range(self.n_splits):
+            # random partition
+            permutation = rng.permutation(n_samples)
+            ind_test = permutation[:n_test]
+            ind_train = permutation[n_test : (n_test + n_train)]
+            yield ind_train, ind_test
 
     def get_n_splits(self, X=None, y=None, groups=None):
         """Returns the number of splitting iterations in the cross-validator.
 
         Parameters
         ----------
         X : object
@@ -1774,15 +1880,15 @@
         """
         return self.n_splits
 
     def __repr__(self):
         return _build_repr(self)
 
 
-class ShuffleSplit(BaseShuffleSplit):
+class ShuffleSplit(_UnsupportedGroupCVMixin, BaseShuffleSplit):
     """Random permutation cross-validator.
 
     Yields indices to split data into training and test sets.
 
     Note: contrary to other cross-validation strategies, random splits
     do not guarantee that all folds will be different, although this is
     still very likely for sizeable datasets.
@@ -1877,33 +1983,16 @@
             n_splits=n_splits,
             test_size=test_size,
             train_size=train_size,
             random_state=random_state,
         )
         self._default_test_size = 0.1
 
-    def _iter_indices(self, X, y=None, groups=None):
-        n_samples = _num_samples(X)
-        n_train, n_test = _validate_shuffle_split(
-            n_samples,
-            self.test_size,
-            self.train_size,
-            default_test_size=self._default_test_size,
-        )
-
-        rng = check_random_state(self.random_state)
-        for i in range(self.n_splits):
-            # random partition
-            permutation = rng.permutation(n_samples)
-            ind_test = permutation[:n_test]
-            ind_train = permutation[n_test : (n_test + n_train)]
-            yield ind_train, ind_test
 
-
-class GroupShuffleSplit(GroupsConsumerMixin, ShuffleSplit):
+class GroupShuffleSplit(GroupsConsumerMixin, BaseShuffleSplit):
     """Shuffle-Group(s)-Out cross-validation iterator.
 
     Provides randomized train/test indices to split data according to a
     third-party provided group. This group information can be used to encode
     arbitrary domain specific stratifications of the samples as integers.
 
     For instance the groups could be the year of collection of the samples
@@ -2133,14 +2222,20 @@
         n_train, n_test = _validate_shuffle_split(
             n_samples,
             self.test_size,
             self.train_size,
             default_test_size=self._default_test_size,
         )
 
+        # Convert to numpy as not all operations are supported by the Array API.
+        # `y` is probably never a very large array, which means that converting it
+        # should be cheap
+        xp, _ = get_namespace(y)
+        y = _convert_to_numpy(y, xp=xp)
+
         if y.ndim == 2:
             # for multi-label y, map each distinct row to a string repr
             # using join because str(row) uses an ellipsis if len(row) > 1000
             y = np.array([" ".join(row.astype("str")) for row in y])
 
         classes, y_indices = np.unique(y, return_inverse=True)
         n_classes = classes.shape[0]
@@ -2225,14 +2320,19 @@
 
         Notes
         -----
         Randomized CV splitters may return different results for each call of
         split. You can make the results identical by setting `random_state`
         to an integer.
         """
+        if groups is not None:
+            warnings.warn(
+                f"The groups parameter is ignored by {self.__class__.__name__}",
+                UserWarning,
+            )
         y = check_array(y, input_name="y", ensure_2d=False, dtype=None)
         return super().split(X, y, groups)
 
 
 def _validate_shuffle_split(n_samples, test_size, train_size, default_test_size=None):
     """
     Validation helper to check if the test/test sizes are meaningful w.r.t. the
@@ -2380,14 +2480,32 @@
         ------
         train : ndarray
             The training set indices for that split.
 
         test : ndarray
             The testing set indices for that split.
         """
+        if groups is not None:
+            warnings.warn(
+                f"The groups parameter is ignored by {self.__class__.__name__}",
+                UserWarning,
+            )
+        return self._split()
+
+    def _split(self):
+        """Generate indices to split data into training and test set.
+
+        Yields
+        ------
+        train : ndarray
+            The training set indices for that split.
+
+        test : ndarray
+            The testing set indices for that split.
+        """
         ind = np.arange(len(self.test_fold))
         for test_index in self._iter_test_masks():
             train_index = ind[np.logical_not(test_index)]
             test_index = ind[test_index]
             yield train_index, test_index
 
     def _iter_test_masks(self):
@@ -2676,14 +2794,16 @@
         else:
             CVClass = ShuffleSplit
 
         cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)
 
         train, test = next(cv.split(X=arrays[0], y=stratify))
 
+    train, test = ensure_common_namespace_device(arrays[0], train, test)
+
     return list(
         chain.from_iterable(
             (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays
         )
     )
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/_validation.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/_validation.py`

 * *Files 1% similar despite different names*

```diff
@@ -354,26 +354,19 @@
 
     if callable(scoring):
         scorers = scoring
     elif scoring is None or isinstance(scoring, str):
         scorers = check_scoring(estimator, scoring)
     else:
         scorers = _check_multimetric_scoring(estimator, scoring)
+        scorers = _MultimetricScorer(
+            scorers=scorers, raise_exc=(error_score == "raise")
+        )
 
     if _routing_enabled():
-        # `cross_validate` will create a `_MultiMetricScorer` if `scoring` is a
-        # dict at a later stage. We need the same object for the purpose of
-        # routing. However, creating it here and passing it around would create
-        # a much larger diff since the dict is used in many places.
-        if isinstance(scorers, dict):
-            _scorer = _MultimetricScorer(
-                scorers=scorers, raise_exc=(error_score == "raise")
-            )
-        else:
-            _scorer = scorers
         # For estimators, a MetadataRouter is created in get_metadata_routing
         # methods. For these router methods, we create the router to use
         # `process_routing` on it.
         router = (
             MetadataRouter(owner="cross_validate")
             .add(
                 splitter=cv,
@@ -382,15 +375,15 @@
             .add(
                 estimator=estimator,
                 # TODO(SLEP6): also pass metadata to the predict method for
                 # scoring?
                 method_mapping=MethodMapping().add(caller="fit", callee="fit"),
             )
             .add(
-                scorer=_scorer,
+                scorer=scorers,
                 method_mapping=MethodMapping().add(caller="fit", callee="score"),
             )
         )
         try:
             routed_params = process_routing(router, "fit", **params)
         except UnsetMetadataPassedError as e:
             # The default exception would mention `fit` since in the above
@@ -897,16 +890,16 @@
     except Exception:
         # Note fit time as time until error
         fit_time = time.time() - start_time
         score_time = 0.0
         if error_score == "raise":
             raise
         elif isinstance(error_score, numbers.Number):
-            if isinstance(scorer, dict):
-                test_scores = {name: error_score for name in scorer}
+            if isinstance(scorer, _MultimetricScorer):
+                test_scores = {name: error_score for name in scorer._scorers}
                 if return_train_score:
                     train_scores = test_scores.copy()
             else:
                 test_scores = error_score
                 if return_train_score:
                     train_scores = error_score
         result["fit_error"] = format_exc()
@@ -962,21 +955,17 @@
         result["estimator"] = estimator
     return result
 
 
 def _score(estimator, X_test, y_test, scorer, score_params, error_score="raise"):
     """Compute the score(s) of an estimator on a given test set.
 
-    Will return a dict of floats if `scorer` is a dict, otherwise a single
+    Will return a dict of floats if `scorer` is a _MultiMetricScorer, otherwise a single
     float is returned.
     """
-    if isinstance(scorer, dict):
-        # will cache method calls if needed. scorer() returns a dict
-        scorer = _MultimetricScorer(scorers=scorer, raise_exc=(error_score == "raise"))
-
     score_params = {} if score_params is None else score_params
 
     try:
         if y_test is None:
             scores = scorer(estimator, X_test, **score_params)
         else:
             scores = scorer(estimator, X_test, y_test, **score_params)
@@ -1037,15 +1026,15 @@
     return scores
 
 
 @validate_params(
     {
         "estimator": [HasMethods(["fit", "predict"])],
         "X": ["array-like", "sparse matrix"],
-        "y": ["array-like", None],
+        "y": ["array-like", "sparse matrix", None],
         "groups": ["array-like", None],
         "cv": ["cv_object"],
         "n_jobs": [Integral, None],
         "verbose": ["verbose"],
         "fit_params": [dict, None],
         "params": [dict, None],
         "pre_dispatch": [Integral, str, None],
@@ -1094,15 +1083,15 @@
     estimator : estimator
         The estimator instance to use to fit the data. It must implement a `fit`
         method and the method given by the `method` parameter.
 
     X : {array-like, sparse matrix} of shape (n_samples, n_features)
         The data to fit. Can be, for example a list, or an array at least 2d.
 
-    y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
+    y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs), \
             default=None
         The target variable to try to predict in the case of
         supervised learning.
 
     groups : array-like of shape (n_samples,), default=None
         Group labels for the samples used while splitting the dataset into
         train/test set. Only used in conjunction with a "Group" :term:`cv`
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/tests/common.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/tests/common.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/tests/test_plot.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_plot.py`

 * *Files 2% similar despite different names*

```diff
@@ -522,37 +522,14 @@
         std_display_style=std_display_style,
         errorbar_kw=errorbar_kw,
     )
 
     assert display.errorbar_[0].lines[0].get_color() == "red"
 
 
-# TODO(1.5): to be removed
-def test_learning_curve_display_deprecate_log_scale(data, pyplot):
-    """Check that we warn for the deprecated parameter `log_scale`."""
-    X, y = data
-    estimator = DecisionTreeClassifier(random_state=0)
-
-    with pytest.warns(FutureWarning, match="`log_scale` parameter is deprecated"):
-        display = LearningCurveDisplay.from_estimator(
-            estimator, X, y, train_sizes=[0.3, 0.6, 0.9], log_scale=True
-        )
-
-    assert display.ax_.get_xscale() == "log"
-    assert display.ax_.get_yscale() == "linear"
-
-    with pytest.warns(FutureWarning, match="`log_scale` parameter is deprecated"):
-        display = LearningCurveDisplay.from_estimator(
-            estimator, X, y, train_sizes=[0.3, 0.6, 0.9], log_scale=False
-        )
-
-    assert display.ax_.get_xscale() == "linear"
-    assert display.ax_.get_yscale() == "linear"
-
-
 @pytest.mark.parametrize(
     "param_range, xscale",
     [([5, 10, 15], "linear"), ([-50, 5, 50, 500], "symlog"), ([5, 50, 500], "log")],
 )
 def test_validation_curve_xscale_from_param_range_provided_as_a_list(
     pyplot, data, param_range, xscale
 ):
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/tests/test_search.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_search.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,35 +1,40 @@
 """Test the search module"""
 
 import pickle
 import re
 import sys
+import warnings
 from collections.abc import Iterable, Sized
 from functools import partial
 from io import StringIO
 from itertools import chain, product
 from types import GeneratorType
 
 import numpy as np
 import pytest
 from scipy.stats import bernoulli, expon, uniform
 
+from sklearn import config_context
 from sklearn.base import BaseEstimator, ClassifierMixin, is_classifier
 from sklearn.cluster import KMeans
 from sklearn.datasets import (
     make_blobs,
     make_classification,
     make_multilabel_classification,
 )
+from sklearn.dummy import DummyClassifier
 from sklearn.ensemble import HistGradientBoostingClassifier
 from sklearn.exceptions import FitFailedWarning
 from sklearn.experimental import enable_halving_search_cv  # noqa
+from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.impute import SimpleImputer
 from sklearn.linear_model import (
     LinearRegression,
+    LogisticRegression,
     Ridge,
     SGDClassifier,
 )
 from sklearn.metrics import (
     accuracy_score,
     confusion_matrix,
     f1_score,
@@ -52,16 +57,18 @@
     RandomizedSearchCV,
     StratifiedKFold,
     StratifiedShuffleSplit,
     train_test_split,
 )
 from sklearn.model_selection._search import BaseSearchCV
 from sklearn.model_selection.tests.common import OneTimeSplitter
+from sklearn.naive_bayes import ComplementNB
 from sklearn.neighbors import KernelDensity, KNeighborsClassifier, LocalOutlierFactor
 from sklearn.pipeline import Pipeline
+from sklearn.preprocessing import StandardScaler
 from sklearn.svm import SVC, LinearSVC
 from sklearn.tests.metadata_routing_common import (
     ConsumingScorer,
     _Registry,
     check_recorded_metadata,
 )
 from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
@@ -253,18 +260,18 @@
         searcher.fit(X, y, spam=np.ones(1), eggs=np.zeros(10))
     searcher.fit(X, y, spam=np.ones(10), eggs=np.zeros(10))
 
 
 @ignore_warnings
 def test_grid_search_no_score():
     # Test grid-search on classifier that has no score function.
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     X, y = make_blobs(random_state=0, centers=2)
     Cs = [0.1, 1, 10]
-    clf_no_score = LinearSVCNoScore(dual="auto", random_state=0)
+    clf_no_score = LinearSVCNoScore(random_state=0)
     grid_search = GridSearchCV(clf, {"C": Cs}, scoring="accuracy")
     grid_search.fit(X, y)
 
     grid_search_no_score = GridSearchCV(clf_no_score, {"C": Cs}, scoring="accuracy")
     # smoketest grid search
     grid_search_no_score.fit(X, y)
 
@@ -277,21 +284,21 @@
     grid_search_no_score = GridSearchCV(clf_no_score, {"C": Cs})
     with pytest.raises(TypeError, match="no scoring"):
         grid_search_no_score.fit([[1]])
 
 
 def test_grid_search_score_method():
     X, y = make_classification(n_samples=100, n_classes=2, flip_y=0.2, random_state=0)
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     grid = {"C": [0.1]}
 
     search_no_scoring = GridSearchCV(clf, grid, scoring=None).fit(X, y)
     search_accuracy = GridSearchCV(clf, grid, scoring="accuracy").fit(X, y)
     search_no_score_method_auc = GridSearchCV(
-        LinearSVCNoScore(dual="auto"), grid, scoring="roc_auc"
+        LinearSVCNoScore(), grid, scoring="roc_auc"
     ).fit(X, y)
     search_auc = GridSearchCV(clf, grid, scoring="roc_auc").fit(X, y)
 
     # Check warning only occurs in situation where behavior changed:
     # estimator requires score method to compete with scoring parameter
     score_no_scoring = search_no_scoring.score(X, y)
     score_accuracy = search_accuracy.score(X, y)
@@ -311,15 +318,15 @@
     # Check if ValueError (when groups is None) propagates to GridSearchCV
     # And also check if groups is correctly passed to the cv object
     rng = np.random.RandomState(0)
 
     X, y = make_classification(n_samples=15, n_classes=2, random_state=0)
     groups = rng.randint(0, 3, 15)
 
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     grid = {"C": [1]}
 
     group_cvs = [
         LeaveOneGroupOut(),
         LeavePGroupsOut(2),
         GroupKFold(n_splits=3),
         GroupShuffleSplit(),
@@ -340,31 +347,29 @@
 
 def test_classes__property():
     # Test that classes_ property matches best_estimator_.classes_
     X = np.arange(100).reshape(10, 10)
     y = np.array([0] * 5 + [1] * 5)
     Cs = [0.1, 1, 10]
 
-    grid_search = GridSearchCV(LinearSVC(dual="auto", random_state=0), {"C": Cs})
+    grid_search = GridSearchCV(LinearSVC(random_state=0), {"C": Cs})
     grid_search.fit(X, y)
     assert_array_equal(grid_search.best_estimator_.classes_, grid_search.classes_)
 
     # Test that regressors do not have a classes_ attribute
     grid_search = GridSearchCV(Ridge(), {"alpha": [1.0, 2.0]})
     grid_search.fit(X, y)
     assert not hasattr(grid_search, "classes_")
 
     # Test that the grid searcher has no classes_ attribute before it's fit
-    grid_search = GridSearchCV(LinearSVC(dual="auto", random_state=0), {"C": Cs})
+    grid_search = GridSearchCV(LinearSVC(random_state=0), {"C": Cs})
     assert not hasattr(grid_search, "classes_")
 
     # Test that the grid searcher has no classes_ attribute without a refit
-    grid_search = GridSearchCV(
-        LinearSVC(dual="auto", random_state=0), {"C": Cs}, refit=False
-    )
+    grid_search = GridSearchCV(LinearSVC(random_state=0), {"C": Cs}, refit=False)
     grid_search.fit(X, y)
     assert not hasattr(grid_search, "classes_")
 
 
 def test_trivial_cv_results_attr():
     # Test search over a "grid" with only one point.
     clf = MockClassifier()
@@ -420,15 +425,15 @@
             ).fit(X, y)
 
 
 def test_grid_search_error():
     # Test that grid search will capture errors on data with different length
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
 
-    clf = LinearSVC(dual="auto")
+    clf = LinearSVC()
     cv = GridSearchCV(clf, {"C": [0.1, 1.0]})
     with pytest.raises(ValueError):
         cv.fit(X_[:180], y_)
 
 
 def test_grid_search_one_grid_point():
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
@@ -494,43 +499,43 @@
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_grid_search_sparse(csr_container):
     # Test that grid search works with both dense and sparse matrices
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
 
-    clf = LinearSVC(dual="auto")
+    clf = LinearSVC()
     cv = GridSearchCV(clf, {"C": [0.1, 1.0]})
     cv.fit(X_[:180], y_[:180])
     y_pred = cv.predict(X_[180:])
     C = cv.best_estimator_.C
 
     X_ = csr_container(X_)
-    clf = LinearSVC(dual="auto")
+    clf = LinearSVC()
     cv = GridSearchCV(clf, {"C": [0.1, 1.0]})
     cv.fit(X_[:180].tocoo(), y_[:180])
     y_pred2 = cv.predict(X_[180:])
     C2 = cv.best_estimator_.C
 
     assert np.mean(y_pred == y_pred2) >= 0.9
     assert C == C2
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_grid_search_sparse_scoring(csr_container):
     X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
 
-    clf = LinearSVC(dual="auto")
+    clf = LinearSVC()
     cv = GridSearchCV(clf, {"C": [0.1, 1.0]}, scoring="f1")
     cv.fit(X_[:180], y_[:180])
     y_pred = cv.predict(X_[180:])
     C = cv.best_estimator_.C
 
     X_ = csr_container(X_)
-    clf = LinearSVC(dual="auto")
+    clf = LinearSVC()
     cv = GridSearchCV(clf, {"C": [0.1, 1.0]}, scoring="f1")
     cv.fit(X_[:180], y_[:180])
     y_pred2 = cv.predict(X_[180:])
     C2 = cv.best_estimator_.C
 
     assert_array_equal(y_pred, y_pred2)
     assert C == C2
@@ -632,15 +637,15 @@
         Return the index of a model that has the least
         `mean_test_score`.
         """
         # Fit a dummy clf with `refit=True` to get a list of keys in
         # clf.cv_results_.
         X, y = make_classification(n_samples=100, n_features=4, random_state=42)
         clf = GridSearchCV(
-            LinearSVC(dual="auto", random_state=42),
+            LinearSVC(random_state=42),
             {"C": [0.01, 0.1, 1]},
             scoring="precision",
             refit=True,
         )
         clf.fit(X, y)
         # Ensure that `best_index_ != 0` for this dummy clf
         assert clf.best_index_ != 0
@@ -649,15 +654,15 @@
         for key in clf.cv_results_.keys():
             assert key in cv_results
 
         return cv_results["mean_test_score"].argmin()
 
     X, y = make_classification(n_samples=100, n_features=4, random_state=42)
     clf = GridSearchCV(
-        LinearSVC(dual="auto", random_state=42),
+        LinearSVC(random_state=42),
         {"C": [0.01, 0.1, 1]},
         scoring="precision",
         refit=refit_callable,
     )
     clf.fit(X, y)
 
     assert clf.best_index_ == 0
@@ -676,15 +681,15 @@
         A dummy function tests when returned 'best_index_' is not integer.
         """
         return None
 
     X, y = make_classification(n_samples=100, n_features=4, random_state=42)
 
     clf = GridSearchCV(
-        LinearSVC(dual="auto", random_state=42),
+        LinearSVC(random_state=42),
         {"C": [0.1, 1]},
         scoring="precision",
         refit=refit_callable_invalid_type,
     )
     with pytest.raises(TypeError, match="best_index_ returned is not an integer"):
         clf.fit(X, y)
 
@@ -702,15 +707,15 @@
         A dummy function tests when returned 'best_index_' is out of bounds.
         """
         return out_bound_value
 
     X, y = make_classification(n_samples=100, n_features=4, random_state=42)
 
     clf = search_cv(
-        LinearSVC(dual="auto", random_state=42),
+        LinearSVC(random_state=42),
         {"C": [0.1, 1]},
         scoring="precision",
         refit=refit_callable_out_bound,
     )
     with pytest.raises(IndexError, match="best_index_ index out of range"):
         clf.fit(X, y)
 
@@ -728,15 +733,15 @@
         """
         assert "mean_test_prec" in cv_results
         return cv_results["mean_test_prec"].argmin()
 
     X, y = make_classification(n_samples=100, n_features=4, random_state=42)
     scoring = {"Accuracy": make_scorer(accuracy_score), "prec": "precision"}
     clf = GridSearchCV(
-        LinearSVC(dual="auto", random_state=42),
+        LinearSVC(random_state=42),
         {"C": [0.01, 0.1, 1]},
         scoring=scoring,
         refit=refit_callable,
     )
     clf.fit(X, y)
 
     assert clf.best_index_ == 0
@@ -894,19 +899,23 @@
     param_distributions = {"C": uniform(0, 1)}
     sampler = ParameterSampler(
         param_distributions=param_distributions, n_iter=10, random_state=0
     )
     assert [x for x in sampler] == [x for x in sampler]
 
 
-def check_cv_results_array_types(search, param_keys, score_keys):
+def check_cv_results_array_types(
+    search, param_keys, score_keys, expected_cv_results_kinds
+):
     # Check if the search `cv_results`'s array are of correct types
     cv_results = search.cv_results_
     assert all(isinstance(cv_results[param], np.ma.MaskedArray) for param in param_keys)
-    assert all(cv_results[key].dtype == object for key in param_keys)
+    assert {
+        key: cv_results[key].dtype.kind for key in param_keys
+    } == expected_cv_results_kinds
     assert not any(isinstance(cv_results[key], np.ma.MaskedArray) for key in score_keys)
     assert all(
         cv_results[key].dtype == np.float64
         for key in score_keys
         if not key.startswith("rank")
     )
 
@@ -971,15 +980,23 @@
     assert (all(cv_results[k] >= 0) for k in score_keys if k != "rank_test_score")
     assert (
         all(cv_results[k] <= 1)
         for k in score_keys
         if "time" not in k and k != "rank_test_score"
     )
     # Check cv_results structure
-    check_cv_results_array_types(search, param_keys, score_keys)
+    expected_cv_results_kinds = {
+        "param_C": "i",
+        "param_degree": "i",
+        "param_gamma": "f",
+        "param_kernel": "O",
+    }
+    check_cv_results_array_types(
+        search, param_keys, score_keys, expected_cv_results_kinds
+    )
     check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)
     # Check masking
     cv_results = search.cv_results_
 
     poly_results = [
         (
             cv_results["param_C"].mask[i]
@@ -1040,15 +1057,23 @@
         cv=3,
         param_distributions=params,
         return_train_score=True,
     )
     search.fit(X, y)
     cv_results = search.cv_results_
     # Check results structure
-    check_cv_results_array_types(search, param_keys, score_keys)
+    expected_cv_results_kinds = {
+        "param_C": "f",
+        "param_degree": "i",
+        "param_gamma": "f",
+        "param_kernel": "O",
+    }
+    check_cv_results_array_types(
+        search, param_keys, score_keys, expected_cv_results_kinds
+    )
     check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates)
     assert all(
         (
             cv_results["param_C"].mask[i]
             and cv_results["param_gamma"].mask[i]
             and not cv_results["param_degree"].mask[i]
         )
@@ -1374,20 +1399,22 @@
 
     for est in estimators:
         grid_search = GridSearchCV(
             est,
             est_parameters,
             cv=cv,
         ).fit(X, y)
-        assert_array_equal(grid_search.cv_results_["param_random_state"], [0, None])
+        assert_array_equal(
+            grid_search.cv_results_["param_random_state"], [0, float("nan")]
+        )
 
 
 @ignore_warnings()
 def test_search_cv_timing():
-    svc = LinearSVC(dual="auto", random_state=0)
+    svc = LinearSVC(random_state=0)
 
     X = [
         [
             1,
         ],
         [
             2,
@@ -1421,15 +1448,15 @@
         assert isinstance(search.refit_time_, float)
         assert search.refit_time_ >= 0
 
 
 def test_grid_search_correct_score_results():
     # test that correct scores are used
     n_splits = 3
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     X, y = make_blobs(random_state=0, centers=2)
     Cs = [0.1, 1, 10]
     for score in ["f1", "roc_auc"]:
         grid_search = GridSearchCV(clf, {"C": Cs}, scoring=score, cv=n_splits)
         cv_results = grid_search.fit(X, y).cv_results_
 
         # Test scorer names
@@ -1760,57 +1787,57 @@
     clf.fit(X, y)
     assert not hasattr(clf, "predict_proba")
 
 
 def test_search_train_scores_set_to_false():
     X = np.arange(6).reshape(6, -1)
     y = [0, 0, 0, 1, 1, 1]
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
 
     gs = GridSearchCV(clf, param_grid={"C": [0.1, 0.2]}, cv=3)
     gs.fit(X, y)
 
 
 def test_grid_search_cv_splits_consistency():
     # Check if a one time iterable is accepted as a cv parameter.
     n_samples = 100
     n_splits = 5
     X, y = make_classification(n_samples=n_samples, random_state=0)
 
     gs = GridSearchCV(
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         param_grid={"C": [0.1, 0.2, 0.3]},
         cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples),
         return_train_score=True,
     )
     gs.fit(X, y)
 
     gs2 = GridSearchCV(
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         param_grid={"C": [0.1, 0.2, 0.3]},
         cv=KFold(n_splits=n_splits),
         return_train_score=True,
     )
     gs2.fit(X, y)
 
     # Give generator as a cv parameter
     assert isinstance(
         KFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y),
         GeneratorType,
     )
     gs3 = GridSearchCV(
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         param_grid={"C": [0.1, 0.2, 0.3]},
         cv=KFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y),
         return_train_score=True,
     )
     gs3.fit(X, y)
 
     gs4 = GridSearchCV(
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         param_grid={"C": [0.1, 0.2, 0.3]},
         cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),
         return_train_score=True,
     )
     gs4.fit(X, y)
 
     def _pop_time_keys(cv_results):
@@ -1838,15 +1865,15 @@
     np.testing.assert_equal(
         {k: v for k, v in gs.cv_results_.items() if not k.endswith("_time")},
         {k: v for k, v in gs2.cv_results_.items() if not k.endswith("_time")},
     )
 
     # Check consistency of folds across the parameters
     gs = GridSearchCV(
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         param_grid={"C": [0.1, 0.1, 0.2, 0.2]},
         cv=KFold(n_splits=n_splits, shuffle=True),
         return_train_score=True,
     )
     gs.fit(X, y)
 
     # As the first two param settings (C=0.1) and the next two param
@@ -2062,15 +2089,15 @@
     # into the search cv object
     def custom_scorer(clf, X, y):
         y_pred = clf.predict(X)
         cm = confusion_matrix(y, y_pred)
         return {"tn": cm[0, 0], "fp": cm[0, 1], "fn": cm[1, 0], "tp": cm[1, 1]}
 
     X, y = make_classification(n_samples=40, n_features=4, random_state=42)
-    est = LinearSVC(dual="auto", random_state=42)
+    est = LinearSVC(random_state=42)
     search = GridSearchCV(est, {"C": [0.1, 1]}, scoring=custom_scorer, refit="fp")
 
     search.fit(X, y)
 
     score_names = ["tn", "fp", "fn", "tp"]
     for name in score_names:
         assert "mean_test_{}".format(name) in search.cv_results_
@@ -2086,15 +2113,15 @@
         y_pred = est.predict(X)
         return {
             "recall": recall_score(y, y_pred),
             "accuracy": accuracy_score(y, y_pred),
         }
 
     X, y = make_classification(n_samples=40, n_features=4, random_state=42)
-    est = LinearSVC(dual="auto", random_state=42)
+    est = LinearSVC(random_state=42)
     search_callable = GridSearchCV(
         est, {"C": [0.1, 1]}, scoring=custom_scorer, refit="recall"
     )
     search_str = GridSearchCV(
         est, {"C": [0.1, 1]}, scoring=["recall", "accuracy"], refit="recall"
     )
 
@@ -2109,15 +2136,15 @@
 def test_callable_single_metric_same_as_single_string():
     # Tests callable scorer is the same as scoring with a single string
     def custom_scorer(est, X, y):
         y_pred = est.predict(X)
         return recall_score(y, y_pred)
 
     X, y = make_classification(n_samples=40, n_features=4, random_state=42)
-    est = LinearSVC(dual="auto", random_state=42)
+    est = LinearSVC(random_state=42)
     search_callable = GridSearchCV(
         est, {"C": [0.1, 1]}, scoring=custom_scorer, refit=True
     )
     search_str = GridSearchCV(est, {"C": [0.1, 1]}, scoring="recall", refit="recall")
     search_list_str = GridSearchCV(
         est, {"C": [0.1, 1]}, scoring=["recall"], refit="recall"
     )
@@ -2137,15 +2164,15 @@
 def test_callable_multimetric_error_on_invalid_key():
     # Raises when the callable scorer does not return a dict with `refit` key.
     def bad_scorer(est, X, y):
         return {"bad_name": 1}
 
     X, y = make_classification(n_samples=40, n_features=4, random_state=42)
     clf = GridSearchCV(
-        LinearSVC(dual="auto", random_state=42),
+        LinearSVC(random_state=42),
         {"C": [0.1, 1]},
         scoring=bad_scorer,
         refit="good_name",
     )
 
     msg = (
         "For multi-metric scoring, the parameter refit must be set to a "
@@ -2419,15 +2446,15 @@
 
 
 @pytest.mark.parametrize("return_train_score", [True, False])
 def test_search_cv_verbose_3(capsys, return_train_score):
     """Check that search cv with verbose>2 shows the score for single
     metrics. non-regression test for #19658."""
     X, y = make_classification(n_samples=100, n_classes=2, flip_y=0.2, random_state=0)
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     grid = {"C": [0.1]}
 
     GridSearchCV(
         clf,
         grid,
         scoring="accuracy",
         verbose=3,
@@ -2450,15 +2477,15 @@
         (HalvingGridSearchCV, "param_grid"),
     ],
 )
 def test_search_estimator_param(SearchCV, param_search):
     # test that SearchCV object doesn't change the object given in the parameter grid
     X, y = make_classification(random_state=42)
 
-    params = {"clf": [LinearSVC(dual="auto")], "clf__C": [0.01]}
+    params = {"clf": [LinearSVC()], "clf__C": [0.01]}
     orig_C = params["clf"][0].C
 
     pipe = Pipeline([("trs", MinimalTransformer()), ("clf", None)])
 
     param_grid_search = {param_search: params}
     gs = SearchCV(pipe, refit=True, cv=2, scoring="accuracy", **param_grid_search).fit(
         X, y
@@ -2466,14 +2493,93 @@
 
     # testing that the original object in params is not changed
     assert params["clf"][0].C == orig_C
     # testing that the GS is setting the parameter of the step correctly
     assert gs.best_estimator_.named_steps["clf"].C == 0.01
 
 
+def test_search_with_2d_array():
+    parameter_grid = {
+        "vect__ngram_range": ((1, 1), (1, 2)),  # unigrams or bigrams
+        "vect__norm": ("l1", "l2"),
+    }
+    pipeline = Pipeline(
+        [
+            ("vect", TfidfVectorizer()),
+            ("clf", ComplementNB()),
+        ]
+    )
+    random_search = RandomizedSearchCV(
+        estimator=pipeline,
+        param_distributions=parameter_grid,
+        n_iter=3,
+        random_state=0,
+        n_jobs=2,
+        verbose=1,
+        cv=3,
+    )
+    data_train = ["one", "two", "three", "four", "five"]
+    data_target = [0, 0, 1, 0, 1]
+    random_search.fit(data_train, data_target)
+    result = random_search.cv_results_["param_vect__ngram_range"]
+    expected_data = np.empty(3, dtype=object)
+    expected_data[:] = [(1, 2), (1, 2), (1, 1)]
+    np.testing.assert_array_equal(result.data, expected_data)
+
+
+def test_search_html_repr():
+    """Test different HTML representations for GridSearchCV."""
+    X, y = make_classification(random_state=42)
+
+    pipeline = Pipeline([("scale", StandardScaler()), ("clf", DummyClassifier())])
+    param_grid = {"clf": [DummyClassifier(), LogisticRegression()]}
+
+    # Unfitted shows the original pipeline
+    search_cv = GridSearchCV(pipeline, param_grid=param_grid, refit=False)
+    with config_context(display="diagram"):
+        repr_html = search_cv._repr_html_()
+        assert "<pre>DummyClassifier()</pre>" in repr_html
+
+    # Fitted with `refit=False` shows the original pipeline
+    search_cv.fit(X, y)
+    with config_context(display="diagram"):
+        repr_html = search_cv._repr_html_()
+        assert "<pre>DummyClassifier()</pre>" in repr_html
+
+    # Fitted with `refit=True` shows the best estimator
+    search_cv = GridSearchCV(pipeline, param_grid=param_grid, refit=True)
+    search_cv.fit(X, y)
+    with config_context(display="diagram"):
+        repr_html = search_cv._repr_html_()
+        assert "<pre>DummyClassifier()</pre>" not in repr_html
+        assert "<pre>LogisticRegression()</pre>" in repr_html
+
+
+# TODO(1.7): remove this test
+@pytest.mark.parametrize("SearchCV", [GridSearchCV, RandomizedSearchCV])
+def test_inverse_transform_Xt_deprecation(SearchCV):
+    clf = MockClassifier()
+    search = SearchCV(clf, {"foo_param": [1, 2, 3]}, cv=3, verbose=3)
+
+    X2 = search.fit(X, y).transform(X)
+
+    with pytest.raises(TypeError, match="Missing required positional argument"):
+        search.inverse_transform()
+
+    with pytest.raises(TypeError, match="Cannot use both X and Xt. Use X only"):
+        search.inverse_transform(X=X2, Xt=X2)
+
+    with warnings.catch_warnings(record=True):
+        warnings.simplefilter("error")
+        search.inverse_transform(X2)
+
+    with pytest.warns(FutureWarning, match="Xt was renamed X in version 1.5"):
+        search.inverse_transform(Xt=X2)
+
+
 # Metadata Routing Tests
 # ======================
 
 
 @pytest.mark.usefixtures("enable_slep006")
 @pytest.mark.parametrize(
     "SearchCV, param_search",
@@ -2486,15 +2592,15 @@
     """Test that *SearchCV forwards metadata correctly when passed multiple metrics."""
     X, y = make_classification(random_state=42)
     n_samples = _num_samples(X)
     rng = np.random.RandomState(0)
     score_weights = rng.rand(n_samples)
     score_metadata = rng.rand(n_samples)
 
-    est = LinearSVC(dual="auto")
+    est = LinearSVC()
     param_grid_search = {param_search: {"C": [1]}}
 
     scorer_registry = _Registry()
     scorer = ConsumingScorer(registry=scorer_registry).set_score_request(
         sample_weight="score_weights", metadata="score_metadata"
     )
     scoring = dict(my_scorer=scorer, accuracy="accuracy")
@@ -2520,15 +2626,15 @@
         (HalvingGridSearchCV, "param_grid"),
     ],
 )
 def test_score_rejects_params_with_no_routing_enabled(SearchCV, param_search):
     """*SearchCV should reject **params when metadata routing is not enabled
     since this is added only when routing is enabled."""
     X, y = make_classification(random_state=42)
-    est = LinearSVC(dual="auto")
+    est = LinearSVC()
     param_grid_search = {param_search: {"C": [1]}}
 
     gs = SearchCV(est, cv=2, **param_grid_search).fit(X, y)
 
     with pytest.raises(ValueError, match="is only supported if"):
         gs.score(X, y, metadata=1)
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/tests/test_split.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_split.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Test the split module"""
+
 import re
 import warnings
 from itertools import combinations, combinations_with_replacement, permutations
 
 import numpy as np
 import pytest
 from scipy import stats
@@ -77,14 +78,15 @@
 GROUP_SPLITTERS = [
     GroupKFold(),
     LeavePGroupsOut(n_groups=1),
     StratifiedGroupKFold(),
     LeaveOneGroupOut(),
     GroupShuffleSplit(),
 ]
+GROUP_SPLITTER_NAMES = set(splitter.__class__.__name__ for splitter in GROUP_SPLITTERS)
 
 ALL_SPLITTERS = NO_GROUP_SPLITTERS + GROUP_SPLITTERS  # type: ignore
 
 X = np.ones(10)
 y = np.arange(10) // 2
 test_groups = (
     np.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]),
@@ -92,14 +94,25 @@
     np.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]),
     np.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]),
     [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3],
     ["1", "1", "1", "1", "2", "2", "2", "3", "3", "3", "3", "3"],
 )
 digits = load_digits()
 
+pytestmark = pytest.mark.filterwarnings(
+    "error:The groups parameter:UserWarning:sklearn.*"
+)
+
+
+def _split(splitter, X, y, groups):
+    if splitter.__class__.__name__ in GROUP_SPLITTER_NAMES:
+        return splitter.split(X, y, groups=groups)
+    else:
+        return splitter.split(X, y)
+
 
 @ignore_warnings
 def test_cross_validator_with_default_params():
     n_samples = 4
     n_unique_groups = 4
     n_splits = 2
     p = 2
@@ -206,18 +219,18 @@
         LeaveOneGroupOut(),
         LeavePGroupsOut(n_groups=2),
         GroupKFold(n_splits=3),
         TimeSeriesSplit(),
         PredefinedSplit(test_fold=groups),
     ]
     for splitter in splitters:
-        list(splitter.split(X, y, groups))
-        list(splitter.split(X, y_2d, groups))
+        list(_split(splitter, X, y, groups=groups))
+        list(_split(splitter, X, y_2d, groups=groups))
         try:
-            list(splitter.split(X, y_multilabel, groups))
+            list(_split(splitter, X, y_multilabel, groups=groups))
         except ValueError as e:
             allowed_target_types = ("binary", "multiclass")
             msg = "Supported target types are: {}. Got 'multilabel".format(
                 allowed_target_types
             )
             assert msg in str(e)
 
@@ -423,15 +436,15 @@
     # ensure perfect stratification with StratifiedGroupKFold
     groups = np.arange(len(y))
     distr = np.bincount(y) / len(y)
 
     test_sizes = []
     random_state = None if not shuffle else 0
     skf = kfold(k, random_state=random_state, shuffle=shuffle)
-    for train, test in skf.split(X, y, groups=groups):
+    for train, test in _split(skf, X, y, groups=groups):
         assert_allclose(np.bincount(y[train]) / len(train), distr, atol=0.02)
         assert_allclose(np.bincount(y[test]) / len(test), distr, atol=0.02)
         test_sizes.append(len(test))
     assert np.ptp(test_sizes) <= 1
 
 
 @pytest.mark.parametrize("shuffle", [False, True])
@@ -449,17 +462,20 @@
     # ensure perfect stratification with StratifiedGroupKFold
     groups = np.arange(len(y))
 
     def get_splits(y):
         random_state = None if not shuffle else 0
         return [
             (list(train), list(test))
-            for train, test in kfold(
-                k, random_state=random_state, shuffle=shuffle
-            ).split(X, y, groups=groups)
+            for train, test in _split(
+                kfold(k, random_state=random_state, shuffle=shuffle),
+                X,
+                y,
+                groups=groups,
+            )
         ]
 
     splits_base = get_splits(y)
     for perm in permutations([0, 1, 2]):
         y_perm = np.take(perm, y)
         splits_perm = get_splits(y_perm)
         assert splits_perm == splits_base
@@ -484,15 +500,15 @@
     y = [0] * 3 + [1] * 14
     # ensure perfect stratification with StratifiedGroupKFold
     groups = np.arange(len(y))
 
     for shuffle in (True, False):
         cv = kfold(3, shuffle=shuffle)
         for i in range(11, 17):
-            skf = cv.split(X[:i], y[:i], groups[:i])
+            skf = _split(cv, X[:i], y[:i], groups[:i])
             sizes = [len(test) for _, test in skf]
 
             assert (np.max(sizes) - np.min(sizes)) <= 1
             assert np.sum(sizes) == i
 
 
 def test_shuffle_kfold():
@@ -528,24 +544,24 @@
     groups_2 = np.arange(len(y2))
 
     # Check that when the shuffle is True, multiple split calls produce the
     # same split when random_state is int
     kf = kfold(3, shuffle=True, random_state=0)
 
     np.testing.assert_equal(
-        list(kf.split(X, y, groups_1)), list(kf.split(X, y, groups_1))
+        list(_split(kf, X, y, groups_1)), list(_split(kf, X, y, groups_1))
     )
 
     # Check that when the shuffle is True, multiple split calls often
     # (not always) produce different splits when random_state is
     # RandomState instance or None
     kf = kfold(3, shuffle=True, random_state=np.random.RandomState(0))
     for data in zip((X, X2), (y, y2), (groups_1, groups_2)):
         # Test if the two splits are different cv
-        for (_, test_a), (_, test_b) in zip(kf.split(*data), kf.split(*data)):
+        for (_, test_a), (_, test_b) in zip(_split(kf, *data), _split(kf, *data)):
             # cv.split(...) returns an array of tuples, each tuple
             # consisting of an array with train indices and test indices
             # Ensure that the splits for data are not same
             # when random state is not set
             with pytest.raises(AssertionError):
                 np.testing.assert_array_equal(test_a, test_b)
 
@@ -1854,14 +1870,15 @@
 
     # Verify proper error is thrown
     with pytest.raises(ValueError, match="Too many splits.*and gap"):
         splits = TimeSeriesSplit(n_splits=4, gap=2).split(X)
         next(splits)
 
 
+@ignore_warnings
 def test_nested_cv():
     # Test if nested cross validation works with different combinations of cv
     rng = np.random.RandomState(0)
 
     X, y = make_classification(n_samples=15, n_classes=2, random_state=0)
     groups = rng.randint(0, 5, 15)
 
@@ -1909,15 +1926,15 @@
     with pytest.raises(
         ValueError,
         match=(
             "With n_samples=1, test_size=0.99 and train_size=None, "
             "the resulting train set will be empty"
         ),
     ):
-        next(cv.split(X, y, groups=[1]))
+        next(_split(cv, X, y, groups=[1]))
 
 
 def test_train_test_split_empty_trainset():
     (X,) = [[1]]  # 1 sample
     with pytest.raises(
         ValueError,
         match=(
@@ -1949,15 +1966,15 @@
 def test_leave_p_out_empty_trainset():
     # No need to check LeavePGroupsOut
     cv = LeavePOut(p=2)
     X, y = [[1], [2]], [0, 3]  # 2 samples
     with pytest.raises(
         ValueError, match="p=2 must be strictly less than the number of samples=2"
     ):
-        next(cv.split(X, y, groups=[1, 2]))
+        next(cv.split(X, y))
 
 
 @pytest.mark.parametrize("Klass", (KFold, StratifiedKFold, StratifiedGroupKFold))
 def test_random_state_shuffle_false(Klass):
     # passing a non-default random_state when shuffle=False makes no sense
     with pytest.raises(ValueError, match="has no effect since shuffle is False"):
         Klass(3, shuffle=False, random_state=0)
@@ -2019,7 +2036,21 @@
 @pytest.mark.parametrize("cv", ALL_SPLITTERS, ids=[str(cv) for cv in ALL_SPLITTERS])
 def test_splitter_set_split_request(cv):
     """Check set_split_request is defined for group splitters and not for others."""
     if cv in GROUP_SPLITTERS:
         assert hasattr(cv, "set_split_request")
     elif cv in NO_GROUP_SPLITTERS:
         assert not hasattr(cv, "set_split_request")
+
+
+@pytest.mark.parametrize("cv", NO_GROUP_SPLITTERS, ids=str)
+def test_no_group_splitters_warns_with_groups(cv):
+    msg = f"The groups parameter is ignored by {cv.__class__.__name__}"
+
+    n_samples = 30
+    rng = np.random.RandomState(1)
+    X = rng.randint(0, 3, size=(n_samples, 2))
+    y = rng.randint(0, 3, size=(n_samples,))
+    groups = rng.randint(0, 3, size=(n_samples,))
+
+    with pytest.warns(UserWarning, match=msg):
+        cv.split(X, y, groups=groups)
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/tests/test_successive_halving.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_successive_halving.py`

 * *Files 1% similar despite different names*

```diff
@@ -728,15 +728,15 @@
     # HalvingGridSearchCV and HalvingRandomSearchCV
     # And also check if groups is correctly passed to the cv object
     rng = np.random.RandomState(0)
 
     X, y = make_classification(n_samples=50, n_classes=2, random_state=0)
     groups = rng.randint(0, 3, 50)
 
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     grid = {"C": [1]}
 
     group_cvs = [
         LeaveOneGroupOut(),
         LeavePGroupsOut(2),
         GroupKFold(n_splits=3),
         GroupShuffleSplit(random_state=0),
@@ -822,15 +822,23 @@
         SVC(), cv=3, param_distributions=params, return_train_score=True, random_state=0
     )
     search.fit(X, y)
     n_candidates = sum(search.n_candidates_)
     cv_results = search.cv_results_
     # Check results structure
     check_cv_results_keys(cv_results, param_keys, score_keys, n_candidates, extra_keys)
-    check_cv_results_array_types(search, param_keys, score_keys)
+    expected_cv_results_kinds = {
+        "param_C": "f",
+        "param_degree": "i",
+        "param_gamma": "f",
+        "param_kernel": "O",
+    }
+    check_cv_results_array_types(
+        search, param_keys, score_keys, expected_cv_results_kinds
+    )
 
     assert all(
         (
             cv_results["param_C"].mask[i]
             and cv_results["param_gamma"].mask[i]
             and not cv_results["param_degree"].mask[i]
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/model_selection/tests/test_validation.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/tests/test_validation.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Test the validation module"""
+
 import os
 import re
 import sys
 import tempfile
 import warnings
 from functools import partial
 from io import StringIO
@@ -39,14 +40,15 @@
     explained_variance_score,
     make_scorer,
     mean_squared_error,
     precision_recall_fscore_support,
     precision_score,
     r2_score,
 )
+from sklearn.metrics._scorer import _MultimetricScorer
 from sklearn.model_selection import (
     GridSearchCV,
     GroupKFold,
     GroupShuffleSplit,
     KFold,
     LeaveOneGroupOut,
     LeaveOneOut,
@@ -2319,24 +2321,28 @@
             (1, 3),
             (0, 1),
             r"\[CV\] END ...................................................."
             r" total time=   0.\ds",
         ),
         (
             True,
-            {"sc1": three_params_scorer, "sc2": three_params_scorer},
+            _MultimetricScorer(
+                scorers={"sc1": three_params_scorer, "sc2": three_params_scorer}
+            ),
             3,
             (1, 3),
             (0, 1),
             r"\[CV 2/3\] END  sc1: \(train=3.421, test=3.421\) sc2: "
             r"\(train=3.421, test=3.421\) total time=   0.\ds",
         ),
         (
             False,
-            {"sc1": three_params_scorer, "sc2": three_params_scorer},
+            _MultimetricScorer(
+                scorers={"sc1": three_params_scorer, "sc2": three_params_scorer}
+            ),
             10,
             (1, 3),
             (0, 1),
             r"\[CV 2/3; 1/1\] END ....... sc1: \(test=3.421\) sc2: \(test=3.421\)"
             r" total time=   0.\ds",
         ),
     ],
@@ -2393,15 +2399,15 @@
 def test_callable_multimetric_confusion_matrix_cross_validate():
     def custom_scorer(clf, X, y):
         y_pred = clf.predict(X)
         cm = confusion_matrix(y, y_pred)
         return {"tn": cm[0, 0], "fp": cm[0, 1], "fn": cm[1, 0], "tp": cm[1, 1]}
 
     X, y = make_classification(n_samples=40, n_features=4, random_state=42)
-    est = LinearSVC(dual="auto", random_state=42)
+    est = LinearSVC(random_state=42)
     est.fit(X, y)
     cv_results = cross_validate(est, X, y, cv=5, scoring=custom_scorer)
 
     score_names = ["tn", "fp", "fn", "tp"]
     for name in score_names:
         assert "test_{}".format(name) in cv_results
```

### Comparing `scikit-learn-1.4.2/sklearn/multiclass.py` & `scikit_learn-1.5.0rc1/sklearn/multiclass.py`

 * *Files 1% similar despite different names*

```diff
@@ -615,16 +615,16 @@
 
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
             .add(
                 estimator=self.estimator,
                 method_mapping=MethodMapping()
-                .add(callee="fit", caller="fit")
-                .add(callee="partial_fit", caller="partial_fit"),
+                .add(caller="fit", callee="fit")
+                .add(caller="partial_fit", callee="partial_fit"),
             )
         )
         return router
 
 
 def _fit_ovo_binary(estimator, X, y, i, j, fit_params):
     """Fit a single binary estimator (one-vs-one)."""
@@ -734,15 +734,15 @@
     >>> from sklearn.model_selection import train_test_split
     >>> from sklearn.multiclass import OneVsOneClassifier
     >>> from sklearn.svm import LinearSVC
     >>> X, y = load_iris(return_X_y=True)
     >>> X_train, X_test, y_train, y_test = train_test_split(
     ...     X, y, test_size=0.33, shuffle=True, random_state=0)
     >>> clf = OneVsOneClassifier(
-    ...     LinearSVC(dual="auto", random_state=0)).fit(X_train, y_train)
+    ...     LinearSVC(random_state=0)).fit(X_train, y_train)
     >>> clf.predict(X_test[:10])
     array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1])
     """
 
     _parameter_constraints: dict = {
         "estimator": [HasMethods(["fit"])],
         "n_jobs": [Integral, None],
@@ -1014,16 +1014,16 @@
 
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
             .add(
                 estimator=self.estimator,
                 method_mapping=MethodMapping()
-                .add(callee="fit", caller="fit")
-                .add(callee="partial_fit", caller="partial_fit"),
+                .add(caller="fit", callee="fit")
+                .add(caller="partial_fit", callee="partial_fit"),
             )
         )
         return router
 
 
 class OutputCodeClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):
     """(Error-Correcting) Output-Code multiclass strategy.
@@ -1260,10 +1260,10 @@
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
 
         router = MetadataRouter(owner=self.__class__.__name__).add(
             estimator=self.estimator,
-            method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
         return router
```

### Comparing `scikit-learn-1.4.2/sklearn/multioutput.py` & `scikit_learn-1.5.0rc1/sklearn/multioutput.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,27 +27,34 @@
     MetaEstimatorMixin,
     RegressorMixin,
     _fit_context,
     clone,
     is_classifier,
 )
 from .model_selection import cross_val_predict
-from .utils import Bunch, _print_elapsed_time, check_random_state
+from .utils import Bunch, check_random_state
 from .utils._param_validation import HasMethods, StrOptions
+from .utils._response import _get_response_values
+from .utils._user_interface import _print_elapsed_time
 from .utils.metadata_routing import (
     MetadataRouter,
     MethodMapping,
     _raise_for_params,
     _routing_enabled,
     process_routing,
 )
 from .utils.metaestimators import available_if
 from .utils.multiclass import check_classification_targets
 from .utils.parallel import Parallel, delayed
-from .utils.validation import _check_method_params, check_is_fitted, has_fit_parameter
+from .utils.validation import (
+    _check_method_params,
+    _check_response_method,
+    check_is_fitted,
+    has_fit_parameter,
+)
 
 __all__ = [
     "MultiOutputRegressor",
     "MultiOutputClassifier",
     "ClassifierChain",
     "RegressorChain",
 ]
@@ -323,16 +330,16 @@
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
         router = MetadataRouter(owner=self.__class__.__name__).add(
             estimator=self.estimator,
             method_mapping=MethodMapping()
-            .add(callee="partial_fit", caller="partial_fit")
-            .add(callee="fit", caller="fit"),
+            .add(caller="partial_fit", callee="partial_fit")
+            .add(caller="fit", callee="fit"),
         )
         return router
 
 
 class MultiOutputRegressor(RegressorMixin, _MultiOutputEstimator):
     """Multi target regression.
 
@@ -646,14 +653,54 @@
         self.verbose = verbose
 
     def _log_message(self, *, estimator_idx, n_estimators, processing_msg):
         if not self.verbose:
             return None
         return f"({estimator_idx} of {n_estimators}) {processing_msg}"
 
+    def _get_predictions(self, X, *, output_method):
+        """Get predictions for each model in the chain."""
+        check_is_fitted(self)
+        X = self._validate_data(X, accept_sparse=True, reset=False)
+        Y_output_chain = np.zeros((X.shape[0], len(self.estimators_)))
+        Y_feature_chain = np.zeros((X.shape[0], len(self.estimators_)))
+
+        # `RegressorChain` does not have a `chain_method_` parameter so we
+        # default to "predict"
+        chain_method = getattr(self, "chain_method_", "predict")
+        hstack = sp.hstack if sp.issparse(X) else np.hstack
+        for chain_idx, estimator in enumerate(self.estimators_):
+            previous_predictions = Y_feature_chain[:, :chain_idx]
+            # if `X` is a scipy sparse dok_array, we convert it to a sparse
+            # coo_array format before hstacking, it's faster; see
+            # https://github.com/scipy/scipy/issues/20060#issuecomment-1937007039:
+            if sp.issparse(X) and not sp.isspmatrix(X) and X.format == "dok":
+                X = sp.coo_array(X)
+            X_aug = hstack((X, previous_predictions))
+
+            feature_predictions, _ = _get_response_values(
+                estimator,
+                X_aug,
+                response_method=chain_method,
+            )
+            Y_feature_chain[:, chain_idx] = feature_predictions
+
+            output_predictions, _ = _get_response_values(
+                estimator,
+                X_aug,
+                response_method=output_method,
+            )
+            Y_output_chain[:, chain_idx] = output_predictions
+
+        inv_order = np.empty_like(self.order_)
+        inv_order[self.order_] = np.arange(len(self.order_))
+        Y_output = Y_output_chain[:, inv_order]
+
+        return Y_output
+
     @abstractmethod
     def fit(self, X, Y, **fit_params):
         """Fit the model to data matrix X and targets Y.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
@@ -694,28 +741,50 @@
             if sp.issparse(X):
                 X_aug = sp.hstack((X, Y_pred_chain), format="lil")
                 X_aug = X_aug.tocsr()
             else:
                 X_aug = np.hstack((X, Y_pred_chain))
 
         elif sp.issparse(X):
-            Y_pred_chain = sp.lil_matrix((X.shape[0], Y.shape[1]))
+            # TODO: remove this condition check when the minimum supported scipy version
+            # doesn't support sparse matrices anymore
+            if not sp.isspmatrix(X):
+                # if `X` is a scipy sparse dok_array, we convert it to a sparse
+                # coo_array format before hstacking, it's faster; see
+                # https://github.com/scipy/scipy/issues/20060#issuecomment-1937007039:
+                if X.format == "dok":
+                    X = sp.coo_array(X)
+                # in case that `X` is a sparse array we create `Y_pred_chain` as a
+                # sparse array format:
+                Y_pred_chain = sp.coo_array((X.shape[0], Y.shape[1]))
+            else:
+                Y_pred_chain = sp.coo_matrix((X.shape[0], Y.shape[1]))
             X_aug = sp.hstack((X, Y_pred_chain), format="lil")
 
         else:
             Y_pred_chain = np.zeros((X.shape[0], Y.shape[1]))
             X_aug = np.hstack((X, Y_pred_chain))
 
         del Y_pred_chain
 
         if _routing_enabled():
             routed_params = process_routing(self, "fit", **fit_params)
         else:
             routed_params = Bunch(estimator=Bunch(fit=fit_params))
 
+        if hasattr(self, "chain_method"):
+            chain_method = _check_response_method(
+                self.base_estimator,
+                self.chain_method,
+            ).__name__
+            self.chain_method_ = chain_method
+        else:
+            # `RegressorChain` does not have a `chain_method` parameter
+            chain_method = "predict"
+
         for chain_idx, estimator in enumerate(self.estimators_):
             message = self._log_message(
                 estimator_idx=chain_idx + 1,
                 n_estimators=len(self.estimators_),
                 processing_msg=f"Processing order {self.order_[chain_idx]}",
             )
             y = Y[:, self.order_[chain_idx]]
@@ -725,16 +794,23 @@
                     y,
                     **routed_params.estimator.fit,
                 )
 
             if self.cv is not None and chain_idx < len(self.estimators_) - 1:
                 col_idx = X.shape[1] + chain_idx
                 cv_result = cross_val_predict(
-                    self.base_estimator, X_aug[:, :col_idx], y=y, cv=self.cv
+                    self.base_estimator,
+                    X_aug[:, :col_idx],
+                    y=y,
+                    cv=self.cv,
+                    method=chain_method,
                 )
+                # `predict_proba` output is 2D, we use only output for classes[-1]
+                if cv_result.ndim > 1:
+                    cv_result = cv_result[:, 1]
                 if sp.issparse(X_aug):
                     X_aug[:, col_idx] = np.expand_dims(cv_result, 1)
                 else:
                     X_aug[:, col_idx] = cv_result
 
         return self
 
@@ -747,33 +823,15 @@
             The input data.
 
         Returns
         -------
         Y_pred : array-like of shape (n_samples, n_classes)
             The predicted values.
         """
-        check_is_fitted(self)
-        X = self._validate_data(X, accept_sparse=True, reset=False)
-        Y_pred_chain = np.zeros((X.shape[0], len(self.estimators_)))
-        for chain_idx, estimator in enumerate(self.estimators_):
-            previous_predictions = Y_pred_chain[:, :chain_idx]
-            if sp.issparse(X):
-                if chain_idx == 0:
-                    X_aug = X
-                else:
-                    X_aug = sp.hstack((X, previous_predictions))
-            else:
-                X_aug = np.hstack((X, previous_predictions))
-            Y_pred_chain[:, chain_idx] = estimator.predict(X_aug)
-
-        inv_order = np.empty_like(self.order_)
-        inv_order[self.order_] = np.arange(len(self.order_))
-        Y_pred = Y_pred_chain[:, inv_order]
-
-        return Y_pred
+        return self._get_predictions(X, output_method="predict")
 
 
 class ClassifierChain(MetaEstimatorMixin, ClassifierMixin, _BaseChain):
     """A multi-label model that arranges binary classifiers into a chain.
 
     Each model makes a prediction in the order specified by the chain using
     all of the available features provided to the model plus the predictions
@@ -816,14 +874,27 @@
         Possible inputs for cv are:
 
         - None, to use true labels when fitting,
         - integer, to specify the number of folds in a (Stratified)KFold,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.
 
+    chain_method : {'predict', 'predict_proba', 'predict_log_proba', \
+            'decision_function'} or list of such str's, default='predict'
+
+        Prediction method to be used by estimators in the chain for
+        the 'prediction' features of previous estimators in the chain.
+
+        - if `str`, name of the method;
+        - if a list of `str`, provides the method names in order of
+          preference. The method used corresponds to the first method in
+          the list that is implemented by `base_estimator`.
+
+        .. versionadded:: 1.5
+
     random_state : int, RandomState instance or None, optional (default=None)
         If ``order='random'``, determines random number generation for the
         chain order.
         In addition, it controls the random seed given at each `base_estimator`
         at each chaining iteration. Thus, it is only used when `base_estimator`
         exposes a `random_state`.
         Pass an int for reproducible output across multiple function calls.
@@ -842,14 +913,18 @@
 
     estimators_ : list
         A list of clones of base_estimator.
 
     order_ : list
         The order of labels in the classifier chain.
 
+    chain_method_ : str
+        Prediction method used by estimators in the chain for the prediction
+        features.
+
     n_features_in_ : int
         Number of features seen during :term:`fit`. Only defined if the
         underlying `base_estimator` exposes such an attribute when fit.
 
         .. versionadded:: 0.24
 
     feature_names_in_ : ndarray of shape (`n_features_in_`,)
@@ -889,14 +964,44 @@
            [0., 1., 0.]])
     >>> chain.predict_proba(X_test)
     array([[0.8387..., 0.9431..., 0.4576...],
            [0.8878..., 0.3684..., 0.2640...],
            [0.0321..., 0.9935..., 0.0626...]])
     """
 
+    _parameter_constraints: dict = {
+        **_BaseChain._parameter_constraints,
+        "chain_method": [
+            list,
+            tuple,
+            StrOptions(
+                {"predict", "predict_proba", "predict_log_proba", "decision_function"}
+            ),
+        ],
+    }
+
+    def __init__(
+        self,
+        base_estimator,
+        *,
+        order=None,
+        cv=None,
+        chain_method="predict",
+        random_state=None,
+        verbose=False,
+    ):
+        super().__init__(
+            base_estimator,
+            order=order,
+            cv=cv,
+            random_state=random_state,
+            verbose=verbose,
+        )
+        self.chain_method = chain_method
+
     @_fit_context(
         # ClassifierChain.base_estimator is not validated yet
         prefer_skip_nested_validation=False
     )
     def fit(self, X, Y, **fit_params):
         """Fit the model to data matrix X and targets Y.
 
@@ -937,30 +1042,15 @@
             The input data.
 
         Returns
         -------
         Y_prob : array-like of shape (n_samples, n_classes)
             The predicted probabilities.
         """
-        X = self._validate_data(X, accept_sparse=True, reset=False)
-        Y_prob_chain = np.zeros((X.shape[0], len(self.estimators_)))
-        Y_pred_chain = np.zeros((X.shape[0], len(self.estimators_)))
-        for chain_idx, estimator in enumerate(self.estimators_):
-            previous_predictions = Y_pred_chain[:, :chain_idx]
-            if sp.issparse(X):
-                X_aug = sp.hstack((X, previous_predictions))
-            else:
-                X_aug = np.hstack((X, previous_predictions))
-            Y_prob_chain[:, chain_idx] = estimator.predict_proba(X_aug)[:, 1]
-            Y_pred_chain[:, chain_idx] = estimator.predict(X_aug)
-        inv_order = np.empty_like(self.order_)
-        inv_order[self.order_] = np.arange(len(self.order_))
-        Y_prob = Y_prob_chain[:, inv_order]
-
-        return Y_prob
+        return self._get_predictions(X, output_method="predict_proba")
 
     def predict_log_proba(self, X):
         """Predict logarithm of probability estimates.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
@@ -984,31 +1074,15 @@
 
         Returns
         -------
         Y_decision : array-like of shape (n_samples, n_classes)
             Returns the decision function of the sample for each model
             in the chain.
         """
-        X = self._validate_data(X, accept_sparse=True, reset=False)
-        Y_decision_chain = np.zeros((X.shape[0], len(self.estimators_)))
-        Y_pred_chain = np.zeros((X.shape[0], len(self.estimators_)))
-        for chain_idx, estimator in enumerate(self.estimators_):
-            previous_predictions = Y_pred_chain[:, :chain_idx]
-            if sp.issparse(X):
-                X_aug = sp.hstack((X, previous_predictions))
-            else:
-                X_aug = np.hstack((X, previous_predictions))
-            Y_decision_chain[:, chain_idx] = estimator.decision_function(X_aug)
-            Y_pred_chain[:, chain_idx] = estimator.predict(X_aug)
-
-        inv_order = np.empty_like(self.order_)
-        inv_order[self.order_] = np.arange(len(self.order_))
-        Y_decision = Y_decision_chain[:, inv_order]
-
-        return Y_decision
+        return self._get_predictions(X, output_method="decision_function")
 
     def get_metadata_routing(self):
         """Get metadata routing of this object.
 
         Please check :ref:`User Guide <metadata_routing>` on how the routing
         mechanism works.
 
@@ -1018,15 +1092,15 @@
         -------
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
         router = MetadataRouter(owner=self.__class__.__name__).add(
             estimator=self.base_estimator,
-            method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
         return router
 
     def _more_tags(self):
         return {"_skip_test": True, "multioutput_only": True}
 
 
@@ -1113,15 +1187,15 @@
     MultiOutputRegressor : Learns each output independently rather than
         chaining.
 
     Examples
     --------
     >>> from sklearn.multioutput import RegressorChain
     >>> from sklearn.linear_model import LogisticRegression
-    >>> logreg = LogisticRegression(solver='lbfgs',multi_class='multinomial')
+    >>> logreg = LogisticRegression(solver='lbfgs')
     >>> X, Y = [[1, 0], [0, 1], [1, 1]], [[0, 2], [1, 1], [2, 0]]
     >>> chain = RegressorChain(base_estimator=logreg, order=[0, 1]).fit(X, Y)
     >>> chain.predict(X)
     array([[0., 2.],
            [1., 1.],
            [2., 0.]])
     """
@@ -1167,13 +1241,13 @@
         -------
         routing : MetadataRouter
             A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
             routing information.
         """
         router = MetadataRouter(owner=self.__class__.__name__).add(
             estimator=self.base_estimator,
-            method_mapping=MethodMapping().add(callee="fit", caller="fit"),
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
         return router
 
     def _more_tags(self):
         return {"multioutput_only": True}
```

### Comparing `scikit-learn-1.4.2/sklearn/naive_bayes.py` & `scikit_learn-1.5.0rc1/sklearn/naive_bayes.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_ball_tree.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_ball_tree.pyx.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_base.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Base and mixin classes for nearest neighbors."""
+
 # Authors: Jake Vanderplas <vanderplas@astro.washington.edu>
 #          Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #          Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #          Sparseness support by Lars Buitinck
 #          Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>
 #
 # License: BSD 3 clause (C) INRIA, University of Amsterdam
@@ -22,23 +23,22 @@
 from ..metrics import DistanceMetric, pairwise_distances_chunked
 from ..metrics._pairwise_distances_reduction import (
     ArgKmin,
     RadiusNeighbors,
 )
 from ..metrics.pairwise import PAIRWISE_DISTANCE_FUNCTIONS
 from ..utils import (
-    _to_object_array,
     check_array,
     gen_even_slices,
 )
 from ..utils._param_validation import Interval, StrOptions, validate_params
 from ..utils.fixes import parse_version, sp_base_version
 from ..utils.multiclass import check_classification_targets
 from ..utils.parallel import Parallel, delayed
-from ..utils.validation import check_is_fitted, check_non_negative
+from ..utils.validation import _to_object_array, check_is_fitted, check_non_negative
 from ._ball_tree import BallTree
 from ._kd_tree import KDTree
 
 SCIPY_METRICS = [
     "braycurtis",
     "canberra",
     "chebyshev",
@@ -441,16 +441,15 @@
 
         if callable(self.metric):
             if self.algorithm == "kd_tree":
                 # callable metric is only valid for brute force and ball_tree
                 raise ValueError(
                     "kd_tree does not support callable metric '%s'"
                     "Function call overhead will result"
-                    "in very poor performance."
-                    % self.metric
+                    "in very poor performance." % self.metric
                 )
         elif self.metric not in VALID_METRICS[alg_check] and not isinstance(
             self.metric, DistanceMetric
         ):
             raise ValueError(
                 "Metric '%s' not valid. Use "
                 "sorted(sklearn.neighbors.VALID_METRICS['%s']) "
@@ -895,16 +894,15 @@
                 )
             )
 
         elif self._fit_method in ["ball_tree", "kd_tree"]:
             if issparse(X):
                 raise ValueError(
                     "%s does not work with sparse matrices. Densify the data, "
-                    "or set algorithm='brute'"
-                    % self._fit_method
+                    "or set algorithm='brute'" % self._fit_method
                 )
             chunked_results = Parallel(n_jobs, prefer="threads")(
                 delayed(_tree_query_parallel_helper)(
                     self._tree, X[s], n_neighbors, return_distance
                 )
                 for s in gen_even_slices(X.shape[0], n_jobs)
             )
@@ -1250,16 +1248,15 @@
                     neigh_dist[ii] = neigh_dist[ii][order]
                 results = neigh_dist, neigh_ind
 
         elif self._fit_method in ["ball_tree", "kd_tree"]:
             if issparse(X):
                 raise ValueError(
                     "%s does not work with sparse matrices. Densify the data, "
-                    "or set algorithm='brute'"
-                    % self._fit_method
+                    "or set algorithm='brute'" % self._fit_method
                 )
 
             n_jobs = effective_n_jobs(self.n_jobs)
             delayed_query = delayed(_tree_query_radius_parallel_helper)
             chunked_results = Parallel(n_jobs, prefer="threads")(
                 delayed_query(
                     self._tree, X[s], radius, return_distance, sort_results=sort_results
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_binary_tree.pxi.tp` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_binary_tree.pxi.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_classification.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_classification.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_graph.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -41,15 +41,15 @@
         X = None
 
     return X
 
 
 @validate_params(
     {
-        "X": ["array-like", KNeighborsMixin],
+        "X": ["array-like", "sparse matrix", KNeighborsMixin],
         "n_neighbors": [Interval(Integral, 1, None, closed="left")],
         "mode": [StrOptions({"connectivity", "distance"})],
         "metric": [StrOptions(set(itertools.chain(*VALID_METRICS.values()))), callable],
         "p": [Interval(Real, 0, None, closed="right"), None],
         "metric_params": [dict, None],
         "include_self": ["boolean", StrOptions({"auto"})],
         "n_jobs": [Integral, None],
@@ -69,15 +69,15 @@
 ):
     """Compute the (weighted) graph of k-Neighbors for points in X.
 
     Read more in the :ref:`User Guide <unsupervised_neighbors>`.
 
     Parameters
     ----------
-    X : array-like of shape (n_samples, n_features)
+    X : {array-like, sparse matrix} of shape (n_samples, n_features)
         Sample data.
 
     n_neighbors : int
         Number of neighbors for each sample.
 
     mode : {'connectivity', 'distance'}, default='connectivity'
         Type of returned matrix: 'connectivity' will return the connectivity
@@ -146,15 +146,15 @@
 
     query = _query_include_self(X._fit_X, include_self, mode)
     return X.kneighbors_graph(X=query, n_neighbors=n_neighbors, mode=mode)
 
 
 @validate_params(
     {
-        "X": ["array-like", RadiusNeighborsMixin],
+        "X": ["array-like", "sparse matrix", RadiusNeighborsMixin],
         "radius": [Interval(Real, 0, None, closed="both")],
         "mode": [StrOptions({"connectivity", "distance"})],
         "metric": [StrOptions(set(itertools.chain(*VALID_METRICS.values()))), callable],
         "p": [Interval(Real, 0, None, closed="right"), None],
         "metric_params": [dict, None],
         "include_self": ["boolean", StrOptions({"auto"})],
         "n_jobs": [Integral, None],
@@ -177,15 +177,15 @@
     Neighborhoods are restricted the points at a distance lower than
     radius.
 
     Read more in the :ref:`User Guide <unsupervised_neighbors>`.
 
     Parameters
     ----------
-    X : array-like of shape (n_samples, n_features)
+    X : {array-like, sparse matrix} of shape (n_samples, n_features)
         Sample data.
 
     radius : float
         Radius of neighborhoods.
 
     mode : {'connectivity', 'distance'}, default='connectivity'
         Type of returned matrix: 'connectivity' will return the connectivity
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_kd_tree.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_kd_tree.pyx.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_kde.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_kde.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 Kernel Density Estimation
 -------------------------
 """
+
 # Author: Jake Vanderplas <jakevdp@cs.washington.edu>
 import itertools
 from numbers import Integral, Real
 
 import numpy as np
 from scipy.special import gammainc
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_lof.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_lof.py`

 * *Files 2% similar despite different names*

```diff
@@ -369,17 +369,17 @@
         -------
         is_inlier : ndarray of shape (n_samples,)
             Returns -1 for anomalies/outliers and +1 for inliers.
         """
         check_is_fitted(self)
 
         if X is not None:
-            X = check_array(X, accept_sparse="csr")
-            is_inlier = np.ones(X.shape[0], dtype=int)
-            is_inlier[self.decision_function(X) < 0] = -1
+            shifted_opposite_lof_scores = self.decision_function(X)
+            is_inlier = np.ones(shifted_opposite_lof_scores.shape[0], dtype=int)
+            is_inlier[shifted_opposite_lof_scores < 0] = -1
         else:
             is_inlier = np.ones(self.n_samples_fit_, dtype=int)
             is_inlier[self.negative_outlier_factor_ < self.offset_] = -1
 
         return is_inlier
 
     def _check_novelty_decision_function(self):
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_nca.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_nca.py`

 * *Files 1% similar despite different names*

```diff
@@ -319,15 +319,14 @@
 
         # Call the optimizer
         self.n_iter_ = 0
         opt_result = minimize(**optimizer_params)
 
         # Reshape the solution found by the optimizer
         self.components_ = opt_result.x.reshape(-1, X.shape[1])
-        self._n_features_out = self.components_.shape[1]
 
         # Stop timer
         t_train = time.time() - t_train
         if self.verbose:
             cls_name = self.__class__.__name__
 
             # Warn the user if the algorithm did not converge
@@ -519,7 +518,12 @@
             )
             sys.stdout.flush()
 
         return sign * loss, sign * gradient.ravel()
 
     def _more_tags(self):
         return {"requires_y": True}
+
+    @property
+    def _n_features_out(self):
+        """Number of transformed output features."""
+        return self.components_.shape[0]
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_nearest_centroid.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_nearest_centroid.py`

 * *Files 23% similar despite different names*

```diff
@@ -3,22 +3,19 @@
 """
 
 # Author: Robert Layton <robertlayton@gmail.com>
 #         Olivier Grisel <olivier.grisel@ensta.org>
 #
 # License: BSD 3 clause
 
-import warnings
 from numbers import Real
 
 import numpy as np
 from scipy import sparse as sp
 
-from sklearn.metrics.pairwise import _VALID_METRICS
-
 from ..base import BaseEstimator, ClassifierMixin, _fit_context
 from ..metrics.pairwise import pairwise_distances_argmin
 from ..preprocessing import LabelEncoder
 from ..utils._param_validation import Interval, StrOptions
 from ..utils.multiclass import check_classification_targets
 from ..utils.sparsefuncs import csc_median_axis_0
 from ..utils.validation import check_is_fitted
@@ -30,33 +27,25 @@
     Each class is represented by its centroid, with test samples classified to
     the class with the nearest centroid.
 
     Read more in the :ref:`User Guide <nearest_centroid_classifier>`.
 
     Parameters
     ----------
-    metric : str or callable, default="euclidean"
-        Metric to use for distance computation. See the documentation of
-        `scipy.spatial.distance
-        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and
-        the metrics listed in
-        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric
-        values. Note that "wminkowski", "seuclidean" and "mahalanobis" are not
-        supported.
-
-        The centroids for the samples corresponding to each class is
-        the point from which the sum of the distances (according to the metric)
-        of all samples that belong to that particular class are minimized.
-        If the `"manhattan"` metric is provided, this centroid is the median
-        and for all other metrics, the centroid is now set to be the mean.
-
-        .. deprecated:: 1.3
-            Support for metrics other than `euclidean` and `manhattan` and for
-            callables was deprecated in version 1.3 and will be removed in
-            version 1.5.
+    metric : {"euclidean", "manhattan"}, default="euclidean"
+        Metric to use for distance computation.
+
+        If `metric="euclidean"`, the centroid for the samples corresponding to each
+        class is the arithmetic mean, which minimizes the sum of squared L1 distances.
+        If `metric="manhattan"`, the centroid is the feature-wise median, which
+        minimizes the sum of L1 distances.
+
+        .. versionchanged:: 1.5
+            All metrics but `"euclidean"` and `"manhattan"` were deprecated and
+            now raise an error.
 
         .. versionchanged:: 0.19
             `metric='precomputed'` was deprecated and now raises an error
 
     shrink_threshold : float, default=None
         Threshold for shrinking centroids to remove features.
 
@@ -104,23 +93,16 @@
     >>> clf = NearestCentroid()
     >>> clf.fit(X, y)
     NearestCentroid()
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
     """
 
-    _valid_metrics = set(_VALID_METRICS) - {"mahalanobis", "seuclidean", "wminkowski"}
-
     _parameter_constraints: dict = {
-        "metric": [
-            StrOptions(
-                _valid_metrics, deprecated=_valid_metrics - {"manhattan", "euclidean"}
-            ),
-            callable,
-        ],
+        "metric": [StrOptions({"manhattan", "euclidean"})],
         "shrink_threshold": [Interval(Real, 0, None, closed="neither"), None],
     }
 
     def __init__(self, metric="euclidean", *, shrink_threshold=None):
         self.metric = metric
         self.shrink_threshold = shrink_threshold
 
@@ -139,27 +121,14 @@
             Target values.
 
         Returns
         -------
         self : object
             Fitted estimator.
         """
-        if isinstance(self.metric, str) and self.metric not in (
-            "manhattan",
-            "euclidean",
-        ):
-            warnings.warn(
-                (
-                    "Support for distance metrics other than euclidean and "
-                    "manhattan and for callables was deprecated in version "
-                    "1.3 and will be removed in version 1.5."
-                ),
-                FutureWarning,
-            )
-
         # If X is sparse and the metric is "manhattan", store it in a csc
         # format is easier to calculate the median.
         if self.metric == "manhattan":
             X, y = self._validate_data(X, y, accept_sparse=["csc"])
         else:
             X, y = self._validate_data(X, y, accept_sparse=["csr", "csc"])
         is_X_sparse = sp.issparse(X)
@@ -191,22 +160,15 @@
 
             if self.metric == "manhattan":
                 # NumPy does not calculate median of sparse matrices.
                 if not is_X_sparse:
                     self.centroids_[cur_class] = np.median(X[center_mask], axis=0)
                 else:
                     self.centroids_[cur_class] = csc_median_axis_0(X[center_mask])
-            else:
-                # TODO(1.5) remove warning when metric is only manhattan or euclidean
-                if self.metric != "euclidean":
-                    warnings.warn(
-                        "Averaging for metrics other than "
-                        "euclidean and manhattan not supported. "
-                        "The average is set to be the mean."
-                    )
+            else:  # metric == "euclidean"
                 self.centroids_[cur_class] = X[center_mask].mean(axis=0)
 
         if self.shrink_threshold:
             if np.all(np.ptp(X, axis=0) == 0):
                 raise ValueError("All features have zero variance. Division by zero.")
             dataset_centroid_ = np.mean(X, axis=0)
 
@@ -227,35 +189,28 @@
             np.clip(deviation, 0, None, out=deviation)
             deviation *= signs
             # Now adjust the centroids using the deviation
             msd = ms * deviation
             self.centroids_ = dataset_centroid_[np.newaxis, :] + msd
         return self
 
-    # TODO(1.5) remove note about precomputed metric
     def predict(self, X):
         """Perform classification on an array of test vectors `X`.
 
         The predicted class `C` for each sample in `X` is returned.
 
         Parameters
         ----------
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Test samples.
 
         Returns
         -------
         C : ndarray of shape (n_samples,)
             The predicted classes.
-
-        Notes
-        -----
-        If the metric constructor parameter is `"precomputed"`, `X` is assumed
-        to be the distance matrix between the data to be predicted and
-        `self.centroids_`.
         """
         check_is_fitted(self)
 
         X = self._validate_data(X, accept_sparse="csr", reset=False)
         return self.classes_[
             pairwise_distances_argmin(X, self.centroids_, metric=self.metric)
         ]
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_partition_nodes.pyx` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_partition_nodes.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_quad_tree.pxd` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_quad_tree.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_quad_tree.pyx` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_quad_tree.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -1,27 +1,25 @@
 # Author: Thomas Moreau <thomas.moreau.2010@gmail.com>
 # Author: Olivier Grisel <olivier.grisel@ensta.fr>
 
 
 from cpython cimport Py_INCREF, PyObject, PyTypeObject
 
+from libc.math cimport fabsf
 from libc.stdlib cimport free
 from libc.string cimport memcpy
 from libc.stdio cimport printf
 from libc.stdint cimport SIZE_MAX
 
 from ..tree._utils cimport safe_realloc
 
 import numpy as np
 cimport numpy as cnp
 cnp.import_array()
 
-cdef extern from "math.h":
-    float fabsf(float x) nogil
-
 cdef extern from "numpy/arrayobject.h":
     object PyArray_NewFromDescr(PyTypeObject* subtype, cnp.dtype descr,
                                 int nd, cnp.npy_intp* dims,
                                 cnp.npy_intp* strides,
                                 void* data, int flags, object obj)
     int PyArray_SetBaseObject(cnp.ndarray arr, PyObject* obj)
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_regression.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_regression.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/_unsupervised.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/_unsupervised.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Unsupervised nearest neighbors learner"""
+
 from ..base import _fit_context
 from ._base import KNeighborsMixin, NeighborsBase, RadiusNeighborsMixin
 
 
 class NearestNeighbors(KNeighborsMixin, RadiusNeighborsMixin, NeighborsBase):
     """Unsupervised learner for implementing neighbor searches.
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/meson.build` & `scikit_learn-1.5.0rc1/sklearn/neighbors/meson.build`

 * *Files 17% similar despite different names*

```diff
@@ -20,15 +20,15 @@
     name + '_pyx',
     output: name + '.pyx',
     input: name + '.pyx.tp',
     command: [py, tempita, '@INPUT@', '-o', '@OUTDIR@']
   )
   py.extension_module(
     name,
-    [pyx, neighbors_cython_tree],
+    [pyx, neighbors_cython_tree, utils_cython_tree],
     dependencies: [np_dep],
     cython_args: cython_args,
     subdir: 'sklearn/neighbors',
     install: true
 )
 endforeach
 
@@ -38,15 +38,15 @@
        'override_options': ['cython_language=cpp'], 'dependencies': [np_dep]},
   '_quad_tree': {'sources': ['_quad_tree.pyx'], 'dependencies': [np_dep]},
 }
 
 foreach ext_name, ext_dict : neighbors_extension_metadata
   py.extension_module(
     ext_name,
-    ext_dict.get('sources'),
+    [ext_dict.get('sources'), utils_cython_tree],
     dependencies: ext_dict.get('dependencies'),
     override_options : ext_dict.get('override_options', []),
     cython_args: cython_args,
     subdir: 'sklearn/neighbors',
     install: true
   )
 endforeach
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_ball_tree.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_ball_tree.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_graph.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_graph.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_kd_tree.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_kd_tree.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_kde.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_kde.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_lof.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_lof.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_nca.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_nca.py`

 * *Files 2% similar despite different names*

```diff
@@ -527,22 +527,34 @@
 
     X = iris_data
     y = iris_target
 
     nca.fit(X, y)
 
 
-def test_nca_feature_names_out():
-    """Check `get_feature_names_out` for `NeighborhoodComponentsAnalysis`."""
+@pytest.mark.parametrize("n_components", [None, 2])
+def test_nca_feature_names_out(n_components):
+    """Check `get_feature_names_out` for `NeighborhoodComponentsAnalysis`.
+
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/28293
+    """
 
     X = iris_data
     y = iris_target
 
-    est = NeighborhoodComponentsAnalysis().fit(X, y)
+    est = NeighborhoodComponentsAnalysis(n_components=n_components).fit(X, y)
     names_out = est.get_feature_names_out()
 
     class_name_lower = est.__class__.__name__.lower()
+
+    if n_components is not None:
+        expected_n_features = n_components
+    else:
+        expected_n_features = X.shape[1]
+
     expected_names_out = np.array(
-        [f"{class_name_lower}{i}" for i in range(est.components_.shape[1])],
+        [f"{class_name_lower}{i}" for i in range(expected_n_features)],
         dtype=object,
     )
+
     assert_array_equal(names_out, expected_names_out)
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_nearest_centroid.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_nearest_centroid.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Testing for the nearest centroid module.
 """
+
 import numpy as np
 import pytest
 from numpy.testing import assert_array_equal
 
 from sklearn import datasets
 from sklearn.neighbors import NearestCentroid
 from sklearn.utils.fixes import CSR_CONTAINERS
@@ -51,29 +52,25 @@
 
     # Fit and predict with non-CSR sparse matrices
     clf = NearestCentroid()
     clf.fit(X_csr.tocoo(), y)
     assert_array_equal(clf.predict(T_csr.tolil()), true_result)
 
 
-# TODO(1.5): Remove filterwarnings when support for some metrics is removed
-@pytest.mark.filterwarnings("ignore:Support for distance metrics:FutureWarning:sklearn")
 def test_iris():
     # Check consistency on dataset iris.
-    for metric in ("euclidean", "cosine"):
+    for metric in ("euclidean", "manhattan"):
         clf = NearestCentroid(metric=metric).fit(iris.data, iris.target)
         score = np.mean(clf.predict(iris.data) == iris.target)
         assert score > 0.9, "Failed with score = " + str(score)
 
 
-# TODO(1.5): Remove filterwarnings when support for some metrics is removed
-@pytest.mark.filterwarnings("ignore:Support for distance metrics:FutureWarning:sklearn")
 def test_iris_shrinkage():
     # Check consistency on dataset iris, when using shrinkage.
-    for metric in ("euclidean", "cosine"):
+    for metric in ("euclidean", "manhattan"):
         for shrink_threshold in [None, 0.1, 0.5]:
             clf = NearestCentroid(metric=metric, shrink_threshold=shrink_threshold)
             clf = clf.fit(iris.data, iris.target)
             score = np.mean(clf.predict(iris.data) == iris.target)
             assert score > 0.8, "Failed with score = " + str(score)
 
 
@@ -146,28 +143,14 @@
     clf.fit(X, y)
     dense_centroid = clf.centroids_
     clf.fit(X_csr, y)
     assert_array_equal(clf.centroids_, dense_centroid)
     assert_array_equal(dense_centroid, [[-1, -1], [1, 1]])
 
 
-# TODO(1.5): remove this test
-@pytest.mark.parametrize(
-    "metric", sorted(list(NearestCentroid._valid_metrics - {"manhattan", "euclidean"}))
-)
-def test_deprecated_distance_metric_supports(metric):
-    # Check that a warning is raised for all deprecated distance metric supports
-    clf = NearestCentroid(metric=metric)
-    with pytest.warns(
-        FutureWarning,
-        match="Support for distance metrics other than euclidean and manhattan",
-    ):
-        clf.fit(X, y)
-
-
 def test_features_zero_var():
     # Test that features with 0 variance throw error
 
     X = np.empty((10, 2))
     X[:, 0] = -0.13725701
     X[:, 1] = -0.9853293
     y = np.zeros((10))
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_neighbors.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_neighbors.py`

 * *Files 1% similar despite different names*

```diff
@@ -1640,16 +1640,24 @@
             neighbors.VALID_METRICS["brute"]
         )
         - set(["pyfunc", *BOOL_METRICS])
     )
     + DISTANCE_METRIC_OBJS,
 )
 def test_neighbors_metrics(
-    global_dtype, metric, n_samples=20, n_features=3, n_query_pts=2, n_neighbors=5
+    global_dtype,
+    global_random_seed,
+    metric,
+    n_samples=20,
+    n_features=3,
+    n_query_pts=2,
+    n_neighbors=5,
 ):
+    rng = np.random.RandomState(global_random_seed)
+
     metric = _parse_metric(metric, global_dtype)
 
     # Test computing the neighbors for various metrics
     algorithms = ["brute", "ball_tree", "kd_tree"]
     X_train = rng.rand(n_samples, n_features).astype(global_dtype, copy=False)
     X_test = rng.rand(n_query_pts, n_features).astype(global_dtype, copy=False)
 
@@ -1693,23 +1701,27 @@
 
             neigh.fit(X_train)
             results[algorithm] = neigh.kneighbors(X_test, return_distance=True)
 
         brute_dst, brute_idx = results["brute"]
         ball_tree_dst, ball_tree_idx = results["ball_tree"]
 
-        assert_allclose(brute_dst, ball_tree_dst)
+        # The returned distances are always in float64 regardless of the input dtype
+        # We need to adjust the tolerance w.r.t the input dtype
+        rtol = 1e-7 if global_dtype == np.float64 else 1e-4
+
+        assert_allclose(brute_dst, ball_tree_dst, rtol=rtol)
         assert_array_equal(brute_idx, ball_tree_idx)
 
         if not exclude_kd_tree:
             kd_tree_dst, kd_tree_idx = results["kd_tree"]
-            assert_allclose(brute_dst, kd_tree_dst)
+            assert_allclose(brute_dst, kd_tree_dst, rtol=rtol)
             assert_array_equal(brute_idx, kd_tree_idx)
 
-            assert_allclose(ball_tree_dst, kd_tree_dst)
+            assert_allclose(ball_tree_dst, kd_tree_dst, rtol=rtol)
             assert_array_equal(ball_tree_idx, kd_tree_idx)
 
 
 @pytest.mark.parametrize(
     "metric", sorted(set(neighbors.VALID_METRICS["brute"]) - set(["precomputed"]))
 )
 def test_kneighbors_brute_backend(
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_neighbors_pipeline.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_neighbors_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -117,15 +117,15 @@
     Xt_chain = est_chain.transform(X2)
     Xt_compact = est_compact.transform(X2)
     assert_array_almost_equal(Xt_chain, Xt_compact)
 
 
 def test_tsne():
     # Test chaining KNeighborsTransformer and TSNE
-    n_iter = 250
+    max_iter = 250
     perplexity = 5
     n_neighbors = int(3.0 * perplexity + 1)
 
     rng = np.random.RandomState(0)
     X = rng.randn(20, 2)
 
     for metric in ["minkowski", "sqeuclidean"]:
@@ -136,22 +136,22 @@
             ),
             TSNE(
                 init="random",
                 metric="precomputed",
                 perplexity=perplexity,
                 method="barnes_hut",
                 random_state=42,
-                n_iter=n_iter,
+                max_iter=max_iter,
             ),
         )
         est_compact = TSNE(
             init="random",
             metric=metric,
             perplexity=perplexity,
-            n_iter=n_iter,
+            max_iter=max_iter,
             method="barnes_hut",
             random_state=42,
         )
 
         Xt_chain = est_chain.fit_transform(X)
         Xt_compact = est_compact.fit_transform(X)
         assert_array_almost_equal(Xt_chain, Xt_compact)
```

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_neighbors_tree.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_neighbors_tree.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neighbors/tests/test_quad_tree.py` & `scikit_learn-1.5.0rc1/sklearn/neighbors/tests/test_quad_tree.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/_base.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Utilities for the neural network modules
-"""
+"""Utilities for the neural network modules"""
 
 # Author: Issam H. Laradji <issam.laradji@gmail.com>
 # License: BSD 3 clause
 
 import numpy as np
 from scipy.special import expit as logistic_sigmoid
 from scipy.special import xlogy
```

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/_multilayer_perceptron.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/_multilayer_perceptron.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Multi-layer Perceptron
-"""
+"""Multi-layer Perceptron"""
 
 # Authors: Issam H. Laradji <issam.laradji@gmail.com>
 #          Andreas Mueller
 #          Jiyuan Qian
 # License: BSD 3 clause
 
 import warnings
@@ -751,16 +750,15 @@
         """
         return self._fit(X, y, incremental=False)
 
     def _check_solver(self):
         if self.solver not in _STOCHASTIC_SOLVERS:
             raise AttributeError(
                 "partial_fit is only available for stochastic"
-                " optimizers. %s is not stochastic."
-                % self.solver
+                " optimizers. %s is not stochastic." % self.solver
             )
         return True
 
 
 class MLPClassifier(ClassifierMixin, BaseMultilayerPerceptron):
     """Multi-layer Perceptron classifier.
 
@@ -796,24 +794,30 @@
         - 'lbfgs' is an optimizer in the family of quasi-Newton methods.
 
         - 'sgd' refers to stochastic gradient descent.
 
         - 'adam' refers to a stochastic gradient-based optimizer proposed
           by Kingma, Diederik, and Jimmy Ba
 
+        For a comparison between Adam optimizer and SGD, see
+        :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`.
+
         Note: The default solver 'adam' works pretty well on relatively
         large datasets (with thousands of training samples or more) in terms of
         both training time and validation score.
         For small datasets, however, 'lbfgs' can converge faster and perform
         better.
 
     alpha : float, default=0.0001
         Strength of the L2 regularization term. The L2 regularization term
         is divided by the sample size when added to the loss.
 
+        For an example usage and visualization of varying regularization, see
+        :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_alpha.py`.
+
     batch_size : int, default='auto'
         Size of minibatches for stochastic optimizers.
         If the solver is 'lbfgs', the classifier will not use minibatch.
         When set to "auto", `batch_size=min(200, n_samples)`.
 
     learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'
         Learning rate schedule for weight updates.
@@ -1289,14 +1293,17 @@
         - 'lbfgs' is an optimizer in the family of quasi-Newton methods.
 
         - 'sgd' refers to stochastic gradient descent.
 
         - 'adam' refers to a stochastic gradient-based optimizer proposed by
           Kingma, Diederik, and Jimmy Ba
 
+        For a comparison between Adam optimizer and SGD, see
+        :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`.
+
         Note: The default solver 'adam' works pretty well on relatively
         large datasets (with thousands of training samples or more) in terms of
         both training time and validation score.
         For small datasets, however, 'lbfgs' can converge faster and perform
         better.
 
     alpha : float, default=0.0001
```

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/_rbm.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/_rbm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Restricted Boltzmann Machine
-"""
+"""Restricted Boltzmann Machine"""
 
 # Authors: Yann N. Dauphin <dauphiya@iro.umontreal.ca>
 #          Vlad Niculae
 #          Gabriel Synnaeve
 #          Lars Buitinck
 # License: BSD 3 clause
 
@@ -123,14 +122,17 @@
 
     >>> import numpy as np
     >>> from sklearn.neural_network import BernoulliRBM
     >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
     >>> model = BernoulliRBM(n_components=2)
     >>> model.fit(X)
     BernoulliRBM(n_components=2)
+
+    For a more detailed example usage, see
+    :ref:`sphx_glr_auto_examples_neural_networks_plot_rbm_logistic_classification.py`.
     """
 
     _parameter_constraints: dict = {
         "n_components": [Interval(Integral, 1, None, closed="left")],
         "learning_rate": [Interval(Real, 0, None, closed="neither")],
         "batch_size": [Interval(Integral, 1, None, closed="left")],
         "n_iter": [Interval(Integral, 0, None, closed="left")],
```

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/_stochastic_optimizers.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/_stochastic_optimizers.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Stochastic optimization methods for MLP
-"""
+"""Stochastic optimization methods for MLP"""
 
 # Authors: Jiyuan Qian <jq401@nyu.edu>
 # License: BSD 3 clause
 
 import numpy as np
```

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_base.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/tests/test_mlp.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_mlp.py`

 * *Files 0% similar despite different names*

```diff
@@ -728,16 +728,15 @@
     for y_i in (y_2classes, y_3classes_alt, y_4classes, y_5classes):
         clf = MLPClassifier(hidden_layer_sizes=2, solver="lbfgs", warm_start=True).fit(
             X, y
         )
         message = (
             "warm_start can only be used where `y` has the same "
             "classes as in the previous call to fit."
-            " Previously got [0 1 2], `y` has %s"
-            % np.unique(y_i)
+            " Previously got [0 1 2], `y` has %s" % np.unique(y_i)
         )
         with pytest.raises(ValueError, match=re.escape(message)):
             clf.fit(X, y_i)
 
 
 @pytest.mark.parametrize("MLPEstimator", [MLPClassifier, MLPRegressor])
 def test_warm_start_full_iteration(MLPEstimator):
```

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/tests/test_rbm.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_rbm.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/neural_network/tests/test_stochastic_optimizers.py` & `scikit_learn-1.5.0rc1/sklearn/neural_network/tests/test_stochastic_optimizers.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/pipeline.py` & `scikit_learn-1.5.0rc1/sklearn/pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,43 +1,44 @@
 """
 The :mod:`sklearn.pipeline` module implements utilities to build a composite
 estimator, as a chain of transforms and estimators.
 """
+
 # Author: Edouard Duchesnay
 #         Gael Varoquaux
 #         Virgile Fritsch
 #         Alexandre Gramfort
 #         Lars Buitinck
 # License: BSD
 
-from collections import defaultdict
-from itertools import islice
+from collections import Counter, defaultdict
+from itertools import chain, islice
 
 import numpy as np
 from scipy import sparse
 
 from .base import TransformerMixin, _fit_context, clone
 from .exceptions import NotFittedError
 from .preprocessing import FunctionTransformer
-from .utils import Bunch, _print_elapsed_time
+from .utils import Bunch, _safe_indexing
 from .utils._estimator_html_repr import _VisualBlock
 from .utils._metadata_requests import METHODS
 from .utils._param_validation import HasMethods, Hidden
 from .utils._set_output import (
     _get_container_adapter,
     _safe_set_output,
 )
 from .utils._tags import _safe_tags
+from .utils._user_interface import _print_elapsed_time
+from .utils.deprecation import _deprecate_Xt_in_inverse_transform
 from .utils.metadata_routing import (
     MetadataRouter,
     MethodMapping,
     _raise_for_params,
-    _raise_for_unsupported_routing,
     _routing_enabled,
-    _RoutingNotSupportedMixin,
     process_routing,
 )
 from .utils.metaestimators import _BaseComposition, available_if
 from .utils.parallel import Parallel, delayed
 from .utils.validation import check_is_fitted, check_memory
 
 __all__ = ["Pipeline", "FeatureUnion", "make_pipeline", "make_union"]
@@ -174,15 +175,15 @@
     def set_output(self, *, transform=None):
         """Set the output container when `"transform"` and `"fit_transform"` are called.
 
         Calling `set_output` will set the output of all estimators in `steps`.
 
         Parameters
         ----------
-        transform : {"default", "pandas"}, default=None
+        transform : {"default", "pandas", "polars"}, default=None
             Configure output of `transform` and `fit_transform`.
 
             - `"default"`: Default output format of a transformer
             - `"pandas"`: DataFrame output
             - `"polars"`: Polars output
             - `None`: Transform configuration is unchanged
 
@@ -905,27 +906,36 @@
             Xt = transform.transform(Xt, **routed_params[name].transform)
         return Xt
 
     def _can_inverse_transform(self):
         return all(hasattr(t, "inverse_transform") for _, _, t in self._iter())
 
     @available_if(_can_inverse_transform)
-    def inverse_transform(self, Xt, **params):
+    def inverse_transform(self, X=None, *, Xt=None, **params):
         """Apply `inverse_transform` for each step in a reverse order.
 
         All estimators in the pipeline must support `inverse_transform`.
 
         Parameters
         ----------
+        X : array-like of shape (n_samples, n_transformed_features)
+            Data samples, where ``n_samples`` is the number of samples and
+            ``n_features`` is the number of features. Must fulfill
+            input requirements of last step of pipeline's
+            ``inverse_transform`` method.
+
         Xt : array-like of shape (n_samples, n_transformed_features)
             Data samples, where ``n_samples`` is the number of samples and
             ``n_features`` is the number of features. Must fulfill
             input requirements of last step of pipeline's
             ``inverse_transform`` method.
 
+            .. deprecated:: 1.5
+                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.
+
         **params : dict of str -> object
             Parameters requested and accepted by steps. Each step must have
             requested certain metadata for these parameters to be forwarded to
             them.
 
             .. versionadded:: 1.4
                 Only available if `enable_metadata_routing=True`. See
@@ -936,23 +946,23 @@
         -------
         Xt : ndarray of shape (n_samples, n_features)
             Inverse transformed data, that is, data in the original feature
             space.
         """
         _raise_for_params(params, self, "inverse_transform")
 
+        X = _deprecate_Xt_in_inverse_transform(X, Xt)
+
         # we don't have to branch here, since params is only non-empty if
         # enable_metadata_routing=True.
         routed_params = process_routing(self, "inverse_transform", **params)
         reverse_iter = reversed(list(self._iter()))
         for _, name, transform in reverse_iter:
-            Xt = transform.inverse_transform(
-                Xt, **routed_params[name].inverse_transform
-            )
-        return Xt
+            X = transform.inverse_transform(X, **routed_params[name].inverse_transform)
+        return X
 
     @available_if(_final_estimator_has("score"))
     def score(self, X, y=None, sample_weight=None, **params):
         """Transform the data, and apply `score` with the final estimator.
 
         Call `transform` of each transformer in the pipeline. The transformed
         data are finally passed to the final estimator that calls
@@ -1254,15 +1264,15 @@
     >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
     Pipeline(steps=[('standardscaler', StandardScaler()),
                     ('gaussiannb', GaussianNB())])
     """
     return Pipeline(_name_estimators(steps), memory=memory, verbose=verbose)
 
 
-def _transform_one(transformer, X, y, weight, params):
+def _transform_one(transformer, X, y, weight, columns=None, params=None):
     """Call transform and apply weight to output.
 
     Parameters
     ----------
     transformer : estimator
         Estimator to be used for transformation.
 
@@ -1271,36 +1281,52 @@
 
     y : ndarray of shape (n_samples,)
         Ignored.
 
     weight : float
         Weight to be applied to the output of the transformation.
 
+    columns : str, array-like of str, int, array-like of int, array-like of bool, slice
+        Columns to select before transforming.
+
     params : dict
         Parameters to be passed to the transformer's ``transform`` method.
 
         This should be of the form ``process_routing()["step_name"]``.
     """
+    if columns is not None:
+        X = _safe_indexing(X, columns, axis=1)
+
     res = transformer.transform(X, **params.transform)
     # if we have a weight for this transformer, multiply output
     if weight is None:
         return res
     return res * weight
 
 
 def _fit_transform_one(
-    transformer, X, y, weight, message_clsname="", message=None, params=None
+    transformer,
+    X,
+    y,
+    weight,
+    columns=None,
+    message_clsname="",
+    message=None,
+    params=None,
 ):
     """
     Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned
     with the fitted transformer. If ``weight`` is not ``None``, the result will
     be multiplied by ``weight``.
 
     ``params`` needs to be of the form ``process_routing()["step_name"]``.
     """
+    if columns is not None:
+        X = _safe_indexing(X, columns, axis=1)
+
     params = params or {}
     with _print_elapsed_time(message_clsname, message):
         if hasattr(transformer, "fit_transform"):
             res = transformer.fit_transform(X, y, **params.get("fit_transform", {}))
         else:
             res = transformer.fit(X, y, **params.get("fit", {})).transform(
                 X, **params.get("transform", {})
@@ -1315,15 +1341,15 @@
     """
     Fits ``transformer`` to ``X`` and ``y``.
     """
     with _print_elapsed_time(message_clsname, message):
         return transformer.fit(X, y, **params["fit"])
 
 
-class FeatureUnion(_RoutingNotSupportedMixin, TransformerMixin, _BaseComposition):
+class FeatureUnion(TransformerMixin, _BaseComposition):
     """Concatenates results of multiple transformer objects.
 
     This estimator applies a list of transformer objects in parallel to the
     input data, then concatenates the results. This is useful to combine
     several feature extraction mechanisms into a single transformer.
 
     Parameters of the transformers may be set using its name and the parameter
@@ -1364,14 +1390,22 @@
         Keys are transformer names, values the weights.
         Raises ValueError if key not present in ``transformer_list``.
 
     verbose : bool, default=False
         If True, the time elapsed while fitting each transformer will be
         printed as it is completed.
 
+    verbose_feature_names_out : bool, default=True
+        If True, :meth:`get_feature_names_out` will prefix all feature names
+        with the name of the transformer that generated that feature.
+        If False, :meth:`get_feature_names_out` will not prefix any feature
+        names and will error if feature names are not unique.
+
+        .. versionadded:: 1.5
+
     Attributes
     ----------
     named_transformers : :class:`~sklearn.utils.Bunch`
         Dictionary-like object, with the following attributes.
         Read-only attribute to access any transformer parameter by user
         given name. Keys are transformer names and values are
         transformer parameters.
@@ -1400,47 +1434,55 @@
     --------
     >>> from sklearn.pipeline import FeatureUnion
     >>> from sklearn.decomposition import PCA, TruncatedSVD
     >>> union = FeatureUnion([("pca", PCA(n_components=1)),
     ...                       ("svd", TruncatedSVD(n_components=2))])
     >>> X = [[0., 1., 3], [2., 2., 5]]
     >>> union.fit_transform(X)
-    array([[ 1.5       ,  3.0...,  0.8...],
-           [-1.5       ,  5.7..., -0.4...]])
+    array([[-1.5       ,  3.0..., -0.8...],
+           [ 1.5       ,  5.7...,  0.4...]])
     >>> # An estimator's parameter can be set using '__' syntax
     >>> union.set_params(svd__n_components=1).fit_transform(X)
-    array([[ 1.5       ,  3.0...],
-           [-1.5       ,  5.7...]])
+    array([[-1.5       ,  3.0...],
+           [ 1.5       ,  5.7...]])
 
     For a more detailed example of usage, see
     :ref:`sphx_glr_auto_examples_compose_plot_feature_union.py`.
     """
 
     _required_parameters = ["transformer_list"]
 
     def __init__(
-        self, transformer_list, *, n_jobs=None, transformer_weights=None, verbose=False
+        self,
+        transformer_list,
+        *,
+        n_jobs=None,
+        transformer_weights=None,
+        verbose=False,
+        verbose_feature_names_out=True,
     ):
         self.transformer_list = transformer_list
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
         self.verbose = verbose
+        self.verbose_feature_names_out = verbose_feature_names_out
 
     def set_output(self, *, transform=None):
         """Set the output container when `"transform"` and `"fit_transform"` are called.
 
         `set_output` will set the output of all estimators in `transformer_list`.
 
         Parameters
         ----------
-        transform : {"default", "pandas"}, default=None
+        transform : {"default", "pandas", "polars"}, default=None
             Configure output of `transform` and `fit_transform`.
 
             - `"default"`: Default output format of a transformer
             - `"pandas"`: DataFrame output
+            - `"polars"`: Polars output
             - `None`: Transform configuration is unchanged
 
         Returns
         -------
         self : estimator instance
             Estimator instance.
         """
@@ -1550,132 +1592,238 @@
             Input features.
 
         Returns
         -------
         feature_names_out : ndarray of str objects
             Transformed feature names.
         """
-        feature_names = []
+        # List of tuples (name, feature_names_out)
+        transformer_with_feature_names_out = []
         for name, trans, _ in self._iter():
             if not hasattr(trans, "get_feature_names_out"):
                 raise AttributeError(
                     "Transformer %s (type %s) does not provide get_feature_names_out."
                     % (str(name), type(trans).__name__)
                 )
-            feature_names.extend(
-                [f"{name}__{f}" for f in trans.get_feature_names_out(input_features)]
+            feature_names_out = trans.get_feature_names_out(input_features)
+            transformer_with_feature_names_out.append((name, feature_names_out))
+
+        return self._add_prefix_for_feature_names_out(
+            transformer_with_feature_names_out
+        )
+
+    def _add_prefix_for_feature_names_out(self, transformer_with_feature_names_out):
+        """Add prefix for feature names out that includes the transformer names.
+
+        Parameters
+        ----------
+        transformer_with_feature_names_out : list of tuples of (str, array-like of str)
+            The tuple consistent of the transformer's name and its feature names out.
+
+        Returns
+        -------
+        feature_names_out : ndarray of shape (n_features,), dtype=str
+            Transformed feature names.
+        """
+        if self.verbose_feature_names_out:
+            # Prefix the feature names out with the transformers name
+            names = list(
+                chain.from_iterable(
+                    (f"{name}__{i}" for i in feature_names_out)
+                    for name, feature_names_out in transformer_with_feature_names_out
+                )
             )
-        return np.asarray(feature_names, dtype=object)
+            return np.asarray(names, dtype=object)
+
+        # verbose_feature_names_out is False
+        # Check that names are all unique without a prefix
+        feature_names_count = Counter(
+            chain.from_iterable(s for _, s in transformer_with_feature_names_out)
+        )
+        top_6_overlap = [
+            name for name, count in feature_names_count.most_common(6) if count > 1
+        ]
+        top_6_overlap.sort()
+        if top_6_overlap:
+            if len(top_6_overlap) == 6:
+                # There are more than 5 overlapping names, we only show the 5
+                # of the feature names
+                names_repr = str(top_6_overlap[:5])[:-1] + ", ...]"
+            else:
+                names_repr = str(top_6_overlap)
+            raise ValueError(
+                f"Output feature names: {names_repr} are not unique. Please set "
+                "verbose_feature_names_out=True to add prefixes to feature names"
+            )
+
+        return np.concatenate(
+            [name for _, name in transformer_with_feature_names_out],
+        )
 
     def fit(self, X, y=None, **fit_params):
         """Fit all transformers using X.
 
         Parameters
         ----------
         X : iterable or array-like, depending on transformers
             Input data, used to fit transformers.
 
         y : array-like of shape (n_samples, n_outputs), default=None
             Targets for supervised learning.
 
         **fit_params : dict, default=None
-            Parameters to pass to the fit method of the estimator.
+            - If `enable_metadata_routing=False` (default):
+              Parameters directly passed to the `fit` methods of the
+              sub-transformers.
+
+            - If `enable_metadata_routing=True`:
+              Parameters safely routed to the `fit` methods of the
+              sub-transformers. See :ref:`Metadata Routing User Guide
+              <metadata_routing>` for more details.
+
+            .. versionchanged:: 1.5
+                `**fit_params` can be routed via metadata routing API.
 
         Returns
         -------
         self : object
             FeatureUnion class instance.
         """
-        _raise_for_unsupported_routing(self, "fit", **fit_params)
-        transformers = self._parallel_func(X, y, fit_params, _fit_one)
+        if _routing_enabled():
+            routed_params = process_routing(self, "fit", **fit_params)
+        else:
+            # TODO(SLEP6): remove when metadata routing cannot be disabled.
+            routed_params = Bunch()
+            for name, _ in self.transformer_list:
+                routed_params[name] = Bunch(fit={})
+                routed_params[name].fit = fit_params
+
+        transformers = self._parallel_func(X, y, _fit_one, routed_params)
+
         if not transformers:
             # All transformers are None
             return self
 
         self._update_transformer_list(transformers)
         return self
 
-    def fit_transform(self, X, y=None, **fit_params):
+    def fit_transform(self, X, y=None, **params):
         """Fit all transformers, transform the data and concatenate results.
 
         Parameters
         ----------
         X : iterable or array-like, depending on transformers
             Input data to be transformed.
 
         y : array-like of shape (n_samples, n_outputs), default=None
             Targets for supervised learning.
 
-        **fit_params : dict, default=None
-            Parameters to pass to the fit method of the estimator.
+        **params : dict, default=None
+            - If `enable_metadata_routing=False` (default):
+              Parameters directly passed to the `fit` methods of the
+              sub-transformers.
+
+            - If `enable_metadata_routing=True`:
+              Parameters safely routed to the `fit` methods of the
+              sub-transformers. See :ref:`Metadata Routing User Guide
+              <metadata_routing>` for more details.
+
+            .. versionchanged:: 1.5
+                `**params` can now be routed via metadata routing API.
 
         Returns
         -------
         X_t : array-like or sparse matrix of \
                 shape (n_samples, sum_n_components)
             The `hstack` of results of transformers. `sum_n_components` is the
             sum of `n_components` (output dimension) over transformers.
         """
-        results = self._parallel_func(X, y, fit_params, _fit_transform_one)
+        if _routing_enabled():
+            routed_params = process_routing(self, "fit_transform", **params)
+        else:
+            # TODO(SLEP6): remove when metadata routing cannot be disabled.
+            routed_params = Bunch()
+            for name, obj in self.transformer_list:
+                if hasattr(obj, "fit_transform"):
+                    routed_params[name] = Bunch(fit_transform={})
+                    routed_params[name].fit_transform = params
+                else:
+                    routed_params[name] = Bunch(fit={})
+                    routed_params[name] = Bunch(transform={})
+                    routed_params[name].fit = params
+
+        results = self._parallel_func(X, y, _fit_transform_one, routed_params)
         if not results:
             # All transformers are None
             return np.zeros((X.shape[0], 0))
 
         Xs, transformers = zip(*results)
         self._update_transformer_list(transformers)
 
         return self._hstack(Xs)
 
     def _log_message(self, name, idx, total):
         if not self.verbose:
             return None
         return "(step %d of %d) Processing %s" % (idx, total, name)
 
-    def _parallel_func(self, X, y, fit_params, func):
+    def _parallel_func(self, X, y, func, routed_params):
         """Runs func in parallel on X and y"""
         self.transformer_list = list(self.transformer_list)
         self._validate_transformers()
         self._validate_transformer_weights()
         transformers = list(self._iter())
 
-        params = Bunch(fit=fit_params, fit_transform=fit_params)
-
         return Parallel(n_jobs=self.n_jobs)(
             delayed(func)(
                 transformer,
                 X,
                 y,
                 weight,
                 message_clsname="FeatureUnion",
                 message=self._log_message(name, idx, len(transformers)),
-                params=params,
+                params=routed_params[name],
             )
             for idx, (name, transformer, weight) in enumerate(transformers, 1)
         )
 
-    def transform(self, X):
+    def transform(self, X, **params):
         """Transform X separately by each transformer, concatenate results.
 
         Parameters
         ----------
         X : iterable or array-like, depending on transformers
             Input data to be transformed.
 
+        **params : dict, default=None
+
+            Parameters routed to the `transform` method of the sub-transformers via the
+            metadata routing API. See :ref:`Metadata Routing User Guide
+            <metadata_routing>` for more details.
+
+            .. versionadded:: 1.5
+
         Returns
         -------
-        X_t : array-like or sparse matrix of \
-                shape (n_samples, sum_n_components)
+        X_t : array-like or sparse matrix of shape (n_samples, sum_n_components)
             The `hstack` of results of transformers. `sum_n_components` is the
             sum of `n_components` (output dimension) over transformers.
         """
-        # TODO(SLEP6): accept **params here in `transform` and route it to the
-        # underlying estimators.
-        params = Bunch(transform={})
+        _raise_for_params(params, self, "transform")
+
+        if _routing_enabled():
+            routed_params = process_routing(self, "transform", **params)
+        else:
+            # TODO(SLEP6): remove when metadata routing cannot be disabled.
+            routed_params = Bunch()
+            for name, _ in self.transformer_list:
+                routed_params[name] = Bunch(transform={})
+
         Xs = Parallel(n_jobs=self.n_jobs)(
-            delayed(_transform_one)(trans, X, None, weight, params)
+            delayed(_transform_one)(trans, X, None, weight, params=routed_params[name])
             for name, trans, weight in self._iter()
         )
         if not Xs:
             # All transformers are None
             return np.zeros((X.shape[0], 0))
 
         return self._hstack(Xs)
@@ -1723,14 +1871,43 @@
 
     def __getitem__(self, name):
         """Return transformer with name."""
         if not isinstance(name, str):
             raise KeyError("Only string keys are supported")
         return self.named_transformers[name]
 
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
+
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
+
+        .. versionadded:: 1.5
+
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__)
+
+        for name, transformer in self.transformer_list:
+            router.add(
+                **{name: transformer},
+                method_mapping=MethodMapping()
+                .add(caller="fit", callee="fit")
+                .add(caller="fit_transform", callee="fit_transform")
+                .add(caller="fit_transform", callee="fit")
+                .add(caller="fit_transform", callee="transform")
+                .add(caller="transform", callee="transform"),
+            )
+
+        return router
+
 
 def make_union(*transformers, n_jobs=None, verbose=False):
     """Construct a :class:`FeatureUnion` from the given transformers.
 
     This is a shorthand for the :class:`FeatureUnion` constructor; it does not
     require, and does not permit, naming the transformers. Instead, they will
     be given names automatically based on their types. It also does not allow
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_csr_polynomial_expansion.pyx` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_csr_polynomial_expansion.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_data.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_data.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,7796 +39,7815 @@
 00000260: 7572 6573 4f75 744d 6978 696e 2c0a 2020  uresOutMixin,.  
 00000270: 2020 4f6e 6554 6f4f 6e65 4665 6174 7572    OneToOneFeatur
 00000280: 654d 6978 696e 2c0a 2020 2020 5472 616e  eMixin,.    Tran
 00000290: 7366 6f72 6d65 724d 6978 696e 2c0a 2020  sformerMixin,.  
 000002a0: 2020 5f66 6974 5f63 6f6e 7465 7874 2c0a    _fit_context,.
 000002b0: 290a 6672 6f6d 202e 2e75 7469 6c73 2069  ).from ..utils i
 000002c0: 6d70 6f72 7420 5f61 7272 6179 5f61 7069  mport _array_api
-000002d0: 2c20 6368 6563 6b5f 6172 7261 790a 6672  , check_array.fr
-000002e0: 6f6d 202e 2e75 7469 6c73 2e5f 6172 7261  om ..utils._arra
-000002f0: 795f 6170 6920 696d 706f 7274 2067 6574  y_api import get
-00000300: 5f6e 616d 6573 7061 6365 0a66 726f 6d20  _namespace.from 
-00000310: 2e2e 7574 696c 732e 5f70 6172 616d 5f76  ..utils._param_v
-00000320: 616c 6964 6174 696f 6e20 696d 706f 7274  alidation import
-00000330: 2049 6e74 6572 7661 6c2c 204f 7074 696f   Interval, Optio
-00000340: 6e73 2c20 5374 724f 7074 696f 6e73 2c20  ns, StrOptions, 
-00000350: 7661 6c69 6461 7465 5f70 6172 616d 730a  validate_params.
-00000360: 6672 6f6d 202e 2e75 7469 6c73 2e65 7874  from ..utils.ext
-00000370: 6d61 7468 2069 6d70 6f72 7420 5f69 6e63  math import _inc
-00000380: 7265 6d65 6e74 616c 5f6d 6561 6e5f 616e  remental_mean_an
-00000390: 645f 7661 722c 2072 6f77 5f6e 6f72 6d73  d_var, row_norms
-000003a0: 0a66 726f 6d20 2e2e 7574 696c 732e 7370  .from ..utils.sp
-000003b0: 6172 7365 6675 6e63 7320 696d 706f 7274  arsefuncs import
-000003c0: 2028 0a20 2020 2069 6e63 725f 6d65 616e   (.    incr_mean
-000003d0: 5f76 6172 6961 6e63 655f 6178 6973 2c0a  _variance_axis,.
-000003e0: 2020 2020 696e 706c 6163 655f 636f 6c75      inplace_colu
-000003f0: 6d6e 5f73 6361 6c65 2c0a 2020 2020 6d65  mn_scale,.    me
-00000400: 616e 5f76 6172 6961 6e63 655f 6178 6973  an_variance_axis
-00000410: 2c0a 2020 2020 6d69 6e5f 6d61 785f 6178  ,.    min_max_ax
-00000420: 6973 2c0a 290a 6672 6f6d 202e 2e75 7469  is,.).from ..uti
-00000430: 6c73 2e73 7061 7273 6566 756e 6373 5f66  ls.sparsefuncs_f
-00000440: 6173 7420 696d 706f 7274 2028 0a20 2020  ast import (.   
-00000450: 2069 6e70 6c61 6365 5f63 7372 5f72 6f77   inplace_csr_row
-00000460: 5f6e 6f72 6d61 6c69 7a65 5f6c 312c 0a20  _normalize_l1,. 
-00000470: 2020 2069 6e70 6c61 6365 5f63 7372 5f72     inplace_csr_r
-00000480: 6f77 5f6e 6f72 6d61 6c69 7a65 5f6c 322c  ow_normalize_l2,
-00000490: 0a29 0a66 726f 6d20 2e2e 7574 696c 732e  .).from ..utils.
-000004a0: 7661 6c69 6461 7469 6f6e 2069 6d70 6f72  validation impor
-000004b0: 7420 280a 2020 2020 464c 4f41 545f 4454  t (.    FLOAT_DT
-000004c0: 5950 4553 2c0a 2020 2020 5f63 6865 636b  YPES,.    _check
-000004d0: 5f73 616d 706c 655f 7765 6967 6874 2c0a  _sample_weight,.
-000004e0: 2020 2020 6368 6563 6b5f 6973 5f66 6974      check_is_fit
-000004f0: 7465 642c 0a20 2020 2063 6865 636b 5f72  ted,.    check_r
-00000500: 616e 646f 6d5f 7374 6174 652c 0a29 0a66  andom_state,.).f
-00000510: 726f 6d20 2e5f 656e 636f 6465 7273 2069  rom ._encoders i
-00000520: 6d70 6f72 7420 4f6e 6548 6f74 456e 636f  mport OneHotEnco
-00000530: 6465 720a 0a42 4f55 4e44 535f 5448 5245  der..BOUNDS_THRE
-00000540: 5348 4f4c 4420 3d20 3165 2d37 0a0a 5f5f  SHOLD = 1e-7..__
-00000550: 616c 6c5f 5f20 3d20 5b0a 2020 2020 2242  all__ = [.    "B
-00000560: 696e 6172 697a 6572 222c 0a20 2020 2022  inarizer",.    "
-00000570: 4b65 726e 656c 4365 6e74 6572 6572 222c  KernelCenterer",
-00000580: 0a20 2020 2022 4d69 6e4d 6178 5363 616c  .    "MinMaxScal
-00000590: 6572 222c 0a20 2020 2022 4d61 7841 6273  er",.    "MaxAbs
-000005a0: 5363 616c 6572 222c 0a20 2020 2022 4e6f  Scaler",.    "No
-000005b0: 726d 616c 697a 6572 222c 0a20 2020 2022  rmalizer",.    "
-000005c0: 4f6e 6548 6f74 456e 636f 6465 7222 2c0a  OneHotEncoder",.
-000005d0: 2020 2020 2252 6f62 7573 7453 6361 6c65      "RobustScale
-000005e0: 7222 2c0a 2020 2020 2253 7461 6e64 6172  r",.    "Standar
-000005f0: 6453 6361 6c65 7222 2c0a 2020 2020 2251  dScaler",.    "Q
-00000600: 7561 6e74 696c 6554 7261 6e73 666f 726d  uantileTransform
-00000610: 6572 222c 0a20 2020 2022 506f 7765 7254  er",.    "PowerT
-00000620: 7261 6e73 666f 726d 6572 222c 0a20 2020  ransformer",.   
-00000630: 2022 6164 645f 6475 6d6d 795f 6665 6174   "add_dummy_feat
-00000640: 7572 6522 2c0a 2020 2020 2262 696e 6172  ure",.    "binar
-00000650: 697a 6522 2c0a 2020 2020 226e 6f72 6d61  ize",.    "norma
-00000660: 6c69 7a65 222c 0a20 2020 2022 7363 616c  lize",.    "scal
-00000670: 6522 2c0a 2020 2020 2272 6f62 7573 745f  e",.    "robust_
-00000680: 7363 616c 6522 2c0a 2020 2020 226d 6178  scale",.    "max
-00000690: 6162 735f 7363 616c 6522 2c0a 2020 2020  abs_scale",.    
-000006a0: 226d 696e 6d61 785f 7363 616c 6522 2c0a  "minmax_scale",.
-000006b0: 2020 2020 2271 7561 6e74 696c 655f 7472      "quantile_tr
-000006c0: 616e 7366 6f72 6d22 2c0a 2020 2020 2270  ansform",.    "p
-000006d0: 6f77 6572 5f74 7261 6e73 666f 726d 222c  ower_transform",
-000006e0: 0a5d 0a0a 0a64 6566 205f 6973 5f63 6f6e  .]...def _is_con
-000006f0: 7374 616e 745f 6665 6174 7572 6528 7661  stant_feature(va
-00000700: 722c 206d 6561 6e2c 206e 5f73 616d 706c  r, mean, n_sampl
-00000710: 6573 293a 0a20 2020 2022 2222 4465 7465  es):.    """Dete
-00000720: 6374 2069 6620 6120 6665 6174 7572 6520  ct if a feature 
-00000730: 6973 2069 6e64 6973 7469 6e67 7569 7368  is indistinguish
-00000740: 6162 6c65 2066 726f 6d20 6120 636f 6e73  able from a cons
-00000750: 7461 6e74 2066 6561 7475 7265 2e0a 0a20  tant feature... 
-00000760: 2020 2054 6865 2064 6574 6563 7469 6f6e     The detection
-00000770: 2069 7320 6261 7365 6420 6f6e 2069 7473   is based on its
-00000780: 2063 6f6d 7075 7465 6420 7661 7269 616e   computed varian
-00000790: 6365 2061 6e64 206f 6e20 7468 6520 7468  ce and on the th
-000007a0: 656f 7265 7469 6361 6c0a 2020 2020 6572  eoretical.    er
-000007b0: 726f 7220 626f 756e 6473 206f 6620 7468  ror bounds of th
-000007c0: 6520 2732 2070 6173 7320 616c 676f 7269  e '2 pass algori
-000007d0: 7468 6d27 2066 6f72 2076 6172 6961 6e63  thm' for varianc
-000007e0: 6520 636f 6d70 7574 6174 696f 6e2e 0a0a  e computation...
-000007f0: 2020 2020 5365 6520 2241 6c67 6f72 6974      See "Algorit
-00000800: 686d 7320 666f 7220 636f 6d70 7574 696e  hms for computin
-00000810: 6720 7468 6520 7361 6d70 6c65 2076 6172  g the sample var
-00000820: 6961 6e63 653a 2061 6e61 6c79 7369 7320  iance: analysis 
-00000830: 616e 640a 2020 2020 7265 636f 6d6d 656e  and.    recommen
-00000840: 6461 7469 6f6e 7322 2c20 6279 2043 6861  dations", by Cha
-00000850: 6e2c 2047 6f6c 7562 2c20 616e 6420 4c65  n, Golub, and Le
-00000860: 5665 7175 652e 0a20 2020 2022 2222 0a20  Veque..    """. 
-00000870: 2020 2023 2049 6e20 7363 696b 6974 2d6c     # In scikit-l
-00000880: 6561 726e 2c20 7661 7269 616e 6365 2069  earn, variance i
-00000890: 7320 616c 7761 7973 2063 6f6d 7075 7465  s always compute
-000008a0: 6420 7573 696e 6720 666c 6f61 7436 3420  d using float64 
-000008b0: 6163 6375 6d75 6c61 746f 7273 2e0a 2020  accumulators..  
-000008c0: 2020 6570 7320 3d20 6e70 2e66 696e 666f    eps = np.finfo
-000008d0: 286e 702e 666c 6f61 7436 3429 2e65 7073  (np.float64).eps
-000008e0: 0a0a 2020 2020 7570 7065 725f 626f 756e  ..    upper_boun
-000008f0: 6420 3d20 6e5f 7361 6d70 6c65 7320 2a20  d = n_samples * 
-00000900: 6570 7320 2a20 7661 7220 2b20 286e 5f73  eps * var + (n_s
-00000910: 616d 706c 6573 202a 206d 6561 6e20 2a20  amples * mean * 
-00000920: 6570 7329 202a 2a20 320a 2020 2020 7265  eps) ** 2.    re
-00000930: 7475 726e 2076 6172 203c 3d20 7570 7065  turn var <= uppe
-00000940: 725f 626f 756e 640a 0a0a 6465 6620 5f68  r_bound...def _h
-00000950: 616e 646c 655f 7a65 726f 735f 696e 5f73  andle_zeros_in_s
-00000960: 6361 6c65 2873 6361 6c65 2c20 636f 7079  cale(scale, copy
-00000970: 3d54 7275 652c 2063 6f6e 7374 616e 745f  =True, constant_
-00000980: 6d61 736b 3d4e 6f6e 6529 3a0a 2020 2020  mask=None):.    
-00000990: 2222 2253 6574 2073 6361 6c65 7320 6f66  """Set scales of
-000009a0: 206e 6561 7220 636f 6e73 7461 6e74 2066   near constant f
-000009b0: 6561 7475 7265 7320 746f 2031 2e0a 0a20  eatures to 1... 
-000009c0: 2020 2054 6865 2067 6f61 6c20 6973 2074     The goal is t
-000009d0: 6f20 6176 6f69 6420 6469 7669 7369 6f6e  o avoid division
-000009e0: 2062 7920 7665 7279 2073 6d61 6c6c 206f   by very small o
-000009f0: 7220 7a65 726f 2076 616c 7565 732e 0a0a  r zero values...
-00000a00: 2020 2020 4e65 6172 2063 6f6e 7374 616e      Near constan
-00000a10: 7420 6665 6174 7572 6573 2061 7265 2064  t features are d
-00000a20: 6574 6563 7465 6420 6175 746f 6d61 7469  etected automati
-00000a30: 6361 6c6c 7920 6279 2069 6465 6e74 6966  cally by identif
-00000a40: 7969 6e67 0a20 2020 2073 6361 6c65 7320  ying.    scales 
-00000a50: 636c 6f73 6520 746f 206d 6163 6869 6e65  close to machine
-00000a60: 2070 7265 6369 7369 6f6e 2075 6e6c 6573   precision unles
-00000a70: 7320 7468 6579 2061 7265 2070 7265 636f  s they are preco
-00000a80: 6d70 7574 6564 2062 790a 2020 2020 7468  mputed by.    th
-00000a90: 6520 6361 6c6c 6572 2061 6e64 2070 6173  e caller and pas
-00000aa0: 7365 6420 7769 7468 2074 6865 2060 636f  sed with the `co
-00000ab0: 6e73 7461 6e74 5f6d 6173 6b60 206b 7761  nstant_mask` kwa
-00000ac0: 7267 2e0a 0a20 2020 2054 7970 6963 616c  rg...    Typical
-00000ad0: 6c79 2066 6f72 2073 7461 6e64 6172 6420  ly for standard 
-00000ae0: 7363 616c 696e 672c 2074 6865 2073 6361  scaling, the sca
-00000af0: 6c65 7320 6172 6520 7468 6520 7374 616e  les are the stan
-00000b00: 6461 7264 0a20 2020 2064 6576 6961 7469  dard.    deviati
-00000b10: 6f6e 2077 6869 6c65 206e 6561 7220 636f  on while near co
-00000b20: 6e73 7461 6e74 2066 6561 7475 7265 7320  nstant features 
-00000b30: 6172 6520 6265 7474 6572 2064 6574 6563  are better detec
-00000b40: 7465 6420 6f6e 2074 6865 0a20 2020 2063  ted on the.    c
-00000b50: 6f6d 7075 7465 6420 7661 7269 616e 6365  omputed variance
-00000b60: 7320 7768 6963 6820 6172 6520 636c 6f73  s which are clos
-00000b70: 6572 2074 6f20 6d61 6368 696e 6520 7072  er to machine pr
-00000b80: 6563 6973 696f 6e20 6279 0a20 2020 2063  ecision by.    c
-00000b90: 6f6e 7374 7275 6374 696f 6e2e 0a20 2020  onstruction..   
-00000ba0: 2022 2222 0a20 2020 2023 2069 6620 7765   """.    # if we
-00000bb0: 2061 7265 2066 6974 7469 6e67 206f 6e20   are fitting on 
-00000bc0: 3144 2061 7272 6179 732c 2073 6361 6c65  1D arrays, scale
-00000bd0: 206d 6967 6874 2062 6520 6120 7363 616c   might be a scal
-00000be0: 6172 0a20 2020 2069 6620 6e70 2e69 7373  ar.    if np.iss
-00000bf0: 6361 6c61 7228 7363 616c 6529 3a0a 2020  calar(scale):.  
-00000c00: 2020 2020 2020 6966 2073 6361 6c65 203d        if scale =
-00000c10: 3d20 302e 303a 0a20 2020 2020 2020 2020  = 0.0:.         
-00000c20: 2020 2073 6361 6c65 203d 2031 2e30 0a20     scale = 1.0. 
-00000c30: 2020 2020 2020 2072 6574 7572 6e20 7363         return sc
-00000c40: 616c 650a 2020 2020 2320 7363 616c 6520  ale.    # scale 
-00000c50: 6973 2061 6e20 6172 7261 790a 2020 2020  is an array.    
-00000c60: 656c 7365 3a0a 2020 2020 2020 2020 7870  else:.        xp
-00000c70: 2c20 5f20 3d20 6765 745f 6e61 6d65 7370  , _ = get_namesp
-00000c80: 6163 6528 7363 616c 6529 0a20 2020 2020  ace(scale).     
-00000c90: 2020 2069 6620 636f 6e73 7461 6e74 5f6d     if constant_m
-00000ca0: 6173 6b20 6973 204e 6f6e 653a 0a20 2020  ask is None:.   
-00000cb0: 2020 2020 2020 2020 2023 2044 6574 6563           # Detec
-00000cc0: 7420 6e65 6172 2063 6f6e 7374 616e 7420  t near constant 
-00000cd0: 7661 6c75 6573 2074 6f20 6176 6f69 6420  values to avoid 
-00000ce0: 6469 7669 6469 6e67 2062 7920 6120 7665  dividing by a ve
-00000cf0: 7279 2073 6d61 6c6c 0a20 2020 2020 2020  ry small.       
-00000d00: 2020 2020 2023 2076 616c 7565 2074 6861       # value tha
-00000d10: 7420 636f 756c 6420 6c65 6164 2074 6f20  t could lead to 
-00000d20: 7375 7270 7269 7369 6e67 2072 6573 756c  surprising resul
-00000d30: 7473 2061 6e64 206e 756d 6572 6963 616c  ts and numerical
-00000d40: 0a20 2020 2020 2020 2020 2020 2023 2073  .            # s
-00000d50: 7461 6269 6c69 7479 2069 7373 7565 732e  tability issues.
-00000d60: 0a20 2020 2020 2020 2020 2020 2063 6f6e  .            con
-00000d70: 7374 616e 745f 6d61 736b 203d 2073 6361  stant_mask = sca
-00000d80: 6c65 203c 2031 3020 2a20 7870 2e66 696e  le < 10 * xp.fin
-00000d90: 666f 2873 6361 6c65 2e64 7479 7065 292e  fo(scale.dtype).
-00000da0: 6570 730a 0a20 2020 2020 2020 2069 6620  eps..        if 
-00000db0: 636f 7079 3a0a 2020 2020 2020 2020 2020  copy:.          
-00000dc0: 2020 2320 4e65 7720 6172 7261 7920 746f    # New array to
-00000dd0: 2061 766f 6964 2073 6964 652d 6566 6665   avoid side-effe
-00000de0: 6374 730a 2020 2020 2020 2020 2020 2020  cts.            
-00000df0: 7363 616c 6520 3d20 7870 2e61 7361 7272  scale = xp.asarr
-00000e00: 6179 2873 6361 6c65 2c20 636f 7079 3d54  ay(scale, copy=T
-00000e10: 7275 6529 0a20 2020 2020 2020 2073 6361  rue).        sca
-00000e20: 6c65 5b63 6f6e 7374 616e 745f 6d61 736b  le[constant_mask
-00000e30: 5d20 3d20 312e 300a 2020 2020 2020 2020  ] = 1.0.        
-00000e40: 7265 7475 726e 2073 6361 6c65 0a0a 0a40  return scale...@
-00000e50: 7661 6c69 6461 7465 5f70 6172 616d 7328  validate_params(
-00000e60: 0a20 2020 207b 0a20 2020 2020 2020 2022  .    {.        "
-00000e70: 5822 3a20 5b22 6172 7261 792d 6c69 6b65  X": ["array-like
-00000e80: 222c 2022 7370 6172 7365 206d 6174 7269  ", "sparse matri
-00000e90: 7822 5d2c 0a20 2020 2020 2020 2022 6178  x"],.        "ax
-00000ea0: 6973 223a 205b 4f70 7469 6f6e 7328 496e  is": [Options(In
-00000eb0: 7465 6772 616c 2c20 7b30 2c20 317d 295d  tegral, {0, 1})]
-00000ec0: 2c0a 2020 2020 2020 2020 2277 6974 685f  ,.        "with_
-00000ed0: 6d65 616e 223a 205b 2262 6f6f 6c65 616e  mean": ["boolean
-00000ee0: 225d 2c0a 2020 2020 2020 2020 2277 6974  "],.        "wit
-00000ef0: 685f 7374 6422 3a20 5b22 626f 6f6c 6561  h_std": ["boolea
-00000f00: 6e22 5d2c 0a20 2020 2020 2020 2022 636f  n"],.        "co
-00000f10: 7079 223a 205b 2262 6f6f 6c65 616e 225d  py": ["boolean"]
-00000f20: 2c0a 2020 2020 7d2c 0a20 2020 2070 7265  ,.    },.    pre
-00000f30: 6665 725f 736b 6970 5f6e 6573 7465 645f  fer_skip_nested_
-00000f40: 7661 6c69 6461 7469 6f6e 3d54 7275 652c  validation=True,
-00000f50: 0a29 0a64 6566 2073 6361 6c65 2858 2c20  .).def scale(X, 
-00000f60: 2a2c 2061 7869 733d 302c 2077 6974 685f  *, axis=0, with_
-00000f70: 6d65 616e 3d54 7275 652c 2077 6974 685f  mean=True, with_
-00000f80: 7374 643d 5472 7565 2c20 636f 7079 3d54  std=True, copy=T
-00000f90: 7275 6529 3a0a 2020 2020 2222 2253 7461  rue):.    """Sta
-00000fa0: 6e64 6172 6469 7a65 2061 2064 6174 6173  ndardize a datas
-00000fb0: 6574 2061 6c6f 6e67 2061 6e79 2061 7869  et along any axi
-00000fc0: 732e 0a0a 2020 2020 4365 6e74 6572 2074  s...    Center t
-00000fd0: 6f20 7468 6520 6d65 616e 2061 6e64 2063  o the mean and c
-00000fe0: 6f6d 706f 6e65 6e74 2077 6973 6520 7363  omponent wise sc
-00000ff0: 616c 6520 746f 2075 6e69 7420 7661 7269  ale to unit vari
-00001000: 616e 6365 2e0a 0a20 2020 2052 6561 6420  ance...    Read 
-00001010: 6d6f 7265 2069 6e20 7468 6520 3a72 6566  more in the :ref
-00001020: 3a60 5573 6572 2047 7569 6465 203c 7072  :`User Guide <pr
-00001030: 6570 726f 6365 7373 696e 675f 7363 616c  eprocessing_scal
-00001040: 6572 3e60 2e0a 0a20 2020 2050 6172 616d  er>`...    Param
-00001050: 6574 6572 730a 2020 2020 2d2d 2d2d 2d2d  eters.    ------
-00001060: 2d2d 2d2d 0a20 2020 2058 203a 207b 6172  ----.    X : {ar
-00001070: 7261 792d 6c69 6b65 2c20 7370 6172 7365  ray-like, sparse
-00001080: 206d 6174 7269 787d 206f 6620 7368 6170   matrix} of shap
-00001090: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
-000010a0: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
-000010b0: 2020 5468 6520 6461 7461 2074 6f20 6365    The data to ce
-000010c0: 6e74 6572 2061 6e64 2073 6361 6c65 2e0a  nter and scale..
-000010d0: 0a20 2020 2061 7869 7320 3a20 7b30 2c20  .    axis : {0, 
-000010e0: 317d 2c20 6465 6661 756c 743d 300a 2020  1}, default=0.  
-000010f0: 2020 2020 2020 4178 6973 2075 7365 6420        Axis used 
-00001100: 746f 2063 6f6d 7075 7465 2074 6865 206d  to compute the m
-00001110: 6561 6e73 2061 6e64 2073 7461 6e64 6172  eans and standar
-00001120: 6420 6465 7669 6174 696f 6e73 2061 6c6f  d deviations alo
-00001130: 6e67 2e20 4966 2030 2c0a 2020 2020 2020  ng. If 0,.      
-00001140: 2020 696e 6465 7065 6e64 656e 746c 7920    independently 
-00001150: 7374 616e 6461 7264 697a 6520 6561 6368  standardize each
-00001160: 2066 6561 7475 7265 2c20 6f74 6865 7277   feature, otherw
-00001170: 6973 6520 2869 6620 3129 2073 7461 6e64  ise (if 1) stand
-00001180: 6172 6469 7a65 0a20 2020 2020 2020 2065  ardize.        e
-00001190: 6163 6820 7361 6d70 6c65 2e0a 0a20 2020  ach sample...   
-000011a0: 2077 6974 685f 6d65 616e 203a 2062 6f6f   with_mean : boo
-000011b0: 6c2c 2064 6566 6175 6c74 3d54 7275 650a  l, default=True.
-000011c0: 2020 2020 2020 2020 4966 2054 7275 652c          If True,
-000011d0: 2063 656e 7465 7220 7468 6520 6461 7461   center the data
-000011e0: 2062 6566 6f72 6520 7363 616c 696e 672e   before scaling.
-000011f0: 0a0a 2020 2020 7769 7468 5f73 7464 203a  ..    with_std :
-00001200: 2062 6f6f 6c2c 2064 6566 6175 6c74 3d54   bool, default=T
-00001210: 7275 650a 2020 2020 2020 2020 4966 2054  rue.        If T
-00001220: 7275 652c 2073 6361 6c65 2074 6865 2064  rue, scale the d
-00001230: 6174 6120 746f 2075 6e69 7420 7661 7269  ata to unit vari
-00001240: 616e 6365 2028 6f72 2065 7175 6976 616c  ance (or equival
-00001250: 656e 746c 792c 0a20 2020 2020 2020 2075  ently,.        u
-00001260: 6e69 7420 7374 616e 6461 7264 2064 6576  nit standard dev
-00001270: 6961 7469 6f6e 292e 0a0a 2020 2020 636f  iation)...    co
-00001280: 7079 203a 2062 6f6f 6c2c 2064 6566 6175  py : bool, defau
-00001290: 6c74 3d54 7275 650a 2020 2020 2020 2020  lt=True.        
-000012a0: 4966 2046 616c 7365 2c20 7472 7920 746f  If False, try to
-000012b0: 2061 766f 6964 2061 2063 6f70 7920 616e   avoid a copy an
-000012c0: 6420 7363 616c 6520 696e 2070 6c61 6365  d scale in place
-000012d0: 2e0a 2020 2020 2020 2020 5468 6973 2069  ..        This i
-000012e0: 7320 6e6f 7420 6775 6172 616e 7465 6564  s not guaranteed
-000012f0: 2074 6f20 616c 7761 7973 2077 6f72 6b20   to always work 
-00001300: 696e 2070 6c61 6365 3b20 652e 672e 2069  in place; e.g. i
-00001310: 6620 7468 6520 6461 7461 2069 730a 2020  f the data is.  
-00001320: 2020 2020 2020 6120 6e75 6d70 7920 6172        a numpy ar
-00001330: 7261 7920 7769 7468 2061 6e20 696e 7420  ray with an int 
-00001340: 6474 7970 652c 2061 2063 6f70 7920 7769  dtype, a copy wi
-00001350: 6c6c 2062 6520 7265 7475 726e 6564 2065  ll be returned e
-00001360: 7665 6e20 7769 7468 0a20 2020 2020 2020  ven with.       
-00001370: 2063 6f70 793d 4661 6c73 652e 0a0a 2020   copy=False...  
-00001380: 2020 5265 7475 726e 730a 2020 2020 2d2d    Returns.    --
-00001390: 2d2d 2d2d 2d0a 2020 2020 585f 7472 203a  -----.    X_tr :
-000013a0: 207b 6e64 6172 7261 792c 2073 7061 7273   {ndarray, spars
-000013b0: 6520 6d61 7472 6978 7d20 6f66 2073 6861  e matrix} of sha
-000013c0: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
-000013d0: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
-000013e0: 2020 2054 6865 2074 7261 6e73 666f 726d     The transform
-000013f0: 6564 2064 6174 612e 0a0a 2020 2020 5365  ed data...    Se
-00001400: 6520 416c 736f 0a20 2020 202d 2d2d 2d2d  e Also.    -----
-00001410: 2d2d 2d0a 2020 2020 5374 616e 6461 7264  ---.    Standard
-00001420: 5363 616c 6572 203a 2050 6572 666f 726d  Scaler : Perform
-00001430: 7320 7363 616c 696e 6720 746f 2075 6e69  s scaling to uni
-00001440: 7420 7661 7269 616e 6365 2075 7369 6e67  t variance using
-00001450: 2074 6865 2054 7261 6e73 666f 726d 6572   the Transformer
-00001460: 0a20 2020 2020 2020 2041 5049 2028 652e  .        API (e.
-00001470: 672e 2061 7320 7061 7274 206f 6620 6120  g. as part of a 
-00001480: 7072 6570 726f 6365 7373 696e 670a 2020  preprocessing.  
-00001490: 2020 2020 2020 3a63 6c61 7373 3a60 7e73        :class:`~s
-000014a0: 6b6c 6561 726e 2e70 6970 656c 696e 652e  klearn.pipeline.
-000014b0: 5069 7065 6c69 6e65 6029 2e0a 0a20 2020  Pipeline`)...   
-000014c0: 204e 6f74 6573 0a20 2020 202d 2d2d 2d2d   Notes.    -----
-000014d0: 0a20 2020 2054 6869 7320 696d 706c 656d  .    This implem
-000014e0: 656e 7461 7469 6f6e 2077 696c 6c20 7265  entation will re
-000014f0: 6675 7365 2074 6f20 6365 6e74 6572 2073  fuse to center s
-00001500: 6369 7079 2e73 7061 7273 6520 6d61 7472  cipy.sparse matr
-00001510: 6963 6573 0a20 2020 2073 696e 6365 2069  ices.    since i
-00001520: 7420 776f 756c 6420 6d61 6b65 2074 6865  t would make the
-00001530: 6d20 6e6f 6e2d 7370 6172 7365 2061 6e64  m non-sparse and
-00001540: 2077 6f75 6c64 2070 6f74 656e 7469 616c   would potential
-00001550: 6c79 2063 7261 7368 2074 6865 0a20 2020  ly crash the.   
-00001560: 2070 726f 6772 616d 2077 6974 6820 6d65   program with me
-00001570: 6d6f 7279 2065 7868 6175 7374 696f 6e20  mory exhaustion 
-00001580: 7072 6f62 6c65 6d73 2e0a 0a20 2020 2049  problems...    I
-00001590: 6e73 7465 6164 2074 6865 2063 616c 6c65  nstead the calle
-000015a0: 7220 6973 2065 7870 6563 7465 6420 746f  r is expected to
-000015b0: 2065 6974 6865 7220 7365 7420 6578 706c   either set expl
-000015c0: 6963 6974 6c79 0a20 2020 2060 7769 7468  icitly.    `with
-000015d0: 5f6d 6561 6e3d 4661 6c73 6560 2028 696e  _mean=False` (in
-000015e0: 2074 6861 7420 6361 7365 2c20 6f6e 6c79   that case, only
-000015f0: 2076 6172 6961 6e63 6520 7363 616c 696e   variance scalin
-00001600: 6720 7769 6c6c 2062 650a 2020 2020 7065  g will be.    pe
-00001610: 7266 6f72 6d65 6420 6f6e 2074 6865 2066  rformed on the f
-00001620: 6561 7475 7265 7320 6f66 2074 6865 2043  eatures of the C
-00001630: 5343 206d 6174 7269 7829 206f 7220 746f  SC matrix) or to
-00001640: 2063 616c 6c20 6058 2e74 6f61 7272 6179   call `X.toarray
-00001650: 2829 600a 2020 2020 6966 2068 652f 7368  ()`.    if he/sh
-00001660: 6520 6578 7065 6374 7320 7468 6520 6d61  e expects the ma
-00001670: 7465 7269 616c 697a 6564 2064 656e 7365  terialized dense
-00001680: 2061 7272 6179 2074 6f20 6669 7420 696e   array to fit in
-00001690: 206d 656d 6f72 792e 0a0a 2020 2020 546f   memory...    To
-000016a0: 2061 766f 6964 206d 656d 6f72 7920 636f   avoid memory co
-000016b0: 7079 2074 6865 2063 616c 6c65 7220 7368  py the caller sh
-000016c0: 6f75 6c64 2070 6173 7320 6120 4353 4320  ould pass a CSC 
-000016d0: 6d61 7472 6978 2e0a 0a20 2020 204e 614e  matrix...    NaN
-000016e0: 7320 6172 6520 7472 6561 7465 6420 6173  s are treated as
-000016f0: 206d 6973 7369 6e67 2076 616c 7565 733a   missing values:
-00001700: 2064 6973 7265 6761 7264 6564 2074 6f20   disregarded to 
-00001710: 636f 6d70 7574 6520 7468 6520 7374 6174  compute the stat
-00001720: 6973 7469 6373 2c0a 2020 2020 616e 6420  istics,.    and 
-00001730: 6d61 696e 7461 696e 6564 2064 7572 696e  maintained durin
-00001740: 6720 7468 6520 6461 7461 2074 7261 6e73  g the data trans
-00001750: 666f 726d 6174 696f 6e2e 0a0a 2020 2020  formation...    
-00001760: 5765 2075 7365 2061 2062 6961 7365 6420  We use a biased 
-00001770: 6573 7469 6d61 746f 7220 666f 7220 7468  estimator for th
-00001780: 6520 7374 616e 6461 7264 2064 6576 6961  e standard devia
-00001790: 7469 6f6e 2c20 6571 7569 7661 6c65 6e74  tion, equivalent
-000017a0: 2074 6f0a 2020 2020 606e 756d 7079 2e73   to.    `numpy.s
-000017b0: 7464 2878 2c20 6464 6f66 3d30 2960 2e20  td(x, ddof=0)`. 
-000017c0: 4e6f 7465 2074 6861 7420 7468 6520 6368  Note that the ch
-000017d0: 6f69 6365 206f 6620 6064 646f 6660 2069  oice of `ddof` i
-000017e0: 7320 756e 6c69 6b65 6c79 2074 6f0a 2020  s unlikely to.  
-000017f0: 2020 6166 6665 6374 206d 6f64 656c 2070    affect model p
-00001800: 6572 666f 726d 616e 6365 2e0a 0a20 2020  erformance...   
-00001810: 2046 6f72 2061 2063 6f6d 7061 7269 736f   For a compariso
-00001820: 6e20 6f66 2074 6865 2064 6966 6665 7265  n of the differe
-00001830: 6e74 2073 6361 6c65 7273 2c20 7472 616e  nt scalers, tran
-00001840: 7366 6f72 6d65 7273 2c20 616e 6420 6e6f  sformers, and no
-00001850: 726d 616c 697a 6572 732c 0a20 2020 2073  rmalizers,.    s
-00001860: 6565 3a20 3a72 6566 3a60 7370 6878 5f67  ee: :ref:`sphx_g
-00001870: 6c72 5f61 7574 6f5f 6578 616d 706c 6573  lr_auto_examples
-00001880: 5f70 7265 7072 6f63 6573 7369 6e67 5f70  _preprocessing_p
-00001890: 6c6f 745f 616c 6c5f 7363 616c 696e 672e  lot_all_scaling.
-000018a0: 7079 602e 0a0a 2020 2020 2e2e 2077 6172  py`...    .. war
-000018b0: 6e69 6e67 3a3a 2052 6973 6b20 6f66 2064  ning:: Risk of d
-000018c0: 6174 6120 6c65 616b 0a0a 2020 2020 2020  ata leak..      
-000018d0: 2020 446f 206e 6f74 2075 7365 203a 6675    Do not use :fu
-000018e0: 6e63 3a60 7e73 6b6c 6561 726e 2e70 7265  nc:`~sklearn.pre
-000018f0: 7072 6f63 6573 7369 6e67 2e73 6361 6c65  processing.scale
-00001900: 6020 756e 6c65 7373 2079 6f75 206b 6e6f  ` unless you kno
-00001910: 770a 2020 2020 2020 2020 7768 6174 2079  w.        what y
-00001920: 6f75 2061 7265 2064 6f69 6e67 2e20 4120  ou are doing. A 
-00001930: 636f 6d6d 6f6e 206d 6973 7461 6b65 2069  common mistake i
-00001940: 7320 746f 2061 7070 6c79 2069 7420 746f  s to apply it to
-00001950: 2074 6865 2065 6e74 6972 6520 6461 7461   the entire data
-00001960: 0a20 2020 2020 2020 202a 6265 666f 7265  .        *before
-00001970: 2a20 7370 6c69 7474 696e 6720 696e 746f  * splitting into
-00001980: 2074 7261 696e 696e 6720 616e 6420 7465   training and te
-00001990: 7374 2073 6574 732e 2054 6869 7320 7769  st sets. This wi
-000019a0: 6c6c 2062 6961 7320 7468 650a 2020 2020  ll bias the.    
-000019b0: 2020 2020 6d6f 6465 6c20 6576 616c 7561      model evalua
-000019c0: 7469 6f6e 2062 6563 6175 7365 2069 6e66  tion because inf
-000019d0: 6f72 6d61 7469 6f6e 2077 6f75 6c64 2068  ormation would h
-000019e0: 6176 6520 6c65 616b 6564 2066 726f 6d20  ave leaked from 
-000019f0: 7468 6520 7465 7374 0a20 2020 2020 2020  the test.       
-00001a00: 2073 6574 2074 6f20 7468 6520 7472 6169   set to the trai
-00001a10: 6e69 6e67 2073 6574 2e0a 2020 2020 2020  ning set..      
-00001a20: 2020 496e 2067 656e 6572 616c 2c20 7765    In general, we
-00001a30: 2072 6563 6f6d 6d65 6e64 2075 7369 6e67   recommend using
-00001a40: 0a20 2020 2020 2020 203a 636c 6173 733a  .        :class:
-00001a50: 607e 736b 6c65 6172 6e2e 7072 6570 726f  `~sklearn.prepro
-00001a60: 6365 7373 696e 672e 5374 616e 6461 7264  cessing.Standard
-00001a70: 5363 616c 6572 6020 7769 7468 696e 2061  Scaler` within a
-00001a80: 0a20 2020 2020 2020 203a 7265 663a 6050  .        :ref:`P
-00001a90: 6970 656c 696e 6520 3c70 6970 656c 696e  ipeline <pipelin
-00001aa0: 653e 6020 696e 206f 7264 6572 2074 6f20  e>` in order to 
-00001ab0: 7072 6576 656e 7420 6d6f 7374 2072 6973  prevent most ris
-00001ac0: 6b73 206f 6620 6461 7461 0a20 2020 2020  ks of data.     
-00001ad0: 2020 206c 6561 6b69 6e67 3a20 6070 6970     leaking: `pip
-00001ae0: 6520 3d20 6d61 6b65 5f70 6970 656c 696e  e = make_pipelin
-00001af0: 6528 5374 616e 6461 7264 5363 616c 6572  e(StandardScaler
-00001b00: 2829 2c20 4c6f 6769 7374 6963 5265 6772  (), LogisticRegr
-00001b10: 6573 7369 6f6e 2829 2960 2e0a 0a20 2020  ession())`...   
-00001b20: 2045 7861 6d70 6c65 730a 2020 2020 2d2d   Examples.    --
-00001b30: 2d2d 2d2d 2d2d 0a20 2020 203e 3e3e 2066  ------.    >>> f
-00001b40: 726f 6d20 736b 6c65 6172 6e2e 7072 6570  rom sklearn.prep
-00001b50: 726f 6365 7373 696e 6720 696d 706f 7274  rocessing import
-00001b60: 2073 6361 6c65 0a20 2020 203e 3e3e 2058   scale.    >>> X
-00001b70: 203d 205b 5b2d 322c 2031 2c20 325d 2c20   = [[-2, 1, 2], 
-00001b80: 5b2d 312c 2030 2c20 315d 5d0a 2020 2020  [-1, 0, 1]].    
-00001b90: 3e3e 3e20 7363 616c 6528 582c 2061 7869  >>> scale(X, axi
-00001ba0: 733d 3029 2020 2320 7363 616c 696e 6720  s=0)  # scaling 
-00001bb0: 6561 6368 2063 6f6c 756d 6e20 696e 6465  each column inde
-00001bc0: 7065 6e64 656e 746c 790a 2020 2020 6172  pendently.    ar
-00001bd0: 7261 7928 5b5b 2d31 2e2c 2020 312e 2c20  ray([[-1.,  1., 
-00001be0: 2031 2e5d 2c0a 2020 2020 2020 2020 2020   1.],.          
-00001bf0: 205b 2031 2e2c 202d 312e 2c20 2d31 2e5d   [ 1., -1., -1.]
-00001c00: 5d29 0a20 2020 203e 3e3e 2073 6361 6c65  ]).    >>> scale
-00001c10: 2858 2c20 6178 6973 3d31 2920 2023 2073  (X, axis=1)  # s
-00001c20: 6361 6c69 6e67 2065 6163 6820 726f 7720  caling each row 
-00001c30: 696e 6465 7065 6e64 656e 746c 790a 2020  independently.  
-00001c40: 2020 6172 7261 7928 5b5b 2d31 2e33 372e    array([[-1.37.
-00001c50: 2e2e 2c20 2030 2e33 392e 2e2e 2c20 2030  ..,  0.39...,  0
-00001c60: 2e39 382e 2e2e 5d2c 0a20 2020 2020 2020  .98...],.       
-00001c70: 2020 2020 5b2d 312e 3232 2e2e 2e2c 2020      [-1.22...,  
-00001c80: 302e 2020 2020 202c 2020 312e 3232 2e2e  0.     ,  1.22..
-00001c90: 2e5d 5d29 0a20 2020 2022 2222 0a20 2020  .]]).    """.   
-00001ca0: 2058 203d 2063 6865 636b 5f61 7272 6179   X = check_array
-00001cb0: 280a 2020 2020 2020 2020 582c 0a20 2020  (.        X,.   
-00001cc0: 2020 2020 2061 6363 6570 745f 7370 6172       accept_spar
-00001cd0: 7365 3d22 6373 6322 2c0a 2020 2020 2020  se="csc",.      
-00001ce0: 2020 636f 7079 3d63 6f70 792c 0a20 2020    copy=copy,.   
-00001cf0: 2020 2020 2065 6e73 7572 655f 3264 3d46       ensure_2d=F
-00001d00: 616c 7365 2c0a 2020 2020 2020 2020 6573  alse,.        es
-00001d10: 7469 6d61 746f 723d 2274 6865 2073 6361  timator="the sca
-00001d20: 6c65 2066 756e 6374 696f 6e22 2c0a 2020  le function",.  
-00001d30: 2020 2020 2020 6474 7970 653d 464c 4f41        dtype=FLOA
-00001d40: 545f 4454 5950 4553 2c0a 2020 2020 2020  T_DTYPES,.      
-00001d50: 2020 666f 7263 655f 616c 6c5f 6669 6e69    force_all_fini
-00001d60: 7465 3d22 616c 6c6f 772d 6e61 6e22 2c0a  te="allow-nan",.
-00001d70: 2020 2020 290a 2020 2020 6966 2073 7061      ).    if spa
-00001d80: 7273 652e 6973 7370 6172 7365 2858 293a  rse.issparse(X):
-00001d90: 0a20 2020 2020 2020 2069 6620 7769 7468  .        if with
-00001da0: 5f6d 6561 6e3a 0a20 2020 2020 2020 2020  _mean:.         
-00001db0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-00001dc0: 726f 7228 0a20 2020 2020 2020 2020 2020  ror(.           
-00001dd0: 2020 2020 2022 4361 6e6e 6f74 2063 656e       "Cannot cen
-00001de0: 7465 7220 7370 6172 7365 206d 6174 7269  ter sparse matri
-00001df0: 6365 733a 2070 6173 7320 6077 6974 685f  ces: pass `with_
-00001e00: 6d65 616e 3d46 616c 7365 6020 696e 7374  mean=False` inst
-00001e10: 6561 6422 0a20 2020 2020 2020 2020 2020  ead".           
-00001e20: 2020 2020 2022 2053 6565 2064 6f63 7374       " See docst
-00001e30: 7269 6e67 2066 6f72 206d 6f74 6976 6174  ring for motivat
-00001e40: 696f 6e20 616e 6420 616c 7465 726e 6174  ion and alternat
-00001e50: 6976 6573 2e22 0a20 2020 2020 2020 2020  ives.".         
-00001e60: 2020 2029 0a20 2020 2020 2020 2069 6620     ).        if 
-00001e70: 6178 6973 2021 3d20 303a 0a20 2020 2020  axis != 0:.     
-00001e80: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
-00001e90: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-00001ea0: 2020 2020 2020 2020 2022 4361 6e20 6f6e           "Can on
-00001eb0: 6c79 2073 6361 6c65 2073 7061 7273 6520  ly scale sparse 
-00001ec0: 6d61 7472 6978 206f 6e20 6178 6973 3d30  matrix on axis=0
-00001ed0: 2c20 2067 6f74 2061 7869 733d 2564 2220  ,  got axis=%d" 
-00001ee0: 2520 6178 6973 0a20 2020 2020 2020 2020  % axis.         
-00001ef0: 2020 2029 0a20 2020 2020 2020 2069 6620     ).        if 
-00001f00: 7769 7468 5f73 7464 3a0a 2020 2020 2020  with_std:.      
-00001f10: 2020 2020 2020 5f2c 2076 6172 203d 206d        _, var = m
-00001f20: 6561 6e5f 7661 7269 616e 6365 5f61 7869  ean_variance_axi
-00001f30: 7328 582c 2061 7869 733d 3029 0a20 2020  s(X, axis=0).   
-00001f40: 2020 2020 2020 2020 2076 6172 203d 205f           var = _
-00001f50: 6861 6e64 6c65 5f7a 6572 6f73 5f69 6e5f  handle_zeros_in_
-00001f60: 7363 616c 6528 7661 722c 2063 6f70 793d  scale(var, copy=
-00001f70: 4661 6c73 6529 0a20 2020 2020 2020 2020  False).         
-00001f80: 2020 2069 6e70 6c61 6365 5f63 6f6c 756d     inplace_colum
-00001f90: 6e5f 7363 616c 6528 582c 2031 202f 206e  n_scale(X, 1 / n
-00001fa0: 702e 7371 7274 2876 6172 2929 0a20 2020  p.sqrt(var)).   
-00001fb0: 2065 6c73 653a 0a20 2020 2020 2020 2058   else:.        X
-00001fc0: 203d 206e 702e 6173 6172 7261 7928 5829   = np.asarray(X)
-00001fd0: 0a20 2020 2020 2020 2069 6620 7769 7468  .        if with
-00001fe0: 5f6d 6561 6e3a 0a20 2020 2020 2020 2020  _mean:.         
-00001ff0: 2020 206d 6561 6e5f 203d 206e 702e 6e61     mean_ = np.na
-00002000: 6e6d 6561 6e28 582c 2061 7869 7329 0a20  nmean(X, axis). 
-00002010: 2020 2020 2020 2069 6620 7769 7468 5f73         if with_s
-00002020: 7464 3a0a 2020 2020 2020 2020 2020 2020  td:.            
-00002030: 7363 616c 655f 203d 206e 702e 6e61 6e73  scale_ = np.nans
-00002040: 7464 2858 2c20 6178 6973 290a 2020 2020  td(X, axis).    
-00002050: 2020 2020 2320 5872 2069 7320 6120 7669      # Xr is a vi
-00002060: 6577 206f 6e20 7468 6520 6f72 6967 696e  ew on the origin
-00002070: 616c 2061 7272 6179 2074 6861 7420 656e  al array that en
-00002080: 6162 6c65 7320 6561 7379 2075 7365 206f  ables easy use o
-00002090: 660a 2020 2020 2020 2020 2320 6272 6f61  f.        # broa
-000020a0: 6463 6173 7469 6e67 206f 6e20 7468 6520  dcasting on the 
-000020b0: 6178 6973 2069 6e20 7768 6963 6820 7765  axis in which we
-000020c0: 2061 7265 2069 6e74 6572 6573 7465 6420   are interested 
-000020d0: 696e 0a20 2020 2020 2020 2058 7220 3d20  in.        Xr = 
-000020e0: 6e70 2e72 6f6c 6c61 7869 7328 582c 2061  np.rollaxis(X, a
-000020f0: 7869 7329 0a20 2020 2020 2020 2069 6620  xis).        if 
-00002100: 7769 7468 5f6d 6561 6e3a 0a20 2020 2020  with_mean:.     
-00002110: 2020 2020 2020 2058 7220 2d3d 206d 6561         Xr -= mea
-00002120: 6e5f 0a20 2020 2020 2020 2020 2020 206d  n_.            m
-00002130: 6561 6e5f 3120 3d20 6e70 2e6e 616e 6d65  ean_1 = np.nanme
-00002140: 616e 2858 722c 2061 7869 733d 3029 0a20  an(Xr, axis=0). 
-00002150: 2020 2020 2020 2020 2020 2023 2056 6572             # Ver
-00002160: 6966 7920 7468 6174 206d 6561 6e5f 3120  ify that mean_1 
-00002170: 6973 2027 636c 6f73 6520 746f 207a 6572  is 'close to zer
-00002180: 6f27 2e20 4966 2058 2063 6f6e 7461 696e  o'. If X contain
-00002190: 7320 7665 7279 0a20 2020 2020 2020 2020  s very.         
-000021a0: 2020 2023 206c 6172 6765 2076 616c 7565     # large value
-000021b0: 732c 206d 6561 6e5f 3120 6361 6e20 616c  s, mean_1 can al
-000021c0: 736f 2062 6520 7665 7279 206c 6172 6765  so be very large
-000021d0: 2c20 6475 6520 746f 2061 206c 6163 6b20  , due to a lack 
-000021e0: 6f66 0a20 2020 2020 2020 2020 2020 2023  of.            #
-000021f0: 2070 7265 6369 7369 6f6e 206f 6620 6d65   precision of me
-00002200: 616e 5f2e 2049 6e20 7468 6973 2063 6173  an_. In this cas
-00002210: 652c 2061 2070 7265 2d73 6361 6c69 6e67  e, a pre-scaling
-00002220: 206f 6620 7468 650a 2020 2020 2020 2020   of the.        
-00002230: 2020 2020 2320 636f 6e63 6572 6e65 6420      # concerned 
-00002240: 6665 6174 7572 6520 6973 2065 6666 6963  feature is effic
-00002250: 6965 6e74 2c20 666f 7220 696e 7374 616e  ient, for instan
-00002260: 6365 2062 7920 6974 7320 6d65 616e 206f  ce by its mean o
-00002270: 720a 2020 2020 2020 2020 2020 2020 2320  r.            # 
-00002280: 6d61 7869 6d75 6d2e 0a20 2020 2020 2020  maximum..       
-00002290: 2020 2020 2069 6620 6e6f 7420 6e70 2e61       if not np.a
-000022a0: 6c6c 636c 6f73 6528 6d65 616e 5f31 2c20  llclose(mean_1, 
-000022b0: 3029 3a0a 2020 2020 2020 2020 2020 2020  0):.            
-000022c0: 2020 2020 7761 726e 696e 6773 2e77 6172      warnings.war
-000022d0: 6e28 0a20 2020 2020 2020 2020 2020 2020  n(.             
-000022e0: 2020 2020 2020 2022 4e75 6d65 7269 6361         "Numerica
-000022f0: 6c20 6973 7375 6573 2077 6572 6520 656e  l issues were en
-00002300: 636f 756e 7465 7265 6420 220a 2020 2020  countered ".    
-00002310: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002320: 2277 6865 6e20 6365 6e74 6572 696e 6720  "when centering 
-00002330: 7468 6520 6461 7461 2022 0a20 2020 2020  the data ".     
-00002340: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00002350: 616e 6420 6d69 6768 7420 6e6f 7420 6265  and might not be
-00002360: 2073 6f6c 7665 642e 2044 6174 6173 6574   solved. Dataset
-00002370: 206d 6179 2022 0a20 2020 2020 2020 2020   may ".         
-00002380: 2020 2020 2020 2020 2020 2022 636f 6e74             "cont
-00002390: 6169 6e20 746f 6f20 6c61 7267 6520 7661  ain too large va
-000023a0: 6c75 6573 2e20 596f 7520 6d61 7920 6e65  lues. You may ne
-000023b0: 6564 2022 0a20 2020 2020 2020 2020 2020  ed ".           
-000023c0: 2020 2020 2020 2020 2022 746f 2070 7265           "to pre
-000023d0: 7363 616c 6520 796f 7572 2066 6561 7475  scale your featu
-000023e0: 7265 732e 220a 2020 2020 2020 2020 2020  res.".          
-000023f0: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
-00002400: 2020 2020 2020 2020 5872 202d 3d20 6d65          Xr -= me
-00002410: 616e 5f31 0a20 2020 2020 2020 2069 6620  an_1.        if 
-00002420: 7769 7468 5f73 7464 3a0a 2020 2020 2020  with_std:.      
-00002430: 2020 2020 2020 7363 616c 655f 203d 205f        scale_ = _
-00002440: 6861 6e64 6c65 5f7a 6572 6f73 5f69 6e5f  handle_zeros_in_
-00002450: 7363 616c 6528 7363 616c 655f 2c20 636f  scale(scale_, co
-00002460: 7079 3d46 616c 7365 290a 2020 2020 2020  py=False).      
-00002470: 2020 2020 2020 5872 202f 3d20 7363 616c        Xr /= scal
-00002480: 655f 0a20 2020 2020 2020 2020 2020 2069  e_.            i
-00002490: 6620 7769 7468 5f6d 6561 6e3a 0a20 2020  f with_mean:.   
-000024a0: 2020 2020 2020 2020 2020 2020 206d 6561               mea
-000024b0: 6e5f 3220 3d20 6e70 2e6e 616e 6d65 616e  n_2 = np.nanmean
-000024c0: 2858 722c 2061 7869 733d 3029 0a20 2020  (Xr, axis=0).   
-000024d0: 2020 2020 2020 2020 2020 2020 2023 2049               # I
-000024e0: 6620 6d65 616e 5f32 2069 7320 6e6f 7420  f mean_2 is not 
-000024f0: 2763 6c6f 7365 2074 6f20 7a65 726f 272c  'close to zero',
-00002500: 2069 7420 636f 6d65 7320 6672 6f6d 2074   it comes from t
-00002510: 6865 2066 6163 7420 7468 6174 0a20 2020  he fact that.   
-00002520: 2020 2020 2020 2020 2020 2020 2023 2073               # s
-00002530: 6361 6c65 5f20 6973 2076 6572 7920 736d  cale_ is very sm
-00002540: 616c 6c20 736f 2074 6861 7420 6d65 616e  all so that mean
-00002550: 5f32 203d 206d 6561 6e5f 312f 7363 616c  _2 = mean_1/scal
-00002560: 655f 203e 2030 2c20 6576 656e 0a20 2020  e_ > 0, even.   
-00002570: 2020 2020 2020 2020 2020 2020 2023 2069               # i
-00002580: 6620 6d65 616e 5f31 2077 6173 2063 6c6f  f mean_1 was clo
-00002590: 7365 2074 6f20 7a65 726f 2e20 5468 6520  se to zero. The 
-000025a0: 7072 6f62 6c65 6d20 6973 2074 6875 7320  problem is thus 
-000025b0: 6573 7365 6e74 6961 6c6c 790a 2020 2020  essentially.    
-000025c0: 2020 2020 2020 2020 2020 2020 2320 6475              # du
-000025d0: 6520 746f 2074 6865 206c 6163 6b20 6f66  e to the lack of
-000025e0: 2070 7265 6369 7369 6f6e 206f 6620 6d65   precision of me
-000025f0: 616e 5f2e 2041 2073 6f6c 7574 696f 6e20  an_. A solution 
-00002600: 6973 2074 6865 6e20 746f 0a20 2020 2020  is then to.     
-00002610: 2020 2020 2020 2020 2020 2023 2073 7562             # sub
-00002620: 7472 6163 7420 7468 6520 6d65 616e 2061  tract the mean a
-00002630: 6761 696e 3a0a 2020 2020 2020 2020 2020  gain:.          
-00002640: 2020 2020 2020 6966 206e 6f74 206e 702e        if not np.
-00002650: 616c 6c63 6c6f 7365 286d 6561 6e5f 322c  allclose(mean_2,
-00002660: 2030 293a 0a20 2020 2020 2020 2020 2020   0):.           
-00002670: 2020 2020 2020 2020 2077 6172 6e69 6e67           warning
-00002680: 732e 7761 726e 280a 2020 2020 2020 2020  s.warn(.        
-00002690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000026a0: 224e 756d 6572 6963 616c 2069 7373 7565  "Numerical issue
-000026b0: 7320 7765 7265 2065 6e63 6f75 6e74 6572  s were encounter
-000026c0: 6564 2022 0a20 2020 2020 2020 2020 2020  ed ".           
-000026d0: 2020 2020 2020 2020 2020 2020 2022 7768               "wh
-000026e0: 656e 2073 6361 6c69 6e67 2074 6865 2064  en scaling the d
-000026f0: 6174 6120 220a 2020 2020 2020 2020 2020  ata ".          
-00002700: 2020 2020 2020 2020 2020 2020 2020 2261                "a
-00002710: 6e64 206d 6967 6874 206e 6f74 2062 6520  nd might not be 
-00002720: 736f 6c76 6564 2e20 5468 6520 7374 616e  solved. The stan
-00002730: 6461 7264 2022 0a20 2020 2020 2020 2020  dard ".         
-00002740: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00002750: 6465 7669 6174 696f 6e20 6f66 2074 6865  deviation of the
-00002760: 2064 6174 6120 6973 2070 726f 6261 626c   data is probabl
-00002770: 7920 220a 2020 2020 2020 2020 2020 2020  y ".            
-00002780: 2020 2020 2020 2020 2020 2020 2276 6572              "ver
-00002790: 7920 636c 6f73 6520 746f 2030 2e20 220a  y close to 0. ".
-000027a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000027b0: 2020 2020 290a 2020 2020 2020 2020 2020      ).          
-000027c0: 2020 2020 2020 2020 2020 5872 202d 3d20            Xr -= 
-000027d0: 6d65 616e 5f32 0a20 2020 2072 6574 7572  mean_2.    retur
-000027e0: 6e20 580a 0a0a 636c 6173 7320 4d69 6e4d  n X...class MinM
-000027f0: 6178 5363 616c 6572 284f 6e65 546f 4f6e  axScaler(OneToOn
-00002800: 6546 6561 7475 7265 4d69 7869 6e2c 2054  eFeatureMixin, T
-00002810: 7261 6e73 666f 726d 6572 4d69 7869 6e2c  ransformerMixin,
-00002820: 2042 6173 6545 7374 696d 6174 6f72 293a   BaseEstimator):
-00002830: 0a20 2020 2022 2222 5472 616e 7366 6f72  .    """Transfor
-00002840: 6d20 6665 6174 7572 6573 2062 7920 7363  m features by sc
-00002850: 616c 696e 6720 6561 6368 2066 6561 7475  aling each featu
-00002860: 7265 2074 6f20 6120 6769 7665 6e20 7261  re to a given ra
-00002870: 6e67 652e 0a0a 2020 2020 5468 6973 2065  nge...    This e
-00002880: 7374 696d 6174 6f72 2073 6361 6c65 7320  stimator scales 
-00002890: 616e 6420 7472 616e 736c 6174 6573 2065  and translates e
-000028a0: 6163 6820 6665 6174 7572 6520 696e 6469  ach feature indi
-000028b0: 7669 6475 616c 6c79 2073 7563 680a 2020  vidually such.  
-000028c0: 2020 7468 6174 2069 7420 6973 2069 6e20    that it is in 
-000028d0: 7468 6520 6769 7665 6e20 7261 6e67 6520  the given range 
-000028e0: 6f6e 2074 6865 2074 7261 696e 696e 6720  on the training 
-000028f0: 7365 742c 2065 2e67 2e20 6265 7477 6565  set, e.g. betwee
-00002900: 6e0a 2020 2020 7a65 726f 2061 6e64 206f  n.    zero and o
-00002910: 6e65 2e0a 0a20 2020 2054 6865 2074 7261  ne...    The tra
-00002920: 6e73 666f 726d 6174 696f 6e20 6973 2067  nsformation is g
-00002930: 6976 656e 2062 793a 3a0a 0a20 2020 2020  iven by::..     
-00002940: 2020 2058 5f73 7464 203d 2028 5820 2d20     X_std = (X - 
-00002950: 582e 6d69 6e28 6178 6973 3d30 2929 202f  X.min(axis=0)) /
-00002960: 2028 582e 6d61 7828 6178 6973 3d30 2920   (X.max(axis=0) 
-00002970: 2d20 582e 6d69 6e28 6178 6973 3d30 2929  - X.min(axis=0))
-00002980: 0a20 2020 2020 2020 2058 5f73 6361 6c65  .        X_scale
-00002990: 6420 3d20 585f 7374 6420 2a20 286d 6178  d = X_std * (max
-000029a0: 202d 206d 696e 2920 2b20 6d69 6e0a 0a20   - min) + min.. 
-000029b0: 2020 2077 6865 7265 206d 696e 2c20 6d61     where min, ma
-000029c0: 7820 3d20 6665 6174 7572 655f 7261 6e67  x = feature_rang
-000029d0: 652e 0a0a 2020 2020 5468 6973 2074 7261  e...    This tra
-000029e0: 6e73 666f 726d 6174 696f 6e20 6973 206f  nsformation is o
-000029f0: 6674 656e 2075 7365 6420 6173 2061 6e20  ften used as an 
-00002a00: 616c 7465 726e 6174 6976 6520 746f 207a  alternative to z
-00002a10: 6572 6f20 6d65 616e 2c0a 2020 2020 756e  ero mean,.    un
-00002a20: 6974 2076 6172 6961 6e63 6520 7363 616c  it variance scal
-00002a30: 696e 672e 0a0a 2020 2020 604d 696e 4d61  ing...    `MinMa
-00002a40: 7853 6361 6c65 7260 2064 6f65 736e 2774  xScaler` doesn't
-00002a50: 2072 6564 7563 6520 7468 6520 6566 6665   reduce the effe
-00002a60: 6374 206f 6620 6f75 746c 6965 7273 2c20  ct of outliers, 
-00002a70: 6275 7420 6974 206c 696e 6561 726c 790a  but it linearly.
-00002a80: 2020 2020 7363 616c 6573 2074 6865 6d20      scales them 
-00002a90: 646f 776e 2069 6e74 6f20 6120 6669 7865  down into a fixe
-00002aa0: 6420 7261 6e67 652c 2077 6865 7265 2074  d range, where t
-00002ab0: 6865 206c 6172 6765 7374 206f 6363 7572  he largest occur
-00002ac0: 7269 6e67 2064 6174 6120 706f 696e 740a  ring data point.
-00002ad0: 2020 2020 636f 7272 6573 706f 6e64 7320      corresponds 
-00002ae0: 746f 2074 6865 206d 6178 696d 756d 2076  to the maximum v
-00002af0: 616c 7565 2061 6e64 2074 6865 2073 6d61  alue and the sma
-00002b00: 6c6c 6573 7420 6f6e 6520 636f 7272 6573  llest one corres
-00002b10: 706f 6e64 7320 746f 2074 6865 0a20 2020  ponds to the.   
-00002b20: 206d 696e 696d 756d 2076 616c 7565 2e20   minimum value. 
-00002b30: 466f 7220 616e 2065 7861 6d70 6c65 2076  For an example v
-00002b40: 6973 7561 6c69 7a61 7469 6f6e 2c20 7265  isualization, re
-00002b50: 6665 7220 746f 203a 7265 663a 6043 6f6d  fer to :ref:`Com
-00002b60: 7061 7265 0a20 2020 204d 696e 4d61 7853  pare.    MinMaxS
-00002b70: 6361 6c65 7220 7769 7468 206f 7468 6572  caler with other
-00002b80: 2073 6361 6c65 7273 203c 706c 6f74 5f61   scalers <plot_a
-00002b90: 6c6c 5f73 6361 6c69 6e67 5f6d 696e 6d61  ll_scaling_minma
-00002ba0: 785f 7363 616c 6572 5f73 6563 7469 6f6e  x_scaler_section
-00002bb0: 3e60 2e0a 0a20 2020 2052 6561 6420 6d6f  >`...    Read mo
-00002bc0: 7265 2069 6e20 7468 6520 3a72 6566 3a60  re in the :ref:`
-00002bd0: 5573 6572 2047 7569 6465 203c 7072 6570  User Guide <prep
-00002be0: 726f 6365 7373 696e 675f 7363 616c 6572  rocessing_scaler
-00002bf0: 3e60 2e0a 0a20 2020 2050 6172 616d 6574  >`...    Paramet
-00002c00: 6572 730a 2020 2020 2d2d 2d2d 2d2d 2d2d  ers.    --------
-00002c10: 2d2d 0a20 2020 2066 6561 7475 7265 5f72  --.    feature_r
-00002c20: 616e 6765 203a 2074 7570 6c65 2028 6d69  ange : tuple (mi
-00002c30: 6e2c 206d 6178 292c 2064 6566 6175 6c74  n, max), default
-00002c40: 3d28 302c 2031 290a 2020 2020 2020 2020  =(0, 1).        
-00002c50: 4465 7369 7265 6420 7261 6e67 6520 6f66  Desired range of
-00002c60: 2074 7261 6e73 666f 726d 6564 2064 6174   transformed dat
-00002c70: 612e 0a0a 2020 2020 636f 7079 203a 2062  a...    copy : b
-00002c80: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
-00002c90: 650a 2020 2020 2020 2020 5365 7420 746f  e.        Set to
-00002ca0: 2046 616c 7365 2074 6f20 7065 7266 6f72   False to perfor
-00002cb0: 6d20 696e 706c 6163 6520 726f 7720 6e6f  m inplace row no
-00002cc0: 726d 616c 697a 6174 696f 6e20 616e 6420  rmalization and 
-00002cd0: 6176 6f69 6420 610a 2020 2020 2020 2020  avoid a.        
-00002ce0: 636f 7079 2028 6966 2074 6865 2069 6e70  copy (if the inp
-00002cf0: 7574 2069 7320 616c 7265 6164 7920 6120  ut is already a 
-00002d00: 6e75 6d70 7920 6172 7261 7929 2e0a 0a20  numpy array)... 
-00002d10: 2020 2063 6c69 7020 3a20 626f 6f6c 2c20     clip : bool, 
-00002d20: 6465 6661 756c 743d 4661 6c73 650a 2020  default=False.  
-00002d30: 2020 2020 2020 5365 7420 746f 2054 7275        Set to Tru
-00002d40: 6520 746f 2063 6c69 7020 7472 616e 7366  e to clip transf
-00002d50: 6f72 6d65 6420 7661 6c75 6573 206f 6620  ormed values of 
-00002d60: 6865 6c64 2d6f 7574 2064 6174 6120 746f  held-out data to
-00002d70: 0a20 2020 2020 2020 2070 726f 7669 6465  .        provide
-00002d80: 6420 6066 6561 7475 7265 2072 616e 6765  d `feature range
-00002d90: 602e 0a0a 2020 2020 2020 2020 2e2e 2076  `...        .. v
-00002da0: 6572 7369 6f6e 6164 6465 643a 3a20 302e  ersionadded:: 0.
-00002db0: 3234 0a0a 2020 2020 4174 7472 6962 7574  24..    Attribut
-00002dc0: 6573 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d  es.    ---------
-00002dd0: 2d0a 2020 2020 6d69 6e5f 203a 206e 6461  -.    min_ : nda
-00002de0: 7272 6179 206f 6620 7368 6170 6520 286e  rray of shape (n
-00002df0: 5f66 6561 7475 7265 732c 290a 2020 2020  _features,).    
-00002e00: 2020 2020 5065 7220 6665 6174 7572 6520      Per feature 
-00002e10: 6164 6a75 7374 6d65 6e74 2066 6f72 206d  adjustment for m
-00002e20: 696e 696d 756d 2e20 4571 7569 7661 6c65  inimum. Equivale
-00002e30: 6e74 2074 6f0a 2020 2020 2020 2020 6060  nt to.        ``
-00002e40: 6d69 6e20 2d20 582e 6d69 6e28 6178 6973  min - X.min(axis
-00002e50: 3d30 2920 2a20 7365 6c66 2e73 6361 6c65  =0) * self.scale
-00002e60: 5f60 600a 0a20 2020 2073 6361 6c65 5f20  _``..    scale_ 
-00002e70: 3a20 6e64 6172 7261 7920 6f66 2073 6861  : ndarray of sha
-00002e80: 7065 2028 6e5f 6665 6174 7572 6573 2c29  pe (n_features,)
-00002e90: 0a20 2020 2020 2020 2050 6572 2066 6561  .        Per fea
-00002ea0: 7475 7265 2072 656c 6174 6976 6520 7363  ture relative sc
-00002eb0: 616c 696e 6720 6f66 2074 6865 2064 6174  aling of the dat
-00002ec0: 612e 2045 7175 6976 616c 656e 7420 746f  a. Equivalent to
-00002ed0: 0a20 2020 2020 2020 2060 6028 6d61 7820  .        ``(max 
-00002ee0: 2d20 6d69 6e29 202f 2028 582e 6d61 7828  - min) / (X.max(
-00002ef0: 6178 6973 3d30 2920 2d20 582e 6d69 6e28  axis=0) - X.min(
-00002f00: 6178 6973 3d30 2929 6060 0a0a 2020 2020  axis=0))``..    
-00002f10: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
-00002f20: 6465 643a 3a20 302e 3137 0a20 2020 2020  ded:: 0.17.     
-00002f30: 2020 2020 2020 2a73 6361 6c65 5f2a 2061        *scale_* a
-00002f40: 7474 7269 6275 7465 2e0a 0a20 2020 2064  ttribute...    d
-00002f50: 6174 615f 6d69 6e5f 203a 206e 6461 7272  ata_min_ : ndarr
-00002f60: 6179 206f 6620 7368 6170 6520 286e 5f66  ay of shape (n_f
-00002f70: 6561 7475 7265 732c 290a 2020 2020 2020  eatures,).      
-00002f80: 2020 5065 7220 6665 6174 7572 6520 6d69    Per feature mi
-00002f90: 6e69 6d75 6d20 7365 656e 2069 6e20 7468  nimum seen in th
-00002fa0: 6520 6461 7461 0a0a 2020 2020 2020 2020  e data..        
-00002fb0: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
-00002fc0: 3a20 302e 3137 0a20 2020 2020 2020 2020  : 0.17.         
-00002fd0: 2020 2a64 6174 615f 6d69 6e5f 2a0a 0a20    *data_min_*.. 
-00002fe0: 2020 2064 6174 615f 6d61 785f 203a 206e     data_max_ : n
-00002ff0: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
-00003000: 286e 5f66 6561 7475 7265 732c 290a 2020  (n_features,).  
-00003010: 2020 2020 2020 5065 7220 6665 6174 7572        Per featur
-00003020: 6520 6d61 7869 6d75 6d20 7365 656e 2069  e maximum seen i
-00003030: 6e20 7468 6520 6461 7461 0a0a 2020 2020  n the data..    
-00003040: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
-00003050: 6465 643a 3a20 302e 3137 0a20 2020 2020  ded:: 0.17.     
-00003060: 2020 2020 2020 2a64 6174 615f 6d61 785f        *data_max_
-00003070: 2a0a 0a20 2020 2064 6174 615f 7261 6e67  *..    data_rang
-00003080: 655f 203a 206e 6461 7272 6179 206f 6620  e_ : ndarray of 
-00003090: 7368 6170 6520 286e 5f66 6561 7475 7265  shape (n_feature
-000030a0: 732c 290a 2020 2020 2020 2020 5065 7220  s,).        Per 
-000030b0: 6665 6174 7572 6520 7261 6e67 6520 6060  feature range ``
-000030c0: 2864 6174 615f 6d61 785f 202d 2064 6174  (data_max_ - dat
-000030d0: 615f 6d69 6e5f 2960 6020 7365 656e 2069  a_min_)`` seen i
-000030e0: 6e20 7468 6520 6461 7461 0a0a 2020 2020  n the data..    
-000030f0: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
-00003100: 6465 643a 3a20 302e 3137 0a20 2020 2020  ded:: 0.17.     
-00003110: 2020 2020 2020 2a64 6174 615f 7261 6e67        *data_rang
-00003120: 655f 2a0a 0a20 2020 206e 5f66 6561 7475  e_*..    n_featu
-00003130: 7265 735f 696e 5f20 3a20 696e 740a 2020  res_in_ : int.  
-00003140: 2020 2020 2020 4e75 6d62 6572 206f 6620        Number of 
-00003150: 6665 6174 7572 6573 2073 6565 6e20 6475  features seen du
-00003160: 7269 6e67 203a 7465 726d 3a60 6669 7460  ring :term:`fit`
-00003170: 2e0a 0a20 2020 2020 2020 202e 2e20 7665  ...        .. ve
-00003180: 7273 696f 6e61 6464 6564 3a3a 2030 2e32  rsionadded:: 0.2
-00003190: 340a 0a20 2020 206e 5f73 616d 706c 6573  4..    n_samples
-000031a0: 5f73 6565 6e5f 203a 2069 6e74 0a20 2020  _seen_ : int.   
-000031b0: 2020 2020 2054 6865 206e 756d 6265 7220       The number 
-000031c0: 6f66 2073 616d 706c 6573 2070 726f 6365  of samples proce
-000031d0: 7373 6564 2062 7920 7468 6520 6573 7469  ssed by the esti
-000031e0: 6d61 746f 722e 0a20 2020 2020 2020 2049  mator..        I
-000031f0: 7420 7769 6c6c 2062 6520 7265 7365 7420  t will be reset 
-00003200: 6f6e 206e 6577 2063 616c 6c73 2074 6f20  on new calls to 
-00003210: 6669 742c 2062 7574 2069 6e63 7265 6d65  fit, but increme
-00003220: 6e74 7320 6163 726f 7373 0a20 2020 2020  nts across.     
-00003230: 2020 2060 6070 6172 7469 616c 5f66 6974     ``partial_fit
-00003240: 6060 2063 616c 6c73 2e0a 0a20 2020 2066  `` calls...    f
-00003250: 6561 7475 7265 5f6e 616d 6573 5f69 6e5f  eature_names_in_
-00003260: 203a 206e 6461 7272 6179 206f 6620 7368   : ndarray of sh
-00003270: 6170 6520 2860 6e5f 6665 6174 7572 6573  ape (`n_features
-00003280: 5f69 6e5f 602c 290a 2020 2020 2020 2020  _in_`,).        
-00003290: 4e61 6d65 7320 6f66 2066 6561 7475 7265  Names of feature
-000032a0: 7320 7365 656e 2064 7572 696e 6720 3a74  s seen during :t
-000032b0: 6572 6d3a 6066 6974 602e 2044 6566 696e  erm:`fit`. Defin
-000032c0: 6564 206f 6e6c 7920 7768 656e 2060 5860  ed only when `X`
-000032d0: 0a20 2020 2020 2020 2068 6173 2066 6561  .        has fea
-000032e0: 7475 7265 206e 616d 6573 2074 6861 7420  ture names that 
-000032f0: 6172 6520 616c 6c20 7374 7269 6e67 732e  are all strings.
-00003300: 0a0a 2020 2020 2020 2020 2e2e 2076 6572  ..        .. ver
-00003310: 7369 6f6e 6164 6465 643a 3a20 312e 300a  sionadded:: 1.0.
-00003320: 0a20 2020 2053 6565 2041 6c73 6f0a 2020  .    See Also.  
-00003330: 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020 206d    --------.    m
-00003340: 696e 6d61 785f 7363 616c 6520 3a20 4571  inmax_scale : Eq
-00003350: 7569 7661 6c65 6e74 2066 756e 6374 696f  uivalent functio
-00003360: 6e20 7769 7468 6f75 7420 7468 6520 6573  n without the es
-00003370: 7469 6d61 746f 7220 4150 492e 0a0a 2020  timator API...  
-00003380: 2020 4e6f 7465 730a 2020 2020 2d2d 2d2d    Notes.    ----
-00003390: 2d0a 2020 2020 4e61 4e73 2061 7265 2074  -.    NaNs are t
-000033a0: 7265 6174 6564 2061 7320 6d69 7373 696e  reated as missin
-000033b0: 6720 7661 6c75 6573 3a20 6469 7372 6567  g values: disreg
-000033c0: 6172 6465 6420 696e 2066 6974 2c20 616e  arded in fit, an
-000033d0: 6420 6d61 696e 7461 696e 6564 2069 6e0a  d maintained in.
-000033e0: 2020 2020 7472 616e 7366 6f72 6d2e 0a0a      transform...
-000033f0: 2020 2020 4578 616d 706c 6573 0a20 2020      Examples.   
-00003400: 202d 2d2d 2d2d 2d2d 2d0a 2020 2020 3e3e   --------.    >>
-00003410: 3e20 6672 6f6d 2073 6b6c 6561 726e 2e70  > from sklearn.p
-00003420: 7265 7072 6f63 6573 7369 6e67 2069 6d70  reprocessing imp
-00003430: 6f72 7420 4d69 6e4d 6178 5363 616c 6572  ort MinMaxScaler
-00003440: 0a20 2020 203e 3e3e 2064 6174 6120 3d20  .    >>> data = 
-00003450: 5b5b 2d31 2c20 325d 2c20 5b2d 302e 352c  [[-1, 2], [-0.5,
-00003460: 2036 5d2c 205b 302c 2031 305d 2c20 5b31   6], [0, 10], [1
-00003470: 2c20 3138 5d5d 0a20 2020 203e 3e3e 2073  , 18]].    >>> s
-00003480: 6361 6c65 7220 3d20 4d69 6e4d 6178 5363  caler = MinMaxSc
-00003490: 616c 6572 2829 0a20 2020 203e 3e3e 2070  aler().    >>> p
-000034a0: 7269 6e74 2873 6361 6c65 722e 6669 7428  rint(scaler.fit(
-000034b0: 6461 7461 2929 0a20 2020 204d 696e 4d61  data)).    MinMa
-000034c0: 7853 6361 6c65 7228 290a 2020 2020 3e3e  xScaler().    >>
-000034d0: 3e20 7072 696e 7428 7363 616c 6572 2e64  > print(scaler.d
-000034e0: 6174 615f 6d61 785f 290a 2020 2020 5b20  ata_max_).    [ 
-000034f0: 312e 2031 382e 5d0a 2020 2020 3e3e 3e20  1. 18.].    >>> 
-00003500: 7072 696e 7428 7363 616c 6572 2e74 7261  print(scaler.tra
-00003510: 6e73 666f 726d 2864 6174 6129 290a 2020  nsform(data)).  
-00003520: 2020 5b5b 302e 2020 2030 2e20 205d 0a20    [[0.   0.  ]. 
-00003530: 2020 2020 5b30 2e32 3520 302e 3235 5d0a      [0.25 0.25].
-00003540: 2020 2020 205b 302e 3520 2030 2e35 205d       [0.5  0.5 ]
-00003550: 0a20 2020 2020 5b31 2e20 2020 312e 2020  .     [1.   1.  
-00003560: 5d5d 0a20 2020 203e 3e3e 2070 7269 6e74  ]].    >>> print
-00003570: 2873 6361 6c65 722e 7472 616e 7366 6f72  (scaler.transfor
-00003580: 6d28 5b5b 322c 2032 5d5d 2929 0a20 2020  m([[2, 2]])).   
-00003590: 205b 5b31 2e35 2030 2e20 5d5d 0a20 2020   [[1.5 0. ]].   
-000035a0: 2022 2222 0a0a 2020 2020 5f70 6172 616d   """..    _param
-000035b0: 6574 6572 5f63 6f6e 7374 7261 696e 7473  eter_constraints
-000035c0: 3a20 6469 6374 203d 207b 0a20 2020 2020  : dict = {.     
-000035d0: 2020 2022 6665 6174 7572 655f 7261 6e67     "feature_rang
-000035e0: 6522 3a20 5b74 7570 6c65 5d2c 0a20 2020  e": [tuple],.   
-000035f0: 2020 2020 2022 636f 7079 223a 205b 2262       "copy": ["b
-00003600: 6f6f 6c65 616e 225d 2c0a 2020 2020 2020  oolean"],.      
-00003610: 2020 2263 6c69 7022 3a20 5b22 626f 6f6c    "clip": ["bool
-00003620: 6561 6e22 5d2c 0a20 2020 207d 0a0a 2020  ean"],.    }..  
-00003630: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
-00003640: 656c 662c 2066 6561 7475 7265 5f72 616e  elf, feature_ran
-00003650: 6765 3d28 302c 2031 292c 202a 2c20 636f  ge=(0, 1), *, co
-00003660: 7079 3d54 7275 652c 2063 6c69 703d 4661  py=True, clip=Fa
-00003670: 6c73 6529 3a0a 2020 2020 2020 2020 7365  lse):.        se
-00003680: 6c66 2e66 6561 7475 7265 5f72 616e 6765  lf.feature_range
-00003690: 203d 2066 6561 7475 7265 5f72 616e 6765   = feature_range
-000036a0: 0a20 2020 2020 2020 2073 656c 662e 636f  .        self.co
-000036b0: 7079 203d 2063 6f70 790a 2020 2020 2020  py = copy.      
-000036c0: 2020 7365 6c66 2e63 6c69 7020 3d20 636c    self.clip = cl
-000036d0: 6970 0a0a 2020 2020 6465 6620 5f72 6573  ip..    def _res
-000036e0: 6574 2873 656c 6629 3a0a 2020 2020 2020  et(self):.      
-000036f0: 2020 2222 2252 6573 6574 2069 6e74 6572    """Reset inter
-00003700: 6e61 6c20 6461 7461 2d64 6570 656e 6465  nal data-depende
-00003710: 6e74 2073 7461 7465 206f 6620 7468 6520  nt state of the 
-00003720: 7363 616c 6572 2c20 6966 206e 6563 6573  scaler, if neces
-00003730: 7361 7279 2e0a 0a20 2020 2020 2020 205f  sary...        _
-00003740: 5f69 6e69 745f 5f20 7061 7261 6d65 7465  _init__ paramete
-00003750: 7273 2061 7265 206e 6f74 2074 6f75 6368  rs are not touch
-00003760: 6564 2e0a 2020 2020 2020 2020 2222 220a  ed..        """.
-00003770: 2020 2020 2020 2020 2320 4368 6563 6b69          # Checki
-00003780: 6e67 206f 6e65 2061 7474 7269 6275 7465  ng one attribute
-00003790: 2069 7320 656e 6f75 6768 2c20 6265 6361   is enough, beca
-000037a0: 7573 6520 7468 6579 2061 7265 2061 6c6c  use they are all
-000037b0: 2073 6574 2074 6f67 6574 6865 720a 2020   set together.  
-000037c0: 2020 2020 2020 2320 696e 2070 6172 7469        # in parti
-000037d0: 616c 5f66 6974 0a20 2020 2020 2020 2069  al_fit.        i
-000037e0: 6620 6861 7361 7474 7228 7365 6c66 2c20  f hasattr(self, 
-000037f0: 2273 6361 6c65 5f22 293a 0a20 2020 2020  "scale_"):.     
-00003800: 2020 2020 2020 2064 656c 2073 656c 662e         del self.
-00003810: 7363 616c 655f 0a20 2020 2020 2020 2020  scale_.         
-00003820: 2020 2064 656c 2073 656c 662e 6d69 6e5f     del self.min_
-00003830: 0a20 2020 2020 2020 2020 2020 2064 656c  .            del
-00003840: 2073 656c 662e 6e5f 7361 6d70 6c65 735f   self.n_samples_
-00003850: 7365 656e 5f0a 2020 2020 2020 2020 2020  seen_.          
-00003860: 2020 6465 6c20 7365 6c66 2e64 6174 615f    del self.data_
-00003870: 6d69 6e5f 0a20 2020 2020 2020 2020 2020  min_.           
-00003880: 2064 656c 2073 656c 662e 6461 7461 5f6d   del self.data_m
-00003890: 6178 5f0a 2020 2020 2020 2020 2020 2020  ax_.            
-000038a0: 6465 6c20 7365 6c66 2e64 6174 615f 7261  del self.data_ra
-000038b0: 6e67 655f 0a0a 2020 2020 6465 6620 6669  nge_..    def fi
-000038c0: 7428 7365 6c66 2c20 582c 2079 3d4e 6f6e  t(self, X, y=Non
-000038d0: 6529 3a0a 2020 2020 2020 2020 2222 2243  e):.        """C
-000038e0: 6f6d 7075 7465 2074 6865 206d 696e 696d  ompute the minim
-000038f0: 756d 2061 6e64 206d 6178 696d 756d 2074  um and maximum t
-00003900: 6f20 6265 2075 7365 6420 666f 7220 6c61  o be used for la
-00003910: 7465 7220 7363 616c 696e 672e 0a0a 2020  ter scaling...  
-00003920: 2020 2020 2020 5061 7261 6d65 7465 7273        Parameters
-00003930: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
-00003940: 2d2d 2d0a 2020 2020 2020 2020 5820 3a20  ---.        X : 
-00003950: 6172 7261 792d 6c69 6b65 206f 6620 7368  array-like of sh
-00003960: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
-00003970: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
-00003980: 2020 2020 2020 2020 5468 6520 6461 7461          The data
-00003990: 2075 7365 6420 746f 2063 6f6d 7075 7465   used to compute
-000039a0: 2074 6865 2070 6572 2d66 6561 7475 7265   the per-feature
-000039b0: 206d 696e 696d 756d 2061 6e64 206d 6178   minimum and max
-000039c0: 696d 756d 0a20 2020 2020 2020 2020 2020  imum.           
-000039d0: 2075 7365 6420 666f 7220 6c61 7465 7220   used for later 
-000039e0: 7363 616c 696e 6720 616c 6f6e 6720 7468  scaling along th
-000039f0: 6520 6665 6174 7572 6573 2061 7869 732e  e features axis.
-00003a00: 0a0a 2020 2020 2020 2020 7920 3a20 4e6f  ..        y : No
-00003a10: 6e65 0a20 2020 2020 2020 2020 2020 2049  ne.            I
-00003a20: 676e 6f72 6564 2e0a 0a20 2020 2020 2020  gnored...       
-00003a30: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
-00003a40: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
-00003a50: 2073 656c 6620 3a20 6f62 6a65 6374 0a20   self : object. 
-00003a60: 2020 2020 2020 2020 2020 2046 6974 7465             Fitte
-00003a70: 6420 7363 616c 6572 2e0a 2020 2020 2020  d scaler..      
-00003a80: 2020 2222 220a 2020 2020 2020 2020 2320    """.        # 
-00003a90: 5265 7365 7420 696e 7465 726e 616c 2073  Reset internal s
-00003aa0: 7461 7465 2062 6566 6f72 6520 6669 7474  tate before fitt
-00003ab0: 696e 670a 2020 2020 2020 2020 7365 6c66  ing.        self
-00003ac0: 2e5f 7265 7365 7428 290a 2020 2020 2020  ._reset().      
-00003ad0: 2020 7265 7475 726e 2073 656c 662e 7061    return self.pa
-00003ae0: 7274 6961 6c5f 6669 7428 582c 2079 290a  rtial_fit(X, y).
-00003af0: 0a20 2020 2040 5f66 6974 5f63 6f6e 7465  .    @_fit_conte
-00003b00: 7874 2870 7265 6665 725f 736b 6970 5f6e  xt(prefer_skip_n
-00003b10: 6573 7465 645f 7661 6c69 6461 7469 6f6e  ested_validation
-00003b20: 3d54 7275 6529 0a20 2020 2064 6566 2070  =True).    def p
-00003b30: 6172 7469 616c 5f66 6974 2873 656c 662c  artial_fit(self,
-00003b40: 2058 2c20 793d 4e6f 6e65 293a 0a20 2020   X, y=None):.   
-00003b50: 2020 2020 2022 2222 4f6e 6c69 6e65 2063       """Online c
-00003b60: 6f6d 7075 7461 7469 6f6e 206f 6620 6d69  omputation of mi
-00003b70: 6e20 616e 6420 6d61 7820 6f6e 2058 2066  n and max on X f
-00003b80: 6f72 206c 6174 6572 2073 6361 6c69 6e67  or later scaling
-00003b90: 2e0a 0a20 2020 2020 2020 2041 6c6c 206f  ...        All o
-00003ba0: 6620 5820 6973 2070 726f 6365 7373 6564  f X is processed
-00003bb0: 2061 7320 6120 7369 6e67 6c65 2062 6174   as a single bat
-00003bc0: 6368 2e20 5468 6973 2069 7320 696e 7465  ch. This is inte
-00003bd0: 6e64 6564 2066 6f72 2063 6173 6573 0a20  nded for cases. 
-00003be0: 2020 2020 2020 2077 6865 6e20 3a6d 6574         when :met
-00003bf0: 683a 6066 6974 6020 6973 206e 6f74 2066  h:`fit` is not f
-00003c00: 6561 7369 626c 6520 6475 6520 746f 2076  easible due to v
-00003c10: 6572 7920 6c61 7267 6520 6e75 6d62 6572  ery large number
-00003c20: 206f 660a 2020 2020 2020 2020 606e 5f73   of.        `n_s
-00003c30: 616d 706c 6573 6020 6f72 2062 6563 6175  amples` or becau
-00003c40: 7365 2058 2069 7320 7265 6164 2066 726f  se X is read fro
-00003c50: 6d20 6120 636f 6e74 696e 756f 7573 2073  m a continuous s
-00003c60: 7472 6561 6d2e 0a0a 2020 2020 2020 2020  tream...        
-00003c70: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
-00003c80: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
-00003c90: 2020 2020 2020 5820 3a20 6172 7261 792d        X : array-
-00003ca0: 6c69 6b65 206f 6620 7368 6170 6520 286e  like of shape (n
-00003cb0: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
-00003cc0: 7572 6573 290a 2020 2020 2020 2020 2020  ures).          
-00003cd0: 2020 5468 6520 6461 7461 2075 7365 6420    The data used 
-00003ce0: 746f 2063 6f6d 7075 7465 2074 6865 206d  to compute the m
-00003cf0: 6561 6e20 616e 6420 7374 616e 6461 7264  ean and standard
-00003d00: 2064 6576 6961 7469 6f6e 0a20 2020 2020   deviation.     
-00003d10: 2020 2020 2020 2075 7365 6420 666f 7220         used for 
-00003d20: 6c61 7465 7220 7363 616c 696e 6720 616c  later scaling al
-00003d30: 6f6e 6720 7468 6520 6665 6174 7572 6573  ong the features
-00003d40: 2061 7869 732e 0a0a 2020 2020 2020 2020   axis...        
-00003d50: 7920 3a20 4e6f 6e65 0a20 2020 2020 2020  y : None.       
-00003d60: 2020 2020 2049 676e 6f72 6564 2e0a 0a20       Ignored... 
-00003d70: 2020 2020 2020 2052 6574 7572 6e73 0a20         Returns. 
-00003d80: 2020 2020 2020 202d 2d2d 2d2d 2d2d 0a20         -------. 
-00003d90: 2020 2020 2020 2073 656c 6620 3a20 6f62         self : ob
-00003da0: 6a65 6374 0a20 2020 2020 2020 2020 2020  ject.           
-00003db0: 2046 6974 7465 6420 7363 616c 6572 2e0a   Fitted scaler..
-00003dc0: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
-00003dd0: 2020 2020 6665 6174 7572 655f 7261 6e67      feature_rang
-00003de0: 6520 3d20 7365 6c66 2e66 6561 7475 7265  e = self.feature
-00003df0: 5f72 616e 6765 0a20 2020 2020 2020 2069  _range.        i
-00003e00: 6620 6665 6174 7572 655f 7261 6e67 655b  f feature_range[
-00003e10: 305d 203e 3d20 6665 6174 7572 655f 7261  0] >= feature_ra
-00003e20: 6e67 655b 315d 3a0a 2020 2020 2020 2020  nge[1]:.        
-00003e30: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00003e40: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
-00003e50: 2020 2020 2020 224d 696e 696d 756d 206f        "Minimum o
-00003e60: 6620 6465 7369 7265 6420 6665 6174 7572  f desired featur
-00003e70: 6520 7261 6e67 6520 6d75 7374 2062 6520  e range must be 
-00003e80: 736d 616c 6c65 7220 7468 616e 206d 6178  smaller than max
-00003e90: 696d 756d 2e20 476f 7420 2573 2e22 0a20  imum. Got %s.". 
-00003ea0: 2020 2020 2020 2020 2020 2020 2020 2025                 %
-00003eb0: 2073 7472 2866 6561 7475 7265 5f72 616e   str(feature_ran
-00003ec0: 6765 290a 2020 2020 2020 2020 2020 2020  ge).            
-00003ed0: 290a 0a20 2020 2020 2020 2069 6620 7370  )..        if sp
-00003ee0: 6172 7365 2e69 7373 7061 7273 6528 5829  arse.issparse(X)
-00003ef0: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-00003f00: 6973 6520 5479 7065 4572 726f 7228 0a20  ise TypeError(. 
-00003f10: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00003f20: 4d69 6e4d 6178 5363 616c 6572 2064 6f65  MinMaxScaler doe
-00003f30: 7320 6e6f 7420 7375 7070 6f72 7420 7370  s not support sp
-00003f40: 6172 7365 2069 6e70 7574 2e20 220a 2020  arse input. ".  
-00003f50: 2020 2020 2020 2020 2020 2020 2020 2243                "C
-00003f60: 6f6e 7369 6465 7220 7573 696e 6720 4d61  onsider using Ma
-00003f70: 7841 6273 5363 616c 6572 2069 6e73 7465  xAbsScaler inste
-00003f80: 6164 2e22 0a20 2020 2020 2020 2020 2020  ad.".           
-00003f90: 2029 0a0a 2020 2020 2020 2020 7870 2c20   )..        xp, 
-00003fa0: 5f20 3d20 6765 745f 6e61 6d65 7370 6163  _ = get_namespac
-00003fb0: 6528 5829 0a0a 2020 2020 2020 2020 6669  e(X)..        fi
-00003fc0: 7273 745f 7061 7373 203d 206e 6f74 2068  rst_pass = not h
-00003fd0: 6173 6174 7472 2873 656c 662c 2022 6e5f  asattr(self, "n_
-00003fe0: 7361 6d70 6c65 735f 7365 656e 5f22 290a  samples_seen_").
-00003ff0: 2020 2020 2020 2020 5820 3d20 7365 6c66          X = self
-00004000: 2e5f 7661 6c69 6461 7465 5f64 6174 6128  ._validate_data(
-00004010: 0a20 2020 2020 2020 2020 2020 2058 2c0a  .            X,.
-00004020: 2020 2020 2020 2020 2020 2020 7265 7365              rese
-00004030: 743d 6669 7273 745f 7061 7373 2c0a 2020  t=first_pass,.  
-00004040: 2020 2020 2020 2020 2020 6474 7970 653d            dtype=
-00004050: 5f61 7272 6179 5f61 7069 2e73 7570 706f  _array_api.suppo
-00004060: 7274 6564 5f66 6c6f 6174 5f64 7479 7065  rted_float_dtype
-00004070: 7328 7870 292c 0a20 2020 2020 2020 2020  s(xp),.         
-00004080: 2020 2066 6f72 6365 5f61 6c6c 5f66 696e     force_all_fin
-00004090: 6974 653d 2261 6c6c 6f77 2d6e 616e 222c  ite="allow-nan",
-000040a0: 0a20 2020 2020 2020 2029 0a0a 2020 2020  .        )..    
-000040b0: 2020 2020 6461 7461 5f6d 696e 203d 205f      data_min = _
-000040c0: 6172 7261 795f 6170 692e 5f6e 616e 6d69  array_api._nanmi
-000040d0: 6e28 582c 2061 7869 733d 3029 0a20 2020  n(X, axis=0).   
-000040e0: 2020 2020 2064 6174 615f 6d61 7820 3d20       data_max = 
-000040f0: 5f61 7272 6179 5f61 7069 2e5f 6e61 6e6d  _array_api._nanm
-00004100: 6178 2858 2c20 6178 6973 3d30 290a 0a20  ax(X, axis=0).. 
-00004110: 2020 2020 2020 2069 6620 6669 7273 745f         if first_
-00004120: 7061 7373 3a0a 2020 2020 2020 2020 2020  pass:.          
-00004130: 2020 7365 6c66 2e6e 5f73 616d 706c 6573    self.n_samples
-00004140: 5f73 6565 6e5f 203d 2058 2e73 6861 7065  _seen_ = X.shape
-00004150: 5b30 5d0a 2020 2020 2020 2020 656c 7365  [0].        else
-00004160: 3a0a 2020 2020 2020 2020 2020 2020 6461  :.            da
-00004170: 7461 5f6d 696e 203d 2078 702e 6d69 6e69  ta_min = xp.mini
-00004180: 6d75 6d28 7365 6c66 2e64 6174 615f 6d69  mum(self.data_mi
-00004190: 6e5f 2c20 6461 7461 5f6d 696e 290a 2020  n_, data_min).  
-000041a0: 2020 2020 2020 2020 2020 6461 7461 5f6d            data_m
-000041b0: 6178 203d 2078 702e 6d61 7869 6d75 6d28  ax = xp.maximum(
-000041c0: 7365 6c66 2e64 6174 615f 6d61 785f 2c20  self.data_max_, 
-000041d0: 6461 7461 5f6d 6178 290a 2020 2020 2020  data_max).      
-000041e0: 2020 2020 2020 7365 6c66 2e6e 5f73 616d        self.n_sam
-000041f0: 706c 6573 5f73 6565 6e5f 202b 3d20 582e  ples_seen_ += X.
-00004200: 7368 6170 655b 305d 0a0a 2020 2020 2020  shape[0]..      
-00004210: 2020 6461 7461 5f72 616e 6765 203d 2064    data_range = d
-00004220: 6174 615f 6d61 7820 2d20 6461 7461 5f6d  ata_max - data_m
-00004230: 696e 0a20 2020 2020 2020 2073 656c 662e  in.        self.
-00004240: 7363 616c 655f 203d 2028 6665 6174 7572  scale_ = (featur
-00004250: 655f 7261 6e67 655b 315d 202d 2066 6561  e_range[1] - fea
-00004260: 7475 7265 5f72 616e 6765 5b30 5d29 202f  ture_range[0]) /
-00004270: 205f 6861 6e64 6c65 5f7a 6572 6f73 5f69   _handle_zeros_i
-00004280: 6e5f 7363 616c 6528 0a20 2020 2020 2020  n_scale(.       
-00004290: 2020 2020 2064 6174 615f 7261 6e67 652c       data_range,
-000042a0: 2063 6f70 793d 5472 7565 0a20 2020 2020   copy=True.     
-000042b0: 2020 2029 0a20 2020 2020 2020 2073 656c     ).        sel
-000042c0: 662e 6d69 6e5f 203d 2066 6561 7475 7265  f.min_ = feature
-000042d0: 5f72 616e 6765 5b30 5d20 2d20 6461 7461  _range[0] - data
-000042e0: 5f6d 696e 202a 2073 656c 662e 7363 616c  _min * self.scal
-000042f0: 655f 0a20 2020 2020 2020 2073 656c 662e  e_.        self.
-00004300: 6461 7461 5f6d 696e 5f20 3d20 6461 7461  data_min_ = data
-00004310: 5f6d 696e 0a20 2020 2020 2020 2073 656c  _min.        sel
-00004320: 662e 6461 7461 5f6d 6178 5f20 3d20 6461  f.data_max_ = da
-00004330: 7461 5f6d 6178 0a20 2020 2020 2020 2073  ta_max.        s
-00004340: 656c 662e 6461 7461 5f72 616e 6765 5f20  elf.data_range_ 
-00004350: 3d20 6461 7461 5f72 616e 6765 0a20 2020  = data_range.   
-00004360: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-00004370: 0a0a 2020 2020 6465 6620 7472 616e 7366  ..    def transf
-00004380: 6f72 6d28 7365 6c66 2c20 5829 3a0a 2020  orm(self, X):.  
-00004390: 2020 2020 2020 2222 2253 6361 6c65 2066        """Scale f
-000043a0: 6561 7475 7265 7320 6f66 2058 2061 6363  eatures of X acc
-000043b0: 6f72 6469 6e67 2074 6f20 6665 6174 7572  ording to featur
-000043c0: 655f 7261 6e67 652e 0a0a 2020 2020 2020  e_range...      
-000043d0: 2020 5061 7261 6d65 7465 7273 0a20 2020    Parameters.   
-000043e0: 2020 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a       ----------.
-000043f0: 2020 2020 2020 2020 5820 3a20 6172 7261          X : arra
-00004400: 792d 6c69 6b65 206f 6620 7368 6170 6520  y-like of shape 
-00004410: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
-00004420: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-00004430: 2020 2020 496e 7075 7420 6461 7461 2074      Input data t
-00004440: 6861 7420 7769 6c6c 2062 6520 7472 616e  hat will be tran
-00004450: 7366 6f72 6d65 642e 0a0a 2020 2020 2020  sformed...      
-00004460: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
-00004470: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
-00004480: 2020 5874 203a 206e 6461 7272 6179 206f    Xt : ndarray o
-00004490: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
-000044a0: 6573 2c20 6e5f 6665 6174 7572 6573 290a  es, n_features).
-000044b0: 2020 2020 2020 2020 2020 2020 5472 616e              Tran
-000044c0: 7366 6f72 6d65 6420 6461 7461 2e0a 2020  sformed data..  
-000044d0: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-000044e0: 2020 6368 6563 6b5f 6973 5f66 6974 7465    check_is_fitte
-000044f0: 6428 7365 6c66 290a 0a20 2020 2020 2020  d(self)..       
-00004500: 2078 702c 205f 203d 2067 6574 5f6e 616d   xp, _ = get_nam
-00004510: 6573 7061 6365 2858 290a 0a20 2020 2020  espace(X)..     
-00004520: 2020 2058 203d 2073 656c 662e 5f76 616c     X = self._val
-00004530: 6964 6174 655f 6461 7461 280a 2020 2020  idate_data(.    
-00004540: 2020 2020 2020 2020 582c 0a20 2020 2020          X,.     
-00004550: 2020 2020 2020 2063 6f70 793d 7365 6c66         copy=self
-00004560: 2e63 6f70 792c 0a20 2020 2020 2020 2020  .copy,.         
-00004570: 2020 2064 7479 7065 3d5f 6172 7261 795f     dtype=_array_
-00004580: 6170 692e 7375 7070 6f72 7465 645f 666c  api.supported_fl
-00004590: 6f61 745f 6474 7970 6573 2878 7029 2c0a  oat_dtypes(xp),.
-000045a0: 2020 2020 2020 2020 2020 2020 666f 7263              forc
-000045b0: 655f 616c 6c5f 6669 6e69 7465 3d22 616c  e_all_finite="al
-000045c0: 6c6f 772d 6e61 6e22 2c0a 2020 2020 2020  low-nan",.      
-000045d0: 2020 2020 2020 7265 7365 743d 4661 6c73        reset=Fals
-000045e0: 652c 0a20 2020 2020 2020 2029 0a0a 2020  e,.        )..  
-000045f0: 2020 2020 2020 5820 2a3d 2073 656c 662e        X *= self.
-00004600: 7363 616c 655f 0a20 2020 2020 2020 2058  scale_.        X
-00004610: 202b 3d20 7365 6c66 2e6d 696e 5f0a 2020   += self.min_.  
-00004620: 2020 2020 2020 6966 2073 656c 662e 636c        if self.cl
-00004630: 6970 3a0a 2020 2020 2020 2020 2020 2020  ip:.            
-00004640: 7870 2e63 6c69 7028 582c 2073 656c 662e  xp.clip(X, self.
-00004650: 6665 6174 7572 655f 7261 6e67 655b 305d  feature_range[0]
-00004660: 2c20 7365 6c66 2e66 6561 7475 7265 5f72  , self.feature_r
-00004670: 616e 6765 5b31 5d2c 206f 7574 3d58 290a  ange[1], out=X).
-00004680: 2020 2020 2020 2020 7265 7475 726e 2058          return X
-00004690: 0a0a 2020 2020 6465 6620 696e 7665 7273  ..    def invers
-000046a0: 655f 7472 616e 7366 6f72 6d28 7365 6c66  e_transform(self
-000046b0: 2c20 5829 3a0a 2020 2020 2020 2020 2222  , X):.        ""
-000046c0: 2255 6e64 6f20 7468 6520 7363 616c 696e  "Undo the scalin
-000046d0: 6720 6f66 2058 2061 6363 6f72 6469 6e67  g of X according
-000046e0: 2074 6f20 6665 6174 7572 655f 7261 6e67   to feature_rang
-000046f0: 652e 0a0a 2020 2020 2020 2020 5061 7261  e...        Para
-00004700: 6d65 7465 7273 0a20 2020 2020 2020 202d  meters.        -
-00004710: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 2020  ---------.      
-00004720: 2020 5820 3a20 6172 7261 792d 6c69 6b65    X : array-like
-00004730: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-00004740: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-00004750: 290a 2020 2020 2020 2020 2020 2020 496e  ).            In
-00004760: 7075 7420 6461 7461 2074 6861 7420 7769  put data that wi
-00004770: 6c6c 2062 6520 7472 616e 7366 6f72 6d65  ll be transforme
-00004780: 642e 2049 7420 6361 6e6e 6f74 2062 6520  d. It cannot be 
-00004790: 7370 6172 7365 2e0a 0a20 2020 2020 2020  sparse...       
-000047a0: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
-000047b0: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
-000047c0: 2058 7420 3a20 6e64 6172 7261 7920 6f66   Xt : ndarray of
-000047d0: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
-000047e0: 732c 206e 5f66 6561 7475 7265 7329 0a20  s, n_features). 
-000047f0: 2020 2020 2020 2020 2020 2054 7261 6e73             Trans
-00004800: 666f 726d 6564 2064 6174 612e 0a20 2020  formed data..   
-00004810: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
-00004820: 2063 6865 636b 5f69 735f 6669 7474 6564   check_is_fitted
-00004830: 2873 656c 6629 0a0a 2020 2020 2020 2020  (self)..        
-00004840: 7870 2c20 5f20 3d20 6765 745f 6e61 6d65  xp, _ = get_name
-00004850: 7370 6163 6528 5829 0a0a 2020 2020 2020  space(X)..      
-00004860: 2020 5820 3d20 6368 6563 6b5f 6172 7261    X = check_arra
-00004870: 7928 0a20 2020 2020 2020 2020 2020 2058  y(.            X
-00004880: 2c0a 2020 2020 2020 2020 2020 2020 636f  ,.            co
-00004890: 7079 3d73 656c 662e 636f 7079 2c0a 2020  py=self.copy,.  
-000048a0: 2020 2020 2020 2020 2020 6474 7970 653d            dtype=
-000048b0: 5f61 7272 6179 5f61 7069 2e73 7570 706f  _array_api.suppo
-000048c0: 7274 6564 5f66 6c6f 6174 5f64 7479 7065  rted_float_dtype
-000048d0: 7328 7870 292c 0a20 2020 2020 2020 2020  s(xp),.         
-000048e0: 2020 2066 6f72 6365 5f61 6c6c 5f66 696e     force_all_fin
-000048f0: 6974 653d 2261 6c6c 6f77 2d6e 616e 222c  ite="allow-nan",
-00004900: 0a20 2020 2020 2020 2029 0a0a 2020 2020  .        )..    
-00004910: 2020 2020 5820 2d3d 2073 656c 662e 6d69      X -= self.mi
-00004920: 6e5f 0a20 2020 2020 2020 2058 202f 3d20  n_.        X /= 
-00004930: 7365 6c66 2e73 6361 6c65 5f0a 2020 2020  self.scale_.    
-00004940: 2020 2020 7265 7475 726e 2058 0a0a 2020      return X..  
-00004950: 2020 6465 6620 5f6d 6f72 655f 7461 6773    def _more_tags
-00004960: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
-00004970: 7265 7475 726e 207b 2261 6c6c 6f77 5f6e  return {"allow_n
-00004980: 616e 223a 2054 7275 657d 0a0a 0a40 7661  an": True}...@va
-00004990: 6c69 6461 7465 5f70 6172 616d 7328 0a20  lidate_params(. 
-000049a0: 2020 207b 0a20 2020 2020 2020 2022 5822     {.        "X"
-000049b0: 3a20 5b22 6172 7261 792d 6c69 6b65 225d  : ["array-like"]
-000049c0: 2c0a 2020 2020 2020 2020 2261 7869 7322  ,.        "axis"
-000049d0: 3a20 5b4f 7074 696f 6e73 2849 6e74 6567  : [Options(Integ
-000049e0: 7261 6c2c 207b 302c 2031 7d29 5d2c 0a20  ral, {0, 1})],. 
-000049f0: 2020 207d 2c0a 2020 2020 7072 6566 6572     },.    prefer
-00004a00: 5f73 6b69 705f 6e65 7374 6564 5f76 616c  _skip_nested_val
-00004a10: 6964 6174 696f 6e3d 4661 6c73 652c 0a29  idation=False,.)
-00004a20: 0a64 6566 206d 696e 6d61 785f 7363 616c  .def minmax_scal
-00004a30: 6528 582c 2066 6561 7475 7265 5f72 616e  e(X, feature_ran
-00004a40: 6765 3d28 302c 2031 292c 202a 2c20 6178  ge=(0, 1), *, ax
-00004a50: 6973 3d30 2c20 636f 7079 3d54 7275 6529  is=0, copy=True)
-00004a60: 3a0a 2020 2020 2222 2254 7261 6e73 666f  :.    """Transfo
-00004a70: 726d 2066 6561 7475 7265 7320 6279 2073  rm features by s
-00004a80: 6361 6c69 6e67 2065 6163 6820 6665 6174  caling each feat
-00004a90: 7572 6520 746f 2061 2067 6976 656e 2072  ure to a given r
-00004aa0: 616e 6765 2e0a 0a20 2020 2054 6869 7320  ange...    This 
-00004ab0: 6573 7469 6d61 746f 7220 7363 616c 6573  estimator scales
-00004ac0: 2061 6e64 2074 7261 6e73 6c61 7465 7320   and translates 
-00004ad0: 6561 6368 2066 6561 7475 7265 2069 6e64  each feature ind
-00004ae0: 6976 6964 7561 6c6c 7920 7375 6368 0a20  ividually such. 
-00004af0: 2020 2074 6861 7420 6974 2069 7320 696e     that it is in
-00004b00: 2074 6865 2067 6976 656e 2072 616e 6765   the given range
-00004b10: 206f 6e20 7468 6520 7472 6169 6e69 6e67   on the training
-00004b20: 2073 6574 2c20 692e 652e 2062 6574 7765   set, i.e. betwe
-00004b30: 656e 0a20 2020 207a 6572 6f20 616e 6420  en.    zero and 
-00004b40: 6f6e 652e 0a0a 2020 2020 5468 6520 7472  one...    The tr
-00004b50: 616e 7366 6f72 6d61 7469 6f6e 2069 7320  ansformation is 
-00004b60: 6769 7665 6e20 6279 2028 7768 656e 2060  given by (when `
-00004b70: 6061 7869 733d 3060 6029 3a3a 0a0a 2020  `axis=0``)::..  
-00004b80: 2020 2020 2020 585f 7374 6420 3d20 2858        X_std = (X
-00004b90: 202d 2058 2e6d 696e 2861 7869 733d 3029   - X.min(axis=0)
-00004ba0: 2920 2f20 2858 2e6d 6178 2861 7869 733d  ) / (X.max(axis=
-00004bb0: 3029 202d 2058 2e6d 696e 2861 7869 733d  0) - X.min(axis=
-00004bc0: 3029 290a 2020 2020 2020 2020 585f 7363  0)).        X_sc
-00004bd0: 616c 6564 203d 2058 5f73 7464 202a 2028  aled = X_std * (
-00004be0: 6d61 7820 2d20 6d69 6e29 202b 206d 696e  max - min) + min
-00004bf0: 0a0a 2020 2020 7768 6572 6520 6d69 6e2c  ..    where min,
-00004c00: 206d 6178 203d 2066 6561 7475 7265 5f72   max = feature_r
-00004c10: 616e 6765 2e0a 0a20 2020 2054 6865 2074  ange...    The t
-00004c20: 7261 6e73 666f 726d 6174 696f 6e20 6973  ransformation is
-00004c30: 2063 616c 6375 6c61 7465 6420 6173 2028   calculated as (
-00004c40: 7768 656e 2060 6061 7869 733d 3060 6029  when ``axis=0``)
-00004c50: 3a3a 0a0a 2020 2020 2020 2058 5f73 6361  ::..       X_sca
-00004c60: 6c65 6420 3d20 7363 616c 6520 2a20 5820  led = scale * X 
-00004c70: 2b20 6d69 6e20 2d20 582e 6d69 6e28 6178  + min - X.min(ax
-00004c80: 6973 3d30 2920 2a20 7363 616c 650a 2020  is=0) * scale.  
-00004c90: 2020 2020 2077 6865 7265 2073 6361 6c65       where scale
-00004ca0: 203d 2028 6d61 7820 2d20 6d69 6e29 202f   = (max - min) /
-00004cb0: 2028 582e 6d61 7828 6178 6973 3d30 2920   (X.max(axis=0) 
-00004cc0: 2d20 582e 6d69 6e28 6178 6973 3d30 2929  - X.min(axis=0))
-00004cd0: 0a0a 2020 2020 5468 6973 2074 7261 6e73  ..    This trans
-00004ce0: 666f 726d 6174 696f 6e20 6973 206f 6674  formation is oft
-00004cf0: 656e 2075 7365 6420 6173 2061 6e20 616c  en used as an al
-00004d00: 7465 726e 6174 6976 6520 746f 207a 6572  ternative to zer
-00004d10: 6f20 6d65 616e 2c0a 2020 2020 756e 6974  o mean,.    unit
-00004d20: 2076 6172 6961 6e63 6520 7363 616c 696e   variance scalin
-00004d30: 672e 0a0a 2020 2020 5265 6164 206d 6f72  g...    Read mor
-00004d40: 6520 696e 2074 6865 203a 7265 663a 6055  e in the :ref:`U
-00004d50: 7365 7220 4775 6964 6520 3c70 7265 7072  ser Guide <prepr
-00004d60: 6f63 6573 7369 6e67 5f73 6361 6c65 723e  ocessing_scaler>
-00004d70: 602e 0a0a 2020 2020 2e2e 2076 6572 7369  `...    .. versi
-00004d80: 6f6e 6164 6465 643a 3a20 302e 3137 0a20  onadded:: 0.17. 
-00004d90: 2020 2020 2020 2a6d 696e 6d61 785f 7363        *minmax_sc
-00004da0: 616c 652a 2066 756e 6374 696f 6e20 696e  ale* function in
-00004db0: 7465 7266 6163 650a 2020 2020 2020 2074  terface.       t
-00004dc0: 6f20 3a63 6c61 7373 3a60 7e73 6b6c 6561  o :class:`~sklea
-00004dd0: 726e 2e70 7265 7072 6f63 6573 7369 6e67  rn.preprocessing
-00004de0: 2e4d 696e 4d61 7853 6361 6c65 7260 2e0a  .MinMaxScaler`..
-00004df0: 0a20 2020 2050 6172 616d 6574 6572 730a  .    Parameters.
-00004e00: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
-00004e10: 2020 2058 203a 2061 7272 6179 2d6c 696b     X : array-lik
-00004e20: 6520 6f66 2073 6861 7065 2028 6e5f 7361  e of shape (n_sa
-00004e30: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
-00004e40: 7329 0a20 2020 2020 2020 2054 6865 2064  s).        The d
-00004e50: 6174 612e 0a0a 2020 2020 6665 6174 7572  ata...    featur
-00004e60: 655f 7261 6e67 6520 3a20 7475 706c 6520  e_range : tuple 
-00004e70: 286d 696e 2c20 6d61 7829 2c20 6465 6661  (min, max), defa
-00004e80: 756c 743d 2830 2c20 3129 0a20 2020 2020  ult=(0, 1).     
-00004e90: 2020 2044 6573 6972 6564 2072 616e 6765     Desired range
-00004ea0: 206f 6620 7472 616e 7366 6f72 6d65 6420   of transformed 
-00004eb0: 6461 7461 2e0a 0a20 2020 2061 7869 7320  data...    axis 
-00004ec0: 3a20 7b30 2c20 317d 2c20 6465 6661 756c  : {0, 1}, defaul
-00004ed0: 743d 300a 2020 2020 2020 2020 4178 6973  t=0.        Axis
-00004ee0: 2075 7365 6420 746f 2073 6361 6c65 2061   used to scale a
-00004ef0: 6c6f 6e67 2e20 4966 2030 2c20 696e 6465  long. If 0, inde
-00004f00: 7065 6e64 656e 746c 7920 7363 616c 6520  pendently scale 
-00004f10: 6561 6368 2066 6561 7475 7265 2c0a 2020  each feature,.  
-00004f20: 2020 2020 2020 6f74 6865 7277 6973 6520        otherwise 
-00004f30: 2869 6620 3129 2073 6361 6c65 2065 6163  (if 1) scale eac
-00004f40: 6820 7361 6d70 6c65 2e0a 0a20 2020 2063  h sample...    c
-00004f50: 6f70 7920 3a20 626f 6f6c 2c20 6465 6661  opy : bool, defa
-00004f60: 756c 743d 5472 7565 0a20 2020 2020 2020  ult=True.       
-00004f70: 2049 6620 4661 6c73 652c 2074 7279 2074   If False, try t
-00004f80: 6f20 6176 6f69 6420 6120 636f 7079 2061  o avoid a copy a
-00004f90: 6e64 2073 6361 6c65 2069 6e20 706c 6163  nd scale in plac
-00004fa0: 652e 0a20 2020 2020 2020 2054 6869 7320  e..        This 
-00004fb0: 6973 206e 6f74 2067 7561 7261 6e74 6565  is not guarantee
-00004fc0: 6420 746f 2061 6c77 6179 7320 776f 726b  d to always work
-00004fd0: 2069 6e20 706c 6163 653b 2065 2e67 2e20   in place; e.g. 
-00004fe0: 6966 2074 6865 2064 6174 6120 6973 0a20  if the data is. 
-00004ff0: 2020 2020 2020 2061 206e 756d 7079 2061         a numpy a
-00005000: 7272 6179 2077 6974 6820 616e 2069 6e74  rray with an int
-00005010: 2064 7479 7065 2c20 6120 636f 7079 2077   dtype, a copy w
-00005020: 696c 6c20 6265 2072 6574 7572 6e65 6420  ill be returned 
-00005030: 6576 656e 2077 6974 680a 2020 2020 2020  even with.      
-00005040: 2020 636f 7079 3d46 616c 7365 2e0a 0a20    copy=False... 
-00005050: 2020 2052 6574 7572 6e73 0a20 2020 202d     Returns.    -
-00005060: 2d2d 2d2d 2d2d 0a20 2020 2058 5f74 7220  ------.    X_tr 
-00005070: 3a20 6e64 6172 7261 7920 6f66 2073 6861  : ndarray of sha
-00005080: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
-00005090: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
-000050a0: 2020 2054 6865 2074 7261 6e73 666f 726d     The transform
-000050b0: 6564 2064 6174 612e 0a0a 2020 2020 2e2e  ed data...    ..
-000050c0: 2077 6172 6e69 6e67 3a3a 2052 6973 6b20   warning:: Risk 
-000050d0: 6f66 2064 6174 6120 6c65 616b 0a0a 2020  of data leak..  
-000050e0: 2020 2020 2020 446f 206e 6f74 2075 7365        Do not use
-000050f0: 203a 6675 6e63 3a60 7e73 6b6c 6561 726e   :func:`~sklearn
-00005100: 2e70 7265 7072 6f63 6573 7369 6e67 2e6d  .preprocessing.m
-00005110: 696e 6d61 785f 7363 616c 6560 2075 6e6c  inmax_scale` unl
-00005120: 6573 7320 796f 7520 6b6e 6f77 0a20 2020  ess you know.   
-00005130: 2020 2020 2077 6861 7420 796f 7520 6172       what you ar
-00005140: 6520 646f 696e 672e 2041 2063 6f6d 6d6f  e doing. A commo
-00005150: 6e20 6d69 7374 616b 6520 6973 2074 6f20  n mistake is to 
-00005160: 6170 706c 7920 6974 2074 6f20 7468 6520  apply it to the 
-00005170: 656e 7469 7265 2064 6174 610a 2020 2020  entire data.    
-00005180: 2020 2020 2a62 6566 6f72 652a 2073 706c      *before* spl
-00005190: 6974 7469 6e67 2069 6e74 6f20 7472 6169  itting into trai
-000051a0: 6e69 6e67 2061 6e64 2074 6573 7420 7365  ning and test se
-000051b0: 7473 2e20 5468 6973 2077 696c 6c20 6269  ts. This will bi
-000051c0: 6173 2074 6865 0a20 2020 2020 2020 206d  as the.        m
-000051d0: 6f64 656c 2065 7661 6c75 6174 696f 6e20  odel evaluation 
-000051e0: 6265 6361 7573 6520 696e 666f 726d 6174  because informat
-000051f0: 696f 6e20 776f 756c 6420 6861 7665 206c  ion would have l
-00005200: 6561 6b65 6420 6672 6f6d 2074 6865 2074  eaked from the t
-00005210: 6573 740a 2020 2020 2020 2020 7365 7420  est.        set 
-00005220: 746f 2074 6865 2074 7261 696e 696e 6720  to the training 
-00005230: 7365 742e 0a20 2020 2020 2020 2049 6e20  set..        In 
-00005240: 6765 6e65 7261 6c2c 2077 6520 7265 636f  general, we reco
-00005250: 6d6d 656e 6420 7573 696e 670a 2020 2020  mmend using.    
-00005260: 2020 2020 3a63 6c61 7373 3a60 7e73 6b6c      :class:`~skl
-00005270: 6561 726e 2e70 7265 7072 6f63 6573 7369  earn.preprocessi
-00005280: 6e67 2e4d 696e 4d61 7853 6361 6c65 7260  ng.MinMaxScaler`
-00005290: 2077 6974 6869 6e20 610a 2020 2020 2020   within a.      
-000052a0: 2020 3a72 6566 3a60 5069 7065 6c69 6e65    :ref:`Pipeline
-000052b0: 203c 7069 7065 6c69 6e65 3e60 2069 6e20   <pipeline>` in 
-000052c0: 6f72 6465 7220 746f 2070 7265 7665 6e74  order to prevent
-000052d0: 206d 6f73 7420 7269 736b 7320 6f66 2064   most risks of d
-000052e0: 6174 610a 2020 2020 2020 2020 6c65 616b  ata.        leak
-000052f0: 696e 673a 2060 7069 7065 203d 206d 616b  ing: `pipe = mak
-00005300: 655f 7069 7065 6c69 6e65 284d 696e 4d61  e_pipeline(MinMa
-00005310: 7853 6361 6c65 7228 292c 204c 6f67 6973  xScaler(), Logis
-00005320: 7469 6352 6567 7265 7373 696f 6e28 2929  ticRegression())
-00005330: 602e 0a0a 2020 2020 5365 6520 416c 736f  `...    See Also
-00005340: 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a 2020  .    --------.  
-00005350: 2020 4d69 6e4d 6178 5363 616c 6572 203a    MinMaxScaler :
-00005360: 2050 6572 666f 726d 7320 7363 616c 696e   Performs scalin
-00005370: 6720 746f 2061 2067 6976 656e 2072 616e  g to a given ran
-00005380: 6765 2075 7369 6e67 2074 6865 2054 7261  ge using the Tra
-00005390: 6e73 666f 726d 6572 0a20 2020 2020 2020  nsformer.       
-000053a0: 2041 5049 2028 652e 672e 2061 7320 7061   API (e.g. as pa
-000053b0: 7274 206f 6620 6120 7072 6570 726f 6365  rt of a preproce
-000053c0: 7373 696e 670a 2020 2020 2020 2020 3a63  ssing.        :c
-000053d0: 6c61 7373 3a60 7e73 6b6c 6561 726e 2e70  lass:`~sklearn.p
-000053e0: 6970 656c 696e 652e 5069 7065 6c69 6e65  ipeline.Pipeline
-000053f0: 6029 2e0a 0a20 2020 204e 6f74 6573 0a20  `)...    Notes. 
-00005400: 2020 202d 2d2d 2d2d 0a20 2020 2046 6f72     -----.    For
-00005410: 2061 2063 6f6d 7061 7269 736f 6e20 6f66   a comparison of
-00005420: 2074 6865 2064 6966 6665 7265 6e74 2073   the different s
-00005430: 6361 6c65 7273 2c20 7472 616e 7366 6f72  calers, transfor
-00005440: 6d65 7273 2c20 616e 6420 6e6f 726d 616c  mers, and normal
-00005450: 697a 6572 732c 0a20 2020 2073 6565 3a20  izers,.    see: 
-00005460: 3a72 6566 3a60 7370 6878 5f67 6c72 5f61  :ref:`sphx_glr_a
-00005470: 7574 6f5f 6578 616d 706c 6573 5f70 7265  uto_examples_pre
-00005480: 7072 6f63 6573 7369 6e67 5f70 6c6f 745f  processing_plot_
-00005490: 616c 6c5f 7363 616c 696e 672e 7079 602e  all_scaling.py`.
-000054a0: 0a0a 2020 2020 4578 616d 706c 6573 0a20  ..    Examples. 
-000054b0: 2020 202d 2d2d 2d2d 2d2d 2d0a 2020 2020     --------.    
-000054c0: 3e3e 3e20 6672 6f6d 2073 6b6c 6561 726e  >>> from sklearn
-000054d0: 2e70 7265 7072 6f63 6573 7369 6e67 2069  .preprocessing i
-000054e0: 6d70 6f72 7420 6d69 6e6d 6178 5f73 6361  mport minmax_sca
-000054f0: 6c65 0a20 2020 203e 3e3e 2058 203d 205b  le.    >>> X = [
-00005500: 5b2d 322c 2031 2c20 325d 2c20 5b2d 312c  [-2, 1, 2], [-1,
-00005510: 2030 2c20 315d 5d0a 2020 2020 3e3e 3e20   0, 1]].    >>> 
-00005520: 6d69 6e6d 6178 5f73 6361 6c65 2858 2c20  minmax_scale(X, 
-00005530: 6178 6973 3d30 2920 2023 2073 6361 6c65  axis=0)  # scale
-00005540: 2065 6163 6820 636f 6c75 6d6e 2069 6e64   each column ind
-00005550: 6570 656e 6465 6e74 6c79 0a20 2020 2061  ependently.    a
-00005560: 7272 6179 285b 5b30 2e2c 2031 2e2c 2031  rray([[0., 1., 1
-00005570: 2e5d 2c0a 2020 2020 2020 2020 2020 205b  .],.           [
-00005580: 312e 2c20 302e 2c20 302e 5d5d 290a 2020  1., 0., 0.]]).  
-00005590: 2020 3e3e 3e20 6d69 6e6d 6178 5f73 6361    >>> minmax_sca
-000055a0: 6c65 2858 2c20 6178 6973 3d31 2920 2023  le(X, axis=1)  #
-000055b0: 2073 6361 6c65 2065 6163 6820 726f 7720   scale each row 
-000055c0: 696e 6465 7065 6e64 656e 746c 790a 2020  independently.  
-000055d0: 2020 6172 7261 7928 5b5b 302e 2020 2c20    array([[0.  , 
-000055e0: 302e 3735 2c20 312e 2020 5d2c 0a20 2020  0.75, 1.  ],.   
-000055f0: 2020 2020 2020 2020 5b30 2e20 202c 2030          [0.  , 0
-00005600: 2e35 202c 2031 2e20 205d 5d29 0a20 2020  .5 , 1.  ]]).   
-00005610: 2022 2222 0a20 2020 2023 2055 6e6c 696b   """.    # Unlik
-00005620: 6520 7468 6520 7363 616c 6572 206f 626a  e the scaler obj
-00005630: 6563 742c 2074 6869 7320 6675 6e63 7469  ect, this functi
-00005640: 6f6e 2061 6c6c 6f77 7320 3164 2069 6e70  on allows 1d inp
-00005650: 7574 2e0a 2020 2020 2320 4966 2063 6f70  ut..    # If cop
-00005660: 7920 6973 2072 6571 7569 7265 642c 2069  y is required, i
-00005670: 7420 7769 6c6c 2062 6520 646f 6e65 2069  t will be done i
-00005680: 6e73 6964 6520 7468 6520 7363 616c 6572  nside the scaler
-00005690: 206f 626a 6563 742e 0a20 2020 2058 203d   object..    X =
-000056a0: 2063 6865 636b 5f61 7272 6179 280a 2020   check_array(.  
-000056b0: 2020 2020 2020 582c 2063 6f70 793d 4661        X, copy=Fa
-000056c0: 6c73 652c 2065 6e73 7572 655f 3264 3d46  lse, ensure_2d=F
-000056d0: 616c 7365 2c20 6474 7970 653d 464c 4f41  alse, dtype=FLOA
-000056e0: 545f 4454 5950 4553 2c20 666f 7263 655f  T_DTYPES, force_
-000056f0: 616c 6c5f 6669 6e69 7465 3d22 616c 6c6f  all_finite="allo
-00005700: 772d 6e61 6e22 0a20 2020 2029 0a20 2020  w-nan".    ).   
-00005710: 206f 7269 6769 6e61 6c5f 6e64 696d 203d   original_ndim =
-00005720: 2058 2e6e 6469 6d0a 0a20 2020 2069 6620   X.ndim..    if 
-00005730: 6f72 6967 696e 616c 5f6e 6469 6d20 3d3d  original_ndim ==
-00005740: 2031 3a0a 2020 2020 2020 2020 5820 3d20   1:.        X = 
-00005750: 582e 7265 7368 6170 6528 582e 7368 6170  X.reshape(X.shap
-00005760: 655b 305d 2c20 3129 0a0a 2020 2020 7320  e[0], 1)..    s 
-00005770: 3d20 4d69 6e4d 6178 5363 616c 6572 2866  = MinMaxScaler(f
-00005780: 6561 7475 7265 5f72 616e 6765 3d66 6561  eature_range=fea
-00005790: 7475 7265 5f72 616e 6765 2c20 636f 7079  ture_range, copy
-000057a0: 3d63 6f70 7929 0a20 2020 2069 6620 6178  =copy).    if ax
-000057b0: 6973 203d 3d20 303a 0a20 2020 2020 2020  is == 0:.       
-000057c0: 2058 203d 2073 2e66 6974 5f74 7261 6e73   X = s.fit_trans
-000057d0: 666f 726d 2858 290a 2020 2020 656c 7365  form(X).    else
-000057e0: 3a0a 2020 2020 2020 2020 5820 3d20 732e  :.        X = s.
-000057f0: 6669 745f 7472 616e 7366 6f72 6d28 582e  fit_transform(X.
-00005800: 5429 2e54 0a0a 2020 2020 6966 206f 7269  T).T..    if ori
-00005810: 6769 6e61 6c5f 6e64 696d 203d 3d20 313a  ginal_ndim == 1:
-00005820: 0a20 2020 2020 2020 2058 203d 2058 2e72  .        X = X.r
-00005830: 6176 656c 2829 0a0a 2020 2020 7265 7475  avel()..    retu
-00005840: 726e 2058 0a0a 0a63 6c61 7373 2053 7461  rn X...class Sta
-00005850: 6e64 6172 6453 6361 6c65 7228 4f6e 6554  ndardScaler(OneT
-00005860: 6f4f 6e65 4665 6174 7572 654d 6978 696e  oOneFeatureMixin
-00005870: 2c20 5472 616e 7366 6f72 6d65 724d 6978  , TransformerMix
-00005880: 696e 2c20 4261 7365 4573 7469 6d61 746f  in, BaseEstimato
-00005890: 7229 3a0a 2020 2020 2222 2253 7461 6e64  r):.    """Stand
-000058a0: 6172 6469 7a65 2066 6561 7475 7265 7320  ardize features 
-000058b0: 6279 2072 656d 6f76 696e 6720 7468 6520  by removing the 
-000058c0: 6d65 616e 2061 6e64 2073 6361 6c69 6e67  mean and scaling
-000058d0: 2074 6f20 756e 6974 2076 6172 6961 6e63   to unit varianc
-000058e0: 652e 0a0a 2020 2020 5468 6520 7374 616e  e...    The stan
-000058f0: 6461 7264 2073 636f 7265 206f 6620 6120  dard score of a 
-00005900: 7361 6d70 6c65 2060 7860 2069 7320 6361  sample `x` is ca
-00005910: 6c63 756c 6174 6564 2061 733a 0a0a 2020  lculated as:..  
-00005920: 2020 2020 2020 7a20 3d20 2878 202d 2075        z = (x - u
-00005930: 2920 2f20 730a 0a20 2020 2077 6865 7265  ) / s..    where
-00005940: 2060 7560 2069 7320 7468 6520 6d65 616e   `u` is the mean
-00005950: 206f 6620 7468 6520 7472 6169 6e69 6e67   of the training
-00005960: 2073 616d 706c 6573 206f 7220 7a65 726f   samples or zero
-00005970: 2069 6620 6077 6974 685f 6d65 616e 3d46   if `with_mean=F
-00005980: 616c 7365 602c 0a20 2020 2061 6e64 2060  alse`,.    and `
-00005990: 7360 2069 7320 7468 6520 7374 616e 6461  s` is the standa
-000059a0: 7264 2064 6576 6961 7469 6f6e 206f 6620  rd deviation of 
-000059b0: 7468 6520 7472 6169 6e69 6e67 2073 616d  the training sam
-000059c0: 706c 6573 206f 7220 6f6e 6520 6966 0a20  ples or one if. 
-000059d0: 2020 2060 7769 7468 5f73 7464 3d46 616c     `with_std=Fal
-000059e0: 7365 602e 0a0a 2020 2020 4365 6e74 6572  se`...    Center
-000059f0: 696e 6720 616e 6420 7363 616c 696e 6720  ing and scaling 
-00005a00: 6861 7070 656e 2069 6e64 6570 656e 6465  happen independe
-00005a10: 6e74 6c79 206f 6e20 6561 6368 2066 6561  ntly on each fea
-00005a20: 7475 7265 2062 7920 636f 6d70 7574 696e  ture by computin
-00005a30: 670a 2020 2020 7468 6520 7265 6c65 7661  g.    the releva
-00005a40: 6e74 2073 7461 7469 7374 6963 7320 6f6e  nt statistics on
-00005a50: 2074 6865 2073 616d 706c 6573 2069 6e20   the samples in 
-00005a60: 7468 6520 7472 6169 6e69 6e67 2073 6574  the training set
-00005a70: 2e20 4d65 616e 2061 6e64 0a20 2020 2073  . Mean and.    s
-00005a80: 7461 6e64 6172 6420 6465 7669 6174 696f  tandard deviatio
-00005a90: 6e20 6172 6520 7468 656e 2073 746f 7265  n are then store
-00005aa0: 6420 746f 2062 6520 7573 6564 206f 6e20  d to be used on 
-00005ab0: 6c61 7465 7220 6461 7461 2075 7369 6e67  later data using
-00005ac0: 0a20 2020 203a 6d65 7468 3a60 7472 616e  .    :meth:`tran
-00005ad0: 7366 6f72 6d60 2e0a 0a20 2020 2053 7461  sform`...    Sta
-00005ae0: 6e64 6172 6469 7a61 7469 6f6e 206f 6620  ndardization of 
-00005af0: 6120 6461 7461 7365 7420 6973 2061 2063  a dataset is a c
-00005b00: 6f6d 6d6f 6e20 7265 7175 6972 656d 656e  ommon requiremen
-00005b10: 7420 666f 7220 6d61 6e79 0a20 2020 206d  t for many.    m
-00005b20: 6163 6869 6e65 206c 6561 726e 696e 6720  achine learning 
-00005b30: 6573 7469 6d61 746f 7273 3a20 7468 6579  estimators: they
-00005b40: 206d 6967 6874 2062 6568 6176 6520 6261   might behave ba
-00005b50: 646c 7920 6966 2074 6865 0a20 2020 2069  dly if the.    i
-00005b60: 6e64 6976 6964 7561 6c20 6665 6174 7572  ndividual featur
-00005b70: 6573 2064 6f20 6e6f 7420 6d6f 7265 206f  es do not more o
-00005b80: 7220 6c65 7373 206c 6f6f 6b20 6c69 6b65  r less look like
-00005b90: 2073 7461 6e64 6172 6420 6e6f 726d 616c   standard normal
-00005ba0: 6c79 0a20 2020 2064 6973 7472 6962 7574  ly.    distribut
-00005bb0: 6564 2064 6174 6120 2865 2e67 2e20 4761  ed data (e.g. Ga
-00005bc0: 7573 7369 616e 2077 6974 6820 3020 6d65  ussian with 0 me
-00005bd0: 616e 2061 6e64 2075 6e69 7420 7661 7269  an and unit vari
-00005be0: 616e 6365 292e 0a0a 2020 2020 466f 7220  ance)...    For 
-00005bf0: 696e 7374 616e 6365 206d 616e 7920 656c  instance many el
-00005c00: 656d 656e 7473 2075 7365 6420 696e 2074  ements used in t
-00005c10: 6865 206f 626a 6563 7469 7665 2066 756e  he objective fun
-00005c20: 6374 696f 6e20 6f66 0a20 2020 2061 206c  ction of.    a l
-00005c30: 6561 726e 696e 6720 616c 676f 7269 7468  earning algorith
-00005c40: 6d20 2873 7563 6820 6173 2074 6865 2052  m (such as the R
-00005c50: 4246 206b 6572 6e65 6c20 6f66 2053 7570  BF kernel of Sup
-00005c60: 706f 7274 2056 6563 746f 720a 2020 2020  port Vector.    
-00005c70: 4d61 6368 696e 6573 206f 7220 7468 6520  Machines or the 
-00005c80: 4c31 2061 6e64 204c 3220 7265 6775 6c61  L1 and L2 regula
-00005c90: 7269 7a65 7273 206f 6620 6c69 6e65 6172  rizers of linear
-00005ca0: 206d 6f64 656c 7329 2061 7373 756d 6520   models) assume 
-00005cb0: 7468 6174 0a20 2020 2061 6c6c 2066 6561  that.    all fea
-00005cc0: 7475 7265 7320 6172 6520 6365 6e74 6572  tures are center
-00005cd0: 6564 2061 726f 756e 6420 3020 616e 6420  ed around 0 and 
-00005ce0: 6861 7665 2076 6172 6961 6e63 6520 696e  have variance in
-00005cf0: 2074 6865 2073 616d 650a 2020 2020 6f72   the same.    or
-00005d00: 6465 722e 2049 6620 6120 6665 6174 7572  der. If a featur
-00005d10: 6520 6861 7320 6120 7661 7269 616e 6365  e has a variance
-00005d20: 2074 6861 7420 6973 206f 7264 6572 7320   that is orders 
-00005d30: 6f66 206d 6167 6e69 7475 6465 206c 6172  of magnitude lar
-00005d40: 6765 720a 2020 2020 7468 616e 206f 7468  ger.    than oth
-00005d50: 6572 732c 2069 7420 6d69 6768 7420 646f  ers, it might do
-00005d60: 6d69 6e61 7465 2074 6865 206f 626a 6563  minate the objec
-00005d70: 7469 7665 2066 756e 6374 696f 6e20 616e  tive function an
-00005d80: 6420 6d61 6b65 2074 6865 0a20 2020 2065  d make the.    e
-00005d90: 7374 696d 6174 6f72 2075 6e61 626c 6520  stimator unable 
-00005da0: 746f 206c 6561 726e 2066 726f 6d20 6f74  to learn from ot
-00005db0: 6865 7220 6665 6174 7572 6573 2063 6f72  her features cor
-00005dc0: 7265 6374 6c79 2061 7320 6578 7065 6374  rectly as expect
-00005dd0: 6564 2e0a 0a20 2020 2060 5374 616e 6461  ed...    `Standa
-00005de0: 7264 5363 616c 6572 6020 6973 2073 656e  rdScaler` is sen
-00005df0: 7369 7469 7665 2074 6f20 6f75 746c 6965  sitive to outlie
-00005e00: 7273 2c20 616e 6420 7468 6520 6665 6174  rs, and the feat
-00005e10: 7572 6573 206d 6179 2073 6361 6c65 0a20  ures may scale. 
-00005e20: 2020 2064 6966 6665 7265 6e74 6c79 2066     differently f
-00005e30: 726f 6d20 6561 6368 206f 7468 6572 2069  rom each other i
-00005e40: 6e20 7468 6520 7072 6573 656e 6365 206f  n the presence o
-00005e50: 6620 6f75 746c 6965 7273 2e20 466f 7220  f outliers. For 
-00005e60: 616e 2065 7861 6d70 6c65 0a20 2020 2076  an example.    v
-00005e70: 6973 7561 6c69 7a61 7469 6f6e 2c20 7265  isualization, re
-00005e80: 6665 7220 746f 203a 7265 663a 6043 6f6d  fer to :ref:`Com
-00005e90: 7061 7265 2053 7461 6e64 6172 6453 6361  pare StandardSca
-00005ea0: 6c65 7220 7769 7468 206f 7468 6572 2073  ler with other s
-00005eb0: 6361 6c65 7273 0a20 2020 203c 706c 6f74  calers.    <plot
-00005ec0: 5f61 6c6c 5f73 6361 6c69 6e67 5f73 7461  _all_scaling_sta
-00005ed0: 6e64 6172 645f 7363 616c 6572 5f73 6563  ndard_scaler_sec
-00005ee0: 7469 6f6e 3e60 2e0a 0a20 2020 2054 6869  tion>`...    Thi
-00005ef0: 7320 7363 616c 6572 2063 616e 2061 6c73  s scaler can als
-00005f00: 6f20 6265 2061 7070 6c69 6564 2074 6f20  o be applied to 
-00005f10: 7370 6172 7365 2043 5352 206f 7220 4353  sparse CSR or CS
-00005f20: 4320 6d61 7472 6963 6573 2062 7920 7061  C matrices by pa
-00005f30: 7373 696e 670a 2020 2020 6077 6974 685f  ssing.    `with_
-00005f40: 6d65 616e 3d46 616c 7365 6020 746f 2061  mean=False` to a
-00005f50: 766f 6964 2062 7265 616b 696e 6720 7468  void breaking th
-00005f60: 6520 7370 6172 7369 7479 2073 7472 7563  e sparsity struc
-00005f70: 7475 7265 206f 6620 7468 6520 6461 7461  ture of the data
-00005f80: 2e0a 0a20 2020 2052 6561 6420 6d6f 7265  ...    Read more
-00005f90: 2069 6e20 7468 6520 3a72 6566 3a60 5573   in the :ref:`Us
-00005fa0: 6572 2047 7569 6465 203c 7072 6570 726f  er Guide <prepro
-00005fb0: 6365 7373 696e 675f 7363 616c 6572 3e60  cessing_scaler>`
-00005fc0: 2e0a 0a20 2020 2050 6172 616d 6574 6572  ...    Parameter
-00005fd0: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
-00005fe0: 0a20 2020 2063 6f70 7920 3a20 626f 6f6c  .    copy : bool
-00005ff0: 2c20 6465 6661 756c 743d 5472 7565 0a20  , default=True. 
-00006000: 2020 2020 2020 2049 6620 4661 6c73 652c         If False,
-00006010: 2074 7279 2074 6f20 6176 6f69 6420 6120   try to avoid a 
-00006020: 636f 7079 2061 6e64 2064 6f20 696e 706c  copy and do inpl
-00006030: 6163 6520 7363 616c 696e 6720 696e 7374  ace scaling inst
-00006040: 6561 642e 0a20 2020 2020 2020 2054 6869  ead..        Thi
-00006050: 7320 6973 206e 6f74 2067 7561 7261 6e74  s is not guarant
-00006060: 6565 6420 746f 2061 6c77 6179 7320 776f  eed to always wo
-00006070: 726b 2069 6e70 6c61 6365 3b20 652e 672e  rk inplace; e.g.
-00006080: 2069 6620 7468 6520 6461 7461 2069 730a   if the data is.
-00006090: 2020 2020 2020 2020 6e6f 7420 6120 4e75          not a Nu
-000060a0: 6d50 7920 6172 7261 7920 6f72 2073 6369  mPy array or sci
-000060b0: 7079 2e73 7061 7273 6520 4353 5220 6d61  py.sparse CSR ma
-000060c0: 7472 6978 2c20 6120 636f 7079 206d 6179  trix, a copy may
-000060d0: 2073 7469 6c6c 2062 650a 2020 2020 2020   still be.      
-000060e0: 2020 7265 7475 726e 6564 2e0a 0a20 2020    returned...   
-000060f0: 2077 6974 685f 6d65 616e 203a 2062 6f6f   with_mean : boo
-00006100: 6c2c 2064 6566 6175 6c74 3d54 7275 650a  l, default=True.
-00006110: 2020 2020 2020 2020 4966 2054 7275 652c          If True,
-00006120: 2063 656e 7465 7220 7468 6520 6461 7461   center the data
-00006130: 2062 6566 6f72 6520 7363 616c 696e 672e   before scaling.
-00006140: 0a20 2020 2020 2020 2054 6869 7320 646f  .        This do
-00006150: 6573 206e 6f74 2077 6f72 6b20 2861 6e64  es not work (and
-00006160: 2077 696c 6c20 7261 6973 6520 616e 2065   will raise an e
-00006170: 7863 6570 7469 6f6e 2920 7768 656e 2061  xception) when a
-00006180: 7474 656d 7074 6564 206f 6e0a 2020 2020  ttempted on.    
-00006190: 2020 2020 7370 6172 7365 206d 6174 7269      sparse matri
-000061a0: 6365 732c 2062 6563 6175 7365 2063 656e  ces, because cen
-000061b0: 7465 7269 6e67 2074 6865 6d20 656e 7461  tering them enta
-000061c0: 696c 7320 6275 696c 6469 6e67 2061 2064  ils building a d
-000061d0: 656e 7365 0a20 2020 2020 2020 206d 6174  ense.        mat
-000061e0: 7269 7820 7768 6963 6820 696e 2063 6f6d  rix which in com
-000061f0: 6d6f 6e20 7573 6520 6361 7365 7320 6973  mon use cases is
-00006200: 206c 696b 656c 7920 746f 2062 6520 746f   likely to be to
-00006210: 6f20 6c61 7267 6520 746f 2066 6974 2069  o large to fit i
-00006220: 6e0a 2020 2020 2020 2020 6d65 6d6f 7279  n.        memory
-00006230: 2e0a 0a20 2020 2077 6974 685f 7374 6420  ...    with_std 
-00006240: 3a20 626f 6f6c 2c20 6465 6661 756c 743d  : bool, default=
-00006250: 5472 7565 0a20 2020 2020 2020 2049 6620  True.        If 
-00006260: 5472 7565 2c20 7363 616c 6520 7468 6520  True, scale the 
-00006270: 6461 7461 2074 6f20 756e 6974 2076 6172  data to unit var
-00006280: 6961 6e63 6520 286f 7220 6571 7569 7661  iance (or equiva
-00006290: 6c65 6e74 6c79 2c0a 2020 2020 2020 2020  lently,.        
-000062a0: 756e 6974 2073 7461 6e64 6172 6420 6465  unit standard de
-000062b0: 7669 6174 696f 6e29 2e0a 0a20 2020 2041  viation)...    A
-000062c0: 7474 7269 6275 7465 730a 2020 2020 2d2d  ttributes.    --
-000062d0: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2073 6361  --------.    sca
-000062e0: 6c65 5f20 3a20 6e64 6172 7261 7920 6f66  le_ : ndarray of
-000062f0: 2073 6861 7065 2028 6e5f 6665 6174 7572   shape (n_featur
-00006300: 6573 2c29 206f 7220 4e6f 6e65 0a20 2020  es,) or None.   
-00006310: 2020 2020 2050 6572 2066 6561 7475 7265       Per feature
-00006320: 2072 656c 6174 6976 6520 7363 616c 696e   relative scalin
-00006330: 6720 6f66 2074 6865 2064 6174 6120 746f  g of the data to
-00006340: 2061 6368 6965 7665 207a 6572 6f20 6d65   achieve zero me
-00006350: 616e 2061 6e64 2075 6e69 740a 2020 2020  an and unit.    
-00006360: 2020 2020 7661 7269 616e 6365 2e20 4765      variance. Ge
-00006370: 6e65 7261 6c6c 7920 7468 6973 2069 7320  nerally this is 
-00006380: 6361 6c63 756c 6174 6564 2075 7369 6e67  calculated using
-00006390: 2060 6e70 2e73 7172 7428 7661 725f 2960   `np.sqrt(var_)`
-000063a0: 2e20 4966 2061 0a20 2020 2020 2020 2076  . If a.        v
-000063b0: 6172 6961 6e63 6520 6973 207a 6572 6f2c  ariance is zero,
-000063c0: 2077 6520 6361 6e27 7420 6163 6869 6576   we can't achiev
-000063d0: 6520 756e 6974 2076 6172 6961 6e63 652c  e unit variance,
-000063e0: 2061 6e64 2074 6865 2064 6174 6120 6973   and the data is
-000063f0: 206c 6566 740a 2020 2020 2020 2020 6173   left.        as
-00006400: 2d69 732c 2067 6976 696e 6720 6120 7363  -is, giving a sc
-00006410: 616c 696e 6720 6661 6374 6f72 206f 6620  aling factor of 
-00006420: 312e 2060 7363 616c 655f 6020 6973 2065  1. `scale_` is e
-00006430: 7175 616c 2074 6f20 604e 6f6e 6560 0a20  qual to `None`. 
-00006440: 2020 2020 2020 2077 6865 6e20 6077 6974         when `wit
-00006450: 685f 7374 643d 4661 6c73 6560 2e0a 0a20  h_std=False`... 
-00006460: 2020 2020 2020 202e 2e20 7665 7273 696f         .. versio
-00006470: 6e61 6464 6564 3a3a 2030 2e31 370a 2020  nadded:: 0.17.  
-00006480: 2020 2020 2020 2020 202a 7363 616c 655f           *scale_
-00006490: 2a0a 0a20 2020 206d 6561 6e5f 203a 206e  *..    mean_ : n
-000064a0: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
-000064b0: 286e 5f66 6561 7475 7265 732c 2920 6f72  (n_features,) or
-000064c0: 204e 6f6e 650a 2020 2020 2020 2020 5468   None.        Th
-000064d0: 6520 6d65 616e 2076 616c 7565 2066 6f72  e mean value for
-000064e0: 2065 6163 6820 6665 6174 7572 6520 696e   each feature in
-000064f0: 2074 6865 2074 7261 696e 696e 6720 7365   the training se
-00006500: 742e 0a20 2020 2020 2020 2045 7175 616c  t..        Equal
-00006510: 2074 6f20 6060 4e6f 6e65 6060 2077 6865   to ``None`` whe
-00006520: 6e20 6060 7769 7468 5f6d 6561 6e3d 4661  n ``with_mean=Fa
-00006530: 6c73 6560 6020 616e 6420 6060 7769 7468  lse`` and ``with
-00006540: 5f73 7464 3d46 616c 7365 6060 2e0a 0a20  _std=False``... 
-00006550: 2020 2076 6172 5f20 3a20 6e64 6172 7261     var_ : ndarra
-00006560: 7920 6f66 2073 6861 7065 2028 6e5f 6665  y of shape (n_fe
-00006570: 6174 7572 6573 2c29 206f 7220 4e6f 6e65  atures,) or None
-00006580: 0a20 2020 2020 2020 2054 6865 2076 6172  .        The var
-00006590: 6961 6e63 6520 666f 7220 6561 6368 2066  iance for each f
-000065a0: 6561 7475 7265 2069 6e20 7468 6520 7472  eature in the tr
-000065b0: 6169 6e69 6e67 2073 6574 2e20 5573 6564  aining set. Used
-000065c0: 2074 6f20 636f 6d70 7574 650a 2020 2020   to compute.    
-000065d0: 2020 2020 6073 6361 6c65 5f60 2e20 4571      `scale_`. Eq
-000065e0: 7561 6c20 746f 2060 604e 6f6e 6560 6020  ual to ``None`` 
-000065f0: 7768 656e 2060 6077 6974 685f 6d65 616e  when ``with_mean
-00006600: 3d46 616c 7365 6060 2061 6e64 0a20 2020  =False`` and.   
-00006610: 2020 2020 2060 6077 6974 685f 7374 643d       ``with_std=
-00006620: 4661 6c73 6560 602e 0a0a 2020 2020 6e5f  False``...    n_
-00006630: 6665 6174 7572 6573 5f69 6e5f 203a 2069  features_in_ : i
-00006640: 6e74 0a20 2020 2020 2020 204e 756d 6265  nt.        Numbe
-00006650: 7220 6f66 2066 6561 7475 7265 7320 7365  r of features se
-00006660: 656e 2064 7572 696e 6720 3a74 6572 6d3a  en during :term:
-00006670: 6066 6974 602e 0a0a 2020 2020 2020 2020  `fit`...        
-00006680: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
-00006690: 3a20 302e 3234 0a0a 2020 2020 6665 6174  : 0.24..    feat
-000066a0: 7572 655f 6e61 6d65 735f 696e 5f20 3a20  ure_names_in_ : 
-000066b0: 6e64 6172 7261 7920 6f66 2073 6861 7065  ndarray of shape
-000066c0: 2028 606e 5f66 6561 7475 7265 735f 696e   (`n_features_in
-000066d0: 5f60 2c29 0a20 2020 2020 2020 204e 616d  _`,).        Nam
-000066e0: 6573 206f 6620 6665 6174 7572 6573 2073  es of features s
-000066f0: 6565 6e20 6475 7269 6e67 203a 7465 726d  een during :term
-00006700: 3a60 6669 7460 2e20 4465 6669 6e65 6420  :`fit`. Defined 
-00006710: 6f6e 6c79 2077 6865 6e20 6058 600a 2020  only when `X`.  
-00006720: 2020 2020 2020 6861 7320 6665 6174 7572        has featur
-00006730: 6520 6e61 6d65 7320 7468 6174 2061 7265  e names that are
-00006740: 2061 6c6c 2073 7472 696e 6773 2e0a 0a20   all strings... 
-00006750: 2020 2020 2020 202e 2e20 7665 7273 696f         .. versio
-00006760: 6e61 6464 6564 3a3a 2031 2e30 0a0a 2020  nadded:: 1.0..  
-00006770: 2020 6e5f 7361 6d70 6c65 735f 7365 656e    n_samples_seen
-00006780: 5f20 3a20 696e 7420 6f72 206e 6461 7272  _ : int or ndarr
-00006790: 6179 206f 6620 7368 6170 6520 286e 5f66  ay of shape (n_f
-000067a0: 6561 7475 7265 732c 290a 2020 2020 2020  eatures,).      
-000067b0: 2020 5468 6520 6e75 6d62 6572 206f 6620    The number of 
-000067c0: 7361 6d70 6c65 7320 7072 6f63 6573 7365  samples processe
-000067d0: 6420 6279 2074 6865 2065 7374 696d 6174  d by the estimat
-000067e0: 6f72 2066 6f72 2065 6163 6820 6665 6174  or for each feat
-000067f0: 7572 652e 0a20 2020 2020 2020 2049 6620  ure..        If 
-00006800: 7468 6572 6520 6172 6520 6e6f 206d 6973  there are no mis
-00006810: 7369 6e67 2073 616d 706c 6573 2c20 7468  sing samples, th
-00006820: 6520 6060 6e5f 7361 6d70 6c65 735f 7365  e ``n_samples_se
-00006830: 656e 6060 2077 696c 6c20 6265 2061 6e0a  en`` will be an.
-00006840: 2020 2020 2020 2020 696e 7465 6765 722c          integer,
-00006850: 206f 7468 6572 7769 7365 2069 7420 7769   otherwise it wi
-00006860: 6c6c 2062 6520 616e 2061 7272 6179 206f  ll be an array o
-00006870: 6620 6474 7970 6520 696e 742e 2049 660a  f dtype int. If.
-00006880: 2020 2020 2020 2020 6073 616d 706c 655f          `sample_
-00006890: 7765 6967 6874 7360 2061 7265 2075 7365  weights` are use
-000068a0: 6420 6974 2077 696c 6c20 6265 2061 2066  d it will be a f
-000068b0: 6c6f 6174 2028 6966 206e 6f20 6d69 7373  loat (if no miss
-000068c0: 696e 6720 6461 7461 290a 2020 2020 2020  ing data).      
-000068d0: 2020 6f72 2061 6e20 6172 7261 7920 6f66    or an array of
-000068e0: 2064 7479 7065 2066 6c6f 6174 2074 6861   dtype float tha
-000068f0: 7420 7375 6d73 2074 6865 2077 6569 6768  t sums the weigh
-00006900: 7473 2073 6565 6e20 736f 2066 6172 2e0a  ts seen so far..
-00006910: 2020 2020 2020 2020 5769 6c6c 2062 6520          Will be 
-00006920: 7265 7365 7420 6f6e 206e 6577 2063 616c  reset on new cal
-00006930: 6c73 2074 6f20 6669 742c 2062 7574 2069  ls to fit, but i
-00006940: 6e63 7265 6d65 6e74 7320 6163 726f 7373  ncrements across
-00006950: 0a20 2020 2020 2020 2060 6070 6172 7469  .        ``parti
-00006960: 616c 5f66 6974 6060 2063 616c 6c73 2e0a  al_fit`` calls..
-00006970: 0a20 2020 2053 6565 2041 6c73 6f0a 2020  .    See Also.  
-00006980: 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020 2073    --------.    s
-00006990: 6361 6c65 203a 2045 7175 6976 616c 656e  cale : Equivalen
-000069a0: 7420 6675 6e63 7469 6f6e 2077 6974 686f  t function witho
-000069b0: 7574 2074 6865 2065 7374 696d 6174 6f72  ut the estimator
-000069c0: 2041 5049 2e0a 0a20 2020 203a 636c 6173   API...    :clas
-000069d0: 733a 607e 736b 6c65 6172 6e2e 6465 636f  s:`~sklearn.deco
-000069e0: 6d70 6f73 6974 696f 6e2e 5043 4160 203a  mposition.PCA` :
-000069f0: 2046 7572 7468 6572 2072 656d 6f76 6573   Further removes
-00006a00: 2074 6865 206c 696e 6561 720a 2020 2020   the linear.    
-00006a10: 2020 2020 636f 7272 656c 6174 696f 6e20      correlation 
-00006a20: 6163 726f 7373 2066 6561 7475 7265 7320  across features 
-00006a30: 7769 7468 2027 7768 6974 656e 3d54 7275  with 'whiten=Tru
-00006a40: 6527 2e0a 0a20 2020 204e 6f74 6573 0a20  e'...    Notes. 
-00006a50: 2020 202d 2d2d 2d2d 0a20 2020 204e 614e     -----.    NaN
-00006a60: 7320 6172 6520 7472 6561 7465 6420 6173  s are treated as
-00006a70: 206d 6973 7369 6e67 2076 616c 7565 733a   missing values:
-00006a80: 2064 6973 7265 6761 7264 6564 2069 6e20   disregarded in 
-00006a90: 6669 742c 2061 6e64 206d 6169 6e74 6169  fit, and maintai
-00006aa0: 6e65 6420 696e 0a20 2020 2074 7261 6e73  ned in.    trans
-00006ab0: 666f 726d 2e0a 0a20 2020 2057 6520 7573  form...    We us
-00006ac0: 6520 6120 6269 6173 6564 2065 7374 696d  e a biased estim
-00006ad0: 6174 6f72 2066 6f72 2074 6865 2073 7461  ator for the sta
-00006ae0: 6e64 6172 6420 6465 7669 6174 696f 6e2c  ndard deviation,
-00006af0: 2065 7175 6976 616c 656e 7420 746f 0a20   equivalent to. 
-00006b00: 2020 2060 6e75 6d70 792e 7374 6428 782c     `numpy.std(x,
-00006b10: 2064 646f 663d 3029 602e 204e 6f74 6520   ddof=0)`. Note 
-00006b20: 7468 6174 2074 6865 2063 686f 6963 6520  that the choice 
-00006b30: 6f66 2060 6464 6f66 6020 6973 2075 6e6c  of `ddof` is unl
-00006b40: 696b 656c 7920 746f 0a20 2020 2061 6666  ikely to.    aff
-00006b50: 6563 7420 6d6f 6465 6c20 7065 7266 6f72  ect model perfor
-00006b60: 6d61 6e63 652e 0a0a 2020 2020 4578 616d  mance...    Exam
-00006b70: 706c 6573 0a20 2020 202d 2d2d 2d2d 2d2d  ples.    -------
-00006b80: 2d0a 2020 2020 3e3e 3e20 6672 6f6d 2073  -.    >>> from s
-00006b90: 6b6c 6561 726e 2e70 7265 7072 6f63 6573  klearn.preproces
-00006ba0: 7369 6e67 2069 6d70 6f72 7420 5374 616e  sing import Stan
-00006bb0: 6461 7264 5363 616c 6572 0a20 2020 203e  dardScaler.    >
-00006bc0: 3e3e 2064 6174 6120 3d20 5b5b 302c 2030  >> data = [[0, 0
-00006bd0: 5d2c 205b 302c 2030 5d2c 205b 312c 2031  ], [0, 0], [1, 1
-00006be0: 5d2c 205b 312c 2031 5d5d 0a20 2020 203e  ], [1, 1]].    >
-00006bf0: 3e3e 2073 6361 6c65 7220 3d20 5374 616e  >> scaler = Stan
-00006c00: 6461 7264 5363 616c 6572 2829 0a20 2020  dardScaler().   
-00006c10: 203e 3e3e 2070 7269 6e74 2873 6361 6c65   >>> print(scale
-00006c20: 722e 6669 7428 6461 7461 2929 0a20 2020  r.fit(data)).   
-00006c30: 2053 7461 6e64 6172 6453 6361 6c65 7228   StandardScaler(
-00006c40: 290a 2020 2020 3e3e 3e20 7072 696e 7428  ).    >>> print(
-00006c50: 7363 616c 6572 2e6d 6561 6e5f 290a 2020  scaler.mean_).  
-00006c60: 2020 5b30 2e35 2030 2e35 5d0a 2020 2020    [0.5 0.5].    
-00006c70: 3e3e 3e20 7072 696e 7428 7363 616c 6572  >>> print(scaler
-00006c80: 2e74 7261 6e73 666f 726d 2864 6174 6129  .transform(data)
-00006c90: 290a 2020 2020 5b5b 2d31 2e20 2d31 2e5d  ).    [[-1. -1.]
-00006ca0: 0a20 2020 2020 5b2d 312e 202d 312e 5d0a  .     [-1. -1.].
-00006cb0: 2020 2020 205b 2031 2e20 2031 2e5d 0a20       [ 1.  1.]. 
-00006cc0: 2020 2020 5b20 312e 2020 312e 5d5d 0a20      [ 1.  1.]]. 
-00006cd0: 2020 203e 3e3e 2070 7269 6e74 2873 6361     >>> print(sca
-00006ce0: 6c65 722e 7472 616e 7366 6f72 6d28 5b5b  ler.transform([[
-00006cf0: 322c 2032 5d5d 2929 0a20 2020 205b 5b33  2, 2]])).    [[3
-00006d00: 2e20 332e 5d5d 0a20 2020 2022 2222 0a0a  . 3.]].    """..
-00006d10: 2020 2020 5f70 6172 616d 6574 6572 5f63      _parameter_c
-00006d20: 6f6e 7374 7261 696e 7473 3a20 6469 6374  onstraints: dict
-00006d30: 203d 207b 0a20 2020 2020 2020 2022 636f   = {.        "co
-00006d40: 7079 223a 205b 2262 6f6f 6c65 616e 225d  py": ["boolean"]
-00006d50: 2c0a 2020 2020 2020 2020 2277 6974 685f  ,.        "with_
-00006d60: 6d65 616e 223a 205b 2262 6f6f 6c65 616e  mean": ["boolean
-00006d70: 225d 2c0a 2020 2020 2020 2020 2277 6974  "],.        "wit
-00006d80: 685f 7374 6422 3a20 5b22 626f 6f6c 6561  h_std": ["boolea
-00006d90: 6e22 5d2c 0a20 2020 207d 0a0a 2020 2020  n"],.    }..    
-00006da0: 6465 6620 5f5f 696e 6974 5f5f 2873 656c  def __init__(sel
-00006db0: 662c 202a 2c20 636f 7079 3d54 7275 652c  f, *, copy=True,
-00006dc0: 2077 6974 685f 6d65 616e 3d54 7275 652c   with_mean=True,
-00006dd0: 2077 6974 685f 7374 643d 5472 7565 293a   with_std=True):
-00006de0: 0a20 2020 2020 2020 2073 656c 662e 7769  .        self.wi
-00006df0: 7468 5f6d 6561 6e20 3d20 7769 7468 5f6d  th_mean = with_m
-00006e00: 6561 6e0a 2020 2020 2020 2020 7365 6c66  ean.        self
-00006e10: 2e77 6974 685f 7374 6420 3d20 7769 7468  .with_std = with
-00006e20: 5f73 7464 0a20 2020 2020 2020 2073 656c  _std.        sel
-00006e30: 662e 636f 7079 203d 2063 6f70 790a 0a20  f.copy = copy.. 
-00006e40: 2020 2064 6566 205f 7265 7365 7428 7365     def _reset(se
-00006e50: 6c66 293a 0a20 2020 2020 2020 2022 2222  lf):.        """
-00006e60: 5265 7365 7420 696e 7465 726e 616c 2064  Reset internal d
-00006e70: 6174 612d 6465 7065 6e64 656e 7420 7374  ata-dependent st
-00006e80: 6174 6520 6f66 2074 6865 2073 6361 6c65  ate of the scale
-00006e90: 722c 2069 6620 6e65 6365 7373 6172 792e  r, if necessary.
-00006ea0: 0a0a 2020 2020 2020 2020 5f5f 696e 6974  ..        __init
-00006eb0: 5f5f 2070 6172 616d 6574 6572 7320 6172  __ parameters ar
-00006ec0: 6520 6e6f 7420 746f 7563 6865 642e 0a20  e not touched.. 
-00006ed0: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-00006ee0: 2020 2023 2043 6865 636b 696e 6720 6f6e     # Checking on
-00006ef0: 6520 6174 7472 6962 7574 6520 6973 2065  e attribute is e
-00006f00: 6e6f 7567 682c 2062 6563 6175 7365 2074  nough, because t
-00006f10: 6865 7920 6172 6520 616c 6c20 7365 7420  hey are all set 
-00006f20: 746f 6765 7468 6572 0a20 2020 2020 2020  together.       
-00006f30: 2023 2069 6e20 7061 7274 6961 6c5f 6669   # in partial_fi
-00006f40: 740a 2020 2020 2020 2020 6966 2068 6173  t.        if has
-00006f50: 6174 7472 2873 656c 662c 2022 7363 616c  attr(self, "scal
-00006f60: 655f 2229 3a0a 2020 2020 2020 2020 2020  e_"):.          
-00006f70: 2020 6465 6c20 7365 6c66 2e73 6361 6c65    del self.scale
-00006f80: 5f0a 2020 2020 2020 2020 2020 2020 6465  _.            de
-00006f90: 6c20 7365 6c66 2e6e 5f73 616d 706c 6573  l self.n_samples
-00006fa0: 5f73 6565 6e5f 0a20 2020 2020 2020 2020  _seen_.         
-00006fb0: 2020 2064 656c 2073 656c 662e 6d65 616e     del self.mean
-00006fc0: 5f0a 2020 2020 2020 2020 2020 2020 6465  _.            de
-00006fd0: 6c20 7365 6c66 2e76 6172 5f0a 0a20 2020  l self.var_..   
-00006fe0: 2064 6566 2066 6974 2873 656c 662c 2058   def fit(self, X
-00006ff0: 2c20 793d 4e6f 6e65 2c20 7361 6d70 6c65  , y=None, sample
-00007000: 5f77 6569 6768 743d 4e6f 6e65 293a 0a20  _weight=None):. 
-00007010: 2020 2020 2020 2022 2222 436f 6d70 7574         """Comput
-00007020: 6520 7468 6520 6d65 616e 2061 6e64 2073  e the mean and s
-00007030: 7464 2074 6f20 6265 2075 7365 6420 666f  td to be used fo
-00007040: 7220 6c61 7465 7220 7363 616c 696e 672e  r later scaling.
-00007050: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
-00007060: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
-00007070: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
-00007080: 5820 3a20 7b61 7272 6179 2d6c 696b 652c  X : {array-like,
-00007090: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
-000070a0: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
-000070b0: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
-000070c0: 0a20 2020 2020 2020 2020 2020 2054 6865  .            The
-000070d0: 2064 6174 6120 7573 6564 2074 6f20 636f   data used to co
-000070e0: 6d70 7574 6520 7468 6520 6d65 616e 2061  mpute the mean a
-000070f0: 6e64 2073 7461 6e64 6172 6420 6465 7669  nd standard devi
-00007100: 6174 696f 6e0a 2020 2020 2020 2020 2020  ation.          
-00007110: 2020 7573 6564 2066 6f72 206c 6174 6572    used for later
-00007120: 2073 6361 6c69 6e67 2061 6c6f 6e67 2074   scaling along t
-00007130: 6865 2066 6561 7475 7265 7320 6178 6973  he features axis
-00007140: 2e0a 0a20 2020 2020 2020 2079 203a 204e  ...        y : N
-00007150: 6f6e 650a 2020 2020 2020 2020 2020 2020  one.            
-00007160: 4967 6e6f 7265 642e 0a0a 2020 2020 2020  Ignored...      
-00007170: 2020 7361 6d70 6c65 5f77 6569 6768 7420    sample_weight 
-00007180: 3a20 6172 7261 792d 6c69 6b65 206f 6620  : array-like of 
-00007190: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
-000071a0: 2c29 2c20 6465 6661 756c 743d 4e6f 6e65  ,), default=None
-000071b0: 0a20 2020 2020 2020 2020 2020 2049 6e64  .            Ind
-000071c0: 6976 6964 7561 6c20 7765 6967 6874 7320  ividual weights 
-000071d0: 666f 7220 6561 6368 2073 616d 706c 652e  for each sample.
-000071e0: 0a0a 2020 2020 2020 2020 2020 2020 2e2e  ..            ..
-000071f0: 2076 6572 7369 6f6e 6164 6465 643a 3a20   versionadded:: 
-00007200: 302e 3234 0a20 2020 2020 2020 2020 2020  0.24.           
-00007210: 2020 2020 7061 7261 6d65 7465 7220 2a73      parameter *s
-00007220: 616d 706c 655f 7765 6967 6874 2a20 7375  ample_weight* su
-00007230: 7070 6f72 7420 746f 2053 7461 6e64 6172  pport to Standar
-00007240: 6453 6361 6c65 722e 0a0a 2020 2020 2020  dScaler...      
-00007250: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
-00007260: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
-00007270: 2020 7365 6c66 203a 206f 626a 6563 740a    self : object.
-00007280: 2020 2020 2020 2020 2020 2020 4669 7474              Fitt
-00007290: 6564 2073 6361 6c65 722e 0a20 2020 2020  ed scaler..     
-000072a0: 2020 2022 2222 0a20 2020 2020 2020 2023     """.        #
-000072b0: 2052 6573 6574 2069 6e74 6572 6e61 6c20   Reset internal 
-000072c0: 7374 6174 6520 6265 666f 7265 2066 6974  state before fit
-000072d0: 7469 6e67 0a20 2020 2020 2020 2073 656c  ting.        sel
-000072e0: 662e 5f72 6573 6574 2829 0a20 2020 2020  f._reset().     
-000072f0: 2020 2072 6574 7572 6e20 7365 6c66 2e70     return self.p
-00007300: 6172 7469 616c 5f66 6974 2858 2c20 792c  artial_fit(X, y,
-00007310: 2073 616d 706c 655f 7765 6967 6874 290a   sample_weight).
-00007320: 0a20 2020 2040 5f66 6974 5f63 6f6e 7465  .    @_fit_conte
-00007330: 7874 2870 7265 6665 725f 736b 6970 5f6e  xt(prefer_skip_n
-00007340: 6573 7465 645f 7661 6c69 6461 7469 6f6e  ested_validation
-00007350: 3d54 7275 6529 0a20 2020 2064 6566 2070  =True).    def p
-00007360: 6172 7469 616c 5f66 6974 2873 656c 662c  artial_fit(self,
-00007370: 2058 2c20 793d 4e6f 6e65 2c20 7361 6d70   X, y=None, samp
-00007380: 6c65 5f77 6569 6768 743d 4e6f 6e65 293a  le_weight=None):
-00007390: 0a20 2020 2020 2020 2022 2222 4f6e 6c69  .        """Onli
-000073a0: 6e65 2063 6f6d 7075 7461 7469 6f6e 206f  ne computation o
-000073b0: 6620 6d65 616e 2061 6e64 2073 7464 206f  f mean and std o
-000073c0: 6e20 5820 666f 7220 6c61 7465 7220 7363  n X for later sc
-000073d0: 616c 696e 672e 0a0a 2020 2020 2020 2020  aling...        
-000073e0: 416c 6c20 6f66 2058 2069 7320 7072 6f63  All of X is proc
-000073f0: 6573 7365 6420 6173 2061 2073 696e 676c  essed as a singl
-00007400: 6520 6261 7463 682e 2054 6869 7320 6973  e batch. This is
-00007410: 2069 6e74 656e 6465 6420 666f 7220 6361   intended for ca
-00007420: 7365 730a 2020 2020 2020 2020 7768 656e  ses.        when
-00007430: 203a 6d65 7468 3a60 6669 7460 2069 7320   :meth:`fit` is 
-00007440: 6e6f 7420 6665 6173 6962 6c65 2064 7565  not feasible due
-00007450: 2074 6f20 7665 7279 206c 6172 6765 206e   to very large n
-00007460: 756d 6265 7220 6f66 0a20 2020 2020 2020  umber of.       
-00007470: 2060 6e5f 7361 6d70 6c65 7360 206f 7220   `n_samples` or 
-00007480: 6265 6361 7573 6520 5820 6973 2072 6561  because X is rea
-00007490: 6420 6672 6f6d 2061 2063 6f6e 7469 6e75  d from a continu
-000074a0: 6f75 7320 7374 7265 616d 2e0a 0a20 2020  ous stream...   
-000074b0: 2020 2020 2054 6865 2061 6c67 6f72 6974       The algorit
-000074c0: 686d 2066 6f72 2069 6e63 7265 6d65 6e74  hm for increment
-000074d0: 616c 206d 6561 6e20 616e 6420 7374 6420  al mean and std 
-000074e0: 6973 2067 6976 656e 2069 6e20 4571 7561  is given in Equa
-000074f0: 7469 6f6e 2031 2e35 612c 620a 2020 2020  tion 1.5a,b.    
-00007500: 2020 2020 696e 2043 6861 6e2c 2054 6f6e      in Chan, Ton
-00007510: 7920 462e 2c20 4765 6e65 2048 2e20 476f  y F., Gene H. Go
-00007520: 6c75 622c 2061 6e64 2052 616e 6461 6c6c  lub, and Randall
-00007530: 204a 2e20 4c65 5665 7175 652e 2022 416c   J. LeVeque. "Al
-00007540: 676f 7269 7468 6d73 0a20 2020 2020 2020  gorithms.       
-00007550: 2066 6f72 2063 6f6d 7075 7469 6e67 2074   for computing t
-00007560: 6865 2073 616d 706c 6520 7661 7269 616e  he sample varian
-00007570: 6365 3a20 416e 616c 7973 6973 2061 6e64  ce: Analysis and
-00007580: 2072 6563 6f6d 6d65 6e64 6174 696f 6e73   recommendations
-00007590: 2e22 0a20 2020 2020 2020 2054 6865 2041  .".        The A
-000075a0: 6d65 7269 6361 6e20 5374 6174 6973 7469  merican Statisti
-000075b0: 6369 616e 2033 372e 3320 2831 3938 3329  cian 37.3 (1983)
-000075c0: 3a20 3234 322d 3234 373a 0a0a 2020 2020  : 242-247:..    
-000075d0: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
-000075e0: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
-000075f0: 2d0a 2020 2020 2020 2020 5820 3a20 7b61  -.        X : {a
-00007600: 7272 6179 2d6c 696b 652c 2073 7061 7273  rray-like, spars
-00007610: 6520 6d61 7472 6978 7d20 6f66 2073 6861  e matrix} of sha
-00007620: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
-00007630: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
-00007640: 2020 2020 2020 2054 6865 2064 6174 6120         The data 
-00007650: 7573 6564 2074 6f20 636f 6d70 7574 6520  used to compute 
-00007660: 7468 6520 6d65 616e 2061 6e64 2073 7461  the mean and sta
-00007670: 6e64 6172 6420 6465 7669 6174 696f 6e0a  ndard deviation.
-00007680: 2020 2020 2020 2020 2020 2020 7573 6564              used
-00007690: 2066 6f72 206c 6174 6572 2073 6361 6c69   for later scali
-000076a0: 6e67 2061 6c6f 6e67 2074 6865 2066 6561  ng along the fea
-000076b0: 7475 7265 7320 6178 6973 2e0a 0a20 2020  tures axis...   
-000076c0: 2020 2020 2079 203a 204e 6f6e 650a 2020       y : None.  
-000076d0: 2020 2020 2020 2020 2020 4967 6e6f 7265            Ignore
-000076e0: 642e 0a0a 2020 2020 2020 2020 7361 6d70  d...        samp
-000076f0: 6c65 5f77 6569 6768 7420 3a20 6172 7261  le_weight : arra
-00007700: 792d 6c69 6b65 206f 6620 7368 6170 6520  y-like of shape 
-00007710: 286e 5f73 616d 706c 6573 2c29 2c20 6465  (n_samples,), de
-00007720: 6661 756c 743d 4e6f 6e65 0a20 2020 2020  fault=None.     
-00007730: 2020 2020 2020 2049 6e64 6976 6964 7561         Individua
-00007740: 6c20 7765 6967 6874 7320 666f 7220 6561  l weights for ea
-00007750: 6368 2073 616d 706c 652e 0a0a 2020 2020  ch sample...    
-00007760: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
-00007770: 6f6e 6164 6465 643a 3a20 302e 3234 0a20  onadded:: 0.24. 
-00007780: 2020 2020 2020 2020 2020 2020 2020 7061                pa
-00007790: 7261 6d65 7465 7220 2a73 616d 706c 655f  rameter *sample_
-000077a0: 7765 6967 6874 2a20 7375 7070 6f72 7420  weight* support 
-000077b0: 746f 2053 7461 6e64 6172 6453 6361 6c65  to StandardScale
-000077c0: 722e 0a0a 2020 2020 2020 2020 5265 7475  r...        Retu
-000077d0: 726e 730a 2020 2020 2020 2020 2d2d 2d2d  rns.        ----
-000077e0: 2d2d 2d0a 2020 2020 2020 2020 7365 6c66  ---.        self
-000077f0: 203a 206f 626a 6563 740a 2020 2020 2020   : object.      
-00007800: 2020 2020 2020 4669 7474 6564 2073 6361        Fitted sca
-00007810: 6c65 722e 0a20 2020 2020 2020 2022 2222  ler..        """
-00007820: 0a20 2020 2020 2020 2066 6972 7374 5f63  .        first_c
-00007830: 616c 6c20 3d20 6e6f 7420 6861 7361 7474  all = not hasatt
-00007840: 7228 7365 6c66 2c20 226e 5f73 616d 706c  r(self, "n_sampl
-00007850: 6573 5f73 6565 6e5f 2229 0a20 2020 2020  es_seen_").     
-00007860: 2020 2058 203d 2073 656c 662e 5f76 616c     X = self._val
-00007870: 6964 6174 655f 6461 7461 280a 2020 2020  idate_data(.    
-00007880: 2020 2020 2020 2020 582c 0a20 2020 2020          X,.     
-00007890: 2020 2020 2020 2061 6363 6570 745f 7370         accept_sp
-000078a0: 6172 7365 3d28 2263 7372 222c 2022 6373  arse=("csr", "cs
-000078b0: 6322 292c 0a20 2020 2020 2020 2020 2020  c"),.           
-000078c0: 2064 7479 7065 3d46 4c4f 4154 5f44 5459   dtype=FLOAT_DTY
-000078d0: 5045 532c 0a20 2020 2020 2020 2020 2020  PES,.           
-000078e0: 2066 6f72 6365 5f61 6c6c 5f66 696e 6974   force_all_finit
-000078f0: 653d 2261 6c6c 6f77 2d6e 616e 222c 0a20  e="allow-nan",. 
-00007900: 2020 2020 2020 2020 2020 2072 6573 6574             reset
-00007910: 3d66 6972 7374 5f63 616c 6c2c 0a20 2020  =first_call,.   
-00007920: 2020 2020 2029 0a20 2020 2020 2020 206e       ).        n
-00007930: 5f66 6561 7475 7265 7320 3d20 582e 7368  _features = X.sh
-00007940: 6170 655b 315d 0a0a 2020 2020 2020 2020  ape[1]..        
-00007950: 6966 2073 616d 706c 655f 7765 6967 6874  if sample_weight
-00007960: 2069 7320 6e6f 7420 4e6f 6e65 3a0a 2020   is not None:.  
-00007970: 2020 2020 2020 2020 2020 7361 6d70 6c65            sample
-00007980: 5f77 6569 6768 7420 3d20 5f63 6865 636b  _weight = _check
-00007990: 5f73 616d 706c 655f 7765 6967 6874 2873  _sample_weight(s
-000079a0: 616d 706c 655f 7765 6967 6874 2c20 582c  ample_weight, X,
-000079b0: 2064 7479 7065 3d58 2e64 7479 7065 290a   dtype=X.dtype).
-000079c0: 0a20 2020 2020 2020 2023 2045 7665 6e20  .        # Even 
-000079d0: 696e 2074 6865 2063 6173 6520 6f66 2060  in the case of `
-000079e0: 7769 7468 5f6d 6561 6e3d 4661 6c73 6560  with_mean=False`
-000079f0: 2c20 7765 2075 7064 6174 6520 7468 6520  , we update the 
-00007a00: 6d65 616e 2061 6e79 7761 790a 2020 2020  mean anyway.    
-00007a10: 2020 2020 2320 5468 6973 2069 7320 6e65      # This is ne
-00007a20: 6564 6564 2066 6f72 2074 6865 2069 6e63  eded for the inc
-00007a30: 7265 6d65 6e74 616c 2063 6f6d 7075 7461  remental computa
-00007a40: 7469 6f6e 206f 6620 7468 6520 7661 720a  tion of the var.
-00007a50: 2020 2020 2020 2020 2320 5365 6520 696e          # See in
-00007a60: 6372 5f6d 6561 6e5f 7661 7269 616e 6365  cr_mean_variance
-00007a70: 5f61 7869 7320 616e 6420 5f69 6e63 7265  _axis and _incre
-00007a80: 6d65 6e74 616c 5f6d 6561 6e5f 7661 7269  mental_mean_vari
-00007a90: 616e 6365 5f61 7869 730a 0a20 2020 2020  ance_axis..     
-00007aa0: 2020 2023 2069 6620 6e5f 7361 6d70 6c65     # if n_sample
-00007ab0: 735f 7365 656e 5f20 6973 2061 6e20 696e  s_seen_ is an in
-00007ac0: 7465 6765 7220 2869 2e65 2e20 6e6f 206d  teger (i.e. no m
-00007ad0: 6973 7369 6e67 2076 616c 7565 7329 2c20  issing values), 
-00007ae0: 7765 206e 6565 6420 746f 0a20 2020 2020  we need to.     
-00007af0: 2020 2023 2074 7261 6e73 666f 726d 2069     # transform i
-00007b00: 7420 746f 2061 204e 756d 5079 2061 7272  t to a NumPy arr
-00007b10: 6179 206f 6620 7368 6170 6520 286e 5f66  ay of shape (n_f
-00007b20: 6561 7475 7265 732c 2920 7265 7175 6972  eatures,) requir
-00007b30: 6564 2062 790a 2020 2020 2020 2020 2320  ed by.        # 
-00007b40: 696e 6372 5f6d 6561 6e5f 7661 7269 616e  incr_mean_varian
-00007b50: 6365 5f61 7869 7320 616e 6420 5f69 6e63  ce_axis and _inc
-00007b60: 7265 6d65 6e74 616c 5f76 6172 6961 6e63  remental_varianc
-00007b70: 655f 6178 6973 0a20 2020 2020 2020 2064  e_axis.        d
-00007b80: 7479 7065 203d 206e 702e 696e 7436 3420  type = np.int64 
-00007b90: 6966 2073 616d 706c 655f 7765 6967 6874  if sample_weight
-00007ba0: 2069 7320 4e6f 6e65 2065 6c73 6520 582e   is None else X.
-00007bb0: 6474 7970 650a 2020 2020 2020 2020 6966  dtype.        if
-00007bc0: 206e 6f74 2068 6173 6174 7472 2873 656c   not hasattr(sel
-00007bd0: 662c 2022 6e5f 7361 6d70 6c65 735f 7365  f, "n_samples_se
-00007be0: 656e 5f22 293a 0a20 2020 2020 2020 2020  en_"):.         
-00007bf0: 2020 2073 656c 662e 6e5f 7361 6d70 6c65     self.n_sample
-00007c00: 735f 7365 656e 5f20 3d20 6e70 2e7a 6572  s_seen_ = np.zer
-00007c10: 6f73 286e 5f66 6561 7475 7265 732c 2064  os(n_features, d
-00007c20: 7479 7065 3d64 7479 7065 290a 2020 2020  type=dtype).    
-00007c30: 2020 2020 656c 6966 206e 702e 7369 7a65      elif np.size
-00007c40: 2873 656c 662e 6e5f 7361 6d70 6c65 735f  (self.n_samples_
-00007c50: 7365 656e 5f29 203d 3d20 313a 0a20 2020  seen_) == 1:.   
-00007c60: 2020 2020 2020 2020 2073 656c 662e 6e5f           self.n_
-00007c70: 7361 6d70 6c65 735f 7365 656e 5f20 3d20  samples_seen_ = 
-00007c80: 6e70 2e72 6570 6561 7428 7365 6c66 2e6e  np.repeat(self.n
-00007c90: 5f73 616d 706c 6573 5f73 6565 6e5f 2c20  _samples_seen_, 
-00007ca0: 582e 7368 6170 655b 315d 290a 2020 2020  X.shape[1]).    
-00007cb0: 2020 2020 2020 2020 7365 6c66 2e6e 5f73          self.n_s
-00007cc0: 616d 706c 6573 5f73 6565 6e5f 203d 2073  amples_seen_ = s
-00007cd0: 656c 662e 6e5f 7361 6d70 6c65 735f 7365  elf.n_samples_se
-00007ce0: 656e 5f2e 6173 7479 7065 2864 7479 7065  en_.astype(dtype
-00007cf0: 2c20 636f 7079 3d46 616c 7365 290a 0a20  , copy=False).. 
-00007d00: 2020 2020 2020 2069 6620 7370 6172 7365         if sparse
-00007d10: 2e69 7373 7061 7273 6528 5829 3a0a 2020  .issparse(X):.  
-00007d20: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
-00007d30: 662e 7769 7468 5f6d 6561 6e3a 0a20 2020  f.with_mean:.   
-00007d40: 2020 2020 2020 2020 2020 2020 2072 6169               rai
-00007d50: 7365 2056 616c 7565 4572 726f 7228 0a20  se ValueError(. 
-00007d60: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007d70: 2020 2022 4361 6e6e 6f74 2063 656e 7465     "Cannot cente
-00007d80: 7220 7370 6172 7365 206d 6174 7269 6365  r sparse matrice
-00007d90: 733a 2070 6173 7320 6077 6974 685f 6d65  s: pass `with_me
-00007da0: 616e 3d46 616c 7365 6020 220a 2020 2020  an=False` ".    
-00007db0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007dc0: 2269 6e73 7465 6164 2e20 5365 6520 646f  "instead. See do
-00007dd0: 6373 7472 696e 6720 666f 7220 6d6f 7469  cstring for moti
-00007de0: 7661 7469 6f6e 2061 6e64 2061 6c74 6572  vation and alter
-00007df0: 6e61 7469 7665 732e 220a 2020 2020 2020  natives.".      
-00007e00: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
-00007e10: 2020 2020 2020 2020 7370 6172 7365 5f63          sparse_c
-00007e20: 6f6e 7374 7275 6374 6f72 203d 2028 0a20  onstructor = (. 
-00007e30: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00007e40: 7061 7273 652e 6373 725f 6d61 7472 6978  parse.csr_matrix
-00007e50: 2069 6620 582e 666f 726d 6174 203d 3d20   if X.format == 
-00007e60: 2263 7372 2220 656c 7365 2073 7061 7273  "csr" else spars
-00007e70: 652e 6373 635f 6d61 7472 6978 0a20 2020  e.csc_matrix.   
-00007e80: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
-00007e90: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00007ea0: 7769 7468 5f73 7464 3a0a 2020 2020 2020  with_std:.      
-00007eb0: 2020 2020 2020 2020 2020 2320 4669 7273            # Firs
-00007ec0: 7420 7061 7373 0a20 2020 2020 2020 2020  t pass.         
-00007ed0: 2020 2020 2020 2069 6620 6e6f 7420 6861         if not ha
-00007ee0: 7361 7474 7228 7365 6c66 2c20 2273 6361  sattr(self, "sca
-00007ef0: 6c65 5f22 293a 0a20 2020 2020 2020 2020  le_"):.         
-00007f00: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00007f10: 6d65 616e 5f2c 2073 656c 662e 7661 725f  mean_, self.var_
-00007f20: 2c20 7365 6c66 2e6e 5f73 616d 706c 6573  , self.n_samples
-00007f30: 5f73 6565 6e5f 203d 206d 6561 6e5f 7661  _seen_ = mean_va
-00007f40: 7269 616e 6365 5f61 7869 7328 0a20 2020  riance_axis(.   
-00007f50: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007f60: 2020 2020 2058 2c20 6178 6973 3d30 2c20       X, axis=0, 
-00007f70: 7765 6967 6874 733d 7361 6d70 6c65 5f77  weights=sample_w
-00007f80: 6569 6768 742c 2072 6574 7572 6e5f 7375  eight, return_su
-00007f90: 6d5f 7765 6967 6874 733d 5472 7565 0a20  m_weights=True. 
-00007fa0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00007fb0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-00007fc0: 2020 2020 2023 204e 6578 7420 7061 7373       # Next pass
-00007fd0: 6573 0a20 2020 2020 2020 2020 2020 2020  es.             
-00007fe0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00007ff0: 2020 2020 2020 2020 2020 2020 2028 0a20               (. 
-00008000: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008010: 2020 2020 2020 2073 656c 662e 6d65 616e         self.mean
-00008020: 5f2c 0a20 2020 2020 2020 2020 2020 2020  _,.             
-00008030: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-00008040: 7661 725f 2c0a 2020 2020 2020 2020 2020  var_,.          
-00008050: 2020 2020 2020 2020 2020 2020 2020 7365                se
-00008060: 6c66 2e6e 5f73 616d 706c 6573 5f73 6565  lf.n_samples_see
-00008070: 6e5f 2c0a 2020 2020 2020 2020 2020 2020  n_,.            
-00008080: 2020 2020 2020 2020 2920 3d20 696e 6372          ) = incr
-00008090: 5f6d 6561 6e5f 7661 7269 616e 6365 5f61  _mean_variance_a
-000080a0: 7869 7328 0a20 2020 2020 2020 2020 2020  xis(.           
-000080b0: 2020 2020 2020 2020 2020 2020 2058 2c0a               X,.
+000002d0: 2c20 6368 6563 6b5f 6172 7261 792c 2072  , check_array, r
+000002e0: 6573 616d 706c 650a 6672 6f6d 202e 2e75  esample.from ..u
+000002f0: 7469 6c73 2e5f 6172 7261 795f 6170 6920  tils._array_api 
+00000300: 696d 706f 7274 2067 6574 5f6e 616d 6573  import get_names
+00000310: 7061 6365 0a66 726f 6d20 2e2e 7574 696c  pace.from ..util
+00000320: 732e 5f70 6172 616d 5f76 616c 6964 6174  s._param_validat
+00000330: 696f 6e20 696d 706f 7274 2049 6e74 6572  ion import Inter
+00000340: 7661 6c2c 204f 7074 696f 6e73 2c20 5374  val, Options, St
+00000350: 724f 7074 696f 6e73 2c20 7661 6c69 6461  rOptions, valida
+00000360: 7465 5f70 6172 616d 730a 6672 6f6d 202e  te_params.from .
+00000370: 2e75 7469 6c73 2e65 7874 6d61 7468 2069  .utils.extmath i
+00000380: 6d70 6f72 7420 5f69 6e63 7265 6d65 6e74  mport _increment
+00000390: 616c 5f6d 6561 6e5f 616e 645f 7661 722c  al_mean_and_var,
+000003a0: 2072 6f77 5f6e 6f72 6d73 0a66 726f 6d20   row_norms.from 
+000003b0: 2e2e 7574 696c 732e 7370 6172 7365 6675  ..utils.sparsefu
+000003c0: 6e63 7320 696d 706f 7274 2028 0a20 2020  ncs import (.   
+000003d0: 2069 6e63 725f 6d65 616e 5f76 6172 6961   incr_mean_varia
+000003e0: 6e63 655f 6178 6973 2c0a 2020 2020 696e  nce_axis,.    in
+000003f0: 706c 6163 655f 636f 6c75 6d6e 5f73 6361  place_column_sca
+00000400: 6c65 2c0a 2020 2020 6d65 616e 5f76 6172  le,.    mean_var
+00000410: 6961 6e63 655f 6178 6973 2c0a 2020 2020  iance_axis,.    
+00000420: 6d69 6e5f 6d61 785f 6178 6973 2c0a 290a  min_max_axis,.).
+00000430: 6672 6f6d 202e 2e75 7469 6c73 2e73 7061  from ..utils.spa
+00000440: 7273 6566 756e 6373 5f66 6173 7420 696d  rsefuncs_fast im
+00000450: 706f 7274 2028 0a20 2020 2069 6e70 6c61  port (.    inpla
+00000460: 6365 5f63 7372 5f72 6f77 5f6e 6f72 6d61  ce_csr_row_norma
+00000470: 6c69 7a65 5f6c 312c 0a20 2020 2069 6e70  lize_l1,.    inp
+00000480: 6c61 6365 5f63 7372 5f72 6f77 5f6e 6f72  lace_csr_row_nor
+00000490: 6d61 6c69 7a65 5f6c 322c 0a29 0a66 726f  malize_l2,.).fro
+000004a0: 6d20 2e2e 7574 696c 732e 7661 6c69 6461  m ..utils.valida
+000004b0: 7469 6f6e 2069 6d70 6f72 7420 280a 2020  tion import (.  
+000004c0: 2020 464c 4f41 545f 4454 5950 4553 2c0a    FLOAT_DTYPES,.
+000004d0: 2020 2020 5f63 6865 636b 5f73 616d 706c      _check_sampl
+000004e0: 655f 7765 6967 6874 2c0a 2020 2020 6368  e_weight,.    ch
+000004f0: 6563 6b5f 6973 5f66 6974 7465 642c 0a20  eck_is_fitted,. 
+00000500: 2020 2063 6865 636b 5f72 616e 646f 6d5f     check_random_
+00000510: 7374 6174 652c 0a29 0a66 726f 6d20 2e5f  state,.).from ._
+00000520: 656e 636f 6465 7273 2069 6d70 6f72 7420  encoders import 
+00000530: 4f6e 6548 6f74 456e 636f 6465 720a 0a42  OneHotEncoder..B
+00000540: 4f55 4e44 535f 5448 5245 5348 4f4c 4420  OUNDS_THRESHOLD 
+00000550: 3d20 3165 2d37 0a0a 5f5f 616c 6c5f 5f20  = 1e-7..__all__ 
+00000560: 3d20 5b0a 2020 2020 2242 696e 6172 697a  = [.    "Binariz
+00000570: 6572 222c 0a20 2020 2022 4b65 726e 656c  er",.    "Kernel
+00000580: 4365 6e74 6572 6572 222c 0a20 2020 2022  Centerer",.    "
+00000590: 4d69 6e4d 6178 5363 616c 6572 222c 0a20  MinMaxScaler",. 
+000005a0: 2020 2022 4d61 7841 6273 5363 616c 6572     "MaxAbsScaler
+000005b0: 222c 0a20 2020 2022 4e6f 726d 616c 697a  ",.    "Normaliz
+000005c0: 6572 222c 0a20 2020 2022 4f6e 6548 6f74  er",.    "OneHot
+000005d0: 456e 636f 6465 7222 2c0a 2020 2020 2252  Encoder",.    "R
+000005e0: 6f62 7573 7453 6361 6c65 7222 2c0a 2020  obustScaler",.  
+000005f0: 2020 2253 7461 6e64 6172 6453 6361 6c65    "StandardScale
+00000600: 7222 2c0a 2020 2020 2251 7561 6e74 696c  r",.    "Quantil
+00000610: 6554 7261 6e73 666f 726d 6572 222c 0a20  eTransformer",. 
+00000620: 2020 2022 506f 7765 7254 7261 6e73 666f     "PowerTransfo
+00000630: 726d 6572 222c 0a20 2020 2022 6164 645f  rmer",.    "add_
+00000640: 6475 6d6d 795f 6665 6174 7572 6522 2c0a  dummy_feature",.
+00000650: 2020 2020 2262 696e 6172 697a 6522 2c0a      "binarize",.
+00000660: 2020 2020 226e 6f72 6d61 6c69 7a65 222c      "normalize",
+00000670: 0a20 2020 2022 7363 616c 6522 2c0a 2020  .    "scale",.  
+00000680: 2020 2272 6f62 7573 745f 7363 616c 6522    "robust_scale"
+00000690: 2c0a 2020 2020 226d 6178 6162 735f 7363  ,.    "maxabs_sc
+000006a0: 616c 6522 2c0a 2020 2020 226d 696e 6d61  ale",.    "minma
+000006b0: 785f 7363 616c 6522 2c0a 2020 2020 2271  x_scale",.    "q
+000006c0: 7561 6e74 696c 655f 7472 616e 7366 6f72  uantile_transfor
+000006d0: 6d22 2c0a 2020 2020 2270 6f77 6572 5f74  m",.    "power_t
+000006e0: 7261 6e73 666f 726d 222c 0a5d 0a0a 0a64  ransform",.]...d
+000006f0: 6566 205f 6973 5f63 6f6e 7374 616e 745f  ef _is_constant_
+00000700: 6665 6174 7572 6528 7661 722c 206d 6561  feature(var, mea
+00000710: 6e2c 206e 5f73 616d 706c 6573 293a 0a20  n, n_samples):. 
+00000720: 2020 2022 2222 4465 7465 6374 2069 6620     """Detect if 
+00000730: 6120 6665 6174 7572 6520 6973 2069 6e64  a feature is ind
+00000740: 6973 7469 6e67 7569 7368 6162 6c65 2066  istinguishable f
+00000750: 726f 6d20 6120 636f 6e73 7461 6e74 2066  rom a constant f
+00000760: 6561 7475 7265 2e0a 0a20 2020 2054 6865  eature...    The
+00000770: 2064 6574 6563 7469 6f6e 2069 7320 6261   detection is ba
+00000780: 7365 6420 6f6e 2069 7473 2063 6f6d 7075  sed on its compu
+00000790: 7465 6420 7661 7269 616e 6365 2061 6e64  ted variance and
+000007a0: 206f 6e20 7468 6520 7468 656f 7265 7469   on the theoreti
+000007b0: 6361 6c0a 2020 2020 6572 726f 7220 626f  cal.    error bo
+000007c0: 756e 6473 206f 6620 7468 6520 2732 2070  unds of the '2 p
+000007d0: 6173 7320 616c 676f 7269 7468 6d27 2066  ass algorithm' f
+000007e0: 6f72 2076 6172 6961 6e63 6520 636f 6d70  or variance comp
+000007f0: 7574 6174 696f 6e2e 0a0a 2020 2020 5365  utation...    Se
+00000800: 6520 2241 6c67 6f72 6974 686d 7320 666f  e "Algorithms fo
+00000810: 7220 636f 6d70 7574 696e 6720 7468 6520  r computing the 
+00000820: 7361 6d70 6c65 2076 6172 6961 6e63 653a  sample variance:
+00000830: 2061 6e61 6c79 7369 7320 616e 640a 2020   analysis and.  
+00000840: 2020 7265 636f 6d6d 656e 6461 7469 6f6e    recommendation
+00000850: 7322 2c20 6279 2043 6861 6e2c 2047 6f6c  s", by Chan, Gol
+00000860: 7562 2c20 616e 6420 4c65 5665 7175 652e  ub, and LeVeque.
+00000870: 0a20 2020 2022 2222 0a20 2020 2023 2049  .    """.    # I
+00000880: 6e20 7363 696b 6974 2d6c 6561 726e 2c20  n scikit-learn, 
+00000890: 7661 7269 616e 6365 2069 7320 616c 7761  variance is alwa
+000008a0: 7973 2063 6f6d 7075 7465 6420 7573 696e  ys computed usin
+000008b0: 6720 666c 6f61 7436 3420 6163 6375 6d75  g float64 accumu
+000008c0: 6c61 746f 7273 2e0a 2020 2020 6570 7320  lators..    eps 
+000008d0: 3d20 6e70 2e66 696e 666f 286e 702e 666c  = np.finfo(np.fl
+000008e0: 6f61 7436 3429 2e65 7073 0a0a 2020 2020  oat64).eps..    
+000008f0: 7570 7065 725f 626f 756e 6420 3d20 6e5f  upper_bound = n_
+00000900: 7361 6d70 6c65 7320 2a20 6570 7320 2a20  samples * eps * 
+00000910: 7661 7220 2b20 286e 5f73 616d 706c 6573  var + (n_samples
+00000920: 202a 206d 6561 6e20 2a20 6570 7329 202a   * mean * eps) *
+00000930: 2a20 320a 2020 2020 7265 7475 726e 2076  * 2.    return v
+00000940: 6172 203c 3d20 7570 7065 725f 626f 756e  ar <= upper_boun
+00000950: 640a 0a0a 6465 6620 5f68 616e 646c 655f  d...def _handle_
+00000960: 7a65 726f 735f 696e 5f73 6361 6c65 2873  zeros_in_scale(s
+00000970: 6361 6c65 2c20 636f 7079 3d54 7275 652c  cale, copy=True,
+00000980: 2063 6f6e 7374 616e 745f 6d61 736b 3d4e   constant_mask=N
+00000990: 6f6e 6529 3a0a 2020 2020 2222 2253 6574  one):.    """Set
+000009a0: 2073 6361 6c65 7320 6f66 206e 6561 7220   scales of near 
+000009b0: 636f 6e73 7461 6e74 2066 6561 7475 7265  constant feature
+000009c0: 7320 746f 2031 2e0a 0a20 2020 2054 6865  s to 1...    The
+000009d0: 2067 6f61 6c20 6973 2074 6f20 6176 6f69   goal is to avoi
+000009e0: 6420 6469 7669 7369 6f6e 2062 7920 7665  d division by ve
+000009f0: 7279 2073 6d61 6c6c 206f 7220 7a65 726f  ry small or zero
+00000a00: 2076 616c 7565 732e 0a0a 2020 2020 4e65   values...    Ne
+00000a10: 6172 2063 6f6e 7374 616e 7420 6665 6174  ar constant feat
+00000a20: 7572 6573 2061 7265 2064 6574 6563 7465  ures are detecte
+00000a30: 6420 6175 746f 6d61 7469 6361 6c6c 7920  d automatically 
+00000a40: 6279 2069 6465 6e74 6966 7969 6e67 0a20  by identifying. 
+00000a50: 2020 2073 6361 6c65 7320 636c 6f73 6520     scales close 
+00000a60: 746f 206d 6163 6869 6e65 2070 7265 6369  to machine preci
+00000a70: 7369 6f6e 2075 6e6c 6573 7320 7468 6579  sion unless they
+00000a80: 2061 7265 2070 7265 636f 6d70 7574 6564   are precomputed
+00000a90: 2062 790a 2020 2020 7468 6520 6361 6c6c   by.    the call
+00000aa0: 6572 2061 6e64 2070 6173 7365 6420 7769  er and passed wi
+00000ab0: 7468 2074 6865 2060 636f 6e73 7461 6e74  th the `constant
+00000ac0: 5f6d 6173 6b60 206b 7761 7267 2e0a 0a20  _mask` kwarg... 
+00000ad0: 2020 2054 7970 6963 616c 6c79 2066 6f72     Typically for
+00000ae0: 2073 7461 6e64 6172 6420 7363 616c 696e   standard scalin
+00000af0: 672c 2074 6865 2073 6361 6c65 7320 6172  g, the scales ar
+00000b00: 6520 7468 6520 7374 616e 6461 7264 0a20  e the standard. 
+00000b10: 2020 2064 6576 6961 7469 6f6e 2077 6869     deviation whi
+00000b20: 6c65 206e 6561 7220 636f 6e73 7461 6e74  le near constant
+00000b30: 2066 6561 7475 7265 7320 6172 6520 6265   features are be
+00000b40: 7474 6572 2064 6574 6563 7465 6420 6f6e  tter detected on
+00000b50: 2074 6865 0a20 2020 2063 6f6d 7075 7465   the.    compute
+00000b60: 6420 7661 7269 616e 6365 7320 7768 6963  d variances whic
+00000b70: 6820 6172 6520 636c 6f73 6572 2074 6f20  h are closer to 
+00000b80: 6d61 6368 696e 6520 7072 6563 6973 696f  machine precisio
+00000b90: 6e20 6279 0a20 2020 2063 6f6e 7374 7275  n by.    constru
+00000ba0: 6374 696f 6e2e 0a20 2020 2022 2222 0a20  ction..    """. 
+00000bb0: 2020 2023 2069 6620 7765 2061 7265 2066     # if we are f
+00000bc0: 6974 7469 6e67 206f 6e20 3144 2061 7272  itting on 1D arr
+00000bd0: 6179 732c 2073 6361 6c65 206d 6967 6874  ays, scale might
+00000be0: 2062 6520 6120 7363 616c 6172 0a20 2020   be a scalar.   
+00000bf0: 2069 6620 6e70 2e69 7373 6361 6c61 7228   if np.isscalar(
+00000c00: 7363 616c 6529 3a0a 2020 2020 2020 2020  scale):.        
+00000c10: 6966 2073 6361 6c65 203d 3d20 302e 303a  if scale == 0.0:
+00000c20: 0a20 2020 2020 2020 2020 2020 2073 6361  .            sca
+00000c30: 6c65 203d 2031 2e30 0a20 2020 2020 2020  le = 1.0.       
+00000c40: 2072 6574 7572 6e20 7363 616c 650a 2020   return scale.  
+00000c50: 2020 2320 7363 616c 6520 6973 2061 6e20    # scale is an 
+00000c60: 6172 7261 790a 2020 2020 656c 7365 3a0a  array.    else:.
+00000c70: 2020 2020 2020 2020 7870 2c20 5f20 3d20          xp, _ = 
+00000c80: 6765 745f 6e61 6d65 7370 6163 6528 7363  get_namespace(sc
+00000c90: 616c 6529 0a20 2020 2020 2020 2069 6620  ale).        if 
+00000ca0: 636f 6e73 7461 6e74 5f6d 6173 6b20 6973  constant_mask is
+00000cb0: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         
+00000cc0: 2020 2023 2044 6574 6563 7420 6e65 6172     # Detect near
+00000cd0: 2063 6f6e 7374 616e 7420 7661 6c75 6573   constant values
+00000ce0: 2074 6f20 6176 6f69 6420 6469 7669 6469   to avoid dividi
+00000cf0: 6e67 2062 7920 6120 7665 7279 2073 6d61  ng by a very sma
+00000d00: 6c6c 0a20 2020 2020 2020 2020 2020 2023  ll.            #
+00000d10: 2076 616c 7565 2074 6861 7420 636f 756c   value that coul
+00000d20: 6420 6c65 6164 2074 6f20 7375 7270 7269  d lead to surpri
+00000d30: 7369 6e67 2072 6573 756c 7473 2061 6e64  sing results and
+00000d40: 206e 756d 6572 6963 616c 0a20 2020 2020   numerical.     
+00000d50: 2020 2020 2020 2023 2073 7461 6269 6c69         # stabili
+00000d60: 7479 2069 7373 7565 732e 0a20 2020 2020  ty issues..     
+00000d70: 2020 2020 2020 2063 6f6e 7374 616e 745f         constant_
+00000d80: 6d61 736b 203d 2073 6361 6c65 203c 2031  mask = scale < 1
+00000d90: 3020 2a20 7870 2e66 696e 666f 2873 6361  0 * xp.finfo(sca
+00000da0: 6c65 2e64 7479 7065 292e 6570 730a 0a20  le.dtype).eps.. 
+00000db0: 2020 2020 2020 2069 6620 636f 7079 3a0a         if copy:.
+00000dc0: 2020 2020 2020 2020 2020 2020 2320 4e65              # Ne
+00000dd0: 7720 6172 7261 7920 746f 2061 766f 6964  w array to avoid
+00000de0: 2073 6964 652d 6566 6665 6374 730a 2020   side-effects.  
+00000df0: 2020 2020 2020 2020 2020 7363 616c 6520            scale 
+00000e00: 3d20 7870 2e61 7361 7272 6179 2873 6361  = xp.asarray(sca
+00000e10: 6c65 2c20 636f 7079 3d54 7275 6529 0a20  le, copy=True). 
+00000e20: 2020 2020 2020 2073 6361 6c65 5b63 6f6e         scale[con
+00000e30: 7374 616e 745f 6d61 736b 5d20 3d20 312e  stant_mask] = 1.
+00000e40: 300a 2020 2020 2020 2020 7265 7475 726e  0.        return
+00000e50: 2073 6361 6c65 0a0a 0a40 7661 6c69 6461   scale...@valida
+00000e60: 7465 5f70 6172 616d 7328 0a20 2020 207b  te_params(.    {
+00000e70: 0a20 2020 2020 2020 2022 5822 3a20 5b22  .        "X": ["
+00000e80: 6172 7261 792d 6c69 6b65 222c 2022 7370  array-like", "sp
+00000e90: 6172 7365 206d 6174 7269 7822 5d2c 0a20  arse matrix"],. 
+00000ea0: 2020 2020 2020 2022 6178 6973 223a 205b         "axis": [
+00000eb0: 4f70 7469 6f6e 7328 496e 7465 6772 616c  Options(Integral
+00000ec0: 2c20 7b30 2c20 317d 295d 2c0a 2020 2020  , {0, 1})],.    
+00000ed0: 2020 2020 2277 6974 685f 6d65 616e 223a      "with_mean":
+00000ee0: 205b 2262 6f6f 6c65 616e 225d 2c0a 2020   ["boolean"],.  
+00000ef0: 2020 2020 2020 2277 6974 685f 7374 6422        "with_std"
+00000f00: 3a20 5b22 626f 6f6c 6561 6e22 5d2c 0a20  : ["boolean"],. 
+00000f10: 2020 2020 2020 2022 636f 7079 223a 205b         "copy": [
+00000f20: 2262 6f6f 6c65 616e 225d 2c0a 2020 2020  "boolean"],.    
+00000f30: 7d2c 0a20 2020 2070 7265 6665 725f 736b  },.    prefer_sk
+00000f40: 6970 5f6e 6573 7465 645f 7661 6c69 6461  ip_nested_valida
+00000f50: 7469 6f6e 3d54 7275 652c 0a29 0a64 6566  tion=True,.).def
+00000f60: 2073 6361 6c65 2858 2c20 2a2c 2061 7869   scale(X, *, axi
+00000f70: 733d 302c 2077 6974 685f 6d65 616e 3d54  s=0, with_mean=T
+00000f80: 7275 652c 2077 6974 685f 7374 643d 5472  rue, with_std=Tr
+00000f90: 7565 2c20 636f 7079 3d54 7275 6529 3a0a  ue, copy=True):.
+00000fa0: 2020 2020 2222 2253 7461 6e64 6172 6469      """Standardi
+00000fb0: 7a65 2061 2064 6174 6173 6574 2061 6c6f  ze a dataset alo
+00000fc0: 6e67 2061 6e79 2061 7869 732e 0a0a 2020  ng any axis...  
+00000fd0: 2020 4365 6e74 6572 2074 6f20 7468 6520    Center to the 
+00000fe0: 6d65 616e 2061 6e64 2063 6f6d 706f 6e65  mean and compone
+00000ff0: 6e74 2077 6973 6520 7363 616c 6520 746f  nt wise scale to
+00001000: 2075 6e69 7420 7661 7269 616e 6365 2e0a   unit variance..
+00001010: 0a20 2020 2052 6561 6420 6d6f 7265 2069  .    Read more i
+00001020: 6e20 7468 6520 3a72 6566 3a60 5573 6572  n the :ref:`User
+00001030: 2047 7569 6465 203c 7072 6570 726f 6365   Guide <preproce
+00001040: 7373 696e 675f 7363 616c 6572 3e60 2e0a  ssing_scaler>`..
+00001050: 0a20 2020 2050 6172 616d 6574 6572 730a  .    Parameters.
+00001060: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
+00001070: 2020 2058 203a 207b 6172 7261 792d 6c69     X : {array-li
+00001080: 6b65 2c20 7370 6172 7365 206d 6174 7269  ke, sparse matri
+00001090: 787d 206f 6620 7368 6170 6520 286e 5f73  x} of shape (n_s
+000010a0: 616d 706c 6573 2c20 6e5f 6665 6174 7572  amples, n_featur
+000010b0: 6573 290a 2020 2020 2020 2020 5468 6520  es).        The 
+000010c0: 6461 7461 2074 6f20 6365 6e74 6572 2061  data to center a
+000010d0: 6e64 2073 6361 6c65 2e0a 0a20 2020 2061  nd scale...    a
+000010e0: 7869 7320 3a20 7b30 2c20 317d 2c20 6465  xis : {0, 1}, de
+000010f0: 6661 756c 743d 300a 2020 2020 2020 2020  fault=0.        
+00001100: 4178 6973 2075 7365 6420 746f 2063 6f6d  Axis used to com
+00001110: 7075 7465 2074 6865 206d 6561 6e73 2061  pute the means a
+00001120: 6e64 2073 7461 6e64 6172 6420 6465 7669  nd standard devi
+00001130: 6174 696f 6e73 2061 6c6f 6e67 2e20 4966  ations along. If
+00001140: 2030 2c0a 2020 2020 2020 2020 696e 6465   0,.        inde
+00001150: 7065 6e64 656e 746c 7920 7374 616e 6461  pendently standa
+00001160: 7264 697a 6520 6561 6368 2066 6561 7475  rdize each featu
+00001170: 7265 2c20 6f74 6865 7277 6973 6520 2869  re, otherwise (i
+00001180: 6620 3129 2073 7461 6e64 6172 6469 7a65  f 1) standardize
+00001190: 0a20 2020 2020 2020 2065 6163 6820 7361  .        each sa
+000011a0: 6d70 6c65 2e0a 0a20 2020 2077 6974 685f  mple...    with_
+000011b0: 6d65 616e 203a 2062 6f6f 6c2c 2064 6566  mean : bool, def
+000011c0: 6175 6c74 3d54 7275 650a 2020 2020 2020  ault=True.      
+000011d0: 2020 4966 2054 7275 652c 2063 656e 7465    If True, cente
+000011e0: 7220 7468 6520 6461 7461 2062 6566 6f72  r the data befor
+000011f0: 6520 7363 616c 696e 672e 0a0a 2020 2020  e scaling...    
+00001200: 7769 7468 5f73 7464 203a 2062 6f6f 6c2c  with_std : bool,
+00001210: 2064 6566 6175 6c74 3d54 7275 650a 2020   default=True.  
+00001220: 2020 2020 2020 4966 2054 7275 652c 2073        If True, s
+00001230: 6361 6c65 2074 6865 2064 6174 6120 746f  cale the data to
+00001240: 2075 6e69 7420 7661 7269 616e 6365 2028   unit variance (
+00001250: 6f72 2065 7175 6976 616c 656e 746c 792c  or equivalently,
+00001260: 0a20 2020 2020 2020 2075 6e69 7420 7374  .        unit st
+00001270: 616e 6461 7264 2064 6576 6961 7469 6f6e  andard deviation
+00001280: 292e 0a0a 2020 2020 636f 7079 203a 2062  )...    copy : b
+00001290: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
+000012a0: 650a 2020 2020 2020 2020 4966 2046 616c  e.        If Fal
+000012b0: 7365 2c20 7472 7920 746f 2061 766f 6964  se, try to avoid
+000012c0: 2061 2063 6f70 7920 616e 6420 7363 616c   a copy and scal
+000012d0: 6520 696e 2070 6c61 6365 2e0a 2020 2020  e in place..    
+000012e0: 2020 2020 5468 6973 2069 7320 6e6f 7420      This is not 
+000012f0: 6775 6172 616e 7465 6564 2074 6f20 616c  guaranteed to al
+00001300: 7761 7973 2077 6f72 6b20 696e 2070 6c61  ways work in pla
+00001310: 6365 3b20 652e 672e 2069 6620 7468 6520  ce; e.g. if the 
+00001320: 6461 7461 2069 730a 2020 2020 2020 2020  data is.        
+00001330: 6120 6e75 6d70 7920 6172 7261 7920 7769  a numpy array wi
+00001340: 7468 2061 6e20 696e 7420 6474 7970 652c  th an int dtype,
+00001350: 2061 2063 6f70 7920 7769 6c6c 2062 6520   a copy will be 
+00001360: 7265 7475 726e 6564 2065 7665 6e20 7769  returned even wi
+00001370: 7468 0a20 2020 2020 2020 2063 6f70 793d  th.        copy=
+00001380: 4661 6c73 652e 0a0a 2020 2020 5265 7475  False...    Retu
+00001390: 726e 730a 2020 2020 2d2d 2d2d 2d2d 2d0a  rns.    -------.
+000013a0: 2020 2020 585f 7472 203a 207b 6e64 6172      X_tr : {ndar
+000013b0: 7261 792c 2073 7061 7273 6520 6d61 7472  ray, sparse matr
+000013c0: 6978 7d20 6f66 2073 6861 7065 2028 6e5f  ix} of shape (n_
+000013d0: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
+000013e0: 7265 7329 0a20 2020 2020 2020 2054 6865  res).        The
+000013f0: 2074 7261 6e73 666f 726d 6564 2064 6174   transformed dat
+00001400: 612e 0a0a 2020 2020 5365 6520 416c 736f  a...    See Also
+00001410: 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a 2020  .    --------.  
+00001420: 2020 5374 616e 6461 7264 5363 616c 6572    StandardScaler
+00001430: 203a 2050 6572 666f 726d 7320 7363 616c   : Performs scal
+00001440: 696e 6720 746f 2075 6e69 7420 7661 7269  ing to unit vari
+00001450: 616e 6365 2075 7369 6e67 2074 6865 2054  ance using the T
+00001460: 7261 6e73 666f 726d 6572 0a20 2020 2020  ransformer.     
+00001470: 2020 2041 5049 2028 652e 672e 2061 7320     API (e.g. as 
+00001480: 7061 7274 206f 6620 6120 7072 6570 726f  part of a prepro
+00001490: 6365 7373 696e 670a 2020 2020 2020 2020  cessing.        
+000014a0: 3a63 6c61 7373 3a60 7e73 6b6c 6561 726e  :class:`~sklearn
+000014b0: 2e70 6970 656c 696e 652e 5069 7065 6c69  .pipeline.Pipeli
+000014c0: 6e65 6029 2e0a 0a20 2020 204e 6f74 6573  ne`)...    Notes
+000014d0: 0a20 2020 202d 2d2d 2d2d 0a20 2020 2054  .    -----.    T
+000014e0: 6869 7320 696d 706c 656d 656e 7461 7469  his implementati
+000014f0: 6f6e 2077 696c 6c20 7265 6675 7365 2074  on will refuse t
+00001500: 6f20 6365 6e74 6572 2073 6369 7079 2e73  o center scipy.s
+00001510: 7061 7273 6520 6d61 7472 6963 6573 0a20  parse matrices. 
+00001520: 2020 2073 696e 6365 2069 7420 776f 756c     since it woul
+00001530: 6420 6d61 6b65 2074 6865 6d20 6e6f 6e2d  d make them non-
+00001540: 7370 6172 7365 2061 6e64 2077 6f75 6c64  sparse and would
+00001550: 2070 6f74 656e 7469 616c 6c79 2063 7261   potentially cra
+00001560: 7368 2074 6865 0a20 2020 2070 726f 6772  sh the.    progr
+00001570: 616d 2077 6974 6820 6d65 6d6f 7279 2065  am with memory e
+00001580: 7868 6175 7374 696f 6e20 7072 6f62 6c65  xhaustion proble
+00001590: 6d73 2e0a 0a20 2020 2049 6e73 7465 6164  ms...    Instead
+000015a0: 2074 6865 2063 616c 6c65 7220 6973 2065   the caller is e
+000015b0: 7870 6563 7465 6420 746f 2065 6974 6865  xpected to eithe
+000015c0: 7220 7365 7420 6578 706c 6963 6974 6c79  r set explicitly
+000015d0: 0a20 2020 2060 7769 7468 5f6d 6561 6e3d  .    `with_mean=
+000015e0: 4661 6c73 6560 2028 696e 2074 6861 7420  False` (in that 
+000015f0: 6361 7365 2c20 6f6e 6c79 2076 6172 6961  case, only varia
+00001600: 6e63 6520 7363 616c 696e 6720 7769 6c6c  nce scaling will
+00001610: 2062 650a 2020 2020 7065 7266 6f72 6d65   be.    performe
+00001620: 6420 6f6e 2074 6865 2066 6561 7475 7265  d on the feature
+00001630: 7320 6f66 2074 6865 2043 5343 206d 6174  s of the CSC mat
+00001640: 7269 7829 206f 7220 746f 2063 616c 6c20  rix) or to call 
+00001650: 6058 2e74 6f61 7272 6179 2829 600a 2020  `X.toarray()`.  
+00001660: 2020 6966 2068 652f 7368 6520 6578 7065    if he/she expe
+00001670: 6374 7320 7468 6520 6d61 7465 7269 616c  cts the material
+00001680: 697a 6564 2064 656e 7365 2061 7272 6179  ized dense array
+00001690: 2074 6f20 6669 7420 696e 206d 656d 6f72   to fit in memor
+000016a0: 792e 0a0a 2020 2020 546f 2061 766f 6964  y...    To avoid
+000016b0: 206d 656d 6f72 7920 636f 7079 2074 6865   memory copy the
+000016c0: 2063 616c 6c65 7220 7368 6f75 6c64 2070   caller should p
+000016d0: 6173 7320 6120 4353 4320 6d61 7472 6978  ass a CSC matrix
+000016e0: 2e0a 0a20 2020 204e 614e 7320 6172 6520  ...    NaNs are 
+000016f0: 7472 6561 7465 6420 6173 206d 6973 7369  treated as missi
+00001700: 6e67 2076 616c 7565 733a 2064 6973 7265  ng values: disre
+00001710: 6761 7264 6564 2074 6f20 636f 6d70 7574  garded to comput
+00001720: 6520 7468 6520 7374 6174 6973 7469 6373  e the statistics
+00001730: 2c0a 2020 2020 616e 6420 6d61 696e 7461  ,.    and mainta
+00001740: 696e 6564 2064 7572 696e 6720 7468 6520  ined during the 
+00001750: 6461 7461 2074 7261 6e73 666f 726d 6174  data transformat
+00001760: 696f 6e2e 0a0a 2020 2020 5765 2075 7365  ion...    We use
+00001770: 2061 2062 6961 7365 6420 6573 7469 6d61   a biased estima
+00001780: 746f 7220 666f 7220 7468 6520 7374 616e  tor for the stan
+00001790: 6461 7264 2064 6576 6961 7469 6f6e 2c20  dard deviation, 
+000017a0: 6571 7569 7661 6c65 6e74 2074 6f0a 2020  equivalent to.  
+000017b0: 2020 606e 756d 7079 2e73 7464 2878 2c20    `numpy.std(x, 
+000017c0: 6464 6f66 3d30 2960 2e20 4e6f 7465 2074  ddof=0)`. Note t
+000017d0: 6861 7420 7468 6520 6368 6f69 6365 206f  hat the choice o
+000017e0: 6620 6064 646f 6660 2069 7320 756e 6c69  f `ddof` is unli
+000017f0: 6b65 6c79 2074 6f0a 2020 2020 6166 6665  kely to.    affe
+00001800: 6374 206d 6f64 656c 2070 6572 666f 726d  ct model perform
+00001810: 616e 6365 2e0a 0a20 2020 2046 6f72 2061  ance...    For a
+00001820: 2063 6f6d 7061 7269 736f 6e20 6f66 2074   comparison of t
+00001830: 6865 2064 6966 6665 7265 6e74 2073 6361  he different sca
+00001840: 6c65 7273 2c20 7472 616e 7366 6f72 6d65  lers, transforme
+00001850: 7273 2c20 616e 6420 6e6f 726d 616c 697a  rs, and normaliz
+00001860: 6572 732c 0a20 2020 2073 6565 3a20 3a72  ers,.    see: :r
+00001870: 6566 3a60 7370 6878 5f67 6c72 5f61 7574  ef:`sphx_glr_aut
+00001880: 6f5f 6578 616d 706c 6573 5f70 7265 7072  o_examples_prepr
+00001890: 6f63 6573 7369 6e67 5f70 6c6f 745f 616c  ocessing_plot_al
+000018a0: 6c5f 7363 616c 696e 672e 7079 602e 0a0a  l_scaling.py`...
+000018b0: 2020 2020 2e2e 2077 6172 6e69 6e67 3a3a      .. warning::
+000018c0: 2052 6973 6b20 6f66 2064 6174 6120 6c65   Risk of data le
+000018d0: 616b 0a0a 2020 2020 2020 2020 446f 206e  ak..        Do n
+000018e0: 6f74 2075 7365 203a 6675 6e63 3a60 7e73  ot use :func:`~s
+000018f0: 6b6c 6561 726e 2e70 7265 7072 6f63 6573  klearn.preproces
+00001900: 7369 6e67 2e73 6361 6c65 6020 756e 6c65  sing.scale` unle
+00001910: 7373 2079 6f75 206b 6e6f 770a 2020 2020  ss you know.    
+00001920: 2020 2020 7768 6174 2079 6f75 2061 7265      what you are
+00001930: 2064 6f69 6e67 2e20 4120 636f 6d6d 6f6e   doing. A common
+00001940: 206d 6973 7461 6b65 2069 7320 746f 2061   mistake is to a
+00001950: 7070 6c79 2069 7420 746f 2074 6865 2065  pply it to the e
+00001960: 6e74 6972 6520 6461 7461 0a20 2020 2020  ntire data.     
+00001970: 2020 202a 6265 666f 7265 2a20 7370 6c69     *before* spli
+00001980: 7474 696e 6720 696e 746f 2074 7261 696e  tting into train
+00001990: 696e 6720 616e 6420 7465 7374 2073 6574  ing and test set
+000019a0: 732e 2054 6869 7320 7769 6c6c 2062 6961  s. This will bia
+000019b0: 7320 7468 650a 2020 2020 2020 2020 6d6f  s the.        mo
+000019c0: 6465 6c20 6576 616c 7561 7469 6f6e 2062  del evaluation b
+000019d0: 6563 6175 7365 2069 6e66 6f72 6d61 7469  ecause informati
+000019e0: 6f6e 2077 6f75 6c64 2068 6176 6520 6c65  on would have le
+000019f0: 616b 6564 2066 726f 6d20 7468 6520 7465  aked from the te
+00001a00: 7374 0a20 2020 2020 2020 2073 6574 2074  st.        set t
+00001a10: 6f20 7468 6520 7472 6169 6e69 6e67 2073  o the training s
+00001a20: 6574 2e0a 2020 2020 2020 2020 496e 2067  et..        In g
+00001a30: 656e 6572 616c 2c20 7765 2072 6563 6f6d  eneral, we recom
+00001a40: 6d65 6e64 2075 7369 6e67 0a20 2020 2020  mend using.     
+00001a50: 2020 203a 636c 6173 733a 607e 736b 6c65     :class:`~skle
+00001a60: 6172 6e2e 7072 6570 726f 6365 7373 696e  arn.preprocessin
+00001a70: 672e 5374 616e 6461 7264 5363 616c 6572  g.StandardScaler
+00001a80: 6020 7769 7468 696e 2061 0a20 2020 2020  ` within a.     
+00001a90: 2020 203a 7265 663a 6050 6970 656c 696e     :ref:`Pipelin
+00001aa0: 6520 3c70 6970 656c 696e 653e 6020 696e  e <pipeline>` in
+00001ab0: 206f 7264 6572 2074 6f20 7072 6576 656e   order to preven
+00001ac0: 7420 6d6f 7374 2072 6973 6b73 206f 6620  t most risks of 
+00001ad0: 6461 7461 0a20 2020 2020 2020 206c 6561  data.        lea
+00001ae0: 6b69 6e67 3a20 6070 6970 6520 3d20 6d61  king: `pipe = ma
+00001af0: 6b65 5f70 6970 656c 696e 6528 5374 616e  ke_pipeline(Stan
+00001b00: 6461 7264 5363 616c 6572 2829 2c20 4c6f  dardScaler(), Lo
+00001b10: 6769 7374 6963 5265 6772 6573 7369 6f6e  gisticRegression
+00001b20: 2829 2960 2e0a 0a20 2020 2045 7861 6d70  ())`...    Examp
+00001b30: 6c65 730a 2020 2020 2d2d 2d2d 2d2d 2d2d  les.    --------
+00001b40: 0a20 2020 203e 3e3e 2066 726f 6d20 736b  .    >>> from sk
+00001b50: 6c65 6172 6e2e 7072 6570 726f 6365 7373  learn.preprocess
+00001b60: 696e 6720 696d 706f 7274 2073 6361 6c65  ing import scale
+00001b70: 0a20 2020 203e 3e3e 2058 203d 205b 5b2d  .    >>> X = [[-
+00001b80: 322c 2031 2c20 325d 2c20 5b2d 312c 2030  2, 1, 2], [-1, 0
+00001b90: 2c20 315d 5d0a 2020 2020 3e3e 3e20 7363  , 1]].    >>> sc
+00001ba0: 616c 6528 582c 2061 7869 733d 3029 2020  ale(X, axis=0)  
+00001bb0: 2320 7363 616c 696e 6720 6561 6368 2063  # scaling each c
+00001bc0: 6f6c 756d 6e20 696e 6465 7065 6e64 656e  olumn independen
+00001bd0: 746c 790a 2020 2020 6172 7261 7928 5b5b  tly.    array([[
+00001be0: 2d31 2e2c 2020 312e 2c20 2031 2e5d 2c0a  -1.,  1.,  1.],.
+00001bf0: 2020 2020 2020 2020 2020 205b 2031 2e2c             [ 1.,
+00001c00: 202d 312e 2c20 2d31 2e5d 5d29 0a20 2020   -1., -1.]]).   
+00001c10: 203e 3e3e 2073 6361 6c65 2858 2c20 6178   >>> scale(X, ax
+00001c20: 6973 3d31 2920 2023 2073 6361 6c69 6e67  is=1)  # scaling
+00001c30: 2065 6163 6820 726f 7720 696e 6465 7065   each row indepe
+00001c40: 6e64 656e 746c 790a 2020 2020 6172 7261  ndently.    arra
+00001c50: 7928 5b5b 2d31 2e33 372e 2e2e 2c20 2030  y([[-1.37...,  0
+00001c60: 2e33 392e 2e2e 2c20 2030 2e39 382e 2e2e  .39...,  0.98...
+00001c70: 5d2c 0a20 2020 2020 2020 2020 2020 5b2d  ],.           [-
+00001c80: 312e 3232 2e2e 2e2c 2020 302e 2020 2020  1.22...,  0.    
+00001c90: 202c 2020 312e 3232 2e2e 2e5d 5d29 0a20   ,  1.22...]]). 
+00001ca0: 2020 2022 2222 0a20 2020 2058 203d 2063     """.    X = c
+00001cb0: 6865 636b 5f61 7272 6179 280a 2020 2020  heck_array(.    
+00001cc0: 2020 2020 582c 0a20 2020 2020 2020 2061      X,.        a
+00001cd0: 6363 6570 745f 7370 6172 7365 3d22 6373  ccept_sparse="cs
+00001ce0: 6322 2c0a 2020 2020 2020 2020 636f 7079  c",.        copy
+00001cf0: 3d63 6f70 792c 0a20 2020 2020 2020 2065  =copy,.        e
+00001d00: 6e73 7572 655f 3264 3d46 616c 7365 2c0a  nsure_2d=False,.
+00001d10: 2020 2020 2020 2020 6573 7469 6d61 746f          estimato
+00001d20: 723d 2274 6865 2073 6361 6c65 2066 756e  r="the scale fun
+00001d30: 6374 696f 6e22 2c0a 2020 2020 2020 2020  ction",.        
+00001d40: 6474 7970 653d 464c 4f41 545f 4454 5950  dtype=FLOAT_DTYP
+00001d50: 4553 2c0a 2020 2020 2020 2020 666f 7263  ES,.        forc
+00001d60: 655f 616c 6c5f 6669 6e69 7465 3d22 616c  e_all_finite="al
+00001d70: 6c6f 772d 6e61 6e22 2c0a 2020 2020 290a  low-nan",.    ).
+00001d80: 2020 2020 6966 2073 7061 7273 652e 6973      if sparse.is
+00001d90: 7370 6172 7365 2858 293a 0a20 2020 2020  sparse(X):.     
+00001da0: 2020 2069 6620 7769 7468 5f6d 6561 6e3a     if with_mean:
+00001db0: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
+00001dc0: 7365 2056 616c 7565 4572 726f 7228 0a20  se ValueError(. 
+00001dd0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00001de0: 4361 6e6e 6f74 2063 656e 7465 7220 7370  Cannot center sp
+00001df0: 6172 7365 206d 6174 7269 6365 733a 2070  arse matrices: p
+00001e00: 6173 7320 6077 6974 685f 6d65 616e 3d46  ass `with_mean=F
+00001e10: 616c 7365 6020 696e 7374 6561 6422 0a20  alse` instead". 
+00001e20: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00001e30: 2053 6565 2064 6f63 7374 7269 6e67 2066   See docstring f
+00001e40: 6f72 206d 6f74 6976 6174 696f 6e20 616e  or motivation an
+00001e50: 6420 616c 7465 726e 6174 6976 6573 2e22  d alternatives."
+00001e60: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). 
+00001e70: 2020 2020 2020 2069 6620 6178 6973 2021         if axis !
+00001e80: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
+00001e90: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00001ea0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+00001eb0: 2020 2022 4361 6e20 6f6e 6c79 2073 6361     "Can only sca
+00001ec0: 6c65 2073 7061 7273 6520 6d61 7472 6978  le sparse matrix
+00001ed0: 206f 6e20 6178 6973 3d30 2c20 2067 6f74   on axis=0,  got
+00001ee0: 2061 7869 733d 2564 2220 2520 6178 6973   axis=%d" % axis
+00001ef0: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). 
+00001f00: 2020 2020 2020 2069 6620 7769 7468 5f73         if with_s
+00001f10: 7464 3a0a 2020 2020 2020 2020 2020 2020  td:.            
+00001f20: 5f2c 2076 6172 203d 206d 6561 6e5f 7661  _, var = mean_va
+00001f30: 7269 616e 6365 5f61 7869 7328 582c 2061  riance_axis(X, a
+00001f40: 7869 733d 3029 0a20 2020 2020 2020 2020  xis=0).         
+00001f50: 2020 2076 6172 203d 205f 6861 6e64 6c65     var = _handle
+00001f60: 5f7a 6572 6f73 5f69 6e5f 7363 616c 6528  _zeros_in_scale(
+00001f70: 7661 722c 2063 6f70 793d 4661 6c73 6529  var, copy=False)
+00001f80: 0a20 2020 2020 2020 2020 2020 2069 6e70  .            inp
+00001f90: 6c61 6365 5f63 6f6c 756d 6e5f 7363 616c  lace_column_scal
+00001fa0: 6528 582c 2031 202f 206e 702e 7371 7274  e(X, 1 / np.sqrt
+00001fb0: 2876 6172 2929 0a20 2020 2065 6c73 653a  (var)).    else:
+00001fc0: 0a20 2020 2020 2020 2058 203d 206e 702e  .        X = np.
+00001fd0: 6173 6172 7261 7928 5829 0a20 2020 2020  asarray(X).     
+00001fe0: 2020 2069 6620 7769 7468 5f6d 6561 6e3a     if with_mean:
+00001ff0: 0a20 2020 2020 2020 2020 2020 206d 6561  .            mea
+00002000: 6e5f 203d 206e 702e 6e61 6e6d 6561 6e28  n_ = np.nanmean(
+00002010: 582c 2061 7869 7329 0a20 2020 2020 2020  X, axis).       
+00002020: 2069 6620 7769 7468 5f73 7464 3a0a 2020   if with_std:.  
+00002030: 2020 2020 2020 2020 2020 7363 616c 655f            scale_
+00002040: 203d 206e 702e 6e61 6e73 7464 2858 2c20   = np.nanstd(X, 
+00002050: 6178 6973 290a 2020 2020 2020 2020 2320  axis).        # 
+00002060: 5872 2069 7320 6120 7669 6577 206f 6e20  Xr is a view on 
+00002070: 7468 6520 6f72 6967 696e 616c 2061 7272  the original arr
+00002080: 6179 2074 6861 7420 656e 6162 6c65 7320  ay that enables 
+00002090: 6561 7379 2075 7365 206f 660a 2020 2020  easy use of.    
+000020a0: 2020 2020 2320 6272 6f61 6463 6173 7469      # broadcasti
+000020b0: 6e67 206f 6e20 7468 6520 6178 6973 2069  ng on the axis i
+000020c0: 6e20 7768 6963 6820 7765 2061 7265 2069  n which we are i
+000020d0: 6e74 6572 6573 7465 6420 696e 0a20 2020  nterested in.   
+000020e0: 2020 2020 2058 7220 3d20 6e70 2e72 6f6c       Xr = np.rol
+000020f0: 6c61 7869 7328 582c 2061 7869 7329 0a20  laxis(X, axis). 
+00002100: 2020 2020 2020 2069 6620 7769 7468 5f6d         if with_m
+00002110: 6561 6e3a 0a20 2020 2020 2020 2020 2020  ean:.           
+00002120: 2058 7220 2d3d 206d 6561 6e5f 0a20 2020   Xr -= mean_.   
+00002130: 2020 2020 2020 2020 206d 6561 6e5f 3120           mean_1 
+00002140: 3d20 6e70 2e6e 616e 6d65 616e 2858 722c  = np.nanmean(Xr,
+00002150: 2061 7869 733d 3029 0a20 2020 2020 2020   axis=0).       
+00002160: 2020 2020 2023 2056 6572 6966 7920 7468       # Verify th
+00002170: 6174 206d 6561 6e5f 3120 6973 2027 636c  at mean_1 is 'cl
+00002180: 6f73 6520 746f 207a 6572 6f27 2e20 4966  ose to zero'. If
+00002190: 2058 2063 6f6e 7461 696e 7320 7665 7279   X contains very
+000021a0: 0a20 2020 2020 2020 2020 2020 2023 206c  .            # l
+000021b0: 6172 6765 2076 616c 7565 732c 206d 6561  arge values, mea
+000021c0: 6e5f 3120 6361 6e20 616c 736f 2062 6520  n_1 can also be 
+000021d0: 7665 7279 206c 6172 6765 2c20 6475 6520  very large, due 
+000021e0: 746f 2061 206c 6163 6b20 6f66 0a20 2020  to a lack of.   
+000021f0: 2020 2020 2020 2020 2023 2070 7265 6369           # preci
+00002200: 7369 6f6e 206f 6620 6d65 616e 5f2e 2049  sion of mean_. I
+00002210: 6e20 7468 6973 2063 6173 652c 2061 2070  n this case, a p
+00002220: 7265 2d73 6361 6c69 6e67 206f 6620 7468  re-scaling of th
+00002230: 650a 2020 2020 2020 2020 2020 2020 2320  e.            # 
+00002240: 636f 6e63 6572 6e65 6420 6665 6174 7572  concerned featur
+00002250: 6520 6973 2065 6666 6963 6965 6e74 2c20  e is efficient, 
+00002260: 666f 7220 696e 7374 616e 6365 2062 7920  for instance by 
+00002270: 6974 7320 6d65 616e 206f 720a 2020 2020  its mean or.    
+00002280: 2020 2020 2020 2020 2320 6d61 7869 6d75          # maximu
+00002290: 6d2e 0a20 2020 2020 2020 2020 2020 2069  m..            i
+000022a0: 6620 6e6f 7420 6e70 2e61 6c6c 636c 6f73  f not np.allclos
+000022b0: 6528 6d65 616e 5f31 2c20 3029 3a0a 2020  e(mean_1, 0):.  
+000022c0: 2020 2020 2020 2020 2020 2020 2020 7761                wa
+000022d0: 726e 696e 6773 2e77 6172 6e28 0a20 2020  rnings.warn(.   
+000022e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000022f0: 2022 4e75 6d65 7269 6361 6c20 6973 7375   "Numerical issu
+00002300: 6573 2077 6572 6520 656e 636f 756e 7465  es were encounte
+00002310: 7265 6420 220a 2020 2020 2020 2020 2020  red ".          
+00002320: 2020 2020 2020 2020 2020 2277 6865 6e20            "when 
+00002330: 6365 6e74 6572 696e 6720 7468 6520 6461  centering the da
+00002340: 7461 2022 0a20 2020 2020 2020 2020 2020  ta ".           
+00002350: 2020 2020 2020 2020 2022 616e 6420 6d69           "and mi
+00002360: 6768 7420 6e6f 7420 6265 2073 6f6c 7665  ght not be solve
+00002370: 642e 2044 6174 6173 6574 206d 6179 2022  d. Dataset may "
+00002380: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002390: 2020 2020 2022 636f 6e74 6169 6e20 746f       "contain to
+000023a0: 6f20 6c61 7267 6520 7661 6c75 6573 2e20  o large values. 
+000023b0: 596f 7520 6d61 7920 6e65 6564 2022 0a20  You may need ". 
+000023c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000023d0: 2020 2022 746f 2070 7265 7363 616c 6520     "to prescale 
+000023e0: 796f 7572 2066 6561 7475 7265 732e 220a  your features.".
+000023f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002400: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+00002410: 2020 5872 202d 3d20 6d65 616e 5f31 0a20    Xr -= mean_1. 
+00002420: 2020 2020 2020 2069 6620 7769 7468 5f73         if with_s
+00002430: 7464 3a0a 2020 2020 2020 2020 2020 2020  td:.            
+00002440: 7363 616c 655f 203d 205f 6861 6e64 6c65  scale_ = _handle
+00002450: 5f7a 6572 6f73 5f69 6e5f 7363 616c 6528  _zeros_in_scale(
+00002460: 7363 616c 655f 2c20 636f 7079 3d46 616c  scale_, copy=Fal
+00002470: 7365 290a 2020 2020 2020 2020 2020 2020  se).            
+00002480: 5872 202f 3d20 7363 616c 655f 0a20 2020  Xr /= scale_.   
+00002490: 2020 2020 2020 2020 2069 6620 7769 7468           if with
+000024a0: 5f6d 6561 6e3a 0a20 2020 2020 2020 2020  _mean:.         
+000024b0: 2020 2020 2020 206d 6561 6e5f 3220 3d20         mean_2 = 
+000024c0: 6e70 2e6e 616e 6d65 616e 2858 722c 2061  np.nanmean(Xr, a
+000024d0: 7869 733d 3029 0a20 2020 2020 2020 2020  xis=0).         
+000024e0: 2020 2020 2020 2023 2049 6620 6d65 616e         # If mean
+000024f0: 5f32 2069 7320 6e6f 7420 2763 6c6f 7365  _2 is not 'close
+00002500: 2074 6f20 7a65 726f 272c 2069 7420 636f   to zero', it co
+00002510: 6d65 7320 6672 6f6d 2074 6865 2066 6163  mes from the fac
+00002520: 7420 7468 6174 0a20 2020 2020 2020 2020  t that.         
+00002530: 2020 2020 2020 2023 2073 6361 6c65 5f20         # scale_ 
+00002540: 6973 2076 6572 7920 736d 616c 6c20 736f  is very small so
+00002550: 2074 6861 7420 6d65 616e 5f32 203d 206d   that mean_2 = m
+00002560: 6561 6e5f 312f 7363 616c 655f 203e 2030  ean_1/scale_ > 0
+00002570: 2c20 6576 656e 0a20 2020 2020 2020 2020  , even.         
+00002580: 2020 2020 2020 2023 2069 6620 6d65 616e         # if mean
+00002590: 5f31 2077 6173 2063 6c6f 7365 2074 6f20  _1 was close to 
+000025a0: 7a65 726f 2e20 5468 6520 7072 6f62 6c65  zero. The proble
+000025b0: 6d20 6973 2074 6875 7320 6573 7365 6e74  m is thus essent
+000025c0: 6961 6c6c 790a 2020 2020 2020 2020 2020  ially.          
+000025d0: 2020 2020 2020 2320 6475 6520 746f 2074        # due to t
+000025e0: 6865 206c 6163 6b20 6f66 2070 7265 6369  he lack of preci
+000025f0: 7369 6f6e 206f 6620 6d65 616e 5f2e 2041  sion of mean_. A
+00002600: 2073 6f6c 7574 696f 6e20 6973 2074 6865   solution is the
+00002610: 6e20 746f 0a20 2020 2020 2020 2020 2020  n to.           
+00002620: 2020 2020 2023 2073 7562 7472 6163 7420       # subtract 
+00002630: 7468 6520 6d65 616e 2061 6761 696e 3a0a  the mean again:.
+00002640: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002650: 6966 206e 6f74 206e 702e 616c 6c63 6c6f  if not np.allclo
+00002660: 7365 286d 6561 6e5f 322c 2030 293a 0a20  se(mean_2, 0):. 
+00002670: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002680: 2020 2077 6172 6e69 6e67 732e 7761 726e     warnings.warn
+00002690: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+000026a0: 2020 2020 2020 2020 2020 224e 756d 6572            "Numer
+000026b0: 6963 616c 2069 7373 7565 7320 7765 7265  ical issues were
+000026c0: 2065 6e63 6f75 6e74 6572 6564 2022 0a20   encountered ". 
+000026d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000026e0: 2020 2020 2020 2022 7768 656e 2073 6361         "when sca
+000026f0: 6c69 6e67 2074 6865 2064 6174 6120 220a  ling the data ".
+00002700: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002710: 2020 2020 2020 2020 2261 6e64 206d 6967          "and mig
+00002720: 6874 206e 6f74 2062 6520 736f 6c76 6564  ht not be solved
+00002730: 2e20 5468 6520 7374 616e 6461 7264 2022  . The standard "
+00002740: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002750: 2020 2020 2020 2020 2022 6465 7669 6174           "deviat
+00002760: 696f 6e20 6f66 2074 6865 2064 6174 6120  ion of the data 
+00002770: 6973 2070 726f 6261 626c 7920 220a 2020  is probably ".  
+00002780: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002790: 2020 2020 2020 2276 6572 7920 636c 6f73        "very clos
+000027a0: 6520 746f 2030 2e20 220a 2020 2020 2020  e to 0. ".      
+000027b0: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+000027c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000027d0: 2020 2020 5872 202d 3d20 6d65 616e 5f32      Xr -= mean_2
+000027e0: 0a20 2020 2072 6574 7572 6e20 580a 0a0a  .    return X...
+000027f0: 636c 6173 7320 4d69 6e4d 6178 5363 616c  class MinMaxScal
+00002800: 6572 284f 6e65 546f 4f6e 6546 6561 7475  er(OneToOneFeatu
+00002810: 7265 4d69 7869 6e2c 2054 7261 6e73 666f  reMixin, Transfo
+00002820: 726d 6572 4d69 7869 6e2c 2042 6173 6545  rmerMixin, BaseE
+00002830: 7374 696d 6174 6f72 293a 0a20 2020 2022  stimator):.    "
+00002840: 2222 5472 616e 7366 6f72 6d20 6665 6174  ""Transform feat
+00002850: 7572 6573 2062 7920 7363 616c 696e 6720  ures by scaling 
+00002860: 6561 6368 2066 6561 7475 7265 2074 6f20  each feature to 
+00002870: 6120 6769 7665 6e20 7261 6e67 652e 0a0a  a given range...
+00002880: 2020 2020 5468 6973 2065 7374 696d 6174      This estimat
+00002890: 6f72 2073 6361 6c65 7320 616e 6420 7472  or scales and tr
+000028a0: 616e 736c 6174 6573 2065 6163 6820 6665  anslates each fe
+000028b0: 6174 7572 6520 696e 6469 7669 6475 616c  ature individual
+000028c0: 6c79 2073 7563 680a 2020 2020 7468 6174  ly such.    that
+000028d0: 2069 7420 6973 2069 6e20 7468 6520 6769   it is in the gi
+000028e0: 7665 6e20 7261 6e67 6520 6f6e 2074 6865  ven range on the
+000028f0: 2074 7261 696e 696e 6720 7365 742c 2065   training set, e
+00002900: 2e67 2e20 6265 7477 6565 6e0a 2020 2020  .g. between.    
+00002910: 7a65 726f 2061 6e64 206f 6e65 2e0a 0a20  zero and one... 
+00002920: 2020 2054 6865 2074 7261 6e73 666f 726d     The transform
+00002930: 6174 696f 6e20 6973 2067 6976 656e 2062  ation is given b
+00002940: 793a 3a0a 0a20 2020 2020 2020 2058 5f73  y::..        X_s
+00002950: 7464 203d 2028 5820 2d20 582e 6d69 6e28  td = (X - X.min(
+00002960: 6178 6973 3d30 2929 202f 2028 582e 6d61  axis=0)) / (X.ma
+00002970: 7828 6178 6973 3d30 2920 2d20 582e 6d69  x(axis=0) - X.mi
+00002980: 6e28 6178 6973 3d30 2929 0a20 2020 2020  n(axis=0)).     
+00002990: 2020 2058 5f73 6361 6c65 6420 3d20 585f     X_scaled = X_
+000029a0: 7374 6420 2a20 286d 6178 202d 206d 696e  std * (max - min
+000029b0: 2920 2b20 6d69 6e0a 0a20 2020 2077 6865  ) + min..    whe
+000029c0: 7265 206d 696e 2c20 6d61 7820 3d20 6665  re min, max = fe
+000029d0: 6174 7572 655f 7261 6e67 652e 0a0a 2020  ature_range...  
+000029e0: 2020 5468 6973 2074 7261 6e73 666f 726d    This transform
+000029f0: 6174 696f 6e20 6973 206f 6674 656e 2075  ation is often u
+00002a00: 7365 6420 6173 2061 6e20 616c 7465 726e  sed as an altern
+00002a10: 6174 6976 6520 746f 207a 6572 6f20 6d65  ative to zero me
+00002a20: 616e 2c0a 2020 2020 756e 6974 2076 6172  an,.    unit var
+00002a30: 6961 6e63 6520 7363 616c 696e 672e 0a0a  iance scaling...
+00002a40: 2020 2020 604d 696e 4d61 7853 6361 6c65      `MinMaxScale
+00002a50: 7260 2064 6f65 736e 2774 2072 6564 7563  r` doesn't reduc
+00002a60: 6520 7468 6520 6566 6665 6374 206f 6620  e the effect of 
+00002a70: 6f75 746c 6965 7273 2c20 6275 7420 6974  outliers, but it
+00002a80: 206c 696e 6561 726c 790a 2020 2020 7363   linearly.    sc
+00002a90: 616c 6573 2074 6865 6d20 646f 776e 2069  ales them down i
+00002aa0: 6e74 6f20 6120 6669 7865 6420 7261 6e67  nto a fixed rang
+00002ab0: 652c 2077 6865 7265 2074 6865 206c 6172  e, where the lar
+00002ac0: 6765 7374 206f 6363 7572 7269 6e67 2064  gest occurring d
+00002ad0: 6174 6120 706f 696e 740a 2020 2020 636f  ata point.    co
+00002ae0: 7272 6573 706f 6e64 7320 746f 2074 6865  rresponds to the
+00002af0: 206d 6178 696d 756d 2076 616c 7565 2061   maximum value a
+00002b00: 6e64 2074 6865 2073 6d61 6c6c 6573 7420  nd the smallest 
+00002b10: 6f6e 6520 636f 7272 6573 706f 6e64 7320  one corresponds 
+00002b20: 746f 2074 6865 0a20 2020 206d 696e 696d  to the.    minim
+00002b30: 756d 2076 616c 7565 2e20 466f 7220 616e  um value. For an
+00002b40: 2065 7861 6d70 6c65 2076 6973 7561 6c69   example visuali
+00002b50: 7a61 7469 6f6e 2c20 7265 6665 7220 746f  zation, refer to
+00002b60: 203a 7265 663a 6043 6f6d 7061 7265 0a20   :ref:`Compare. 
+00002b70: 2020 204d 696e 4d61 7853 6361 6c65 7220     MinMaxScaler 
+00002b80: 7769 7468 206f 7468 6572 2073 6361 6c65  with other scale
+00002b90: 7273 203c 706c 6f74 5f61 6c6c 5f73 6361  rs <plot_all_sca
+00002ba0: 6c69 6e67 5f6d 696e 6d61 785f 7363 616c  ling_minmax_scal
+00002bb0: 6572 5f73 6563 7469 6f6e 3e60 2e0a 0a20  er_section>`... 
+00002bc0: 2020 2052 6561 6420 6d6f 7265 2069 6e20     Read more in 
+00002bd0: 7468 6520 3a72 6566 3a60 5573 6572 2047  the :ref:`User G
+00002be0: 7569 6465 203c 7072 6570 726f 6365 7373  uide <preprocess
+00002bf0: 696e 675f 7363 616c 6572 3e60 2e0a 0a20  ing_scaler>`... 
+00002c00: 2020 2050 6172 616d 6574 6572 730a 2020     Parameters.  
+00002c10: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020    ----------.   
+00002c20: 2066 6561 7475 7265 5f72 616e 6765 203a   feature_range :
+00002c30: 2074 7570 6c65 2028 6d69 6e2c 206d 6178   tuple (min, max
+00002c40: 292c 2064 6566 6175 6c74 3d28 302c 2031  ), default=(0, 1
+00002c50: 290a 2020 2020 2020 2020 4465 7369 7265  ).        Desire
+00002c60: 6420 7261 6e67 6520 6f66 2074 7261 6e73  d range of trans
+00002c70: 666f 726d 6564 2064 6174 612e 0a0a 2020  formed data...  
+00002c80: 2020 636f 7079 203a 2062 6f6f 6c2c 2064    copy : bool, d
+00002c90: 6566 6175 6c74 3d54 7275 650a 2020 2020  efault=True.    
+00002ca0: 2020 2020 5365 7420 746f 2046 616c 7365      Set to False
+00002cb0: 2074 6f20 7065 7266 6f72 6d20 696e 706c   to perform inpl
+00002cc0: 6163 6520 726f 7720 6e6f 726d 616c 697a  ace row normaliz
+00002cd0: 6174 696f 6e20 616e 6420 6176 6f69 6420  ation and avoid 
+00002ce0: 610a 2020 2020 2020 2020 636f 7079 2028  a.        copy (
+00002cf0: 6966 2074 6865 2069 6e70 7574 2069 7320  if the input is 
+00002d00: 616c 7265 6164 7920 6120 6e75 6d70 7920  already a numpy 
+00002d10: 6172 7261 7929 2e0a 0a20 2020 2063 6c69  array)...    cli
+00002d20: 7020 3a20 626f 6f6c 2c20 6465 6661 756c  p : bool, defaul
+00002d30: 743d 4661 6c73 650a 2020 2020 2020 2020  t=False.        
+00002d40: 5365 7420 746f 2054 7275 6520 746f 2063  Set to True to c
+00002d50: 6c69 7020 7472 616e 7366 6f72 6d65 6420  lip transformed 
+00002d60: 7661 6c75 6573 206f 6620 6865 6c64 2d6f  values of held-o
+00002d70: 7574 2064 6174 6120 746f 0a20 2020 2020  ut data to.     
+00002d80: 2020 2070 726f 7669 6465 6420 6066 6561     provided `fea
+00002d90: 7475 7265 2072 616e 6765 602e 0a0a 2020  ture range`...  
+00002da0: 2020 2020 2020 2e2e 2076 6572 7369 6f6e        .. version
+00002db0: 6164 6465 643a 3a20 302e 3234 0a0a 2020  added:: 0.24..  
+00002dc0: 2020 4174 7472 6962 7574 6573 0a20 2020    Attributes.   
+00002dd0: 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020   ----------.    
+00002de0: 6d69 6e5f 203a 206e 6461 7272 6179 206f  min_ : ndarray o
+00002df0: 6620 7368 6170 6520 286e 5f66 6561 7475  f shape (n_featu
+00002e00: 7265 732c 290a 2020 2020 2020 2020 5065  res,).        Pe
+00002e10: 7220 6665 6174 7572 6520 6164 6a75 7374  r feature adjust
+00002e20: 6d65 6e74 2066 6f72 206d 696e 696d 756d  ment for minimum
+00002e30: 2e20 4571 7569 7661 6c65 6e74 2074 6f0a  . Equivalent to.
+00002e40: 2020 2020 2020 2020 6060 6d69 6e20 2d20          ``min - 
+00002e50: 582e 6d69 6e28 6178 6973 3d30 2920 2a20  X.min(axis=0) * 
+00002e60: 7365 6c66 2e73 6361 6c65 5f60 600a 0a20  self.scale_``.. 
+00002e70: 2020 2073 6361 6c65 5f20 3a20 6e64 6172     scale_ : ndar
+00002e80: 7261 7920 6f66 2073 6861 7065 2028 6e5f  ray of shape (n_
+00002e90: 6665 6174 7572 6573 2c29 0a20 2020 2020  features,).     
+00002ea0: 2020 2050 6572 2066 6561 7475 7265 2072     Per feature r
+00002eb0: 656c 6174 6976 6520 7363 616c 696e 6720  elative scaling 
+00002ec0: 6f66 2074 6865 2064 6174 612e 2045 7175  of the data. Equ
+00002ed0: 6976 616c 656e 7420 746f 0a20 2020 2020  ivalent to.     
+00002ee0: 2020 2060 6028 6d61 7820 2d20 6d69 6e29     ``(max - min)
+00002ef0: 202f 2028 582e 6d61 7828 6178 6973 3d30   / (X.max(axis=0
+00002f00: 2920 2d20 582e 6d69 6e28 6178 6973 3d30  ) - X.min(axis=0
+00002f10: 2929 6060 0a0a 2020 2020 2020 2020 2e2e  ))``..        ..
+00002f20: 2076 6572 7369 6f6e 6164 6465 643a 3a20   versionadded:: 
+00002f30: 302e 3137 0a20 2020 2020 2020 2020 2020  0.17.           
+00002f40: 2a73 6361 6c65 5f2a 2061 7474 7269 6275  *scale_* attribu
+00002f50: 7465 2e0a 0a20 2020 2064 6174 615f 6d69  te...    data_mi
+00002f60: 6e5f 203a 206e 6461 7272 6179 206f 6620  n_ : ndarray of 
+00002f70: 7368 6170 6520 286e 5f66 6561 7475 7265  shape (n_feature
+00002f80: 732c 290a 2020 2020 2020 2020 5065 7220  s,).        Per 
+00002f90: 6665 6174 7572 6520 6d69 6e69 6d75 6d20  feature minimum 
+00002fa0: 7365 656e 2069 6e20 7468 6520 6461 7461  seen in the data
+00002fb0: 0a0a 2020 2020 2020 2020 2e2e 2076 6572  ..        .. ver
+00002fc0: 7369 6f6e 6164 6465 643a 3a20 302e 3137  sionadded:: 0.17
+00002fd0: 0a20 2020 2020 2020 2020 2020 2a64 6174  .           *dat
+00002fe0: 615f 6d69 6e5f 2a0a 0a20 2020 2064 6174  a_min_*..    dat
+00002ff0: 615f 6d61 785f 203a 206e 6461 7272 6179  a_max_ : ndarray
+00003000: 206f 6620 7368 6170 6520 286e 5f66 6561   of shape (n_fea
+00003010: 7475 7265 732c 290a 2020 2020 2020 2020  tures,).        
+00003020: 5065 7220 6665 6174 7572 6520 6d61 7869  Per feature maxi
+00003030: 6d75 6d20 7365 656e 2069 6e20 7468 6520  mum seen in the 
+00003040: 6461 7461 0a0a 2020 2020 2020 2020 2e2e  data..        ..
+00003050: 2076 6572 7369 6f6e 6164 6465 643a 3a20   versionadded:: 
+00003060: 302e 3137 0a20 2020 2020 2020 2020 2020  0.17.           
+00003070: 2a64 6174 615f 6d61 785f 2a0a 0a20 2020  *data_max_*..   
+00003080: 2064 6174 615f 7261 6e67 655f 203a 206e   data_range_ : n
+00003090: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
+000030a0: 286e 5f66 6561 7475 7265 732c 290a 2020  (n_features,).  
+000030b0: 2020 2020 2020 5065 7220 6665 6174 7572        Per featur
+000030c0: 6520 7261 6e67 6520 6060 2864 6174 615f  e range ``(data_
+000030d0: 6d61 785f 202d 2064 6174 615f 6d69 6e5f  max_ - data_min_
+000030e0: 2960 6020 7365 656e 2069 6e20 7468 6520  )`` seen in the 
+000030f0: 6461 7461 0a0a 2020 2020 2020 2020 2e2e  data..        ..
+00003100: 2076 6572 7369 6f6e 6164 6465 643a 3a20   versionadded:: 
+00003110: 302e 3137 0a20 2020 2020 2020 2020 2020  0.17.           
+00003120: 2a64 6174 615f 7261 6e67 655f 2a0a 0a20  *data_range_*.. 
+00003130: 2020 206e 5f66 6561 7475 7265 735f 696e     n_features_in
+00003140: 5f20 3a20 696e 740a 2020 2020 2020 2020  _ : int.        
+00003150: 4e75 6d62 6572 206f 6620 6665 6174 7572  Number of featur
+00003160: 6573 2073 6565 6e20 6475 7269 6e67 203a  es seen during :
+00003170: 7465 726d 3a60 6669 7460 2e0a 0a20 2020  term:`fit`...   
+00003180: 2020 2020 202e 2e20 7665 7273 696f 6e61       .. versiona
+00003190: 6464 6564 3a3a 2030 2e32 340a 0a20 2020  dded:: 0.24..   
+000031a0: 206e 5f73 616d 706c 6573 5f73 6565 6e5f   n_samples_seen_
+000031b0: 203a 2069 6e74 0a20 2020 2020 2020 2054   : int.        T
+000031c0: 6865 206e 756d 6265 7220 6f66 2073 616d  he number of sam
+000031d0: 706c 6573 2070 726f 6365 7373 6564 2062  ples processed b
+000031e0: 7920 7468 6520 6573 7469 6d61 746f 722e  y the estimator.
+000031f0: 0a20 2020 2020 2020 2049 7420 7769 6c6c  .        It will
+00003200: 2062 6520 7265 7365 7420 6f6e 206e 6577   be reset on new
+00003210: 2063 616c 6c73 2074 6f20 6669 742c 2062   calls to fit, b
+00003220: 7574 2069 6e63 7265 6d65 6e74 7320 6163  ut increments ac
+00003230: 726f 7373 0a20 2020 2020 2020 2060 6070  ross.        ``p
+00003240: 6172 7469 616c 5f66 6974 6060 2063 616c  artial_fit`` cal
+00003250: 6c73 2e0a 0a20 2020 2066 6561 7475 7265  ls...    feature
+00003260: 5f6e 616d 6573 5f69 6e5f 203a 206e 6461  _names_in_ : nda
+00003270: 7272 6179 206f 6620 7368 6170 6520 2860  rray of shape (`
+00003280: 6e5f 6665 6174 7572 6573 5f69 6e5f 602c  n_features_in_`,
+00003290: 290a 2020 2020 2020 2020 4e61 6d65 7320  ).        Names 
+000032a0: 6f66 2066 6561 7475 7265 7320 7365 656e  of features seen
+000032b0: 2064 7572 696e 6720 3a74 6572 6d3a 6066   during :term:`f
+000032c0: 6974 602e 2044 6566 696e 6564 206f 6e6c  it`. Defined onl
+000032d0: 7920 7768 656e 2060 5860 0a20 2020 2020  y when `X`.     
+000032e0: 2020 2068 6173 2066 6561 7475 7265 206e     has feature n
+000032f0: 616d 6573 2074 6861 7420 6172 6520 616c  ames that are al
+00003300: 6c20 7374 7269 6e67 732e 0a0a 2020 2020  l strings...    
+00003310: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
+00003320: 6465 643a 3a20 312e 300a 0a20 2020 2053  ded:: 1.0..    S
+00003330: 6565 2041 6c73 6f0a 2020 2020 2d2d 2d2d  ee Also.    ----
+00003340: 2d2d 2d2d 0a20 2020 206d 696e 6d61 785f  ----.    minmax_
+00003350: 7363 616c 6520 3a20 4571 7569 7661 6c65  scale : Equivale
+00003360: 6e74 2066 756e 6374 696f 6e20 7769 7468  nt function with
+00003370: 6f75 7420 7468 6520 6573 7469 6d61 746f  out the estimato
+00003380: 7220 4150 492e 0a0a 2020 2020 4e6f 7465  r API...    Note
+00003390: 730a 2020 2020 2d2d 2d2d 2d0a 2020 2020  s.    -----.    
+000033a0: 4e61 4e73 2061 7265 2074 7265 6174 6564  NaNs are treated
+000033b0: 2061 7320 6d69 7373 696e 6720 7661 6c75   as missing valu
+000033c0: 6573 3a20 6469 7372 6567 6172 6465 6420  es: disregarded 
+000033d0: 696e 2066 6974 2c20 616e 6420 6d61 696e  in fit, and main
+000033e0: 7461 696e 6564 2069 6e0a 2020 2020 7472  tained in.    tr
+000033f0: 616e 7366 6f72 6d2e 0a0a 2020 2020 4578  ansform...    Ex
+00003400: 616d 706c 6573 0a20 2020 202d 2d2d 2d2d  amples.    -----
+00003410: 2d2d 2d0a 2020 2020 3e3e 3e20 6672 6f6d  ---.    >>> from
+00003420: 2073 6b6c 6561 726e 2e70 7265 7072 6f63   sklearn.preproc
+00003430: 6573 7369 6e67 2069 6d70 6f72 7420 4d69  essing import Mi
+00003440: 6e4d 6178 5363 616c 6572 0a20 2020 203e  nMaxScaler.    >
+00003450: 3e3e 2064 6174 6120 3d20 5b5b 2d31 2c20  >> data = [[-1, 
+00003460: 325d 2c20 5b2d 302e 352c 2036 5d2c 205b  2], [-0.5, 6], [
+00003470: 302c 2031 305d 2c20 5b31 2c20 3138 5d5d  0, 10], [1, 18]]
+00003480: 0a20 2020 203e 3e3e 2073 6361 6c65 7220  .    >>> scaler 
+00003490: 3d20 4d69 6e4d 6178 5363 616c 6572 2829  = MinMaxScaler()
+000034a0: 0a20 2020 203e 3e3e 2070 7269 6e74 2873  .    >>> print(s
+000034b0: 6361 6c65 722e 6669 7428 6461 7461 2929  caler.fit(data))
+000034c0: 0a20 2020 204d 696e 4d61 7853 6361 6c65  .    MinMaxScale
+000034d0: 7228 290a 2020 2020 3e3e 3e20 7072 696e  r().    >>> prin
+000034e0: 7428 7363 616c 6572 2e64 6174 615f 6d61  t(scaler.data_ma
+000034f0: 785f 290a 2020 2020 5b20 312e 2031 382e  x_).    [ 1. 18.
+00003500: 5d0a 2020 2020 3e3e 3e20 7072 696e 7428  ].    >>> print(
+00003510: 7363 616c 6572 2e74 7261 6e73 666f 726d  scaler.transform
+00003520: 2864 6174 6129 290a 2020 2020 5b5b 302e  (data)).    [[0.
+00003530: 2020 2030 2e20 205d 0a20 2020 2020 5b30     0.  ].     [0
+00003540: 2e32 3520 302e 3235 5d0a 2020 2020 205b  .25 0.25].     [
+00003550: 302e 3520 2030 2e35 205d 0a20 2020 2020  0.5  0.5 ].     
+00003560: 5b31 2e20 2020 312e 2020 5d5d 0a20 2020  [1.   1.  ]].   
+00003570: 203e 3e3e 2070 7269 6e74 2873 6361 6c65   >>> print(scale
+00003580: 722e 7472 616e 7366 6f72 6d28 5b5b 322c  r.transform([[2,
+00003590: 2032 5d5d 2929 0a20 2020 205b 5b31 2e35   2]])).    [[1.5
+000035a0: 2030 2e20 5d5d 0a20 2020 2022 2222 0a0a   0. ]].    """..
+000035b0: 2020 2020 5f70 6172 616d 6574 6572 5f63      _parameter_c
+000035c0: 6f6e 7374 7261 696e 7473 3a20 6469 6374  onstraints: dict
+000035d0: 203d 207b 0a20 2020 2020 2020 2022 6665   = {.        "fe
+000035e0: 6174 7572 655f 7261 6e67 6522 3a20 5b74  ature_range": [t
+000035f0: 7570 6c65 5d2c 0a20 2020 2020 2020 2022  uple],.        "
+00003600: 636f 7079 223a 205b 2262 6f6f 6c65 616e  copy": ["boolean
+00003610: 225d 2c0a 2020 2020 2020 2020 2263 6c69  "],.        "cli
+00003620: 7022 3a20 5b22 626f 6f6c 6561 6e22 5d2c  p": ["boolean"],
+00003630: 0a20 2020 207d 0a0a 2020 2020 6465 6620  .    }..    def 
+00003640: 5f5f 696e 6974 5f5f 2873 656c 662c 2066  __init__(self, f
+00003650: 6561 7475 7265 5f72 616e 6765 3d28 302c  eature_range=(0,
+00003660: 2031 292c 202a 2c20 636f 7079 3d54 7275   1), *, copy=Tru
+00003670: 652c 2063 6c69 703d 4661 6c73 6529 3a0a  e, clip=False):.
+00003680: 2020 2020 2020 2020 7365 6c66 2e66 6561          self.fea
+00003690: 7475 7265 5f72 616e 6765 203d 2066 6561  ture_range = fea
+000036a0: 7475 7265 5f72 616e 6765 0a20 2020 2020  ture_range.     
+000036b0: 2020 2073 656c 662e 636f 7079 203d 2063     self.copy = c
+000036c0: 6f70 790a 2020 2020 2020 2020 7365 6c66  opy.        self
+000036d0: 2e63 6c69 7020 3d20 636c 6970 0a0a 2020  .clip = clip..  
+000036e0: 2020 6465 6620 5f72 6573 6574 2873 656c    def _reset(sel
+000036f0: 6629 3a0a 2020 2020 2020 2020 2222 2252  f):.        """R
+00003700: 6573 6574 2069 6e74 6572 6e61 6c20 6461  eset internal da
+00003710: 7461 2d64 6570 656e 6465 6e74 2073 7461  ta-dependent sta
+00003720: 7465 206f 6620 7468 6520 7363 616c 6572  te of the scaler
+00003730: 2c20 6966 206e 6563 6573 7361 7279 2e0a  , if necessary..
+00003740: 0a20 2020 2020 2020 205f 5f69 6e69 745f  .        __init_
+00003750: 5f20 7061 7261 6d65 7465 7273 2061 7265  _ parameters are
+00003760: 206e 6f74 2074 6f75 6368 6564 2e0a 2020   not touched..  
+00003770: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
+00003780: 2020 2320 4368 6563 6b69 6e67 206f 6e65    # Checking one
+00003790: 2061 7474 7269 6275 7465 2069 7320 656e   attribute is en
+000037a0: 6f75 6768 2c20 6265 6361 7573 6520 7468  ough, because th
+000037b0: 6579 2061 7265 2061 6c6c 2073 6574 2074  ey are all set t
+000037c0: 6f67 6574 6865 720a 2020 2020 2020 2020  ogether.        
+000037d0: 2320 696e 2070 6172 7469 616c 5f66 6974  # in partial_fit
+000037e0: 0a20 2020 2020 2020 2069 6620 6861 7361  .        if hasa
+000037f0: 7474 7228 7365 6c66 2c20 2273 6361 6c65  ttr(self, "scale
+00003800: 5f22 293a 0a20 2020 2020 2020 2020 2020  _"):.           
+00003810: 2064 656c 2073 656c 662e 7363 616c 655f   del self.scale_
+00003820: 0a20 2020 2020 2020 2020 2020 2064 656c  .            del
+00003830: 2073 656c 662e 6d69 6e5f 0a20 2020 2020   self.min_.     
+00003840: 2020 2020 2020 2064 656c 2073 656c 662e         del self.
+00003850: 6e5f 7361 6d70 6c65 735f 7365 656e 5f0a  n_samples_seen_.
+00003860: 2020 2020 2020 2020 2020 2020 6465 6c20              del 
+00003870: 7365 6c66 2e64 6174 615f 6d69 6e5f 0a20  self.data_min_. 
+00003880: 2020 2020 2020 2020 2020 2064 656c 2073             del s
+00003890: 656c 662e 6461 7461 5f6d 6178 5f0a 2020  elf.data_max_.  
+000038a0: 2020 2020 2020 2020 2020 6465 6c20 7365            del se
+000038b0: 6c66 2e64 6174 615f 7261 6e67 655f 0a0a  lf.data_range_..
+000038c0: 2020 2020 6465 6620 6669 7428 7365 6c66      def fit(self
+000038d0: 2c20 582c 2079 3d4e 6f6e 6529 3a0a 2020  , X, y=None):.  
+000038e0: 2020 2020 2020 2222 2243 6f6d 7075 7465        """Compute
+000038f0: 2074 6865 206d 696e 696d 756d 2061 6e64   the minimum and
+00003900: 206d 6178 696d 756d 2074 6f20 6265 2075   maximum to be u
+00003910: 7365 6420 666f 7220 6c61 7465 7220 7363  sed for later sc
+00003920: 616c 696e 672e 0a0a 2020 2020 2020 2020  aling...        
+00003930: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
+00003940: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
+00003950: 2020 2020 2020 5820 3a20 6172 7261 792d        X : array-
+00003960: 6c69 6b65 206f 6620 7368 6170 6520 286e  like of shape (n
+00003970: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
+00003980: 7572 6573 290a 2020 2020 2020 2020 2020  ures).          
+00003990: 2020 5468 6520 6461 7461 2075 7365 6420    The data used 
+000039a0: 746f 2063 6f6d 7075 7465 2074 6865 2070  to compute the p
+000039b0: 6572 2d66 6561 7475 7265 206d 696e 696d  er-feature minim
+000039c0: 756d 2061 6e64 206d 6178 696d 756d 0a20  um and maximum. 
+000039d0: 2020 2020 2020 2020 2020 2075 7365 6420             used 
+000039e0: 666f 7220 6c61 7465 7220 7363 616c 696e  for later scalin
+000039f0: 6720 616c 6f6e 6720 7468 6520 6665 6174  g along the feat
+00003a00: 7572 6573 2061 7869 732e 0a0a 2020 2020  ures axis...    
+00003a10: 2020 2020 7920 3a20 4e6f 6e65 0a20 2020      y : None.   
+00003a20: 2020 2020 2020 2020 2049 676e 6f72 6564           Ignored
+00003a30: 2e0a 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
+00003a40: 6e73 0a20 2020 2020 2020 202d 2d2d 2d2d  ns.        -----
+00003a50: 2d2d 0a20 2020 2020 2020 2073 656c 6620  --.        self 
+00003a60: 3a20 6f62 6a65 6374 0a20 2020 2020 2020  : object.       
+00003a70: 2020 2020 2046 6974 7465 6420 7363 616c       Fitted scal
+00003a80: 6572 2e0a 2020 2020 2020 2020 2222 220a  er..        """.
+00003a90: 2020 2020 2020 2020 2320 5265 7365 7420          # Reset 
+00003aa0: 696e 7465 726e 616c 2073 7461 7465 2062  internal state b
+00003ab0: 6566 6f72 6520 6669 7474 696e 670a 2020  efore fitting.  
+00003ac0: 2020 2020 2020 7365 6c66 2e5f 7265 7365        self._rese
+00003ad0: 7428 290a 2020 2020 2020 2020 7265 7475  t().        retu
+00003ae0: 726e 2073 656c 662e 7061 7274 6961 6c5f  rn self.partial_
+00003af0: 6669 7428 582c 2079 290a 0a20 2020 2040  fit(X, y)..    @
+00003b00: 5f66 6974 5f63 6f6e 7465 7874 2870 7265  _fit_context(pre
+00003b10: 6665 725f 736b 6970 5f6e 6573 7465 645f  fer_skip_nested_
+00003b20: 7661 6c69 6461 7469 6f6e 3d54 7275 6529  validation=True)
+00003b30: 0a20 2020 2064 6566 2070 6172 7469 616c  .    def partial
+00003b40: 5f66 6974 2873 656c 662c 2058 2c20 793d  _fit(self, X, y=
+00003b50: 4e6f 6e65 293a 0a20 2020 2020 2020 2022  None):.        "
+00003b60: 2222 4f6e 6c69 6e65 2063 6f6d 7075 7461  ""Online computa
+00003b70: 7469 6f6e 206f 6620 6d69 6e20 616e 6420  tion of min and 
+00003b80: 6d61 7820 6f6e 2058 2066 6f72 206c 6174  max on X for lat
+00003b90: 6572 2073 6361 6c69 6e67 2e0a 0a20 2020  er scaling...   
+00003ba0: 2020 2020 2041 6c6c 206f 6620 5820 6973       All of X is
+00003bb0: 2070 726f 6365 7373 6564 2061 7320 6120   processed as a 
+00003bc0: 7369 6e67 6c65 2062 6174 6368 2e20 5468  single batch. Th
+00003bd0: 6973 2069 7320 696e 7465 6e64 6564 2066  is is intended f
+00003be0: 6f72 2063 6173 6573 0a20 2020 2020 2020  or cases.       
+00003bf0: 2077 6865 6e20 3a6d 6574 683a 6066 6974   when :meth:`fit
+00003c00: 6020 6973 206e 6f74 2066 6561 7369 626c  ` is not feasibl
+00003c10: 6520 6475 6520 746f 2076 6572 7920 6c61  e due to very la
+00003c20: 7267 6520 6e75 6d62 6572 206f 660a 2020  rge number of.  
+00003c30: 2020 2020 2020 606e 5f73 616d 706c 6573        `n_samples
+00003c40: 6020 6f72 2062 6563 6175 7365 2058 2069  ` or because X i
+00003c50: 7320 7265 6164 2066 726f 6d20 6120 636f  s read from a co
+00003c60: 6e74 696e 756f 7573 2073 7472 6561 6d2e  ntinuous stream.
+00003c70: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
+00003c80: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
+00003c90: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+00003ca0: 5820 3a20 6172 7261 792d 6c69 6b65 206f  X : array-like o
+00003cb0: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
+00003cc0: 6573 2c20 6e5f 6665 6174 7572 6573 290a  es, n_features).
+00003cd0: 2020 2020 2020 2020 2020 2020 5468 6520              The 
+00003ce0: 6461 7461 2075 7365 6420 746f 2063 6f6d  data used to com
+00003cf0: 7075 7465 2074 6865 206d 6561 6e20 616e  pute the mean an
+00003d00: 6420 7374 616e 6461 7264 2064 6576 6961  d standard devia
+00003d10: 7469 6f6e 0a20 2020 2020 2020 2020 2020  tion.           
+00003d20: 2075 7365 6420 666f 7220 6c61 7465 7220   used for later 
+00003d30: 7363 616c 696e 6720 616c 6f6e 6720 7468  scaling along th
+00003d40: 6520 6665 6174 7572 6573 2061 7869 732e  e features axis.
+00003d50: 0a0a 2020 2020 2020 2020 7920 3a20 4e6f  ..        y : No
+00003d60: 6e65 0a20 2020 2020 2020 2020 2020 2049  ne.            I
+00003d70: 676e 6f72 6564 2e0a 0a20 2020 2020 2020  gnored...       
+00003d80: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
+00003d90: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
+00003da0: 2073 656c 6620 3a20 6f62 6a65 6374 0a20   self : object. 
+00003db0: 2020 2020 2020 2020 2020 2046 6974 7465             Fitte
+00003dc0: 6420 7363 616c 6572 2e0a 2020 2020 2020  d scaler..      
+00003dd0: 2020 2222 220a 2020 2020 2020 2020 6665    """.        fe
+00003de0: 6174 7572 655f 7261 6e67 6520 3d20 7365  ature_range = se
+00003df0: 6c66 2e66 6561 7475 7265 5f72 616e 6765  lf.feature_range
+00003e00: 0a20 2020 2020 2020 2069 6620 6665 6174  .        if feat
+00003e10: 7572 655f 7261 6e67 655b 305d 203e 3d20  ure_range[0] >= 
+00003e20: 6665 6174 7572 655f 7261 6e67 655b 315d  feature_range[1]
+00003e30: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
+00003e40: 6973 6520 5661 6c75 6545 7272 6f72 280a  ise ValueError(.
+00003e50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00003e60: 224d 696e 696d 756d 206f 6620 6465 7369  "Minimum of desi
+00003e70: 7265 6420 6665 6174 7572 6520 7261 6e67  red feature rang
+00003e80: 6520 6d75 7374 2062 6520 736d 616c 6c65  e must be smalle
+00003e90: 7220 7468 616e 206d 6178 696d 756d 2e20  r than maximum. 
+00003ea0: 476f 7420 2573 2e22 0a20 2020 2020 2020  Got %s.".       
+00003eb0: 2020 2020 2020 2020 2025 2073 7472 2866           % str(f
+00003ec0: 6561 7475 7265 5f72 616e 6765 290a 2020  eature_range).  
+00003ed0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
+00003ee0: 2020 2020 2069 6620 7370 6172 7365 2e69       if sparse.i
+00003ef0: 7373 7061 7273 6528 5829 3a0a 2020 2020  ssparse(X):.    
+00003f00: 2020 2020 2020 2020 7261 6973 6520 5479          raise Ty
+00003f10: 7065 4572 726f 7228 0a20 2020 2020 2020  peError(.       
+00003f20: 2020 2020 2020 2020 2022 4d69 6e4d 6178           "MinMax
+00003f30: 5363 616c 6572 2064 6f65 7320 6e6f 7420  Scaler does not 
+00003f40: 7375 7070 6f72 7420 7370 6172 7365 2069  support sparse i
+00003f50: 6e70 7574 2e20 220a 2020 2020 2020 2020  nput. ".        
+00003f60: 2020 2020 2020 2020 2243 6f6e 7369 6465          "Conside
+00003f70: 7220 7573 696e 6720 4d61 7841 6273 5363  r using MaxAbsSc
+00003f80: 616c 6572 2069 6e73 7465 6164 2e22 0a20  aler instead.". 
+00003f90: 2020 2020 2020 2020 2020 2029 0a0a 2020             )..  
+00003fa0: 2020 2020 2020 7870 2c20 5f20 3d20 6765        xp, _ = ge
+00003fb0: 745f 6e61 6d65 7370 6163 6528 5829 0a0a  t_namespace(X)..
+00003fc0: 2020 2020 2020 2020 6669 7273 745f 7061          first_pa
+00003fd0: 7373 203d 206e 6f74 2068 6173 6174 7472  ss = not hasattr
+00003fe0: 2873 656c 662c 2022 6e5f 7361 6d70 6c65  (self, "n_sample
+00003ff0: 735f 7365 656e 5f22 290a 2020 2020 2020  s_seen_").      
+00004000: 2020 5820 3d20 7365 6c66 2e5f 7661 6c69    X = self._vali
+00004010: 6461 7465 5f64 6174 6128 0a20 2020 2020  date_data(.     
+00004020: 2020 2020 2020 2058 2c0a 2020 2020 2020         X,.      
+00004030: 2020 2020 2020 7265 7365 743d 6669 7273        reset=firs
+00004040: 745f 7061 7373 2c0a 2020 2020 2020 2020  t_pass,.        
+00004050: 2020 2020 6474 7970 653d 5f61 7272 6179      dtype=_array
+00004060: 5f61 7069 2e73 7570 706f 7274 6564 5f66  _api.supported_f
+00004070: 6c6f 6174 5f64 7479 7065 7328 7870 292c  loat_dtypes(xp),
+00004080: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
+00004090: 6365 5f61 6c6c 5f66 696e 6974 653d 2261  ce_all_finite="a
+000040a0: 6c6c 6f77 2d6e 616e 222c 0a20 2020 2020  llow-nan",.     
+000040b0: 2020 2029 0a0a 2020 2020 2020 2020 6461     )..        da
+000040c0: 7461 5f6d 696e 203d 205f 6172 7261 795f  ta_min = _array_
+000040d0: 6170 692e 5f6e 616e 6d69 6e28 582c 2061  api._nanmin(X, a
+000040e0: 7869 733d 302c 2078 703d 7870 290a 2020  xis=0, xp=xp).  
+000040f0: 2020 2020 2020 6461 7461 5f6d 6178 203d        data_max =
+00004100: 205f 6172 7261 795f 6170 692e 5f6e 616e   _array_api._nan
+00004110: 6d61 7828 582c 2061 7869 733d 302c 2078  max(X, axis=0, x
+00004120: 703d 7870 290a 0a20 2020 2020 2020 2069  p=xp)..        i
+00004130: 6620 6669 7273 745f 7061 7373 3a0a 2020  f first_pass:.  
+00004140: 2020 2020 2020 2020 2020 7365 6c66 2e6e            self.n
+00004150: 5f73 616d 706c 6573 5f73 6565 6e5f 203d  _samples_seen_ =
+00004160: 2058 2e73 6861 7065 5b30 5d0a 2020 2020   X.shape[0].    
+00004170: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00004180: 2020 2020 2020 6461 7461 5f6d 696e 203d        data_min =
+00004190: 2078 702e 6d69 6e69 6d75 6d28 7365 6c66   xp.minimum(self
+000041a0: 2e64 6174 615f 6d69 6e5f 2c20 6461 7461  .data_min_, data
+000041b0: 5f6d 696e 290a 2020 2020 2020 2020 2020  _min).          
+000041c0: 2020 6461 7461 5f6d 6178 203d 2078 702e    data_max = xp.
+000041d0: 6d61 7869 6d75 6d28 7365 6c66 2e64 6174  maximum(self.dat
+000041e0: 615f 6d61 785f 2c20 6461 7461 5f6d 6178  a_max_, data_max
+000041f0: 290a 2020 2020 2020 2020 2020 2020 7365  ).            se
+00004200: 6c66 2e6e 5f73 616d 706c 6573 5f73 6565  lf.n_samples_see
+00004210: 6e5f 202b 3d20 582e 7368 6170 655b 305d  n_ += X.shape[0]
+00004220: 0a0a 2020 2020 2020 2020 6461 7461 5f72  ..        data_r
+00004230: 616e 6765 203d 2064 6174 615f 6d61 7820  ange = data_max 
+00004240: 2d20 6461 7461 5f6d 696e 0a20 2020 2020  - data_min.     
+00004250: 2020 2073 656c 662e 7363 616c 655f 203d     self.scale_ =
+00004260: 2028 6665 6174 7572 655f 7261 6e67 655b   (feature_range[
+00004270: 315d 202d 2066 6561 7475 7265 5f72 616e  1] - feature_ran
+00004280: 6765 5b30 5d29 202f 205f 6861 6e64 6c65  ge[0]) / _handle
+00004290: 5f7a 6572 6f73 5f69 6e5f 7363 616c 6528  _zeros_in_scale(
+000042a0: 0a20 2020 2020 2020 2020 2020 2064 6174  .            dat
+000042b0: 615f 7261 6e67 652c 2063 6f70 793d 5472  a_range, copy=Tr
+000042c0: 7565 0a20 2020 2020 2020 2029 0a20 2020  ue.        ).   
+000042d0: 2020 2020 2073 656c 662e 6d69 6e5f 203d       self.min_ =
+000042e0: 2066 6561 7475 7265 5f72 616e 6765 5b30   feature_range[0
+000042f0: 5d20 2d20 6461 7461 5f6d 696e 202a 2073  ] - data_min * s
+00004300: 656c 662e 7363 616c 655f 0a20 2020 2020  elf.scale_.     
+00004310: 2020 2073 656c 662e 6461 7461 5f6d 696e     self.data_min
+00004320: 5f20 3d20 6461 7461 5f6d 696e 0a20 2020  _ = data_min.   
+00004330: 2020 2020 2073 656c 662e 6461 7461 5f6d       self.data_m
+00004340: 6178 5f20 3d20 6461 7461 5f6d 6178 0a20  ax_ = data_max. 
+00004350: 2020 2020 2020 2073 656c 662e 6461 7461         self.data
+00004360: 5f72 616e 6765 5f20 3d20 6461 7461 5f72  _range_ = data_r
+00004370: 616e 6765 0a20 2020 2020 2020 2072 6574  ange.        ret
+00004380: 7572 6e20 7365 6c66 0a0a 2020 2020 6465  urn self..    de
+00004390: 6620 7472 616e 7366 6f72 6d28 7365 6c66  f transform(self
+000043a0: 2c20 5829 3a0a 2020 2020 2020 2020 2222  , X):.        ""
+000043b0: 2253 6361 6c65 2066 6561 7475 7265 7320  "Scale features 
+000043c0: 6f66 2058 2061 6363 6f72 6469 6e67 2074  of X according t
+000043d0: 6f20 6665 6174 7572 655f 7261 6e67 652e  o feature_range.
+000043e0: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
+000043f0: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
+00004400: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+00004410: 5820 3a20 6172 7261 792d 6c69 6b65 206f  X : array-like o
+00004420: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
+00004430: 6573 2c20 6e5f 6665 6174 7572 6573 290a  es, n_features).
+00004440: 2020 2020 2020 2020 2020 2020 496e 7075              Inpu
+00004450: 7420 6461 7461 2074 6861 7420 7769 6c6c  t data that will
+00004460: 2062 6520 7472 616e 7366 6f72 6d65 642e   be transformed.
+00004470: 0a0a 2020 2020 2020 2020 5265 7475 726e  ..        Return
+00004480: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
+00004490: 2d0a 2020 2020 2020 2020 5874 203a 206e  -.        Xt : n
+000044a0: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
+000044b0: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
+000044c0: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
+000044d0: 2020 2020 5472 616e 7366 6f72 6d65 6420      Transformed 
+000044e0: 6461 7461 2e0a 2020 2020 2020 2020 2222  data..        ""
+000044f0: 220a 2020 2020 2020 2020 6368 6563 6b5f  ".        check_
+00004500: 6973 5f66 6974 7465 6428 7365 6c66 290a  is_fitted(self).
+00004510: 0a20 2020 2020 2020 2078 702c 205f 203d  .        xp, _ =
+00004520: 2067 6574 5f6e 616d 6573 7061 6365 2858   get_namespace(X
+00004530: 290a 0a20 2020 2020 2020 2058 203d 2073  )..        X = s
+00004540: 656c 662e 5f76 616c 6964 6174 655f 6461  elf._validate_da
+00004550: 7461 280a 2020 2020 2020 2020 2020 2020  ta(.            
+00004560: 582c 0a20 2020 2020 2020 2020 2020 2063  X,.            c
+00004570: 6f70 793d 7365 6c66 2e63 6f70 792c 0a20  opy=self.copy,. 
+00004580: 2020 2020 2020 2020 2020 2064 7479 7065             dtype
+00004590: 3d5f 6172 7261 795f 6170 692e 7375 7070  =_array_api.supp
+000045a0: 6f72 7465 645f 666c 6f61 745f 6474 7970  orted_float_dtyp
+000045b0: 6573 2878 7029 2c0a 2020 2020 2020 2020  es(xp),.        
+000045c0: 2020 2020 666f 7263 655f 616c 6c5f 6669      force_all_fi
+000045d0: 6e69 7465 3d22 616c 6c6f 772d 6e61 6e22  nite="allow-nan"
+000045e0: 2c0a 2020 2020 2020 2020 2020 2020 7265  ,.            re
+000045f0: 7365 743d 4661 6c73 652c 0a20 2020 2020  set=False,.     
+00004600: 2020 2029 0a0a 2020 2020 2020 2020 5820     )..        X 
+00004610: 2a3d 2073 656c 662e 7363 616c 655f 0a20  *= self.scale_. 
+00004620: 2020 2020 2020 2058 202b 3d20 7365 6c66         X += self
+00004630: 2e6d 696e 5f0a 2020 2020 2020 2020 6966  .min_.        if
+00004640: 2073 656c 662e 636c 6970 3a0a 2020 2020   self.clip:.    
+00004650: 2020 2020 2020 2020 7870 2e63 6c69 7028          xp.clip(
+00004660: 582c 2073 656c 662e 6665 6174 7572 655f  X, self.feature_
+00004670: 7261 6e67 655b 305d 2c20 7365 6c66 2e66  range[0], self.f
+00004680: 6561 7475 7265 5f72 616e 6765 5b31 5d2c  eature_range[1],
+00004690: 206f 7574 3d58 290a 2020 2020 2020 2020   out=X).        
+000046a0: 7265 7475 726e 2058 0a0a 2020 2020 6465  return X..    de
+000046b0: 6620 696e 7665 7273 655f 7472 616e 7366  f inverse_transf
+000046c0: 6f72 6d28 7365 6c66 2c20 5829 3a0a 2020  orm(self, X):.  
+000046d0: 2020 2020 2020 2222 2255 6e64 6f20 7468        """Undo th
+000046e0: 6520 7363 616c 696e 6720 6f66 2058 2061  e scaling of X a
+000046f0: 6363 6f72 6469 6e67 2074 6f20 6665 6174  ccording to feat
+00004700: 7572 655f 7261 6e67 652e 0a0a 2020 2020  ure_range...    
+00004710: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
+00004720: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
+00004730: 2d0a 2020 2020 2020 2020 5820 3a20 6172  -.        X : ar
+00004740: 7261 792d 6c69 6b65 206f 6620 7368 6170  ray-like of shap
+00004750: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
+00004760: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
+00004770: 2020 2020 2020 496e 7075 7420 6461 7461        Input data
+00004780: 2074 6861 7420 7769 6c6c 2062 6520 7472   that will be tr
+00004790: 616e 7366 6f72 6d65 642e 2049 7420 6361  ansformed. It ca
+000047a0: 6e6e 6f74 2062 6520 7370 6172 7365 2e0a  nnot be sparse..
+000047b0: 0a20 2020 2020 2020 2052 6574 7572 6e73  .        Returns
+000047c0: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+000047d0: 0a20 2020 2020 2020 2058 7420 3a20 6e64  .        Xt : nd
+000047e0: 6172 7261 7920 6f66 2073 6861 7065 2028  array of shape (
+000047f0: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+00004800: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
+00004810: 2020 2054 7261 6e73 666f 726d 6564 2064     Transformed d
+00004820: 6174 612e 0a20 2020 2020 2020 2022 2222  ata..        """
+00004830: 0a20 2020 2020 2020 2063 6865 636b 5f69  .        check_i
+00004840: 735f 6669 7474 6564 2873 656c 6629 0a0a  s_fitted(self)..
+00004850: 2020 2020 2020 2020 7870 2c20 5f20 3d20          xp, _ = 
+00004860: 6765 745f 6e61 6d65 7370 6163 6528 5829  get_namespace(X)
+00004870: 0a0a 2020 2020 2020 2020 5820 3d20 6368  ..        X = ch
+00004880: 6563 6b5f 6172 7261 7928 0a20 2020 2020  eck_array(.     
+00004890: 2020 2020 2020 2058 2c0a 2020 2020 2020         X,.      
+000048a0: 2020 2020 2020 636f 7079 3d73 656c 662e        copy=self.
+000048b0: 636f 7079 2c0a 2020 2020 2020 2020 2020  copy,.          
+000048c0: 2020 6474 7970 653d 5f61 7272 6179 5f61    dtype=_array_a
+000048d0: 7069 2e73 7570 706f 7274 6564 5f66 6c6f  pi.supported_flo
+000048e0: 6174 5f64 7479 7065 7328 7870 292c 0a20  at_dtypes(xp),. 
+000048f0: 2020 2020 2020 2020 2020 2066 6f72 6365             force
+00004900: 5f61 6c6c 5f66 696e 6974 653d 2261 6c6c  _all_finite="all
+00004910: 6f77 2d6e 616e 222c 0a20 2020 2020 2020  ow-nan",.       
+00004920: 2029 0a0a 2020 2020 2020 2020 5820 2d3d   )..        X -=
+00004930: 2073 656c 662e 6d69 6e5f 0a20 2020 2020   self.min_.     
+00004940: 2020 2058 202f 3d20 7365 6c66 2e73 6361     X /= self.sca
+00004950: 6c65 5f0a 2020 2020 2020 2020 7265 7475  le_.        retu
+00004960: 726e 2058 0a0a 2020 2020 6465 6620 5f6d  rn X..    def _m
+00004970: 6f72 655f 7461 6773 2873 656c 6629 3a0a  ore_tags(self):.
+00004980: 2020 2020 2020 2020 7265 7475 726e 207b          return {
+00004990: 2261 6c6c 6f77 5f6e 616e 223a 2054 7275  "allow_nan": Tru
+000049a0: 657d 0a0a 0a40 7661 6c69 6461 7465 5f70  e}...@validate_p
+000049b0: 6172 616d 7328 0a20 2020 207b 0a20 2020  arams(.    {.   
+000049c0: 2020 2020 2022 5822 3a20 5b22 6172 7261       "X": ["arra
+000049d0: 792d 6c69 6b65 225d 2c0a 2020 2020 2020  y-like"],.      
+000049e0: 2020 2261 7869 7322 3a20 5b4f 7074 696f    "axis": [Optio
+000049f0: 6e73 2849 6e74 6567 7261 6c2c 207b 302c  ns(Integral, {0,
+00004a00: 2031 7d29 5d2c 0a20 2020 207d 2c0a 2020   1})],.    },.  
+00004a10: 2020 7072 6566 6572 5f73 6b69 705f 6e65    prefer_skip_ne
+00004a20: 7374 6564 5f76 616c 6964 6174 696f 6e3d  sted_validation=
+00004a30: 4661 6c73 652c 0a29 0a64 6566 206d 696e  False,.).def min
+00004a40: 6d61 785f 7363 616c 6528 582c 2066 6561  max_scale(X, fea
+00004a50: 7475 7265 5f72 616e 6765 3d28 302c 2031  ture_range=(0, 1
+00004a60: 292c 202a 2c20 6178 6973 3d30 2c20 636f  ), *, axis=0, co
+00004a70: 7079 3d54 7275 6529 3a0a 2020 2020 2222  py=True):.    ""
+00004a80: 2254 7261 6e73 666f 726d 2066 6561 7475  "Transform featu
+00004a90: 7265 7320 6279 2073 6361 6c69 6e67 2065  res by scaling e
+00004aa0: 6163 6820 6665 6174 7572 6520 746f 2061  ach feature to a
+00004ab0: 2067 6976 656e 2072 616e 6765 2e0a 0a20   given range... 
+00004ac0: 2020 2054 6869 7320 6573 7469 6d61 746f     This estimato
+00004ad0: 7220 7363 616c 6573 2061 6e64 2074 7261  r scales and tra
+00004ae0: 6e73 6c61 7465 7320 6561 6368 2066 6561  nslates each fea
+00004af0: 7475 7265 2069 6e64 6976 6964 7561 6c6c  ture individuall
+00004b00: 7920 7375 6368 0a20 2020 2074 6861 7420  y such.    that 
+00004b10: 6974 2069 7320 696e 2074 6865 2067 6976  it is in the giv
+00004b20: 656e 2072 616e 6765 206f 6e20 7468 6520  en range on the 
+00004b30: 7472 6169 6e69 6e67 2073 6574 2c20 692e  training set, i.
+00004b40: 652e 2062 6574 7765 656e 0a20 2020 207a  e. between.    z
+00004b50: 6572 6f20 616e 6420 6f6e 652e 0a0a 2020  ero and one...  
+00004b60: 2020 5468 6520 7472 616e 7366 6f72 6d61    The transforma
+00004b70: 7469 6f6e 2069 7320 6769 7665 6e20 6279  tion is given by
+00004b80: 2028 7768 656e 2060 6061 7869 733d 3060   (when ``axis=0`
+00004b90: 6029 3a3a 0a0a 2020 2020 2020 2020 585f  `)::..        X_
+00004ba0: 7374 6420 3d20 2858 202d 2058 2e6d 696e  std = (X - X.min
+00004bb0: 2861 7869 733d 3029 2920 2f20 2858 2e6d  (axis=0)) / (X.m
+00004bc0: 6178 2861 7869 733d 3029 202d 2058 2e6d  ax(axis=0) - X.m
+00004bd0: 696e 2861 7869 733d 3029 290a 2020 2020  in(axis=0)).    
+00004be0: 2020 2020 585f 7363 616c 6564 203d 2058      X_scaled = X
+00004bf0: 5f73 7464 202a 2028 6d61 7820 2d20 6d69  _std * (max - mi
+00004c00: 6e29 202b 206d 696e 0a0a 2020 2020 7768  n) + min..    wh
+00004c10: 6572 6520 6d69 6e2c 206d 6178 203d 2066  ere min, max = f
+00004c20: 6561 7475 7265 5f72 616e 6765 2e0a 0a20  eature_range... 
+00004c30: 2020 2054 6865 2074 7261 6e73 666f 726d     The transform
+00004c40: 6174 696f 6e20 6973 2063 616c 6375 6c61  ation is calcula
+00004c50: 7465 6420 6173 2028 7768 656e 2060 6061  ted as (when ``a
+00004c60: 7869 733d 3060 6029 3a3a 0a0a 2020 2020  xis=0``)::..    
+00004c70: 2020 2058 5f73 6361 6c65 6420 3d20 7363     X_scaled = sc
+00004c80: 616c 6520 2a20 5820 2b20 6d69 6e20 2d20  ale * X + min - 
+00004c90: 582e 6d69 6e28 6178 6973 3d30 2920 2a20  X.min(axis=0) * 
+00004ca0: 7363 616c 650a 2020 2020 2020 2077 6865  scale.       whe
+00004cb0: 7265 2073 6361 6c65 203d 2028 6d61 7820  re scale = (max 
+00004cc0: 2d20 6d69 6e29 202f 2028 582e 6d61 7828  - min) / (X.max(
+00004cd0: 6178 6973 3d30 2920 2d20 582e 6d69 6e28  axis=0) - X.min(
+00004ce0: 6178 6973 3d30 2929 0a0a 2020 2020 5468  axis=0))..    Th
+00004cf0: 6973 2074 7261 6e73 666f 726d 6174 696f  is transformatio
+00004d00: 6e20 6973 206f 6674 656e 2075 7365 6420  n is often used 
+00004d10: 6173 2061 6e20 616c 7465 726e 6174 6976  as an alternativ
+00004d20: 6520 746f 207a 6572 6f20 6d65 616e 2c0a  e to zero mean,.
+00004d30: 2020 2020 756e 6974 2076 6172 6961 6e63      unit varianc
+00004d40: 6520 7363 616c 696e 672e 0a0a 2020 2020  e scaling...    
+00004d50: 5265 6164 206d 6f72 6520 696e 2074 6865  Read more in the
+00004d60: 203a 7265 663a 6055 7365 7220 4775 6964   :ref:`User Guid
+00004d70: 6520 3c70 7265 7072 6f63 6573 7369 6e67  e <preprocessing
+00004d80: 5f73 6361 6c65 723e 602e 0a0a 2020 2020  _scaler>`...    
+00004d90: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
+00004da0: 3a20 302e 3137 0a20 2020 2020 2020 2a6d  : 0.17.       *m
+00004db0: 696e 6d61 785f 7363 616c 652a 2066 756e  inmax_scale* fun
+00004dc0: 6374 696f 6e20 696e 7465 7266 6163 650a  ction interface.
+00004dd0: 2020 2020 2020 2074 6f20 3a63 6c61 7373         to :class
+00004de0: 3a60 7e73 6b6c 6561 726e 2e70 7265 7072  :`~sklearn.prepr
+00004df0: 6f63 6573 7369 6e67 2e4d 696e 4d61 7853  ocessing.MinMaxS
+00004e00: 6361 6c65 7260 2e0a 0a20 2020 2050 6172  caler`...    Par
+00004e10: 616d 6574 6572 730a 2020 2020 2d2d 2d2d  ameters.    ----
+00004e20: 2d2d 2d2d 2d2d 0a20 2020 2058 203a 2061  ------.    X : a
+00004e30: 7272 6179 2d6c 696b 6520 6f66 2073 6861  rray-like of sha
+00004e40: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
+00004e50: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
+00004e60: 2020 2054 6865 2064 6174 612e 0a0a 2020     The data...  
+00004e70: 2020 6665 6174 7572 655f 7261 6e67 6520    feature_range 
+00004e80: 3a20 7475 706c 6520 286d 696e 2c20 6d61  : tuple (min, ma
+00004e90: 7829 2c20 6465 6661 756c 743d 2830 2c20  x), default=(0, 
+00004ea0: 3129 0a20 2020 2020 2020 2044 6573 6972  1).        Desir
+00004eb0: 6564 2072 616e 6765 206f 6620 7472 616e  ed range of tran
+00004ec0: 7366 6f72 6d65 6420 6461 7461 2e0a 0a20  sformed data... 
+00004ed0: 2020 2061 7869 7320 3a20 7b30 2c20 317d     axis : {0, 1}
+00004ee0: 2c20 6465 6661 756c 743d 300a 2020 2020  , default=0.    
+00004ef0: 2020 2020 4178 6973 2075 7365 6420 746f      Axis used to
+00004f00: 2073 6361 6c65 2061 6c6f 6e67 2e20 4966   scale along. If
+00004f10: 2030 2c20 696e 6465 7065 6e64 656e 746c   0, independentl
+00004f20: 7920 7363 616c 6520 6561 6368 2066 6561  y scale each fea
+00004f30: 7475 7265 2c0a 2020 2020 2020 2020 6f74  ture,.        ot
+00004f40: 6865 7277 6973 6520 2869 6620 3129 2073  herwise (if 1) s
+00004f50: 6361 6c65 2065 6163 6820 7361 6d70 6c65  cale each sample
+00004f60: 2e0a 0a20 2020 2063 6f70 7920 3a20 626f  ...    copy : bo
+00004f70: 6f6c 2c20 6465 6661 756c 743d 5472 7565  ol, default=True
+00004f80: 0a20 2020 2020 2020 2049 6620 4661 6c73  .        If Fals
+00004f90: 652c 2074 7279 2074 6f20 6176 6f69 6420  e, try to avoid 
+00004fa0: 6120 636f 7079 2061 6e64 2073 6361 6c65  a copy and scale
+00004fb0: 2069 6e20 706c 6163 652e 0a20 2020 2020   in place..     
+00004fc0: 2020 2054 6869 7320 6973 206e 6f74 2067     This is not g
+00004fd0: 7561 7261 6e74 6565 6420 746f 2061 6c77  uaranteed to alw
+00004fe0: 6179 7320 776f 726b 2069 6e20 706c 6163  ays work in plac
+00004ff0: 653b 2065 2e67 2e20 6966 2074 6865 2064  e; e.g. if the d
+00005000: 6174 6120 6973 0a20 2020 2020 2020 2061  ata is.        a
+00005010: 206e 756d 7079 2061 7272 6179 2077 6974   numpy array wit
+00005020: 6820 616e 2069 6e74 2064 7479 7065 2c20  h an int dtype, 
+00005030: 6120 636f 7079 2077 696c 6c20 6265 2072  a copy will be r
+00005040: 6574 7572 6e65 6420 6576 656e 2077 6974  eturned even wit
+00005050: 680a 2020 2020 2020 2020 636f 7079 3d46  h.        copy=F
+00005060: 616c 7365 2e0a 0a20 2020 2052 6574 7572  alse...    Retur
+00005070: 6e73 0a20 2020 202d 2d2d 2d2d 2d2d 0a20  ns.    -------. 
+00005080: 2020 2058 5f74 7220 3a20 6e64 6172 7261     X_tr : ndarra
+00005090: 7920 6f66 2073 6861 7065 2028 6e5f 7361  y of shape (n_sa
+000050a0: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
+000050b0: 7329 0a20 2020 2020 2020 2054 6865 2074  s).        The t
+000050c0: 7261 6e73 666f 726d 6564 2064 6174 612e  ransformed data.
+000050d0: 0a0a 2020 2020 2e2e 2077 6172 6e69 6e67  ..    .. warning
+000050e0: 3a3a 2052 6973 6b20 6f66 2064 6174 6120  :: Risk of data 
+000050f0: 6c65 616b 0a0a 2020 2020 2020 2020 446f  leak..        Do
+00005100: 206e 6f74 2075 7365 203a 6675 6e63 3a60   not use :func:`
+00005110: 7e73 6b6c 6561 726e 2e70 7265 7072 6f63  ~sklearn.preproc
+00005120: 6573 7369 6e67 2e6d 696e 6d61 785f 7363  essing.minmax_sc
+00005130: 616c 6560 2075 6e6c 6573 7320 796f 7520  ale` unless you 
+00005140: 6b6e 6f77 0a20 2020 2020 2020 2077 6861  know.        wha
+00005150: 7420 796f 7520 6172 6520 646f 696e 672e  t you are doing.
+00005160: 2041 2063 6f6d 6d6f 6e20 6d69 7374 616b   A common mistak
+00005170: 6520 6973 2074 6f20 6170 706c 7920 6974  e is to apply it
+00005180: 2074 6f20 7468 6520 656e 7469 7265 2064   to the entire d
+00005190: 6174 610a 2020 2020 2020 2020 2a62 6566  ata.        *bef
+000051a0: 6f72 652a 2073 706c 6974 7469 6e67 2069  ore* splitting i
+000051b0: 6e74 6f20 7472 6169 6e69 6e67 2061 6e64  nto training and
+000051c0: 2074 6573 7420 7365 7473 2e20 5468 6973   test sets. This
+000051d0: 2077 696c 6c20 6269 6173 2074 6865 0a20   will bias the. 
+000051e0: 2020 2020 2020 206d 6f64 656c 2065 7661         model eva
+000051f0: 6c75 6174 696f 6e20 6265 6361 7573 6520  luation because 
+00005200: 696e 666f 726d 6174 696f 6e20 776f 756c  information woul
+00005210: 6420 6861 7665 206c 6561 6b65 6420 6672  d have leaked fr
+00005220: 6f6d 2074 6865 2074 6573 740a 2020 2020  om the test.    
+00005230: 2020 2020 7365 7420 746f 2074 6865 2074      set to the t
+00005240: 7261 696e 696e 6720 7365 742e 0a20 2020  raining set..   
+00005250: 2020 2020 2049 6e20 6765 6e65 7261 6c2c       In general,
+00005260: 2077 6520 7265 636f 6d6d 656e 6420 7573   we recommend us
+00005270: 696e 670a 2020 2020 2020 2020 3a63 6c61  ing.        :cla
+00005280: 7373 3a60 7e73 6b6c 6561 726e 2e70 7265  ss:`~sklearn.pre
+00005290: 7072 6f63 6573 7369 6e67 2e4d 696e 4d61  processing.MinMa
+000052a0: 7853 6361 6c65 7260 2077 6974 6869 6e20  xScaler` within 
+000052b0: 610a 2020 2020 2020 2020 3a72 6566 3a60  a.        :ref:`
+000052c0: 5069 7065 6c69 6e65 203c 7069 7065 6c69  Pipeline <pipeli
+000052d0: 6e65 3e60 2069 6e20 6f72 6465 7220 746f  ne>` in order to
+000052e0: 2070 7265 7665 6e74 206d 6f73 7420 7269   prevent most ri
+000052f0: 736b 7320 6f66 2064 6174 610a 2020 2020  sks of data.    
+00005300: 2020 2020 6c65 616b 696e 673a 2060 7069      leaking: `pi
+00005310: 7065 203d 206d 616b 655f 7069 7065 6c69  pe = make_pipeli
+00005320: 6e65 284d 696e 4d61 7853 6361 6c65 7228  ne(MinMaxScaler(
+00005330: 292c 204c 6f67 6973 7469 6352 6567 7265  ), LogisticRegre
+00005340: 7373 696f 6e28 2929 602e 0a0a 2020 2020  ssion())`...    
+00005350: 5365 6520 416c 736f 0a20 2020 202d 2d2d  See Also.    ---
+00005360: 2d2d 2d2d 2d0a 2020 2020 4d69 6e4d 6178  -----.    MinMax
+00005370: 5363 616c 6572 203a 2050 6572 666f 726d  Scaler : Perform
+00005380: 7320 7363 616c 696e 6720 746f 2061 2067  s scaling to a g
+00005390: 6976 656e 2072 616e 6765 2075 7369 6e67  iven range using
+000053a0: 2074 6865 2054 7261 6e73 666f 726d 6572   the Transformer
+000053b0: 0a20 2020 2020 2020 2041 5049 2028 652e  .        API (e.
+000053c0: 672e 2061 7320 7061 7274 206f 6620 6120  g. as part of a 
+000053d0: 7072 6570 726f 6365 7373 696e 670a 2020  preprocessing.  
+000053e0: 2020 2020 2020 3a63 6c61 7373 3a60 7e73        :class:`~s
+000053f0: 6b6c 6561 726e 2e70 6970 656c 696e 652e  klearn.pipeline.
+00005400: 5069 7065 6c69 6e65 6029 2e0a 0a20 2020  Pipeline`)...   
+00005410: 204e 6f74 6573 0a20 2020 202d 2d2d 2d2d   Notes.    -----
+00005420: 0a20 2020 2046 6f72 2061 2063 6f6d 7061  .    For a compa
+00005430: 7269 736f 6e20 6f66 2074 6865 2064 6966  rison of the dif
+00005440: 6665 7265 6e74 2073 6361 6c65 7273 2c20  ferent scalers, 
+00005450: 7472 616e 7366 6f72 6d65 7273 2c20 616e  transformers, an
+00005460: 6420 6e6f 726d 616c 697a 6572 732c 0a20  d normalizers,. 
+00005470: 2020 2073 6565 3a20 3a72 6566 3a60 7370     see: :ref:`sp
+00005480: 6878 5f67 6c72 5f61 7574 6f5f 6578 616d  hx_glr_auto_exam
+00005490: 706c 6573 5f70 7265 7072 6f63 6573 7369  ples_preprocessi
+000054a0: 6e67 5f70 6c6f 745f 616c 6c5f 7363 616c  ng_plot_all_scal
+000054b0: 696e 672e 7079 602e 0a0a 2020 2020 4578  ing.py`...    Ex
+000054c0: 616d 706c 6573 0a20 2020 202d 2d2d 2d2d  amples.    -----
+000054d0: 2d2d 2d0a 2020 2020 3e3e 3e20 6672 6f6d  ---.    >>> from
+000054e0: 2073 6b6c 6561 726e 2e70 7265 7072 6f63   sklearn.preproc
+000054f0: 6573 7369 6e67 2069 6d70 6f72 7420 6d69  essing import mi
+00005500: 6e6d 6178 5f73 6361 6c65 0a20 2020 203e  nmax_scale.    >
+00005510: 3e3e 2058 203d 205b 5b2d 322c 2031 2c20  >> X = [[-2, 1, 
+00005520: 325d 2c20 5b2d 312c 2030 2c20 315d 5d0a  2], [-1, 0, 1]].
+00005530: 2020 2020 3e3e 3e20 6d69 6e6d 6178 5f73      >>> minmax_s
+00005540: 6361 6c65 2858 2c20 6178 6973 3d30 2920  cale(X, axis=0) 
+00005550: 2023 2073 6361 6c65 2065 6163 6820 636f   # scale each co
+00005560: 6c75 6d6e 2069 6e64 6570 656e 6465 6e74  lumn independent
+00005570: 6c79 0a20 2020 2061 7272 6179 285b 5b30  ly.    array([[0
+00005580: 2e2c 2031 2e2c 2031 2e5d 2c0a 2020 2020  ., 1., 1.],.    
+00005590: 2020 2020 2020 205b 312e 2c20 302e 2c20         [1., 0., 
+000055a0: 302e 5d5d 290a 2020 2020 3e3e 3e20 6d69  0.]]).    >>> mi
+000055b0: 6e6d 6178 5f73 6361 6c65 2858 2c20 6178  nmax_scale(X, ax
+000055c0: 6973 3d31 2920 2023 2073 6361 6c65 2065  is=1)  # scale e
+000055d0: 6163 6820 726f 7720 696e 6465 7065 6e64  ach row independ
+000055e0: 656e 746c 790a 2020 2020 6172 7261 7928  ently.    array(
+000055f0: 5b5b 302e 2020 2c20 302e 3735 2c20 312e  [[0.  , 0.75, 1.
+00005600: 2020 5d2c 0a20 2020 2020 2020 2020 2020    ],.           
+00005610: 5b30 2e20 202c 2030 2e35 202c 2031 2e20  [0.  , 0.5 , 1. 
+00005620: 205d 5d29 0a20 2020 2022 2222 0a20 2020   ]]).    """.   
+00005630: 2023 2055 6e6c 696b 6520 7468 6520 7363   # Unlike the sc
+00005640: 616c 6572 206f 626a 6563 742c 2074 6869  aler object, thi
+00005650: 7320 6675 6e63 7469 6f6e 2061 6c6c 6f77  s function allow
+00005660: 7320 3164 2069 6e70 7574 2e0a 2020 2020  s 1d input..    
+00005670: 2320 4966 2063 6f70 7920 6973 2072 6571  # If copy is req
+00005680: 7569 7265 642c 2069 7420 7769 6c6c 2062  uired, it will b
+00005690: 6520 646f 6e65 2069 6e73 6964 6520 7468  e done inside th
+000056a0: 6520 7363 616c 6572 206f 626a 6563 742e  e scaler object.
+000056b0: 0a20 2020 2058 203d 2063 6865 636b 5f61  .    X = check_a
+000056c0: 7272 6179 280a 2020 2020 2020 2020 582c  rray(.        X,
+000056d0: 2063 6f70 793d 4661 6c73 652c 2065 6e73   copy=False, ens
+000056e0: 7572 655f 3264 3d46 616c 7365 2c20 6474  ure_2d=False, dt
+000056f0: 7970 653d 464c 4f41 545f 4454 5950 4553  ype=FLOAT_DTYPES
+00005700: 2c20 666f 7263 655f 616c 6c5f 6669 6e69  , force_all_fini
+00005710: 7465 3d22 616c 6c6f 772d 6e61 6e22 0a20  te="allow-nan". 
+00005720: 2020 2029 0a20 2020 206f 7269 6769 6e61     ).    origina
+00005730: 6c5f 6e64 696d 203d 2058 2e6e 6469 6d0a  l_ndim = X.ndim.
+00005740: 0a20 2020 2069 6620 6f72 6967 696e 616c  .    if original
+00005750: 5f6e 6469 6d20 3d3d 2031 3a0a 2020 2020  _ndim == 1:.    
+00005760: 2020 2020 5820 3d20 582e 7265 7368 6170      X = X.reshap
+00005770: 6528 582e 7368 6170 655b 305d 2c20 3129  e(X.shape[0], 1)
+00005780: 0a0a 2020 2020 7320 3d20 4d69 6e4d 6178  ..    s = MinMax
+00005790: 5363 616c 6572 2866 6561 7475 7265 5f72  Scaler(feature_r
+000057a0: 616e 6765 3d66 6561 7475 7265 5f72 616e  ange=feature_ran
+000057b0: 6765 2c20 636f 7079 3d63 6f70 7929 0a20  ge, copy=copy). 
+000057c0: 2020 2069 6620 6178 6973 203d 3d20 303a     if axis == 0:
+000057d0: 0a20 2020 2020 2020 2058 203d 2073 2e66  .        X = s.f
+000057e0: 6974 5f74 7261 6e73 666f 726d 2858 290a  it_transform(X).
+000057f0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00005800: 2020 5820 3d20 732e 6669 745f 7472 616e    X = s.fit_tran
+00005810: 7366 6f72 6d28 582e 5429 2e54 0a0a 2020  sform(X.T).T..  
+00005820: 2020 6966 206f 7269 6769 6e61 6c5f 6e64    if original_nd
+00005830: 696d 203d 3d20 313a 0a20 2020 2020 2020  im == 1:.       
+00005840: 2058 203d 2058 2e72 6176 656c 2829 0a0a   X = X.ravel()..
+00005850: 2020 2020 7265 7475 726e 2058 0a0a 0a63      return X...c
+00005860: 6c61 7373 2053 7461 6e64 6172 6453 6361  lass StandardSca
+00005870: 6c65 7228 4f6e 6554 6f4f 6e65 4665 6174  ler(OneToOneFeat
+00005880: 7572 654d 6978 696e 2c20 5472 616e 7366  ureMixin, Transf
+00005890: 6f72 6d65 724d 6978 696e 2c20 4261 7365  ormerMixin, Base
+000058a0: 4573 7469 6d61 746f 7229 3a0a 2020 2020  Estimator):.    
+000058b0: 2222 2253 7461 6e64 6172 6469 7a65 2066  """Standardize f
+000058c0: 6561 7475 7265 7320 6279 2072 656d 6f76  eatures by remov
+000058d0: 696e 6720 7468 6520 6d65 616e 2061 6e64  ing the mean and
+000058e0: 2073 6361 6c69 6e67 2074 6f20 756e 6974   scaling to unit
+000058f0: 2076 6172 6961 6e63 652e 0a0a 2020 2020   variance...    
+00005900: 5468 6520 7374 616e 6461 7264 2073 636f  The standard sco
+00005910: 7265 206f 6620 6120 7361 6d70 6c65 2060  re of a sample `
+00005920: 7860 2069 7320 6361 6c63 756c 6174 6564  x` is calculated
+00005930: 2061 733a 0a0a 2020 2020 2020 2020 7a20   as:..        z 
+00005940: 3d20 2878 202d 2075 2920 2f20 730a 0a20  = (x - u) / s.. 
+00005950: 2020 2077 6865 7265 2060 7560 2069 7320     where `u` is 
+00005960: 7468 6520 6d65 616e 206f 6620 7468 6520  the mean of the 
+00005970: 7472 6169 6e69 6e67 2073 616d 706c 6573  training samples
+00005980: 206f 7220 7a65 726f 2069 6620 6077 6974   or zero if `wit
+00005990: 685f 6d65 616e 3d46 616c 7365 602c 0a20  h_mean=False`,. 
+000059a0: 2020 2061 6e64 2060 7360 2069 7320 7468     and `s` is th
+000059b0: 6520 7374 616e 6461 7264 2064 6576 6961  e standard devia
+000059c0: 7469 6f6e 206f 6620 7468 6520 7472 6169  tion of the trai
+000059d0: 6e69 6e67 2073 616d 706c 6573 206f 7220  ning samples or 
+000059e0: 6f6e 6520 6966 0a20 2020 2060 7769 7468  one if.    `with
+000059f0: 5f73 7464 3d46 616c 7365 602e 0a0a 2020  _std=False`...  
+00005a00: 2020 4365 6e74 6572 696e 6720 616e 6420    Centering and 
+00005a10: 7363 616c 696e 6720 6861 7070 656e 2069  scaling happen i
+00005a20: 6e64 6570 656e 6465 6e74 6c79 206f 6e20  ndependently on 
+00005a30: 6561 6368 2066 6561 7475 7265 2062 7920  each feature by 
+00005a40: 636f 6d70 7574 696e 670a 2020 2020 7468  computing.    th
+00005a50: 6520 7265 6c65 7661 6e74 2073 7461 7469  e relevant stati
+00005a60: 7374 6963 7320 6f6e 2074 6865 2073 616d  stics on the sam
+00005a70: 706c 6573 2069 6e20 7468 6520 7472 6169  ples in the trai
+00005a80: 6e69 6e67 2073 6574 2e20 4d65 616e 2061  ning set. Mean a
+00005a90: 6e64 0a20 2020 2073 7461 6e64 6172 6420  nd.    standard 
+00005aa0: 6465 7669 6174 696f 6e20 6172 6520 7468  deviation are th
+00005ab0: 656e 2073 746f 7265 6420 746f 2062 6520  en stored to be 
+00005ac0: 7573 6564 206f 6e20 6c61 7465 7220 6461  used on later da
+00005ad0: 7461 2075 7369 6e67 0a20 2020 203a 6d65  ta using.    :me
+00005ae0: 7468 3a60 7472 616e 7366 6f72 6d60 2e0a  th:`transform`..
+00005af0: 0a20 2020 2053 7461 6e64 6172 6469 7a61  .    Standardiza
+00005b00: 7469 6f6e 206f 6620 6120 6461 7461 7365  tion of a datase
+00005b10: 7420 6973 2061 2063 6f6d 6d6f 6e20 7265  t is a common re
+00005b20: 7175 6972 656d 656e 7420 666f 7220 6d61  quirement for ma
+00005b30: 6e79 0a20 2020 206d 6163 6869 6e65 206c  ny.    machine l
+00005b40: 6561 726e 696e 6720 6573 7469 6d61 746f  earning estimato
+00005b50: 7273 3a20 7468 6579 206d 6967 6874 2062  rs: they might b
+00005b60: 6568 6176 6520 6261 646c 7920 6966 2074  ehave badly if t
+00005b70: 6865 0a20 2020 2069 6e64 6976 6964 7561  he.    individua
+00005b80: 6c20 6665 6174 7572 6573 2064 6f20 6e6f  l features do no
+00005b90: 7420 6d6f 7265 206f 7220 6c65 7373 206c  t more or less l
+00005ba0: 6f6f 6b20 6c69 6b65 2073 7461 6e64 6172  ook like standar
+00005bb0: 6420 6e6f 726d 616c 6c79 0a20 2020 2064  d normally.    d
+00005bc0: 6973 7472 6962 7574 6564 2064 6174 6120  istributed data 
+00005bd0: 2865 2e67 2e20 4761 7573 7369 616e 2077  (e.g. Gaussian w
+00005be0: 6974 6820 3020 6d65 616e 2061 6e64 2075  ith 0 mean and u
+00005bf0: 6e69 7420 7661 7269 616e 6365 292e 0a0a  nit variance)...
+00005c00: 2020 2020 466f 7220 696e 7374 616e 6365      For instance
+00005c10: 206d 616e 7920 656c 656d 656e 7473 2075   many elements u
+00005c20: 7365 6420 696e 2074 6865 206f 626a 6563  sed in the objec
+00005c30: 7469 7665 2066 756e 6374 696f 6e20 6f66  tive function of
+00005c40: 0a20 2020 2061 206c 6561 726e 696e 6720  .    a learning 
+00005c50: 616c 676f 7269 7468 6d20 2873 7563 6820  algorithm (such 
+00005c60: 6173 2074 6865 2052 4246 206b 6572 6e65  as the RBF kerne
+00005c70: 6c20 6f66 2053 7570 706f 7274 2056 6563  l of Support Vec
+00005c80: 746f 720a 2020 2020 4d61 6368 696e 6573  tor.    Machines
+00005c90: 206f 7220 7468 6520 4c31 2061 6e64 204c   or the L1 and L
+00005ca0: 3220 7265 6775 6c61 7269 7a65 7273 206f  2 regularizers o
+00005cb0: 6620 6c69 6e65 6172 206d 6f64 656c 7329  f linear models)
+00005cc0: 2061 7373 756d 6520 7468 6174 0a20 2020   assume that.   
+00005cd0: 2061 6c6c 2066 6561 7475 7265 7320 6172   all features ar
+00005ce0: 6520 6365 6e74 6572 6564 2061 726f 756e  e centered aroun
+00005cf0: 6420 3020 616e 6420 6861 7665 2076 6172  d 0 and have var
+00005d00: 6961 6e63 6520 696e 2074 6865 2073 616d  iance in the sam
+00005d10: 650a 2020 2020 6f72 6465 722e 2049 6620  e.    order. If 
+00005d20: 6120 6665 6174 7572 6520 6861 7320 6120  a feature has a 
+00005d30: 7661 7269 616e 6365 2074 6861 7420 6973  variance that is
+00005d40: 206f 7264 6572 7320 6f66 206d 6167 6e69   orders of magni
+00005d50: 7475 6465 206c 6172 6765 720a 2020 2020  tude larger.    
+00005d60: 7468 616e 206f 7468 6572 732c 2069 7420  than others, it 
+00005d70: 6d69 6768 7420 646f 6d69 6e61 7465 2074  might dominate t
+00005d80: 6865 206f 626a 6563 7469 7665 2066 756e  he objective fun
+00005d90: 6374 696f 6e20 616e 6420 6d61 6b65 2074  ction and make t
+00005da0: 6865 0a20 2020 2065 7374 696d 6174 6f72  he.    estimator
+00005db0: 2075 6e61 626c 6520 746f 206c 6561 726e   unable to learn
+00005dc0: 2066 726f 6d20 6f74 6865 7220 6665 6174   from other feat
+00005dd0: 7572 6573 2063 6f72 7265 6374 6c79 2061  ures correctly a
+00005de0: 7320 6578 7065 6374 6564 2e0a 0a20 2020  s expected...   
+00005df0: 2060 5374 616e 6461 7264 5363 616c 6572   `StandardScaler
+00005e00: 6020 6973 2073 656e 7369 7469 7665 2074  ` is sensitive t
+00005e10: 6f20 6f75 746c 6965 7273 2c20 616e 6420  o outliers, and 
+00005e20: 7468 6520 6665 6174 7572 6573 206d 6179  the features may
+00005e30: 2073 6361 6c65 0a20 2020 2064 6966 6665   scale.    diffe
+00005e40: 7265 6e74 6c79 2066 726f 6d20 6561 6368  rently from each
+00005e50: 206f 7468 6572 2069 6e20 7468 6520 7072   other in the pr
+00005e60: 6573 656e 6365 206f 6620 6f75 746c 6965  esence of outlie
+00005e70: 7273 2e20 466f 7220 616e 2065 7861 6d70  rs. For an examp
+00005e80: 6c65 0a20 2020 2076 6973 7561 6c69 7a61  le.    visualiza
+00005e90: 7469 6f6e 2c20 7265 6665 7220 746f 203a  tion, refer to :
+00005ea0: 7265 663a 6043 6f6d 7061 7265 2053 7461  ref:`Compare Sta
+00005eb0: 6e64 6172 6453 6361 6c65 7220 7769 7468  ndardScaler with
+00005ec0: 206f 7468 6572 2073 6361 6c65 7273 0a20   other scalers. 
+00005ed0: 2020 203c 706c 6f74 5f61 6c6c 5f73 6361     <plot_all_sca
+00005ee0: 6c69 6e67 5f73 7461 6e64 6172 645f 7363  ling_standard_sc
+00005ef0: 616c 6572 5f73 6563 7469 6f6e 3e60 2e0a  aler_section>`..
+00005f00: 0a20 2020 2054 6869 7320 7363 616c 6572  .    This scaler
+00005f10: 2063 616e 2061 6c73 6f20 6265 2061 7070   can also be app
+00005f20: 6c69 6564 2074 6f20 7370 6172 7365 2043  lied to sparse C
+00005f30: 5352 206f 7220 4353 4320 6d61 7472 6963  SR or CSC matric
+00005f40: 6573 2062 7920 7061 7373 696e 670a 2020  es by passing.  
+00005f50: 2020 6077 6974 685f 6d65 616e 3d46 616c    `with_mean=Fal
+00005f60: 7365 6020 746f 2061 766f 6964 2062 7265  se` to avoid bre
+00005f70: 616b 696e 6720 7468 6520 7370 6172 7369  aking the sparsi
+00005f80: 7479 2073 7472 7563 7475 7265 206f 6620  ty structure of 
+00005f90: 7468 6520 6461 7461 2e0a 0a20 2020 2052  the data...    R
+00005fa0: 6561 6420 6d6f 7265 2069 6e20 7468 6520  ead more in the 
+00005fb0: 3a72 6566 3a60 5573 6572 2047 7569 6465  :ref:`User Guide
+00005fc0: 203c 7072 6570 726f 6365 7373 696e 675f   <preprocessing_
+00005fd0: 7363 616c 6572 3e60 2e0a 0a20 2020 2050  scaler>`...    P
+00005fe0: 6172 616d 6574 6572 730a 2020 2020 2d2d  arameters.    --
+00005ff0: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2063 6f70  --------.    cop
+00006000: 7920 3a20 626f 6f6c 2c20 6465 6661 756c  y : bool, defaul
+00006010: 743d 5472 7565 0a20 2020 2020 2020 2049  t=True.        I
+00006020: 6620 4661 6c73 652c 2074 7279 2074 6f20  f False, try to 
+00006030: 6176 6f69 6420 6120 636f 7079 2061 6e64  avoid a copy and
+00006040: 2064 6f20 696e 706c 6163 6520 7363 616c   do inplace scal
+00006050: 696e 6720 696e 7374 6561 642e 0a20 2020  ing instead..   
+00006060: 2020 2020 2054 6869 7320 6973 206e 6f74       This is not
+00006070: 2067 7561 7261 6e74 6565 6420 746f 2061   guaranteed to a
+00006080: 6c77 6179 7320 776f 726b 2069 6e70 6c61  lways work inpla
+00006090: 6365 3b20 652e 672e 2069 6620 7468 6520  ce; e.g. if the 
+000060a0: 6461 7461 2069 730a 2020 2020 2020 2020  data is.        
+000060b0: 6e6f 7420 6120 4e75 6d50 7920 6172 7261  not a NumPy arra
+000060c0: 7920 6f72 2073 6369 7079 2e73 7061 7273  y or scipy.spars
+000060d0: 6520 4353 5220 6d61 7472 6978 2c20 6120  e CSR matrix, a 
+000060e0: 636f 7079 206d 6179 2073 7469 6c6c 2062  copy may still b
+000060f0: 650a 2020 2020 2020 2020 7265 7475 726e  e.        return
+00006100: 6564 2e0a 0a20 2020 2077 6974 685f 6d65  ed...    with_me
+00006110: 616e 203a 2062 6f6f 6c2c 2064 6566 6175  an : bool, defau
+00006120: 6c74 3d54 7275 650a 2020 2020 2020 2020  lt=True.        
+00006130: 4966 2054 7275 652c 2063 656e 7465 7220  If True, center 
+00006140: 7468 6520 6461 7461 2062 6566 6f72 6520  the data before 
+00006150: 7363 616c 696e 672e 0a20 2020 2020 2020  scaling..       
+00006160: 2054 6869 7320 646f 6573 206e 6f74 2077   This does not w
+00006170: 6f72 6b20 2861 6e64 2077 696c 6c20 7261  ork (and will ra
+00006180: 6973 6520 616e 2065 7863 6570 7469 6f6e  ise an exception
+00006190: 2920 7768 656e 2061 7474 656d 7074 6564  ) when attempted
+000061a0: 206f 6e0a 2020 2020 2020 2020 7370 6172   on.        spar
+000061b0: 7365 206d 6174 7269 6365 732c 2062 6563  se matrices, bec
+000061c0: 6175 7365 2063 656e 7465 7269 6e67 2074  ause centering t
+000061d0: 6865 6d20 656e 7461 696c 7320 6275 696c  hem entails buil
+000061e0: 6469 6e67 2061 2064 656e 7365 0a20 2020  ding a dense.   
+000061f0: 2020 2020 206d 6174 7269 7820 7768 6963       matrix whic
+00006200: 6820 696e 2063 6f6d 6d6f 6e20 7573 6520  h in common use 
+00006210: 6361 7365 7320 6973 206c 696b 656c 7920  cases is likely 
+00006220: 746f 2062 6520 746f 6f20 6c61 7267 6520  to be too large 
+00006230: 746f 2066 6974 2069 6e0a 2020 2020 2020  to fit in.      
+00006240: 2020 6d65 6d6f 7279 2e0a 0a20 2020 2077    memory...    w
+00006250: 6974 685f 7374 6420 3a20 626f 6f6c 2c20  ith_std : bool, 
+00006260: 6465 6661 756c 743d 5472 7565 0a20 2020  default=True.   
+00006270: 2020 2020 2049 6620 5472 7565 2c20 7363       If True, sc
+00006280: 616c 6520 7468 6520 6461 7461 2074 6f20  ale the data to 
+00006290: 756e 6974 2076 6172 6961 6e63 6520 286f  unit variance (o
+000062a0: 7220 6571 7569 7661 6c65 6e74 6c79 2c0a  r equivalently,.
+000062b0: 2020 2020 2020 2020 756e 6974 2073 7461          unit sta
+000062c0: 6e64 6172 6420 6465 7669 6174 696f 6e29  ndard deviation)
+000062d0: 2e0a 0a20 2020 2041 7474 7269 6275 7465  ...    Attribute
+000062e0: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
+000062f0: 0a20 2020 2073 6361 6c65 5f20 3a20 6e64  .    scale_ : nd
+00006300: 6172 7261 7920 6f66 2073 6861 7065 2028  array of shape (
+00006310: 6e5f 6665 6174 7572 6573 2c29 206f 7220  n_features,) or 
+00006320: 4e6f 6e65 0a20 2020 2020 2020 2050 6572  None.        Per
+00006330: 2066 6561 7475 7265 2072 656c 6174 6976   feature relativ
+00006340: 6520 7363 616c 696e 6720 6f66 2074 6865  e scaling of the
+00006350: 2064 6174 6120 746f 2061 6368 6965 7665   data to achieve
+00006360: 207a 6572 6f20 6d65 616e 2061 6e64 2075   zero mean and u
+00006370: 6e69 740a 2020 2020 2020 2020 7661 7269  nit.        vari
+00006380: 616e 6365 2e20 4765 6e65 7261 6c6c 7920  ance. Generally 
+00006390: 7468 6973 2069 7320 6361 6c63 756c 6174  this is calculat
+000063a0: 6564 2075 7369 6e67 2060 6e70 2e73 7172  ed using `np.sqr
+000063b0: 7428 7661 725f 2960 2e20 4966 2061 0a20  t(var_)`. If a. 
+000063c0: 2020 2020 2020 2076 6172 6961 6e63 6520         variance 
+000063d0: 6973 207a 6572 6f2c 2077 6520 6361 6e27  is zero, we can'
+000063e0: 7420 6163 6869 6576 6520 756e 6974 2076  t achieve unit v
+000063f0: 6172 6961 6e63 652c 2061 6e64 2074 6865  ariance, and the
+00006400: 2064 6174 6120 6973 206c 6566 740a 2020   data is left.  
+00006410: 2020 2020 2020 6173 2d69 732c 2067 6976        as-is, giv
+00006420: 696e 6720 6120 7363 616c 696e 6720 6661  ing a scaling fa
+00006430: 6374 6f72 206f 6620 312e 2060 7363 616c  ctor of 1. `scal
+00006440: 655f 6020 6973 2065 7175 616c 2074 6f20  e_` is equal to 
+00006450: 604e 6f6e 6560 0a20 2020 2020 2020 2077  `None`.        w
+00006460: 6865 6e20 6077 6974 685f 7374 643d 4661  hen `with_std=Fa
+00006470: 6c73 6560 2e0a 0a20 2020 2020 2020 202e  lse`...        .
+00006480: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
+00006490: 2030 2e31 370a 2020 2020 2020 2020 2020   0.17.          
+000064a0: 202a 7363 616c 655f 2a0a 0a20 2020 206d   *scale_*..    m
+000064b0: 6561 6e5f 203a 206e 6461 7272 6179 206f  ean_ : ndarray o
+000064c0: 6620 7368 6170 6520 286e 5f66 6561 7475  f shape (n_featu
+000064d0: 7265 732c 2920 6f72 204e 6f6e 650a 2020  res,) or None.  
+000064e0: 2020 2020 2020 5468 6520 6d65 616e 2076        The mean v
+000064f0: 616c 7565 2066 6f72 2065 6163 6820 6665  alue for each fe
+00006500: 6174 7572 6520 696e 2074 6865 2074 7261  ature in the tra
+00006510: 696e 696e 6720 7365 742e 0a20 2020 2020  ining set..     
+00006520: 2020 2045 7175 616c 2074 6f20 6060 4e6f     Equal to ``No
+00006530: 6e65 6060 2077 6865 6e20 6060 7769 7468  ne`` when ``with
+00006540: 5f6d 6561 6e3d 4661 6c73 6560 6020 616e  _mean=False`` an
+00006550: 6420 6060 7769 7468 5f73 7464 3d46 616c  d ``with_std=Fal
+00006560: 7365 6060 2e0a 0a20 2020 2076 6172 5f20  se``...    var_ 
+00006570: 3a20 6e64 6172 7261 7920 6f66 2073 6861  : ndarray of sha
+00006580: 7065 2028 6e5f 6665 6174 7572 6573 2c29  pe (n_features,)
+00006590: 206f 7220 4e6f 6e65 0a20 2020 2020 2020   or None.       
+000065a0: 2054 6865 2076 6172 6961 6e63 6520 666f   The variance fo
+000065b0: 7220 6561 6368 2066 6561 7475 7265 2069  r each feature i
+000065c0: 6e20 7468 6520 7472 6169 6e69 6e67 2073  n the training s
+000065d0: 6574 2e20 5573 6564 2074 6f20 636f 6d70  et. Used to comp
+000065e0: 7574 650a 2020 2020 2020 2020 6073 6361  ute.        `sca
+000065f0: 6c65 5f60 2e20 4571 7561 6c20 746f 2060  le_`. Equal to `
+00006600: 604e 6f6e 6560 6020 7768 656e 2060 6077  `None`` when ``w
+00006610: 6974 685f 6d65 616e 3d46 616c 7365 6060  ith_mean=False``
+00006620: 2061 6e64 0a20 2020 2020 2020 2060 6077   and.        ``w
+00006630: 6974 685f 7374 643d 4661 6c73 6560 602e  ith_std=False``.
+00006640: 0a0a 2020 2020 6e5f 6665 6174 7572 6573  ..    n_features
+00006650: 5f69 6e5f 203a 2069 6e74 0a20 2020 2020  _in_ : int.     
+00006660: 2020 204e 756d 6265 7220 6f66 2066 6561     Number of fea
+00006670: 7475 7265 7320 7365 656e 2064 7572 696e  tures seen durin
+00006680: 6720 3a74 6572 6d3a 6066 6974 602e 0a0a  g :term:`fit`...
+00006690: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
+000066a0: 6f6e 6164 6465 643a 3a20 302e 3234 0a0a  onadded:: 0.24..
+000066b0: 2020 2020 6665 6174 7572 655f 6e61 6d65      feature_name
+000066c0: 735f 696e 5f20 3a20 6e64 6172 7261 7920  s_in_ : ndarray 
+000066d0: 6f66 2073 6861 7065 2028 606e 5f66 6561  of shape (`n_fea
+000066e0: 7475 7265 735f 696e 5f60 2c29 0a20 2020  tures_in_`,).   
+000066f0: 2020 2020 204e 616d 6573 206f 6620 6665       Names of fe
+00006700: 6174 7572 6573 2073 6565 6e20 6475 7269  atures seen duri
+00006710: 6e67 203a 7465 726d 3a60 6669 7460 2e20  ng :term:`fit`. 
+00006720: 4465 6669 6e65 6420 6f6e 6c79 2077 6865  Defined only whe
+00006730: 6e20 6058 600a 2020 2020 2020 2020 6861  n `X`.        ha
+00006740: 7320 6665 6174 7572 6520 6e61 6d65 7320  s feature names 
+00006750: 7468 6174 2061 7265 2061 6c6c 2073 7472  that are all str
+00006760: 696e 6773 2e0a 0a20 2020 2020 2020 202e  ings...        .
+00006770: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
+00006780: 2031 2e30 0a0a 2020 2020 6e5f 7361 6d70   1.0..    n_samp
+00006790: 6c65 735f 7365 656e 5f20 3a20 696e 7420  les_seen_ : int 
+000067a0: 6f72 206e 6461 7272 6179 206f 6620 7368  or ndarray of sh
+000067b0: 6170 6520 286e 5f66 6561 7475 7265 732c  ape (n_features,
+000067c0: 290a 2020 2020 2020 2020 5468 6520 6e75  ).        The nu
+000067d0: 6d62 6572 206f 6620 7361 6d70 6c65 7320  mber of samples 
+000067e0: 7072 6f63 6573 7365 6420 6279 2074 6865  processed by the
+000067f0: 2065 7374 696d 6174 6f72 2066 6f72 2065   estimator for e
+00006800: 6163 6820 6665 6174 7572 652e 0a20 2020  ach feature..   
+00006810: 2020 2020 2049 6620 7468 6572 6520 6172       If there ar
+00006820: 6520 6e6f 206d 6973 7369 6e67 2073 616d  e no missing sam
+00006830: 706c 6573 2c20 7468 6520 6060 6e5f 7361  ples, the ``n_sa
+00006840: 6d70 6c65 735f 7365 656e 6060 2077 696c  mples_seen`` wil
+00006850: 6c20 6265 2061 6e0a 2020 2020 2020 2020  l be an.        
+00006860: 696e 7465 6765 722c 206f 7468 6572 7769  integer, otherwi
+00006870: 7365 2069 7420 7769 6c6c 2062 6520 616e  se it will be an
+00006880: 2061 7272 6179 206f 6620 6474 7970 6520   array of dtype 
+00006890: 696e 742e 2049 660a 2020 2020 2020 2020  int. If.        
+000068a0: 6073 616d 706c 655f 7765 6967 6874 7360  `sample_weights`
+000068b0: 2061 7265 2075 7365 6420 6974 2077 696c   are used it wil
+000068c0: 6c20 6265 2061 2066 6c6f 6174 2028 6966  l be a float (if
+000068d0: 206e 6f20 6d69 7373 696e 6720 6461 7461   no missing data
+000068e0: 290a 2020 2020 2020 2020 6f72 2061 6e20  ).        or an 
+000068f0: 6172 7261 7920 6f66 2064 7479 7065 2066  array of dtype f
+00006900: 6c6f 6174 2074 6861 7420 7375 6d73 2074  loat that sums t
+00006910: 6865 2077 6569 6768 7473 2073 6565 6e20  he weights seen 
+00006920: 736f 2066 6172 2e0a 2020 2020 2020 2020  so far..        
+00006930: 5769 6c6c 2062 6520 7265 7365 7420 6f6e  Will be reset on
+00006940: 206e 6577 2063 616c 6c73 2074 6f20 6669   new calls to fi
+00006950: 742c 2062 7574 2069 6e63 7265 6d65 6e74  t, but increment
+00006960: 7320 6163 726f 7373 0a20 2020 2020 2020  s across.       
+00006970: 2060 6070 6172 7469 616c 5f66 6974 6060   ``partial_fit``
+00006980: 2063 616c 6c73 2e0a 0a20 2020 2053 6565   calls...    See
+00006990: 2041 6c73 6f0a 2020 2020 2d2d 2d2d 2d2d   Also.    ------
+000069a0: 2d2d 0a20 2020 2073 6361 6c65 203a 2045  --.    scale : E
+000069b0: 7175 6976 616c 656e 7420 6675 6e63 7469  quivalent functi
+000069c0: 6f6e 2077 6974 686f 7574 2074 6865 2065  on without the e
+000069d0: 7374 696d 6174 6f72 2041 5049 2e0a 0a20  stimator API... 
+000069e0: 2020 203a 636c 6173 733a 607e 736b 6c65     :class:`~skle
+000069f0: 6172 6e2e 6465 636f 6d70 6f73 6974 696f  arn.decompositio
+00006a00: 6e2e 5043 4160 203a 2046 7572 7468 6572  n.PCA` : Further
+00006a10: 2072 656d 6f76 6573 2074 6865 206c 696e   removes the lin
+00006a20: 6561 720a 2020 2020 2020 2020 636f 7272  ear.        corr
+00006a30: 656c 6174 696f 6e20 6163 726f 7373 2066  elation across f
+00006a40: 6561 7475 7265 7320 7769 7468 2027 7768  eatures with 'wh
+00006a50: 6974 656e 3d54 7275 6527 2e0a 0a20 2020  iten=True'...   
+00006a60: 204e 6f74 6573 0a20 2020 202d 2d2d 2d2d   Notes.    -----
+00006a70: 0a20 2020 204e 614e 7320 6172 6520 7472  .    NaNs are tr
+00006a80: 6561 7465 6420 6173 206d 6973 7369 6e67  eated as missing
+00006a90: 2076 616c 7565 733a 2064 6973 7265 6761   values: disrega
+00006aa0: 7264 6564 2069 6e20 6669 742c 2061 6e64  rded in fit, and
+00006ab0: 206d 6169 6e74 6169 6e65 6420 696e 0a20   maintained in. 
+00006ac0: 2020 2074 7261 6e73 666f 726d 2e0a 0a20     transform... 
+00006ad0: 2020 2057 6520 7573 6520 6120 6269 6173     We use a bias
+00006ae0: 6564 2065 7374 696d 6174 6f72 2066 6f72  ed estimator for
+00006af0: 2074 6865 2073 7461 6e64 6172 6420 6465   the standard de
+00006b00: 7669 6174 696f 6e2c 2065 7175 6976 616c  viation, equival
+00006b10: 656e 7420 746f 0a20 2020 2060 6e75 6d70  ent to.    `nump
+00006b20: 792e 7374 6428 782c 2064 646f 663d 3029  y.std(x, ddof=0)
+00006b30: 602e 204e 6f74 6520 7468 6174 2074 6865  `. Note that the
+00006b40: 2063 686f 6963 6520 6f66 2060 6464 6f66   choice of `ddof
+00006b50: 6020 6973 2075 6e6c 696b 656c 7920 746f  ` is unlikely to
+00006b60: 0a20 2020 2061 6666 6563 7420 6d6f 6465  .    affect mode
+00006b70: 6c20 7065 7266 6f72 6d61 6e63 652e 0a0a  l performance...
+00006b80: 2020 2020 4578 616d 706c 6573 0a20 2020      Examples.   
+00006b90: 202d 2d2d 2d2d 2d2d 2d0a 2020 2020 3e3e   --------.    >>
+00006ba0: 3e20 6672 6f6d 2073 6b6c 6561 726e 2e70  > from sklearn.p
+00006bb0: 7265 7072 6f63 6573 7369 6e67 2069 6d70  reprocessing imp
+00006bc0: 6f72 7420 5374 616e 6461 7264 5363 616c  ort StandardScal
+00006bd0: 6572 0a20 2020 203e 3e3e 2064 6174 6120  er.    >>> data 
+00006be0: 3d20 5b5b 302c 2030 5d2c 205b 302c 2030  = [[0, 0], [0, 0
+00006bf0: 5d2c 205b 312c 2031 5d2c 205b 312c 2031  ], [1, 1], [1, 1
+00006c00: 5d5d 0a20 2020 203e 3e3e 2073 6361 6c65  ]].    >>> scale
+00006c10: 7220 3d20 5374 616e 6461 7264 5363 616c  r = StandardScal
+00006c20: 6572 2829 0a20 2020 203e 3e3e 2070 7269  er().    >>> pri
+00006c30: 6e74 2873 6361 6c65 722e 6669 7428 6461  nt(scaler.fit(da
+00006c40: 7461 2929 0a20 2020 2053 7461 6e64 6172  ta)).    Standar
+00006c50: 6453 6361 6c65 7228 290a 2020 2020 3e3e  dScaler().    >>
+00006c60: 3e20 7072 696e 7428 7363 616c 6572 2e6d  > print(scaler.m
+00006c70: 6561 6e5f 290a 2020 2020 5b30 2e35 2030  ean_).    [0.5 0
+00006c80: 2e35 5d0a 2020 2020 3e3e 3e20 7072 696e  .5].    >>> prin
+00006c90: 7428 7363 616c 6572 2e74 7261 6e73 666f  t(scaler.transfo
+00006ca0: 726d 2864 6174 6129 290a 2020 2020 5b5b  rm(data)).    [[
+00006cb0: 2d31 2e20 2d31 2e5d 0a20 2020 2020 5b2d  -1. -1.].     [-
+00006cc0: 312e 202d 312e 5d0a 2020 2020 205b 2031  1. -1.].     [ 1
+00006cd0: 2e20 2031 2e5d 0a20 2020 2020 5b20 312e  .  1.].     [ 1.
+00006ce0: 2020 312e 5d5d 0a20 2020 203e 3e3e 2070    1.]].    >>> p
+00006cf0: 7269 6e74 2873 6361 6c65 722e 7472 616e  rint(scaler.tran
+00006d00: 7366 6f72 6d28 5b5b 322c 2032 5d5d 2929  sform([[2, 2]]))
+00006d10: 0a20 2020 205b 5b33 2e20 332e 5d5d 0a20  .    [[3. 3.]]. 
+00006d20: 2020 2022 2222 0a0a 2020 2020 5f70 6172     """..    _par
+00006d30: 616d 6574 6572 5f63 6f6e 7374 7261 696e  ameter_constrain
+00006d40: 7473 3a20 6469 6374 203d 207b 0a20 2020  ts: dict = {.   
+00006d50: 2020 2020 2022 636f 7079 223a 205b 2262       "copy": ["b
+00006d60: 6f6f 6c65 616e 225d 2c0a 2020 2020 2020  oolean"],.      
+00006d70: 2020 2277 6974 685f 6d65 616e 223a 205b    "with_mean": [
+00006d80: 2262 6f6f 6c65 616e 225d 2c0a 2020 2020  "boolean"],.    
+00006d90: 2020 2020 2277 6974 685f 7374 6422 3a20      "with_std": 
+00006da0: 5b22 626f 6f6c 6561 6e22 5d2c 0a20 2020  ["boolean"],.   
+00006db0: 207d 0a0a 2020 2020 6465 6620 5f5f 696e   }..    def __in
+00006dc0: 6974 5f5f 2873 656c 662c 202a 2c20 636f  it__(self, *, co
+00006dd0: 7079 3d54 7275 652c 2077 6974 685f 6d65  py=True, with_me
+00006de0: 616e 3d54 7275 652c 2077 6974 685f 7374  an=True, with_st
+00006df0: 643d 5472 7565 293a 0a20 2020 2020 2020  d=True):.       
+00006e00: 2073 656c 662e 7769 7468 5f6d 6561 6e20   self.with_mean 
+00006e10: 3d20 7769 7468 5f6d 6561 6e0a 2020 2020  = with_mean.    
+00006e20: 2020 2020 7365 6c66 2e77 6974 685f 7374      self.with_st
+00006e30: 6420 3d20 7769 7468 5f73 7464 0a20 2020  d = with_std.   
+00006e40: 2020 2020 2073 656c 662e 636f 7079 203d       self.copy =
+00006e50: 2063 6f70 790a 0a20 2020 2064 6566 205f   copy..    def _
+00006e60: 7265 7365 7428 7365 6c66 293a 0a20 2020  reset(self):.   
+00006e70: 2020 2020 2022 2222 5265 7365 7420 696e       """Reset in
+00006e80: 7465 726e 616c 2064 6174 612d 6465 7065  ternal data-depe
+00006e90: 6e64 656e 7420 7374 6174 6520 6f66 2074  ndent state of t
+00006ea0: 6865 2073 6361 6c65 722c 2069 6620 6e65  he scaler, if ne
+00006eb0: 6365 7373 6172 792e 0a0a 2020 2020 2020  cessary...      
+00006ec0: 2020 5f5f 696e 6974 5f5f 2070 6172 616d    __init__ param
+00006ed0: 6574 6572 7320 6172 6520 6e6f 7420 746f  eters are not to
+00006ee0: 7563 6865 642e 0a20 2020 2020 2020 2022  uched..        "
+00006ef0: 2222 0a20 2020 2020 2020 2023 2043 6865  "".        # Che
+00006f00: 636b 696e 6720 6f6e 6520 6174 7472 6962  cking one attrib
+00006f10: 7574 6520 6973 2065 6e6f 7567 682c 2062  ute is enough, b
+00006f20: 6563 6175 7365 2074 6865 7920 6172 6520  ecause they are 
+00006f30: 616c 6c20 7365 7420 746f 6765 7468 6572  all set together
+00006f40: 0a20 2020 2020 2020 2023 2069 6e20 7061  .        # in pa
+00006f50: 7274 6961 6c5f 6669 740a 2020 2020 2020  rtial_fit.      
+00006f60: 2020 6966 2068 6173 6174 7472 2873 656c    if hasattr(sel
+00006f70: 662c 2022 7363 616c 655f 2229 3a0a 2020  f, "scale_"):.  
+00006f80: 2020 2020 2020 2020 2020 6465 6c20 7365            del se
+00006f90: 6c66 2e73 6361 6c65 5f0a 2020 2020 2020  lf.scale_.      
+00006fa0: 2020 2020 2020 6465 6c20 7365 6c66 2e6e        del self.n
+00006fb0: 5f73 616d 706c 6573 5f73 6565 6e5f 0a20  _samples_seen_. 
+00006fc0: 2020 2020 2020 2020 2020 2064 656c 2073             del s
+00006fd0: 656c 662e 6d65 616e 5f0a 2020 2020 2020  elf.mean_.      
+00006fe0: 2020 2020 2020 6465 6c20 7365 6c66 2e76        del self.v
+00006ff0: 6172 5f0a 0a20 2020 2064 6566 2066 6974  ar_..    def fit
+00007000: 2873 656c 662c 2058 2c20 793d 4e6f 6e65  (self, X, y=None
+00007010: 2c20 7361 6d70 6c65 5f77 6569 6768 743d  , sample_weight=
+00007020: 4e6f 6e65 293a 0a20 2020 2020 2020 2022  None):.        "
+00007030: 2222 436f 6d70 7574 6520 7468 6520 6d65  ""Compute the me
+00007040: 616e 2061 6e64 2073 7464 2074 6f20 6265  an and std to be
+00007050: 2075 7365 6420 666f 7220 6c61 7465 7220   used for later 
+00007060: 7363 616c 696e 672e 0a0a 2020 2020 2020  scaling...      
+00007070: 2020 5061 7261 6d65 7465 7273 0a20 2020    Parameters.   
+00007080: 2020 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a       ----------.
+00007090: 2020 2020 2020 2020 5820 3a20 7b61 7272          X : {arr
+000070a0: 6179 2d6c 696b 652c 2073 7061 7273 6520  ay-like, sparse 
+000070b0: 6d61 7472 6978 7d20 6f66 2073 6861 7065  matrix} of shape
+000070c0: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
+000070d0: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
+000070e0: 2020 2020 2054 6865 2064 6174 6120 7573       The data us
+000070f0: 6564 2074 6f20 636f 6d70 7574 6520 7468  ed to compute th
+00007100: 6520 6d65 616e 2061 6e64 2073 7461 6e64  e mean and stand
+00007110: 6172 6420 6465 7669 6174 696f 6e0a 2020  ard deviation.  
+00007120: 2020 2020 2020 2020 2020 7573 6564 2066            used f
+00007130: 6f72 206c 6174 6572 2073 6361 6c69 6e67  or later scaling
+00007140: 2061 6c6f 6e67 2074 6865 2066 6561 7475   along the featu
+00007150: 7265 7320 6178 6973 2e0a 0a20 2020 2020  res axis...     
+00007160: 2020 2079 203a 204e 6f6e 650a 2020 2020     y : None.    
+00007170: 2020 2020 2020 2020 4967 6e6f 7265 642e          Ignored.
+00007180: 0a0a 2020 2020 2020 2020 7361 6d70 6c65  ..        sample
+00007190: 5f77 6569 6768 7420 3a20 6172 7261 792d  _weight : array-
+000071a0: 6c69 6b65 206f 6620 7368 6170 6520 286e  like of shape (n
+000071b0: 5f73 616d 706c 6573 2c29 2c20 6465 6661  _samples,), defa
+000071c0: 756c 743d 4e6f 6e65 0a20 2020 2020 2020  ult=None.       
+000071d0: 2020 2020 2049 6e64 6976 6964 7561 6c20       Individual 
+000071e0: 7765 6967 6874 7320 666f 7220 6561 6368  weights for each
+000071f0: 2073 616d 706c 652e 0a0a 2020 2020 2020   sample...      
+00007200: 2020 2020 2020 2e2e 2076 6572 7369 6f6e        .. version
+00007210: 6164 6465 643a 3a20 302e 3234 0a20 2020  added:: 0.24.   
+00007220: 2020 2020 2020 2020 2020 2020 7061 7261              para
+00007230: 6d65 7465 7220 2a73 616d 706c 655f 7765  meter *sample_we
+00007240: 6967 6874 2a20 7375 7070 6f72 7420 746f  ight* support to
+00007250: 2053 7461 6e64 6172 6453 6361 6c65 722e   StandardScaler.
+00007260: 0a0a 2020 2020 2020 2020 5265 7475 726e  ..        Return
+00007270: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
+00007280: 2d0a 2020 2020 2020 2020 7365 6c66 203a  -.        self :
+00007290: 206f 626a 6563 740a 2020 2020 2020 2020   object.        
+000072a0: 2020 2020 4669 7474 6564 2073 6361 6c65      Fitted scale
+000072b0: 722e 0a20 2020 2020 2020 2022 2222 0a20  r..        """. 
+000072c0: 2020 2020 2020 2023 2052 6573 6574 2069         # Reset i
+000072d0: 6e74 6572 6e61 6c20 7374 6174 6520 6265  nternal state be
+000072e0: 666f 7265 2066 6974 7469 6e67 0a20 2020  fore fitting.   
+000072f0: 2020 2020 2073 656c 662e 5f72 6573 6574       self._reset
+00007300: 2829 0a20 2020 2020 2020 2072 6574 7572  ().        retur
+00007310: 6e20 7365 6c66 2e70 6172 7469 616c 5f66  n self.partial_f
+00007320: 6974 2858 2c20 792c 2073 616d 706c 655f  it(X, y, sample_
+00007330: 7765 6967 6874 290a 0a20 2020 2040 5f66  weight)..    @_f
+00007340: 6974 5f63 6f6e 7465 7874 2870 7265 6665  it_context(prefe
+00007350: 725f 736b 6970 5f6e 6573 7465 645f 7661  r_skip_nested_va
+00007360: 6c69 6461 7469 6f6e 3d54 7275 6529 0a20  lidation=True). 
+00007370: 2020 2064 6566 2070 6172 7469 616c 5f66     def partial_f
+00007380: 6974 2873 656c 662c 2058 2c20 793d 4e6f  it(self, X, y=No
+00007390: 6e65 2c20 7361 6d70 6c65 5f77 6569 6768  ne, sample_weigh
+000073a0: 743d 4e6f 6e65 293a 0a20 2020 2020 2020  t=None):.       
+000073b0: 2022 2222 4f6e 6c69 6e65 2063 6f6d 7075   """Online compu
+000073c0: 7461 7469 6f6e 206f 6620 6d65 616e 2061  tation of mean a
+000073d0: 6e64 2073 7464 206f 6e20 5820 666f 7220  nd std on X for 
+000073e0: 6c61 7465 7220 7363 616c 696e 672e 0a0a  later scaling...
+000073f0: 2020 2020 2020 2020 416c 6c20 6f66 2058          All of X
+00007400: 2069 7320 7072 6f63 6573 7365 6420 6173   is processed as
+00007410: 2061 2073 696e 676c 6520 6261 7463 682e   a single batch.
+00007420: 2054 6869 7320 6973 2069 6e74 656e 6465   This is intende
+00007430: 6420 666f 7220 6361 7365 730a 2020 2020  d for cases.    
+00007440: 2020 2020 7768 656e 203a 6d65 7468 3a60      when :meth:`
+00007450: 6669 7460 2069 7320 6e6f 7420 6665 6173  fit` is not feas
+00007460: 6962 6c65 2064 7565 2074 6f20 7665 7279  ible due to very
+00007470: 206c 6172 6765 206e 756d 6265 7220 6f66   large number of
+00007480: 0a20 2020 2020 2020 2060 6e5f 7361 6d70  .        `n_samp
+00007490: 6c65 7360 206f 7220 6265 6361 7573 6520  les` or because 
+000074a0: 5820 6973 2072 6561 6420 6672 6f6d 2061  X is read from a
+000074b0: 2063 6f6e 7469 6e75 6f75 7320 7374 7265   continuous stre
+000074c0: 616d 2e0a 0a20 2020 2020 2020 2054 6865  am...        The
+000074d0: 2061 6c67 6f72 6974 686d 2066 6f72 2069   algorithm for i
+000074e0: 6e63 7265 6d65 6e74 616c 206d 6561 6e20  ncremental mean 
+000074f0: 616e 6420 7374 6420 6973 2067 6976 656e  and std is given
+00007500: 2069 6e20 4571 7561 7469 6f6e 2031 2e35   in Equation 1.5
+00007510: 612c 620a 2020 2020 2020 2020 696e 2043  a,b.        in C
+00007520: 6861 6e2c 2054 6f6e 7920 462e 2c20 4765  han, Tony F., Ge
+00007530: 6e65 2048 2e20 476f 6c75 622c 2061 6e64  ne H. Golub, and
+00007540: 2052 616e 6461 6c6c 204a 2e20 4c65 5665   Randall J. LeVe
+00007550: 7175 652e 2022 416c 676f 7269 7468 6d73  que. "Algorithms
+00007560: 0a20 2020 2020 2020 2066 6f72 2063 6f6d  .        for com
+00007570: 7075 7469 6e67 2074 6865 2073 616d 706c  puting the sampl
+00007580: 6520 7661 7269 616e 6365 3a20 416e 616c  e variance: Anal
+00007590: 7973 6973 2061 6e64 2072 6563 6f6d 6d65  ysis and recomme
+000075a0: 6e64 6174 696f 6e73 2e22 0a20 2020 2020  ndations.".     
+000075b0: 2020 2054 6865 2041 6d65 7269 6361 6e20     The American 
+000075c0: 5374 6174 6973 7469 6369 616e 2033 372e  Statistician 37.
+000075d0: 3320 2831 3938 3329 3a20 3234 322d 3234  3 (1983): 242-24
+000075e0: 373a 0a0a 2020 2020 2020 2020 5061 7261  7:..        Para
+000075f0: 6d65 7465 7273 0a20 2020 2020 2020 202d  meters.        -
+00007600: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 2020  ---------.      
+00007610: 2020 5820 3a20 7b61 7272 6179 2d6c 696b    X : {array-lik
+00007620: 652c 2073 7061 7273 6520 6d61 7472 6978  e, sparse matrix
+00007630: 7d20 6f66 2073 6861 7065 2028 6e5f 7361  } of shape (n_sa
+00007640: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
+00007650: 7329 0a20 2020 2020 2020 2020 2020 2054  s).            T
+00007660: 6865 2064 6174 6120 7573 6564 2074 6f20  he data used to 
+00007670: 636f 6d70 7574 6520 7468 6520 6d65 616e  compute the mean
+00007680: 2061 6e64 2073 7461 6e64 6172 6420 6465   and standard de
+00007690: 7669 6174 696f 6e0a 2020 2020 2020 2020  viation.        
+000076a0: 2020 2020 7573 6564 2066 6f72 206c 6174      used for lat
+000076b0: 6572 2073 6361 6c69 6e67 2061 6c6f 6e67  er scaling along
+000076c0: 2074 6865 2066 6561 7475 7265 7320 6178   the features ax
+000076d0: 6973 2e0a 0a20 2020 2020 2020 2079 203a  is...        y :
+000076e0: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
+000076f0: 2020 4967 6e6f 7265 642e 0a0a 2020 2020    Ignored...    
+00007700: 2020 2020 7361 6d70 6c65 5f77 6569 6768      sample_weigh
+00007710: 7420 3a20 6172 7261 792d 6c69 6b65 206f  t : array-like o
+00007720: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
+00007730: 6573 2c29 2c20 6465 6661 756c 743d 4e6f  es,), default=No
+00007740: 6e65 0a20 2020 2020 2020 2020 2020 2049  ne.            I
+00007750: 6e64 6976 6964 7561 6c20 7765 6967 6874  ndividual weight
+00007760: 7320 666f 7220 6561 6368 2073 616d 706c  s for each sampl
+00007770: 652e 0a0a 2020 2020 2020 2020 2020 2020  e...            
+00007780: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
+00007790: 3a20 302e 3234 0a20 2020 2020 2020 2020  : 0.24.         
+000077a0: 2020 2020 2020 7061 7261 6d65 7465 7220        parameter 
+000077b0: 2a73 616d 706c 655f 7765 6967 6874 2a20  *sample_weight* 
+000077c0: 7375 7070 6f72 7420 746f 2053 7461 6e64  support to Stand
+000077d0: 6172 6453 6361 6c65 722e 0a0a 2020 2020  ardScaler...    
+000077e0: 2020 2020 5265 7475 726e 730a 2020 2020      Returns.    
+000077f0: 2020 2020 2d2d 2d2d 2d2d 2d0a 2020 2020      -------.    
+00007800: 2020 2020 7365 6c66 203a 206f 626a 6563      self : objec
+00007810: 740a 2020 2020 2020 2020 2020 2020 4669  t.            Fi
+00007820: 7474 6564 2073 6361 6c65 722e 0a20 2020  tted scaler..   
+00007830: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
+00007840: 2066 6972 7374 5f63 616c 6c20 3d20 6e6f   first_call = no
+00007850: 7420 6861 7361 7474 7228 7365 6c66 2c20  t hasattr(self, 
+00007860: 226e 5f73 616d 706c 6573 5f73 6565 6e5f  "n_samples_seen_
+00007870: 2229 0a20 2020 2020 2020 2058 203d 2073  ").        X = s
+00007880: 656c 662e 5f76 616c 6964 6174 655f 6461  elf._validate_da
+00007890: 7461 280a 2020 2020 2020 2020 2020 2020  ta(.            
+000078a0: 582c 0a20 2020 2020 2020 2020 2020 2061  X,.            a
+000078b0: 6363 6570 745f 7370 6172 7365 3d28 2263  ccept_sparse=("c
+000078c0: 7372 222c 2022 6373 6322 292c 0a20 2020  sr", "csc"),.   
+000078d0: 2020 2020 2020 2020 2064 7479 7065 3d46           dtype=F
+000078e0: 4c4f 4154 5f44 5459 5045 532c 0a20 2020  LOAT_DTYPES,.   
+000078f0: 2020 2020 2020 2020 2066 6f72 6365 5f61           force_a
+00007900: 6c6c 5f66 696e 6974 653d 2261 6c6c 6f77  ll_finite="allow
+00007910: 2d6e 616e 222c 0a20 2020 2020 2020 2020  -nan",.         
+00007920: 2020 2072 6573 6574 3d66 6972 7374 5f63     reset=first_c
+00007930: 616c 6c2c 0a20 2020 2020 2020 2029 0a20  all,.        ). 
+00007940: 2020 2020 2020 206e 5f66 6561 7475 7265         n_feature
+00007950: 7320 3d20 582e 7368 6170 655b 315d 0a0a  s = X.shape[1]..
+00007960: 2020 2020 2020 2020 6966 2073 616d 706c          if sampl
+00007970: 655f 7765 6967 6874 2069 7320 6e6f 7420  e_weight is not 
+00007980: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
+00007990: 2020 7361 6d70 6c65 5f77 6569 6768 7420    sample_weight 
+000079a0: 3d20 5f63 6865 636b 5f73 616d 706c 655f  = _check_sample_
+000079b0: 7765 6967 6874 2873 616d 706c 655f 7765  weight(sample_we
+000079c0: 6967 6874 2c20 582c 2064 7479 7065 3d58  ight, X, dtype=X
+000079d0: 2e64 7479 7065 290a 0a20 2020 2020 2020  .dtype)..       
+000079e0: 2023 2045 7665 6e20 696e 2074 6865 2063   # Even in the c
+000079f0: 6173 6520 6f66 2060 7769 7468 5f6d 6561  ase of `with_mea
+00007a00: 6e3d 4661 6c73 6560 2c20 7765 2075 7064  n=False`, we upd
+00007a10: 6174 6520 7468 6520 6d65 616e 2061 6e79  ate the mean any
+00007a20: 7761 790a 2020 2020 2020 2020 2320 5468  way.        # Th
+00007a30: 6973 2069 7320 6e65 6564 6564 2066 6f72  is is needed for
+00007a40: 2074 6865 2069 6e63 7265 6d65 6e74 616c   the incremental
+00007a50: 2063 6f6d 7075 7461 7469 6f6e 206f 6620   computation of 
+00007a60: 7468 6520 7661 720a 2020 2020 2020 2020  the var.        
+00007a70: 2320 5365 6520 696e 6372 5f6d 6561 6e5f  # See incr_mean_
+00007a80: 7661 7269 616e 6365 5f61 7869 7320 616e  variance_axis an
+00007a90: 6420 5f69 6e63 7265 6d65 6e74 616c 5f6d  d _incremental_m
+00007aa0: 6561 6e5f 7661 7269 616e 6365 5f61 7869  ean_variance_axi
+00007ab0: 730a 0a20 2020 2020 2020 2023 2069 6620  s..        # if 
+00007ac0: 6e5f 7361 6d70 6c65 735f 7365 656e 5f20  n_samples_seen_ 
+00007ad0: 6973 2061 6e20 696e 7465 6765 7220 2869  is an integer (i
+00007ae0: 2e65 2e20 6e6f 206d 6973 7369 6e67 2076  .e. no missing v
+00007af0: 616c 7565 7329 2c20 7765 206e 6565 6420  alues), we need 
+00007b00: 746f 0a20 2020 2020 2020 2023 2074 7261  to.        # tra
+00007b10: 6e73 666f 726d 2069 7420 746f 2061 204e  nsform it to a N
+00007b20: 756d 5079 2061 7272 6179 206f 6620 7368  umPy array of sh
+00007b30: 6170 6520 286e 5f66 6561 7475 7265 732c  ape (n_features,
+00007b40: 2920 7265 7175 6972 6564 2062 790a 2020  ) required by.  
+00007b50: 2020 2020 2020 2320 696e 6372 5f6d 6561        # incr_mea
+00007b60: 6e5f 7661 7269 616e 6365 5f61 7869 7320  n_variance_axis 
+00007b70: 616e 6420 5f69 6e63 7265 6d65 6e74 616c  and _incremental
+00007b80: 5f76 6172 6961 6e63 655f 6178 6973 0a20  _variance_axis. 
+00007b90: 2020 2020 2020 2064 7479 7065 203d 206e         dtype = n
+00007ba0: 702e 696e 7436 3420 6966 2073 616d 706c  p.int64 if sampl
+00007bb0: 655f 7765 6967 6874 2069 7320 4e6f 6e65  e_weight is None
+00007bc0: 2065 6c73 6520 582e 6474 7970 650a 2020   else X.dtype.  
+00007bd0: 2020 2020 2020 6966 206e 6f74 2068 6173        if not has
+00007be0: 6174 7472 2873 656c 662c 2022 6e5f 7361  attr(self, "n_sa
+00007bf0: 6d70 6c65 735f 7365 656e 5f22 293a 0a20  mples_seen_"):. 
+00007c00: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00007c10: 6e5f 7361 6d70 6c65 735f 7365 656e 5f20  n_samples_seen_ 
+00007c20: 3d20 6e70 2e7a 6572 6f73 286e 5f66 6561  = np.zeros(n_fea
+00007c30: 7475 7265 732c 2064 7479 7065 3d64 7479  tures, dtype=dty
+00007c40: 7065 290a 2020 2020 2020 2020 656c 6966  pe).        elif
+00007c50: 206e 702e 7369 7a65 2873 656c 662e 6e5f   np.size(self.n_
+00007c60: 7361 6d70 6c65 735f 7365 656e 5f29 203d  samples_seen_) =
+00007c70: 3d20 313a 0a20 2020 2020 2020 2020 2020  = 1:.           
+00007c80: 2073 656c 662e 6e5f 7361 6d70 6c65 735f   self.n_samples_
+00007c90: 7365 656e 5f20 3d20 6e70 2e72 6570 6561  seen_ = np.repea
+00007ca0: 7428 7365 6c66 2e6e 5f73 616d 706c 6573  t(self.n_samples
+00007cb0: 5f73 6565 6e5f 2c20 582e 7368 6170 655b  _seen_, X.shape[
+00007cc0: 315d 290a 2020 2020 2020 2020 2020 2020  1]).            
+00007cd0: 7365 6c66 2e6e 5f73 616d 706c 6573 5f73  self.n_samples_s
+00007ce0: 6565 6e5f 203d 2073 656c 662e 6e5f 7361  een_ = self.n_sa
+00007cf0: 6d70 6c65 735f 7365 656e 5f2e 6173 7479  mples_seen_.asty
+00007d00: 7065 2864 7479 7065 2c20 636f 7079 3d46  pe(dtype, copy=F
+00007d10: 616c 7365 290a 0a20 2020 2020 2020 2069  alse)..        i
+00007d20: 6620 7370 6172 7365 2e69 7373 7061 7273  f sparse.isspars
+00007d30: 6528 5829 3a0a 2020 2020 2020 2020 2020  e(X):.          
+00007d40: 2020 6966 2073 656c 662e 7769 7468 5f6d    if self.with_m
+00007d50: 6561 6e3a 0a20 2020 2020 2020 2020 2020  ean:.           
+00007d60: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+00007d70: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
+00007d80: 2020 2020 2020 2020 2020 2022 4361 6e6e             "Cann
+00007d90: 6f74 2063 656e 7465 7220 7370 6172 7365  ot center sparse
+00007da0: 206d 6174 7269 6365 733a 2070 6173 7320   matrices: pass 
+00007db0: 6077 6974 685f 6d65 616e 3d46 616c 7365  `with_mean=False
+00007dc0: 6020 220a 2020 2020 2020 2020 2020 2020  ` ".            
+00007dd0: 2020 2020 2020 2020 2269 6e73 7465 6164          "instead
+00007de0: 2e20 5365 6520 646f 6373 7472 696e 6720  . See docstring 
+00007df0: 666f 7220 6d6f 7469 7661 7469 6f6e 2061  for motivation a
+00007e00: 6e64 2061 6c74 6572 6e61 7469 7665 732e  nd alternatives.
+00007e10: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00007e20: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
+00007e30: 7370 6172 7365 5f63 6f6e 7374 7275 6374  sparse_construct
+00007e40: 6f72 203d 2028 0a20 2020 2020 2020 2020  or = (.         
+00007e50: 2020 2020 2020 2073 7061 7273 652e 6373         sparse.cs
+00007e60: 725f 6d61 7472 6978 2069 6620 582e 666f  r_matrix if X.fo
+00007e70: 726d 6174 203d 3d20 2263 7372 2220 656c  rmat == "csr" el
+00007e80: 7365 2073 7061 7273 652e 6373 635f 6d61  se sparse.csc_ma
+00007e90: 7472 6978 0a20 2020 2020 2020 2020 2020  trix.           
+00007ea0: 2029 0a0a 2020 2020 2020 2020 2020 2020   )..            
+00007eb0: 6966 2073 656c 662e 7769 7468 5f73 7464  if self.with_std
+00007ec0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00007ed0: 2020 2320 4669 7273 7420 7061 7373 0a20    # First pass. 
+00007ee0: 2020 2020 2020 2020 2020 2020 2020 2069                 i
+00007ef0: 6620 6e6f 7420 6861 7361 7474 7228 7365  f not hasattr(se
+00007f00: 6c66 2c20 2273 6361 6c65 5f22 293a 0a20  lf, "scale_"):. 
+00007f10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00007f20: 2020 2073 656c 662e 6d65 616e 5f2c 2073     self.mean_, s
+00007f30: 656c 662e 7661 725f 2c20 7365 6c66 2e6e  elf.var_, self.n
+00007f40: 5f73 616d 706c 6573 5f73 6565 6e5f 203d  _samples_seen_ =
+00007f50: 206d 6561 6e5f 7661 7269 616e 6365 5f61   mean_variance_a
+00007f60: 7869 7328 0a20 2020 2020 2020 2020 2020  xis(.           
+00007f70: 2020 2020 2020 2020 2020 2020 2058 2c20               X, 
+00007f80: 6178 6973 3d30 2c20 7765 6967 6874 733d  axis=0, weights=
+00007f90: 7361 6d70 6c65 5f77 6569 6768 742c 2072  sample_weight, r
+00007fa0: 6574 7572 6e5f 7375 6d5f 7765 6967 6874  eturn_sum_weight
+00007fb0: 733d 5472 7565 0a20 2020 2020 2020 2020  s=True.         
+00007fc0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   
+00007fd0: 2020 2020 2020 2020 2020 2020 2023 204e               # N
+00007fe0: 6578 7420 7061 7373 6573 0a20 2020 2020  ext passes.     
+00007ff0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00008000: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00008010: 2020 2020 2028 0a20 2020 2020 2020 2020       (.         
+00008020: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00008030: 656c 662e 6d65 616e 5f2c 0a20 2020 2020  elf.mean_,.     
+00008040: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008050: 2020 2073 656c 662e 7661 725f 2c0a 2020     self.var_,.  
+00008060: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008070: 2020 2020 2020 7365 6c66 2e6e 5f73 616d        self.n_sam
+00008080: 706c 6573 5f73 6565 6e5f 2c0a 2020 2020  ples_seen_,.    
+00008090: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000080a0: 2920 3d20 696e 6372 5f6d 6561 6e5f 7661  ) = incr_mean_va
+000080b0: 7269 616e 6365 5f61 7869 7328 0a20 2020  riance_axis(.   
 000080c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000080d0: 2020 2020 2020 2020 6178 6973 3d30 2c0a          axis=0,.
+000080d0: 2020 2020 2058 2c0a 2020 2020 2020 2020       X,.        
 000080e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000080f0: 2020 2020 2020 2020 6c61 7374 5f6d 6561          last_mea
-00008100: 6e3d 7365 6c66 2e6d 6561 6e5f 2c0a 2020  n=self.mean_,.  
-00008110: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008120: 2020 2020 2020 6c61 7374 5f76 6172 3d73        last_var=s
-00008130: 656c 662e 7661 725f 2c0a 2020 2020 2020  elf.var_,.      
-00008140: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008150: 2020 6c61 7374 5f6e 3d73 656c 662e 6e5f    last_n=self.n_
-00008160: 7361 6d70 6c65 735f 7365 656e 5f2c 0a20  samples_seen_,. 
-00008170: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008180: 2020 2020 2020 2077 6569 6768 7473 3d73         weights=s
-00008190: 616d 706c 655f 7765 6967 6874 2c0a 2020  ample_weight,.  
-000081a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000081b0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-000081c0: 2020 2020 2320 5765 2066 6f72 6365 2074      # We force t
-000081d0: 6865 206d 6561 6e20 616e 6420 7661 7269  he mean and vari
-000081e0: 616e 6365 2074 6f20 666c 6f61 7436 3420  ance to float64 
-000081f0: 666f 7220 6c61 7267 6520 6172 7261 7973  for large arrays
-00008200: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00008210: 2023 2053 6565 2068 7474 7073 3a2f 2f67   # See https://g
-00008220: 6974 6875 622e 636f 6d2f 7363 696b 6974  ithub.com/scikit
-00008230: 2d6c 6561 726e 2f73 6369 6b69 742d 6c65  -learn/scikit-le
-00008240: 6172 6e2f 7075 6c6c 2f31 3233 3338 0a20  arn/pull/12338. 
-00008250: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-00008260: 656c 662e 6d65 616e 5f20 3d20 7365 6c66  elf.mean_ = self
-00008270: 2e6d 6561 6e5f 2e61 7374 7970 6528 6e70  .mean_.astype(np
-00008280: 2e66 6c6f 6174 3634 2c20 636f 7079 3d46  .float64, copy=F
-00008290: 616c 7365 290a 2020 2020 2020 2020 2020  alse).          
-000082a0: 2020 2020 2020 7365 6c66 2e76 6172 5f20        self.var_ 
-000082b0: 3d20 7365 6c66 2e76 6172 5f2e 6173 7479  = self.var_.asty
-000082c0: 7065 286e 702e 666c 6f61 7436 342c 2063  pe(np.float64, c
-000082d0: 6f70 793d 4661 6c73 6529 0a20 2020 2020  opy=False).     
-000082e0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
-000082f0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00008300: 662e 6d65 616e 5f20 3d20 4e6f 6e65 2020  f.mean_ = None  
-00008310: 2320 6173 2077 6974 685f 6d65 616e 206d  # as with_mean m
-00008320: 7573 7420 6265 2046 616c 7365 2066 6f72  ust be False for
-00008330: 2073 7061 7273 650a 2020 2020 2020 2020   sparse.        
-00008340: 2020 2020 2020 2020 7365 6c66 2e76 6172          self.var
-00008350: 5f20 3d20 4e6f 6e65 0a20 2020 2020 2020  _ = None.       
-00008360: 2020 2020 2020 2020 2077 6569 6768 7473           weights
-00008370: 203d 205f 6368 6563 6b5f 7361 6d70 6c65   = _check_sample
-00008380: 5f77 6569 6768 7428 7361 6d70 6c65 5f77  _weight(sample_w
-00008390: 6569 6768 742c 2058 290a 2020 2020 2020  eight, X).      
-000083a0: 2020 2020 2020 2020 2020 7375 6d5f 7765            sum_we
-000083b0: 6967 6874 735f 6e61 6e20 3d20 7765 6967  ights_nan = weig
-000083c0: 6874 7320 4020 7370 6172 7365 5f63 6f6e  hts @ sparse_con
-000083d0: 7374 7275 6374 6f72 280a 2020 2020 2020  structor(.      
-000083e0: 2020 2020 2020 2020 2020 2020 2020 286e                (n
-000083f0: 702e 6973 6e61 6e28 582e 6461 7461 292c  p.isnan(X.data),
-00008400: 2058 2e69 6e64 6963 6573 2c20 582e 696e   X.indices, X.in
-00008410: 6470 7472 292c 2073 6861 7065 3d58 2e73  dptr), shape=X.s
-00008420: 6861 7065 0a20 2020 2020 2020 2020 2020  hape.           
-00008430: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-00008440: 2020 2020 2020 2073 656c 662e 6e5f 7361         self.n_sa
-00008450: 6d70 6c65 735f 7365 656e 5f20 2b3d 2028  mples_seen_ += (
-00008460: 6e70 2e73 756d 2877 6569 6768 7473 2920  np.sum(weights) 
-00008470: 2d20 7375 6d5f 7765 6967 6874 735f 6e61  - sum_weights_na
-00008480: 6e29 2e61 7374 7970 6528 0a20 2020 2020  n).astype(.     
-00008490: 2020 2020 2020 2020 2020 2020 2020 2064                 d
-000084a0: 7479 7065 0a20 2020 2020 2020 2020 2020  type.           
-000084b0: 2020 2020 2029 0a20 2020 2020 2020 2065       ).        e
-000084c0: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-000084d0: 2023 2046 6972 7374 2070 6173 730a 2020   # First pass.  
-000084e0: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-000084f0: 2068 6173 6174 7472 2873 656c 662c 2022   hasattr(self, "
-00008500: 7363 616c 655f 2229 3a0a 2020 2020 2020  scale_"):.      
-00008510: 2020 2020 2020 2020 2020 7365 6c66 2e6d            self.m
-00008520: 6561 6e5f 203d 2030 2e30 0a20 2020 2020  ean_ = 0.0.     
-00008530: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00008540: 6c66 2e77 6974 685f 7374 643a 0a20 2020  lf.with_std:.   
-00008550: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00008560: 2073 656c 662e 7661 725f 203d 2030 2e30   self.var_ = 0.0
-00008570: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00008580: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-00008590: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000085a0: 7661 725f 203d 204e 6f6e 650a 0a20 2020  var_ = None..   
-000085b0: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not 
-000085c0: 7365 6c66 2e77 6974 685f 6d65 616e 2061  self.with_mean a
-000085d0: 6e64 206e 6f74 2073 656c 662e 7769 7468  nd not self.with
-000085e0: 5f73 7464 3a0a 2020 2020 2020 2020 2020  _std:.          
-000085f0: 2020 2020 2020 7365 6c66 2e6d 6561 6e5f        self.mean_
-00008600: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        
-00008610: 2020 2020 2020 2020 7365 6c66 2e76 6172          self.var
-00008620: 5f20 3d20 4e6f 6e65 0a20 2020 2020 2020  _ = None.       
-00008630: 2020 2020 2020 2020 2073 656c 662e 6e5f           self.n_
-00008640: 7361 6d70 6c65 735f 7365 656e 5f20 2b3d  samples_seen_ +=
-00008650: 2058 2e73 6861 7065 5b30 5d20 2d20 6e70   X.shape[0] - np
-00008660: 2e69 736e 616e 2858 292e 7375 6d28 6178  .isnan(X).sum(ax
-00008670: 6973 3d30 290a 0a20 2020 2020 2020 2020  is=0)..         
-00008680: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-00008690: 2020 2020 2020 2020 2073 656c 662e 6d65           self.me
-000086a0: 616e 5f2c 2073 656c 662e 7661 725f 2c20  an_, self.var_, 
-000086b0: 7365 6c66 2e6e 5f73 616d 706c 6573 5f73  self.n_samples_s
-000086c0: 6565 6e5f 203d 205f 696e 6372 656d 656e  een_ = _incremen
-000086d0: 7461 6c5f 6d65 616e 5f61 6e64 5f76 6172  tal_mean_and_var
-000086e0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-000086f0: 2020 2020 2020 582c 0a20 2020 2020 2020        X,.       
-00008700: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00008710: 662e 6d65 616e 5f2c 0a20 2020 2020 2020  f.mean_,.       
-00008720: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00008730: 662e 7661 725f 2c0a 2020 2020 2020 2020  f.var_,.        
-00008740: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-00008750: 2e6e 5f73 616d 706c 6573 5f73 6565 6e5f  .n_samples_seen_
-00008760: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
-00008770: 2020 2020 2020 7361 6d70 6c65 5f77 6569        sample_wei
-00008780: 6768 743d 7361 6d70 6c65 5f77 6569 6768  ght=sample_weigh
-00008790: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
-000087a0: 2020 2029 0a0a 2020 2020 2020 2020 2320     )..        # 
-000087b0: 666f 7220 6261 636b 7761 7264 2d63 6f6d  for backward-com
-000087c0: 7061 7469 6269 6c69 7479 2c20 7265 6475  patibility, redu
-000087d0: 6365 206e 5f73 616d 706c 6573 5f73 6565  ce n_samples_see
-000087e0: 6e5f 2074 6f20 616e 2069 6e74 6567 6572  n_ to an integer
-000087f0: 0a20 2020 2020 2020 2023 2069 6620 7468  .        # if th
-00008800: 6520 6e75 6d62 6572 206f 6620 7361 6d70  e number of samp
-00008810: 6c65 7320 6973 2074 6865 2073 616d 6520  les is the same 
-00008820: 666f 7220 6561 6368 2066 6561 7475 7265  for each feature
-00008830: 2028 692e 652e 206e 6f0a 2020 2020 2020   (i.e. no.      
-00008840: 2020 2320 6d69 7373 696e 6720 7661 6c75    # missing valu
-00008850: 6573 290a 2020 2020 2020 2020 6966 206e  es).        if n
-00008860: 702e 7074 7028 7365 6c66 2e6e 5f73 616d  p.ptp(self.n_sam
-00008870: 706c 6573 5f73 6565 6e5f 2920 3d3d 2030  ples_seen_) == 0
-00008880: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
-00008890: 6c66 2e6e 5f73 616d 706c 6573 5f73 6565  lf.n_samples_see
-000088a0: 6e5f 203d 2073 656c 662e 6e5f 7361 6d70  n_ = self.n_samp
-000088b0: 6c65 735f 7365 656e 5f5b 305d 0a0a 2020  les_seen_[0]..  
-000088c0: 2020 2020 2020 6966 2073 656c 662e 7769        if self.wi
-000088d0: 7468 5f73 7464 3a0a 2020 2020 2020 2020  th_std:.        
-000088e0: 2020 2020 2320 4578 7472 6163 7420 7468      # Extract th
-000088f0: 6520 6c69 7374 206f 6620 6e65 6172 2063  e list of near c
-00008900: 6f6e 7374 616e 7420 6665 6174 7572 6573  onstant features
-00008910: 206f 6e20 7468 6520 7261 7720 7661 7269   on the raw vari
-00008920: 616e 6365 732c 0a20 2020 2020 2020 2020  ances,.         
-00008930: 2020 2023 2062 6566 6f72 6520 7461 6b69     # before taki
-00008940: 6e67 2074 6865 2073 7175 6172 6520 726f  ng the square ro
-00008950: 6f74 2e0a 2020 2020 2020 2020 2020 2020  ot..            
-00008960: 636f 6e73 7461 6e74 5f6d 6173 6b20 3d20  constant_mask = 
-00008970: 5f69 735f 636f 6e73 7461 6e74 5f66 6561  _is_constant_fea
-00008980: 7475 7265 280a 2020 2020 2020 2020 2020  ture(.          
-00008990: 2020 2020 2020 7365 6c66 2e76 6172 5f2c        self.var_,
-000089a0: 2073 656c 662e 6d65 616e 5f2c 2073 656c   self.mean_, sel
-000089b0: 662e 6e5f 7361 6d70 6c65 735f 7365 656e  f.n_samples_seen
-000089c0: 5f0a 2020 2020 2020 2020 2020 2020 290a  _.            ).
-000089d0: 2020 2020 2020 2020 2020 2020 7365 6c66              self
-000089e0: 2e73 6361 6c65 5f20 3d20 5f68 616e 646c  .scale_ = _handl
-000089f0: 655f 7a65 726f 735f 696e 5f73 6361 6c65  e_zeros_in_scale
-00008a00: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
-00008a10: 2020 6e70 2e73 7172 7428 7365 6c66 2e76    np.sqrt(self.v
-00008a20: 6172 5f29 2c20 636f 7079 3d46 616c 7365  ar_), copy=False
-00008a30: 2c20 636f 6e73 7461 6e74 5f6d 6173 6b3d  , constant_mask=
-00008a40: 636f 6e73 7461 6e74 5f6d 6173 6b0a 2020  constant_mask.  
-00008a50: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
-00008a60: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-00008a70: 2020 2020 2020 7365 6c66 2e73 6361 6c65        self.scale
-00008a80: 5f20 3d20 4e6f 6e65 0a0a 2020 2020 2020  _ = None..      
-00008a90: 2020 7265 7475 726e 2073 656c 660a 0a20    return self.. 
-00008aa0: 2020 2064 6566 2074 7261 6e73 666f 726d     def transform
-00008ab0: 2873 656c 662c 2058 2c20 636f 7079 3d4e  (self, X, copy=N
-00008ac0: 6f6e 6529 3a0a 2020 2020 2020 2020 2222  one):.        ""
-00008ad0: 2250 6572 666f 726d 2073 7461 6e64 6172  "Perform standar
-00008ae0: 6469 7a61 7469 6f6e 2062 7920 6365 6e74  dization by cent
-00008af0: 6572 696e 6720 616e 6420 7363 616c 696e  ering and scalin
-00008b00: 672e 0a0a 2020 2020 2020 2020 5061 7261  g...        Para
-00008b10: 6d65 7465 7273 0a20 2020 2020 2020 202d  meters.        -
-00008b20: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 2020  ---------.      
-00008b30: 2020 5820 3a20 7b61 7272 6179 2d6c 696b    X : {array-lik
-00008b40: 652c 2073 7061 7273 6520 6d61 7472 6978  e, sparse matrix
-00008b50: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-00008b60: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-00008b70: 290a 2020 2020 2020 2020 2020 2020 5468  ).            Th
-00008b80: 6520 6461 7461 2075 7365 6420 746f 2073  e data used to s
-00008b90: 6361 6c65 2061 6c6f 6e67 2074 6865 2066  cale along the f
-00008ba0: 6561 7475 7265 7320 6178 6973 2e0a 2020  eatures axis..  
-00008bb0: 2020 2020 2020 636f 7079 203a 2062 6f6f        copy : boo
-00008bc0: 6c2c 2064 6566 6175 6c74 3d4e 6f6e 650a  l, default=None.
-00008bd0: 2020 2020 2020 2020 2020 2020 436f 7079              Copy
-00008be0: 2074 6865 2069 6e70 7574 2058 206f 7220   the input X or 
-00008bf0: 6e6f 742e 0a0a 2020 2020 2020 2020 5265  not...        Re
-00008c00: 7475 726e 730a 2020 2020 2020 2020 2d2d  turns.        --
-00008c10: 2d2d 2d2d 2d0a 2020 2020 2020 2020 585f  -----.        X_
-00008c20: 7472 203a 207b 6e64 6172 7261 792c 2073  tr : {ndarray, s
-00008c30: 7061 7273 6520 6d61 7472 6978 7d20 6f66  parse matrix} of
-00008c40: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
-00008c50: 732c 206e 5f66 6561 7475 7265 7329 0a20  s, n_features). 
-00008c60: 2020 2020 2020 2020 2020 2054 7261 6e73             Trans
-00008c70: 666f 726d 6564 2061 7272 6179 2e0a 2020  formed array..  
-00008c80: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-00008c90: 2020 6368 6563 6b5f 6973 5f66 6974 7465    check_is_fitte
-00008ca0: 6428 7365 6c66 290a 0a20 2020 2020 2020  d(self)..       
-00008cb0: 2063 6f70 7920 3d20 636f 7079 2069 6620   copy = copy if 
-00008cc0: 636f 7079 2069 7320 6e6f 7420 4e6f 6e65  copy is not None
-00008cd0: 2065 6c73 6520 7365 6c66 2e63 6f70 790a   else self.copy.
-00008ce0: 2020 2020 2020 2020 5820 3d20 7365 6c66          X = self
-00008cf0: 2e5f 7661 6c69 6461 7465 5f64 6174 6128  ._validate_data(
-00008d00: 0a20 2020 2020 2020 2020 2020 2058 2c0a  .            X,.
-00008d10: 2020 2020 2020 2020 2020 2020 7265 7365              rese
-00008d20: 743d 4661 6c73 652c 0a20 2020 2020 2020  t=False,.       
-00008d30: 2020 2020 2061 6363 6570 745f 7370 6172       accept_spar
-00008d40: 7365 3d22 6373 7222 2c0a 2020 2020 2020  se="csr",.      
-00008d50: 2020 2020 2020 636f 7079 3d63 6f70 792c        copy=copy,
-00008d60: 0a20 2020 2020 2020 2020 2020 2064 7479  .            dty
-00008d70: 7065 3d46 4c4f 4154 5f44 5459 5045 532c  pe=FLOAT_DTYPES,
-00008d80: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-00008d90: 6365 5f61 6c6c 5f66 696e 6974 653d 2261  ce_all_finite="a
-00008da0: 6c6c 6f77 2d6e 616e 222c 0a20 2020 2020  llow-nan",.     
-00008db0: 2020 2029 0a0a 2020 2020 2020 2020 6966     )..        if
-00008dc0: 2073 7061 7273 652e 6973 7370 6172 7365   sparse.issparse
-00008dd0: 2858 293a 0a20 2020 2020 2020 2020 2020  (X):.           
-00008de0: 2069 6620 7365 6c66 2e77 6974 685f 6d65   if self.with_me
-00008df0: 616e 3a0a 2020 2020 2020 2020 2020 2020  an:.            
-00008e00: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00008e10: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
-00008e20: 2020 2020 2020 2020 2020 2243 616e 6e6f            "Canno
-00008e30: 7420 6365 6e74 6572 2073 7061 7273 6520  t center sparse 
-00008e40: 6d61 7472 6963 6573 3a20 7061 7373 2060  matrices: pass `
-00008e50: 7769 7468 5f6d 6561 6e3d 4661 6c73 6560  with_mean=False`
-00008e60: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-00008e70: 2020 2020 2020 2022 696e 7374 6561 642e         "instead.
-00008e80: 2053 6565 2064 6f63 7374 7269 6e67 2066   See docstring f
-00008e90: 6f72 206d 6f74 6976 6174 696f 6e20 616e  or motivation an
-00008ea0: 6420 616c 7465 726e 6174 6976 6573 2e22  d alternatives."
-00008eb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00008ec0: 2029 0a20 2020 2020 2020 2020 2020 2069   ).            i
-00008ed0: 6620 7365 6c66 2e73 6361 6c65 5f20 6973  f self.scale_ is
-00008ee0: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
-00008ef0: 2020 2020 2020 2020 2020 2069 6e70 6c61             inpla
-00008f00: 6365 5f63 6f6c 756d 6e5f 7363 616c 6528  ce_column_scale(
-00008f10: 582c 2031 202f 2073 656c 662e 7363 616c  X, 1 / self.scal
-00008f20: 655f 290a 2020 2020 2020 2020 656c 7365  e_).        else
-00008f30: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
-00008f40: 2073 656c 662e 7769 7468 5f6d 6561 6e3a   self.with_mean:
-00008f50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00008f60: 2058 202d 3d20 7365 6c66 2e6d 6561 6e5f   X -= self.mean_
-00008f70: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00008f80: 7365 6c66 2e77 6974 685f 7374 643a 0a20  self.with_std:. 
-00008f90: 2020 2020 2020 2020 2020 2020 2020 2058                 X
-00008fa0: 202f 3d20 7365 6c66 2e73 6361 6c65 5f0a   /= self.scale_.
-00008fb0: 2020 2020 2020 2020 7265 7475 726e 2058          return X
-00008fc0: 0a0a 2020 2020 6465 6620 696e 7665 7273  ..    def invers
-00008fd0: 655f 7472 616e 7366 6f72 6d28 7365 6c66  e_transform(self
-00008fe0: 2c20 582c 2063 6f70 793d 4e6f 6e65 293a  , X, copy=None):
-00008ff0: 0a20 2020 2020 2020 2022 2222 5363 616c  .        """Scal
-00009000: 6520 6261 636b 2074 6865 2064 6174 6120  e back the data 
-00009010: 746f 2074 6865 206f 7269 6769 6e61 6c20  to the original 
-00009020: 7265 7072 6573 656e 7461 7469 6f6e 2e0a  representation..
-00009030: 0a20 2020 2020 2020 2050 6172 616d 6574  .        Paramet
-00009040: 6572 730a 2020 2020 2020 2020 2d2d 2d2d  ers.        ----
-00009050: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2058  ------.        X
-00009060: 203a 207b 6172 7261 792d 6c69 6b65 2c20   : {array-like, 
-00009070: 7370 6172 7365 206d 6174 7269 787d 206f  sparse matrix} o
-00009080: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
-00009090: 6573 2c20 6e5f 6665 6174 7572 6573 290a  es, n_features).
-000090a0: 2020 2020 2020 2020 2020 2020 5468 6520              The 
-000090b0: 6461 7461 2075 7365 6420 746f 2073 6361  data used to sca
-000090c0: 6c65 2061 6c6f 6e67 2074 6865 2066 6561  le along the fea
-000090d0: 7475 7265 7320 6178 6973 2e0a 2020 2020  tures axis..    
-000090e0: 2020 2020 636f 7079 203a 2062 6f6f 6c2c      copy : bool,
-000090f0: 2064 6566 6175 6c74 3d4e 6f6e 650a 2020   default=None.  
-00009100: 2020 2020 2020 2020 2020 436f 7079 2074            Copy t
-00009110: 6865 2069 6e70 7574 2058 206f 7220 6e6f  he input X or no
-00009120: 742e 0a0a 2020 2020 2020 2020 5265 7475  t...        Retu
-00009130: 726e 730a 2020 2020 2020 2020 2d2d 2d2d  rns.        ----
-00009140: 2d2d 2d0a 2020 2020 2020 2020 585f 7472  ---.        X_tr
-00009150: 203a 207b 6e64 6172 7261 792c 2073 7061   : {ndarray, spa
-00009160: 7273 6520 6d61 7472 6978 7d20 6f66 2073  rse matrix} of s
-00009170: 6861 7065 2028 6e5f 7361 6d70 6c65 732c  hape (n_samples,
-00009180: 206e 5f66 6561 7475 7265 7329 0a20 2020   n_features).   
-00009190: 2020 2020 2020 2020 2054 7261 6e73 666f           Transfo
-000091a0: 726d 6564 2061 7272 6179 2e0a 2020 2020  rmed array..    
-000091b0: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-000091c0: 6368 6563 6b5f 6973 5f66 6974 7465 6428  check_is_fitted(
-000091d0: 7365 6c66 290a 0a20 2020 2020 2020 2063  self)..        c
-000091e0: 6f70 7920 3d20 636f 7079 2069 6620 636f  opy = copy if co
-000091f0: 7079 2069 7320 6e6f 7420 4e6f 6e65 2065  py is not None e
-00009200: 6c73 6520 7365 6c66 2e63 6f70 790a 2020  lse self.copy.  
-00009210: 2020 2020 2020 5820 3d20 6368 6563 6b5f        X = check_
-00009220: 6172 7261 7928 0a20 2020 2020 2020 2020  array(.         
-00009230: 2020 2058 2c0a 2020 2020 2020 2020 2020     X,.          
-00009240: 2020 6163 6365 7074 5f73 7061 7273 653d    accept_sparse=
-00009250: 2263 7372 222c 0a20 2020 2020 2020 2020  "csr",.         
-00009260: 2020 2063 6f70 793d 636f 7079 2c0a 2020     copy=copy,.  
-00009270: 2020 2020 2020 2020 2020 6474 7970 653d            dtype=
-00009280: 464c 4f41 545f 4454 5950 4553 2c0a 2020  FLOAT_DTYPES,.  
-00009290: 2020 2020 2020 2020 2020 666f 7263 655f            force_
-000092a0: 616c 6c5f 6669 6e69 7465 3d22 616c 6c6f  all_finite="allo
-000092b0: 772d 6e61 6e22 2c0a 2020 2020 2020 2020  w-nan",.        
-000092c0: 290a 0a20 2020 2020 2020 2069 6620 7370  )..        if sp
-000092d0: 6172 7365 2e69 7373 7061 7273 6528 5829  arse.issparse(X)
-000092e0: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
-000092f0: 2073 656c 662e 7769 7468 5f6d 6561 6e3a   self.with_mean:
-00009300: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00009310: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-00009320: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
-00009330: 2020 2020 2020 2022 4361 6e6e 6f74 2075         "Cannot u
-00009340: 6e63 656e 7465 7220 7370 6172 7365 206d  ncenter sparse m
-00009350: 6174 7269 6365 733a 2070 6173 7320 6077  atrices: pass `w
-00009360: 6974 685f 6d65 616e 3d46 616c 7365 6020  ith_mean=False` 
-00009370: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
-00009380: 2020 2020 2020 2269 6e73 7465 6164 2053        "instead S
-00009390: 6565 2064 6f63 7374 7269 6e67 2066 6f72  ee docstring for
-000093a0: 206d 6f74 6976 6174 696f 6e20 616e 6420   motivation and 
-000093b0: 616c 7465 726e 6174 6976 6573 2e22 0a20  alternatives.". 
-000093c0: 2020 2020 2020 2020 2020 2020 2020 2029                 )
-000093d0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-000093e0: 7365 6c66 2e73 6361 6c65 5f20 6973 206e  self.scale_ is n
-000093f0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       
-00009400: 2020 2020 2020 2020 2069 6e70 6c61 6365           inplace
-00009410: 5f63 6f6c 756d 6e5f 7363 616c 6528 582c  _column_scale(X,
-00009420: 2073 656c 662e 7363 616c 655f 290a 2020   self.scale_).  
-00009430: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-00009440: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00009450: 7769 7468 5f73 7464 3a0a 2020 2020 2020  with_std:.      
-00009460: 2020 2020 2020 2020 2020 5820 2a3d 2073            X *= s
-00009470: 656c 662e 7363 616c 655f 0a20 2020 2020  elf.scale_.     
-00009480: 2020 2020 2020 2069 6620 7365 6c66 2e77         if self.w
-00009490: 6974 685f 6d65 616e 3a0a 2020 2020 2020  ith_mean:.      
-000094a0: 2020 2020 2020 2020 2020 5820 2b3d 2073            X += s
-000094b0: 656c 662e 6d65 616e 5f0a 2020 2020 2020  elf.mean_.      
-000094c0: 2020 7265 7475 726e 2058 0a0a 2020 2020    return X..    
-000094d0: 6465 6620 5f6d 6f72 655f 7461 6773 2873  def _more_tags(s
-000094e0: 656c 6629 3a0a 2020 2020 2020 2020 7265  elf):.        re
-000094f0: 7475 726e 207b 2261 6c6c 6f77 5f6e 616e  turn {"allow_nan
-00009500: 223a 2054 7275 652c 2022 7072 6573 6572  ": True, "preser
-00009510: 7665 735f 6474 7970 6522 3a20 5b6e 702e  ves_dtype": [np.
-00009520: 666c 6f61 7436 342c 206e 702e 666c 6f61  float64, np.floa
-00009530: 7433 325d 7d0a 0a0a 636c 6173 7320 4d61  t32]}...class Ma
-00009540: 7841 6273 5363 616c 6572 284f 6e65 546f  xAbsScaler(OneTo
-00009550: 4f6e 6546 6561 7475 7265 4d69 7869 6e2c  OneFeatureMixin,
-00009560: 2054 7261 6e73 666f 726d 6572 4d69 7869   TransformerMixi
-00009570: 6e2c 2042 6173 6545 7374 696d 6174 6f72  n, BaseEstimator
-00009580: 293a 0a20 2020 2022 2222 5363 616c 6520  ):.    """Scale 
-00009590: 6561 6368 2066 6561 7475 7265 2062 7920  each feature by 
-000095a0: 6974 7320 6d61 7869 6d75 6d20 6162 736f  its maximum abso
-000095b0: 6c75 7465 2076 616c 7565 2e0a 0a20 2020  lute value...   
-000095c0: 2054 6869 7320 6573 7469 6d61 746f 7220   This estimator 
-000095d0: 7363 616c 6573 2061 6e64 2074 7261 6e73  scales and trans
-000095e0: 6c61 7465 7320 6561 6368 2066 6561 7475  lates each featu
-000095f0: 7265 2069 6e64 6976 6964 7561 6c6c 7920  re individually 
-00009600: 7375 6368 0a20 2020 2074 6861 7420 7468  such.    that th
-00009610: 6520 6d61 7869 6d61 6c20 6162 736f 6c75  e maximal absolu
-00009620: 7465 2076 616c 7565 206f 6620 6561 6368  te value of each
-00009630: 2066 6561 7475 7265 2069 6e20 7468 650a   feature in the.
-00009640: 2020 2020 7472 6169 6e69 6e67 2073 6574      training set
-00009650: 2077 696c 6c20 6265 2031 2e30 2e20 4974   will be 1.0. It
-00009660: 2064 6f65 7320 6e6f 7420 7368 6966 742f   does not shift/
-00009670: 6365 6e74 6572 2074 6865 2064 6174 612c  center the data,
-00009680: 2061 6e64 0a20 2020 2074 6875 7320 646f   and.    thus do
-00009690: 6573 206e 6f74 2064 6573 7472 6f79 2061  es not destroy a
-000096a0: 6e79 2073 7061 7273 6974 792e 0a0a 2020  ny sparsity...  
-000096b0: 2020 5468 6973 2073 6361 6c65 7220 6361    This scaler ca
-000096c0: 6e20 616c 736f 2062 6520 6170 706c 6965  n also be applie
-000096d0: 6420 746f 2073 7061 7273 6520 4353 5220  d to sparse CSR 
-000096e0: 6f72 2043 5343 206d 6174 7269 6365 732e  or CSC matrices.
-000096f0: 0a0a 2020 2020 604d 6178 4162 7353 6361  ..    `MaxAbsSca
-00009700: 6c65 7260 2064 6f65 736e 2774 2072 6564  ler` doesn't red
-00009710: 7563 6520 7468 6520 6566 6665 6374 206f  uce the effect o
-00009720: 6620 6f75 746c 6965 7273 3b20 6974 206f  f outliers; it o
-00009730: 6e6c 7920 6c69 6e65 6172 6c79 0a20 2020  nly linearly.   
-00009740: 2073 6361 6c65 7320 7468 656d 2064 6f77   scales them dow
-00009750: 6e2e 2046 6f72 2061 6e20 6578 616d 706c  n. For an exampl
-00009760: 6520 7669 7375 616c 697a 6174 696f 6e2c  e visualization,
-00009770: 2072 6566 6572 2074 6f20 3a72 6566 3a60   refer to :ref:`
-00009780: 436f 6d70 6172 650a 2020 2020 4d61 7841  Compare.    MaxA
-00009790: 6273 5363 616c 6572 2077 6974 6820 6f74  bsScaler with ot
-000097a0: 6865 7220 7363 616c 6572 7320 3c70 6c6f  her scalers <plo
-000097b0: 745f 616c 6c5f 7363 616c 696e 675f 6d61  t_all_scaling_ma
-000097c0: 785f 6162 735f 7363 616c 6572 5f73 6563  x_abs_scaler_sec
-000097d0: 7469 6f6e 3e60 2e0a 0a20 2020 202e 2e20  tion>`...    .. 
-000097e0: 7665 7273 696f 6e61 6464 6564 3a3a 2030  versionadded:: 0
-000097f0: 2e31 370a 0a20 2020 2050 6172 616d 6574  .17..    Paramet
-00009800: 6572 730a 2020 2020 2d2d 2d2d 2d2d 2d2d  ers.    --------
-00009810: 2d2d 0a20 2020 2063 6f70 7920 3a20 626f  --.    copy : bo
-00009820: 6f6c 2c20 6465 6661 756c 743d 5472 7565  ol, default=True
-00009830: 0a20 2020 2020 2020 2053 6574 2074 6f20  .        Set to 
-00009840: 4661 6c73 6520 746f 2070 6572 666f 726d  False to perform
-00009850: 2069 6e70 6c61 6365 2073 6361 6c69 6e67   inplace scaling
-00009860: 2061 6e64 2061 766f 6964 2061 2063 6f70   and avoid a cop
-00009870: 7920 2869 6620 7468 6520 696e 7075 740a  y (if the input.
-00009880: 2020 2020 2020 2020 6973 2061 6c72 6561          is alrea
-00009890: 6479 2061 206e 756d 7079 2061 7272 6179  dy a numpy array
-000098a0: 292e 0a0a 2020 2020 4174 7472 6962 7574  )...    Attribut
-000098b0: 6573 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d  es.    ---------
-000098c0: 2d0a 2020 2020 7363 616c 655f 203a 206e  -.    scale_ : n
-000098d0: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
-000098e0: 286e 5f66 6561 7475 7265 732c 290a 2020  (n_features,).  
-000098f0: 2020 2020 2020 5065 7220 6665 6174 7572        Per featur
-00009900: 6520 7265 6c61 7469 7665 2073 6361 6c69  e relative scali
-00009910: 6e67 206f 6620 7468 6520 6461 7461 2e0a  ng of the data..
-00009920: 0a20 2020 2020 2020 202e 2e20 7665 7273  .        .. vers
-00009930: 696f 6e61 6464 6564 3a3a 2030 2e31 370a  ionadded:: 0.17.
-00009940: 2020 2020 2020 2020 2020 202a 7363 616c             *scal
-00009950: 655f 2a20 6174 7472 6962 7574 652e 0a0a  e_* attribute...
-00009960: 2020 2020 6d61 785f 6162 735f 203a 206e      max_abs_ : n
-00009970: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
-00009980: 286e 5f66 6561 7475 7265 732c 290a 2020  (n_features,).  
-00009990: 2020 2020 2020 5065 7220 6665 6174 7572        Per featur
-000099a0: 6520 6d61 7869 6d75 6d20 6162 736f 6c75  e maximum absolu
-000099b0: 7465 2076 616c 7565 2e0a 0a20 2020 206e  te value...    n
-000099c0: 5f66 6561 7475 7265 735f 696e 5f20 3a20  _features_in_ : 
-000099d0: 696e 740a 2020 2020 2020 2020 4e75 6d62  int.        Numb
-000099e0: 6572 206f 6620 6665 6174 7572 6573 2073  er of features s
-000099f0: 6565 6e20 6475 7269 6e67 203a 7465 726d  een during :term
-00009a00: 3a60 6669 7460 2e0a 0a20 2020 2020 2020  :`fit`...       
-00009a10: 202e 2e20 7665 7273 696f 6e61 6464 6564   .. versionadded
-00009a20: 3a3a 2030 2e32 340a 0a20 2020 2066 6561  :: 0.24..    fea
-00009a30: 7475 7265 5f6e 616d 6573 5f69 6e5f 203a  ture_names_in_ :
-00009a40: 206e 6461 7272 6179 206f 6620 7368 6170   ndarray of shap
-00009a50: 6520 2860 6e5f 6665 6174 7572 6573 5f69  e (`n_features_i
-00009a60: 6e5f 602c 290a 2020 2020 2020 2020 4e61  n_`,).        Na
-00009a70: 6d65 7320 6f66 2066 6561 7475 7265 7320  mes of features 
-00009a80: 7365 656e 2064 7572 696e 6720 3a74 6572  seen during :ter
-00009a90: 6d3a 6066 6974 602e 2044 6566 696e 6564  m:`fit`. Defined
-00009aa0: 206f 6e6c 7920 7768 656e 2060 5860 0a20   only when `X`. 
-00009ab0: 2020 2020 2020 2068 6173 2066 6561 7475         has featu
-00009ac0: 7265 206e 616d 6573 2074 6861 7420 6172  re names that ar
-00009ad0: 6520 616c 6c20 7374 7269 6e67 732e 0a0a  e all strings...
-00009ae0: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
-00009af0: 6f6e 6164 6465 643a 3a20 312e 300a 0a20  onadded:: 1.0.. 
-00009b00: 2020 206e 5f73 616d 706c 6573 5f73 6565     n_samples_see
-00009b10: 6e5f 203a 2069 6e74 0a20 2020 2020 2020  n_ : int.       
-00009b20: 2054 6865 206e 756d 6265 7220 6f66 2073   The number of s
-00009b30: 616d 706c 6573 2070 726f 6365 7373 6564  amples processed
-00009b40: 2062 7920 7468 6520 6573 7469 6d61 746f   by the estimato
-00009b50: 722e 2057 696c 6c20 6265 2072 6573 6574  r. Will be reset
-00009b60: 206f 6e0a 2020 2020 2020 2020 6e65 7720   on.        new 
-00009b70: 6361 6c6c 7320 746f 2066 6974 2c20 6275  calls to fit, bu
-00009b80: 7420 696e 6372 656d 656e 7473 2061 6372  t increments acr
-00009b90: 6f73 7320 6060 7061 7274 6961 6c5f 6669  oss ``partial_fi
-00009ba0: 7460 6020 6361 6c6c 732e 0a0a 2020 2020  t`` calls...    
-00009bb0: 5365 6520 416c 736f 0a20 2020 202d 2d2d  See Also.    ---
-00009bc0: 2d2d 2d2d 2d0a 2020 2020 6d61 7861 6273  -----.    maxabs
-00009bd0: 5f73 6361 6c65 203a 2045 7175 6976 616c  _scale : Equival
-00009be0: 656e 7420 6675 6e63 7469 6f6e 2077 6974  ent function wit
-00009bf0: 686f 7574 2074 6865 2065 7374 696d 6174  hout the estimat
-00009c00: 6f72 2041 5049 2e0a 0a20 2020 204e 6f74  or API...    Not
-00009c10: 6573 0a20 2020 202d 2d2d 2d2d 0a20 2020  es.    -----.   
-00009c20: 204e 614e 7320 6172 6520 7472 6561 7465   NaNs are treate
-00009c30: 6420 6173 206d 6973 7369 6e67 2076 616c  d as missing val
-00009c40: 7565 733a 2064 6973 7265 6761 7264 6564  ues: disregarded
-00009c50: 2069 6e20 6669 742c 2061 6e64 206d 6169   in fit, and mai
-00009c60: 6e74 6169 6e65 6420 696e 0a20 2020 2074  ntained in.    t
-00009c70: 7261 6e73 666f 726d 2e0a 0a20 2020 2045  ransform...    E
-00009c80: 7861 6d70 6c65 730a 2020 2020 2d2d 2d2d  xamples.    ----
-00009c90: 2d2d 2d2d 0a20 2020 203e 3e3e 2066 726f  ----.    >>> fro
-00009ca0: 6d20 736b 6c65 6172 6e2e 7072 6570 726f  m sklearn.prepro
-00009cb0: 6365 7373 696e 6720 696d 706f 7274 204d  cessing import M
-00009cc0: 6178 4162 7353 6361 6c65 720a 2020 2020  axAbsScaler.    
-00009cd0: 3e3e 3e20 5820 3d20 5b5b 2031 2e2c 202d  >>> X = [[ 1., -
-00009ce0: 312e 2c20 2032 2e5d 2c0a 2020 2020 2e2e  1.,  2.],.    ..
-00009cf0: 2e20 2020 2020 205b 2032 2e2c 2020 302e  .      [ 2.,  0.
-00009d00: 2c20 2030 2e5d 2c0a 2020 2020 2e2e 2e20  ,  0.],.    ... 
-00009d10: 2020 2020 205b 2030 2e2c 2020 312e 2c20       [ 0.,  1., 
-00009d20: 2d31 2e5d 5d0a 2020 2020 3e3e 3e20 7472  -1.]].    >>> tr
-00009d30: 616e 7366 6f72 6d65 7220 3d20 4d61 7841  ansformer = MaxA
-00009d40: 6273 5363 616c 6572 2829 2e66 6974 2858  bsScaler().fit(X
-00009d50: 290a 2020 2020 3e3e 3e20 7472 616e 7366  ).    >>> transf
-00009d60: 6f72 6d65 720a 2020 2020 4d61 7841 6273  ormer.    MaxAbs
-00009d70: 5363 616c 6572 2829 0a20 2020 203e 3e3e  Scaler().    >>>
-00009d80: 2074 7261 6e73 666f 726d 6572 2e74 7261   transformer.tra
-00009d90: 6e73 666f 726d 2858 290a 2020 2020 6172  nsform(X).    ar
-00009da0: 7261 7928 5b5b 2030 2e35 2c20 2d31 2e20  ray([[ 0.5, -1. 
-00009db0: 2c20 2031 2e20 5d2c 0a20 2020 2020 2020  ,  1. ],.       
-00009dc0: 2020 2020 5b20 312e 202c 2020 302e 202c      [ 1. ,  0. ,
-00009dd0: 2020 302e 205d 2c0a 2020 2020 2020 2020    0. ],.        
-00009de0: 2020 205b 2030 2e20 2c20 2031 2e20 2c20     [ 0. ,  1. , 
-00009df0: 2d30 2e35 5d5d 290a 2020 2020 2222 220a  -0.5]]).    """.
-00009e00: 0a20 2020 205f 7061 7261 6d65 7465 725f  .    _parameter_
-00009e10: 636f 6e73 7472 6169 6e74 733a 2064 6963  constraints: dic
-00009e20: 7420 3d20 7b22 636f 7079 223a 205b 2262  t = {"copy": ["b
-00009e30: 6f6f 6c65 616e 225d 7d0a 0a20 2020 2064  oolean"]}..    d
-00009e40: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
-00009e50: 2c20 2a2c 2063 6f70 793d 5472 7565 293a  , *, copy=True):
-00009e60: 0a20 2020 2020 2020 2073 656c 662e 636f  .        self.co
-00009e70: 7079 203d 2063 6f70 790a 0a20 2020 2064  py = copy..    d
-00009e80: 6566 205f 7265 7365 7428 7365 6c66 293a  ef _reset(self):
-00009e90: 0a20 2020 2020 2020 2022 2222 5265 7365  .        """Rese
-00009ea0: 7420 696e 7465 726e 616c 2064 6174 612d  t internal data-
-00009eb0: 6465 7065 6e64 656e 7420 7374 6174 6520  dependent state 
-00009ec0: 6f66 2074 6865 2073 6361 6c65 722c 2069  of the scaler, i
-00009ed0: 6620 6e65 6365 7373 6172 792e 0a0a 2020  f necessary...  
-00009ee0: 2020 2020 2020 5f5f 696e 6974 5f5f 2070        __init__ p
-00009ef0: 6172 616d 6574 6572 7320 6172 6520 6e6f  arameters are no
-00009f00: 7420 746f 7563 6865 642e 0a20 2020 2020  t touched..     
-00009f10: 2020 2022 2222 0a20 2020 2020 2020 2023     """.        #
-00009f20: 2043 6865 636b 696e 6720 6f6e 6520 6174   Checking one at
-00009f30: 7472 6962 7574 6520 6973 2065 6e6f 7567  tribute is enoug
-00009f40: 682c 2062 6563 6175 7365 2074 6865 7920  h, because they 
-00009f50: 6172 6520 616c 6c20 7365 7420 746f 6765  are all set toge
-00009f60: 7468 6572 0a20 2020 2020 2020 2023 2069  ther.        # i
-00009f70: 6e20 7061 7274 6961 6c5f 6669 740a 2020  n partial_fit.  
-00009f80: 2020 2020 2020 6966 2068 6173 6174 7472        if hasattr
-00009f90: 2873 656c 662c 2022 7363 616c 655f 2229  (self, "scale_")
-00009fa0: 3a0a 2020 2020 2020 2020 2020 2020 6465  :.            de
-00009fb0: 6c20 7365 6c66 2e73 6361 6c65 5f0a 2020  l self.scale_.  
-00009fc0: 2020 2020 2020 2020 2020 6465 6c20 7365            del se
-00009fd0: 6c66 2e6e 5f73 616d 706c 6573 5f73 6565  lf.n_samples_see
-00009fe0: 6e5f 0a20 2020 2020 2020 2020 2020 2064  n_.            d
-00009ff0: 656c 2073 656c 662e 6d61 785f 6162 735f  el self.max_abs_
-0000a000: 0a0a 2020 2020 6465 6620 6669 7428 7365  ..    def fit(se
-0000a010: 6c66 2c20 582c 2079 3d4e 6f6e 6529 3a0a  lf, X, y=None):.
-0000a020: 2020 2020 2020 2020 2222 2243 6f6d 7075          """Compu
-0000a030: 7465 2074 6865 206d 6178 696d 756d 2061  te the maximum a
-0000a040: 6273 6f6c 7574 6520 7661 6c75 6520 746f  bsolute value to
-0000a050: 2062 6520 7573 6564 2066 6f72 206c 6174   be used for lat
-0000a060: 6572 2073 6361 6c69 6e67 2e0a 0a20 2020  er scaling...   
-0000a070: 2020 2020 2050 6172 616d 6574 6572 730a       Parameters.
-0000a080: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d2d          --------
-0000a090: 2d2d 0a20 2020 2020 2020 2058 203a 207b  --.        X : {
-0000a0a0: 6172 7261 792d 6c69 6b65 2c20 7370 6172  array-like, spar
-0000a0b0: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
-0000a0c0: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
-0000a0d0: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
-0000a0e0: 2020 2020 2020 2020 5468 6520 6461 7461          The data
-0000a0f0: 2075 7365 6420 746f 2063 6f6d 7075 7465   used to compute
-0000a100: 2074 6865 2070 6572 2d66 6561 7475 7265   the per-feature
-0000a110: 206d 696e 696d 756d 2061 6e64 206d 6178   minimum and max
-0000a120: 696d 756d 0a20 2020 2020 2020 2020 2020  imum.           
-0000a130: 2075 7365 6420 666f 7220 6c61 7465 7220   used for later 
-0000a140: 7363 616c 696e 6720 616c 6f6e 6720 7468  scaling along th
-0000a150: 6520 6665 6174 7572 6573 2061 7869 732e  e features axis.
-0000a160: 0a0a 2020 2020 2020 2020 7920 3a20 4e6f  ..        y : No
-0000a170: 6e65 0a20 2020 2020 2020 2020 2020 2049  ne.            I
-0000a180: 676e 6f72 6564 2e0a 0a20 2020 2020 2020  gnored...       
-0000a190: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
-0000a1a0: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
-0000a1b0: 2073 656c 6620 3a20 6f62 6a65 6374 0a20   self : object. 
-0000a1c0: 2020 2020 2020 2020 2020 2046 6974 7465             Fitte
-0000a1d0: 6420 7363 616c 6572 2e0a 2020 2020 2020  d scaler..      
-0000a1e0: 2020 2222 220a 2020 2020 2020 2020 2320    """.        # 
-0000a1f0: 5265 7365 7420 696e 7465 726e 616c 2073  Reset internal s
-0000a200: 7461 7465 2062 6566 6f72 6520 6669 7474  tate before fitt
-0000a210: 696e 670a 2020 2020 2020 2020 7365 6c66  ing.        self
-0000a220: 2e5f 7265 7365 7428 290a 2020 2020 2020  ._reset().      
-0000a230: 2020 7265 7475 726e 2073 656c 662e 7061    return self.pa
-0000a240: 7274 6961 6c5f 6669 7428 582c 2079 290a  rtial_fit(X, y).
-0000a250: 0a20 2020 2040 5f66 6974 5f63 6f6e 7465  .    @_fit_conte
-0000a260: 7874 2870 7265 6665 725f 736b 6970 5f6e  xt(prefer_skip_n
-0000a270: 6573 7465 645f 7661 6c69 6461 7469 6f6e  ested_validation
-0000a280: 3d54 7275 6529 0a20 2020 2064 6566 2070  =True).    def p
-0000a290: 6172 7469 616c 5f66 6974 2873 656c 662c  artial_fit(self,
-0000a2a0: 2058 2c20 793d 4e6f 6e65 293a 0a20 2020   X, y=None):.   
-0000a2b0: 2020 2020 2022 2222 4f6e 6c69 6e65 2063       """Online c
-0000a2c0: 6f6d 7075 7461 7469 6f6e 206f 6620 6d61  omputation of ma
-0000a2d0: 7820 6162 736f 6c75 7465 2076 616c 7565  x absolute value
-0000a2e0: 206f 6620 5820 666f 7220 6c61 7465 7220   of X for later 
-0000a2f0: 7363 616c 696e 672e 0a0a 2020 2020 2020  scaling...      
-0000a300: 2020 416c 6c20 6f66 2058 2069 7320 7072    All of X is pr
-0000a310: 6f63 6573 7365 6420 6173 2061 2073 696e  ocessed as a sin
-0000a320: 676c 6520 6261 7463 682e 2054 6869 7320  gle batch. This 
-0000a330: 6973 2069 6e74 656e 6465 6420 666f 7220  is intended for 
-0000a340: 6361 7365 730a 2020 2020 2020 2020 7768  cases.        wh
-0000a350: 656e 203a 6d65 7468 3a60 6669 7460 2069  en :meth:`fit` i
-0000a360: 7320 6e6f 7420 6665 6173 6962 6c65 2064  s not feasible d
-0000a370: 7565 2074 6f20 7665 7279 206c 6172 6765  ue to very large
-0000a380: 206e 756d 6265 7220 6f66 0a20 2020 2020   number of.     
-0000a390: 2020 2060 6e5f 7361 6d70 6c65 7360 206f     `n_samples` o
-0000a3a0: 7220 6265 6361 7573 6520 5820 6973 2072  r because X is r
-0000a3b0: 6561 6420 6672 6f6d 2061 2063 6f6e 7469  ead from a conti
-0000a3c0: 6e75 6f75 7320 7374 7265 616d 2e0a 0a20  nuous stream... 
-0000a3d0: 2020 2020 2020 2050 6172 616d 6574 6572         Parameter
-0000a3e0: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
-0000a3f0: 2d2d 2d2d 0a20 2020 2020 2020 2058 203a  ----.        X :
-0000a400: 207b 6172 7261 792d 6c69 6b65 2c20 7370   {array-like, sp
-0000a410: 6172 7365 206d 6174 7269 787d 206f 6620  arse matrix} of 
-0000a420: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
-0000a430: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
-0000a440: 2020 2020 2020 2020 2020 5468 6520 6461            The da
-0000a450: 7461 2075 7365 6420 746f 2063 6f6d 7075  ta used to compu
-0000a460: 7465 2074 6865 206d 6561 6e20 616e 6420  te the mean and 
-0000a470: 7374 616e 6461 7264 2064 6576 6961 7469  standard deviati
-0000a480: 6f6e 0a20 2020 2020 2020 2020 2020 2075  on.            u
-0000a490: 7365 6420 666f 7220 6c61 7465 7220 7363  sed for later sc
-0000a4a0: 616c 696e 6720 616c 6f6e 6720 7468 6520  aling along the 
-0000a4b0: 6665 6174 7572 6573 2061 7869 732e 0a0a  features axis...
-0000a4c0: 2020 2020 2020 2020 7920 3a20 4e6f 6e65          y : None
-0000a4d0: 0a20 2020 2020 2020 2020 2020 2049 676e  .            Ign
-0000a4e0: 6f72 6564 2e0a 0a20 2020 2020 2020 2052  ored...        R
-0000a4f0: 6574 7572 6e73 0a20 2020 2020 2020 202d  eturns.        -
-0000a500: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2073  ------.        s
-0000a510: 656c 6620 3a20 6f62 6a65 6374 0a20 2020  elf : object.   
-0000a520: 2020 2020 2020 2020 2046 6974 7465 6420           Fitted 
-0000a530: 7363 616c 6572 2e0a 2020 2020 2020 2020  scaler..        
-0000a540: 2222 220a 2020 2020 2020 2020 7870 2c20  """.        xp, 
-0000a550: 5f20 3d20 6765 745f 6e61 6d65 7370 6163  _ = get_namespac
-0000a560: 6528 5829 0a0a 2020 2020 2020 2020 6669  e(X)..        fi
-0000a570: 7273 745f 7061 7373 203d 206e 6f74 2068  rst_pass = not h
-0000a580: 6173 6174 7472 2873 656c 662c 2022 6e5f  asattr(self, "n_
-0000a590: 7361 6d70 6c65 735f 7365 656e 5f22 290a  samples_seen_").
-0000a5a0: 2020 2020 2020 2020 5820 3d20 7365 6c66          X = self
-0000a5b0: 2e5f 7661 6c69 6461 7465 5f64 6174 6128  ._validate_data(
-0000a5c0: 0a20 2020 2020 2020 2020 2020 2058 2c0a  .            X,.
-0000a5d0: 2020 2020 2020 2020 2020 2020 7265 7365              rese
-0000a5e0: 743d 6669 7273 745f 7061 7373 2c0a 2020  t=first_pass,.  
-0000a5f0: 2020 2020 2020 2020 2020 6163 6365 7074            accept
-0000a600: 5f73 7061 7273 653d 2822 6373 7222 2c20  _sparse=("csr", 
-0000a610: 2263 7363 2229 2c0a 2020 2020 2020 2020  "csc"),.        
-0000a620: 2020 2020 6474 7970 653d 5f61 7272 6179      dtype=_array
-0000a630: 5f61 7069 2e73 7570 706f 7274 6564 5f66  _api.supported_f
-0000a640: 6c6f 6174 5f64 7479 7065 7328 7870 292c  loat_dtypes(xp),
-0000a650: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-0000a660: 6365 5f61 6c6c 5f66 696e 6974 653d 2261  ce_all_finite="a
-0000a670: 6c6c 6f77 2d6e 616e 222c 0a20 2020 2020  llow-nan",.     
-0000a680: 2020 2029 0a0a 2020 2020 2020 2020 6966     )..        if
-0000a690: 2073 7061 7273 652e 6973 7370 6172 7365   sparse.issparse
-0000a6a0: 2858 293a 0a20 2020 2020 2020 2020 2020  (X):.           
-0000a6b0: 206d 696e 732c 206d 6178 7320 3d20 6d69   mins, maxs = mi
-0000a6c0: 6e5f 6d61 785f 6178 6973 2858 2c20 6178  n_max_axis(X, ax
-0000a6d0: 6973 3d30 2c20 6967 6e6f 7265 5f6e 616e  is=0, ignore_nan
-0000a6e0: 3d54 7275 6529 0a20 2020 2020 2020 2020  =True).         
-0000a6f0: 2020 206d 6178 5f61 6273 203d 206e 702e     max_abs = np.
-0000a700: 6d61 7869 6d75 6d28 6e70 2e61 6273 286d  maximum(np.abs(m
-0000a710: 696e 7329 2c20 6e70 2e61 6273 286d 6178  ins), np.abs(max
-0000a720: 7329 290a 2020 2020 2020 2020 656c 7365  s)).        else
-0000a730: 3a0a 2020 2020 2020 2020 2020 2020 6d61  :.            ma
-0000a740: 785f 6162 7320 3d20 5f61 7272 6179 5f61  x_abs = _array_a
-0000a750: 7069 2e5f 6e61 6e6d 6178 2878 702e 6162  pi._nanmax(xp.ab
-0000a760: 7328 5829 2c20 6178 6973 3d30 290a 0a20  s(X), axis=0).. 
-0000a770: 2020 2020 2020 2069 6620 6669 7273 745f         if first_
-0000a780: 7061 7373 3a0a 2020 2020 2020 2020 2020  pass:.          
-0000a790: 2020 7365 6c66 2e6e 5f73 616d 706c 6573    self.n_samples
-0000a7a0: 5f73 6565 6e5f 203d 2058 2e73 6861 7065  _seen_ = X.shape
-0000a7b0: 5b30 5d0a 2020 2020 2020 2020 656c 7365  [0].        else
-0000a7c0: 3a0a 2020 2020 2020 2020 2020 2020 6d61  :.            ma
-0000a7d0: 785f 6162 7320 3d20 7870 2e6d 6178 696d  x_abs = xp.maxim
-0000a7e0: 756d 2873 656c 662e 6d61 785f 6162 735f  um(self.max_abs_
-0000a7f0: 2c20 6d61 785f 6162 7329 0a20 2020 2020  , max_abs).     
-0000a800: 2020 2020 2020 2073 656c 662e 6e5f 7361         self.n_sa
-0000a810: 6d70 6c65 735f 7365 656e 5f20 2b3d 2058  mples_seen_ += X
-0000a820: 2e73 6861 7065 5b30 5d0a 0a20 2020 2020  .shape[0]..     
-0000a830: 2020 2073 656c 662e 6d61 785f 6162 735f     self.max_abs_
-0000a840: 203d 206d 6178 5f61 6273 0a20 2020 2020   = max_abs.     
-0000a850: 2020 2073 656c 662e 7363 616c 655f 203d     self.scale_ =
-0000a860: 205f 6861 6e64 6c65 5f7a 6572 6f73 5f69   _handle_zeros_i
-0000a870: 6e5f 7363 616c 6528 6d61 785f 6162 732c  n_scale(max_abs,
-0000a880: 2063 6f70 793d 5472 7565 290a 2020 2020   copy=True).    
-0000a890: 2020 2020 7265 7475 726e 2073 656c 660a      return self.
-0000a8a0: 0a20 2020 2064 6566 2074 7261 6e73 666f  .    def transfo
-0000a8b0: 726d 2873 656c 662c 2058 293a 0a20 2020  rm(self, X):.   
-0000a8c0: 2020 2020 2022 2222 5363 616c 6520 7468       """Scale th
-0000a8d0: 6520 6461 7461 2e0a 0a20 2020 2020 2020  e data...       
-0000a8e0: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
-0000a8f0: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
-0000a900: 2020 2020 2020 2058 203a 207b 6172 7261         X : {arra
-0000a910: 792d 6c69 6b65 2c20 7370 6172 7365 206d  y-like, sparse m
-0000a920: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
-0000a930: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
-0000a940: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-0000a950: 2020 2020 5468 6520 6461 7461 2074 6861      The data tha
-0000a960: 7420 7368 6f75 6c64 2062 6520 7363 616c  t should be scal
-0000a970: 6564 2e0a 0a20 2020 2020 2020 2052 6574  ed...        Ret
-0000a980: 7572 6e73 0a20 2020 2020 2020 202d 2d2d  urns.        ---
-0000a990: 2d2d 2d2d 0a20 2020 2020 2020 2058 5f74  ----.        X_t
-0000a9a0: 7220 3a20 7b6e 6461 7272 6179 2c20 7370  r : {ndarray, sp
-0000a9b0: 6172 7365 206d 6174 7269 787d 206f 6620  arse matrix} of 
-0000a9c0: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
-0000a9d0: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
-0000a9e0: 2020 2020 2020 2020 2020 5472 616e 7366            Transf
-0000a9f0: 6f72 6d65 6420 6172 7261 792e 0a20 2020  ormed array..   
-0000aa00: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
-0000aa10: 2063 6865 636b 5f69 735f 6669 7474 6564   check_is_fitted
-0000aa20: 2873 656c 6629 0a0a 2020 2020 2020 2020  (self)..        
-0000aa30: 7870 2c20 5f20 3d20 6765 745f 6e61 6d65  xp, _ = get_name
-0000aa40: 7370 6163 6528 5829 0a0a 2020 2020 2020  space(X)..      
-0000aa50: 2020 5820 3d20 7365 6c66 2e5f 7661 6c69    X = self._vali
-0000aa60: 6461 7465 5f64 6174 6128 0a20 2020 2020  date_data(.     
-0000aa70: 2020 2020 2020 2058 2c0a 2020 2020 2020         X,.      
-0000aa80: 2020 2020 2020 6163 6365 7074 5f73 7061        accept_spa
-0000aa90: 7273 653d 2822 6373 7222 2c20 2263 7363  rse=("csr", "csc
-0000aaa0: 2229 2c0a 2020 2020 2020 2020 2020 2020  "),.            
-0000aab0: 636f 7079 3d73 656c 662e 636f 7079 2c0a  copy=self.copy,.
-0000aac0: 2020 2020 2020 2020 2020 2020 7265 7365              rese
-0000aad0: 743d 4661 6c73 652c 0a20 2020 2020 2020  t=False,.       
-0000aae0: 2020 2020 2064 7479 7065 3d5f 6172 7261       dtype=_arra
-0000aaf0: 795f 6170 692e 7375 7070 6f72 7465 645f  y_api.supported_
-0000ab00: 666c 6f61 745f 6474 7970 6573 2878 7029  float_dtypes(xp)
-0000ab10: 2c0a 2020 2020 2020 2020 2020 2020 666f  ,.            fo
-0000ab20: 7263 655f 616c 6c5f 6669 6e69 7465 3d22  rce_all_finite="
-0000ab30: 616c 6c6f 772d 6e61 6e22 2c0a 2020 2020  allow-nan",.    
-0000ab40: 2020 2020 290a 0a20 2020 2020 2020 2069      )..        i
-0000ab50: 6620 7370 6172 7365 2e69 7373 7061 7273  f sparse.isspars
-0000ab60: 6528 5829 3a0a 2020 2020 2020 2020 2020  e(X):.          
-0000ab70: 2020 696e 706c 6163 655f 636f 6c75 6d6e    inplace_column
-0000ab80: 5f73 6361 6c65 2858 2c20 312e 3020 2f20  _scale(X, 1.0 / 
-0000ab90: 7365 6c66 2e73 6361 6c65 5f29 0a20 2020  self.scale_).   
-0000aba0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0000abb0: 2020 2020 2020 2058 202f 3d20 7365 6c66         X /= self
-0000abc0: 2e73 6361 6c65 5f0a 2020 2020 2020 2020  .scale_.        
-0000abd0: 7265 7475 726e 2058 0a0a 2020 2020 6465  return X..    de
-0000abe0: 6620 696e 7665 7273 655f 7472 616e 7366  f inverse_transf
-0000abf0: 6f72 6d28 7365 6c66 2c20 5829 3a0a 2020  orm(self, X):.  
-0000ac00: 2020 2020 2020 2222 2253 6361 6c65 2062        """Scale b
-0000ac10: 6163 6b20 7468 6520 6461 7461 2074 6f20  ack the data to 
-0000ac20: 7468 6520 6f72 6967 696e 616c 2072 6570  the original rep
-0000ac30: 7265 7365 6e74 6174 696f 6e2e 0a0a 2020  resentation...  
-0000ac40: 2020 2020 2020 5061 7261 6d65 7465 7273        Parameters
-0000ac50: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
-0000ac60: 2d2d 2d0a 2020 2020 2020 2020 5820 3a20  ---.        X : 
-0000ac70: 7b61 7272 6179 2d6c 696b 652c 2073 7061  {array-like, spa
-0000ac80: 7273 6520 6d61 7472 6978 7d20 6f66 2073  rse matrix} of s
-0000ac90: 6861 7065 2028 6e5f 7361 6d70 6c65 732c  hape (n_samples,
-0000aca0: 206e 5f66 6561 7475 7265 7329 0a20 2020   n_features).   
-0000acb0: 2020 2020 2020 2020 2054 6865 2064 6174           The dat
-0000acc0: 6120 7468 6174 2073 686f 756c 6420 6265  a that should be
-0000acd0: 2074 7261 6e73 666f 726d 6564 2062 6163   transformed bac
-0000ace0: 6b2e 0a0a 2020 2020 2020 2020 5265 7475  k...        Retu
-0000acf0: 726e 730a 2020 2020 2020 2020 2d2d 2d2d  rns.        ----
-0000ad00: 2d2d 2d0a 2020 2020 2020 2020 585f 7472  ---.        X_tr
-0000ad10: 203a 207b 6e64 6172 7261 792c 2073 7061   : {ndarray, spa
-0000ad20: 7273 6520 6d61 7472 6978 7d20 6f66 2073  rse matrix} of s
-0000ad30: 6861 7065 2028 6e5f 7361 6d70 6c65 732c  hape (n_samples,
-0000ad40: 206e 5f66 6561 7475 7265 7329 0a20 2020   n_features).   
-0000ad50: 2020 2020 2020 2020 2054 7261 6e73 666f           Transfo
-0000ad60: 726d 6564 2061 7272 6179 2e0a 2020 2020  rmed array..    
-0000ad70: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-0000ad80: 6368 6563 6b5f 6973 5f66 6974 7465 6428  check_is_fitted(
-0000ad90: 7365 6c66 290a 0a20 2020 2020 2020 2078  self)..        x
-0000ada0: 702c 205f 203d 2067 6574 5f6e 616d 6573  p, _ = get_names
-0000adb0: 7061 6365 2858 290a 0a20 2020 2020 2020  pace(X)..       
-0000adc0: 2058 203d 2063 6865 636b 5f61 7272 6179   X = check_array
-0000add0: 280a 2020 2020 2020 2020 2020 2020 582c  (.            X,
-0000ade0: 0a20 2020 2020 2020 2020 2020 2061 6363  .            acc
-0000adf0: 6570 745f 7370 6172 7365 3d28 2263 7372  ept_sparse=("csr
-0000ae00: 222c 2022 6373 6322 292c 0a20 2020 2020  ", "csc"),.     
-0000ae10: 2020 2020 2020 2063 6f70 793d 7365 6c66         copy=self
-0000ae20: 2e63 6f70 792c 0a20 2020 2020 2020 2020  .copy,.         
-0000ae30: 2020 2064 7479 7065 3d5f 6172 7261 795f     dtype=_array_
-0000ae40: 6170 692e 7375 7070 6f72 7465 645f 666c  api.supported_fl
-0000ae50: 6f61 745f 6474 7970 6573 2878 7029 2c0a  oat_dtypes(xp),.
-0000ae60: 2020 2020 2020 2020 2020 2020 666f 7263              forc
-0000ae70: 655f 616c 6c5f 6669 6e69 7465 3d22 616c  e_all_finite="al
-0000ae80: 6c6f 772d 6e61 6e22 2c0a 2020 2020 2020  low-nan",.      
-0000ae90: 2020 290a 0a20 2020 2020 2020 2069 6620    )..        if 
-0000aea0: 7370 6172 7365 2e69 7373 7061 7273 6528  sparse.issparse(
-0000aeb0: 5829 3a0a 2020 2020 2020 2020 2020 2020  X):.            
-0000aec0: 696e 706c 6163 655f 636f 6c75 6d6e 5f73  inplace_column_s
-0000aed0: 6361 6c65 2858 2c20 7365 6c66 2e73 6361  cale(X, self.sca
-0000aee0: 6c65 5f29 0a20 2020 2020 2020 2065 6c73  le_).        els
-0000aef0: 653a 0a20 2020 2020 2020 2020 2020 2058  e:.            X
-0000af00: 202a 3d20 7365 6c66 2e73 6361 6c65 5f0a   *= self.scale_.
-0000af10: 2020 2020 2020 2020 7265 7475 726e 2058          return X
-0000af20: 0a0a 2020 2020 6465 6620 5f6d 6f72 655f  ..    def _more_
-0000af30: 7461 6773 2873 656c 6629 3a0a 2020 2020  tags(self):.    
-0000af40: 2020 2020 7265 7475 726e 207b 2261 6c6c      return {"all
-0000af50: 6f77 5f6e 616e 223a 2054 7275 657d 0a0a  ow_nan": True}..
-0000af60: 0a40 7661 6c69 6461 7465 5f70 6172 616d  .@validate_param
-0000af70: 7328 0a20 2020 207b 0a20 2020 2020 2020  s(.    {.       
-0000af80: 2022 5822 3a20 5b22 6172 7261 792d 6c69   "X": ["array-li
-0000af90: 6b65 222c 2022 7370 6172 7365 206d 6174  ke", "sparse mat
-0000afa0: 7269 7822 5d2c 0a20 2020 2020 2020 2022  rix"],.        "
-0000afb0: 6178 6973 223a 205b 4f70 7469 6f6e 7328  axis": [Options(
-0000afc0: 496e 7465 6772 616c 2c20 7b30 2c20 317d  Integral, {0, 1}
-0000afd0: 295d 2c0a 2020 2020 7d2c 0a20 2020 2070  )],.    },.    p
-0000afe0: 7265 6665 725f 736b 6970 5f6e 6573 7465  refer_skip_neste
-0000aff0: 645f 7661 6c69 6461 7469 6f6e 3d46 616c  d_validation=Fal
-0000b000: 7365 2c0a 290a 6465 6620 6d61 7861 6273  se,.).def maxabs
-0000b010: 5f73 6361 6c65 2858 2c20 2a2c 2061 7869  _scale(X, *, axi
-0000b020: 733d 302c 2063 6f70 793d 5472 7565 293a  s=0, copy=True):
-0000b030: 0a20 2020 2022 2222 5363 616c 6520 6561  .    """Scale ea
-0000b040: 6368 2066 6561 7475 7265 2074 6f20 7468  ch feature to th
-0000b050: 6520 5b2d 312c 2031 5d20 7261 6e67 6520  e [-1, 1] range 
-0000b060: 7769 7468 6f75 7420 6272 6561 6b69 6e67  without breaking
-0000b070: 2074 6865 2073 7061 7273 6974 792e 0a0a   the sparsity...
-0000b080: 2020 2020 5468 6973 2065 7374 696d 6174      This estimat
-0000b090: 6f72 2073 6361 6c65 7320 6561 6368 2066  or scales each f
-0000b0a0: 6561 7475 7265 2069 6e64 6976 6964 7561  eature individua
-0000b0b0: 6c6c 7920 7375 6368 0a20 2020 2074 6861  lly such.    tha
-0000b0c0: 7420 7468 6520 6d61 7869 6d61 6c20 6162  t the maximal ab
-0000b0d0: 736f 6c75 7465 2076 616c 7565 206f 6620  solute value of 
-0000b0e0: 6561 6368 2066 6561 7475 7265 2069 6e20  each feature in 
-0000b0f0: 7468 650a 2020 2020 7472 6169 6e69 6e67  the.    training
-0000b100: 2073 6574 2077 696c 6c20 6265 2031 2e30   set will be 1.0
-0000b110: 2e0a 0a20 2020 2054 6869 7320 7363 616c  ...    This scal
-0000b120: 6572 2063 616e 2061 6c73 6f20 6265 2061  er can also be a
-0000b130: 7070 6c69 6564 2074 6f20 7370 6172 7365  pplied to sparse
-0000b140: 2043 5352 206f 7220 4353 4320 6d61 7472   CSR or CSC matr
-0000b150: 6963 6573 2e0a 0a20 2020 2050 6172 616d  ices...    Param
-0000b160: 6574 6572 730a 2020 2020 2d2d 2d2d 2d2d  eters.    ------
-0000b170: 2d2d 2d2d 0a20 2020 2058 203a 207b 6172  ----.    X : {ar
-0000b180: 7261 792d 6c69 6b65 2c20 7370 6172 7365  ray-like, sparse
-0000b190: 206d 6174 7269 787d 206f 6620 7368 6170   matrix} of shap
-0000b1a0: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
-0000b1b0: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
-0000b1c0: 2020 5468 6520 6461 7461 2e0a 0a20 2020    The data...   
-0000b1d0: 2061 7869 7320 3a20 7b30 2c20 317d 2c20   axis : {0, 1}, 
-0000b1e0: 6465 6661 756c 743d 300a 2020 2020 2020  default=0.      
-0000b1f0: 2020 4178 6973 2075 7365 6420 746f 2073    Axis used to s
-0000b200: 6361 6c65 2061 6c6f 6e67 2e20 4966 2030  cale along. If 0
-0000b210: 2c20 696e 6465 7065 6e64 656e 746c 7920  , independently 
-0000b220: 7363 616c 6520 6561 6368 2066 6561 7475  scale each featu
-0000b230: 7265 2c0a 2020 2020 2020 2020 6f74 6865  re,.        othe
-0000b240: 7277 6973 6520 2869 6620 3129 2073 6361  rwise (if 1) sca
-0000b250: 6c65 2065 6163 6820 7361 6d70 6c65 2e0a  le each sample..
-0000b260: 0a20 2020 2063 6f70 7920 3a20 626f 6f6c  .    copy : bool
-0000b270: 2c20 6465 6661 756c 743d 5472 7565 0a20  , default=True. 
-0000b280: 2020 2020 2020 2049 6620 4661 6c73 652c         If False,
-0000b290: 2074 7279 2074 6f20 6176 6f69 6420 6120   try to avoid a 
-0000b2a0: 636f 7079 2061 6e64 2073 6361 6c65 2069  copy and scale i
-0000b2b0: 6e20 706c 6163 652e 0a20 2020 2020 2020  n place..       
-0000b2c0: 2054 6869 7320 6973 206e 6f74 2067 7561   This is not gua
-0000b2d0: 7261 6e74 6565 6420 746f 2061 6c77 6179  ranteed to alway
-0000b2e0: 7320 776f 726b 2069 6e20 706c 6163 653b  s work in place;
-0000b2f0: 2065 2e67 2e20 6966 2074 6865 2064 6174   e.g. if the dat
-0000b300: 6120 6973 0a20 2020 2020 2020 2061 206e  a is.        a n
-0000b310: 756d 7079 2061 7272 6179 2077 6974 6820  umpy array with 
-0000b320: 616e 2069 6e74 2064 7479 7065 2c20 6120  an int dtype, a 
-0000b330: 636f 7079 2077 696c 6c20 6265 2072 6574  copy will be ret
-0000b340: 7572 6e65 6420 6576 656e 2077 6974 680a  urned even with.
-0000b350: 2020 2020 2020 2020 636f 7079 3d46 616c          copy=Fal
-0000b360: 7365 2e0a 0a20 2020 2052 6574 7572 6e73  se...    Returns
-0000b370: 0a20 2020 202d 2d2d 2d2d 2d2d 0a20 2020  .    -------.   
-0000b380: 2058 5f74 7220 3a20 7b6e 6461 7272 6179   X_tr : {ndarray
-0000b390: 2c20 7370 6172 7365 206d 6174 7269 787d  , sparse matrix}
-0000b3a0: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-0000b3b0: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-0000b3c0: 290a 2020 2020 2020 2020 5468 6520 7472  ).        The tr
-0000b3d0: 616e 7366 6f72 6d65 6420 6461 7461 2e0a  ansformed data..
-0000b3e0: 0a20 2020 202e 2e20 7761 726e 696e 673a  .    .. warning:
-0000b3f0: 3a20 5269 736b 206f 6620 6461 7461 206c  : Risk of data l
-0000b400: 6561 6b0a 0a20 2020 2020 2020 2044 6f20  eak..        Do 
-0000b410: 6e6f 7420 7573 6520 3a66 756e 633a 607e  not use :func:`~
-0000b420: 736b 6c65 6172 6e2e 7072 6570 726f 6365  sklearn.preproce
-0000b430: 7373 696e 672e 6d61 7861 6273 5f73 6361  ssing.maxabs_sca
-0000b440: 6c65 6020 756e 6c65 7373 2079 6f75 206b  le` unless you k
-0000b450: 6e6f 770a 2020 2020 2020 2020 7768 6174  now.        what
-0000b460: 2079 6f75 2061 7265 2064 6f69 6e67 2e20   you are doing. 
-0000b470: 4120 636f 6d6d 6f6e 206d 6973 7461 6b65  A common mistake
-0000b480: 2069 7320 746f 2061 7070 6c79 2069 7420   is to apply it 
-0000b490: 746f 2074 6865 2065 6e74 6972 6520 6461  to the entire da
-0000b4a0: 7461 0a20 2020 2020 2020 202a 6265 666f  ta.        *befo
-0000b4b0: 7265 2a20 7370 6c69 7474 696e 6720 696e  re* splitting in
-0000b4c0: 746f 2074 7261 696e 696e 6720 616e 6420  to training and 
-0000b4d0: 7465 7374 2073 6574 732e 2054 6869 7320  test sets. This 
-0000b4e0: 7769 6c6c 2062 6961 7320 7468 650a 2020  will bias the.  
-0000b4f0: 2020 2020 2020 6d6f 6465 6c20 6576 616c        model eval
-0000b500: 7561 7469 6f6e 2062 6563 6175 7365 2069  uation because i
-0000b510: 6e66 6f72 6d61 7469 6f6e 2077 6f75 6c64  nformation would
-0000b520: 2068 6176 6520 6c65 616b 6564 2066 726f   have leaked fro
-0000b530: 6d20 7468 6520 7465 7374 0a20 2020 2020  m the test.     
-0000b540: 2020 2073 6574 2074 6f20 7468 6520 7472     set to the tr
-0000b550: 6169 6e69 6e67 2073 6574 2e0a 2020 2020  aining set..    
-0000b560: 2020 2020 496e 2067 656e 6572 616c 2c20      In general, 
-0000b570: 7765 2072 6563 6f6d 6d65 6e64 2075 7369  we recommend usi
-0000b580: 6e67 0a20 2020 2020 2020 203a 636c 6173  ng.        :clas
-0000b590: 733a 607e 736b 6c65 6172 6e2e 7072 6570  s:`~sklearn.prep
-0000b5a0: 726f 6365 7373 696e 672e 4d61 7841 6273  rocessing.MaxAbs
-0000b5b0: 5363 616c 6572 6020 7769 7468 696e 2061  Scaler` within a
-0000b5c0: 0a20 2020 2020 2020 203a 7265 663a 6050  .        :ref:`P
-0000b5d0: 6970 656c 696e 6520 3c70 6970 656c 696e  ipeline <pipelin
-0000b5e0: 653e 6020 696e 206f 7264 6572 2074 6f20  e>` in order to 
-0000b5f0: 7072 6576 656e 7420 6d6f 7374 2072 6973  prevent most ris
-0000b600: 6b73 206f 6620 6461 7461 0a20 2020 2020  ks of data.     
-0000b610: 2020 206c 6561 6b69 6e67 3a20 6070 6970     leaking: `pip
-0000b620: 6520 3d20 6d61 6b65 5f70 6970 656c 696e  e = make_pipelin
-0000b630: 6528 4d61 7841 6273 5363 616c 6572 2829  e(MaxAbsScaler()
-0000b640: 2c20 4c6f 6769 7374 6963 5265 6772 6573  , LogisticRegres
-0000b650: 7369 6f6e 2829 2960 2e0a 0a20 2020 2053  sion())`...    S
-0000b660: 6565 2041 6c73 6f0a 2020 2020 2d2d 2d2d  ee Also.    ----
-0000b670: 2d2d 2d2d 0a20 2020 204d 6178 4162 7353  ----.    MaxAbsS
-0000b680: 6361 6c65 7220 3a20 5065 7266 6f72 6d73  caler : Performs
-0000b690: 2073 6361 6c69 6e67 2074 6f20 7468 6520   scaling to the 
-0000b6a0: 5b2d 312c 2031 5d20 7261 6e67 6520 7573  [-1, 1] range us
-0000b6b0: 696e 670a 2020 2020 2020 2020 7468 6520  ing.        the 
-0000b6c0: 5472 616e 7366 6f72 6d65 7220 4150 4920  Transformer API 
-0000b6d0: 2865 2e67 2e20 6173 2070 6172 7420 6f66  (e.g. as part of
-0000b6e0: 2061 2070 7265 7072 6f63 6573 7369 6e67   a preprocessing
-0000b6f0: 0a20 2020 2020 2020 203a 636c 6173 733a  .        :class:
-0000b700: 607e 736b 6c65 6172 6e2e 7069 7065 6c69  `~sklearn.pipeli
-0000b710: 6e65 2e50 6970 656c 696e 6560 292e 0a0a  ne.Pipeline`)...
-0000b720: 2020 2020 4e6f 7465 730a 2020 2020 2d2d      Notes.    --
-0000b730: 2d2d 2d0a 2020 2020 4e61 4e73 2061 7265  ---.    NaNs are
-0000b740: 2074 7265 6174 6564 2061 7320 6d69 7373   treated as miss
-0000b750: 696e 6720 7661 6c75 6573 3a20 6469 7372  ing values: disr
-0000b760: 6567 6172 6465 6420 746f 2063 6f6d 7075  egarded to compu
-0000b770: 7465 2074 6865 2073 7461 7469 7374 6963  te the statistic
-0000b780: 732c 0a20 2020 2061 6e64 206d 6169 6e74  s,.    and maint
-0000b790: 6169 6e65 6420 6475 7269 6e67 2074 6865  ained during the
-0000b7a0: 2064 6174 6120 7472 616e 7366 6f72 6d61   data transforma
-0000b7b0: 7469 6f6e 2e0a 0a20 2020 2046 6f72 2061  tion...    For a
-0000b7c0: 2063 6f6d 7061 7269 736f 6e20 6f66 2074   comparison of t
-0000b7d0: 6865 2064 6966 6665 7265 6e74 2073 6361  he different sca
-0000b7e0: 6c65 7273 2c20 7472 616e 7366 6f72 6d65  lers, transforme
-0000b7f0: 7273 2c20 616e 6420 6e6f 726d 616c 697a  rs, and normaliz
-0000b800: 6572 732c 0a20 2020 2073 6565 3a20 3a72  ers,.    see: :r
-0000b810: 6566 3a60 7370 6878 5f67 6c72 5f61 7574  ef:`sphx_glr_aut
-0000b820: 6f5f 6578 616d 706c 6573 5f70 7265 7072  o_examples_prepr
-0000b830: 6f63 6573 7369 6e67 5f70 6c6f 745f 616c  ocessing_plot_al
-0000b840: 6c5f 7363 616c 696e 672e 7079 602e 0a0a  l_scaling.py`...
-0000b850: 2020 2020 4578 616d 706c 6573 0a20 2020      Examples.   
-0000b860: 202d 2d2d 2d2d 2d2d 2d0a 2020 2020 3e3e   --------.    >>
-0000b870: 3e20 6672 6f6d 2073 6b6c 6561 726e 2e70  > from sklearn.p
-0000b880: 7265 7072 6f63 6573 7369 6e67 2069 6d70  reprocessing imp
-0000b890: 6f72 7420 6d61 7861 6273 5f73 6361 6c65  ort maxabs_scale
-0000b8a0: 0a20 2020 203e 3e3e 2058 203d 205b 5b2d  .    >>> X = [[-
-0000b8b0: 322c 2031 2c20 325d 2c20 5b2d 312c 2030  2, 1, 2], [-1, 0
-0000b8c0: 2c20 315d 5d0a 2020 2020 3e3e 3e20 6d61  , 1]].    >>> ma
-0000b8d0: 7861 6273 5f73 6361 6c65 2858 2c20 6178  xabs_scale(X, ax
-0000b8e0: 6973 3d30 2920 2023 2073 6361 6c65 2065  is=0)  # scale e
-0000b8f0: 6163 6820 636f 6c75 6d6e 2069 6e64 6570  ach column indep
-0000b900: 656e 6465 6e74 6c79 0a20 2020 2061 7272  endently.    arr
-0000b910: 6179 285b 5b2d 312e 202c 2020 312e 202c  ay([[-1. ,  1. ,
-0000b920: 2020 312e 205d 2c0a 2020 2020 2020 2020    1. ],.        
-0000b930: 2020 205b 2d30 2e35 2c20 2030 2e20 2c20     [-0.5,  0. , 
-0000b940: 2030 2e35 5d5d 290a 2020 2020 3e3e 3e20   0.5]]).    >>> 
-0000b950: 6d61 7861 6273 5f73 6361 6c65 2858 2c20  maxabs_scale(X, 
-0000b960: 6178 6973 3d31 2920 2023 2073 6361 6c65  axis=1)  # scale
-0000b970: 2065 6163 6820 726f 7720 696e 6465 7065   each row indepe
-0000b980: 6e64 656e 746c 790a 2020 2020 6172 7261  ndently.    arra
-0000b990: 7928 5b5b 2d31 2e20 2c20 2030 2e35 2c20  y([[-1. ,  0.5, 
-0000b9a0: 2031 2e20 5d2c 0a20 2020 2020 2020 2020   1. ],.         
-0000b9b0: 2020 5b2d 312e 202c 2020 302e 202c 2020    [-1. ,  0. ,  
-0000b9c0: 312e 205d 5d29 0a20 2020 2022 2222 0a20  1. ]]).    """. 
-0000b9d0: 2020 2023 2055 6e6c 696b 6520 7468 6520     # Unlike the 
-0000b9e0: 7363 616c 6572 206f 626a 6563 742c 2074  scaler object, t
-0000b9f0: 6869 7320 6675 6e63 7469 6f6e 2061 6c6c  his function all
-0000ba00: 6f77 7320 3164 2069 6e70 7574 2e0a 0a20  ows 1d input... 
-0000ba10: 2020 2023 2049 6620 636f 7079 2069 7320     # If copy is 
-0000ba20: 7265 7175 6972 6564 2c20 6974 2077 696c  required, it wil
-0000ba30: 6c20 6265 2064 6f6e 6520 696e 7369 6465  l be done inside
-0000ba40: 2074 6865 2073 6361 6c65 7220 6f62 6a65   the scaler obje
-0000ba50: 6374 2e0a 2020 2020 5820 3d20 6368 6563  ct..    X = chec
-0000ba60: 6b5f 6172 7261 7928 0a20 2020 2020 2020  k_array(.       
-0000ba70: 2058 2c0a 2020 2020 2020 2020 6163 6365   X,.        acce
-0000ba80: 7074 5f73 7061 7273 653d 2822 6373 7222  pt_sparse=("csr"
-0000ba90: 2c20 2263 7363 2229 2c0a 2020 2020 2020  , "csc"),.      
-0000baa0: 2020 636f 7079 3d46 616c 7365 2c0a 2020    copy=False,.  
-0000bab0: 2020 2020 2020 656e 7375 7265 5f32 643d        ensure_2d=
-0000bac0: 4661 6c73 652c 0a20 2020 2020 2020 2064  False,.        d
-0000bad0: 7479 7065 3d46 4c4f 4154 5f44 5459 5045  type=FLOAT_DTYPE
-0000bae0: 532c 0a20 2020 2020 2020 2066 6f72 6365  S,.        force
-0000baf0: 5f61 6c6c 5f66 696e 6974 653d 2261 6c6c  _all_finite="all
-0000bb00: 6f77 2d6e 616e 222c 0a20 2020 2029 0a20  ow-nan",.    ). 
-0000bb10: 2020 206f 7269 6769 6e61 6c5f 6e64 696d     original_ndim
-0000bb20: 203d 2058 2e6e 6469 6d0a 0a20 2020 2069   = X.ndim..    i
-0000bb30: 6620 6f72 6967 696e 616c 5f6e 6469 6d20  f original_ndim 
-0000bb40: 3d3d 2031 3a0a 2020 2020 2020 2020 5820  == 1:.        X 
-0000bb50: 3d20 582e 7265 7368 6170 6528 582e 7368  = X.reshape(X.sh
-0000bb60: 6170 655b 305d 2c20 3129 0a0a 2020 2020  ape[0], 1)..    
-0000bb70: 7320 3d20 4d61 7841 6273 5363 616c 6572  s = MaxAbsScaler
-0000bb80: 2863 6f70 793d 636f 7079 290a 2020 2020  (copy=copy).    
-0000bb90: 6966 2061 7869 7320 3d3d 2030 3a0a 2020  if axis == 0:.  
-0000bba0: 2020 2020 2020 5820 3d20 732e 6669 745f        X = s.fit_
-0000bbb0: 7472 616e 7366 6f72 6d28 5829 0a20 2020  transform(X).   
-0000bbc0: 2065 6c73 653a 0a20 2020 2020 2020 2058   else:.        X
-0000bbd0: 203d 2073 2e66 6974 5f74 7261 6e73 666f   = s.fit_transfo
-0000bbe0: 726d 2858 2e54 292e 540a 0a20 2020 2069  rm(X.T).T..    i
-0000bbf0: 6620 6f72 6967 696e 616c 5f6e 6469 6d20  f original_ndim 
-0000bc00: 3d3d 2031 3a0a 2020 2020 2020 2020 5820  == 1:.        X 
-0000bc10: 3d20 582e 7261 7665 6c28 290a 0a20 2020  = X.ravel()..   
-0000bc20: 2072 6574 7572 6e20 580a 0a0a 636c 6173   return X...clas
-0000bc30: 7320 526f 6275 7374 5363 616c 6572 284f  s RobustScaler(O
-0000bc40: 6e65 546f 4f6e 6546 6561 7475 7265 4d69  neToOneFeatureMi
-0000bc50: 7869 6e2c 2054 7261 6e73 666f 726d 6572  xin, Transformer
-0000bc60: 4d69 7869 6e2c 2042 6173 6545 7374 696d  Mixin, BaseEstim
-0000bc70: 6174 6f72 293a 0a20 2020 2022 2222 5363  ator):.    """Sc
-0000bc80: 616c 6520 6665 6174 7572 6573 2075 7369  ale features usi
-0000bc90: 6e67 2073 7461 7469 7374 6963 7320 7468  ng statistics th
-0000bca0: 6174 2061 7265 2072 6f62 7573 7420 746f  at are robust to
-0000bcb0: 206f 7574 6c69 6572 732e 0a0a 2020 2020   outliers...    
-0000bcc0: 5468 6973 2053 6361 6c65 7220 7265 6d6f  This Scaler remo
-0000bcd0: 7665 7320 7468 6520 6d65 6469 616e 2061  ves the median a
-0000bce0: 6e64 2073 6361 6c65 7320 7468 6520 6461  nd scales the da
-0000bcf0: 7461 2061 6363 6f72 6469 6e67 2074 6f0a  ta according to.
-0000bd00: 2020 2020 7468 6520 7175 616e 7469 6c65      the quantile
-0000bd10: 2072 616e 6765 2028 6465 6661 756c 7473   range (defaults
-0000bd20: 2074 6f20 4951 523a 2049 6e74 6572 7175   to IQR: Interqu
-0000bd30: 6172 7469 6c65 2052 616e 6765 292e 0a20  artile Range).. 
-0000bd40: 2020 2054 6865 2049 5152 2069 7320 7468     The IQR is th
-0000bd50: 6520 7261 6e67 6520 6265 7477 6565 6e20  e range between 
-0000bd60: 7468 6520 3173 7420 7175 6172 7469 6c65  the 1st quartile
-0000bd70: 2028 3235 7468 2071 7561 6e74 696c 6529   (25th quantile)
-0000bd80: 0a20 2020 2061 6e64 2074 6865 2033 7264  .    and the 3rd
-0000bd90: 2071 7561 7274 696c 6520 2837 3574 6820   quartile (75th 
-0000bda0: 7175 616e 7469 6c65 292e 0a0a 2020 2020  quantile)...    
-0000bdb0: 4365 6e74 6572 696e 6720 616e 6420 7363  Centering and sc
-0000bdc0: 616c 696e 6720 6861 7070 656e 2069 6e64  aling happen ind
-0000bdd0: 6570 656e 6465 6e74 6c79 206f 6e20 6561  ependently on ea
-0000bde0: 6368 2066 6561 7475 7265 2062 790a 2020  ch feature by.  
-0000bdf0: 2020 636f 6d70 7574 696e 6720 7468 6520    computing the 
-0000be00: 7265 6c65 7661 6e74 2073 7461 7469 7374  relevant statist
-0000be10: 6963 7320 6f6e 2074 6865 2073 616d 706c  ics on the sampl
-0000be20: 6573 2069 6e20 7468 6520 7472 6169 6e69  es in the traini
-0000be30: 6e67 0a20 2020 2073 6574 2e20 4d65 6469  ng.    set. Medi
-0000be40: 616e 2061 6e64 2069 6e74 6572 7175 6172  an and interquar
-0000be50: 7469 6c65 2072 616e 6765 2061 7265 2074  tile range are t
-0000be60: 6865 6e20 7374 6f72 6564 2074 6f20 6265  hen stored to be
-0000be70: 2075 7365 6420 6f6e 0a20 2020 206c 6174   used on.    lat
-0000be80: 6572 2064 6174 6120 7573 696e 6720 7468  er data using th
-0000be90: 6520 3a6d 6574 683a 6074 7261 6e73 666f  e :meth:`transfo
-0000bea0: 726d 6020 6d65 7468 6f64 2e0a 0a20 2020  rm` method...   
-0000beb0: 2053 7461 6e64 6172 6469 7a61 7469 6f6e   Standardization
-0000bec0: 206f 6620 6120 6461 7461 7365 7420 6973   of a dataset is
-0000bed0: 2061 2063 6f6d 6d6f 6e20 7072 6570 726f   a common prepro
-0000bee0: 6365 7373 696e 6720 666f 7220 6d61 6e79  cessing for many
-0000bef0: 206d 6163 6869 6e65 0a20 2020 206c 6561   machine.    lea
-0000bf00: 726e 696e 6720 6573 7469 6d61 746f 7273  rning estimators
-0000bf10: 2e20 5479 7069 6361 6c6c 7920 7468 6973  . Typically this
-0000bf20: 2069 7320 646f 6e65 2062 7920 7265 6d6f   is done by remo
-0000bf30: 7669 6e67 2074 6865 206d 6561 6e20 616e  ving the mean an
-0000bf40: 640a 2020 2020 7363 616c 696e 6720 746f  d.    scaling to
-0000bf50: 2075 6e69 7420 7661 7269 616e 6365 2e20   unit variance. 
-0000bf60: 486f 7765 7665 722c 206f 7574 6c69 6572  However, outlier
-0000bf70: 7320 6361 6e20 6f66 7465 6e20 696e 666c  s can often infl
-0000bf80: 7565 6e63 6520 7468 6520 7361 6d70 6c65  uence the sample
-0000bf90: 0a20 2020 206d 6561 6e20 2f20 7661 7269  .    mean / vari
-0000bfa0: 616e 6365 2069 6e20 6120 6e65 6761 7469  ance in a negati
-0000bfb0: 7665 2077 6179 2e20 496e 2073 7563 6820  ve way. In such 
-0000bfc0: 6361 7365 732c 2075 7369 6e67 2074 6865  cases, using the
-0000bfd0: 206d 6564 6961 6e20 616e 6420 7468 650a   median and the.
-0000bfe0: 2020 2020 696e 7465 7271 7561 7274 696c      interquartil
-0000bff0: 6520 7261 6e67 6520 6f66 7465 6e20 6769  e range often gi
-0000c000: 7665 2062 6574 7465 7220 7265 7375 6c74  ve better result
-0000c010: 732e 2046 6f72 2061 6e20 6578 616d 706c  s. For an exampl
-0000c020: 6520 7669 7375 616c 697a 6174 696f 6e0a  e visualization.
-0000c030: 2020 2020 616e 6420 636f 6d70 6172 6973      and comparis
-0000c040: 6f6e 2074 6f20 6f74 6865 7220 7363 616c  on to other scal
-0000c050: 6572 732c 2072 6566 6572 2074 6f20 3a72  ers, refer to :r
-0000c060: 6566 3a60 436f 6d70 6172 6520 526f 6275  ef:`Compare Robu
-0000c070: 7374 5363 616c 6572 2077 6974 680a 2020  stScaler with.  
-0000c080: 2020 6f74 6865 7220 7363 616c 6572 7320    other scalers 
-0000c090: 3c70 6c6f 745f 616c 6c5f 7363 616c 696e  <plot_all_scalin
-0000c0a0: 675f 726f 6275 7374 5f73 6361 6c65 725f  g_robust_scaler_
-0000c0b0: 7365 6374 696f 6e3e 602e 0a0a 2020 2020  section>`...    
-0000c0c0: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
-0000c0d0: 3a20 302e 3137 0a0a 2020 2020 5265 6164  : 0.17..    Read
-0000c0e0: 206d 6f72 6520 696e 2074 6865 203a 7265   more in the :re
-0000c0f0: 663a 6055 7365 7220 4775 6964 6520 3c70  f:`User Guide <p
-0000c100: 7265 7072 6f63 6573 7369 6e67 5f73 6361  reprocessing_sca
-0000c110: 6c65 723e 602e 0a0a 2020 2020 5061 7261  ler>`...    Para
-0000c120: 6d65 7465 7273 0a20 2020 202d 2d2d 2d2d  meters.    -----
-0000c130: 2d2d 2d2d 2d0a 2020 2020 7769 7468 5f63  -----.    with_c
-0000c140: 656e 7465 7269 6e67 203a 2062 6f6f 6c2c  entering : bool,
-0000c150: 2064 6566 6175 6c74 3d54 7275 650a 2020   default=True.  
-0000c160: 2020 2020 2020 4966 2060 5472 7565 602c        If `True`,
-0000c170: 2063 656e 7465 7220 7468 6520 6461 7461   center the data
-0000c180: 2062 6566 6f72 6520 7363 616c 696e 672e   before scaling.
-0000c190: 0a20 2020 2020 2020 2054 6869 7320 7769  .        This wi
-0000c1a0: 6c6c 2063 6175 7365 203a 6d65 7468 3a60  ll cause :meth:`
-0000c1b0: 7472 616e 7366 6f72 6d60 2074 6f20 7261  transform` to ra
-0000c1c0: 6973 6520 616e 2065 7863 6570 7469 6f6e  ise an exception
-0000c1d0: 2077 6865 6e20 6174 7465 6d70 7465 640a   when attempted.
-0000c1e0: 2020 2020 2020 2020 6f6e 2073 7061 7273          on spars
-0000c1f0: 6520 6d61 7472 6963 6573 2c20 6265 6361  e matrices, beca
-0000c200: 7573 6520 6365 6e74 6572 696e 6720 7468  use centering th
-0000c210: 656d 2065 6e74 6169 6c73 2062 7569 6c64  em entails build
-0000c220: 696e 6720 6120 6465 6e73 650a 2020 2020  ing a dense.    
-0000c230: 2020 2020 6d61 7472 6978 2077 6869 6368      matrix which
-0000c240: 2069 6e20 636f 6d6d 6f6e 2075 7365 2063   in common use c
-0000c250: 6173 6573 2069 7320 6c69 6b65 6c79 2074  ases is likely t
-0000c260: 6f20 6265 2074 6f6f 206c 6172 6765 2074  o be too large t
-0000c270: 6f20 6669 7420 696e 0a20 2020 2020 2020  o fit in.       
-0000c280: 206d 656d 6f72 792e 0a0a 2020 2020 7769   memory...    wi
-0000c290: 7468 5f73 6361 6c69 6e67 203a 2062 6f6f  th_scaling : boo
-0000c2a0: 6c2c 2064 6566 6175 6c74 3d54 7275 650a  l, default=True.
-0000c2b0: 2020 2020 2020 2020 4966 2060 5472 7565          If `True
-0000c2c0: 602c 2073 6361 6c65 2074 6865 2064 6174  `, scale the dat
-0000c2d0: 6120 746f 2069 6e74 6572 7175 6172 7469  a to interquarti
-0000c2e0: 6c65 2072 616e 6765 2e0a 0a20 2020 2071  le range...    q
-0000c2f0: 7561 6e74 696c 655f 7261 6e67 6520 3a20  uantile_range : 
-0000c300: 7475 706c 6520 2871 5f6d 696e 2c20 715f  tuple (q_min, q_
-0000c310: 6d61 7829 2c20 302e 3020 3c20 715f 6d69  max), 0.0 < q_mi
-0000c320: 6e20 3c20 715f 6d61 7820 3c20 3130 302e  n < q_max < 100.
-0000c330: 302c 205c 0a20 2020 2020 2020 2064 6566  0, \.        def
-0000c340: 6175 6c74 3d28 3235 2e30 2c20 3735 2e30  ault=(25.0, 75.0
-0000c350: 290a 2020 2020 2020 2020 5175 616e 7469  ).        Quanti
-0000c360: 6c65 2072 616e 6765 2075 7365 6420 746f  le range used to
-0000c370: 2063 616c 6375 6c61 7465 2060 7363 616c   calculate `scal
-0000c380: 655f 602e 2042 7920 6465 6661 756c 7420  e_`. By default 
-0000c390: 7468 6973 2069 7320 6571 7561 6c20 746f  this is equal to
-0000c3a0: 0a20 2020 2020 2020 2074 6865 2049 5152  .        the IQR
-0000c3b0: 2c20 692e 652e 2c20 6071 5f6d 696e 6020  , i.e., `q_min` 
-0000c3c0: 6973 2074 6865 2066 6972 7374 2071 7561  is the first qua
-0000c3d0: 6e74 696c 6520 616e 6420 6071 5f6d 6178  ntile and `q_max
-0000c3e0: 6020 6973 2074 6865 2074 6869 7264 0a20  ` is the third. 
-0000c3f0: 2020 2020 2020 2071 7561 6e74 696c 652e         quantile.
-0000c400: 0a0a 2020 2020 2020 2020 2e2e 2076 6572  ..        .. ver
-0000c410: 7369 6f6e 6164 6465 643a 3a20 302e 3138  sionadded:: 0.18
-0000c420: 0a0a 2020 2020 636f 7079 203a 2062 6f6f  ..    copy : boo
-0000c430: 6c2c 2064 6566 6175 6c74 3d54 7275 650a  l, default=True.
-0000c440: 2020 2020 2020 2020 4966 2060 4661 6c73          If `Fals
-0000c450: 6560 2c20 7472 7920 746f 2061 766f 6964  e`, try to avoid
-0000c460: 2061 2063 6f70 7920 616e 6420 646f 2069   a copy and do i
-0000c470: 6e70 6c61 6365 2073 6361 6c69 6e67 2069  nplace scaling i
-0000c480: 6e73 7465 6164 2e0a 2020 2020 2020 2020  nstead..        
-0000c490: 5468 6973 2069 7320 6e6f 7420 6775 6172  This is not guar
-0000c4a0: 616e 7465 6564 2074 6f20 616c 7761 7973  anteed to always
-0000c4b0: 2077 6f72 6b20 696e 706c 6163 653b 2065   work inplace; e
-0000c4c0: 2e67 2e20 6966 2074 6865 2064 6174 6120  .g. if the data 
-0000c4d0: 6973 0a20 2020 2020 2020 206e 6f74 2061  is.        not a
-0000c4e0: 204e 756d 5079 2061 7272 6179 206f 7220   NumPy array or 
-0000c4f0: 7363 6970 792e 7370 6172 7365 2043 5352  scipy.sparse CSR
-0000c500: 206d 6174 7269 782c 2061 2063 6f70 7920   matrix, a copy 
-0000c510: 6d61 7920 7374 696c 6c20 6265 0a20 2020  may still be.   
-0000c520: 2020 2020 2072 6574 7572 6e65 642e 0a0a       returned...
-0000c530: 2020 2020 756e 6974 5f76 6172 6961 6e63      unit_varianc
-0000c540: 6520 3a20 626f 6f6c 2c20 6465 6661 756c  e : bool, defaul
-0000c550: 743d 4661 6c73 650a 2020 2020 2020 2020  t=False.        
-0000c560: 4966 2060 5472 7565 602c 2073 6361 6c65  If `True`, scale
-0000c570: 2064 6174 6120 736f 2074 6861 7420 6e6f   data so that no
-0000c580: 726d 616c 6c79 2064 6973 7472 6962 7574  rmally distribut
-0000c590: 6564 2066 6561 7475 7265 7320 6861 7665  ed features have
-0000c5a0: 2061 0a20 2020 2020 2020 2076 6172 6961   a.        varia
-0000c5b0: 6e63 6520 6f66 2031 2e20 496e 2067 656e  nce of 1. In gen
-0000c5c0: 6572 616c 2c20 6966 2074 6865 2064 6966  eral, if the dif
-0000c5d0: 6665 7265 6e63 6520 6265 7477 6565 6e20  ference between 
-0000c5e0: 7468 6520 782d 7661 6c75 6573 206f 660a  the x-values of.
-0000c5f0: 2020 2020 2020 2020 6071 5f6d 6178 6020          `q_max` 
-0000c600: 616e 6420 6071 5f6d 696e 6020 666f 7220  and `q_min` for 
-0000c610: 6120 7374 616e 6461 7264 206e 6f72 6d61  a standard norma
-0000c620: 6c20 6469 7374 7269 6275 7469 6f6e 2069  l distribution i
-0000c630: 7320 6772 6561 7465 720a 2020 2020 2020  s greater.      
-0000c640: 2020 7468 616e 2031 2c20 7468 6520 6461    than 1, the da
-0000c650: 7461 7365 7420 7769 6c6c 2062 6520 7363  taset will be sc
-0000c660: 616c 6564 2064 6f77 6e2e 2049 6620 6c65  aled down. If le
-0000c670: 7373 2074 6861 6e20 312c 2074 6865 2064  ss than 1, the d
-0000c680: 6174 6173 6574 0a20 2020 2020 2020 2077  ataset.        w
-0000c690: 696c 6c20 6265 2073 6361 6c65 6420 7570  ill be scaled up
-0000c6a0: 2e0a 0a20 2020 2020 2020 202e 2e20 7665  ...        .. ve
-0000c6b0: 7273 696f 6e61 6464 6564 3a3a 2030 2e32  rsionadded:: 0.2
-0000c6c0: 340a 0a20 2020 2041 7474 7269 6275 7465  4..    Attribute
-0000c6d0: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
-0000c6e0: 0a20 2020 2063 656e 7465 725f 203a 2061  .    center_ : a
-0000c6f0: 7272 6179 206f 6620 666c 6f61 7473 0a20  rray of floats. 
-0000c700: 2020 2020 2020 2054 6865 206d 6564 6961         The media
-0000c710: 6e20 7661 6c75 6520 666f 7220 6561 6368  n value for each
-0000c720: 2066 6561 7475 7265 2069 6e20 7468 6520   feature in the 
-0000c730: 7472 6169 6e69 6e67 2073 6574 2e0a 0a20  training set... 
-0000c740: 2020 2073 6361 6c65 5f20 3a20 6172 7261     scale_ : arra
-0000c750: 7920 6f66 2066 6c6f 6174 730a 2020 2020  y of floats.    
-0000c760: 2020 2020 5468 6520 2873 6361 6c65 6429      The (scaled)
-0000c770: 2069 6e74 6572 7175 6172 7469 6c65 2072   interquartile r
-0000c780: 616e 6765 2066 6f72 2065 6163 6820 6665  ange for each fe
-0000c790: 6174 7572 6520 696e 2074 6865 2074 7261  ature in the tra
-0000c7a0: 696e 696e 6720 7365 742e 0a0a 2020 2020  ining set...    
-0000c7b0: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
-0000c7c0: 6465 643a 3a20 302e 3137 0a20 2020 2020  ded:: 0.17.     
-0000c7d0: 2020 2020 2020 2a73 6361 6c65 5f2a 2061        *scale_* a
-0000c7e0: 7474 7269 6275 7465 2e0a 0a20 2020 206e  ttribute...    n
-0000c7f0: 5f66 6561 7475 7265 735f 696e 5f20 3a20  _features_in_ : 
-0000c800: 696e 740a 2020 2020 2020 2020 4e75 6d62  int.        Numb
-0000c810: 6572 206f 6620 6665 6174 7572 6573 2073  er of features s
-0000c820: 6565 6e20 6475 7269 6e67 203a 7465 726d  een during :term
-0000c830: 3a60 6669 7460 2e0a 0a20 2020 2020 2020  :`fit`...       
-0000c840: 202e 2e20 7665 7273 696f 6e61 6464 6564   .. versionadded
-0000c850: 3a3a 2030 2e32 340a 0a20 2020 2066 6561  :: 0.24..    fea
-0000c860: 7475 7265 5f6e 616d 6573 5f69 6e5f 203a  ture_names_in_ :
-0000c870: 206e 6461 7272 6179 206f 6620 7368 6170   ndarray of shap
-0000c880: 6520 2860 6e5f 6665 6174 7572 6573 5f69  e (`n_features_i
-0000c890: 6e5f 602c 290a 2020 2020 2020 2020 4e61  n_`,).        Na
-0000c8a0: 6d65 7320 6f66 2066 6561 7475 7265 7320  mes of features 
-0000c8b0: 7365 656e 2064 7572 696e 6720 3a74 6572  seen during :ter
-0000c8c0: 6d3a 6066 6974 602e 2044 6566 696e 6564  m:`fit`. Defined
-0000c8d0: 206f 6e6c 7920 7768 656e 2060 5860 0a20   only when `X`. 
-0000c8e0: 2020 2020 2020 2068 6173 2066 6561 7475         has featu
-0000c8f0: 7265 206e 616d 6573 2074 6861 7420 6172  re names that ar
-0000c900: 6520 616c 6c20 7374 7269 6e67 732e 0a0a  e all strings...
-0000c910: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
-0000c920: 6f6e 6164 6465 643a 3a20 312e 300a 0a20  onadded:: 1.0.. 
-0000c930: 2020 2053 6565 2041 6c73 6f0a 2020 2020     See Also.    
-0000c940: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2072 6f62  --------.    rob
-0000c950: 7573 745f 7363 616c 6520 3a20 4571 7569  ust_scale : Equi
-0000c960: 7661 6c65 6e74 2066 756e 6374 696f 6e20  valent function 
-0000c970: 7769 7468 6f75 7420 7468 6520 6573 7469  without the esti
-0000c980: 6d61 746f 7220 4150 492e 0a20 2020 2073  mator API..    s
-0000c990: 6b6c 6561 726e 2e64 6563 6f6d 706f 7369  klearn.decomposi
-0000c9a0: 7469 6f6e 2e50 4341 203a 2046 7572 7468  tion.PCA : Furth
-0000c9b0: 6572 2072 656d 6f76 6573 2074 6865 206c  er removes the l
-0000c9c0: 696e 6561 7220 636f 7272 656c 6174 696f  inear correlatio
-0000c9d0: 6e20 6163 726f 7373 0a20 2020 2020 2020  n across.       
-0000c9e0: 2066 6561 7475 7265 7320 7769 7468 2027   features with '
-0000c9f0: 7768 6974 656e 3d54 7275 6527 2e0a 0a20  whiten=True'... 
-0000ca00: 2020 204e 6f74 6573 0a20 2020 202d 2d2d     Notes.    ---
-0000ca10: 2d2d 0a0a 2020 2020 6874 7470 733a 2f2f  --..    https://
-0000ca20: 656e 2e77 696b 6970 6564 6961 2e6f 7267  en.wikipedia.org
-0000ca30: 2f77 696b 692f 4d65 6469 616e 0a20 2020  /wiki/Median.   
-0000ca40: 2068 7474 7073 3a2f 2f65 6e2e 7769 6b69   https://en.wiki
-0000ca50: 7065 6469 612e 6f72 672f 7769 6b69 2f49  pedia.org/wiki/I
-0000ca60: 6e74 6572 7175 6172 7469 6c65 5f72 616e  nterquartile_ran
-0000ca70: 6765 0a0a 2020 2020 4578 616d 706c 6573  ge..    Examples
-0000ca80: 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a 2020  .    --------.  
-0000ca90: 2020 3e3e 3e20 6672 6f6d 2073 6b6c 6561    >>> from sklea
-0000caa0: 726e 2e70 7265 7072 6f63 6573 7369 6e67  rn.preprocessing
-0000cab0: 2069 6d70 6f72 7420 526f 6275 7374 5363   import RobustSc
-0000cac0: 616c 6572 0a20 2020 203e 3e3e 2058 203d  aler.    >>> X =
-0000cad0: 205b 5b20 312e 2c20 2d32 2e2c 2020 322e   [[ 1., -2.,  2.
-0000cae0: 5d2c 0a20 2020 202e 2e2e 2020 2020 2020  ],.    ...      
-0000caf0: 5b20 2d32 2e2c 2020 312e 2c20 2033 2e5d  [ -2.,  1.,  3.]
+000080f0: 6178 6973 3d30 2c0a 2020 2020 2020 2020  axis=0,.        
+00008100: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008110: 6c61 7374 5f6d 6561 6e3d 7365 6c66 2e6d  last_mean=self.m
+00008120: 6561 6e5f 2c0a 2020 2020 2020 2020 2020  ean_,.          
+00008130: 2020 2020 2020 2020 2020 2020 2020 6c61                la
+00008140: 7374 5f76 6172 3d73 656c 662e 7661 725f  st_var=self.var_
+00008150: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00008160: 2020 2020 2020 2020 2020 6c61 7374 5f6e            last_n
+00008170: 3d73 656c 662e 6e5f 7361 6d70 6c65 735f  =self.n_samples_
+00008180: 7365 656e 5f2c 0a20 2020 2020 2020 2020  seen_,.         
+00008190: 2020 2020 2020 2020 2020 2020 2020 2077                 w
+000081a0: 6569 6768 7473 3d73 616d 706c 655f 7765  eights=sample_we
+000081b0: 6967 6874 2c0a 2020 2020 2020 2020 2020  ight,.          
+000081c0: 2020 2020 2020 2020 2020 290a 2020 2020            ).    
+000081d0: 2020 2020 2020 2020 2020 2020 2320 5765              # We
+000081e0: 2066 6f72 6365 2074 6865 206d 6561 6e20   force the mean 
+000081f0: 616e 6420 7661 7269 616e 6365 2074 6f20  and variance to 
+00008200: 666c 6f61 7436 3420 666f 7220 6c61 7267  float64 for larg
+00008210: 6520 6172 7261 7973 0a20 2020 2020 2020  e arrays.       
+00008220: 2020 2020 2020 2020 2023 2053 6565 2068           # See h
+00008230: 7474 7073 3a2f 2f67 6974 6875 622e 636f  ttps://github.co
+00008240: 6d2f 7363 696b 6974 2d6c 6561 726e 2f73  m/scikit-learn/s
+00008250: 6369 6b69 742d 6c65 6172 6e2f 7075 6c6c  cikit-learn/pull
+00008260: 2f31 3233 3338 0a20 2020 2020 2020 2020  /12338.         
+00008270: 2020 2020 2020 2073 656c 662e 6d65 616e         self.mean
+00008280: 5f20 3d20 7365 6c66 2e6d 6561 6e5f 2e61  _ = self.mean_.a
+00008290: 7374 7970 6528 6e70 2e66 6c6f 6174 3634  stype(np.float64
+000082a0: 2c20 636f 7079 3d46 616c 7365 290a 2020  , copy=False).  
+000082b0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+000082c0: 6c66 2e76 6172 5f20 3d20 7365 6c66 2e76  lf.var_ = self.v
+000082d0: 6172 5f2e 6173 7479 7065 286e 702e 666c  ar_.astype(np.fl
+000082e0: 6f61 7436 342c 2063 6f70 793d 4661 6c73  oat64, copy=Fals
+000082f0: 6529 0a20 2020 2020 2020 2020 2020 2065  e).            e
+00008300: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
+00008310: 2020 2020 2073 656c 662e 6d65 616e 5f20       self.mean_ 
+00008320: 3d20 4e6f 6e65 2020 2320 6173 2077 6974  = None  # as wit
+00008330: 685f 6d65 616e 206d 7573 7420 6265 2046  h_mean must be F
+00008340: 616c 7365 2066 6f72 2073 7061 7273 650a  alse for sparse.
+00008350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008360: 7365 6c66 2e76 6172 5f20 3d20 4e6f 6e65  self.var_ = None
+00008370: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00008380: 2077 6569 6768 7473 203d 205f 6368 6563   weights = _chec
+00008390: 6b5f 7361 6d70 6c65 5f77 6569 6768 7428  k_sample_weight(
+000083a0: 7361 6d70 6c65 5f77 6569 6768 742c 2058  sample_weight, X
+000083b0: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
+000083c0: 2020 7375 6d5f 7765 6967 6874 735f 6e61    sum_weights_na
+000083d0: 6e20 3d20 7765 6967 6874 7320 4020 7370  n = weights @ sp
+000083e0: 6172 7365 5f63 6f6e 7374 7275 6374 6f72  arse_constructor
+000083f0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              
+00008400: 2020 2020 2020 286e 702e 6973 6e61 6e28        (np.isnan(
+00008410: 582e 6461 7461 292c 2058 2e69 6e64 6963  X.data), X.indic
+00008420: 6573 2c20 582e 696e 6470 7472 292c 2073  es, X.indptr), s
+00008430: 6861 7065 3d58 2e73 6861 7065 0a20 2020  hape=X.shape.   
+00008440: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+00008450: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00008460: 656c 662e 6e5f 7361 6d70 6c65 735f 7365  elf.n_samples_se
+00008470: 656e 5f20 2b3d 2028 6e70 2e73 756d 2877  en_ += (np.sum(w
+00008480: 6569 6768 7473 2920 2d20 7375 6d5f 7765  eights) - sum_we
+00008490: 6967 6874 735f 6e61 6e29 2e61 7374 7970  ights_nan).astyp
+000084a0: 6528 0a20 2020 2020 2020 2020 2020 2020  e(.             
+000084b0: 2020 2020 2020 2064 7479 7065 0a20 2020         dtype.   
+000084c0: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+000084d0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
+000084e0: 2020 2020 2020 2020 2023 2046 6972 7374           # First
+000084f0: 2070 6173 730a 2020 2020 2020 2020 2020   pass.          
+00008500: 2020 6966 206e 6f74 2068 6173 6174 7472    if not hasattr
+00008510: 2873 656c 662c 2022 7363 616c 655f 2229  (self, "scale_")
+00008520: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00008530: 2020 7365 6c66 2e6d 6561 6e5f 203d 2030    self.mean_ = 0
+00008540: 2e30 0a20 2020 2020 2020 2020 2020 2020  .0.             
+00008550: 2020 2069 6620 7365 6c66 2e77 6974 685f     if self.with_
+00008560: 7374 643a 0a20 2020 2020 2020 2020 2020  std:.           
+00008570: 2020 2020 2020 2020 2073 656c 662e 7661           self.va
+00008580: 725f 203d 2030 2e30 0a20 2020 2020 2020  r_ = 0.0.       
+00008590: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+000085a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000085b0: 2020 2073 656c 662e 7661 725f 203d 204e     self.var_ = N
+000085c0: 6f6e 650a 0a20 2020 2020 2020 2020 2020  one..           
+000085d0: 2069 6620 6e6f 7420 7365 6c66 2e77 6974   if not self.wit
+000085e0: 685f 6d65 616e 2061 6e64 206e 6f74 2073  h_mean and not s
+000085f0: 656c 662e 7769 7468 5f73 7464 3a0a 2020  elf.with_std:.  
+00008600: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00008610: 6c66 2e6d 6561 6e5f 203d 204e 6f6e 650a  lf.mean_ = None.
+00008620: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008630: 7365 6c66 2e76 6172 5f20 3d20 4e6f 6e65  self.var_ = None
+00008640: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00008650: 2073 656c 662e 6e5f 7361 6d70 6c65 735f   self.n_samples_
+00008660: 7365 656e 5f20 2b3d 2058 2e73 6861 7065  seen_ += X.shape
+00008670: 5b30 5d20 2d20 6e70 2e69 736e 616e 2858  [0] - np.isnan(X
+00008680: 292e 7375 6d28 6178 6973 3d30 290a 0a20  ).sum(axis=0).. 
+00008690: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+000086a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000086b0: 2073 656c 662e 6d65 616e 5f2c 2073 656c   self.mean_, sel
+000086c0: 662e 7661 725f 2c20 7365 6c66 2e6e 5f73  f.var_, self.n_s
+000086d0: 616d 706c 6573 5f73 6565 6e5f 203d 205f  amples_seen_ = _
+000086e0: 696e 6372 656d 656e 7461 6c5f 6d65 616e  incremental_mean
+000086f0: 5f61 6e64 5f76 6172 280a 2020 2020 2020  _and_var(.      
+00008700: 2020 2020 2020 2020 2020 2020 2020 582c                X,
+00008710: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00008720: 2020 2020 2073 656c 662e 6d65 616e 5f2c       self.mean_,
+00008730: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00008740: 2020 2020 2073 656c 662e 7661 725f 2c0a       self.var_,.
+00008750: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008760: 2020 2020 7365 6c66 2e6e 5f73 616d 706c      self.n_sampl
+00008770: 6573 5f73 6565 6e5f 2c0a 2020 2020 2020  es_seen_,.      
+00008780: 2020 2020 2020 2020 2020 2020 2020 7361                sa
+00008790: 6d70 6c65 5f77 6569 6768 743d 7361 6d70  mple_weight=samp
+000087a0: 6c65 5f77 6569 6768 742c 0a20 2020 2020  le_weight,.     
+000087b0: 2020 2020 2020 2020 2020 2029 0a0a 2020             )..  
+000087c0: 2020 2020 2020 2320 666f 7220 6261 636b        # for back
+000087d0: 7761 7264 2d63 6f6d 7061 7469 6269 6c69  ward-compatibili
+000087e0: 7479 2c20 7265 6475 6365 206e 5f73 616d  ty, reduce n_sam
+000087f0: 706c 6573 5f73 6565 6e5f 2074 6f20 616e  ples_seen_ to an
+00008800: 2069 6e74 6567 6572 0a20 2020 2020 2020   integer.       
+00008810: 2023 2069 6620 7468 6520 6e75 6d62 6572   # if the number
+00008820: 206f 6620 7361 6d70 6c65 7320 6973 2074   of samples is t
+00008830: 6865 2073 616d 6520 666f 7220 6561 6368  he same for each
+00008840: 2066 6561 7475 7265 2028 692e 652e 206e   feature (i.e. n
+00008850: 6f0a 2020 2020 2020 2020 2320 6d69 7373  o.        # miss
+00008860: 696e 6720 7661 6c75 6573 290a 2020 2020  ing values).    
+00008870: 2020 2020 6966 206e 702e 7074 7028 7365      if np.ptp(se
+00008880: 6c66 2e6e 5f73 616d 706c 6573 5f73 6565  lf.n_samples_see
+00008890: 6e5f 2920 3d3d 2030 3a0a 2020 2020 2020  n_) == 0:.      
+000088a0: 2020 2020 2020 7365 6c66 2e6e 5f73 616d        self.n_sam
+000088b0: 706c 6573 5f73 6565 6e5f 203d 2073 656c  ples_seen_ = sel
+000088c0: 662e 6e5f 7361 6d70 6c65 735f 7365 656e  f.n_samples_seen
+000088d0: 5f5b 305d 0a0a 2020 2020 2020 2020 6966  _[0]..        if
+000088e0: 2073 656c 662e 7769 7468 5f73 7464 3a0a   self.with_std:.
+000088f0: 2020 2020 2020 2020 2020 2020 2320 4578              # Ex
+00008900: 7472 6163 7420 7468 6520 6c69 7374 206f  tract the list o
+00008910: 6620 6e65 6172 2063 6f6e 7374 616e 7420  f near constant 
+00008920: 6665 6174 7572 6573 206f 6e20 7468 6520  features on the 
+00008930: 7261 7720 7661 7269 616e 6365 732c 0a20  raw variances,. 
+00008940: 2020 2020 2020 2020 2020 2023 2062 6566             # bef
+00008950: 6f72 6520 7461 6b69 6e67 2074 6865 2073  ore taking the s
+00008960: 7175 6172 6520 726f 6f74 2e0a 2020 2020  quare root..    
+00008970: 2020 2020 2020 2020 636f 6e73 7461 6e74          constant
+00008980: 5f6d 6173 6b20 3d20 5f69 735f 636f 6e73  _mask = _is_cons
+00008990: 7461 6e74 5f66 6561 7475 7265 280a 2020  tant_feature(.  
+000089a0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+000089b0: 6c66 2e76 6172 5f2c 2073 656c 662e 6d65  lf.var_, self.me
+000089c0: 616e 5f2c 2073 656c 662e 6e5f 7361 6d70  an_, self.n_samp
+000089d0: 6c65 735f 7365 656e 5f0a 2020 2020 2020  les_seen_.      
+000089e0: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+000089f0: 2020 2020 7365 6c66 2e73 6361 6c65 5f20      self.scale_ 
+00008a00: 3d20 5f68 616e 646c 655f 7a65 726f 735f  = _handle_zeros_
+00008a10: 696e 5f73 6361 6c65 280a 2020 2020 2020  in_scale(.      
+00008a20: 2020 2020 2020 2020 2020 6e70 2e73 7172            np.sqr
+00008a30: 7428 7365 6c66 2e76 6172 5f29 2c20 636f  t(self.var_), co
+00008a40: 7079 3d46 616c 7365 2c20 636f 6e73 7461  py=False, consta
+00008a50: 6e74 5f6d 6173 6b3d 636f 6e73 7461 6e74  nt_mask=constant
+00008a60: 5f6d 6173 6b0a 2020 2020 2020 2020 2020  _mask.          
+00008a70: 2020 290a 2020 2020 2020 2020 656c 7365    ).        else
+00008a80: 3a0a 2020 2020 2020 2020 2020 2020 7365  :.            se
+00008a90: 6c66 2e73 6361 6c65 5f20 3d20 4e6f 6e65  lf.scale_ = None
+00008aa0: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return
+00008ab0: 2073 656c 660a 0a20 2020 2064 6566 2074   self..    def t
+00008ac0: 7261 6e73 666f 726d 2873 656c 662c 2058  ransform(self, X
+00008ad0: 2c20 636f 7079 3d4e 6f6e 6529 3a0a 2020  , copy=None):.  
+00008ae0: 2020 2020 2020 2222 2250 6572 666f 726d        """Perform
+00008af0: 2073 7461 6e64 6172 6469 7a61 7469 6f6e   standardization
+00008b00: 2062 7920 6365 6e74 6572 696e 6720 616e   by centering an
+00008b10: 6420 7363 616c 696e 672e 0a0a 2020 2020  d scaling...    
+00008b20: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
+00008b30: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
+00008b40: 2d0a 2020 2020 2020 2020 5820 3a20 7b61  -.        X : {a
+00008b50: 7272 6179 2d6c 696b 652c 2073 7061 7273  rray-like, spars
+00008b60: 6520 6d61 7472 6978 206f 6620 7368 6170  e matrix of shap
+00008b70: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
+00008b80: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
+00008b90: 2020 2020 2020 5468 6520 6461 7461 2075        The data u
+00008ba0: 7365 6420 746f 2073 6361 6c65 2061 6c6f  sed to scale alo
+00008bb0: 6e67 2074 6865 2066 6561 7475 7265 7320  ng the features 
+00008bc0: 6178 6973 2e0a 2020 2020 2020 2020 636f  axis..        co
+00008bd0: 7079 203a 2062 6f6f 6c2c 2064 6566 6175  py : bool, defau
+00008be0: 6c74 3d4e 6f6e 650a 2020 2020 2020 2020  lt=None.        
+00008bf0: 2020 2020 436f 7079 2074 6865 2069 6e70      Copy the inp
+00008c00: 7574 2058 206f 7220 6e6f 742e 0a0a 2020  ut X or not...  
+00008c10: 2020 2020 2020 5265 7475 726e 730a 2020        Returns.  
+00008c20: 2020 2020 2020 2d2d 2d2d 2d2d 2d0a 2020        -------.  
+00008c30: 2020 2020 2020 585f 7472 203a 207b 6e64        X_tr : {nd
+00008c40: 6172 7261 792c 2073 7061 7273 6520 6d61  array, sparse ma
+00008c50: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
+00008c60: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+00008c70: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
+00008c80: 2020 2054 7261 6e73 666f 726d 6564 2061     Transformed a
+00008c90: 7272 6179 2e0a 2020 2020 2020 2020 2222  rray..        ""
+00008ca0: 220a 2020 2020 2020 2020 6368 6563 6b5f  ".        check_
+00008cb0: 6973 5f66 6974 7465 6428 7365 6c66 290a  is_fitted(self).
+00008cc0: 0a20 2020 2020 2020 2063 6f70 7920 3d20  .        copy = 
+00008cd0: 636f 7079 2069 6620 636f 7079 2069 7320  copy if copy is 
+00008ce0: 6e6f 7420 4e6f 6e65 2065 6c73 6520 7365  not None else se
+00008cf0: 6c66 2e63 6f70 790a 2020 2020 2020 2020  lf.copy.        
+00008d00: 5820 3d20 7365 6c66 2e5f 7661 6c69 6461  X = self._valida
+00008d10: 7465 5f64 6174 6128 0a20 2020 2020 2020  te_data(.       
+00008d20: 2020 2020 2058 2c0a 2020 2020 2020 2020       X,.        
+00008d30: 2020 2020 7265 7365 743d 4661 6c73 652c      reset=False,
+00008d40: 0a20 2020 2020 2020 2020 2020 2061 6363  .            acc
+00008d50: 6570 745f 7370 6172 7365 3d22 6373 7222  ept_sparse="csr"
+00008d60: 2c0a 2020 2020 2020 2020 2020 2020 636f  ,.            co
+00008d70: 7079 3d63 6f70 792c 0a20 2020 2020 2020  py=copy,.       
+00008d80: 2020 2020 2064 7479 7065 3d46 4c4f 4154       dtype=FLOAT
+00008d90: 5f44 5459 5045 532c 0a20 2020 2020 2020  _DTYPES,.       
+00008da0: 2020 2020 2066 6f72 6365 5f61 6c6c 5f66       force_all_f
+00008db0: 696e 6974 653d 2261 6c6c 6f77 2d6e 616e  inite="allow-nan
+00008dc0: 222c 0a20 2020 2020 2020 2029 0a0a 2020  ",.        )..  
+00008dd0: 2020 2020 2020 6966 2073 7061 7273 652e        if sparse.
+00008de0: 6973 7370 6172 7365 2858 293a 0a20 2020  issparse(X):.   
+00008df0: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+00008e00: 2e77 6974 685f 6d65 616e 3a0a 2020 2020  .with_mean:.    
+00008e10: 2020 2020 2020 2020 2020 2020 7261 6973              rais
+00008e20: 6520 5661 6c75 6545 7272 6f72 280a 2020  e ValueError(.  
+00008e30: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00008e40: 2020 2243 616e 6e6f 7420 6365 6e74 6572    "Cannot center
+00008e50: 2073 7061 7273 6520 6d61 7472 6963 6573   sparse matrices
+00008e60: 3a20 7061 7373 2060 7769 7468 5f6d 6561  : pass `with_mea
+00008e70: 6e3d 4661 6c73 6560 2022 0a20 2020 2020  n=False` ".     
+00008e80: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00008e90: 696e 7374 6561 642e 2053 6565 2064 6f63  instead. See doc
+00008ea0: 7374 7269 6e67 2066 6f72 206d 6f74 6976  string for motiv
+00008eb0: 6174 696f 6e20 616e 6420 616c 7465 726e  ation and altern
+00008ec0: 6174 6976 6573 2e22 0a20 2020 2020 2020  atives.".       
+00008ed0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+00008ee0: 2020 2020 2020 2069 6620 7365 6c66 2e73         if self.s
+00008ef0: 6361 6c65 5f20 6973 206e 6f74 204e 6f6e  cale_ is not Non
+00008f00: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+00008f10: 2020 2069 6e70 6c61 6365 5f63 6f6c 756d     inplace_colum
+00008f20: 6e5f 7363 616c 6528 582c 2031 202f 2073  n_scale(X, 1 / s
+00008f30: 656c 662e 7363 616c 655f 290a 2020 2020  elf.scale_).    
+00008f40: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+00008f50: 2020 2020 2020 6966 2073 656c 662e 7769        if self.wi
+00008f60: 7468 5f6d 6561 6e3a 0a20 2020 2020 2020  th_mean:.       
+00008f70: 2020 2020 2020 2020 2058 202d 3d20 7365           X -= se
+00008f80: 6c66 2e6d 6561 6e5f 0a20 2020 2020 2020  lf.mean_.       
+00008f90: 2020 2020 2069 6620 7365 6c66 2e77 6974       if self.wit
+00008fa0: 685f 7374 643a 0a20 2020 2020 2020 2020  h_std:.         
+00008fb0: 2020 2020 2020 2058 202f 3d20 7365 6c66         X /= self
+00008fc0: 2e73 6361 6c65 5f0a 2020 2020 2020 2020  .scale_.        
+00008fd0: 7265 7475 726e 2058 0a0a 2020 2020 6465  return X..    de
+00008fe0: 6620 696e 7665 7273 655f 7472 616e 7366  f inverse_transf
+00008ff0: 6f72 6d28 7365 6c66 2c20 582c 2063 6f70  orm(self, X, cop
+00009000: 793d 4e6f 6e65 293a 0a20 2020 2020 2020  y=None):.       
+00009010: 2022 2222 5363 616c 6520 6261 636b 2074   """Scale back t
+00009020: 6865 2064 6174 6120 746f 2074 6865 206f  he data to the o
+00009030: 7269 6769 6e61 6c20 7265 7072 6573 656e  riginal represen
+00009040: 7461 7469 6f6e 2e0a 0a20 2020 2020 2020  tation...       
+00009050: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
+00009060: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
+00009070: 2020 2020 2020 2058 203a 207b 6172 7261         X : {arra
+00009080: 792d 6c69 6b65 2c20 7370 6172 7365 206d  y-like, sparse m
+00009090: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
+000090a0: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
+000090b0: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
+000090c0: 2020 2020 5468 6520 6461 7461 2075 7365      The data use
+000090d0: 6420 746f 2073 6361 6c65 2061 6c6f 6e67  d to scale along
+000090e0: 2074 6865 2066 6561 7475 7265 7320 6178   the features ax
+000090f0: 6973 2e0a 2020 2020 2020 2020 636f 7079  is..        copy
+00009100: 203a 2062 6f6f 6c2c 2064 6566 6175 6c74   : bool, default
+00009110: 3d4e 6f6e 650a 2020 2020 2020 2020 2020  =None.          
+00009120: 2020 436f 7079 2074 6865 2069 6e70 7574    Copy the input
+00009130: 2058 206f 7220 6e6f 742e 0a0a 2020 2020   X or not...    
+00009140: 2020 2020 5265 7475 726e 730a 2020 2020      Returns.    
+00009150: 2020 2020 2d2d 2d2d 2d2d 2d0a 2020 2020      -------.    
+00009160: 2020 2020 585f 7472 203a 207b 6e64 6172      X_tr : {ndar
+00009170: 7261 792c 2073 7061 7273 6520 6d61 7472  ray, sparse matr
+00009180: 6978 7d20 6f66 2073 6861 7065 2028 6e5f  ix} of shape (n_
+00009190: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
+000091a0: 7265 7329 0a20 2020 2020 2020 2020 2020  res).           
+000091b0: 2054 7261 6e73 666f 726d 6564 2061 7272   Transformed arr
+000091c0: 6179 2e0a 2020 2020 2020 2020 2222 220a  ay..        """.
+000091d0: 2020 2020 2020 2020 6368 6563 6b5f 6973          check_is
+000091e0: 5f66 6974 7465 6428 7365 6c66 290a 0a20  _fitted(self).. 
+000091f0: 2020 2020 2020 2063 6f70 7920 3d20 636f         copy = co
+00009200: 7079 2069 6620 636f 7079 2069 7320 6e6f  py if copy is no
+00009210: 7420 4e6f 6e65 2065 6c73 6520 7365 6c66  t None else self
+00009220: 2e63 6f70 790a 2020 2020 2020 2020 5820  .copy.        X 
+00009230: 3d20 6368 6563 6b5f 6172 7261 7928 0a20  = check_array(. 
+00009240: 2020 2020 2020 2020 2020 2058 2c0a 2020             X,.  
+00009250: 2020 2020 2020 2020 2020 6163 6365 7074            accept
+00009260: 5f73 7061 7273 653d 2263 7372 222c 0a20  _sparse="csr",. 
+00009270: 2020 2020 2020 2020 2020 2063 6f70 793d             copy=
+00009280: 636f 7079 2c0a 2020 2020 2020 2020 2020  copy,.          
+00009290: 2020 6474 7970 653d 464c 4f41 545f 4454    dtype=FLOAT_DT
+000092a0: 5950 4553 2c0a 2020 2020 2020 2020 2020  YPES,.          
+000092b0: 2020 666f 7263 655f 616c 6c5f 6669 6e69    force_all_fini
+000092c0: 7465 3d22 616c 6c6f 772d 6e61 6e22 2c0a  te="allow-nan",.
+000092d0: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     
+000092e0: 2020 2069 6620 7370 6172 7365 2e69 7373     if sparse.iss
+000092f0: 7061 7273 6528 5829 3a0a 2020 2020 2020  parse(X):.      
+00009300: 2020 2020 2020 6966 2073 656c 662e 7769        if self.wi
+00009310: 7468 5f6d 6561 6e3a 0a20 2020 2020 2020  th_mean:.       
+00009320: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
+00009330: 616c 7565 4572 726f 7228 0a20 2020 2020  alueError(.     
+00009340: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+00009350: 4361 6e6e 6f74 2075 6e63 656e 7465 7220  Cannot uncenter 
+00009360: 7370 6172 7365 206d 6174 7269 6365 733a  sparse matrices:
+00009370: 2070 6173 7320 6077 6974 685f 6d65 616e   pass `with_mean
+00009380: 3d46 616c 7365 6020 220a 2020 2020 2020  =False` ".      
+00009390: 2020 2020 2020 2020 2020 2020 2020 2269                "i
+000093a0: 6e73 7465 6164 2053 6565 2064 6f63 7374  nstead See docst
+000093b0: 7269 6e67 2066 6f72 206d 6f74 6976 6174  ring for motivat
+000093c0: 696f 6e20 616e 6420 616c 7465 726e 6174  ion and alternat
+000093d0: 6976 6573 2e22 0a20 2020 2020 2020 2020  ives.".         
+000093e0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+000093f0: 2020 2020 2069 6620 7365 6c66 2e73 6361       if self.sca
+00009400: 6c65 5f20 6973 206e 6f74 204e 6f6e 653a  le_ is not None:
+00009410: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00009420: 2069 6e70 6c61 6365 5f63 6f6c 756d 6e5f   inplace_column_
+00009430: 7363 616c 6528 582c 2073 656c 662e 7363  scale(X, self.sc
+00009440: 616c 655f 290a 2020 2020 2020 2020 656c  ale_).        el
+00009450: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00009460: 6966 2073 656c 662e 7769 7468 5f73 7464  if self.with_std
+00009470: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+00009480: 2020 5820 2a3d 2073 656c 662e 7363 616c    X *= self.scal
+00009490: 655f 0a20 2020 2020 2020 2020 2020 2069  e_.            i
+000094a0: 6620 7365 6c66 2e77 6974 685f 6d65 616e  f self.with_mean
+000094b0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+000094c0: 2020 5820 2b3d 2073 656c 662e 6d65 616e    X += self.mean
+000094d0: 5f0a 2020 2020 2020 2020 7265 7475 726e  _.        return
+000094e0: 2058 0a0a 2020 2020 6465 6620 5f6d 6f72   X..    def _mor
+000094f0: 655f 7461 6773 2873 656c 6629 3a0a 2020  e_tags(self):.  
+00009500: 2020 2020 2020 7265 7475 726e 207b 2261        return {"a
+00009510: 6c6c 6f77 5f6e 616e 223a 2054 7275 652c  llow_nan": True,
+00009520: 2022 7072 6573 6572 7665 735f 6474 7970   "preserves_dtyp
+00009530: 6522 3a20 5b6e 702e 666c 6f61 7436 342c  e": [np.float64,
+00009540: 206e 702e 666c 6f61 7433 325d 7d0a 0a0a   np.float32]}...
+00009550: 636c 6173 7320 4d61 7841 6273 5363 616c  class MaxAbsScal
+00009560: 6572 284f 6e65 546f 4f6e 6546 6561 7475  er(OneToOneFeatu
+00009570: 7265 4d69 7869 6e2c 2054 7261 6e73 666f  reMixin, Transfo
+00009580: 726d 6572 4d69 7869 6e2c 2042 6173 6545  rmerMixin, BaseE
+00009590: 7374 696d 6174 6f72 293a 0a20 2020 2022  stimator):.    "
+000095a0: 2222 5363 616c 6520 6561 6368 2066 6561  ""Scale each fea
+000095b0: 7475 7265 2062 7920 6974 7320 6d61 7869  ture by its maxi
+000095c0: 6d75 6d20 6162 736f 6c75 7465 2076 616c  mum absolute val
+000095d0: 7565 2e0a 0a20 2020 2054 6869 7320 6573  ue...    This es
+000095e0: 7469 6d61 746f 7220 7363 616c 6573 2061  timator scales a
+000095f0: 6e64 2074 7261 6e73 6c61 7465 7320 6561  nd translates ea
+00009600: 6368 2066 6561 7475 7265 2069 6e64 6976  ch feature indiv
+00009610: 6964 7561 6c6c 7920 7375 6368 0a20 2020  idually such.   
+00009620: 2074 6861 7420 7468 6520 6d61 7869 6d61   that the maxima
+00009630: 6c20 6162 736f 6c75 7465 2076 616c 7565  l absolute value
+00009640: 206f 6620 6561 6368 2066 6561 7475 7265   of each feature
+00009650: 2069 6e20 7468 650a 2020 2020 7472 6169   in the.    trai
+00009660: 6e69 6e67 2073 6574 2077 696c 6c20 6265  ning set will be
+00009670: 2031 2e30 2e20 4974 2064 6f65 7320 6e6f   1.0. It does no
+00009680: 7420 7368 6966 742f 6365 6e74 6572 2074  t shift/center t
+00009690: 6865 2064 6174 612c 2061 6e64 0a20 2020  he data, and.   
+000096a0: 2074 6875 7320 646f 6573 206e 6f74 2064   thus does not d
+000096b0: 6573 7472 6f79 2061 6e79 2073 7061 7273  estroy any spars
+000096c0: 6974 792e 0a0a 2020 2020 5468 6973 2073  ity...    This s
+000096d0: 6361 6c65 7220 6361 6e20 616c 736f 2062  caler can also b
+000096e0: 6520 6170 706c 6965 6420 746f 2073 7061  e applied to spa
+000096f0: 7273 6520 4353 5220 6f72 2043 5343 206d  rse CSR or CSC m
+00009700: 6174 7269 6365 732e 0a0a 2020 2020 604d  atrices...    `M
+00009710: 6178 4162 7353 6361 6c65 7260 2064 6f65  axAbsScaler` doe
+00009720: 736e 2774 2072 6564 7563 6520 7468 6520  sn't reduce the 
+00009730: 6566 6665 6374 206f 6620 6f75 746c 6965  effect of outlie
+00009740: 7273 3b20 6974 206f 6e6c 7920 6c69 6e65  rs; it only line
+00009750: 6172 6c79 0a20 2020 2073 6361 6c65 7320  arly.    scales 
+00009760: 7468 656d 2064 6f77 6e2e 2046 6f72 2061  them down. For a
+00009770: 6e20 6578 616d 706c 6520 7669 7375 616c  n example visual
+00009780: 697a 6174 696f 6e2c 2072 6566 6572 2074  ization, refer t
+00009790: 6f20 3a72 6566 3a60 436f 6d70 6172 650a  o :ref:`Compare.
+000097a0: 2020 2020 4d61 7841 6273 5363 616c 6572      MaxAbsScaler
+000097b0: 2077 6974 6820 6f74 6865 7220 7363 616c   with other scal
+000097c0: 6572 7320 3c70 6c6f 745f 616c 6c5f 7363  ers <plot_all_sc
+000097d0: 616c 696e 675f 6d61 785f 6162 735f 7363  aling_max_abs_sc
+000097e0: 616c 6572 5f73 6563 7469 6f6e 3e60 2e0a  aler_section>`..
+000097f0: 0a20 2020 202e 2e20 7665 7273 696f 6e61  .    .. versiona
+00009800: 6464 6564 3a3a 2030 2e31 370a 0a20 2020  dded:: 0.17..   
+00009810: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
+00009820: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2063  ----------.    c
+00009830: 6f70 7920 3a20 626f 6f6c 2c20 6465 6661  opy : bool, defa
+00009840: 756c 743d 5472 7565 0a20 2020 2020 2020  ult=True.       
+00009850: 2053 6574 2074 6f20 4661 6c73 6520 746f   Set to False to
+00009860: 2070 6572 666f 726d 2069 6e70 6c61 6365   perform inplace
+00009870: 2073 6361 6c69 6e67 2061 6e64 2061 766f   scaling and avo
+00009880: 6964 2061 2063 6f70 7920 2869 6620 7468  id a copy (if th
+00009890: 6520 696e 7075 740a 2020 2020 2020 2020  e input.        
+000098a0: 6973 2061 6c72 6561 6479 2061 206e 756d  is already a num
+000098b0: 7079 2061 7272 6179 292e 0a0a 2020 2020  py array)...    
+000098c0: 4174 7472 6962 7574 6573 0a20 2020 202d  Attributes.    -
+000098d0: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 7363  ---------.    sc
+000098e0: 616c 655f 203a 206e 6461 7272 6179 206f  ale_ : ndarray o
+000098f0: 6620 7368 6170 6520 286e 5f66 6561 7475  f shape (n_featu
+00009900: 7265 732c 290a 2020 2020 2020 2020 5065  res,).        Pe
+00009910: 7220 6665 6174 7572 6520 7265 6c61 7469  r feature relati
+00009920: 7665 2073 6361 6c69 6e67 206f 6620 7468  ve scaling of th
+00009930: 6520 6461 7461 2e0a 0a20 2020 2020 2020  e data...       
+00009940: 202e 2e20 7665 7273 696f 6e61 6464 6564   .. versionadded
+00009950: 3a3a 2030 2e31 370a 2020 2020 2020 2020  :: 0.17.        
+00009960: 2020 202a 7363 616c 655f 2a20 6174 7472     *scale_* attr
+00009970: 6962 7574 652e 0a0a 2020 2020 6d61 785f  ibute...    max_
+00009980: 6162 735f 203a 206e 6461 7272 6179 206f  abs_ : ndarray o
+00009990: 6620 7368 6170 6520 286e 5f66 6561 7475  f shape (n_featu
+000099a0: 7265 732c 290a 2020 2020 2020 2020 5065  res,).        Pe
+000099b0: 7220 6665 6174 7572 6520 6d61 7869 6d75  r feature maximu
+000099c0: 6d20 6162 736f 6c75 7465 2076 616c 7565  m absolute value
+000099d0: 2e0a 0a20 2020 206e 5f66 6561 7475 7265  ...    n_feature
+000099e0: 735f 696e 5f20 3a20 696e 740a 2020 2020  s_in_ : int.    
+000099f0: 2020 2020 4e75 6d62 6572 206f 6620 6665      Number of fe
+00009a00: 6174 7572 6573 2073 6565 6e20 6475 7269  atures seen duri
+00009a10: 6e67 203a 7465 726d 3a60 6669 7460 2e0a  ng :term:`fit`..
+00009a20: 0a20 2020 2020 2020 202e 2e20 7665 7273  .        .. vers
+00009a30: 696f 6e61 6464 6564 3a3a 2030 2e32 340a  ionadded:: 0.24.
+00009a40: 0a20 2020 2066 6561 7475 7265 5f6e 616d  .    feature_nam
+00009a50: 6573 5f69 6e5f 203a 206e 6461 7272 6179  es_in_ : ndarray
+00009a60: 206f 6620 7368 6170 6520 2860 6e5f 6665   of shape (`n_fe
+00009a70: 6174 7572 6573 5f69 6e5f 602c 290a 2020  atures_in_`,).  
+00009a80: 2020 2020 2020 4e61 6d65 7320 6f66 2066        Names of f
+00009a90: 6561 7475 7265 7320 7365 656e 2064 7572  eatures seen dur
+00009aa0: 696e 6720 3a74 6572 6d3a 6066 6974 602e  ing :term:`fit`.
+00009ab0: 2044 6566 696e 6564 206f 6e6c 7920 7768   Defined only wh
+00009ac0: 656e 2060 5860 0a20 2020 2020 2020 2068  en `X`.        h
+00009ad0: 6173 2066 6561 7475 7265 206e 616d 6573  as feature names
+00009ae0: 2074 6861 7420 6172 6520 616c 6c20 7374   that are all st
+00009af0: 7269 6e67 732e 0a0a 2020 2020 2020 2020  rings...        
+00009b00: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
+00009b10: 3a20 312e 300a 0a20 2020 206e 5f73 616d  : 1.0..    n_sam
+00009b20: 706c 6573 5f73 6565 6e5f 203a 2069 6e74  ples_seen_ : int
+00009b30: 0a20 2020 2020 2020 2054 6865 206e 756d  .        The num
+00009b40: 6265 7220 6f66 2073 616d 706c 6573 2070  ber of samples p
+00009b50: 726f 6365 7373 6564 2062 7920 7468 6520  rocessed by the 
+00009b60: 6573 7469 6d61 746f 722e 2057 696c 6c20  estimator. Will 
+00009b70: 6265 2072 6573 6574 206f 6e0a 2020 2020  be reset on.    
+00009b80: 2020 2020 6e65 7720 6361 6c6c 7320 746f      new calls to
+00009b90: 2066 6974 2c20 6275 7420 696e 6372 656d   fit, but increm
+00009ba0: 656e 7473 2061 6372 6f73 7320 6060 7061  ents across ``pa
+00009bb0: 7274 6961 6c5f 6669 7460 6020 6361 6c6c  rtial_fit`` call
+00009bc0: 732e 0a0a 2020 2020 5365 6520 416c 736f  s...    See Also
+00009bd0: 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a 2020  .    --------.  
+00009be0: 2020 6d61 7861 6273 5f73 6361 6c65 203a    maxabs_scale :
+00009bf0: 2045 7175 6976 616c 656e 7420 6675 6e63   Equivalent func
+00009c00: 7469 6f6e 2077 6974 686f 7574 2074 6865  tion without the
+00009c10: 2065 7374 696d 6174 6f72 2041 5049 2e0a   estimator API..
+00009c20: 0a20 2020 204e 6f74 6573 0a20 2020 202d  .    Notes.    -
+00009c30: 2d2d 2d2d 0a20 2020 204e 614e 7320 6172  ----.    NaNs ar
+00009c40: 6520 7472 6561 7465 6420 6173 206d 6973  e treated as mis
+00009c50: 7369 6e67 2076 616c 7565 733a 2064 6973  sing values: dis
+00009c60: 7265 6761 7264 6564 2069 6e20 6669 742c  regarded in fit,
+00009c70: 2061 6e64 206d 6169 6e74 6169 6e65 6420   and maintained 
+00009c80: 696e 0a20 2020 2074 7261 6e73 666f 726d  in.    transform
+00009c90: 2e0a 0a20 2020 2045 7861 6d70 6c65 730a  ...    Examples.
+00009ca0: 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020      --------.   
+00009cb0: 203e 3e3e 2066 726f 6d20 736b 6c65 6172   >>> from sklear
+00009cc0: 6e2e 7072 6570 726f 6365 7373 696e 6720  n.preprocessing 
+00009cd0: 696d 706f 7274 204d 6178 4162 7353 6361  import MaxAbsSca
+00009ce0: 6c65 720a 2020 2020 3e3e 3e20 5820 3d20  ler.    >>> X = 
+00009cf0: 5b5b 2031 2e2c 202d 312e 2c20 2032 2e5d  [[ 1., -1.,  2.]
+00009d00: 2c0a 2020 2020 2e2e 2e20 2020 2020 205b  ,.    ...      [
+00009d10: 2032 2e2c 2020 302e 2c20 2030 2e5d 2c0a   2.,  0.,  0.],.
+00009d20: 2020 2020 2e2e 2e20 2020 2020 205b 2030      ...      [ 0
+00009d30: 2e2c 2020 312e 2c20 2d31 2e5d 5d0a 2020  .,  1., -1.]].  
+00009d40: 2020 3e3e 3e20 7472 616e 7366 6f72 6d65    >>> transforme
+00009d50: 7220 3d20 4d61 7841 6273 5363 616c 6572  r = MaxAbsScaler
+00009d60: 2829 2e66 6974 2858 290a 2020 2020 3e3e  ().fit(X).    >>
+00009d70: 3e20 7472 616e 7366 6f72 6d65 720a 2020  > transformer.  
+00009d80: 2020 4d61 7841 6273 5363 616c 6572 2829    MaxAbsScaler()
+00009d90: 0a20 2020 203e 3e3e 2074 7261 6e73 666f  .    >>> transfo
+00009da0: 726d 6572 2e74 7261 6e73 666f 726d 2858  rmer.transform(X
+00009db0: 290a 2020 2020 6172 7261 7928 5b5b 2030  ).    array([[ 0
+00009dc0: 2e35 2c20 2d31 2e20 2c20 2031 2e20 5d2c  .5, -1. ,  1. ],
+00009dd0: 0a20 2020 2020 2020 2020 2020 5b20 312e  .           [ 1.
+00009de0: 202c 2020 302e 202c 2020 302e 205d 2c0a   ,  0. ,  0. ],.
+00009df0: 2020 2020 2020 2020 2020 205b 2030 2e20             [ 0. 
+00009e00: 2c20 2031 2e20 2c20 2d30 2e35 5d5d 290a  ,  1. , -0.5]]).
+00009e10: 2020 2020 2222 220a 0a20 2020 205f 7061      """..    _pa
+00009e20: 7261 6d65 7465 725f 636f 6e73 7472 6169  rameter_constrai
+00009e30: 6e74 733a 2064 6963 7420 3d20 7b22 636f  nts: dict = {"co
+00009e40: 7079 223a 205b 2262 6f6f 6c65 616e 225d  py": ["boolean"]
+00009e50: 7d0a 0a20 2020 2064 6566 205f 5f69 6e69  }..    def __ini
+00009e60: 745f 5f28 7365 6c66 2c20 2a2c 2063 6f70  t__(self, *, cop
+00009e70: 793d 5472 7565 293a 0a20 2020 2020 2020  y=True):.       
+00009e80: 2073 656c 662e 636f 7079 203d 2063 6f70   self.copy = cop
+00009e90: 790a 0a20 2020 2064 6566 205f 7265 7365  y..    def _rese
+00009ea0: 7428 7365 6c66 293a 0a20 2020 2020 2020  t(self):.       
+00009eb0: 2022 2222 5265 7365 7420 696e 7465 726e   """Reset intern
+00009ec0: 616c 2064 6174 612d 6465 7065 6e64 656e  al data-dependen
+00009ed0: 7420 7374 6174 6520 6f66 2074 6865 2073  t state of the s
+00009ee0: 6361 6c65 722c 2069 6620 6e65 6365 7373  caler, if necess
+00009ef0: 6172 792e 0a0a 2020 2020 2020 2020 5f5f  ary...        __
+00009f00: 696e 6974 5f5f 2070 6172 616d 6574 6572  init__ parameter
+00009f10: 7320 6172 6520 6e6f 7420 746f 7563 6865  s are not touche
+00009f20: 642e 0a20 2020 2020 2020 2022 2222 0a20  d..        """. 
+00009f30: 2020 2020 2020 2023 2043 6865 636b 696e         # Checkin
+00009f40: 6720 6f6e 6520 6174 7472 6962 7574 6520  g one attribute 
+00009f50: 6973 2065 6e6f 7567 682c 2062 6563 6175  is enough, becau
+00009f60: 7365 2074 6865 7920 6172 6520 616c 6c20  se they are all 
+00009f70: 7365 7420 746f 6765 7468 6572 0a20 2020  set together.   
+00009f80: 2020 2020 2023 2069 6e20 7061 7274 6961       # in partia
+00009f90: 6c5f 6669 740a 2020 2020 2020 2020 6966  l_fit.        if
+00009fa0: 2068 6173 6174 7472 2873 656c 662c 2022   hasattr(self, "
+00009fb0: 7363 616c 655f 2229 3a0a 2020 2020 2020  scale_"):.      
+00009fc0: 2020 2020 2020 6465 6c20 7365 6c66 2e73        del self.s
+00009fd0: 6361 6c65 5f0a 2020 2020 2020 2020 2020  cale_.          
+00009fe0: 2020 6465 6c20 7365 6c66 2e6e 5f73 616d    del self.n_sam
+00009ff0: 706c 6573 5f73 6565 6e5f 0a20 2020 2020  ples_seen_.     
+0000a000: 2020 2020 2020 2064 656c 2073 656c 662e         del self.
+0000a010: 6d61 785f 6162 735f 0a0a 2020 2020 6465  max_abs_..    de
+0000a020: 6620 6669 7428 7365 6c66 2c20 582c 2079  f fit(self, X, y
+0000a030: 3d4e 6f6e 6529 3a0a 2020 2020 2020 2020  =None):.        
+0000a040: 2222 2243 6f6d 7075 7465 2074 6865 206d  """Compute the m
+0000a050: 6178 696d 756d 2061 6273 6f6c 7574 6520  aximum absolute 
+0000a060: 7661 6c75 6520 746f 2062 6520 7573 6564  value to be used
+0000a070: 2066 6f72 206c 6174 6572 2073 6361 6c69   for later scali
+0000a080: 6e67 2e0a 0a20 2020 2020 2020 2050 6172  ng...        Par
+0000a090: 616d 6574 6572 730a 2020 2020 2020 2020  ameters.        
+0000a0a0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
+0000a0b0: 2020 2058 203a 207b 6172 7261 792d 6c69     X : {array-li
+0000a0c0: 6b65 2c20 7370 6172 7365 206d 6174 7269  ke, sparse matri
+0000a0d0: 787d 206f 6620 7368 6170 6520 286e 5f73  x} of shape (n_s
+0000a0e0: 616d 706c 6573 2c20 6e5f 6665 6174 7572  amples, n_featur
+0000a0f0: 6573 290a 2020 2020 2020 2020 2020 2020  es).            
+0000a100: 5468 6520 6461 7461 2075 7365 6420 746f  The data used to
+0000a110: 2063 6f6d 7075 7465 2074 6865 2070 6572   compute the per
+0000a120: 2d66 6561 7475 7265 206d 696e 696d 756d  -feature minimum
+0000a130: 2061 6e64 206d 6178 696d 756d 0a20 2020   and maximum.   
+0000a140: 2020 2020 2020 2020 2075 7365 6420 666f           used fo
+0000a150: 7220 6c61 7465 7220 7363 616c 696e 6720  r later scaling 
+0000a160: 616c 6f6e 6720 7468 6520 6665 6174 7572  along the featur
+0000a170: 6573 2061 7869 732e 0a0a 2020 2020 2020  es axis...      
+0000a180: 2020 7920 3a20 4e6f 6e65 0a20 2020 2020    y : None.     
+0000a190: 2020 2020 2020 2049 676e 6f72 6564 2e0a         Ignored..
+0000a1a0: 0a20 2020 2020 2020 2052 6574 7572 6e73  .        Returns
+0000a1b0: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+0000a1c0: 0a20 2020 2020 2020 2073 656c 6620 3a20  .        self : 
+0000a1d0: 6f62 6a65 6374 0a20 2020 2020 2020 2020  object.         
+0000a1e0: 2020 2046 6974 7465 6420 7363 616c 6572     Fitted scaler
+0000a1f0: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
+0000a200: 2020 2020 2020 2320 5265 7365 7420 696e        # Reset in
+0000a210: 7465 726e 616c 2073 7461 7465 2062 6566  ternal state bef
+0000a220: 6f72 6520 6669 7474 696e 670a 2020 2020  ore fitting.    
+0000a230: 2020 2020 7365 6c66 2e5f 7265 7365 7428      self._reset(
+0000a240: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
+0000a250: 2073 656c 662e 7061 7274 6961 6c5f 6669   self.partial_fi
+0000a260: 7428 582c 2079 290a 0a20 2020 2040 5f66  t(X, y)..    @_f
+0000a270: 6974 5f63 6f6e 7465 7874 2870 7265 6665  it_context(prefe
+0000a280: 725f 736b 6970 5f6e 6573 7465 645f 7661  r_skip_nested_va
+0000a290: 6c69 6461 7469 6f6e 3d54 7275 6529 0a20  lidation=True). 
+0000a2a0: 2020 2064 6566 2070 6172 7469 616c 5f66     def partial_f
+0000a2b0: 6974 2873 656c 662c 2058 2c20 793d 4e6f  it(self, X, y=No
+0000a2c0: 6e65 293a 0a20 2020 2020 2020 2022 2222  ne):.        """
+0000a2d0: 4f6e 6c69 6e65 2063 6f6d 7075 7461 7469  Online computati
+0000a2e0: 6f6e 206f 6620 6d61 7820 6162 736f 6c75  on of max absolu
+0000a2f0: 7465 2076 616c 7565 206f 6620 5820 666f  te value of X fo
+0000a300: 7220 6c61 7465 7220 7363 616c 696e 672e  r later scaling.
+0000a310: 0a0a 2020 2020 2020 2020 416c 6c20 6f66  ..        All of
+0000a320: 2058 2069 7320 7072 6f63 6573 7365 6420   X is processed 
+0000a330: 6173 2061 2073 696e 676c 6520 6261 7463  as a single batc
+0000a340: 682e 2054 6869 7320 6973 2069 6e74 656e  h. This is inten
+0000a350: 6465 6420 666f 7220 6361 7365 730a 2020  ded for cases.  
+0000a360: 2020 2020 2020 7768 656e 203a 6d65 7468        when :meth
+0000a370: 3a60 6669 7460 2069 7320 6e6f 7420 6665  :`fit` is not fe
+0000a380: 6173 6962 6c65 2064 7565 2074 6f20 7665  asible due to ve
+0000a390: 7279 206c 6172 6765 206e 756d 6265 7220  ry large number 
+0000a3a0: 6f66 0a20 2020 2020 2020 2060 6e5f 7361  of.        `n_sa
+0000a3b0: 6d70 6c65 7360 206f 7220 6265 6361 7573  mples` or becaus
+0000a3c0: 6520 5820 6973 2072 6561 6420 6672 6f6d  e X is read from
+0000a3d0: 2061 2063 6f6e 7469 6e75 6f75 7320 7374   a continuous st
+0000a3e0: 7265 616d 2e0a 0a20 2020 2020 2020 2050  ream...        P
+0000a3f0: 6172 616d 6574 6572 730a 2020 2020 2020  arameters.      
+0000a400: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020    ----------.   
+0000a410: 2020 2020 2058 203a 207b 6172 7261 792d       X : {array-
+0000a420: 6c69 6b65 2c20 7370 6172 7365 206d 6174  like, sparse mat
+0000a430: 7269 787d 206f 6620 7368 6170 6520 286e  rix} of shape (n
+0000a440: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
+0000a450: 7572 6573 290a 2020 2020 2020 2020 2020  ures).          
+0000a460: 2020 5468 6520 6461 7461 2075 7365 6420    The data used 
+0000a470: 746f 2063 6f6d 7075 7465 2074 6865 206d  to compute the m
+0000a480: 6561 6e20 616e 6420 7374 616e 6461 7264  ean and standard
+0000a490: 2064 6576 6961 7469 6f6e 0a20 2020 2020   deviation.     
+0000a4a0: 2020 2020 2020 2075 7365 6420 666f 7220         used for 
+0000a4b0: 6c61 7465 7220 7363 616c 696e 6720 616c  later scaling al
+0000a4c0: 6f6e 6720 7468 6520 6665 6174 7572 6573  ong the features
+0000a4d0: 2061 7869 732e 0a0a 2020 2020 2020 2020   axis...        
+0000a4e0: 7920 3a20 4e6f 6e65 0a20 2020 2020 2020  y : None.       
+0000a4f0: 2020 2020 2049 676e 6f72 6564 2e0a 0a20       Ignored... 
+0000a500: 2020 2020 2020 2052 6574 7572 6e73 0a20         Returns. 
+0000a510: 2020 2020 2020 202d 2d2d 2d2d 2d2d 0a20         -------. 
+0000a520: 2020 2020 2020 2073 656c 6620 3a20 6f62         self : ob
+0000a530: 6a65 6374 0a20 2020 2020 2020 2020 2020  ject.           
+0000a540: 2046 6974 7465 6420 7363 616c 6572 2e0a   Fitted scaler..
+0000a550: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
+0000a560: 2020 2020 7870 2c20 5f20 3d20 6765 745f      xp, _ = get_
+0000a570: 6e61 6d65 7370 6163 6528 5829 0a0a 2020  namespace(X)..  
+0000a580: 2020 2020 2020 6669 7273 745f 7061 7373        first_pass
+0000a590: 203d 206e 6f74 2068 6173 6174 7472 2873   = not hasattr(s
+0000a5a0: 656c 662c 2022 6e5f 7361 6d70 6c65 735f  elf, "n_samples_
+0000a5b0: 7365 656e 5f22 290a 2020 2020 2020 2020  seen_").        
+0000a5c0: 5820 3d20 7365 6c66 2e5f 7661 6c69 6461  X = self._valida
+0000a5d0: 7465 5f64 6174 6128 0a20 2020 2020 2020  te_data(.       
+0000a5e0: 2020 2020 2058 2c0a 2020 2020 2020 2020       X,.        
+0000a5f0: 2020 2020 7265 7365 743d 6669 7273 745f      reset=first_
+0000a600: 7061 7373 2c0a 2020 2020 2020 2020 2020  pass,.          
+0000a610: 2020 6163 6365 7074 5f73 7061 7273 653d    accept_sparse=
+0000a620: 2822 6373 7222 2c20 2263 7363 2229 2c0a  ("csr", "csc"),.
+0000a630: 2020 2020 2020 2020 2020 2020 6474 7970              dtyp
+0000a640: 653d 5f61 7272 6179 5f61 7069 2e73 7570  e=_array_api.sup
+0000a650: 706f 7274 6564 5f66 6c6f 6174 5f64 7479  ported_float_dty
+0000a660: 7065 7328 7870 292c 0a20 2020 2020 2020  pes(xp),.       
+0000a670: 2020 2020 2066 6f72 6365 5f61 6c6c 5f66       force_all_f
+0000a680: 696e 6974 653d 2261 6c6c 6f77 2d6e 616e  inite="allow-nan
+0000a690: 222c 0a20 2020 2020 2020 2029 0a0a 2020  ",.        )..  
+0000a6a0: 2020 2020 2020 6966 2073 7061 7273 652e        if sparse.
+0000a6b0: 6973 7370 6172 7365 2858 293a 0a20 2020  issparse(X):.   
+0000a6c0: 2020 2020 2020 2020 206d 696e 732c 206d           mins, m
+0000a6d0: 6178 7320 3d20 6d69 6e5f 6d61 785f 6178  axs = min_max_ax
+0000a6e0: 6973 2858 2c20 6178 6973 3d30 2c20 6967  is(X, axis=0, ig
+0000a6f0: 6e6f 7265 5f6e 616e 3d54 7275 6529 0a20  nore_nan=True). 
+0000a700: 2020 2020 2020 2020 2020 206d 6178 5f61             max_a
+0000a710: 6273 203d 206e 702e 6d61 7869 6d75 6d28  bs = np.maximum(
+0000a720: 6e70 2e61 6273 286d 696e 7329 2c20 6e70  np.abs(mins), np
+0000a730: 2e61 6273 286d 6178 7329 290a 2020 2020  .abs(maxs)).    
+0000a740: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0000a750: 2020 2020 2020 6d61 785f 6162 7320 3d20        max_abs = 
+0000a760: 5f61 7272 6179 5f61 7069 2e5f 6e61 6e6d  _array_api._nanm
+0000a770: 6178 2878 702e 6162 7328 5829 2c20 6178  ax(xp.abs(X), ax
+0000a780: 6973 3d30 2c20 7870 3d78 7029 0a0a 2020  is=0, xp=xp)..  
+0000a790: 2020 2020 2020 6966 2066 6972 7374 5f70        if first_p
+0000a7a0: 6173 733a 0a20 2020 2020 2020 2020 2020  ass:.           
+0000a7b0: 2073 656c 662e 6e5f 7361 6d70 6c65 735f   self.n_samples_
+0000a7c0: 7365 656e 5f20 3d20 582e 7368 6170 655b  seen_ = X.shape[
+0000a7d0: 305d 0a20 2020 2020 2020 2065 6c73 653a  0].        else:
+0000a7e0: 0a20 2020 2020 2020 2020 2020 206d 6178  .            max
+0000a7f0: 5f61 6273 203d 2078 702e 6d61 7869 6d75  _abs = xp.maximu
+0000a800: 6d28 7365 6c66 2e6d 6178 5f61 6273 5f2c  m(self.max_abs_,
+0000a810: 206d 6178 5f61 6273 290a 2020 2020 2020   max_abs).      
+0000a820: 2020 2020 2020 7365 6c66 2e6e 5f73 616d        self.n_sam
+0000a830: 706c 6573 5f73 6565 6e5f 202b 3d20 582e  ples_seen_ += X.
+0000a840: 7368 6170 655b 305d 0a0a 2020 2020 2020  shape[0]..      
+0000a850: 2020 7365 6c66 2e6d 6178 5f61 6273 5f20    self.max_abs_ 
+0000a860: 3d20 6d61 785f 6162 730a 2020 2020 2020  = max_abs.      
+0000a870: 2020 7365 6c66 2e73 6361 6c65 5f20 3d20    self.scale_ = 
+0000a880: 5f68 616e 646c 655f 7a65 726f 735f 696e  _handle_zeros_in
+0000a890: 5f73 6361 6c65 286d 6178 5f61 6273 2c20  _scale(max_abs, 
+0000a8a0: 636f 7079 3d54 7275 6529 0a20 2020 2020  copy=True).     
+0000a8b0: 2020 2072 6574 7572 6e20 7365 6c66 0a0a     return self..
+0000a8c0: 2020 2020 6465 6620 7472 616e 7366 6f72      def transfor
+0000a8d0: 6d28 7365 6c66 2c20 5829 3a0a 2020 2020  m(self, X):.    
+0000a8e0: 2020 2020 2222 2253 6361 6c65 2074 6865      """Scale the
+0000a8f0: 2064 6174 612e 0a0a 2020 2020 2020 2020   data...        
+0000a900: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
+0000a910: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
+0000a920: 2020 2020 2020 5820 3a20 7b61 7272 6179        X : {array
+0000a930: 2d6c 696b 652c 2073 7061 7273 6520 6d61  -like, sparse ma
+0000a940: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
+0000a950: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+0000a960: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
+0000a970: 2020 2054 6865 2064 6174 6120 7468 6174     The data that
+0000a980: 2073 686f 756c 6420 6265 2073 6361 6c65   should be scale
+0000a990: 642e 0a0a 2020 2020 2020 2020 5265 7475  d...        Retu
+0000a9a0: 726e 730a 2020 2020 2020 2020 2d2d 2d2d  rns.        ----
+0000a9b0: 2d2d 2d0a 2020 2020 2020 2020 585f 7472  ---.        X_tr
+0000a9c0: 203a 207b 6e64 6172 7261 792c 2073 7061   : {ndarray, spa
+0000a9d0: 7273 6520 6d61 7472 6978 7d20 6f66 2073  rse matrix} of s
+0000a9e0: 6861 7065 2028 6e5f 7361 6d70 6c65 732c  hape (n_samples,
+0000a9f0: 206e 5f66 6561 7475 7265 7329 0a20 2020   n_features).   
+0000aa00: 2020 2020 2020 2020 2054 7261 6e73 666f           Transfo
+0000aa10: 726d 6564 2061 7272 6179 2e0a 2020 2020  rmed array..    
+0000aa20: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
+0000aa30: 6368 6563 6b5f 6973 5f66 6974 7465 6428  check_is_fitted(
+0000aa40: 7365 6c66 290a 0a20 2020 2020 2020 2078  self)..        x
+0000aa50: 702c 205f 203d 2067 6574 5f6e 616d 6573  p, _ = get_names
+0000aa60: 7061 6365 2858 290a 0a20 2020 2020 2020  pace(X)..       
+0000aa70: 2058 203d 2073 656c 662e 5f76 616c 6964   X = self._valid
+0000aa80: 6174 655f 6461 7461 280a 2020 2020 2020  ate_data(.      
+0000aa90: 2020 2020 2020 582c 0a20 2020 2020 2020        X,.       
+0000aaa0: 2020 2020 2061 6363 6570 745f 7370 6172       accept_spar
+0000aab0: 7365 3d28 2263 7372 222c 2022 6373 6322  se=("csr", "csc"
+0000aac0: 292c 0a20 2020 2020 2020 2020 2020 2063  ),.            c
+0000aad0: 6f70 793d 7365 6c66 2e63 6f70 792c 0a20  opy=self.copy,. 
+0000aae0: 2020 2020 2020 2020 2020 2072 6573 6574             reset
+0000aaf0: 3d46 616c 7365 2c0a 2020 2020 2020 2020  =False,.        
+0000ab00: 2020 2020 6474 7970 653d 5f61 7272 6179      dtype=_array
+0000ab10: 5f61 7069 2e73 7570 706f 7274 6564 5f66  _api.supported_f
+0000ab20: 6c6f 6174 5f64 7479 7065 7328 7870 292c  loat_dtypes(xp),
+0000ab30: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
+0000ab40: 6365 5f61 6c6c 5f66 696e 6974 653d 2261  ce_all_finite="a
+0000ab50: 6c6c 6f77 2d6e 616e 222c 0a20 2020 2020  llow-nan",.     
+0000ab60: 2020 2029 0a0a 2020 2020 2020 2020 6966     )..        if
+0000ab70: 2073 7061 7273 652e 6973 7370 6172 7365   sparse.issparse
+0000ab80: 2858 293a 0a20 2020 2020 2020 2020 2020  (X):.           
+0000ab90: 2069 6e70 6c61 6365 5f63 6f6c 756d 6e5f   inplace_column_
+0000aba0: 7363 616c 6528 582c 2031 2e30 202f 2073  scale(X, 1.0 / s
+0000abb0: 656c 662e 7363 616c 655f 290a 2020 2020  elf.scale_).    
+0000abc0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0000abd0: 2020 2020 2020 5820 2f3d 2073 656c 662e        X /= self.
+0000abe0: 7363 616c 655f 0a20 2020 2020 2020 2072  scale_.        r
+0000abf0: 6574 7572 6e20 580a 0a20 2020 2064 6566  eturn X..    def
+0000ac00: 2069 6e76 6572 7365 5f74 7261 6e73 666f   inverse_transfo
+0000ac10: 726d 2873 656c 662c 2058 293a 0a20 2020  rm(self, X):.   
+0000ac20: 2020 2020 2022 2222 5363 616c 6520 6261       """Scale ba
+0000ac30: 636b 2074 6865 2064 6174 6120 746f 2074  ck the data to t
+0000ac40: 6865 206f 7269 6769 6e61 6c20 7265 7072  he original repr
+0000ac50: 6573 656e 7461 7469 6f6e 2e0a 0a20 2020  esentation...   
+0000ac60: 2020 2020 2050 6172 616d 6574 6572 730a       Parameters.
+0000ac70: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d2d          --------
+0000ac80: 2d2d 0a20 2020 2020 2020 2058 203a 207b  --.        X : {
+0000ac90: 6172 7261 792d 6c69 6b65 2c20 7370 6172  array-like, spar
+0000aca0: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
+0000acb0: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
+0000acc0: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
+0000acd0: 2020 2020 2020 2020 5468 6520 6461 7461          The data
+0000ace0: 2074 6861 7420 7368 6f75 6c64 2062 6520   that should be 
+0000acf0: 7472 616e 7366 6f72 6d65 6420 6261 636b  transformed back
+0000ad00: 2e0a 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
+0000ad10: 6e73 0a20 2020 2020 2020 202d 2d2d 2d2d  ns.        -----
+0000ad20: 2d2d 0a20 2020 2020 2020 2058 5f74 7220  --.        X_tr 
+0000ad30: 3a20 7b6e 6461 7272 6179 2c20 7370 6172  : {ndarray, spar
+0000ad40: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
+0000ad50: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
+0000ad60: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
+0000ad70: 2020 2020 2020 2020 5472 616e 7366 6f72          Transfor
+0000ad80: 6d65 6420 6172 7261 792e 0a20 2020 2020  med array..     
+0000ad90: 2020 2022 2222 0a20 2020 2020 2020 2063     """.        c
+0000ada0: 6865 636b 5f69 735f 6669 7474 6564 2873  heck_is_fitted(s
+0000adb0: 656c 6629 0a0a 2020 2020 2020 2020 7870  elf)..        xp
+0000adc0: 2c20 5f20 3d20 6765 745f 6e61 6d65 7370  , _ = get_namesp
+0000add0: 6163 6528 5829 0a0a 2020 2020 2020 2020  ace(X)..        
+0000ade0: 5820 3d20 6368 6563 6b5f 6172 7261 7928  X = check_array(
+0000adf0: 0a20 2020 2020 2020 2020 2020 2058 2c0a  .            X,.
+0000ae00: 2020 2020 2020 2020 2020 2020 6163 6365              acce
+0000ae10: 7074 5f73 7061 7273 653d 2822 6373 7222  pt_sparse=("csr"
+0000ae20: 2c20 2263 7363 2229 2c0a 2020 2020 2020  , "csc"),.      
+0000ae30: 2020 2020 2020 636f 7079 3d73 656c 662e        copy=self.
+0000ae40: 636f 7079 2c0a 2020 2020 2020 2020 2020  copy,.          
+0000ae50: 2020 6474 7970 653d 5f61 7272 6179 5f61    dtype=_array_a
+0000ae60: 7069 2e73 7570 706f 7274 6564 5f66 6c6f  pi.supported_flo
+0000ae70: 6174 5f64 7479 7065 7328 7870 292c 0a20  at_dtypes(xp),. 
+0000ae80: 2020 2020 2020 2020 2020 2066 6f72 6365             force
+0000ae90: 5f61 6c6c 5f66 696e 6974 653d 2261 6c6c  _all_finite="all
+0000aea0: 6f77 2d6e 616e 222c 0a20 2020 2020 2020  ow-nan",.       
+0000aeb0: 2029 0a0a 2020 2020 2020 2020 6966 2073   )..        if s
+0000aec0: 7061 7273 652e 6973 7370 6172 7365 2858  parse.issparse(X
+0000aed0: 293a 0a20 2020 2020 2020 2020 2020 2069  ):.            i
+0000aee0: 6e70 6c61 6365 5f63 6f6c 756d 6e5f 7363  nplace_column_sc
+0000aef0: 616c 6528 582c 2073 656c 662e 7363 616c  ale(X, self.scal
+0000af00: 655f 290a 2020 2020 2020 2020 656c 7365  e_).        else
+0000af10: 3a0a 2020 2020 2020 2020 2020 2020 5820  :.            X 
+0000af20: 2a3d 2073 656c 662e 7363 616c 655f 0a20  *= self.scale_. 
+0000af30: 2020 2020 2020 2072 6574 7572 6e20 580a         return X.
+0000af40: 0a20 2020 2064 6566 205f 6d6f 7265 5f74  .    def _more_t
+0000af50: 6167 7328 7365 6c66 293a 0a20 2020 2020  ags(self):.     
+0000af60: 2020 2072 6574 7572 6e20 7b22 616c 6c6f     return {"allo
+0000af70: 775f 6e61 6e22 3a20 5472 7565 7d0a 0a0a  w_nan": True}...
+0000af80: 4076 616c 6964 6174 655f 7061 7261 6d73  @validate_params
+0000af90: 280a 2020 2020 7b0a 2020 2020 2020 2020  (.    {.        
+0000afa0: 2258 223a 205b 2261 7272 6179 2d6c 696b  "X": ["array-lik
+0000afb0: 6522 2c20 2273 7061 7273 6520 6d61 7472  e", "sparse matr
+0000afc0: 6978 225d 2c0a 2020 2020 2020 2020 2261  ix"],.        "a
+0000afd0: 7869 7322 3a20 5b4f 7074 696f 6e73 2849  xis": [Options(I
+0000afe0: 6e74 6567 7261 6c2c 207b 302c 2031 7d29  ntegral, {0, 1})
+0000aff0: 5d2c 0a20 2020 207d 2c0a 2020 2020 7072  ],.    },.    pr
+0000b000: 6566 6572 5f73 6b69 705f 6e65 7374 6564  efer_skip_nested
+0000b010: 5f76 616c 6964 6174 696f 6e3d 4661 6c73  _validation=Fals
+0000b020: 652c 0a29 0a64 6566 206d 6178 6162 735f  e,.).def maxabs_
+0000b030: 7363 616c 6528 582c 202a 2c20 6178 6973  scale(X, *, axis
+0000b040: 3d30 2c20 636f 7079 3d54 7275 6529 3a0a  =0, copy=True):.
+0000b050: 2020 2020 2222 2253 6361 6c65 2065 6163      """Scale eac
+0000b060: 6820 6665 6174 7572 6520 746f 2074 6865  h feature to the
+0000b070: 205b 2d31 2c20 315d 2072 616e 6765 2077   [-1, 1] range w
+0000b080: 6974 686f 7574 2062 7265 616b 696e 6720  ithout breaking 
+0000b090: 7468 6520 7370 6172 7369 7479 2e0a 0a20  the sparsity... 
+0000b0a0: 2020 2054 6869 7320 6573 7469 6d61 746f     This estimato
+0000b0b0: 7220 7363 616c 6573 2065 6163 6820 6665  r scales each fe
+0000b0c0: 6174 7572 6520 696e 6469 7669 6475 616c  ature individual
+0000b0d0: 6c79 2073 7563 680a 2020 2020 7468 6174  ly such.    that
+0000b0e0: 2074 6865 206d 6178 696d 616c 2061 6273   the maximal abs
+0000b0f0: 6f6c 7574 6520 7661 6c75 6520 6f66 2065  olute value of e
+0000b100: 6163 6820 6665 6174 7572 6520 696e 2074  ach feature in t
+0000b110: 6865 0a20 2020 2074 7261 696e 696e 6720  he.    training 
+0000b120: 7365 7420 7769 6c6c 2062 6520 312e 302e  set will be 1.0.
+0000b130: 0a0a 2020 2020 5468 6973 2073 6361 6c65  ..    This scale
+0000b140: 7220 6361 6e20 616c 736f 2062 6520 6170  r can also be ap
+0000b150: 706c 6965 6420 746f 2073 7061 7273 6520  plied to sparse 
+0000b160: 4353 5220 6f72 2043 5343 206d 6174 7269  CSR or CSC matri
+0000b170: 6365 732e 0a0a 2020 2020 5061 7261 6d65  ces...    Parame
+0000b180: 7465 7273 0a20 2020 202d 2d2d 2d2d 2d2d  ters.    -------
+0000b190: 2d2d 2d0a 2020 2020 5820 3a20 7b61 7272  ---.    X : {arr
+0000b1a0: 6179 2d6c 696b 652c 2073 7061 7273 6520  ay-like, sparse 
+0000b1b0: 6d61 7472 6978 7d20 6f66 2073 6861 7065  matrix} of shape
+0000b1c0: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
+0000b1d0: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
+0000b1e0: 2054 6865 2064 6174 612e 0a0a 2020 2020   The data...    
+0000b1f0: 6178 6973 203a 207b 302c 2031 7d2c 2064  axis : {0, 1}, d
+0000b200: 6566 6175 6c74 3d30 0a20 2020 2020 2020  efault=0.       
+0000b210: 2041 7869 7320 7573 6564 2074 6f20 7363   Axis used to sc
+0000b220: 616c 6520 616c 6f6e 672e 2049 6620 302c  ale along. If 0,
+0000b230: 2069 6e64 6570 656e 6465 6e74 6c79 2073   independently s
+0000b240: 6361 6c65 2065 6163 6820 6665 6174 7572  cale each featur
+0000b250: 652c 0a20 2020 2020 2020 206f 7468 6572  e,.        other
+0000b260: 7769 7365 2028 6966 2031 2920 7363 616c  wise (if 1) scal
+0000b270: 6520 6561 6368 2073 616d 706c 652e 0a0a  e each sample...
+0000b280: 2020 2020 636f 7079 203a 2062 6f6f 6c2c      copy : bool,
+0000b290: 2064 6566 6175 6c74 3d54 7275 650a 2020   default=True.  
+0000b2a0: 2020 2020 2020 4966 2046 616c 7365 2c20        If False, 
+0000b2b0: 7472 7920 746f 2061 766f 6964 2061 2063  try to avoid a c
+0000b2c0: 6f70 7920 616e 6420 7363 616c 6520 696e  opy and scale in
+0000b2d0: 2070 6c61 6365 2e0a 2020 2020 2020 2020   place..        
+0000b2e0: 5468 6973 2069 7320 6e6f 7420 6775 6172  This is not guar
+0000b2f0: 616e 7465 6564 2074 6f20 616c 7761 7973  anteed to always
+0000b300: 2077 6f72 6b20 696e 2070 6c61 6365 3b20   work in place; 
+0000b310: 652e 672e 2069 6620 7468 6520 6461 7461  e.g. if the data
+0000b320: 2069 730a 2020 2020 2020 2020 6120 6e75   is.        a nu
+0000b330: 6d70 7920 6172 7261 7920 7769 7468 2061  mpy array with a
+0000b340: 6e20 696e 7420 6474 7970 652c 2061 2063  n int dtype, a c
+0000b350: 6f70 7920 7769 6c6c 2062 6520 7265 7475  opy will be retu
+0000b360: 726e 6564 2065 7665 6e20 7769 7468 0a20  rned even with. 
+0000b370: 2020 2020 2020 2063 6f70 793d 4661 6c73         copy=Fals
+0000b380: 652e 0a0a 2020 2020 5265 7475 726e 730a  e...    Returns.
+0000b390: 2020 2020 2d2d 2d2d 2d2d 2d0a 2020 2020      -------.    
+0000b3a0: 585f 7472 203a 207b 6e64 6172 7261 792c  X_tr : {ndarray,
+0000b3b0: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
+0000b3c0: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
+0000b3d0: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
+0000b3e0: 0a20 2020 2020 2020 2054 6865 2074 7261  .        The tra
+0000b3f0: 6e73 666f 726d 6564 2064 6174 612e 0a0a  nsformed data...
+0000b400: 2020 2020 2e2e 2077 6172 6e69 6e67 3a3a      .. warning::
+0000b410: 2052 6973 6b20 6f66 2064 6174 6120 6c65   Risk of data le
+0000b420: 616b 0a0a 2020 2020 2020 2020 446f 206e  ak..        Do n
+0000b430: 6f74 2075 7365 203a 6675 6e63 3a60 7e73  ot use :func:`~s
+0000b440: 6b6c 6561 726e 2e70 7265 7072 6f63 6573  klearn.preproces
+0000b450: 7369 6e67 2e6d 6178 6162 735f 7363 616c  sing.maxabs_scal
+0000b460: 6560 2075 6e6c 6573 7320 796f 7520 6b6e  e` unless you kn
+0000b470: 6f77 0a20 2020 2020 2020 2077 6861 7420  ow.        what 
+0000b480: 796f 7520 6172 6520 646f 696e 672e 2041  you are doing. A
+0000b490: 2063 6f6d 6d6f 6e20 6d69 7374 616b 6520   common mistake 
+0000b4a0: 6973 2074 6f20 6170 706c 7920 6974 2074  is to apply it t
+0000b4b0: 6f20 7468 6520 656e 7469 7265 2064 6174  o the entire dat
+0000b4c0: 610a 2020 2020 2020 2020 2a62 6566 6f72  a.        *befor
+0000b4d0: 652a 2073 706c 6974 7469 6e67 2069 6e74  e* splitting int
+0000b4e0: 6f20 7472 6169 6e69 6e67 2061 6e64 2074  o training and t
+0000b4f0: 6573 7420 7365 7473 2e20 5468 6973 2077  est sets. This w
+0000b500: 696c 6c20 6269 6173 2074 6865 0a20 2020  ill bias the.   
+0000b510: 2020 2020 206d 6f64 656c 2065 7661 6c75       model evalu
+0000b520: 6174 696f 6e20 6265 6361 7573 6520 696e  ation because in
+0000b530: 666f 726d 6174 696f 6e20 776f 756c 6420  formation would 
+0000b540: 6861 7665 206c 6561 6b65 6420 6672 6f6d  have leaked from
+0000b550: 2074 6865 2074 6573 740a 2020 2020 2020   the test.      
+0000b560: 2020 7365 7420 746f 2074 6865 2074 7261    set to the tra
+0000b570: 696e 696e 6720 7365 742e 0a20 2020 2020  ining set..     
+0000b580: 2020 2049 6e20 6765 6e65 7261 6c2c 2077     In general, w
+0000b590: 6520 7265 636f 6d6d 656e 6420 7573 696e  e recommend usin
+0000b5a0: 670a 2020 2020 2020 2020 3a63 6c61 7373  g.        :class
+0000b5b0: 3a60 7e73 6b6c 6561 726e 2e70 7265 7072  :`~sklearn.prepr
+0000b5c0: 6f63 6573 7369 6e67 2e4d 6178 4162 7353  ocessing.MaxAbsS
+0000b5d0: 6361 6c65 7260 2077 6974 6869 6e20 610a  caler` within a.
+0000b5e0: 2020 2020 2020 2020 3a72 6566 3a60 5069          :ref:`Pi
+0000b5f0: 7065 6c69 6e65 203c 7069 7065 6c69 6e65  peline <pipeline
+0000b600: 3e60 2069 6e20 6f72 6465 7220 746f 2070  >` in order to p
+0000b610: 7265 7665 6e74 206d 6f73 7420 7269 736b  revent most risk
+0000b620: 7320 6f66 2064 6174 610a 2020 2020 2020  s of data.      
+0000b630: 2020 6c65 616b 696e 673a 2060 7069 7065    leaking: `pipe
+0000b640: 203d 206d 616b 655f 7069 7065 6c69 6e65   = make_pipeline
+0000b650: 284d 6178 4162 7353 6361 6c65 7228 292c  (MaxAbsScaler(),
+0000b660: 204c 6f67 6973 7469 6352 6567 7265 7373   LogisticRegress
+0000b670: 696f 6e28 2929 602e 0a0a 2020 2020 5365  ion())`...    Se
+0000b680: 6520 416c 736f 0a20 2020 202d 2d2d 2d2d  e Also.    -----
+0000b690: 2d2d 2d0a 2020 2020 4d61 7841 6273 5363  ---.    MaxAbsSc
+0000b6a0: 616c 6572 203a 2050 6572 666f 726d 7320  aler : Performs 
+0000b6b0: 7363 616c 696e 6720 746f 2074 6865 205b  scaling to the [
+0000b6c0: 2d31 2c20 315d 2072 616e 6765 2075 7369  -1, 1] range usi
+0000b6d0: 6e67 0a20 2020 2020 2020 2074 6865 2054  ng.        the T
+0000b6e0: 7261 6e73 666f 726d 6572 2041 5049 2028  ransformer API (
+0000b6f0: 652e 672e 2061 7320 7061 7274 206f 6620  e.g. as part of 
+0000b700: 6120 7072 6570 726f 6365 7373 696e 670a  a preprocessing.
+0000b710: 2020 2020 2020 2020 3a63 6c61 7373 3a60          :class:`
+0000b720: 7e73 6b6c 6561 726e 2e70 6970 656c 696e  ~sklearn.pipelin
+0000b730: 652e 5069 7065 6c69 6e65 6029 2e0a 0a20  e.Pipeline`)... 
+0000b740: 2020 204e 6f74 6573 0a20 2020 202d 2d2d     Notes.    ---
+0000b750: 2d2d 0a20 2020 204e 614e 7320 6172 6520  --.    NaNs are 
+0000b760: 7472 6561 7465 6420 6173 206d 6973 7369  treated as missi
+0000b770: 6e67 2076 616c 7565 733a 2064 6973 7265  ng values: disre
+0000b780: 6761 7264 6564 2074 6f20 636f 6d70 7574  garded to comput
+0000b790: 6520 7468 6520 7374 6174 6973 7469 6373  e the statistics
+0000b7a0: 2c0a 2020 2020 616e 6420 6d61 696e 7461  ,.    and mainta
+0000b7b0: 696e 6564 2064 7572 696e 6720 7468 6520  ined during the 
+0000b7c0: 6461 7461 2074 7261 6e73 666f 726d 6174  data transformat
+0000b7d0: 696f 6e2e 0a0a 2020 2020 466f 7220 6120  ion...    For a 
+0000b7e0: 636f 6d70 6172 6973 6f6e 206f 6620 7468  comparison of th
+0000b7f0: 6520 6469 6666 6572 656e 7420 7363 616c  e different scal
+0000b800: 6572 732c 2074 7261 6e73 666f 726d 6572  ers, transformer
+0000b810: 732c 2061 6e64 206e 6f72 6d61 6c69 7a65  s, and normalize
+0000b820: 7273 2c0a 2020 2020 7365 653a 203a 7265  rs,.    see: :re
+0000b830: 663a 6073 7068 785f 676c 725f 6175 746f  f:`sphx_glr_auto
+0000b840: 5f65 7861 6d70 6c65 735f 7072 6570 726f  _examples_prepro
+0000b850: 6365 7373 696e 675f 706c 6f74 5f61 6c6c  cessing_plot_all
+0000b860: 5f73 6361 6c69 6e67 2e70 7960 2e0a 0a20  _scaling.py`... 
+0000b870: 2020 2045 7861 6d70 6c65 730a 2020 2020     Examples.    
+0000b880: 2d2d 2d2d 2d2d 2d2d 0a20 2020 203e 3e3e  --------.    >>>
+0000b890: 2066 726f 6d20 736b 6c65 6172 6e2e 7072   from sklearn.pr
+0000b8a0: 6570 726f 6365 7373 696e 6720 696d 706f  eprocessing impo
+0000b8b0: 7274 206d 6178 6162 735f 7363 616c 650a  rt maxabs_scale.
+0000b8c0: 2020 2020 3e3e 3e20 5820 3d20 5b5b 2d32      >>> X = [[-2
+0000b8d0: 2c20 312c 2032 5d2c 205b 2d31 2c20 302c  , 1, 2], [-1, 0,
+0000b8e0: 2031 5d5d 0a20 2020 203e 3e3e 206d 6178   1]].    >>> max
+0000b8f0: 6162 735f 7363 616c 6528 582c 2061 7869  abs_scale(X, axi
+0000b900: 733d 3029 2020 2320 7363 616c 6520 6561  s=0)  # scale ea
+0000b910: 6368 2063 6f6c 756d 6e20 696e 6465 7065  ch column indepe
+0000b920: 6e64 656e 746c 790a 2020 2020 6172 7261  ndently.    arra
+0000b930: 7928 5b5b 2d31 2e20 2c20 2031 2e20 2c20  y([[-1. ,  1. , 
+0000b940: 2031 2e20 5d2c 0a20 2020 2020 2020 2020   1. ],.         
+0000b950: 2020 5b2d 302e 352c 2020 302e 202c 2020    [-0.5,  0. ,  
+0000b960: 302e 355d 5d29 0a20 2020 203e 3e3e 206d  0.5]]).    >>> m
+0000b970: 6178 6162 735f 7363 616c 6528 582c 2061  axabs_scale(X, a
+0000b980: 7869 733d 3129 2020 2320 7363 616c 6520  xis=1)  # scale 
+0000b990: 6561 6368 2072 6f77 2069 6e64 6570 656e  each row indepen
+0000b9a0: 6465 6e74 6c79 0a20 2020 2061 7272 6179  dently.    array
+0000b9b0: 285b 5b2d 312e 202c 2020 302e 352c 2020  ([[-1. ,  0.5,  
+0000b9c0: 312e 205d 2c0a 2020 2020 2020 2020 2020  1. ],.          
+0000b9d0: 205b 2d31 2e20 2c20 2030 2e20 2c20 2031   [-1. ,  0. ,  1
+0000b9e0: 2e20 5d5d 290a 2020 2020 2222 220a 2020  . ]]).    """.  
+0000b9f0: 2020 2320 556e 6c69 6b65 2074 6865 2073    # Unlike the s
+0000ba00: 6361 6c65 7220 6f62 6a65 6374 2c20 7468  caler object, th
+0000ba10: 6973 2066 756e 6374 696f 6e20 616c 6c6f  is function allo
+0000ba20: 7773 2031 6420 696e 7075 742e 0a0a 2020  ws 1d input...  
+0000ba30: 2020 2320 4966 2063 6f70 7920 6973 2072    # If copy is r
+0000ba40: 6571 7569 7265 642c 2069 7420 7769 6c6c  equired, it will
+0000ba50: 2062 6520 646f 6e65 2069 6e73 6964 6520   be done inside 
+0000ba60: 7468 6520 7363 616c 6572 206f 626a 6563  the scaler objec
+0000ba70: 742e 0a20 2020 2058 203d 2063 6865 636b  t..    X = check
+0000ba80: 5f61 7272 6179 280a 2020 2020 2020 2020  _array(.        
+0000ba90: 582c 0a20 2020 2020 2020 2061 6363 6570  X,.        accep
+0000baa0: 745f 7370 6172 7365 3d28 2263 7372 222c  t_sparse=("csr",
+0000bab0: 2022 6373 6322 292c 0a20 2020 2020 2020   "csc"),.       
+0000bac0: 2063 6f70 793d 4661 6c73 652c 0a20 2020   copy=False,.   
+0000bad0: 2020 2020 2065 6e73 7572 655f 3264 3d46       ensure_2d=F
+0000bae0: 616c 7365 2c0a 2020 2020 2020 2020 6474  alse,.        dt
+0000baf0: 7970 653d 464c 4f41 545f 4454 5950 4553  ype=FLOAT_DTYPES
+0000bb00: 2c0a 2020 2020 2020 2020 666f 7263 655f  ,.        force_
+0000bb10: 616c 6c5f 6669 6e69 7465 3d22 616c 6c6f  all_finite="allo
+0000bb20: 772d 6e61 6e22 2c0a 2020 2020 290a 2020  w-nan",.    ).  
+0000bb30: 2020 6f72 6967 696e 616c 5f6e 6469 6d20    original_ndim 
+0000bb40: 3d20 582e 6e64 696d 0a0a 2020 2020 6966  = X.ndim..    if
+0000bb50: 206f 7269 6769 6e61 6c5f 6e64 696d 203d   original_ndim =
+0000bb60: 3d20 313a 0a20 2020 2020 2020 2058 203d  = 1:.        X =
+0000bb70: 2058 2e72 6573 6861 7065 2858 2e73 6861   X.reshape(X.sha
+0000bb80: 7065 5b30 5d2c 2031 290a 0a20 2020 2073  pe[0], 1)..    s
+0000bb90: 203d 204d 6178 4162 7353 6361 6c65 7228   = MaxAbsScaler(
+0000bba0: 636f 7079 3d63 6f70 7929 0a20 2020 2069  copy=copy).    i
+0000bbb0: 6620 6178 6973 203d 3d20 303a 0a20 2020  f axis == 0:.   
+0000bbc0: 2020 2020 2058 203d 2073 2e66 6974 5f74       X = s.fit_t
+0000bbd0: 7261 6e73 666f 726d 2858 290a 2020 2020  ransform(X).    
+0000bbe0: 656c 7365 3a0a 2020 2020 2020 2020 5820  else:.        X 
+0000bbf0: 3d20 732e 6669 745f 7472 616e 7366 6f72  = s.fit_transfor
+0000bc00: 6d28 582e 5429 2e54 0a0a 2020 2020 6966  m(X.T).T..    if
+0000bc10: 206f 7269 6769 6e61 6c5f 6e64 696d 203d   original_ndim =
+0000bc20: 3d20 313a 0a20 2020 2020 2020 2058 203d  = 1:.        X =
+0000bc30: 2058 2e72 6176 656c 2829 0a0a 2020 2020   X.ravel()..    
+0000bc40: 7265 7475 726e 2058 0a0a 0a63 6c61 7373  return X...class
+0000bc50: 2052 6f62 7573 7453 6361 6c65 7228 4f6e   RobustScaler(On
+0000bc60: 6554 6f4f 6e65 4665 6174 7572 654d 6978  eToOneFeatureMix
+0000bc70: 696e 2c20 5472 616e 7366 6f72 6d65 724d  in, TransformerM
+0000bc80: 6978 696e 2c20 4261 7365 4573 7469 6d61  ixin, BaseEstima
+0000bc90: 746f 7229 3a0a 2020 2020 2222 2253 6361  tor):.    """Sca
+0000bca0: 6c65 2066 6561 7475 7265 7320 7573 696e  le features usin
+0000bcb0: 6720 7374 6174 6973 7469 6373 2074 6861  g statistics tha
+0000bcc0: 7420 6172 6520 726f 6275 7374 2074 6f20  t are robust to 
+0000bcd0: 6f75 746c 6965 7273 2e0a 0a20 2020 2054  outliers...    T
+0000bce0: 6869 7320 5363 616c 6572 2072 656d 6f76  his Scaler remov
+0000bcf0: 6573 2074 6865 206d 6564 6961 6e20 616e  es the median an
+0000bd00: 6420 7363 616c 6573 2074 6865 2064 6174  d scales the dat
+0000bd10: 6120 6163 636f 7264 696e 6720 746f 0a20  a according to. 
+0000bd20: 2020 2074 6865 2071 7561 6e74 696c 6520     the quantile 
+0000bd30: 7261 6e67 6520 2864 6566 6175 6c74 7320  range (defaults 
+0000bd40: 746f 2049 5152 3a20 496e 7465 7271 7561  to IQR: Interqua
+0000bd50: 7274 696c 6520 5261 6e67 6529 2e0a 2020  rtile Range)..  
+0000bd60: 2020 5468 6520 4951 5220 6973 2074 6865    The IQR is the
+0000bd70: 2072 616e 6765 2062 6574 7765 656e 2074   range between t
+0000bd80: 6865 2031 7374 2071 7561 7274 696c 6520  he 1st quartile 
+0000bd90: 2832 3574 6820 7175 616e 7469 6c65 290a  (25th quantile).
+0000bda0: 2020 2020 616e 6420 7468 6520 3372 6420      and the 3rd 
+0000bdb0: 7175 6172 7469 6c65 2028 3735 7468 2071  quartile (75th q
+0000bdc0: 7561 6e74 696c 6529 2e0a 0a20 2020 2043  uantile)...    C
+0000bdd0: 656e 7465 7269 6e67 2061 6e64 2073 6361  entering and sca
+0000bde0: 6c69 6e67 2068 6170 7065 6e20 696e 6465  ling happen inde
+0000bdf0: 7065 6e64 656e 746c 7920 6f6e 2065 6163  pendently on eac
+0000be00: 6820 6665 6174 7572 6520 6279 0a20 2020  h feature by.   
+0000be10: 2063 6f6d 7075 7469 6e67 2074 6865 2072   computing the r
+0000be20: 656c 6576 616e 7420 7374 6174 6973 7469  elevant statisti
+0000be30: 6373 206f 6e20 7468 6520 7361 6d70 6c65  cs on the sample
+0000be40: 7320 696e 2074 6865 2074 7261 696e 696e  s in the trainin
+0000be50: 670a 2020 2020 7365 742e 204d 6564 6961  g.    set. Media
+0000be60: 6e20 616e 6420 696e 7465 7271 7561 7274  n and interquart
+0000be70: 696c 6520 7261 6e67 6520 6172 6520 7468  ile range are th
+0000be80: 656e 2073 746f 7265 6420 746f 2062 6520  en stored to be 
+0000be90: 7573 6564 206f 6e0a 2020 2020 6c61 7465  used on.    late
+0000bea0: 7220 6461 7461 2075 7369 6e67 2074 6865  r data using the
+0000beb0: 203a 6d65 7468 3a60 7472 616e 7366 6f72   :meth:`transfor
+0000bec0: 6d60 206d 6574 686f 642e 0a0a 2020 2020  m` method...    
+0000bed0: 5374 616e 6461 7264 697a 6174 696f 6e20  Standardization 
+0000bee0: 6f66 2061 2064 6174 6173 6574 2069 7320  of a dataset is 
+0000bef0: 6120 636f 6d6d 6f6e 2070 7265 7072 6f63  a common preproc
+0000bf00: 6573 7369 6e67 2066 6f72 206d 616e 7920  essing for many 
+0000bf10: 6d61 6368 696e 650a 2020 2020 6c65 6172  machine.    lear
+0000bf20: 6e69 6e67 2065 7374 696d 6174 6f72 732e  ning estimators.
+0000bf30: 2054 7970 6963 616c 6c79 2074 6869 7320   Typically this 
+0000bf40: 6973 2064 6f6e 6520 6279 2072 656d 6f76  is done by remov
+0000bf50: 696e 6720 7468 6520 6d65 616e 2061 6e64  ing the mean and
+0000bf60: 0a20 2020 2073 6361 6c69 6e67 2074 6f20  .    scaling to 
+0000bf70: 756e 6974 2076 6172 6961 6e63 652e 2048  unit variance. H
+0000bf80: 6f77 6576 6572 2c20 6f75 746c 6965 7273  owever, outliers
+0000bf90: 2063 616e 206f 6674 656e 2069 6e66 6c75   can often influ
+0000bfa0: 656e 6365 2074 6865 2073 616d 706c 650a  ence the sample.
+0000bfb0: 2020 2020 6d65 616e 202f 2076 6172 6961      mean / varia
+0000bfc0: 6e63 6520 696e 2061 206e 6567 6174 6976  nce in a negativ
+0000bfd0: 6520 7761 792e 2049 6e20 7375 6368 2063  e way. In such c
+0000bfe0: 6173 6573 2c20 7573 696e 6720 7468 6520  ases, using the 
+0000bff0: 6d65 6469 616e 2061 6e64 2074 6865 0a20  median and the. 
+0000c000: 2020 2069 6e74 6572 7175 6172 7469 6c65     interquartile
+0000c010: 2072 616e 6765 206f 6674 656e 2067 6976   range often giv
+0000c020: 6520 6265 7474 6572 2072 6573 756c 7473  e better results
+0000c030: 2e20 466f 7220 616e 2065 7861 6d70 6c65  . For an example
+0000c040: 2076 6973 7561 6c69 7a61 7469 6f6e 0a20   visualization. 
+0000c050: 2020 2061 6e64 2063 6f6d 7061 7269 736f     and compariso
+0000c060: 6e20 746f 206f 7468 6572 2073 6361 6c65  n to other scale
+0000c070: 7273 2c20 7265 6665 7220 746f 203a 7265  rs, refer to :re
+0000c080: 663a 6043 6f6d 7061 7265 2052 6f62 7573  f:`Compare Robus
+0000c090: 7453 6361 6c65 7220 7769 7468 0a20 2020  tScaler with.   
+0000c0a0: 206f 7468 6572 2073 6361 6c65 7273 203c   other scalers <
+0000c0b0: 706c 6f74 5f61 6c6c 5f73 6361 6c69 6e67  plot_all_scaling
+0000c0c0: 5f72 6f62 7573 745f 7363 616c 6572 5f73  _robust_scaler_s
+0000c0d0: 6563 7469 6f6e 3e60 2e0a 0a20 2020 202e  ection>`...    .
+0000c0e0: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
+0000c0f0: 2030 2e31 370a 0a20 2020 2052 6561 6420   0.17..    Read 
+0000c100: 6d6f 7265 2069 6e20 7468 6520 3a72 6566  more in the :ref
+0000c110: 3a60 5573 6572 2047 7569 6465 203c 7072  :`User Guide <pr
+0000c120: 6570 726f 6365 7373 696e 675f 7363 616c  eprocessing_scal
+0000c130: 6572 3e60 2e0a 0a20 2020 2050 6172 616d  er>`...    Param
+0000c140: 6574 6572 730a 2020 2020 2d2d 2d2d 2d2d  eters.    ------
+0000c150: 2d2d 2d2d 0a20 2020 2077 6974 685f 6365  ----.    with_ce
+0000c160: 6e74 6572 696e 6720 3a20 626f 6f6c 2c20  ntering : bool, 
+0000c170: 6465 6661 756c 743d 5472 7565 0a20 2020  default=True.   
+0000c180: 2020 2020 2049 6620 6054 7275 6560 2c20       If `True`, 
+0000c190: 6365 6e74 6572 2074 6865 2064 6174 6120  center the data 
+0000c1a0: 6265 666f 7265 2073 6361 6c69 6e67 2e0a  before scaling..
+0000c1b0: 2020 2020 2020 2020 5468 6973 2077 696c          This wil
+0000c1c0: 6c20 6361 7573 6520 3a6d 6574 683a 6074  l cause :meth:`t
+0000c1d0: 7261 6e73 666f 726d 6020 746f 2072 6169  ransform` to rai
+0000c1e0: 7365 2061 6e20 6578 6365 7074 696f 6e20  se an exception 
+0000c1f0: 7768 656e 2061 7474 656d 7074 6564 0a20  when attempted. 
+0000c200: 2020 2020 2020 206f 6e20 7370 6172 7365         on sparse
+0000c210: 206d 6174 7269 6365 732c 2062 6563 6175   matrices, becau
+0000c220: 7365 2063 656e 7465 7269 6e67 2074 6865  se centering the
+0000c230: 6d20 656e 7461 696c 7320 6275 696c 6469  m entails buildi
+0000c240: 6e67 2061 2064 656e 7365 0a20 2020 2020  ng a dense.     
+0000c250: 2020 206d 6174 7269 7820 7768 6963 6820     matrix which 
+0000c260: 696e 2063 6f6d 6d6f 6e20 7573 6520 6361  in common use ca
+0000c270: 7365 7320 6973 206c 696b 656c 7920 746f  ses is likely to
+0000c280: 2062 6520 746f 6f20 6c61 7267 6520 746f   be too large to
+0000c290: 2066 6974 2069 6e0a 2020 2020 2020 2020   fit in.        
+0000c2a0: 6d65 6d6f 7279 2e0a 0a20 2020 2077 6974  memory...    wit
+0000c2b0: 685f 7363 616c 696e 6720 3a20 626f 6f6c  h_scaling : bool
+0000c2c0: 2c20 6465 6661 756c 743d 5472 7565 0a20  , default=True. 
+0000c2d0: 2020 2020 2020 2049 6620 6054 7275 6560         If `True`
+0000c2e0: 2c20 7363 616c 6520 7468 6520 6461 7461  , scale the data
+0000c2f0: 2074 6f20 696e 7465 7271 7561 7274 696c   to interquartil
+0000c300: 6520 7261 6e67 652e 0a0a 2020 2020 7175  e range...    qu
+0000c310: 616e 7469 6c65 5f72 616e 6765 203a 2074  antile_range : t
+0000c320: 7570 6c65 2028 715f 6d69 6e2c 2071 5f6d  uple (q_min, q_m
+0000c330: 6178 292c 2030 2e30 203c 2071 5f6d 696e  ax), 0.0 < q_min
+0000c340: 203c 2071 5f6d 6178 203c 2031 3030 2e30   < q_max < 100.0
+0000c350: 2c20 5c0a 2020 2020 2020 2020 6465 6661  , \.        defa
+0000c360: 756c 743d 2832 352e 302c 2037 352e 3029  ult=(25.0, 75.0)
+0000c370: 0a20 2020 2020 2020 2051 7561 6e74 696c  .        Quantil
+0000c380: 6520 7261 6e67 6520 7573 6564 2074 6f20  e range used to 
+0000c390: 6361 6c63 756c 6174 6520 6073 6361 6c65  calculate `scale
+0000c3a0: 5f60 2e20 4279 2064 6566 6175 6c74 2074  _`. By default t
+0000c3b0: 6869 7320 6973 2065 7175 616c 2074 6f0a  his is equal to.
+0000c3c0: 2020 2020 2020 2020 7468 6520 4951 522c          the IQR,
+0000c3d0: 2069 2e65 2e2c 2060 715f 6d69 6e60 2069   i.e., `q_min` i
+0000c3e0: 7320 7468 6520 6669 7273 7420 7175 616e  s the first quan
+0000c3f0: 7469 6c65 2061 6e64 2060 715f 6d61 7860  tile and `q_max`
+0000c400: 2069 7320 7468 6520 7468 6972 640a 2020   is the third.  
+0000c410: 2020 2020 2020 7175 616e 7469 6c65 2e0a        quantile..
+0000c420: 0a20 2020 2020 2020 202e 2e20 7665 7273  .        .. vers
+0000c430: 696f 6e61 6464 6564 3a3a 2030 2e31 380a  ionadded:: 0.18.
+0000c440: 0a20 2020 2063 6f70 7920 3a20 626f 6f6c  .    copy : bool
+0000c450: 2c20 6465 6661 756c 743d 5472 7565 0a20  , default=True. 
+0000c460: 2020 2020 2020 2049 6620 6046 616c 7365         If `False
+0000c470: 602c 2074 7279 2074 6f20 6176 6f69 6420  `, try to avoid 
+0000c480: 6120 636f 7079 2061 6e64 2064 6f20 696e  a copy and do in
+0000c490: 706c 6163 6520 7363 616c 696e 6720 696e  place scaling in
+0000c4a0: 7374 6561 642e 0a20 2020 2020 2020 2054  stead..        T
+0000c4b0: 6869 7320 6973 206e 6f74 2067 7561 7261  his is not guara
+0000c4c0: 6e74 6565 6420 746f 2061 6c77 6179 7320  nteed to always 
+0000c4d0: 776f 726b 2069 6e70 6c61 6365 3b20 652e  work inplace; e.
+0000c4e0: 672e 2069 6620 7468 6520 6461 7461 2069  g. if the data i
+0000c4f0: 730a 2020 2020 2020 2020 6e6f 7420 6120  s.        not a 
+0000c500: 4e75 6d50 7920 6172 7261 7920 6f72 2073  NumPy array or s
+0000c510: 6369 7079 2e73 7061 7273 6520 4353 5220  cipy.sparse CSR 
+0000c520: 6d61 7472 6978 2c20 6120 636f 7079 206d  matrix, a copy m
+0000c530: 6179 2073 7469 6c6c 2062 650a 2020 2020  ay still be.    
+0000c540: 2020 2020 7265 7475 726e 6564 2e0a 0a20      returned... 
+0000c550: 2020 2075 6e69 745f 7661 7269 616e 6365     unit_variance
+0000c560: 203a 2062 6f6f 6c2c 2064 6566 6175 6c74   : bool, default
+0000c570: 3d46 616c 7365 0a20 2020 2020 2020 2049  =False.        I
+0000c580: 6620 6054 7275 6560 2c20 7363 616c 6520  f `True`, scale 
+0000c590: 6461 7461 2073 6f20 7468 6174 206e 6f72  data so that nor
+0000c5a0: 6d61 6c6c 7920 6469 7374 7269 6275 7465  mally distribute
+0000c5b0: 6420 6665 6174 7572 6573 2068 6176 6520  d features have 
+0000c5c0: 610a 2020 2020 2020 2020 7661 7269 616e  a.        varian
+0000c5d0: 6365 206f 6620 312e 2049 6e20 6765 6e65  ce of 1. In gene
+0000c5e0: 7261 6c2c 2069 6620 7468 6520 6469 6666  ral, if the diff
+0000c5f0: 6572 656e 6365 2062 6574 7765 656e 2074  erence between t
+0000c600: 6865 2078 2d76 616c 7565 7320 6f66 0a20  he x-values of. 
+0000c610: 2020 2020 2020 2060 715f 6d61 7860 2061         `q_max` a
+0000c620: 6e64 2060 715f 6d69 6e60 2066 6f72 2061  nd `q_min` for a
+0000c630: 2073 7461 6e64 6172 6420 6e6f 726d 616c   standard normal
+0000c640: 2064 6973 7472 6962 7574 696f 6e20 6973   distribution is
+0000c650: 2067 7265 6174 6572 0a20 2020 2020 2020   greater.       
+0000c660: 2074 6861 6e20 312c 2074 6865 2064 6174   than 1, the dat
+0000c670: 6173 6574 2077 696c 6c20 6265 2073 6361  aset will be sca
+0000c680: 6c65 6420 646f 776e 2e20 4966 206c 6573  led down. If les
+0000c690: 7320 7468 616e 2031 2c20 7468 6520 6461  s than 1, the da
+0000c6a0: 7461 7365 740a 2020 2020 2020 2020 7769  taset.        wi
+0000c6b0: 6c6c 2062 6520 7363 616c 6564 2075 702e  ll be scaled up.
+0000c6c0: 0a0a 2020 2020 2020 2020 2e2e 2076 6572  ..        .. ver
+0000c6d0: 7369 6f6e 6164 6465 643a 3a20 302e 3234  sionadded:: 0.24
+0000c6e0: 0a0a 2020 2020 4174 7472 6962 7574 6573  ..    Attributes
+0000c6f0: 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a  .    ----------.
+0000c700: 2020 2020 6365 6e74 6572 5f20 3a20 6172      center_ : ar
+0000c710: 7261 7920 6f66 2066 6c6f 6174 730a 2020  ray of floats.  
+0000c720: 2020 2020 2020 5468 6520 6d65 6469 616e        The median
+0000c730: 2076 616c 7565 2066 6f72 2065 6163 6820   value for each 
+0000c740: 6665 6174 7572 6520 696e 2074 6865 2074  feature in the t
+0000c750: 7261 696e 696e 6720 7365 742e 0a0a 2020  raining set...  
+0000c760: 2020 7363 616c 655f 203a 2061 7272 6179    scale_ : array
+0000c770: 206f 6620 666c 6f61 7473 0a20 2020 2020   of floats.     
+0000c780: 2020 2054 6865 2028 7363 616c 6564 2920     The (scaled) 
+0000c790: 696e 7465 7271 7561 7274 696c 6520 7261  interquartile ra
+0000c7a0: 6e67 6520 666f 7220 6561 6368 2066 6561  nge for each fea
+0000c7b0: 7475 7265 2069 6e20 7468 6520 7472 6169  ture in the trai
+0000c7c0: 6e69 6e67 2073 6574 2e0a 0a20 2020 2020  ning set...     
+0000c7d0: 2020 202e 2e20 7665 7273 696f 6e61 6464     .. versionadd
+0000c7e0: 6564 3a3a 2030 2e31 370a 2020 2020 2020  ed:: 0.17.      
+0000c7f0: 2020 2020 202a 7363 616c 655f 2a20 6174       *scale_* at
+0000c800: 7472 6962 7574 652e 0a0a 2020 2020 6e5f  tribute...    n_
+0000c810: 6665 6174 7572 6573 5f69 6e5f 203a 2069  features_in_ : i
+0000c820: 6e74 0a20 2020 2020 2020 204e 756d 6265  nt.        Numbe
+0000c830: 7220 6f66 2066 6561 7475 7265 7320 7365  r of features se
+0000c840: 656e 2064 7572 696e 6720 3a74 6572 6d3a  en during :term:
+0000c850: 6066 6974 602e 0a0a 2020 2020 2020 2020  `fit`...        
+0000c860: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
+0000c870: 3a20 302e 3234 0a0a 2020 2020 6665 6174  : 0.24..    feat
+0000c880: 7572 655f 6e61 6d65 735f 696e 5f20 3a20  ure_names_in_ : 
+0000c890: 6e64 6172 7261 7920 6f66 2073 6861 7065  ndarray of shape
+0000c8a0: 2028 606e 5f66 6561 7475 7265 735f 696e   (`n_features_in
+0000c8b0: 5f60 2c29 0a20 2020 2020 2020 204e 616d  _`,).        Nam
+0000c8c0: 6573 206f 6620 6665 6174 7572 6573 2073  es of features s
+0000c8d0: 6565 6e20 6475 7269 6e67 203a 7465 726d  een during :term
+0000c8e0: 3a60 6669 7460 2e20 4465 6669 6e65 6420  :`fit`. Defined 
+0000c8f0: 6f6e 6c79 2077 6865 6e20 6058 600a 2020  only when `X`.  
+0000c900: 2020 2020 2020 6861 7320 6665 6174 7572        has featur
+0000c910: 6520 6e61 6d65 7320 7468 6174 2061 7265  e names that are
+0000c920: 2061 6c6c 2073 7472 696e 6773 2e0a 0a20   all strings... 
+0000c930: 2020 2020 2020 202e 2e20 7665 7273 696f         .. versio
+0000c940: 6e61 6464 6564 3a3a 2031 2e30 0a0a 2020  nadded:: 1.0..  
+0000c950: 2020 5365 6520 416c 736f 0a20 2020 202d    See Also.    -
+0000c960: 2d2d 2d2d 2d2d 2d0a 2020 2020 726f 6275  -------.    robu
+0000c970: 7374 5f73 6361 6c65 203a 2045 7175 6976  st_scale : Equiv
+0000c980: 616c 656e 7420 6675 6e63 7469 6f6e 2077  alent function w
+0000c990: 6974 686f 7574 2074 6865 2065 7374 696d  ithout the estim
+0000c9a0: 6174 6f72 2041 5049 2e0a 2020 2020 736b  ator API..    sk
+0000c9b0: 6c65 6172 6e2e 6465 636f 6d70 6f73 6974  learn.decomposit
+0000c9c0: 696f 6e2e 5043 4120 3a20 4675 7274 6865  ion.PCA : Furthe
+0000c9d0: 7220 7265 6d6f 7665 7320 7468 6520 6c69  r removes the li
+0000c9e0: 6e65 6172 2063 6f72 7265 6c61 7469 6f6e  near correlation
+0000c9f0: 2061 6372 6f73 730a 2020 2020 2020 2020   across.        
+0000ca00: 6665 6174 7572 6573 2077 6974 6820 2777  features with 'w
+0000ca10: 6869 7465 6e3d 5472 7565 272e 0a0a 2020  hiten=True'...  
+0000ca20: 2020 4e6f 7465 730a 2020 2020 2d2d 2d2d    Notes.    ----
+0000ca30: 2d0a 0a20 2020 2068 7474 7073 3a2f 2f65  -..    https://e
+0000ca40: 6e2e 7769 6b69 7065 6469 612e 6f72 672f  n.wikipedia.org/
+0000ca50: 7769 6b69 2f4d 6564 6961 6e0a 2020 2020  wiki/Median.    
+0000ca60: 6874 7470 733a 2f2f 656e 2e77 696b 6970  https://en.wikip
+0000ca70: 6564 6961 2e6f 7267 2f77 696b 692f 496e  edia.org/wiki/In
+0000ca80: 7465 7271 7561 7274 696c 655f 7261 6e67  terquartile_rang
+0000ca90: 650a 0a20 2020 2045 7861 6d70 6c65 730a  e..    Examples.
+0000caa0: 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020      --------.   
+0000cab0: 203e 3e3e 2066 726f 6d20 736b 6c65 6172   >>> from sklear
+0000cac0: 6e2e 7072 6570 726f 6365 7373 696e 6720  n.preprocessing 
+0000cad0: 696d 706f 7274 2052 6f62 7573 7453 6361  import RobustSca
+0000cae0: 6c65 720a 2020 2020 3e3e 3e20 5820 3d20  ler.    >>> X = 
+0000caf0: 5b5b 2031 2e2c 202d 322e 2c20 2032 2e5d  [[ 1., -2.,  2.]
 0000cb00: 2c0a 2020 2020 2e2e 2e20 2020 2020 205b  ,.    ...      [
-0000cb10: 2034 2e2c 2020 312e 2c20 2d32 2e5d 5d0a   4.,  1., -2.]].
-0000cb20: 2020 2020 3e3e 3e20 7472 616e 7366 6f72      >>> transfor
-0000cb30: 6d65 7220 3d20 526f 6275 7374 5363 616c  mer = RobustScal
-0000cb40: 6572 2829 2e66 6974 2858 290a 2020 2020  er().fit(X).    
-0000cb50: 3e3e 3e20 7472 616e 7366 6f72 6d65 720a  >>> transformer.
-0000cb60: 2020 2020 526f 6275 7374 5363 616c 6572      RobustScaler
-0000cb70: 2829 0a20 2020 203e 3e3e 2074 7261 6e73  ().    >>> trans
-0000cb80: 666f 726d 6572 2e74 7261 6e73 666f 726d  former.transform
-0000cb90: 2858 290a 2020 2020 6172 7261 7928 5b5b  (X).    array([[
-0000cba0: 2030 2e20 2c20 2d32 2e20 2c20 2030 2e20   0. , -2. ,  0. 
-0000cbb0: 5d2c 0a20 2020 2020 2020 2020 2020 5b2d  ],.           [-
-0000cbc0: 312e 202c 2020 302e 202c 2020 302e 345d  1. ,  0. ,  0.4]
-0000cbd0: 2c0a 2020 2020 2020 2020 2020 205b 2031  ,.           [ 1
-0000cbe0: 2e20 2c20 2030 2e20 2c20 2d31 2e36 5d5d  . ,  0. , -1.6]]
-0000cbf0: 290a 2020 2020 2222 220a 0a20 2020 205f  ).    """..    _
-0000cc00: 7061 7261 6d65 7465 725f 636f 6e73 7472  parameter_constr
-0000cc10: 6169 6e74 733a 2064 6963 7420 3d20 7b0a  aints: dict = {.
-0000cc20: 2020 2020 2020 2020 2277 6974 685f 6365          "with_ce
-0000cc30: 6e74 6572 696e 6722 3a20 5b22 626f 6f6c  ntering": ["bool
-0000cc40: 6561 6e22 5d2c 0a20 2020 2020 2020 2022  ean"],.        "
-0000cc50: 7769 7468 5f73 6361 6c69 6e67 223a 205b  with_scaling": [
-0000cc60: 2262 6f6f 6c65 616e 225d 2c0a 2020 2020  "boolean"],.    
-0000cc70: 2020 2020 2271 7561 6e74 696c 655f 7261      "quantile_ra
-0000cc80: 6e67 6522 3a20 5b74 7570 6c65 5d2c 0a20  nge": [tuple],. 
-0000cc90: 2020 2020 2020 2022 636f 7079 223a 205b         "copy": [
-0000cca0: 2262 6f6f 6c65 616e 225d 2c0a 2020 2020  "boolean"],.    
-0000ccb0: 2020 2020 2275 6e69 745f 7661 7269 616e      "unit_varian
-0000ccc0: 6365 223a 205b 2262 6f6f 6c65 616e 225d  ce": ["boolean"]
-0000ccd0: 2c0a 2020 2020 7d0a 0a20 2020 2064 6566  ,.    }..    def
-0000cce0: 205f 5f69 6e69 745f 5f28 0a20 2020 2020   __init__(.     
-0000ccf0: 2020 2073 656c 662c 0a20 2020 2020 2020     self,.       
-0000cd00: 202a 2c0a 2020 2020 2020 2020 7769 7468   *,.        with
-0000cd10: 5f63 656e 7465 7269 6e67 3d54 7275 652c  _centering=True,
-0000cd20: 0a20 2020 2020 2020 2077 6974 685f 7363  .        with_sc
-0000cd30: 616c 696e 673d 5472 7565 2c0a 2020 2020  aling=True,.    
-0000cd40: 2020 2020 7175 616e 7469 6c65 5f72 616e      quantile_ran
-0000cd50: 6765 3d28 3235 2e30 2c20 3735 2e30 292c  ge=(25.0, 75.0),
-0000cd60: 0a20 2020 2020 2020 2063 6f70 793d 5472  .        copy=Tr
-0000cd70: 7565 2c0a 2020 2020 2020 2020 756e 6974  ue,.        unit
-0000cd80: 5f76 6172 6961 6e63 653d 4661 6c73 652c  _variance=False,
-0000cd90: 0a20 2020 2029 3a0a 2020 2020 2020 2020  .    ):.        
-0000cda0: 7365 6c66 2e77 6974 685f 6365 6e74 6572  self.with_center
-0000cdb0: 696e 6720 3d20 7769 7468 5f63 656e 7465  ing = with_cente
-0000cdc0: 7269 6e67 0a20 2020 2020 2020 2073 656c  ring.        sel
-0000cdd0: 662e 7769 7468 5f73 6361 6c69 6e67 203d  f.with_scaling =
-0000cde0: 2077 6974 685f 7363 616c 696e 670a 2020   with_scaling.  
-0000cdf0: 2020 2020 2020 7365 6c66 2e71 7561 6e74        self.quant
-0000ce00: 696c 655f 7261 6e67 6520 3d20 7175 616e  ile_range = quan
-0000ce10: 7469 6c65 5f72 616e 6765 0a20 2020 2020  tile_range.     
-0000ce20: 2020 2073 656c 662e 756e 6974 5f76 6172     self.unit_var
-0000ce30: 6961 6e63 6520 3d20 756e 6974 5f76 6172  iance = unit_var
-0000ce40: 6961 6e63 650a 2020 2020 2020 2020 7365  iance.        se
-0000ce50: 6c66 2e63 6f70 7920 3d20 636f 7079 0a0a  lf.copy = copy..
-0000ce60: 2020 2020 405f 6669 745f 636f 6e74 6578      @_fit_contex
-0000ce70: 7428 7072 6566 6572 5f73 6b69 705f 6e65  t(prefer_skip_ne
-0000ce80: 7374 6564 5f76 616c 6964 6174 696f 6e3d  sted_validation=
-0000ce90: 5472 7565 290a 2020 2020 6465 6620 6669  True).    def fi
-0000cea0: 7428 7365 6c66 2c20 582c 2079 3d4e 6f6e  t(self, X, y=Non
-0000ceb0: 6529 3a0a 2020 2020 2020 2020 2222 2243  e):.        """C
-0000cec0: 6f6d 7075 7465 2074 6865 206d 6564 6961  ompute the media
-0000ced0: 6e20 616e 6420 7175 616e 7469 6c65 7320  n and quantiles 
-0000cee0: 746f 2062 6520 7573 6564 2066 6f72 2073  to be used for s
-0000cef0: 6361 6c69 6e67 2e0a 0a20 2020 2020 2020  caling...       
-0000cf00: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
-0000cf10: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
-0000cf20: 2020 2020 2020 2058 203a 207b 6172 7261         X : {arra
-0000cf30: 792d 6c69 6b65 2c20 7370 6172 7365 206d  y-like, sparse m
-0000cf40: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
-0000cf50: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
-0000cf60: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-0000cf70: 2020 2020 5468 6520 6461 7461 2075 7365      The data use
-0000cf80: 6420 746f 2063 6f6d 7075 7465 2074 6865  d to compute the
-0000cf90: 206d 6564 6961 6e20 616e 6420 7175 616e   median and quan
-0000cfa0: 7469 6c65 730a 2020 2020 2020 2020 2020  tiles.          
-0000cfb0: 2020 7573 6564 2066 6f72 206c 6174 6572    used for later
-0000cfc0: 2073 6361 6c69 6e67 2061 6c6f 6e67 2074   scaling along t
-0000cfd0: 6865 2066 6561 7475 7265 7320 6178 6973  he features axis
-0000cfe0: 2e0a 0a20 2020 2020 2020 2079 203a 2049  ...        y : I
-0000cff0: 676e 6f72 6564 0a20 2020 2020 2020 2020  gnored.         
-0000d000: 2020 204e 6f74 2075 7365 642c 2070 7265     Not used, pre
-0000d010: 7365 6e74 2068 6572 6520 666f 7220 4150  sent here for AP
-0000d020: 4920 636f 6e73 6973 7465 6e63 7920 6279  I consistency by
-0000d030: 2063 6f6e 7665 6e74 696f 6e2e 0a0a 2020   convention...  
-0000d040: 2020 2020 2020 5265 7475 726e 730a 2020        Returns.  
-0000d050: 2020 2020 2020 2d2d 2d2d 2d2d 2d0a 2020        -------.  
-0000d060: 2020 2020 2020 7365 6c66 203a 206f 626a        self : obj
-0000d070: 6563 740a 2020 2020 2020 2020 2020 2020  ect.            
-0000d080: 4669 7474 6564 2073 6361 6c65 722e 0a20  Fitted scaler.. 
-0000d090: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-0000d0a0: 2020 2023 2061 7420 6669 742c 2063 6f6e     # at fit, con
-0000d0b0: 7665 7274 2073 7061 7273 6520 6d61 7472  vert sparse matr
-0000d0c0: 6963 6573 2074 6f20 6373 6320 666f 7220  ices to csc for 
-0000d0d0: 6f70 7469 6d69 7a65 6420 636f 6d70 7574  optimized comput
-0000d0e0: 6174 696f 6e20 6f66 0a20 2020 2020 2020  ation of.       
-0000d0f0: 2023 2074 6865 2071 7561 6e74 696c 6573   # the quantiles
-0000d100: 0a20 2020 2020 2020 2058 203d 2073 656c  .        X = sel
-0000d110: 662e 5f76 616c 6964 6174 655f 6461 7461  f._validate_data
-0000d120: 280a 2020 2020 2020 2020 2020 2020 582c  (.            X,
-0000d130: 0a20 2020 2020 2020 2020 2020 2061 6363  .            acc
-0000d140: 6570 745f 7370 6172 7365 3d22 6373 6322  ept_sparse="csc"
-0000d150: 2c0a 2020 2020 2020 2020 2020 2020 6474  ,.            dt
-0000d160: 7970 653d 464c 4f41 545f 4454 5950 4553  ype=FLOAT_DTYPES
-0000d170: 2c0a 2020 2020 2020 2020 2020 2020 666f  ,.            fo
-0000d180: 7263 655f 616c 6c5f 6669 6e69 7465 3d22  rce_all_finite="
-0000d190: 616c 6c6f 772d 6e61 6e22 2c0a 2020 2020  allow-nan",.    
-0000d1a0: 2020 2020 290a 0a20 2020 2020 2020 2071      )..        q
-0000d1b0: 5f6d 696e 2c20 715f 6d61 7820 3d20 7365  _min, q_max = se
-0000d1c0: 6c66 2e71 7561 6e74 696c 655f 7261 6e67  lf.quantile_rang
-0000d1d0: 650a 2020 2020 2020 2020 6966 206e 6f74  e.        if not
-0000d1e0: 2030 203c 3d20 715f 6d69 6e20 3c3d 2071   0 <= q_min <= q
-0000d1f0: 5f6d 6178 203c 3d20 3130 303a 0a20 2020  _max <= 100:.   
-0000d200: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
-0000d210: 616c 7565 4572 726f 7228 2249 6e76 616c  alueError("Inval
-0000d220: 6964 2071 7561 6e74 696c 6520 7261 6e67  id quantile rang
-0000d230: 653a 2025 7322 2025 2073 7472 2873 656c  e: %s" % str(sel
-0000d240: 662e 7175 616e 7469 6c65 5f72 616e 6765  f.quantile_range
-0000d250: 2929 0a0a 2020 2020 2020 2020 6966 2073  ))..        if s
-0000d260: 656c 662e 7769 7468 5f63 656e 7465 7269  elf.with_centeri
-0000d270: 6e67 3a0a 2020 2020 2020 2020 2020 2020  ng:.            
-0000d280: 6966 2073 7061 7273 652e 6973 7370 6172  if sparse.isspar
-0000d290: 7365 2858 293a 0a20 2020 2020 2020 2020  se(X):.         
-0000d2a0: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
-0000d2b0: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-0000d2c0: 2020 2020 2020 2020 2020 2020 2022 4361               "Ca
-0000d2d0: 6e6e 6f74 2063 656e 7465 7220 7370 6172  nnot center spar
-0000d2e0: 7365 206d 6174 7269 6365 733a 2075 7365  se matrices: use
-0000d2f0: 2060 7769 7468 5f63 656e 7465 7269 6e67   `with_centering
-0000d300: 3d46 616c 7365 6022 0a20 2020 2020 2020  =False`".       
-0000d310: 2020 2020 2020 2020 2020 2020 2022 2069               " i
-0000d320: 6e73 7465 6164 2e20 5365 6520 646f 6373  nstead. See docs
-0000d330: 7472 696e 6720 666f 7220 6d6f 7469 7661  tring for motiva
-0000d340: 7469 6f6e 2061 6e64 2061 6c74 6572 6e61  tion and alterna
-0000d350: 7469 7665 732e 220a 2020 2020 2020 2020  tives.".        
-0000d360: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-0000d370: 2020 2020 2020 7365 6c66 2e63 656e 7465        self.cente
-0000d380: 725f 203d 206e 702e 6e61 6e6d 6564 6961  r_ = np.nanmedia
-0000d390: 6e28 582c 2061 7869 733d 3029 0a20 2020  n(X, axis=0).   
-0000d3a0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-0000d3b0: 2020 2020 2020 2073 656c 662e 6365 6e74         self.cent
-0000d3c0: 6572 5f20 3d20 4e6f 6e65 0a0a 2020 2020  er_ = None..    
-0000d3d0: 2020 2020 6966 2073 656c 662e 7769 7468      if self.with
-0000d3e0: 5f73 6361 6c69 6e67 3a0a 2020 2020 2020  _scaling:.      
-0000d3f0: 2020 2020 2020 7175 616e 7469 6c65 7320        quantiles 
-0000d400: 3d20 5b5d 0a20 2020 2020 2020 2020 2020  = [].           
-0000d410: 2066 6f72 2066 6561 7475 7265 5f69 6478   for feature_idx
-0000d420: 2069 6e20 7261 6e67 6528 582e 7368 6170   in range(X.shap
-0000d430: 655b 315d 293a 0a20 2020 2020 2020 2020  e[1]):.         
-0000d440: 2020 2020 2020 2069 6620 7370 6172 7365         if sparse
-0000d450: 2e69 7373 7061 7273 6528 5829 3a0a 2020  .issparse(X):.  
-0000d460: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d470: 2020 636f 6c75 6d6e 5f6e 6e7a 5f64 6174    column_nnz_dat
-0000d480: 6120 3d20 582e 6461 7461 5b0a 2020 2020  a = X.data[.    
-0000d490: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d4a0: 2020 2020 582e 696e 6470 7472 5b66 6561      X.indptr[fea
-0000d4b0: 7475 7265 5f69 6478 5d20 3a20 582e 696e  ture_idx] : X.in
-0000d4c0: 6470 7472 5b66 6561 7475 7265 5f69 6478  dptr[feature_idx
-0000d4d0: 202b 2031 5d0a 2020 2020 2020 2020 2020   + 1].          
-0000d4e0: 2020 2020 2020 2020 2020 5d0a 2020 2020            ].    
-0000d4f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000d500: 636f 6c75 6d6e 5f64 6174 6120 3d20 6e70  column_data = np
-0000d510: 2e7a 6572 6f73 2873 6861 7065 3d58 2e73  .zeros(shape=X.s
-0000d520: 6861 7065 5b30 5d2c 2064 7479 7065 3d58  hape[0], dtype=X
-0000d530: 2e64 7479 7065 290a 2020 2020 2020 2020  .dtype).        
-0000d540: 2020 2020 2020 2020 2020 2020 636f 6c75              colu
-0000d550: 6d6e 5f64 6174 615b 3a20 6c65 6e28 636f  mn_data[: len(co
-0000d560: 6c75 6d6e 5f6e 6e7a 5f64 6174 6129 5d20  lumn_nnz_data)] 
-0000d570: 3d20 636f 6c75 6d6e 5f6e 6e7a 5f64 6174  = column_nnz_dat
-0000d580: 610a 2020 2020 2020 2020 2020 2020 2020  a.              
-0000d590: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-0000d5a0: 2020 2020 2020 2020 2020 2020 636f 6c75              colu
-0000d5b0: 6d6e 5f64 6174 6120 3d20 585b 3a2c 2066  mn_data = X[:, f
-0000d5c0: 6561 7475 7265 5f69 6478 5d0a 0a20 2020  eature_idx]..   
-0000d5d0: 2020 2020 2020 2020 2020 2020 2071 7561               qua
-0000d5e0: 6e74 696c 6573 2e61 7070 656e 6428 6e70  ntiles.append(np
-0000d5f0: 2e6e 616e 7065 7263 656e 7469 6c65 2863  .nanpercentile(c
-0000d600: 6f6c 756d 6e5f 6461 7461 2c20 7365 6c66  olumn_data, self
-0000d610: 2e71 7561 6e74 696c 655f 7261 6e67 6529  .quantile_range)
-0000d620: 290a 0a20 2020 2020 2020 2020 2020 2071  )..            q
-0000d630: 7561 6e74 696c 6573 203d 206e 702e 7472  uantiles = np.tr
-0000d640: 616e 7370 6f73 6528 7175 616e 7469 6c65  anspose(quantile
-0000d650: 7329 0a0a 2020 2020 2020 2020 2020 2020  s)..            
-0000d660: 7365 6c66 2e73 6361 6c65 5f20 3d20 7175  self.scale_ = qu
-0000d670: 616e 7469 6c65 735b 315d 202d 2071 7561  antiles[1] - qua
-0000d680: 6e74 696c 6573 5b30 5d0a 2020 2020 2020  ntiles[0].      
-0000d690: 2020 2020 2020 7365 6c66 2e73 6361 6c65        self.scale
-0000d6a0: 5f20 3d20 5f68 616e 646c 655f 7a65 726f  _ = _handle_zero
-0000d6b0: 735f 696e 5f73 6361 6c65 2873 656c 662e  s_in_scale(self.
-0000d6c0: 7363 616c 655f 2c20 636f 7079 3d46 616c  scale_, copy=Fal
-0000d6d0: 7365 290a 2020 2020 2020 2020 2020 2020  se).            
-0000d6e0: 6966 2073 656c 662e 756e 6974 5f76 6172  if self.unit_var
-0000d6f0: 6961 6e63 653a 0a20 2020 2020 2020 2020  iance:.         
-0000d700: 2020 2020 2020 2061 646a 7573 7420 3d20         adjust = 
-0000d710: 7374 6174 732e 6e6f 726d 2e70 7066 2871  stats.norm.ppf(q
-0000d720: 5f6d 6178 202f 2031 3030 2e30 2920 2d20  _max / 100.0) - 
-0000d730: 7374 6174 732e 6e6f 726d 2e70 7066 2871  stats.norm.ppf(q
-0000d740: 5f6d 696e 202f 2031 3030 2e30 290a 2020  _min / 100.0).  
-0000d750: 2020 2020 2020 2020 2020 2020 2020 7365                se
-0000d760: 6c66 2e73 6361 6c65 5f20 3d20 7365 6c66  lf.scale_ = self
-0000d770: 2e73 6361 6c65 5f20 2f20 6164 6a75 7374  .scale_ / adjust
-0000d780: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-0000d790: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-0000d7a0: 7363 616c 655f 203d 204e 6f6e 650a 0a20  scale_ = None.. 
-0000d7b0: 2020 2020 2020 2072 6574 7572 6e20 7365         return se
-0000d7c0: 6c66 0a0a 2020 2020 6465 6620 7472 616e  lf..    def tran
-0000d7d0: 7366 6f72 6d28 7365 6c66 2c20 5829 3a0a  sform(self, X):.
-0000d7e0: 2020 2020 2020 2020 2222 2243 656e 7465          """Cente
-0000d7f0: 7220 616e 6420 7363 616c 6520 7468 6520  r and scale the 
-0000d800: 6461 7461 2e0a 0a20 2020 2020 2020 2050  data...        P
-0000d810: 6172 616d 6574 6572 730a 2020 2020 2020  arameters.      
-0000d820: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020    ----------.   
-0000d830: 2020 2020 2058 203a 207b 6172 7261 792d       X : {array-
-0000d840: 6c69 6b65 2c20 7370 6172 7365 206d 6174  like, sparse mat
-0000d850: 7269 787d 206f 6620 7368 6170 6520 286e  rix} of shape (n
-0000d860: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
-0000d870: 7572 6573 290a 2020 2020 2020 2020 2020  ures).          
-0000d880: 2020 5468 6520 6461 7461 2075 7365 6420    The data used 
-0000d890: 746f 2073 6361 6c65 2061 6c6f 6e67 2074  to scale along t
-0000d8a0: 6865 2073 7065 6369 6669 6564 2061 7869  he specified axi
-0000d8b0: 732e 0a0a 2020 2020 2020 2020 5265 7475  s...        Retu
-0000d8c0: 726e 730a 2020 2020 2020 2020 2d2d 2d2d  rns.        ----
-0000d8d0: 2d2d 2d0a 2020 2020 2020 2020 585f 7472  ---.        X_tr
-0000d8e0: 203a 207b 6e64 6172 7261 792c 2073 7061   : {ndarray, spa
-0000d8f0: 7273 6520 6d61 7472 6978 7d20 6f66 2073  rse matrix} of s
-0000d900: 6861 7065 2028 6e5f 7361 6d70 6c65 732c  hape (n_samples,
-0000d910: 206e 5f66 6561 7475 7265 7329 0a20 2020   n_features).   
-0000d920: 2020 2020 2020 2020 2054 7261 6e73 666f           Transfo
-0000d930: 726d 6564 2061 7272 6179 2e0a 2020 2020  rmed array..    
-0000d940: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-0000d950: 6368 6563 6b5f 6973 5f66 6974 7465 6428  check_is_fitted(
-0000d960: 7365 6c66 290a 2020 2020 2020 2020 5820  self).        X 
-0000d970: 3d20 7365 6c66 2e5f 7661 6c69 6461 7465  = self._validate
-0000d980: 5f64 6174 6128 0a20 2020 2020 2020 2020  _data(.         
-0000d990: 2020 2058 2c0a 2020 2020 2020 2020 2020     X,.          
-0000d9a0: 2020 6163 6365 7074 5f73 7061 7273 653d    accept_sparse=
-0000d9b0: 2822 6373 7222 2c20 2263 7363 2229 2c0a  ("csr", "csc"),.
-0000d9c0: 2020 2020 2020 2020 2020 2020 636f 7079              copy
-0000d9d0: 3d73 656c 662e 636f 7079 2c0a 2020 2020  =self.copy,.    
-0000d9e0: 2020 2020 2020 2020 6474 7970 653d 464c          dtype=FL
-0000d9f0: 4f41 545f 4454 5950 4553 2c0a 2020 2020  OAT_DTYPES,.    
-0000da00: 2020 2020 2020 2020 7265 7365 743d 4661          reset=Fa
-0000da10: 6c73 652c 0a20 2020 2020 2020 2020 2020  lse,.           
-0000da20: 2066 6f72 6365 5f61 6c6c 5f66 696e 6974   force_all_finit
-0000da30: 653d 2261 6c6c 6f77 2d6e 616e 222c 0a20  e="allow-nan",. 
-0000da40: 2020 2020 2020 2029 0a0a 2020 2020 2020         )..      
-0000da50: 2020 6966 2073 7061 7273 652e 6973 7370    if sparse.issp
-0000da60: 6172 7365 2858 293a 0a20 2020 2020 2020  arse(X):.       
-0000da70: 2020 2020 2069 6620 7365 6c66 2e77 6974       if self.wit
-0000da80: 685f 7363 616c 696e 673a 0a20 2020 2020  h_scaling:.     
-0000da90: 2020 2020 2020 2020 2020 2069 6e70 6c61             inpla
-0000daa0: 6365 5f63 6f6c 756d 6e5f 7363 616c 6528  ce_column_scale(
-0000dab0: 582c 2031 2e30 202f 2073 656c 662e 7363  X, 1.0 / self.sc
-0000dac0: 616c 655f 290a 2020 2020 2020 2020 656c  ale_).        el
-0000dad0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-0000dae0: 6966 2073 656c 662e 7769 7468 5f63 656e  if self.with_cen
-0000daf0: 7465 7269 6e67 3a0a 2020 2020 2020 2020  tering:.        
-0000db00: 2020 2020 2020 2020 5820 2d3d 2073 656c          X -= sel
-0000db10: 662e 6365 6e74 6572 5f0a 2020 2020 2020  f.center_.      
-0000db20: 2020 2020 2020 6966 2073 656c 662e 7769        if self.wi
-0000db30: 7468 5f73 6361 6c69 6e67 3a0a 2020 2020  th_scaling:.    
-0000db40: 2020 2020 2020 2020 2020 2020 5820 2f3d              X /=
-0000db50: 2073 656c 662e 7363 616c 655f 0a20 2020   self.scale_.   
-0000db60: 2020 2020 2072 6574 7572 6e20 580a 0a20       return X.. 
-0000db70: 2020 2064 6566 2069 6e76 6572 7365 5f74     def inverse_t
-0000db80: 7261 6e73 666f 726d 2873 656c 662c 2058  ransform(self, X
-0000db90: 293a 0a20 2020 2020 2020 2022 2222 5363  ):.        """Sc
-0000dba0: 616c 6520 6261 636b 2074 6865 2064 6174  ale back the dat
-0000dbb0: 6120 746f 2074 6865 206f 7269 6769 6e61  a to the origina
-0000dbc0: 6c20 7265 7072 6573 656e 7461 7469 6f6e  l representation
-0000dbd0: 2e0a 0a20 2020 2020 2020 2050 6172 616d  ...        Param
-0000dbe0: 6574 6572 730a 2020 2020 2020 2020 2d2d  eters.        --
-0000dbf0: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020 2020  --------.       
-0000dc00: 2058 203a 207b 6172 7261 792d 6c69 6b65   X : {array-like
-0000dc10: 2c20 7370 6172 7365 206d 6174 7269 787d  , sparse matrix}
-0000dc20: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-0000dc30: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-0000dc40: 290a 2020 2020 2020 2020 2020 2020 5468  ).            Th
-0000dc50: 6520 7265 7363 616c 6564 2064 6174 6120  e rescaled data 
-0000dc60: 746f 2062 6520 7472 616e 7366 6f72 6d65  to be transforme
-0000dc70: 6420 6261 636b 2e0a 0a20 2020 2020 2020  d back...       
-0000dc80: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
-0000dc90: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
-0000dca0: 2058 5f74 7220 3a20 7b6e 6461 7272 6179   X_tr : {ndarray
-0000dcb0: 2c20 7370 6172 7365 206d 6174 7269 787d  , sparse matrix}
-0000dcc0: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-0000dcd0: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-0000dce0: 290a 2020 2020 2020 2020 2020 2020 5472  ).            Tr
-0000dcf0: 616e 7366 6f72 6d65 6420 6172 7261 792e  ansformed array.
-0000dd00: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-0000dd10: 2020 2020 2063 6865 636b 5f69 735f 6669       check_is_fi
-0000dd20: 7474 6564 2873 656c 6629 0a20 2020 2020  tted(self).     
-0000dd30: 2020 2058 203d 2063 6865 636b 5f61 7272     X = check_arr
-0000dd40: 6179 280a 2020 2020 2020 2020 2020 2020  ay(.            
-0000dd50: 582c 0a20 2020 2020 2020 2020 2020 2061  X,.            a
-0000dd60: 6363 6570 745f 7370 6172 7365 3d28 2263  ccept_sparse=("c
-0000dd70: 7372 222c 2022 6373 6322 292c 0a20 2020  sr", "csc"),.   
-0000dd80: 2020 2020 2020 2020 2063 6f70 793d 7365           copy=se
-0000dd90: 6c66 2e63 6f70 792c 0a20 2020 2020 2020  lf.copy,.       
-0000dda0: 2020 2020 2064 7479 7065 3d46 4c4f 4154       dtype=FLOAT
-0000ddb0: 5f44 5459 5045 532c 0a20 2020 2020 2020  _DTYPES,.       
-0000ddc0: 2020 2020 2066 6f72 6365 5f61 6c6c 5f66       force_all_f
-0000ddd0: 696e 6974 653d 2261 6c6c 6f77 2d6e 616e  inite="allow-nan
-0000dde0: 222c 0a20 2020 2020 2020 2029 0a0a 2020  ",.        )..  
-0000ddf0: 2020 2020 2020 6966 2073 7061 7273 652e        if sparse.
-0000de00: 6973 7370 6172 7365 2858 293a 0a20 2020  issparse(X):.   
-0000de10: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
-0000de20: 2e77 6974 685f 7363 616c 696e 673a 0a20  .with_scaling:. 
-0000de30: 2020 2020 2020 2020 2020 2020 2020 2069                 i
-0000de40: 6e70 6c61 6365 5f63 6f6c 756d 6e5f 7363  nplace_column_sc
-0000de50: 616c 6528 582c 2073 656c 662e 7363 616c  ale(X, self.scal
-0000de60: 655f 290a 2020 2020 2020 2020 656c 7365  e_).        else
-0000de70: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
-0000de80: 2073 656c 662e 7769 7468 5f73 6361 6c69   self.with_scali
-0000de90: 6e67 3a0a 2020 2020 2020 2020 2020 2020  ng:.            
-0000dea0: 2020 2020 5820 2a3d 2073 656c 662e 7363      X *= self.sc
-0000deb0: 616c 655f 0a20 2020 2020 2020 2020 2020  ale_.           
-0000dec0: 2069 6620 7365 6c66 2e77 6974 685f 6365   if self.with_ce
-0000ded0: 6e74 6572 696e 673a 0a20 2020 2020 2020  ntering:.       
-0000dee0: 2020 2020 2020 2020 2058 202b 3d20 7365           X += se
-0000def0: 6c66 2e63 656e 7465 725f 0a20 2020 2020  lf.center_.     
-0000df00: 2020 2072 6574 7572 6e20 580a 0a20 2020     return X..   
-0000df10: 2064 6566 205f 6d6f 7265 5f74 6167 7328   def _more_tags(
-0000df20: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
-0000df30: 6574 7572 6e20 7b22 616c 6c6f 775f 6e61  eturn {"allow_na
-0000df40: 6e22 3a20 5472 7565 7d0a 0a0a 4076 616c  n": True}...@val
-0000df50: 6964 6174 655f 7061 7261 6d73 280a 2020  idate_params(.  
-0000df60: 2020 7b22 5822 3a20 5b22 6172 7261 792d    {"X": ["array-
-0000df70: 6c69 6b65 222c 2022 7370 6172 7365 206d  like", "sparse m
-0000df80: 6174 7269 7822 5d2c 2022 6178 6973 223a  atrix"], "axis":
-0000df90: 205b 4f70 7469 6f6e 7328 496e 7465 6772   [Options(Integr
-0000dfa0: 616c 2c20 7b30 2c20 317d 295d 7d2c 0a20  al, {0, 1})]},. 
-0000dfb0: 2020 2070 7265 6665 725f 736b 6970 5f6e     prefer_skip_n
-0000dfc0: 6573 7465 645f 7661 6c69 6461 7469 6f6e  ested_validation
-0000dfd0: 3d46 616c 7365 2c0a 290a 6465 6620 726f  =False,.).def ro
-0000dfe0: 6275 7374 5f73 6361 6c65 280a 2020 2020  bust_scale(.    
-0000dff0: 582c 0a20 2020 202a 2c0a 2020 2020 6178  X,.    *,.    ax
-0000e000: 6973 3d30 2c0a 2020 2020 7769 7468 5f63  is=0,.    with_c
-0000e010: 656e 7465 7269 6e67 3d54 7275 652c 0a20  entering=True,. 
-0000e020: 2020 2077 6974 685f 7363 616c 696e 673d     with_scaling=
-0000e030: 5472 7565 2c0a 2020 2020 7175 616e 7469  True,.    quanti
-0000e040: 6c65 5f72 616e 6765 3d28 3235 2e30 2c20  le_range=(25.0, 
-0000e050: 3735 2e30 292c 0a20 2020 2063 6f70 793d  75.0),.    copy=
-0000e060: 5472 7565 2c0a 2020 2020 756e 6974 5f76  True,.    unit_v
-0000e070: 6172 6961 6e63 653d 4661 6c73 652c 0a29  ariance=False,.)
-0000e080: 3a0a 2020 2020 2222 2253 7461 6e64 6172  :.    """Standar
-0000e090: 6469 7a65 2061 2064 6174 6173 6574 2061  dize a dataset a
-0000e0a0: 6c6f 6e67 2061 6e79 2061 7869 732e 0a0a  long any axis...
-0000e0b0: 2020 2020 4365 6e74 6572 2074 6f20 7468      Center to th
-0000e0c0: 6520 6d65 6469 616e 2061 6e64 2063 6f6d  e median and com
-0000e0d0: 706f 6e65 6e74 2077 6973 6520 7363 616c  ponent wise scal
-0000e0e0: 650a 2020 2020 6163 636f 7264 696e 6720  e.    according 
-0000e0f0: 746f 2074 6865 2069 6e74 6572 7175 6172  to the interquar
-0000e100: 7469 6c65 2072 616e 6765 2e0a 0a20 2020  tile range...   
-0000e110: 2052 6561 6420 6d6f 7265 2069 6e20 7468   Read more in th
-0000e120: 6520 3a72 6566 3a60 5573 6572 2047 7569  e :ref:`User Gui
-0000e130: 6465 203c 7072 6570 726f 6365 7373 696e  de <preprocessin
-0000e140: 675f 7363 616c 6572 3e60 2e0a 0a20 2020  g_scaler>`...   
-0000e150: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
-0000e160: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2058  ----------.    X
-0000e170: 203a 207b 6172 7261 792d 6c69 6b65 2c20   : {array-like, 
-0000e180: 7370 6172 7365 206d 6174 7269 787d 206f  sparse matrix} o
-0000e190: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
-0000e1a0: 652c 206e 5f66 6561 7475 7265 7329 0a20  e, n_features). 
-0000e1b0: 2020 2020 2020 2054 6865 2064 6174 6120         The data 
-0000e1c0: 746f 2063 656e 7465 7220 616e 6420 7363  to center and sc
-0000e1d0: 616c 652e 0a0a 2020 2020 6178 6973 203a  ale...    axis :
-0000e1e0: 2069 6e74 2c20 6465 6661 756c 743d 300a   int, default=0.
-0000e1f0: 2020 2020 2020 2020 4178 6973 2075 7365          Axis use
-0000e200: 6420 746f 2063 6f6d 7075 7465 2074 6865  d to compute the
-0000e210: 206d 6564 6961 6e73 2061 6e64 2049 5152   medians and IQR
-0000e220: 2061 6c6f 6e67 2e20 4966 2030 2c0a 2020   along. If 0,.  
-0000e230: 2020 2020 2020 696e 6465 7065 6e64 656e        independen
-0000e240: 746c 7920 7363 616c 6520 6561 6368 2066  tly scale each f
-0000e250: 6561 7475 7265 2c20 6f74 6865 7277 6973  eature, otherwis
-0000e260: 6520 2869 6620 3129 2073 6361 6c65 0a20  e (if 1) scale. 
-0000e270: 2020 2020 2020 2065 6163 6820 7361 6d70         each samp
-0000e280: 6c65 2e0a 0a20 2020 2077 6974 685f 6365  le...    with_ce
-0000e290: 6e74 6572 696e 6720 3a20 626f 6f6c 2c20  ntering : bool, 
-0000e2a0: 6465 6661 756c 743d 5472 7565 0a20 2020  default=True.   
-0000e2b0: 2020 2020 2049 6620 6054 7275 6560 2c20       If `True`, 
-0000e2c0: 6365 6e74 6572 2074 6865 2064 6174 6120  center the data 
-0000e2d0: 6265 666f 7265 2073 6361 6c69 6e67 2e0a  before scaling..
-0000e2e0: 0a20 2020 2077 6974 685f 7363 616c 696e  .    with_scalin
-0000e2f0: 6720 3a20 626f 6f6c 2c20 6465 6661 756c  g : bool, defaul
-0000e300: 743d 5472 7565 0a20 2020 2020 2020 2049  t=True.        I
-0000e310: 6620 6054 7275 6560 2c20 7363 616c 6520  f `True`, scale 
-0000e320: 7468 6520 6461 7461 2074 6f20 756e 6974  the data to unit
-0000e330: 2076 6172 6961 6e63 6520 286f 7220 6571   variance (or eq
-0000e340: 7569 7661 6c65 6e74 6c79 2c0a 2020 2020  uivalently,.    
-0000e350: 2020 2020 756e 6974 2073 7461 6e64 6172      unit standar
-0000e360: 6420 6465 7669 6174 696f 6e29 2e0a 0a20  d deviation)... 
-0000e370: 2020 2071 7561 6e74 696c 655f 7261 6e67     quantile_rang
-0000e380: 6520 3a20 7475 706c 6520 2871 5f6d 696e  e : tuple (q_min
-0000e390: 2c20 715f 6d61 7829 2c20 302e 3020 3c20  , q_max), 0.0 < 
-0000e3a0: 715f 6d69 6e20 3c20 715f 6d61 7820 3c20  q_min < q_max < 
-0000e3b0: 3130 302e 302c 5c0a 2020 2020 2020 2020  100.0,\.        
-0000e3c0: 6465 6661 756c 743d 2832 352e 302c 2037  default=(25.0, 7
-0000e3d0: 352e 3029 0a20 2020 2020 2020 2051 7561  5.0).        Qua
-0000e3e0: 6e74 696c 6520 7261 6e67 6520 7573 6564  ntile range used
-0000e3f0: 2074 6f20 6361 6c63 756c 6174 6520 6073   to calculate `s
-0000e400: 6361 6c65 5f60 2e20 4279 2064 6566 6175  cale_`. By defau
-0000e410: 6c74 2074 6869 7320 6973 2065 7175 616c  lt this is equal
-0000e420: 2074 6f0a 2020 2020 2020 2020 7468 6520   to.        the 
-0000e430: 4951 522c 2069 2e65 2e2c 2060 715f 6d69  IQR, i.e., `q_mi
-0000e440: 6e60 2069 7320 7468 6520 6669 7273 7420  n` is the first 
-0000e450: 7175 616e 7469 6c65 2061 6e64 2060 715f  quantile and `q_
-0000e460: 6d61 7860 2069 7320 7468 6520 7468 6972  max` is the thir
-0000e470: 640a 2020 2020 2020 2020 7175 616e 7469  d.        quanti
-0000e480: 6c65 2e0a 0a20 2020 2020 2020 202e 2e20  le...        .. 
-0000e490: 7665 7273 696f 6e61 6464 6564 3a3a 2030  versionadded:: 0
-0000e4a0: 2e31 380a 0a20 2020 2063 6f70 7920 3a20  .18..    copy : 
-0000e4b0: 626f 6f6c 2c20 6465 6661 756c 743d 5472  bool, default=Tr
-0000e4c0: 7565 0a20 2020 2020 2020 2049 6620 4661  ue.        If Fa
-0000e4d0: 6c73 652c 2074 7279 2074 6f20 6176 6f69  lse, try to avoi
-0000e4e0: 6420 6120 636f 7079 2061 6e64 2073 6361  d a copy and sca
-0000e4f0: 6c65 2069 6e20 706c 6163 652e 0a20 2020  le in place..   
-0000e500: 2020 2020 2054 6869 7320 6973 206e 6f74       This is not
-0000e510: 2067 7561 7261 6e74 6565 6420 746f 2061   guaranteed to a
-0000e520: 6c77 6179 7320 776f 726b 2069 6e20 706c  lways work in pl
-0000e530: 6163 653b 2065 2e67 2e20 6966 2074 6865  ace; e.g. if the
-0000e540: 2064 6174 6120 6973 0a20 2020 2020 2020   data is.       
-0000e550: 2061 206e 756d 7079 2061 7272 6179 2077   a numpy array w
-0000e560: 6974 6820 616e 2069 6e74 2064 7479 7065  ith an int dtype
-0000e570: 2c20 6120 636f 7079 2077 696c 6c20 6265  , a copy will be
-0000e580: 2072 6574 7572 6e65 6420 6576 656e 2077   returned even w
-0000e590: 6974 680a 2020 2020 2020 2020 636f 7079  ith.        copy
-0000e5a0: 3d46 616c 7365 2e0a 0a20 2020 2075 6e69  =False...    uni
-0000e5b0: 745f 7661 7269 616e 6365 203a 2062 6f6f  t_variance : boo
-0000e5c0: 6c2c 2064 6566 6175 6c74 3d46 616c 7365  l, default=False
-0000e5d0: 0a20 2020 2020 2020 2049 6620 6054 7275  .        If `Tru
-0000e5e0: 6560 2c20 7363 616c 6520 6461 7461 2073  e`, scale data s
-0000e5f0: 6f20 7468 6174 206e 6f72 6d61 6c6c 7920  o that normally 
-0000e600: 6469 7374 7269 6275 7465 6420 6665 6174  distributed feat
-0000e610: 7572 6573 2068 6176 6520 610a 2020 2020  ures have a.    
-0000e620: 2020 2020 7661 7269 616e 6365 206f 6620      variance of 
-0000e630: 312e 2049 6e20 6765 6e65 7261 6c2c 2069  1. In general, i
-0000e640: 6620 7468 6520 6469 6666 6572 656e 6365  f the difference
-0000e650: 2062 6574 7765 656e 2074 6865 2078 2d76   between the x-v
-0000e660: 616c 7565 7320 6f66 0a20 2020 2020 2020  alues of.       
-0000e670: 2060 715f 6d61 7860 2061 6e64 2060 715f   `q_max` and `q_
-0000e680: 6d69 6e60 2066 6f72 2061 2073 7461 6e64  min` for a stand
-0000e690: 6172 6420 6e6f 726d 616c 2064 6973 7472  ard normal distr
-0000e6a0: 6962 7574 696f 6e20 6973 2067 7265 6174  ibution is great
-0000e6b0: 6572 0a20 2020 2020 2020 2074 6861 6e20  er.        than 
-0000e6c0: 312c 2074 6865 2064 6174 6173 6574 2077  1, the dataset w
-0000e6d0: 696c 6c20 6265 2073 6361 6c65 6420 646f  ill be scaled do
-0000e6e0: 776e 2e20 4966 206c 6573 7320 7468 616e  wn. If less than
-0000e6f0: 2031 2c20 7468 6520 6461 7461 7365 740a   1, the dataset.
-0000e700: 2020 2020 2020 2020 7769 6c6c 2062 6520          will be 
-0000e710: 7363 616c 6564 2075 702e 0a0a 2020 2020  scaled up...    
-0000e720: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
-0000e730: 6465 643a 3a20 302e 3234 0a0a 2020 2020  ded:: 0.24..    
-0000e740: 5265 7475 726e 730a 2020 2020 2d2d 2d2d  Returns.    ----
-0000e750: 2d2d 2d0a 2020 2020 585f 7472 203a 207b  ---.    X_tr : {
-0000e760: 6e64 6172 7261 792c 2073 7061 7273 6520  ndarray, sparse 
-0000e770: 6d61 7472 6978 7d20 6f66 2073 6861 7065  matrix} of shape
-0000e780: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
-0000e790: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
-0000e7a0: 2054 6865 2074 7261 6e73 666f 726d 6564   The transformed
-0000e7b0: 2064 6174 612e 0a0a 2020 2020 5365 6520   data...    See 
-0000e7c0: 416c 736f 0a20 2020 202d 2d2d 2d2d 2d2d  Also.    -------
-0000e7d0: 2d0a 2020 2020 526f 6275 7374 5363 616c  -.    RobustScal
-0000e7e0: 6572 203a 2050 6572 666f 726d 7320 6365  er : Performs ce
-0000e7f0: 6e74 6572 696e 6720 616e 6420 7363 616c  ntering and scal
-0000e800: 696e 6720 7573 696e 6720 7468 6520 5472  ing using the Tr
-0000e810: 616e 7366 6f72 6d65 7220 4150 490a 2020  ansformer API.  
-0000e820: 2020 2020 2020 2865 2e67 2e20 6173 2070        (e.g. as p
-0000e830: 6172 7420 6f66 2061 2070 7265 7072 6f63  art of a preproc
-0000e840: 6573 7369 6e67 203a 636c 6173 733a 607e  essing :class:`~
-0000e850: 736b 6c65 6172 6e2e 7069 7065 6c69 6e65  sklearn.pipeline
-0000e860: 2e50 6970 656c 696e 6560 292e 0a0a 2020  .Pipeline`)...  
-0000e870: 2020 4e6f 7465 730a 2020 2020 2d2d 2d2d    Notes.    ----
-0000e880: 2d0a 2020 2020 5468 6973 2069 6d70 6c65  -.    This imple
-0000e890: 6d65 6e74 6174 696f 6e20 7769 6c6c 2072  mentation will r
-0000e8a0: 6566 7573 6520 746f 2063 656e 7465 7220  efuse to center 
-0000e8b0: 7363 6970 792e 7370 6172 7365 206d 6174  scipy.sparse mat
-0000e8c0: 7269 6365 730a 2020 2020 7369 6e63 6520  rices.    since 
-0000e8d0: 6974 2077 6f75 6c64 206d 616b 6520 7468  it would make th
-0000e8e0: 656d 206e 6f6e 2d73 7061 7273 6520 616e  em non-sparse an
-0000e8f0: 6420 776f 756c 6420 706f 7465 6e74 6961  d would potentia
-0000e900: 6c6c 7920 6372 6173 6820 7468 650a 2020  lly crash the.  
-0000e910: 2020 7072 6f67 7261 6d20 7769 7468 206d    program with m
-0000e920: 656d 6f72 7920 6578 6861 7573 7469 6f6e  emory exhaustion
-0000e930: 2070 726f 626c 656d 732e 0a0a 2020 2020   problems...    
-0000e940: 496e 7374 6561 6420 7468 6520 6361 6c6c  Instead the call
-0000e950: 6572 2069 7320 6578 7065 6374 6564 2074  er is expected t
-0000e960: 6f20 6569 7468 6572 2073 6574 2065 7870  o either set exp
-0000e970: 6c69 6369 746c 790a 2020 2020 6077 6974  licitly.    `wit
-0000e980: 685f 6365 6e74 6572 696e 673d 4661 6c73  h_centering=Fals
-0000e990: 6560 2028 696e 2074 6861 7420 6361 7365  e` (in that case
-0000e9a0: 2c20 6f6e 6c79 2076 6172 6961 6e63 6520  , only variance 
-0000e9b0: 7363 616c 696e 6720 7769 6c6c 2062 650a  scaling will be.
-0000e9c0: 2020 2020 7065 7266 6f72 6d65 6420 6f6e      performed on
-0000e9d0: 2074 6865 2066 6561 7475 7265 7320 6f66   the features of
-0000e9e0: 2074 6865 2043 5352 206d 6174 7269 7829   the CSR matrix)
-0000e9f0: 206f 7220 746f 2063 616c 6c20 6058 2e74   or to call `X.t
-0000ea00: 6f61 7272 6179 2829 600a 2020 2020 6966  oarray()`.    if
-0000ea10: 2068 652f 7368 6520 6578 7065 6374 7320   he/she expects 
-0000ea20: 7468 6520 6d61 7465 7269 616c 697a 6564  the materialized
-0000ea30: 2064 656e 7365 2061 7272 6179 2074 6f20   dense array to 
-0000ea40: 6669 7420 696e 206d 656d 6f72 792e 0a0a  fit in memory...
-0000ea50: 2020 2020 546f 2061 766f 6964 206d 656d      To avoid mem
-0000ea60: 6f72 7920 636f 7079 2074 6865 2063 616c  ory copy the cal
-0000ea70: 6c65 7220 7368 6f75 6c64 2070 6173 7320  ler should pass 
-0000ea80: 6120 4353 5220 6d61 7472 6978 2e0a 0a20  a CSR matrix... 
-0000ea90: 2020 2046 6f72 2061 2063 6f6d 7061 7269     For a compari
-0000eaa0: 736f 6e20 6f66 2074 6865 2064 6966 6665  son of the diffe
-0000eab0: 7265 6e74 2073 6361 6c65 7273 2c20 7472  rent scalers, tr
-0000eac0: 616e 7366 6f72 6d65 7273 2c20 616e 6420  ansformers, and 
-0000ead0: 6e6f 726d 616c 697a 6572 732c 0a20 2020  normalizers,.   
-0000eae0: 2073 6565 3a20 3a72 6566 3a60 7370 6878   see: :ref:`sphx
-0000eaf0: 5f67 6c72 5f61 7574 6f5f 6578 616d 706c  _glr_auto_exampl
-0000eb00: 6573 5f70 7265 7072 6f63 6573 7369 6e67  es_preprocessing
-0000eb10: 5f70 6c6f 745f 616c 6c5f 7363 616c 696e  _plot_all_scalin
-0000eb20: 672e 7079 602e 0a0a 2020 2020 2e2e 2077  g.py`...    .. w
-0000eb30: 6172 6e69 6e67 3a3a 2052 6973 6b20 6f66  arning:: Risk of
-0000eb40: 2064 6174 6120 6c65 616b 0a0a 2020 2020   data leak..    
-0000eb50: 2020 2020 446f 206e 6f74 2075 7365 203a      Do not use :
-0000eb60: 6675 6e63 3a60 7e73 6b6c 6561 726e 2e70  func:`~sklearn.p
-0000eb70: 7265 7072 6f63 6573 7369 6e67 2e72 6f62  reprocessing.rob
-0000eb80: 7573 745f 7363 616c 6560 2075 6e6c 6573  ust_scale` unles
-0000eb90: 7320 796f 7520 6b6e 6f77 0a20 2020 2020  s you know.     
-0000eba0: 2020 2077 6861 7420 796f 7520 6172 6520     what you are 
-0000ebb0: 646f 696e 672e 2041 2063 6f6d 6d6f 6e20  doing. A common 
-0000ebc0: 6d69 7374 616b 6520 6973 2074 6f20 6170  mistake is to ap
-0000ebd0: 706c 7920 6974 2074 6f20 7468 6520 656e  ply it to the en
-0000ebe0: 7469 7265 2064 6174 610a 2020 2020 2020  tire data.      
-0000ebf0: 2020 2a62 6566 6f72 652a 2073 706c 6974    *before* split
-0000ec00: 7469 6e67 2069 6e74 6f20 7472 6169 6e69  ting into traini
-0000ec10: 6e67 2061 6e64 2074 6573 7420 7365 7473  ng and test sets
-0000ec20: 2e20 5468 6973 2077 696c 6c20 6269 6173  . This will bias
-0000ec30: 2074 6865 0a20 2020 2020 2020 206d 6f64   the.        mod
-0000ec40: 656c 2065 7661 6c75 6174 696f 6e20 6265  el evaluation be
-0000ec50: 6361 7573 6520 696e 666f 726d 6174 696f  cause informatio
-0000ec60: 6e20 776f 756c 6420 6861 7665 206c 6561  n would have lea
-0000ec70: 6b65 6420 6672 6f6d 2074 6865 2074 6573  ked from the tes
-0000ec80: 740a 2020 2020 2020 2020 7365 7420 746f  t.        set to
-0000ec90: 2074 6865 2074 7261 696e 696e 6720 7365   the training se
-0000eca0: 742e 0a20 2020 2020 2020 2049 6e20 6765  t..        In ge
-0000ecb0: 6e65 7261 6c2c 2077 6520 7265 636f 6d6d  neral, we recomm
-0000ecc0: 656e 6420 7573 696e 670a 2020 2020 2020  end using.      
-0000ecd0: 2020 3a63 6c61 7373 3a60 7e73 6b6c 6561    :class:`~sklea
-0000ece0: 726e 2e70 7265 7072 6f63 6573 7369 6e67  rn.preprocessing
-0000ecf0: 2e52 6f62 7573 7453 6361 6c65 7260 2077  .RobustScaler` w
-0000ed00: 6974 6869 6e20 610a 2020 2020 2020 2020  ithin a.        
-0000ed10: 3a72 6566 3a60 5069 7065 6c69 6e65 203c  :ref:`Pipeline <
-0000ed20: 7069 7065 6c69 6e65 3e60 2069 6e20 6f72  pipeline>` in or
-0000ed30: 6465 7220 746f 2070 7265 7665 6e74 206d  der to prevent m
-0000ed40: 6f73 7420 7269 736b 7320 6f66 2064 6174  ost risks of dat
-0000ed50: 610a 2020 2020 2020 2020 6c65 616b 696e  a.        leakin
-0000ed60: 673a 2060 7069 7065 203d 206d 616b 655f  g: `pipe = make_
-0000ed70: 7069 7065 6c69 6e65 2852 6f62 7573 7453  pipeline(RobustS
-0000ed80: 6361 6c65 7228 292c 204c 6f67 6973 7469  caler(), Logisti
-0000ed90: 6352 6567 7265 7373 696f 6e28 2929 602e  cRegression())`.
-0000eda0: 0a0a 2020 2020 4578 616d 706c 6573 0a20  ..    Examples. 
-0000edb0: 2020 202d 2d2d 2d2d 2d2d 2d0a 2020 2020     --------.    
-0000edc0: 3e3e 3e20 6672 6f6d 2073 6b6c 6561 726e  >>> from sklearn
-0000edd0: 2e70 7265 7072 6f63 6573 7369 6e67 2069  .preprocessing i
-0000ede0: 6d70 6f72 7420 726f 6275 7374 5f73 6361  mport robust_sca
-0000edf0: 6c65 0a20 2020 203e 3e3e 2058 203d 205b  le.    >>> X = [
-0000ee00: 5b2d 322c 2031 2c20 325d 2c20 5b2d 312c  [-2, 1, 2], [-1,
-0000ee10: 2030 2c20 315d 5d0a 2020 2020 3e3e 3e20   0, 1]].    >>> 
-0000ee20: 726f 6275 7374 5f73 6361 6c65 2858 2c20  robust_scale(X, 
-0000ee30: 6178 6973 3d30 2920 2023 2073 6361 6c65  axis=0)  # scale
-0000ee40: 2065 6163 6820 636f 6c75 6d6e 2069 6e64   each column ind
-0000ee50: 6570 656e 6465 6e74 6c79 0a20 2020 2061  ependently.    a
-0000ee60: 7272 6179 285b 5b2d 312e 2c20 2031 2e2c  rray([[-1.,  1.,
-0000ee70: 2020 312e 5d2c 0a20 2020 2020 2020 2020    1.],.         
-0000ee80: 2020 5b20 312e 2c20 2d31 2e2c 202d 312e    [ 1., -1., -1.
-0000ee90: 5d5d 290a 2020 2020 3e3e 3e20 726f 6275  ]]).    >>> robu
-0000eea0: 7374 5f73 6361 6c65 2858 2c20 6178 6973  st_scale(X, axis
-0000eeb0: 3d31 2920 2023 2073 6361 6c65 2065 6163  =1)  # scale eac
-0000eec0: 6820 726f 7720 696e 6465 7065 6e64 656e  h row independen
-0000eed0: 746c 790a 2020 2020 6172 7261 7928 5b5b  tly.    array([[
-0000eee0: 2d31 2e35 2c20 2030 2e20 2c20 2030 2e35  -1.5,  0. ,  0.5
-0000eef0: 5d2c 0a20 2020 2020 2020 2020 2020 5b2d  ],.           [-
-0000ef00: 312e 202c 2020 302e 202c 2020 312e 205d  1. ,  0. ,  1. ]
-0000ef10: 5d29 0a20 2020 2022 2222 0a20 2020 2058  ]).    """.    X
-0000ef20: 203d 2063 6865 636b 5f61 7272 6179 280a   = check_array(.
-0000ef30: 2020 2020 2020 2020 582c 0a20 2020 2020          X,.     
-0000ef40: 2020 2061 6363 6570 745f 7370 6172 7365     accept_sparse
-0000ef50: 3d28 2263 7372 222c 2022 6373 6322 292c  =("csr", "csc"),
-0000ef60: 0a20 2020 2020 2020 2063 6f70 793d 4661  .        copy=Fa
-0000ef70: 6c73 652c 0a20 2020 2020 2020 2065 6e73  lse,.        ens
-0000ef80: 7572 655f 3264 3d46 616c 7365 2c0a 2020  ure_2d=False,.  
-0000ef90: 2020 2020 2020 6474 7970 653d 464c 4f41        dtype=FLOA
-0000efa0: 545f 4454 5950 4553 2c0a 2020 2020 2020  T_DTYPES,.      
-0000efb0: 2020 666f 7263 655f 616c 6c5f 6669 6e69    force_all_fini
-0000efc0: 7465 3d22 616c 6c6f 772d 6e61 6e22 2c0a  te="allow-nan",.
-0000efd0: 2020 2020 290a 2020 2020 6f72 6967 696e      ).    origin
-0000efe0: 616c 5f6e 6469 6d20 3d20 582e 6e64 696d  al_ndim = X.ndim
-0000eff0: 0a0a 2020 2020 6966 206f 7269 6769 6e61  ..    if origina
-0000f000: 6c5f 6e64 696d 203d 3d20 313a 0a20 2020  l_ndim == 1:.   
-0000f010: 2020 2020 2058 203d 2058 2e72 6573 6861       X = X.resha
-0000f020: 7065 2858 2e73 6861 7065 5b30 5d2c 2031  pe(X.shape[0], 1
-0000f030: 290a 0a20 2020 2073 203d 2052 6f62 7573  )..    s = Robus
-0000f040: 7453 6361 6c65 7228 0a20 2020 2020 2020  tScaler(.       
-0000f050: 2077 6974 685f 6365 6e74 6572 696e 673d   with_centering=
-0000f060: 7769 7468 5f63 656e 7465 7269 6e67 2c0a  with_centering,.
-0000f070: 2020 2020 2020 2020 7769 7468 5f73 6361          with_sca
-0000f080: 6c69 6e67 3d77 6974 685f 7363 616c 696e  ling=with_scalin
-0000f090: 672c 0a20 2020 2020 2020 2071 7561 6e74  g,.        quant
-0000f0a0: 696c 655f 7261 6e67 653d 7175 616e 7469  ile_range=quanti
-0000f0b0: 6c65 5f72 616e 6765 2c0a 2020 2020 2020  le_range,.      
-0000f0c0: 2020 756e 6974 5f76 6172 6961 6e63 653d    unit_variance=
-0000f0d0: 756e 6974 5f76 6172 6961 6e63 652c 0a20  unit_variance,. 
-0000f0e0: 2020 2020 2020 2063 6f70 793d 636f 7079         copy=copy
-0000f0f0: 2c0a 2020 2020 290a 2020 2020 6966 2061  ,.    ).    if a
-0000f100: 7869 7320 3d3d 2030 3a0a 2020 2020 2020  xis == 0:.      
-0000f110: 2020 5820 3d20 732e 6669 745f 7472 616e    X = s.fit_tran
-0000f120: 7366 6f72 6d28 5829 0a20 2020 2065 6c73  sform(X).    els
-0000f130: 653a 0a20 2020 2020 2020 2058 203d 2073  e:.        X = s
-0000f140: 2e66 6974 5f74 7261 6e73 666f 726d 2858  .fit_transform(X
-0000f150: 2e54 292e 540a 0a20 2020 2069 6620 6f72  .T).T..    if or
-0000f160: 6967 696e 616c 5f6e 6469 6d20 3d3d 2031  iginal_ndim == 1
-0000f170: 3a0a 2020 2020 2020 2020 5820 3d20 582e  :.        X = X.
-0000f180: 7261 7665 6c28 290a 0a20 2020 2072 6574  ravel()..    ret
-0000f190: 7572 6e20 580a 0a0a 4076 616c 6964 6174  urn X...@validat
-0000f1a0: 655f 7061 7261 6d73 280a 2020 2020 7b0a  e_params(.    {.
-0000f1b0: 2020 2020 2020 2020 2258 223a 205b 2261          "X": ["a
-0000f1c0: 7272 6179 2d6c 696b 6522 2c20 2273 7061  rray-like", "spa
-0000f1d0: 7273 6520 6d61 7472 6978 225d 2c0a 2020  rse matrix"],.  
-0000f1e0: 2020 2020 2020 226e 6f72 6d22 3a20 5b53        "norm": [S
-0000f1f0: 7472 4f70 7469 6f6e 7328 7b22 6c31 222c  trOptions({"l1",
-0000f200: 2022 6c32 222c 2022 6d61 7822 7d29 5d2c   "l2", "max"})],
-0000f210: 0a20 2020 2020 2020 2022 6178 6973 223a  .        "axis":
-0000f220: 205b 4f70 7469 6f6e 7328 496e 7465 6772   [Options(Integr
-0000f230: 616c 2c20 7b30 2c20 317d 295d 2c0a 2020  al, {0, 1})],.  
-0000f240: 2020 2020 2020 2263 6f70 7922 3a20 5b22        "copy": ["
-0000f250: 626f 6f6c 6561 6e22 5d2c 0a20 2020 2020  boolean"],.     
-0000f260: 2020 2022 7265 7475 726e 5f6e 6f72 6d22     "return_norm"
-0000f270: 3a20 5b22 626f 6f6c 6561 6e22 5d2c 0a20  : ["boolean"],. 
-0000f280: 2020 207d 2c0a 2020 2020 7072 6566 6572     },.    prefer
-0000f290: 5f73 6b69 705f 6e65 7374 6564 5f76 616c  _skip_nested_val
-0000f2a0: 6964 6174 696f 6e3d 5472 7565 2c0a 290a  idation=True,.).
-0000f2b0: 6465 6620 6e6f 726d 616c 697a 6528 582c  def normalize(X,
-0000f2c0: 206e 6f72 6d3d 226c 3222 2c20 2a2c 2061   norm="l2", *, a
-0000f2d0: 7869 733d 312c 2063 6f70 793d 5472 7565  xis=1, copy=True
-0000f2e0: 2c20 7265 7475 726e 5f6e 6f72 6d3d 4661  , return_norm=Fa
-0000f2f0: 6c73 6529 3a0a 2020 2020 2222 2253 6361  lse):.    """Sca
-0000f300: 6c65 2069 6e70 7574 2076 6563 746f 7273  le input vectors
-0000f310: 2069 6e64 6976 6964 7561 6c6c 7920 746f   individually to
-0000f320: 2075 6e69 7420 6e6f 726d 2028 7665 6374   unit norm (vect
-0000f330: 6f72 206c 656e 6774 6829 2e0a 0a20 2020  or length)...   
-0000f340: 2052 6561 6420 6d6f 7265 2069 6e20 7468   Read more in th
-0000f350: 6520 3a72 6566 3a60 5573 6572 2047 7569  e :ref:`User Gui
-0000f360: 6465 203c 7072 6570 726f 6365 7373 696e  de <preprocessin
-0000f370: 675f 6e6f 726d 616c 697a 6174 696f 6e3e  g_normalization>
-0000f380: 602e 0a0a 2020 2020 5061 7261 6d65 7465  `...    Paramete
-0000f390: 7273 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d  rs.    ---------
-0000f3a0: 2d0a 2020 2020 5820 3a20 7b61 7272 6179  -.    X : {array
-0000f3b0: 2d6c 696b 652c 2073 7061 7273 6520 6d61  -like, sparse ma
-0000f3c0: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
-0000f3d0: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
-0000f3e0: 7475 7265 7329 0a20 2020 2020 2020 2054  tures).        T
-0000f3f0: 6865 2064 6174 6120 746f 206e 6f72 6d61  he data to norma
-0000f400: 6c69 7a65 2c20 656c 656d 656e 7420 6279  lize, element by
-0000f410: 2065 6c65 6d65 6e74 2e0a 2020 2020 2020   element..      
-0000f420: 2020 7363 6970 792e 7370 6172 7365 206d    scipy.sparse m
-0000f430: 6174 7269 6365 7320 7368 6f75 6c64 2062  atrices should b
-0000f440: 6520 696e 2043 5352 2066 6f72 6d61 7420  e in CSR format 
-0000f450: 746f 2061 766f 6964 2061 6e0a 2020 2020  to avoid an.    
-0000f460: 2020 2020 756e 2d6e 6563 6573 7361 7279      un-necessary
-0000f470: 2063 6f70 792e 0a0a 2020 2020 6e6f 726d   copy...    norm
-0000f480: 203a 207b 276c 3127 2c20 276c 3227 2c20   : {'l1', 'l2', 
-0000f490: 276d 6178 277d 2c20 6465 6661 756c 743d  'max'}, default=
-0000f4a0: 276c 3227 0a20 2020 2020 2020 2054 6865  'l2'.        The
-0000f4b0: 206e 6f72 6d20 746f 2075 7365 2074 6f20   norm to use to 
-0000f4c0: 6e6f 726d 616c 697a 6520 6561 6368 206e  normalize each n
-0000f4d0: 6f6e 207a 6572 6f20 7361 6d70 6c65 2028  on zero sample (
-0000f4e0: 6f72 2065 6163 6820 6e6f 6e2d 7a65 726f  or each non-zero
-0000f4f0: 0a20 2020 2020 2020 2066 6561 7475 7265  .        feature
-0000f500: 2069 6620 6178 6973 2069 7320 3029 2e0a   if axis is 0)..
-0000f510: 0a20 2020 2061 7869 7320 3a20 7b30 2c20  .    axis : {0, 
-0000f520: 317d 2c20 6465 6661 756c 743d 310a 2020  1}, default=1.  
-0000f530: 2020 2020 2020 4465 6669 6e65 2061 7869        Define axi
-0000f540: 7320 7573 6564 2074 6f20 6e6f 726d 616c  s used to normal
-0000f550: 697a 6520 7468 6520 6461 7461 2061 6c6f  ize the data alo
-0000f560: 6e67 2e20 4966 2031 2c20 696e 6465 7065  ng. If 1, indepe
-0000f570: 6e64 656e 746c 790a 2020 2020 2020 2020  ndently.        
-0000f580: 6e6f 726d 616c 697a 6520 6561 6368 2073  normalize each s
-0000f590: 616d 706c 652c 206f 7468 6572 7769 7365  ample, otherwise
-0000f5a0: 2028 6966 2030 2920 6e6f 726d 616c 697a   (if 0) normaliz
-0000f5b0: 6520 6561 6368 2066 6561 7475 7265 2e0a  e each feature..
-0000f5c0: 0a20 2020 2063 6f70 7920 3a20 626f 6f6c  .    copy : bool
-0000f5d0: 2c20 6465 6661 756c 743d 5472 7565 0a20  , default=True. 
-0000f5e0: 2020 2020 2020 2049 6620 4661 6c73 652c         If False,
-0000f5f0: 2074 7279 2074 6f20 6176 6f69 6420 6120   try to avoid a 
-0000f600: 636f 7079 2061 6e64 206e 6f72 6d61 6c69  copy and normali
-0000f610: 7a65 2069 6e20 706c 6163 652e 0a20 2020  ze in place..   
-0000f620: 2020 2020 2054 6869 7320 6973 206e 6f74       This is not
-0000f630: 2067 7561 7261 6e74 6565 6420 746f 2061   guaranteed to a
-0000f640: 6c77 6179 7320 776f 726b 2069 6e20 706c  lways work in pl
-0000f650: 6163 653b 2065 2e67 2e20 6966 2074 6865  ace; e.g. if the
-0000f660: 2064 6174 6120 6973 0a20 2020 2020 2020   data is.       
-0000f670: 2061 206e 756d 7079 2061 7272 6179 2077   a numpy array w
-0000f680: 6974 6820 616e 2069 6e74 2064 7479 7065  ith an int dtype
-0000f690: 2c20 6120 636f 7079 2077 696c 6c20 6265  , a copy will be
-0000f6a0: 2072 6574 7572 6e65 6420 6576 656e 2077   returned even w
-0000f6b0: 6974 680a 2020 2020 2020 2020 636f 7079  ith.        copy
-0000f6c0: 3d46 616c 7365 2e0a 0a20 2020 2072 6574  =False...    ret
-0000f6d0: 7572 6e5f 6e6f 726d 203a 2062 6f6f 6c2c  urn_norm : bool,
-0000f6e0: 2064 6566 6175 6c74 3d46 616c 7365 0a20   default=False. 
-0000f6f0: 2020 2020 2020 2057 6865 7468 6572 2074         Whether t
-0000f700: 6f20 7265 7475 726e 2074 6865 2063 6f6d  o return the com
-0000f710: 7075 7465 6420 6e6f 726d 732e 0a0a 2020  puted norms...  
-0000f720: 2020 5265 7475 726e 730a 2020 2020 2d2d    Returns.    --
-0000f730: 2d2d 2d2d 2d0a 2020 2020 5820 3a20 7b6e  -----.    X : {n
-0000f740: 6461 7272 6179 2c20 7370 6172 7365 206d  darray, sparse m
-0000f750: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
-0000f760: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
-0000f770: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-0000f780: 4e6f 726d 616c 697a 6564 2069 6e70 7574  Normalized input
-0000f790: 2058 2e0a 0a20 2020 206e 6f72 6d73 203a   X...    norms :
-0000f7a0: 206e 6461 7272 6179 206f 6620 7368 6170   ndarray of shap
-0000f7b0: 6520 286e 5f73 616d 706c 6573 2c20 2920  e (n_samples, ) 
-0000f7c0: 6966 2061 7869 733d 3120 656c 7365 2028  if axis=1 else (
-0000f7d0: 6e5f 6665 6174 7572 6573 2c20 290a 2020  n_features, ).  
-0000f7e0: 2020 2020 2020 416e 2061 7272 6179 206f        An array o
-0000f7f0: 6620 6e6f 726d 7320 616c 6f6e 6720 6769  f norms along gi
-0000f800: 7665 6e20 6178 6973 2066 6f72 2058 2e0a  ven axis for X..
-0000f810: 2020 2020 2020 2020 5768 656e 2058 2069          When X i
-0000f820: 7320 7370 6172 7365 2c20 6120 4e6f 7449  s sparse, a NotI
-0000f830: 6d70 6c65 6d65 6e74 6564 4572 726f 7220  mplementedError 
-0000f840: 7769 6c6c 2062 6520 7261 6973 6564 0a20  will be raised. 
-0000f850: 2020 2020 2020 2066 6f72 206e 6f72 6d20         for norm 
-0000f860: 276c 3127 206f 7220 276c 3227 2e0a 0a20  'l1' or 'l2'... 
-0000f870: 2020 2053 6565 2041 6c73 6f0a 2020 2020     See Also.    
-0000f880: 2d2d 2d2d 2d2d 2d2d 0a20 2020 204e 6f72  --------.    Nor
-0000f890: 6d61 6c69 7a65 7220 3a20 5065 7266 6f72  malizer : Perfor
-0000f8a0: 6d73 206e 6f72 6d61 6c69 7a61 7469 6f6e  ms normalization
-0000f8b0: 2075 7369 6e67 2074 6865 2054 7261 6e73   using the Trans
-0000f8c0: 666f 726d 6572 2041 5049 0a20 2020 2020  former API.     
-0000f8d0: 2020 2028 652e 672e 2061 7320 7061 7274     (e.g. as part
-0000f8e0: 206f 6620 6120 7072 6570 726f 6365 7373   of a preprocess
-0000f8f0: 696e 6720 3a63 6c61 7373 3a60 7e73 6b6c  ing :class:`~skl
-0000f900: 6561 726e 2e70 6970 656c 696e 652e 5069  earn.pipeline.Pi
-0000f910: 7065 6c69 6e65 6029 2e0a 0a20 2020 204e  peline`)...    N
-0000f920: 6f74 6573 0a20 2020 202d 2d2d 2d2d 0a20  otes.    -----. 
-0000f930: 2020 2046 6f72 2061 2063 6f6d 7061 7269     For a compari
-0000f940: 736f 6e20 6f66 2074 6865 2064 6966 6665  son of the diffe
-0000f950: 7265 6e74 2073 6361 6c65 7273 2c20 7472  rent scalers, tr
-0000f960: 616e 7366 6f72 6d65 7273 2c20 616e 6420  ansformers, and 
-0000f970: 6e6f 726d 616c 697a 6572 732c 0a20 2020  normalizers,.   
-0000f980: 2073 6565 3a20 3a72 6566 3a60 7370 6878   see: :ref:`sphx
-0000f990: 5f67 6c72 5f61 7574 6f5f 6578 616d 706c  _glr_auto_exampl
-0000f9a0: 6573 5f70 7265 7072 6f63 6573 7369 6e67  es_preprocessing
-0000f9b0: 5f70 6c6f 745f 616c 6c5f 7363 616c 696e  _plot_all_scalin
-0000f9c0: 672e 7079 602e 0a0a 2020 2020 4578 616d  g.py`...    Exam
-0000f9d0: 706c 6573 0a20 2020 202d 2d2d 2d2d 2d2d  ples.    -------
-0000f9e0: 2d0a 2020 2020 3e3e 3e20 6672 6f6d 2073  -.    >>> from s
-0000f9f0: 6b6c 6561 726e 2e70 7265 7072 6f63 6573  klearn.preproces
-0000fa00: 7369 6e67 2069 6d70 6f72 7420 6e6f 726d  sing import norm
-0000fa10: 616c 697a 650a 2020 2020 3e3e 3e20 5820  alize.    >>> X 
-0000fa20: 3d20 5b5b 2d32 2c20 312c 2032 5d2c 205b  = [[-2, 1, 2], [
-0000fa30: 2d31 2c20 302c 2031 5d5d 0a20 2020 203e  -1, 0, 1]].    >
-0000fa40: 3e3e 206e 6f72 6d61 6c69 7a65 2858 2c20  >> normalize(X, 
-0000fa50: 6e6f 726d 3d22 6c31 2229 2020 2320 4c31  norm="l1")  # L1
-0000fa60: 206e 6f72 6d61 6c69 7a61 7469 6f6e 2065   normalization e
-0000fa70: 6163 6820 726f 7720 696e 6465 7065 6e64  ach row independ
-0000fa80: 656e 746c 790a 2020 2020 6172 7261 7928  ently.    array(
-0000fa90: 5b5b 2d30 2e34 2c20 2030 2e32 2c20 2030  [[-0.4,  0.2,  0
-0000faa0: 2e34 5d2c 0a20 2020 2020 2020 2020 2020  .4],.           
-0000fab0: 5b2d 302e 352c 2020 302e 202c 2020 302e  [-0.5,  0. ,  0.
-0000fac0: 355d 5d29 0a20 2020 203e 3e3e 206e 6f72  5]]).    >>> nor
-0000fad0: 6d61 6c69 7a65 2858 2c20 6e6f 726d 3d22  malize(X, norm="
-0000fae0: 6c32 2229 2020 2320 4c32 206e 6f72 6d61  l2")  # L2 norma
-0000faf0: 6c69 7a61 7469 6f6e 2065 6163 6820 726f  lization each ro
-0000fb00: 7720 696e 6465 7065 6e64 656e 746c 790a  w independently.
-0000fb10: 2020 2020 6172 7261 7928 5b5b 2d30 2e36      array([[-0.6
-0000fb20: 362e 2e2e 2c20 2030 2e33 332e 2e2e 2c20  6...,  0.33..., 
-0000fb30: 2030 2e36 362e 2e2e 5d2c 0a20 2020 2020   0.66...],.     
-0000fb40: 2020 2020 2020 5b2d 302e 3730 2e2e 2e2c        [-0.70...,
-0000fb50: 2020 302e 2020 2020 202c 2020 302e 3730    0.     ,  0.70
-0000fb60: 2e2e 2e5d 5d29 0a20 2020 2022 2222 0a20  ...]]).    """. 
-0000fb70: 2020 2069 6620 6178 6973 203d 3d20 303a     if axis == 0:
-0000fb80: 0a20 2020 2020 2020 2073 7061 7273 655f  .        sparse_
-0000fb90: 666f 726d 6174 203d 2022 6373 6322 0a20  format = "csc". 
-0000fba0: 2020 2065 6c73 653a 2020 2320 6178 6973     else:  # axis
-0000fbb0: 203d 3d20 313a 0a20 2020 2020 2020 2073   == 1:.        s
-0000fbc0: 7061 7273 655f 666f 726d 6174 203d 2022  parse_format = "
-0000fbd0: 6373 7222 0a0a 2020 2020 7870 2c20 5f20  csr"..    xp, _ 
-0000fbe0: 3d20 6765 745f 6e61 6d65 7370 6163 6528  = get_namespace(
-0000fbf0: 5829 0a0a 2020 2020 5820 3d20 6368 6563  X)..    X = chec
-0000fc00: 6b5f 6172 7261 7928 0a20 2020 2020 2020  k_array(.       
-0000fc10: 2058 2c0a 2020 2020 2020 2020 6163 6365   X,.        acce
-0000fc20: 7074 5f73 7061 7273 653d 7370 6172 7365  pt_sparse=sparse
-0000fc30: 5f66 6f72 6d61 742c 0a20 2020 2020 2020  _format,.       
-0000fc40: 2063 6f70 793d 636f 7079 2c0a 2020 2020   copy=copy,.    
-0000fc50: 2020 2020 6573 7469 6d61 746f 723d 2274      estimator="t
-0000fc60: 6865 206e 6f72 6d61 6c69 7a65 2066 756e  he normalize fun
-0000fc70: 6374 696f 6e22 2c0a 2020 2020 2020 2020  ction",.        
-0000fc80: 6474 7970 653d 5f61 7272 6179 5f61 7069  dtype=_array_api
-0000fc90: 2e73 7570 706f 7274 6564 5f66 6c6f 6174  .supported_float
-0000fca0: 5f64 7479 7065 7328 7870 292c 0a20 2020  _dtypes(xp),.   
-0000fcb0: 2029 0a20 2020 2069 6620 6178 6973 203d   ).    if axis =
-0000fcc0: 3d20 303a 0a20 2020 2020 2020 2058 203d  = 0:.        X =
-0000fcd0: 2058 2e54 0a0a 2020 2020 6966 2073 7061   X.T..    if spa
-0000fce0: 7273 652e 6973 7370 6172 7365 2858 293a  rse.issparse(X):
-0000fcf0: 0a20 2020 2020 2020 2069 6620 7265 7475  .        if retu
-0000fd00: 726e 5f6e 6f72 6d20 616e 6420 6e6f 726d  rn_norm and norm
-0000fd10: 2069 6e20 2822 6c31 222c 2022 6c32 2229   in ("l1", "l2")
-0000fd20: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-0000fd30: 6973 6520 4e6f 7449 6d70 6c65 6d65 6e74  ise NotImplement
-0000fd40: 6564 4572 726f 7228 0a20 2020 2020 2020  edError(.       
-0000fd50: 2020 2020 2020 2020 2022 7265 7475 726e           "return
-0000fd60: 5f6e 6f72 6d3d 5472 7565 2069 7320 6e6f  _norm=True is no
-0000fd70: 7420 696d 706c 656d 656e 7465 6420 220a  t implemented ".
-0000fd80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0000fd90: 2266 6f72 2073 7061 7273 6520 6d61 7472  "for sparse matr
-0000fda0: 6963 6573 2077 6974 6820 6e6f 726d 2027  ices with norm '
-0000fdb0: 6c31 2720 220a 2020 2020 2020 2020 2020  l1' ".          
-0000fdc0: 2020 2020 2020 226f 7220 6e6f 726d 2027        "or norm '
-0000fdd0: 6c32 2722 0a20 2020 2020 2020 2020 2020  l2'".           
-0000fde0: 2029 0a20 2020 2020 2020 2069 6620 6e6f   ).        if no
-0000fdf0: 726d 203d 3d20 226c 3122 3a0a 2020 2020  rm == "l1":.    
-0000fe00: 2020 2020 2020 2020 696e 706c 6163 655f          inplace_
-0000fe10: 6373 725f 726f 775f 6e6f 726d 616c 697a  csr_row_normaliz
-0000fe20: 655f 6c31 2858 290a 2020 2020 2020 2020  e_l1(X).        
-0000fe30: 656c 6966 206e 6f72 6d20 3d3d 2022 6c32  elif norm == "l2
-0000fe40: 223a 0a20 2020 2020 2020 2020 2020 2069  ":.            i
-0000fe50: 6e70 6c61 6365 5f63 7372 5f72 6f77 5f6e  nplace_csr_row_n
-0000fe60: 6f72 6d61 6c69 7a65 5f6c 3228 5829 0a20  ormalize_l2(X). 
-0000fe70: 2020 2020 2020 2065 6c69 6620 6e6f 726d         elif norm
-0000fe80: 203d 3d20 226d 6178 223a 0a20 2020 2020   == "max":.     
-0000fe90: 2020 2020 2020 206d 696e 732c 206d 6178         mins, max
-0000fea0: 6573 203d 206d 696e 5f6d 6178 5f61 7869  es = min_max_axi
-0000feb0: 7328 582c 2031 290a 2020 2020 2020 2020  s(X, 1).        
-0000fec0: 2020 2020 6e6f 726d 7320 3d20 6e70 2e6d      norms = np.m
-0000fed0: 6178 696d 756d 2861 6273 286d 696e 7329  aximum(abs(mins)
-0000fee0: 2c20 6d61 7865 7329 0a20 2020 2020 2020  , maxes).       
-0000fef0: 2020 2020 206e 6f72 6d73 5f65 6c65 6d65       norms_eleme
-0000ff00: 6e74 7769 7365 203d 206e 6f72 6d73 2e72  ntwise = norms.r
-0000ff10: 6570 6561 7428 6e70 2e64 6966 6628 582e  epeat(np.diff(X.
-0000ff20: 696e 6470 7472 2929 0a20 2020 2020 2020  indptr)).       
-0000ff30: 2020 2020 206d 6173 6b20 3d20 6e6f 726d       mask = norm
-0000ff40: 735f 656c 656d 656e 7477 6973 6520 213d  s_elementwise !=
-0000ff50: 2030 0a20 2020 2020 2020 2020 2020 2058   0.            X
-0000ff60: 2e64 6174 615b 6d61 736b 5d20 2f3d 206e  .data[mask] /= n
-0000ff70: 6f72 6d73 5f65 6c65 6d65 6e74 7769 7365  orms_elementwise
-0000ff80: 5b6d 6173 6b5d 0a20 2020 2065 6c73 653a  [mask].    else:
-0000ff90: 0a20 2020 2020 2020 2069 6620 6e6f 726d  .        if norm
-0000ffa0: 203d 3d20 226c 3122 3a0a 2020 2020 2020   == "l1":.      
-0000ffb0: 2020 2020 2020 6e6f 726d 7320 3d20 7870        norms = xp
-0000ffc0: 2e73 756d 2878 702e 6162 7328 5829 2c20  .sum(xp.abs(X), 
-0000ffd0: 6178 6973 3d31 290a 2020 2020 2020 2020  axis=1).        
-0000ffe0: 656c 6966 206e 6f72 6d20 3d3d 2022 6c32  elif norm == "l2
-0000fff0: 223a 0a20 2020 2020 2020 2020 2020 206e  ":.            n
-00010000: 6f72 6d73 203d 2072 6f77 5f6e 6f72 6d73  orms = row_norms
-00010010: 2858 290a 2020 2020 2020 2020 656c 6966  (X).        elif
-00010020: 206e 6f72 6d20 3d3d 2022 6d61 7822 3a0a   norm == "max":.
-00010030: 2020 2020 2020 2020 2020 2020 6e6f 726d              norm
-00010040: 7320 3d20 7870 2e6d 6178 2878 702e 6162  s = xp.max(xp.ab
-00010050: 7328 5829 2c20 6178 6973 3d31 290a 2020  s(X), axis=1).  
-00010060: 2020 2020 2020 6e6f 726d 7320 3d20 5f68        norms = _h
-00010070: 616e 646c 655f 7a65 726f 735f 696e 5f73  andle_zeros_in_s
-00010080: 6361 6c65 286e 6f72 6d73 2c20 636f 7079  cale(norms, copy
-00010090: 3d46 616c 7365 290a 2020 2020 2020 2020  =False).        
-000100a0: 5820 2f3d 206e 6f72 6d73 5b3a 2c20 4e6f  X /= norms[:, No
-000100b0: 6e65 5d0a 0a20 2020 2069 6620 6178 6973  ne]..    if axis
-000100c0: 203d 3d20 303a 0a20 2020 2020 2020 2058   == 0:.        X
-000100d0: 203d 2058 2e54 0a0a 2020 2020 6966 2072   = X.T..    if r
-000100e0: 6574 7572 6e5f 6e6f 726d 3a0a 2020 2020  eturn_norm:.    
-000100f0: 2020 2020 7265 7475 726e 2058 2c20 6e6f      return X, no
-00010100: 726d 730a 2020 2020 656c 7365 3a0a 2020  rms.    else:.  
-00010110: 2020 2020 2020 7265 7475 726e 2058 0a0a        return X..
-00010120: 0a63 6c61 7373 204e 6f72 6d61 6c69 7a65  .class Normalize
-00010130: 7228 4f6e 6554 6f4f 6e65 4665 6174 7572  r(OneToOneFeatur
-00010140: 654d 6978 696e 2c20 5472 616e 7366 6f72  eMixin, Transfor
-00010150: 6d65 724d 6978 696e 2c20 4261 7365 4573  merMixin, BaseEs
-00010160: 7469 6d61 746f 7229 3a0a 2020 2020 2222  timator):.    ""
-00010170: 224e 6f72 6d61 6c69 7a65 2073 616d 706c  "Normalize sampl
-00010180: 6573 2069 6e64 6976 6964 7561 6c6c 7920  es individually 
-00010190: 746f 2075 6e69 7420 6e6f 726d 2e0a 0a20  to unit norm... 
-000101a0: 2020 2045 6163 6820 7361 6d70 6c65 2028     Each sample (
-000101b0: 692e 652e 2065 6163 6820 726f 7720 6f66  i.e. each row of
-000101c0: 2074 6865 2064 6174 6120 6d61 7472 6978   the data matrix
-000101d0: 2920 7769 7468 2061 7420 6c65 6173 7420  ) with at least 
-000101e0: 6f6e 650a 2020 2020 6e6f 6e20 7a65 726f  one.    non zero
-000101f0: 2063 6f6d 706f 6e65 6e74 2069 7320 7265   component is re
-00010200: 7363 616c 6564 2069 6e64 6570 656e 6465  scaled independe
-00010210: 6e74 6c79 206f 6620 6f74 6865 7220 7361  ntly of other sa
-00010220: 6d70 6c65 7320 736f 0a20 2020 2074 6861  mples so.    tha
-00010230: 7420 6974 7320 6e6f 726d 2028 6c31 2c20  t its norm (l1, 
-00010240: 6c32 206f 7220 696e 6629 2065 7175 616c  l2 or inf) equal
-00010250: 7320 6f6e 652e 0a0a 2020 2020 5468 6973  s one...    This
-00010260: 2074 7261 6e73 666f 726d 6572 2069 7320   transformer is 
-00010270: 6162 6c65 2074 6f20 776f 726b 2062 6f74  able to work bot
-00010280: 6820 7769 7468 2064 656e 7365 206e 756d  h with dense num
-00010290: 7079 2061 7272 6179 7320 616e 640a 2020  py arrays and.  
-000102a0: 2020 7363 6970 792e 7370 6172 7365 206d    scipy.sparse m
-000102b0: 6174 7269 7820 2875 7365 2043 5352 2066  atrix (use CSR f
-000102c0: 6f72 6d61 7420 6966 2079 6f75 2077 616e  ormat if you wan
-000102d0: 7420 746f 2061 766f 6964 2074 6865 2062  t to avoid the b
-000102e0: 7572 6465 6e20 6f66 0a20 2020 2061 2063  urden of.    a c
-000102f0: 6f70 7920 2f20 636f 6e76 6572 7369 6f6e  opy / conversion
-00010300: 292e 0a0a 2020 2020 5363 616c 696e 6720  )...    Scaling 
-00010310: 696e 7075 7473 2074 6f20 756e 6974 206e  inputs to unit n
-00010320: 6f72 6d73 2069 7320 6120 636f 6d6d 6f6e  orms is a common
-00010330: 206f 7065 7261 7469 6f6e 2066 6f72 2074   operation for t
-00010340: 6578 740a 2020 2020 636c 6173 7369 6669  ext.    classifi
-00010350: 6361 7469 6f6e 206f 7220 636c 7573 7465  cation or cluste
-00010360: 7269 6e67 2066 6f72 2069 6e73 7461 6e63  ring for instanc
-00010370: 652e 2046 6f72 2069 6e73 7461 6e63 6520  e. For instance 
-00010380: 7468 6520 646f 740a 2020 2020 7072 6f64  the dot.    prod
-00010390: 7563 7420 6f66 2074 776f 206c 322d 6e6f  uct of two l2-no
-000103a0: 726d 616c 697a 6564 2054 462d 4944 4620  rmalized TF-IDF 
-000103b0: 7665 6374 6f72 7320 6973 2074 6865 2063  vectors is the c
-000103c0: 6f73 696e 6520 7369 6d69 6c61 7269 7479  osine similarity
-000103d0: 0a20 2020 206f 6620 7468 6520 7665 6374  .    of the vect
-000103e0: 6f72 7320 616e 6420 6973 2074 6865 2062  ors and is the b
-000103f0: 6173 6520 7369 6d69 6c61 7269 7479 206d  ase similarity m
-00010400: 6574 7269 6320 666f 7220 7468 6520 5665  etric for the Ve
-00010410: 6374 6f72 0a20 2020 2053 7061 6365 204d  ctor.    Space M
-00010420: 6f64 656c 2063 6f6d 6d6f 6e6c 7920 7573  odel commonly us
-00010430: 6564 2062 7920 7468 6520 496e 666f 726d  ed by the Inform
-00010440: 6174 696f 6e20 5265 7472 6965 7661 6c20  ation Retrieval 
-00010450: 636f 6d6d 756e 6974 792e 0a0a 2020 2020  community...    
-00010460: 466f 7220 616e 2065 7861 6d70 6c65 2076  For an example v
-00010470: 6973 7561 6c69 7a61 7469 6f6e 2c20 7265  isualization, re
-00010480: 6665 7220 746f 203a 7265 663a 6043 6f6d  fer to :ref:`Com
-00010490: 7061 7265 204e 6f72 6d61 6c69 7a65 7220  pare Normalizer 
-000104a0: 7769 7468 206f 7468 6572 0a20 2020 2073  with other.    s
-000104b0: 6361 6c65 7273 203c 706c 6f74 5f61 6c6c  calers <plot_all
-000104c0: 5f73 6361 6c69 6e67 5f6e 6f72 6d61 6c69  _scaling_normali
-000104d0: 7a65 725f 7365 6374 696f 6e3e 602e 0a0a  zer_section>`...
-000104e0: 2020 2020 5265 6164 206d 6f72 6520 696e      Read more in
-000104f0: 2074 6865 203a 7265 663a 6055 7365 7220   the :ref:`User 
-00010500: 4775 6964 6520 3c70 7265 7072 6f63 6573  Guide <preproces
-00010510: 7369 6e67 5f6e 6f72 6d61 6c69 7a61 7469  sing_normalizati
-00010520: 6f6e 3e60 2e0a 0a20 2020 2050 6172 616d  on>`...    Param
-00010530: 6574 6572 730a 2020 2020 2d2d 2d2d 2d2d  eters.    ------
-00010540: 2d2d 2d2d 0a20 2020 206e 6f72 6d20 3a20  ----.    norm : 
-00010550: 7b27 6c31 272c 2027 6c32 272c 2027 6d61  {'l1', 'l2', 'ma
-00010560: 7827 7d2c 2064 6566 6175 6c74 3d27 6c32  x'}, default='l2
-00010570: 270a 2020 2020 2020 2020 5468 6520 6e6f  '.        The no
-00010580: 726d 2074 6f20 7573 6520 746f 206e 6f72  rm to use to nor
-00010590: 6d61 6c69 7a65 2065 6163 6820 6e6f 6e20  malize each non 
-000105a0: 7a65 726f 2073 616d 706c 652e 2049 6620  zero sample. If 
-000105b0: 6e6f 726d 3d27 6d61 7827 0a20 2020 2020  norm='max'.     
-000105c0: 2020 2069 7320 7573 6564 2c20 7661 6c75     is used, valu
-000105d0: 6573 2077 696c 6c20 6265 2072 6573 6361  es will be resca
-000105e0: 6c65 6420 6279 2074 6865 206d 6178 696d  led by the maxim
-000105f0: 756d 206f 6620 7468 6520 6162 736f 6c75  um of the absolu
-00010600: 7465 0a20 2020 2020 2020 2076 616c 7565  te.        value
-00010610: 732e 0a0a 2020 2020 636f 7079 203a 2062  s...    copy : b
-00010620: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
-00010630: 650a 2020 2020 2020 2020 5365 7420 746f  e.        Set to
-00010640: 2046 616c 7365 2074 6f20 7065 7266 6f72   False to perfor
-00010650: 6d20 696e 706c 6163 6520 726f 7720 6e6f  m inplace row no
-00010660: 726d 616c 697a 6174 696f 6e20 616e 6420  rmalization and 
-00010670: 6176 6f69 6420 610a 2020 2020 2020 2020  avoid a.        
-00010680: 636f 7079 2028 6966 2074 6865 2069 6e70  copy (if the inp
-00010690: 7574 2069 7320 616c 7265 6164 7920 6120  ut is already a 
-000106a0: 6e75 6d70 7920 6172 7261 7920 6f72 2061  numpy array or a
-000106b0: 2073 6369 7079 2e73 7061 7273 650a 2020   scipy.sparse.  
-000106c0: 2020 2020 2020 4353 5220 6d61 7472 6978        CSR matrix
-000106d0: 292e 0a0a 2020 2020 4174 7472 6962 7574  )...    Attribut
-000106e0: 6573 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d  es.    ---------
-000106f0: 2d0a 2020 2020 6e5f 6665 6174 7572 6573  -.    n_features
-00010700: 5f69 6e5f 203a 2069 6e74 0a20 2020 2020  _in_ : int.     
-00010710: 2020 204e 756d 6265 7220 6f66 2066 6561     Number of fea
-00010720: 7475 7265 7320 7365 656e 2064 7572 696e  tures seen durin
-00010730: 6720 3a74 6572 6d3a 6066 6974 602e 0a0a  g :term:`fit`...
-00010740: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
-00010750: 6f6e 6164 6465 643a 3a20 302e 3234 0a0a  onadded:: 0.24..
-00010760: 2020 2020 6665 6174 7572 655f 6e61 6d65      feature_name
-00010770: 735f 696e 5f20 3a20 6e64 6172 7261 7920  s_in_ : ndarray 
-00010780: 6f66 2073 6861 7065 2028 606e 5f66 6561  of shape (`n_fea
-00010790: 7475 7265 735f 696e 5f60 2c29 0a20 2020  tures_in_`,).   
-000107a0: 2020 2020 204e 616d 6573 206f 6620 6665       Names of fe
-000107b0: 6174 7572 6573 2073 6565 6e20 6475 7269  atures seen duri
-000107c0: 6e67 203a 7465 726d 3a60 6669 7460 2e20  ng :term:`fit`. 
-000107d0: 4465 6669 6e65 6420 6f6e 6c79 2077 6865  Defined only whe
-000107e0: 6e20 6058 600a 2020 2020 2020 2020 6861  n `X`.        ha
-000107f0: 7320 6665 6174 7572 6520 6e61 6d65 7320  s feature names 
-00010800: 7468 6174 2061 7265 2061 6c6c 2073 7472  that are all str
-00010810: 696e 6773 2e0a 0a20 2020 2020 2020 202e  ings...        .
-00010820: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
-00010830: 2031 2e30 0a0a 2020 2020 5365 6520 416c   1.0..    See Al
-00010840: 736f 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a  so.    --------.
-00010850: 2020 2020 6e6f 726d 616c 697a 6520 3a20      normalize : 
-00010860: 4571 7569 7661 6c65 6e74 2066 756e 6374  Equivalent funct
-00010870: 696f 6e20 7769 7468 6f75 7420 7468 6520  ion without the 
-00010880: 6573 7469 6d61 746f 7220 4150 492e 0a0a  estimator API...
-00010890: 2020 2020 4e6f 7465 730a 2020 2020 2d2d      Notes.    --
-000108a0: 2d2d 2d0a 2020 2020 5468 6973 2065 7374  ---.    This est
-000108b0: 696d 6174 6f72 2069 7320 3a74 6572 6d3a  imator is :term:
-000108c0: 6073 7461 7465 6c65 7373 6020 616e 6420  `stateless` and 
-000108d0: 646f 6573 206e 6f74 206e 6565 6420 746f  does not need to
-000108e0: 2062 6520 6669 7474 6564 2e0a 2020 2020   be fitted..    
-000108f0: 486f 7765 7665 722c 2077 6520 7265 636f  However, we reco
-00010900: 6d6d 656e 6420 746f 2063 616c 6c20 3a6d  mmend to call :m
-00010910: 6574 683a 6066 6974 5f74 7261 6e73 666f  eth:`fit_transfo
-00010920: 726d 6020 696e 7374 6561 6420 6f66 0a20  rm` instead of. 
-00010930: 2020 203a 6d65 7468 3a60 7472 616e 7366     :meth:`transf
-00010940: 6f72 6d60 2c20 6173 2070 6172 616d 6574  orm`, as paramet
-00010950: 6572 2076 616c 6964 6174 696f 6e20 6973  er validation is
-00010960: 206f 6e6c 7920 7065 7266 6f72 6d65 6420   only performed 
-00010970: 696e 0a20 2020 203a 6d65 7468 3a60 6669  in.    :meth:`fi
-00010980: 7460 2e0a 0a20 2020 2045 7861 6d70 6c65  t`...    Example
-00010990: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20  s.    --------. 
-000109a0: 2020 203e 3e3e 2066 726f 6d20 736b 6c65     >>> from skle
-000109b0: 6172 6e2e 7072 6570 726f 6365 7373 696e  arn.preprocessin
-000109c0: 6720 696d 706f 7274 204e 6f72 6d61 6c69  g import Normali
-000109d0: 7a65 720a 2020 2020 3e3e 3e20 5820 3d20  zer.    >>> X = 
-000109e0: 5b5b 342c 2031 2c20 322c 2032 5d2c 0a20  [[4, 1, 2, 2],. 
-000109f0: 2020 202e 2e2e 2020 2020 2020 5b31 2c20     ...      [1, 
-00010a00: 332c 2039 2c20 335d 2c0a 2020 2020 2e2e  3, 9, 3],.    ..
-00010a10: 2e20 2020 2020 205b 352c 2037 2c20 352c  .      [5, 7, 5,
-00010a20: 2031 5d5d 0a20 2020 203e 3e3e 2074 7261   1]].    >>> tra
-00010a30: 6e73 666f 726d 6572 203d 204e 6f72 6d61  nsformer = Norma
-00010a40: 6c69 7a65 7228 292e 6669 7428 5829 2020  lizer().fit(X)  
-00010a50: 2320 6669 7420 646f 6573 206e 6f74 6869  # fit does nothi
-00010a60: 6e67 2e0a 2020 2020 3e3e 3e20 7472 616e  ng..    >>> tran
-00010a70: 7366 6f72 6d65 720a 2020 2020 4e6f 726d  sformer.    Norm
-00010a80: 616c 697a 6572 2829 0a20 2020 203e 3e3e  alizer().    >>>
-00010a90: 2074 7261 6e73 666f 726d 6572 2e74 7261   transformer.tra
-00010aa0: 6e73 666f 726d 2858 290a 2020 2020 6172  nsform(X).    ar
-00010ab0: 7261 7928 5b5b 302e 382c 2030 2e32 2c20  ray([[0.8, 0.2, 
-00010ac0: 302e 342c 2030 2e34 5d2c 0a20 2020 2020  0.4, 0.4],.     
-00010ad0: 2020 2020 2020 5b30 2e31 2c20 302e 332c        [0.1, 0.3,
-00010ae0: 2030 2e39 2c20 302e 335d 2c0a 2020 2020   0.9, 0.3],.    
-00010af0: 2020 2020 2020 205b 302e 352c 2030 2e37         [0.5, 0.7
-00010b00: 2c20 302e 352c 2030 2e31 5d5d 290a 2020  , 0.5, 0.1]]).  
-00010b10: 2020 2222 220a 0a20 2020 205f 7061 7261    """..    _para
-00010b20: 6d65 7465 725f 636f 6e73 7472 6169 6e74  meter_constraint
-00010b30: 733a 2064 6963 7420 3d20 7b0a 2020 2020  s: dict = {.    
-00010b40: 2020 2020 226e 6f72 6d22 3a20 5b53 7472      "norm": [Str
-00010b50: 4f70 7469 6f6e 7328 7b22 6c31 222c 2022  Options({"l1", "
-00010b60: 6c32 222c 2022 6d61 7822 7d29 5d2c 0a20  l2", "max"})],. 
-00010b70: 2020 2020 2020 2022 636f 7079 223a 205b         "copy": [
-00010b80: 2262 6f6f 6c65 616e 225d 2c0a 2020 2020  "boolean"],.    
-00010b90: 7d0a 0a20 2020 2064 6566 205f 5f69 6e69  }..    def __ini
-00010ba0: 745f 5f28 7365 6c66 2c20 6e6f 726d 3d22  t__(self, norm="
-00010bb0: 6c32 222c 202a 2c20 636f 7079 3d54 7275  l2", *, copy=Tru
-00010bc0: 6529 3a0a 2020 2020 2020 2020 7365 6c66  e):.        self
-00010bd0: 2e6e 6f72 6d20 3d20 6e6f 726d 0a20 2020  .norm = norm.   
-00010be0: 2020 2020 2073 656c 662e 636f 7079 203d       self.copy =
-00010bf0: 2063 6f70 790a 0a20 2020 2040 5f66 6974   copy..    @_fit
-00010c00: 5f63 6f6e 7465 7874 2870 7265 6665 725f  _context(prefer_
-00010c10: 736b 6970 5f6e 6573 7465 645f 7661 6c69  skip_nested_vali
-00010c20: 6461 7469 6f6e 3d54 7275 6529 0a20 2020  dation=True).   
-00010c30: 2064 6566 2066 6974 2873 656c 662c 2058   def fit(self, X
-00010c40: 2c20 793d 4e6f 6e65 293a 0a20 2020 2020  , y=None):.     
-00010c50: 2020 2022 2222 4f6e 6c79 2076 616c 6964     """Only valid
-00010c60: 6174 6573 2065 7374 696d 6174 6f72 2773  ates estimator's
-00010c70: 2070 6172 616d 6574 6572 732e 0a0a 2020   parameters...  
-00010c80: 2020 2020 2020 5468 6973 206d 6574 686f        This metho
-00010c90: 6420 616c 6c6f 7773 2074 6f3a 2028 6929  d allows to: (i)
-00010ca0: 2076 616c 6964 6174 6520 7468 6520 6573   validate the es
-00010cb0: 7469 6d61 746f 7227 7320 7061 7261 6d65  timator's parame
-00010cc0: 7465 7273 2061 6e64 0a20 2020 2020 2020  ters and.       
-00010cd0: 2028 6969 2920 6265 2063 6f6e 7369 7374   (ii) be consist
-00010ce0: 656e 7420 7769 7468 2074 6865 2073 6369  ent with the sci
-00010cf0: 6b69 742d 6c65 6172 6e20 7472 616e 7366  kit-learn transf
-00010d00: 6f72 6d65 7220 4150 492e 0a0a 2020 2020  ormer API...    
-00010d10: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
-00010d20: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
-00010d30: 2d0a 2020 2020 2020 2020 5820 3a20 7b61  -.        X : {a
-00010d40: 7272 6179 2d6c 696b 652c 2073 7061 7273  rray-like, spars
-00010d50: 6520 6d61 7472 6978 7d20 6f66 2073 6861  e matrix} of sha
-00010d60: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
-00010d70: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
-00010d80: 2020 2020 2020 2054 6865 2064 6174 6120         The data 
-00010d90: 746f 2065 7374 696d 6174 6520 7468 6520  to estimate the 
-00010da0: 6e6f 726d 616c 697a 6174 696f 6e20 7061  normalization pa
-00010db0: 7261 6d65 7465 7273 2e0a 0a20 2020 2020  rameters...     
-00010dc0: 2020 2079 203a 2049 676e 6f72 6564 0a20     y : Ignored. 
-00010dd0: 2020 2020 2020 2020 2020 204e 6f74 2075             Not u
-00010de0: 7365 642c 2070 7265 7365 6e74 2068 6572  sed, present her
-00010df0: 6520 666f 7220 4150 4920 636f 6e73 6973  e for API consis
-00010e00: 7465 6e63 7920 6279 2063 6f6e 7665 6e74  tency by convent
-00010e10: 696f 6e2e 0a0a 2020 2020 2020 2020 5265  ion...        Re
-00010e20: 7475 726e 730a 2020 2020 2020 2020 2d2d  turns.        --
-00010e30: 2d2d 2d2d 2d0a 2020 2020 2020 2020 7365  -----.        se
-00010e40: 6c66 203a 206f 626a 6563 740a 2020 2020  lf : object.    
-00010e50: 2020 2020 2020 2020 4669 7474 6564 2074          Fitted t
-00010e60: 7261 6e73 666f 726d 6572 2e0a 2020 2020  ransformer..    
-00010e70: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-00010e80: 7365 6c66 2e5f 7661 6c69 6461 7465 5f64  self._validate_d
-00010e90: 6174 6128 582c 2061 6363 6570 745f 7370  ata(X, accept_sp
-00010ea0: 6172 7365 3d22 6373 7222 290a 2020 2020  arse="csr").    
-00010eb0: 2020 2020 7265 7475 726e 2073 656c 660a      return self.
-00010ec0: 0a20 2020 2064 6566 2074 7261 6e73 666f  .    def transfo
-00010ed0: 726d 2873 656c 662c 2058 2c20 636f 7079  rm(self, X, copy
-00010ee0: 3d4e 6f6e 6529 3a0a 2020 2020 2020 2020  =None):.        
-00010ef0: 2222 2253 6361 6c65 2065 6163 6820 6e6f  """Scale each no
-00010f00: 6e20 7a65 726f 2072 6f77 206f 6620 5820  n zero row of X 
-00010f10: 746f 2075 6e69 7420 6e6f 726d 2e0a 0a20  to unit norm... 
-00010f20: 2020 2020 2020 2050 6172 616d 6574 6572         Parameter
-00010f30: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
-00010f40: 2d2d 2d2d 0a20 2020 2020 2020 2058 203a  ----.        X :
-00010f50: 207b 6172 7261 792d 6c69 6b65 2c20 7370   {array-like, sp
-00010f60: 6172 7365 206d 6174 7269 787d 206f 6620  arse matrix} of 
-00010f70: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
-00010f80: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
-00010f90: 2020 2020 2020 2020 2020 5468 6520 6461            The da
-00010fa0: 7461 2074 6f20 6e6f 726d 616c 697a 652c  ta to normalize,
-00010fb0: 2072 6f77 2062 7920 726f 772e 2073 6369   row by row. sci
-00010fc0: 7079 2e73 7061 7273 6520 6d61 7472 6963  py.sparse matric
-00010fd0: 6573 2073 686f 756c 6420 6265 0a20 2020  es should be.   
-00010fe0: 2020 2020 2020 2020 2069 6e20 4353 5220           in CSR 
-00010ff0: 666f 726d 6174 2074 6f20 6176 6f69 6420  format to avoid 
-00011000: 616e 2075 6e2d 6e65 6365 7373 6172 7920  an un-necessary 
-00011010: 636f 7079 2e0a 0a20 2020 2020 2020 2063  copy...        c
-00011020: 6f70 7920 3a20 626f 6f6c 2c20 6465 6661  opy : bool, defa
-00011030: 756c 743d 4e6f 6e65 0a20 2020 2020 2020  ult=None.       
-00011040: 2020 2020 2043 6f70 7920 7468 6520 696e       Copy the in
-00011050: 7075 7420 5820 6f72 206e 6f74 2e0a 0a20  put X or not... 
-00011060: 2020 2020 2020 2052 6574 7572 6e73 0a20         Returns. 
-00011070: 2020 2020 2020 202d 2d2d 2d2d 2d2d 0a20         -------. 
-00011080: 2020 2020 2020 2058 5f74 7220 3a20 7b6e         X_tr : {n
-00011090: 6461 7272 6179 2c20 7370 6172 7365 206d  darray, sparse m
-000110a0: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
-000110b0: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
-000110c0: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-000110d0: 2020 2020 5472 616e 7366 6f72 6d65 6420      Transformed 
-000110e0: 6172 7261 792e 0a20 2020 2020 2020 2022  array..        "
-000110f0: 2222 0a20 2020 2020 2020 2063 6f70 7920  "".        copy 
-00011100: 3d20 636f 7079 2069 6620 636f 7079 2069  = copy if copy i
-00011110: 7320 6e6f 7420 4e6f 6e65 2065 6c73 6520  s not None else 
-00011120: 7365 6c66 2e63 6f70 790a 2020 2020 2020  self.copy.      
-00011130: 2020 5820 3d20 7365 6c66 2e5f 7661 6c69    X = self._vali
-00011140: 6461 7465 5f64 6174 6128 582c 2061 6363  date_data(X, acc
-00011150: 6570 745f 7370 6172 7365 3d22 6373 7222  ept_sparse="csr"
-00011160: 2c20 7265 7365 743d 4661 6c73 6529 0a20  , reset=False). 
-00011170: 2020 2020 2020 2072 6574 7572 6e20 6e6f         return no
-00011180: 726d 616c 697a 6528 582c 206e 6f72 6d3d  rmalize(X, norm=
-00011190: 7365 6c66 2e6e 6f72 6d2c 2061 7869 733d  self.norm, axis=
-000111a0: 312c 2063 6f70 793d 636f 7079 290a 0a20  1, copy=copy).. 
-000111b0: 2020 2064 6566 205f 6d6f 7265 5f74 6167     def _more_tag
-000111c0: 7328 7365 6c66 293a 0a20 2020 2020 2020  s(self):.       
-000111d0: 2072 6574 7572 6e20 7b22 7374 6174 656c   return {"statel
-000111e0: 6573 7322 3a20 5472 7565 2c20 2261 7272  ess": True, "arr
-000111f0: 6179 5f61 7069 5f73 7570 706f 7274 223a  ay_api_support":
-00011200: 2054 7275 657d 0a0a 0a40 7661 6c69 6461   True}...@valida
-00011210: 7465 5f70 6172 616d 7328 0a20 2020 207b  te_params(.    {
-00011220: 0a20 2020 2020 2020 2022 5822 3a20 5b22  .        "X": ["
-00011230: 6172 7261 792d 6c69 6b65 222c 2022 7370  array-like", "sp
-00011240: 6172 7365 206d 6174 7269 7822 5d2c 0a20  arse matrix"],. 
-00011250: 2020 2020 2020 2022 7468 7265 7368 6f6c         "threshol
-00011260: 6422 3a20 5b49 6e74 6572 7661 6c28 5265  d": [Interval(Re
-00011270: 616c 2c20 4e6f 6e65 2c20 4e6f 6e65 2c20  al, None, None, 
-00011280: 636c 6f73 6564 3d22 6e65 6974 6865 7222  closed="neither"
-00011290: 295d 2c0a 2020 2020 2020 2020 2263 6f70  )],.        "cop
-000112a0: 7922 3a20 5b22 626f 6f6c 6561 6e22 5d2c  y": ["boolean"],
-000112b0: 0a20 2020 207d 2c0a 2020 2020 7072 6566  .    },.    pref
-000112c0: 6572 5f73 6b69 705f 6e65 7374 6564 5f76  er_skip_nested_v
-000112d0: 616c 6964 6174 696f 6e3d 5472 7565 2c0a  alidation=True,.
-000112e0: 290a 6465 6620 6269 6e61 7269 7a65 2858  ).def binarize(X
-000112f0: 2c20 2a2c 2074 6872 6573 686f 6c64 3d30  , *, threshold=0
-00011300: 2e30 2c20 636f 7079 3d54 7275 6529 3a0a  .0, copy=True):.
-00011310: 2020 2020 2222 2242 6f6f 6c65 616e 2074      """Boolean t
-00011320: 6872 6573 686f 6c64 696e 6720 6f66 2061  hresholding of a
-00011330: 7272 6179 2d6c 696b 6520 6f72 2073 6369  rray-like or sci
-00011340: 7079 2e73 7061 7273 6520 6d61 7472 6978  py.sparse matrix
-00011350: 2e0a 0a20 2020 2052 6561 6420 6d6f 7265  ...    Read more
-00011360: 2069 6e20 7468 6520 3a72 6566 3a60 5573   in the :ref:`Us
-00011370: 6572 2047 7569 6465 203c 7072 6570 726f  er Guide <prepro
-00011380: 6365 7373 696e 675f 6269 6e61 7269 7a61  cessing_binariza
-00011390: 7469 6f6e 3e60 2e0a 0a20 2020 2050 6172  tion>`...    Par
-000113a0: 616d 6574 6572 730a 2020 2020 2d2d 2d2d  ameters.    ----
-000113b0: 2d2d 2d2d 2d2d 0a20 2020 2058 203a 207b  ------.    X : {
-000113c0: 6172 7261 792d 6c69 6b65 2c20 7370 6172  array-like, spar
-000113d0: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
-000113e0: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
-000113f0: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
-00011400: 2020 2020 5468 6520 6461 7461 2074 6f20      The data to 
-00011410: 6269 6e61 7269 7a65 2c20 656c 656d 656e  binarize, elemen
-00011420: 7420 6279 2065 6c65 6d65 6e74 2e0a 2020  t by element..  
-00011430: 2020 2020 2020 7363 6970 792e 7370 6172        scipy.spar
-00011440: 7365 206d 6174 7269 6365 7320 7368 6f75  se matrices shou
-00011450: 6c64 2062 6520 696e 2043 5352 206f 7220  ld be in CSR or 
-00011460: 4353 4320 666f 726d 6174 2074 6f20 6176  CSC format to av
-00011470: 6f69 6420 616e 0a20 2020 2020 2020 2075  oid an.        u
-00011480: 6e2d 6e65 6365 7373 6172 7920 636f 7079  n-necessary copy
-00011490: 2e0a 0a20 2020 2074 6872 6573 686f 6c64  ...    threshold
-000114a0: 203a 2066 6c6f 6174 2c20 6465 6661 756c   : float, defaul
-000114b0: 743d 302e 300a 2020 2020 2020 2020 4665  t=0.0.        Fe
-000114c0: 6174 7572 6520 7661 6c75 6573 2062 656c  ature values bel
-000114d0: 6f77 206f 7220 6571 7561 6c20 746f 2074  ow or equal to t
-000114e0: 6869 7320 6172 6520 7265 706c 6163 6564  his are replaced
-000114f0: 2062 7920 302c 2061 626f 7665 2069 7420   by 0, above it 
-00011500: 6279 2031 2e0a 2020 2020 2020 2020 5468  by 1..        Th
-00011510: 7265 7368 6f6c 6420 6d61 7920 6e6f 7420  reshold may not 
-00011520: 6265 206c 6573 7320 7468 616e 2030 2066  be less than 0 f
-00011530: 6f72 206f 7065 7261 7469 6f6e 7320 6f6e  or operations on
-00011540: 2073 7061 7273 6520 6d61 7472 6963 6573   sparse matrices
-00011550: 2e0a 0a20 2020 2063 6f70 7920 3a20 626f  ...    copy : bo
-00011560: 6f6c 2c20 6465 6661 756c 743d 5472 7565  ol, default=True
-00011570: 0a20 2020 2020 2020 2049 6620 4661 6c73  .        If Fals
-00011580: 652c 2074 7279 2074 6f20 6176 6f69 6420  e, try to avoid 
-00011590: 6120 636f 7079 2061 6e64 2062 696e 6172  a copy and binar
-000115a0: 697a 6520 696e 2070 6c61 6365 2e0a 2020  ize in place..  
-000115b0: 2020 2020 2020 5468 6973 2069 7320 6e6f        This is no
-000115c0: 7420 6775 6172 616e 7465 6564 2074 6f20  t guaranteed to 
-000115d0: 616c 7761 7973 2077 6f72 6b20 696e 2070  always work in p
-000115e0: 6c61 6365 3b20 652e 672e 2069 6620 7468  lace; e.g. if th
-000115f0: 6520 6461 7461 2069 730a 2020 2020 2020  e data is.      
-00011600: 2020 6120 6e75 6d70 7920 6172 7261 7920    a numpy array 
-00011610: 7769 7468 2061 6e20 6f62 6a65 6374 2064  with an object d
-00011620: 7479 7065 2c20 6120 636f 7079 2077 696c  type, a copy wil
-00011630: 6c20 6265 2072 6574 7572 6e65 6420 6576  l be returned ev
-00011640: 656e 2077 6974 680a 2020 2020 2020 2020  en with.        
-00011650: 636f 7079 3d46 616c 7365 2e0a 0a20 2020  copy=False...   
-00011660: 2052 6574 7572 6e73 0a20 2020 202d 2d2d   Returns.    ---
-00011670: 2d2d 2d2d 0a20 2020 2058 5f74 7220 3a20  ----.    X_tr : 
-00011680: 7b6e 6461 7272 6179 2c20 7370 6172 7365  {ndarray, sparse
-00011690: 206d 6174 7269 787d 206f 6620 7368 6170   matrix} of shap
-000116a0: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
-000116b0: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
-000116c0: 2020 5468 6520 7472 616e 7366 6f72 6d65    The transforme
-000116d0: 6420 6461 7461 2e0a 0a20 2020 2053 6565  d data...    See
-000116e0: 2041 6c73 6f0a 2020 2020 2d2d 2d2d 2d2d   Also.    ------
-000116f0: 2d2d 0a20 2020 2042 696e 6172 697a 6572  --.    Binarizer
-00011700: 203a 2050 6572 666f 726d 7320 6269 6e61   : Performs bina
-00011710: 7269 7a61 7469 6f6e 2075 7369 6e67 2074  rization using t
-00011720: 6865 2054 7261 6e73 666f 726d 6572 2041  he Transformer A
-00011730: 5049 0a20 2020 2020 2020 2028 652e 672e  PI.        (e.g.
-00011740: 2061 7320 7061 7274 206f 6620 6120 7072   as part of a pr
-00011750: 6570 726f 6365 7373 696e 6720 3a63 6c61  eprocessing :cla
-00011760: 7373 3a60 7e73 6b6c 6561 726e 2e70 6970  ss:`~sklearn.pip
-00011770: 656c 696e 652e 5069 7065 6c69 6e65 6029  eline.Pipeline`)
-00011780: 2e0a 0a20 2020 2045 7861 6d70 6c65 730a  ...    Examples.
-00011790: 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020      --------.   
-000117a0: 203e 3e3e 2066 726f 6d20 736b 6c65 6172   >>> from sklear
-000117b0: 6e2e 7072 6570 726f 6365 7373 696e 6720  n.preprocessing 
-000117c0: 696d 706f 7274 2062 696e 6172 697a 650a  import binarize.
-000117d0: 2020 2020 3e3e 3e20 5820 3d20 5b5b 302e      >>> X = [[0.
-000117e0: 342c 2030 2e36 2c20 302e 355d 2c20 5b30  4, 0.6, 0.5], [0
-000117f0: 2e36 2c20 302e 312c 2030 2e32 5d5d 0a20  .6, 0.1, 0.2]]. 
-00011800: 2020 203e 3e3e 2062 696e 6172 697a 6528     >>> binarize(
-00011810: 582c 2074 6872 6573 686f 6c64 3d30 2e35  X, threshold=0.5
-00011820: 290a 2020 2020 6172 7261 7928 5b5b 302e  ).    array([[0.
-00011830: 2c20 312e 2c20 302e 5d2c 0a20 2020 2020  , 1., 0.],.     
-00011840: 2020 2020 2020 5b31 2e2c 2030 2e2c 2030        [1., 0., 0
-00011850: 2e5d 5d29 0a20 2020 2022 2222 0a20 2020  .]]).    """.   
-00011860: 2058 203d 2063 6865 636b 5f61 7272 6179   X = check_array
-00011870: 2858 2c20 6163 6365 7074 5f73 7061 7273  (X, accept_spars
-00011880: 653d 5b22 6373 7222 2c20 2263 7363 225d  e=["csr", "csc"]
-00011890: 2c20 636f 7079 3d63 6f70 7929 0a20 2020  , copy=copy).   
-000118a0: 2069 6620 7370 6172 7365 2e69 7373 7061   if sparse.isspa
-000118b0: 7273 6528 5829 3a0a 2020 2020 2020 2020  rse(X):.        
-000118c0: 6966 2074 6872 6573 686f 6c64 203c 2030  if threshold < 0
-000118d0: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-000118e0: 6973 6520 5661 6c75 6545 7272 6f72 2822  ise ValueError("
-000118f0: 4361 6e6e 6f74 2062 696e 6172 697a 6520  Cannot binarize 
-00011900: 6120 7370 6172 7365 206d 6174 7269 7820  a sparse matrix 
-00011910: 7769 7468 2074 6872 6573 686f 6c64 203c  with threshold <
-00011920: 2030 2229 0a20 2020 2020 2020 2063 6f6e   0").        con
-00011930: 6420 3d20 582e 6461 7461 203e 2074 6872  d = X.data > thr
-00011940: 6573 686f 6c64 0a20 2020 2020 2020 206e  eshold.        n
-00011950: 6f74 5f63 6f6e 6420 3d20 6e70 2e6c 6f67  ot_cond = np.log
-00011960: 6963 616c 5f6e 6f74 2863 6f6e 6429 0a20  ical_not(cond). 
-00011970: 2020 2020 2020 2058 2e64 6174 615b 636f         X.data[co
-00011980: 6e64 5d20 3d20 310a 2020 2020 2020 2020  nd] = 1.        
-00011990: 582e 6461 7461 5b6e 6f74 5f63 6f6e 645d  X.data[not_cond]
-000119a0: 203d 2030 0a20 2020 2020 2020 2058 2e65   = 0.        X.e
-000119b0: 6c69 6d69 6e61 7465 5f7a 6572 6f73 2829  liminate_zeros()
-000119c0: 0a20 2020 2065 6c73 653a 0a20 2020 2020  .    else:.     
-000119d0: 2020 2063 6f6e 6420 3d20 5820 3e20 7468     cond = X > th
-000119e0: 7265 7368 6f6c 640a 2020 2020 2020 2020  reshold.        
-000119f0: 6e6f 745f 636f 6e64 203d 206e 702e 6c6f  not_cond = np.lo
-00011a00: 6769 6361 6c5f 6e6f 7428 636f 6e64 290a  gical_not(cond).
-00011a10: 2020 2020 2020 2020 585b 636f 6e64 5d20          X[cond] 
-00011a20: 3d20 310a 2020 2020 2020 2020 585b 6e6f  = 1.        X[no
-00011a30: 745f 636f 6e64 5d20 3d20 300a 2020 2020  t_cond] = 0.    
-00011a40: 7265 7475 726e 2058 0a0a 0a63 6c61 7373  return X...class
-00011a50: 2042 696e 6172 697a 6572 284f 6e65 546f   Binarizer(OneTo
-00011a60: 4f6e 6546 6561 7475 7265 4d69 7869 6e2c  OneFeatureMixin,
-00011a70: 2054 7261 6e73 666f 726d 6572 4d69 7869   TransformerMixi
-00011a80: 6e2c 2042 6173 6545 7374 696d 6174 6f72  n, BaseEstimator
-00011a90: 293a 0a20 2020 2022 2222 4269 6e61 7269  ):.    """Binari
-00011aa0: 7a65 2064 6174 6120 2873 6574 2066 6561  ze data (set fea
-00011ab0: 7475 7265 2076 616c 7565 7320 746f 2030  ture values to 0
-00011ac0: 206f 7220 3129 2061 6363 6f72 6469 6e67   or 1) according
-00011ad0: 2074 6f20 6120 7468 7265 7368 6f6c 642e   to a threshold.
-00011ae0: 0a0a 2020 2020 5661 6c75 6573 2067 7265  ..    Values gre
-00011af0: 6174 6572 2074 6861 6e20 7468 6520 7468  ater than the th
-00011b00: 7265 7368 6f6c 6420 6d61 7020 746f 2031  reshold map to 1
-00011b10: 2c20 7768 696c 6520 7661 6c75 6573 206c  , while values l
-00011b20: 6573 7320 7468 616e 0a20 2020 206f 7220  ess than.    or 
-00011b30: 6571 7561 6c20 746f 2074 6865 2074 6872  equal to the thr
-00011b40: 6573 686f 6c64 206d 6170 2074 6f20 302e  eshold map to 0.
-00011b50: 2057 6974 6820 7468 6520 6465 6661 756c   With the defaul
-00011b60: 7420 7468 7265 7368 6f6c 6420 6f66 2030  t threshold of 0
-00011b70: 2c0a 2020 2020 6f6e 6c79 2070 6f73 6974  ,.    only posit
-00011b80: 6976 6520 7661 6c75 6573 206d 6170 2074  ive values map t
-00011b90: 6f20 312e 0a0a 2020 2020 4269 6e61 7269  o 1...    Binari
-00011ba0: 7a61 7469 6f6e 2069 7320 6120 636f 6d6d  zation is a comm
-00011bb0: 6f6e 206f 7065 7261 7469 6f6e 206f 6e20  on operation on 
-00011bc0: 7465 7874 2063 6f75 6e74 2064 6174 6120  text count data 
-00011bd0: 7768 6572 6520 7468 650a 2020 2020 616e  where the.    an
-00011be0: 616c 7973 7420 6361 6e20 6465 6369 6465  alyst can decide
-00011bf0: 2074 6f20 6f6e 6c79 2063 6f6e 7369 6465   to only conside
-00011c00: 7220 7468 6520 7072 6573 656e 6365 206f  r the presence o
-00011c10: 7220 6162 7365 6e63 6520 6f66 2061 0a20  r absence of a. 
-00011c20: 2020 2066 6561 7475 7265 2072 6174 6865     feature rathe
-00011c30: 7220 7468 616e 2061 2071 7561 6e74 6966  r than a quantif
-00011c40: 6965 6420 6e75 6d62 6572 206f 6620 6f63  ied number of oc
-00011c50: 6375 7272 656e 6365 7320 666f 7220 696e  currences for in
-00011c60: 7374 616e 6365 2e0a 0a20 2020 2049 7420  stance...    It 
-00011c70: 6361 6e20 616c 736f 2062 6520 7573 6564  can also be used
-00011c80: 2061 7320 6120 7072 652d 7072 6f63 6573   as a pre-proces
-00011c90: 7369 6e67 2073 7465 7020 666f 7220 6573  sing step for es
-00011ca0: 7469 6d61 746f 7273 2074 6861 740a 2020  timators that.  
-00011cb0: 2020 636f 6e73 6964 6572 2062 6f6f 6c65    consider boole
-00011cc0: 616e 2072 616e 646f 6d20 7661 7269 6162  an random variab
-00011cd0: 6c65 7320 2865 2e67 2e20 6d6f 6465 6c6c  les (e.g. modell
-00011ce0: 6564 2075 7369 6e67 2074 6865 2042 6572  ed using the Ber
-00011cf0: 6e6f 756c 6c69 0a20 2020 2064 6973 7472  noulli.    distr
-00011d00: 6962 7574 696f 6e20 696e 2061 2042 6179  ibution in a Bay
-00011d10: 6573 6961 6e20 7365 7474 696e 6729 2e0a  esian setting)..
-00011d20: 0a20 2020 2052 6561 6420 6d6f 7265 2069  .    Read more i
-00011d30: 6e20 7468 6520 3a72 6566 3a60 5573 6572  n the :ref:`User
-00011d40: 2047 7569 6465 203c 7072 6570 726f 6365   Guide <preproce
-00011d50: 7373 696e 675f 6269 6e61 7269 7a61 7469  ssing_binarizati
-00011d60: 6f6e 3e60 2e0a 0a20 2020 2050 6172 616d  on>`...    Param
-00011d70: 6574 6572 730a 2020 2020 2d2d 2d2d 2d2d  eters.    ------
-00011d80: 2d2d 2d2d 0a20 2020 2074 6872 6573 686f  ----.    thresho
-00011d90: 6c64 203a 2066 6c6f 6174 2c20 6465 6661  ld : float, defa
-00011da0: 756c 743d 302e 300a 2020 2020 2020 2020  ult=0.0.        
-00011db0: 4665 6174 7572 6520 7661 6c75 6573 2062  Feature values b
-00011dc0: 656c 6f77 206f 7220 6571 7561 6c20 746f  elow or equal to
-00011dd0: 2074 6869 7320 6172 6520 7265 706c 6163   this are replac
-00011de0: 6564 2062 7920 302c 2061 626f 7665 2069  ed by 0, above i
-00011df0: 7420 6279 2031 2e0a 2020 2020 2020 2020  t by 1..        
-00011e00: 5468 7265 7368 6f6c 6420 6d61 7920 6e6f  Threshold may no
-00011e10: 7420 6265 206c 6573 7320 7468 616e 2030  t be less than 0
-00011e20: 2066 6f72 206f 7065 7261 7469 6f6e 7320   for operations 
-00011e30: 6f6e 2073 7061 7273 6520 6d61 7472 6963  on sparse matric
-00011e40: 6573 2e0a 0a20 2020 2063 6f70 7920 3a20  es...    copy : 
-00011e50: 626f 6f6c 2c20 6465 6661 756c 743d 5472  bool, default=Tr
-00011e60: 7565 0a20 2020 2020 2020 2053 6574 2074  ue.        Set t
-00011e70: 6f20 4661 6c73 6520 746f 2070 6572 666f  o False to perfo
-00011e80: 726d 2069 6e70 6c61 6365 2062 696e 6172  rm inplace binar
-00011e90: 697a 6174 696f 6e20 616e 6420 6176 6f69  ization and avoi
-00011ea0: 6420 6120 636f 7079 2028 6966 0a20 2020  d a copy (if.   
-00011eb0: 2020 2020 2074 6865 2069 6e70 7574 2069       the input i
-00011ec0: 7320 616c 7265 6164 7920 6120 6e75 6d70  s already a nump
-00011ed0: 7920 6172 7261 7920 6f72 2061 2073 6369  y array or a sci
-00011ee0: 7079 2e73 7061 7273 6520 4353 5220 6d61  py.sparse CSR ma
-00011ef0: 7472 6978 292e 0a0a 2020 2020 4174 7472  trix)...    Attr
-00011f00: 6962 7574 6573 0a20 2020 202d 2d2d 2d2d  ibutes.    -----
-00011f10: 2d2d 2d2d 2d0a 2020 2020 6e5f 6665 6174  -----.    n_feat
-00011f20: 7572 6573 5f69 6e5f 203a 2069 6e74 0a20  ures_in_ : int. 
-00011f30: 2020 2020 2020 204e 756d 6265 7220 6f66         Number of
-00011f40: 2066 6561 7475 7265 7320 7365 656e 2064   features seen d
-00011f50: 7572 696e 6720 3a74 6572 6d3a 6066 6974  uring :term:`fit
-00011f60: 602e 0a0a 2020 2020 2020 2020 2e2e 2076  `...        .. v
-00011f70: 6572 7369 6f6e 6164 6465 643a 3a20 302e  ersionadded:: 0.
-00011f80: 3234 0a0a 2020 2020 6665 6174 7572 655f  24..    feature_
-00011f90: 6e61 6d65 735f 696e 5f20 3a20 6e64 6172  names_in_ : ndar
-00011fa0: 7261 7920 6f66 2073 6861 7065 2028 606e  ray of shape (`n
-00011fb0: 5f66 6561 7475 7265 735f 696e 5f60 2c29  _features_in_`,)
-00011fc0: 0a20 2020 2020 2020 204e 616d 6573 206f  .        Names o
-00011fd0: 6620 6665 6174 7572 6573 2073 6565 6e20  f features seen 
-00011fe0: 6475 7269 6e67 203a 7465 726d 3a60 6669  during :term:`fi
-00011ff0: 7460 2e20 4465 6669 6e65 6420 6f6e 6c79  t`. Defined only
-00012000: 2077 6865 6e20 6058 600a 2020 2020 2020   when `X`.      
-00012010: 2020 6861 7320 6665 6174 7572 6520 6e61    has feature na
-00012020: 6d65 7320 7468 6174 2061 7265 2061 6c6c  mes that are all
-00012030: 2073 7472 696e 6773 2e0a 0a20 2020 2020   strings...     
-00012040: 2020 202e 2e20 7665 7273 696f 6e61 6464     .. versionadd
-00012050: 6564 3a3a 2031 2e30 0a0a 2020 2020 5365  ed:: 1.0..    Se
-00012060: 6520 416c 736f 0a20 2020 202d 2d2d 2d2d  e Also.    -----
-00012070: 2d2d 2d0a 2020 2020 6269 6e61 7269 7a65  ---.    binarize
-00012080: 203a 2045 7175 6976 616c 656e 7420 6675   : Equivalent fu
-00012090: 6e63 7469 6f6e 2077 6974 686f 7574 2074  nction without t
-000120a0: 6865 2065 7374 696d 6174 6f72 2041 5049  he estimator API
-000120b0: 2e0a 2020 2020 4b42 696e 7344 6973 6372  ..    KBinsDiscr
-000120c0: 6574 697a 6572 203a 2042 696e 2063 6f6e  etizer : Bin con
-000120d0: 7469 6e75 6f75 7320 6461 7461 2069 6e74  tinuous data int
-000120e0: 6f20 696e 7465 7276 616c 732e 0a20 2020  o intervals..   
-000120f0: 204f 6e65 486f 7445 6e63 6f64 6572 203a   OneHotEncoder :
-00012100: 2045 6e63 6f64 6520 6361 7465 676f 7269   Encode categori
-00012110: 6361 6c20 6665 6174 7572 6573 2061 7320  cal features as 
-00012120: 6120 6f6e 652d 686f 7420 6e75 6d65 7269  a one-hot numeri
-00012130: 6320 6172 7261 792e 0a0a 2020 2020 4e6f  c array...    No
-00012140: 7465 730a 2020 2020 2d2d 2d2d 2d0a 2020  tes.    -----.  
-00012150: 2020 4966 2074 6865 2069 6e70 7574 2069    If the input i
-00012160: 7320 6120 7370 6172 7365 206d 6174 7269  s a sparse matri
-00012170: 782c 206f 6e6c 7920 7468 6520 6e6f 6e2d  x, only the non-
-00012180: 7a65 726f 2076 616c 7565 7320 6172 6520  zero values are 
-00012190: 7375 626a 6563 740a 2020 2020 746f 2075  subject.    to u
-000121a0: 7064 6174 6520 6279 2074 6865 203a 636c  pdate by the :cl
-000121b0: 6173 733a 6042 696e 6172 697a 6572 6020  ass:`Binarizer` 
-000121c0: 636c 6173 732e 0a0a 2020 2020 5468 6973  class...    This
-000121d0: 2065 7374 696d 6174 6f72 2069 7320 3a74   estimator is :t
-000121e0: 6572 6d3a 6073 7461 7465 6c65 7373 6020  erm:`stateless` 
-000121f0: 616e 6420 646f 6573 206e 6f74 206e 6565  and does not nee
-00012200: 6420 746f 2062 6520 6669 7474 6564 2e0a  d to be fitted..
-00012210: 2020 2020 486f 7765 7665 722c 2077 6520      However, we 
-00012220: 7265 636f 6d6d 656e 6420 746f 2063 616c  recommend to cal
-00012230: 6c20 3a6d 6574 683a 6066 6974 5f74 7261  l :meth:`fit_tra
-00012240: 6e73 666f 726d 6020 696e 7374 6561 6420  nsform` instead 
-00012250: 6f66 0a20 2020 203a 6d65 7468 3a60 7472  of.    :meth:`tr
-00012260: 616e 7366 6f72 6d60 2c20 6173 2070 6172  ansform`, as par
-00012270: 616d 6574 6572 2076 616c 6964 6174 696f  ameter validatio
-00012280: 6e20 6973 206f 6e6c 7920 7065 7266 6f72  n is only perfor
-00012290: 6d65 6420 696e 0a20 2020 203a 6d65 7468  med in.    :meth
-000122a0: 3a60 6669 7460 2e0a 0a20 2020 2045 7861  :`fit`...    Exa
-000122b0: 6d70 6c65 730a 2020 2020 2d2d 2d2d 2d2d  mples.    ------
-000122c0: 2d2d 0a20 2020 203e 3e3e 2066 726f 6d20  --.    >>> from 
-000122d0: 736b 6c65 6172 6e2e 7072 6570 726f 6365  sklearn.preproce
-000122e0: 7373 696e 6720 696d 706f 7274 2042 696e  ssing import Bin
-000122f0: 6172 697a 6572 0a20 2020 203e 3e3e 2058  arizer.    >>> X
-00012300: 203d 205b 5b20 312e 2c20 2d31 2e2c 2020   = [[ 1., -1.,  
-00012310: 322e 5d2c 0a20 2020 202e 2e2e 2020 2020  2.],.    ...    
-00012320: 2020 5b20 322e 2c20 2030 2e2c 2020 302e    [ 2.,  0.,  0.
-00012330: 5d2c 0a20 2020 202e 2e2e 2020 2020 2020  ],.    ...      
-00012340: 5b20 302e 2c20 2031 2e2c 202d 312e 5d5d  [ 0.,  1., -1.]]
-00012350: 0a20 2020 203e 3e3e 2074 7261 6e73 666f  .    >>> transfo
-00012360: 726d 6572 203d 2042 696e 6172 697a 6572  rmer = Binarizer
-00012370: 2829 2e66 6974 2858 2920 2023 2066 6974  ().fit(X)  # fit
-00012380: 2064 6f65 7320 6e6f 7468 696e 672e 0a20   does nothing.. 
-00012390: 2020 203e 3e3e 2074 7261 6e73 666f 726d     >>> transform
-000123a0: 6572 0a20 2020 2042 696e 6172 697a 6572  er.    Binarizer
-000123b0: 2829 0a20 2020 203e 3e3e 2074 7261 6e73  ().    >>> trans
-000123c0: 666f 726d 6572 2e74 7261 6e73 666f 726d  former.transform
-000123d0: 2858 290a 2020 2020 6172 7261 7928 5b5b  (X).    array([[
-000123e0: 312e 2c20 302e 2c20 312e 5d2c 0a20 2020  1., 0., 1.],.   
-000123f0: 2020 2020 2020 2020 5b31 2e2c 2030 2e2c          [1., 0.,
-00012400: 2030 2e5d 2c0a 2020 2020 2020 2020 2020   0.],.          
-00012410: 205b 302e 2c20 312e 2c20 302e 5d5d 290a   [0., 1., 0.]]).
-00012420: 2020 2020 2222 220a 0a20 2020 205f 7061      """..    _pa
-00012430: 7261 6d65 7465 725f 636f 6e73 7472 6169  rameter_constrai
-00012440: 6e74 733a 2064 6963 7420 3d20 7b0a 2020  nts: dict = {.  
-00012450: 2020 2020 2020 2274 6872 6573 686f 6c64        "threshold
-00012460: 223a 205b 5265 616c 5d2c 0a20 2020 2020  ": [Real],.     
-00012470: 2020 2022 636f 7079 223a 205b 2262 6f6f     "copy": ["boo
-00012480: 6c65 616e 225d 2c0a 2020 2020 7d0a 0a20  lean"],.    }.. 
-00012490: 2020 2064 6566 205f 5f69 6e69 745f 5f28     def __init__(
-000124a0: 7365 6c66 2c20 2a2c 2074 6872 6573 686f  self, *, thresho
-000124b0: 6c64 3d30 2e30 2c20 636f 7079 3d54 7275  ld=0.0, copy=Tru
-000124c0: 6529 3a0a 2020 2020 2020 2020 7365 6c66  e):.        self
-000124d0: 2e74 6872 6573 686f 6c64 203d 2074 6872  .threshold = thr
-000124e0: 6573 686f 6c64 0a20 2020 2020 2020 2073  eshold.        s
-000124f0: 656c 662e 636f 7079 203d 2063 6f70 790a  elf.copy = copy.
-00012500: 0a20 2020 2040 5f66 6974 5f63 6f6e 7465  .    @_fit_conte
-00012510: 7874 2870 7265 6665 725f 736b 6970 5f6e  xt(prefer_skip_n
-00012520: 6573 7465 645f 7661 6c69 6461 7469 6f6e  ested_validation
-00012530: 3d54 7275 6529 0a20 2020 2064 6566 2066  =True).    def f
-00012540: 6974 2873 656c 662c 2058 2c20 793d 4e6f  it(self, X, y=No
-00012550: 6e65 293a 0a20 2020 2020 2020 2022 2222  ne):.        """
-00012560: 4f6e 6c79 2076 616c 6964 6174 6573 2065  Only validates e
-00012570: 7374 696d 6174 6f72 2773 2070 6172 616d  stimator's param
-00012580: 6574 6572 732e 0a0a 2020 2020 2020 2020  eters...        
-00012590: 5468 6973 206d 6574 686f 6420 616c 6c6f  This method allo
-000125a0: 7773 2074 6f3a 2028 6929 2076 616c 6964  ws to: (i) valid
-000125b0: 6174 6520 7468 6520 6573 7469 6d61 746f  ate the estimato
-000125c0: 7227 7320 7061 7261 6d65 7465 7273 2061  r's parameters a
-000125d0: 6e64 0a20 2020 2020 2020 2028 6969 2920  nd.        (ii) 
-000125e0: 6265 2063 6f6e 7369 7374 656e 7420 7769  be consistent wi
-000125f0: 7468 2074 6865 2073 6369 6b69 742d 6c65  th the scikit-le
-00012600: 6172 6e20 7472 616e 7366 6f72 6d65 7220  arn transformer 
-00012610: 4150 492e 0a0a 2020 2020 2020 2020 5061  API...        Pa
-00012620: 7261 6d65 7465 7273 0a20 2020 2020 2020  rameters.       
-00012630: 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020   ----------.    
-00012640: 2020 2020 5820 3a20 7b61 7272 6179 2d6c      X : {array-l
-00012650: 696b 652c 2073 7061 7273 6520 6d61 7472  ike, sparse matr
-00012660: 6978 7d20 6f66 2073 6861 7065 2028 6e5f  ix} of shape (n_
-00012670: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
-00012680: 7265 7329 0a20 2020 2020 2020 2020 2020  res).           
-00012690: 2054 6865 2064 6174 612e 0a0a 2020 2020   The data...    
-000126a0: 2020 2020 7920 3a20 4e6f 6e65 0a20 2020      y : None.   
-000126b0: 2020 2020 2020 2020 2049 676e 6f72 6564           Ignored
-000126c0: 2e0a 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
-000126d0: 6e73 0a20 2020 2020 2020 202d 2d2d 2d2d  ns.        -----
-000126e0: 2d2d 0a20 2020 2020 2020 2073 656c 6620  --.        self 
-000126f0: 3a20 6f62 6a65 6374 0a20 2020 2020 2020  : object.       
-00012700: 2020 2020 2046 6974 7465 6420 7472 616e       Fitted tran
-00012710: 7366 6f72 6d65 722e 0a20 2020 2020 2020  sformer..       
-00012720: 2022 2222 0a20 2020 2020 2020 2073 656c   """.        sel
-00012730: 662e 5f76 616c 6964 6174 655f 6461 7461  f._validate_data
-00012740: 2858 2c20 6163 6365 7074 5f73 7061 7273  (X, accept_spars
-00012750: 653d 2263 7372 2229 0a20 2020 2020 2020  e="csr").       
-00012760: 2072 6574 7572 6e20 7365 6c66 0a0a 2020   return self..  
-00012770: 2020 6465 6620 7472 616e 7366 6f72 6d28    def transform(
-00012780: 7365 6c66 2c20 582c 2063 6f70 793d 4e6f  self, X, copy=No
-00012790: 6e65 293a 0a20 2020 2020 2020 2022 2222  ne):.        """
-000127a0: 4269 6e61 7269 7a65 2065 6163 6820 656c  Binarize each el
-000127b0: 656d 656e 7420 6f66 2058 2e0a 0a20 2020  ement of X...   
-000127c0: 2020 2020 2050 6172 616d 6574 6572 730a       Parameters.
-000127d0: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d2d          --------
-000127e0: 2d2d 0a20 2020 2020 2020 2058 203a 207b  --.        X : {
-000127f0: 6172 7261 792d 6c69 6b65 2c20 7370 6172  array-like, spar
-00012800: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
-00012810: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
-00012820: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
-00012830: 2020 2020 2020 2020 5468 6520 6461 7461          The data
-00012840: 2074 6f20 6269 6e61 7269 7a65 2c20 656c   to binarize, el
-00012850: 656d 656e 7420 6279 2065 6c65 6d65 6e74  ement by element
-00012860: 2e0a 2020 2020 2020 2020 2020 2020 7363  ..            sc
-00012870: 6970 792e 7370 6172 7365 206d 6174 7269  ipy.sparse matri
-00012880: 6365 7320 7368 6f75 6c64 2062 6520 696e  ces should be in
-00012890: 2043 5352 2066 6f72 6d61 7420 746f 2061   CSR format to a
-000128a0: 766f 6964 2061 6e0a 2020 2020 2020 2020  void an.        
-000128b0: 2020 2020 756e 2d6e 6563 6573 7361 7279      un-necessary
-000128c0: 2063 6f70 792e 0a0a 2020 2020 2020 2020   copy...        
-000128d0: 636f 7079 203a 2062 6f6f 6c0a 2020 2020  copy : bool.    
-000128e0: 2020 2020 2020 2020 436f 7079 2074 6865          Copy the
-000128f0: 2069 6e70 7574 2058 206f 7220 6e6f 742e   input X or not.
-00012900: 0a0a 2020 2020 2020 2020 5265 7475 726e  ..        Return
-00012910: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
-00012920: 2d0a 2020 2020 2020 2020 585f 7472 203a  -.        X_tr :
-00012930: 207b 6e64 6172 7261 792c 2073 7061 7273   {ndarray, spars
-00012940: 6520 6d61 7472 6978 7d20 6f66 2073 6861  e matrix} of sha
-00012950: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
-00012960: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
-00012970: 2020 2020 2020 2054 7261 6e73 666f 726d         Transform
-00012980: 6564 2061 7272 6179 2e0a 2020 2020 2020  ed array..      
-00012990: 2020 2222 220a 2020 2020 2020 2020 636f    """.        co
-000129a0: 7079 203d 2063 6f70 7920 6966 2063 6f70  py = copy if cop
-000129b0: 7920 6973 206e 6f74 204e 6f6e 6520 656c  y is not None el
-000129c0: 7365 2073 656c 662e 636f 7079 0a20 2020  se self.copy.   
-000129d0: 2020 2020 2023 2054 4f44 4f3a 2054 6869       # TODO: Thi
-000129e0: 7320 7368 6f75 6c64 2062 6520 7265 6661  s should be refa
-000129f0: 6374 6f72 6564 2062 6563 6175 7365 2062  ctored because b
-00012a00: 696e 6172 697a 6520 616c 736f 2063 616c  inarize also cal
-00012a10: 6c73 0a20 2020 2020 2020 2023 2063 6865  ls.        # che
-00012a20: 636b 5f61 7272 6179 0a20 2020 2020 2020  ck_array.       
-00012a30: 2058 203d 2073 656c 662e 5f76 616c 6964   X = self._valid
-00012a40: 6174 655f 6461 7461 2858 2c20 6163 6365  ate_data(X, acce
-00012a50: 7074 5f73 7061 7273 653d 5b22 6373 7222  pt_sparse=["csr"
-00012a60: 2c20 2263 7363 225d 2c20 636f 7079 3d63  , "csc"], copy=c
-00012a70: 6f70 792c 2072 6573 6574 3d46 616c 7365  opy, reset=False
-00012a80: 290a 2020 2020 2020 2020 7265 7475 726e  ).        return
-00012a90: 2062 696e 6172 697a 6528 582c 2074 6872   binarize(X, thr
-00012aa0: 6573 686f 6c64 3d73 656c 662e 7468 7265  eshold=self.thre
-00012ab0: 7368 6f6c 642c 2063 6f70 793d 4661 6c73  shold, copy=Fals
-00012ac0: 6529 0a0a 2020 2020 6465 6620 5f6d 6f72  e)..    def _mor
-00012ad0: 655f 7461 6773 2873 656c 6629 3a0a 2020  e_tags(self):.  
-00012ae0: 2020 2020 2020 7265 7475 726e 207b 2273        return {"s
-00012af0: 7461 7465 6c65 7373 223a 2054 7275 657d  tateless": True}
-00012b00: 0a0a 0a63 6c61 7373 204b 6572 6e65 6c43  ...class KernelC
-00012b10: 656e 7465 7265 7228 436c 6173 734e 616d  enterer(ClassNam
-00012b20: 6550 7265 6669 7846 6561 7475 7265 734f  ePrefixFeaturesO
-00012b30: 7574 4d69 7869 6e2c 2054 7261 6e73 666f  utMixin, Transfo
-00012b40: 726d 6572 4d69 7869 6e2c 2042 6173 6545  rmerMixin, BaseE
-00012b50: 7374 696d 6174 6f72 293a 0a20 2020 2072  stimator):.    r
-00012b60: 2222 2243 656e 7465 7220 616e 2061 7262  """Center an arb
-00012b70: 6974 7261 7279 206b 6572 6e65 6c20 6d61  itrary kernel ma
-00012b80: 7472 6978 203a 6d61 7468 3a60 4b60 2e0a  trix :math:`K`..
-00012b90: 0a20 2020 204c 6574 2064 6566 696e 6520  .    Let define 
-00012ba0: 6120 6b65 726e 656c 203a 6d61 7468 3a60  a kernel :math:`
-00012bb0: 4b60 2073 7563 6820 7468 6174 3a0a 0a20  K` such that:.. 
-00012bc0: 2020 202e 2e20 6d61 7468 3a3a 0a20 2020     .. math::.   
-00012bd0: 2020 2020 204b 2858 2c20 5929 203d 205c       K(X, Y) = \
-00012be0: 7068 6928 5829 202e 205c 7068 6928 5929  phi(X) . \phi(Y)
-00012bf0: 5e7b 547d 0a0a 2020 2020 3a6d 6174 683a  ^{T}..    :math:
-00012c00: 605c 7068 6928 5829 6020 6973 2061 2066  `\phi(X)` is a f
-00012c10: 756e 6374 696f 6e20 6d61 7070 696e 6720  unction mapping 
-00012c20: 6f66 2072 6f77 7320 6f66 203a 6d61 7468  of rows of :math
-00012c30: 3a60 5860 2074 6f20 610a 2020 2020 4869  :`X` to a.    Hi
-00012c40: 6c62 6572 7420 7370 6163 6520 616e 6420  lbert space and 
-00012c50: 3a6d 6174 683a 604b 6020 6973 206f 6620  :math:`K` is of 
-00012c60: 7368 6170 6520 6028 6e5f 7361 6d70 6c65  shape `(n_sample
-00012c70: 732c 206e 5f73 616d 706c 6573 2960 2e0a  s, n_samples)`..
-00012c80: 0a20 2020 2054 6869 7320 636c 6173 7320  .    This class 
-00012c90: 616c 6c6f 7773 2074 6f20 636f 6d70 7574  allows to comput
-00012ca0: 6520 3a6d 6174 683a 605c 7469 6c64 657b  e :math:`\tilde{
-00012cb0: 4b7d 2858 2c20 5929 6020 7375 6368 2074  K}(X, Y)` such t
-00012cc0: 6861 743a 0a0a 2020 2020 2e2e 206d 6174  hat:..    .. mat
-00012cd0: 683a 3a0a 2020 2020 2020 2020 5c74 696c  h::.        \til
-00012ce0: 6465 7b4b 2858 2c20 5929 7d20 3d20 5c74  de{K(X, Y)} = \t
-00012cf0: 696c 6465 7b5c 7068 697d 2858 2920 2e20  ilde{\phi}(X) . 
-00012d00: 5c74 696c 6465 7b5c 7068 697d 2859 295e  \tilde{\phi}(Y)^
-00012d10: 7b54 7d0a 0a20 2020 203a 6d61 7468 3a60  {T}..    :math:`
-00012d20: 5c74 696c 6465 7b5c 7068 697d 2858 2960  \tilde{\phi}(X)`
-00012d30: 2069 7320 7468 6520 6365 6e74 6572 6564   is the centered
-00012d40: 206d 6170 7065 6420 6461 7461 2069 6e20   mapped data in 
-00012d50: 7468 6520 4869 6c62 6572 740a 2020 2020  the Hilbert.    
-00012d60: 7370 6163 652e 0a0a 2020 2020 604b 6572  space...    `Ker
-00012d70: 6e65 6c43 656e 7465 7265 7260 2063 656e  nelCenterer` cen
-00012d80: 7465 7273 2074 6865 2066 6561 7475 7265  ters the feature
-00012d90: 7320 7769 7468 6f75 7420 6578 706c 6963  s without explic
-00012da0: 6974 6c79 2063 6f6d 7075 7469 6e67 2074  itly computing t
-00012db0: 6865 0a20 2020 206d 6170 7069 6e67 203a  he.    mapping :
-00012dc0: 6d61 7468 3a60 5c70 6869 285c 6364 6f74  math:`\phi(\cdot
-00012dd0: 2960 2e20 576f 726b 696e 6720 7769 7468  )`. Working with
-00012de0: 2063 656e 7465 7265 6420 6b65 726e 656c   centered kernel
-00012df0: 7320 6973 2073 6f6d 6574 696d 650a 2020  s is sometime.  
-00012e00: 2020 6578 7065 6374 6564 2077 6865 6e20    expected when 
-00012e10: 6465 616c 696e 6720 7769 7468 2061 6c67  dealing with alg
-00012e20: 6562 7261 2063 6f6d 7075 7461 7469 6f6e  ebra computation
-00012e30: 2073 7563 6820 6173 2065 6967 656e 6465   such as eigende
-00012e40: 636f 6d70 6f73 6974 696f 6e0a 2020 2020  composition.    
-00012e50: 666f 7220 3a63 6c61 7373 3a60 7e73 6b6c  for :class:`~skl
-00012e60: 6561 726e 2e64 6563 6f6d 706f 7369 7469  earn.decompositi
-00012e70: 6f6e 2e4b 6572 6e65 6c50 4341 6020 666f  on.KernelPCA` fo
-00012e80: 7220 696e 7374 616e 6365 2e0a 0a20 2020  r instance...   
-00012e90: 2052 6561 6420 6d6f 7265 2069 6e20 7468   Read more in th
-00012ea0: 6520 3a72 6566 3a60 5573 6572 2047 7569  e :ref:`User Gui
-00012eb0: 6465 203c 6b65 726e 656c 5f63 656e 7465  de <kernel_cente
-00012ec0: 7269 6e67 3e60 2e0a 0a20 2020 2041 7474  ring>`...    Att
-00012ed0: 7269 6275 7465 730a 2020 2020 2d2d 2d2d  ributes.    ----
-00012ee0: 2d2d 2d2d 2d2d 0a20 2020 204b 5f66 6974  ------.    K_fit
-00012ef0: 5f72 6f77 735f 203a 206e 6461 7272 6179  _rows_ : ndarray
-00012f00: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-00012f10: 706c 6573 2c29 0a20 2020 2020 2020 2041  ples,).        A
-00012f20: 7665 7261 6765 206f 6620 6561 6368 2063  verage of each c
-00012f30: 6f6c 756d 6e20 6f66 206b 6572 6e65 6c20  olumn of kernel 
-00012f40: 6d61 7472 6978 2e0a 0a20 2020 204b 5f66  matrix...    K_f
-00012f50: 6974 5f61 6c6c 5f20 3a20 666c 6f61 740a  it_all_ : float.
-00012f60: 2020 2020 2020 2020 4176 6572 6167 6520          Average 
-00012f70: 6f66 206b 6572 6e65 6c20 6d61 7472 6978  of kernel matrix
-00012f80: 2e0a 0a20 2020 206e 5f66 6561 7475 7265  ...    n_feature
-00012f90: 735f 696e 5f20 3a20 696e 740a 2020 2020  s_in_ : int.    
-00012fa0: 2020 2020 4e75 6d62 6572 206f 6620 6665      Number of fe
-00012fb0: 6174 7572 6573 2073 6565 6e20 6475 7269  atures seen duri
-00012fc0: 6e67 203a 7465 726d 3a60 6669 7460 2e0a  ng :term:`fit`..
-00012fd0: 0a20 2020 2020 2020 202e 2e20 7665 7273  .        .. vers
-00012fe0: 696f 6e61 6464 6564 3a3a 2030 2e32 340a  ionadded:: 0.24.
-00012ff0: 0a20 2020 2066 6561 7475 7265 5f6e 616d  .    feature_nam
-00013000: 6573 5f69 6e5f 203a 206e 6461 7272 6179  es_in_ : ndarray
-00013010: 206f 6620 7368 6170 6520 2860 6e5f 6665   of shape (`n_fe
-00013020: 6174 7572 6573 5f69 6e5f 602c 290a 2020  atures_in_`,).  
-00013030: 2020 2020 2020 4e61 6d65 7320 6f66 2066        Names of f
-00013040: 6561 7475 7265 7320 7365 656e 2064 7572  eatures seen dur
-00013050: 696e 6720 3a74 6572 6d3a 6066 6974 602e  ing :term:`fit`.
-00013060: 2044 6566 696e 6564 206f 6e6c 7920 7768   Defined only wh
-00013070: 656e 2060 5860 0a20 2020 2020 2020 2068  en `X`.        h
-00013080: 6173 2066 6561 7475 7265 206e 616d 6573  as feature names
-00013090: 2074 6861 7420 6172 6520 616c 6c20 7374   that are all st
-000130a0: 7269 6e67 732e 0a0a 2020 2020 2020 2020  rings...        
-000130b0: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
-000130c0: 3a20 312e 300a 0a20 2020 2053 6565 2041  : 1.0..    See A
-000130d0: 6c73 6f0a 2020 2020 2d2d 2d2d 2d2d 2d2d  lso.    --------
-000130e0: 0a20 2020 2073 6b6c 6561 726e 2e6b 6572  .    sklearn.ker
-000130f0: 6e65 6c5f 6170 7072 6f78 696d 6174 696f  nel_approximatio
-00013100: 6e2e 4e79 7374 726f 656d 203a 2041 7070  n.Nystroem : App
-00013110: 726f 7869 6d61 7465 2061 206b 6572 6e65  roximate a kerne
-00013120: 6c20 6d61 700a 2020 2020 2020 2020 7573  l map.        us
-00013130: 696e 6720 6120 7375 6273 6574 206f 6620  ing a subset of 
-00013140: 7468 6520 7472 6169 6e69 6e67 2064 6174  the training dat
-00013150: 612e 0a0a 2020 2020 5265 6665 7265 6e63  a...    Referenc
-00013160: 6573 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d  es.    ---------
-00013170: 2d0a 2020 2020 2e2e 205b 315d 2060 5363  -.    .. [1] `Sc
-00013180: 68c3 b66c 6b6f 7066 2c20 4265 726e 6861  h..lkopf, Bernha
-00013190: 7264 2c20 416c 6578 616e 6465 7220 536d  rd, Alexander Sm
-000131a0: 6f6c 612c 2061 6e64 204b 6c61 7573 2d52  ola, and Klaus-R
-000131b0: 6f62 6572 7420 4dc3 bc6c 6c65 722e 0a20  obert M..ller.. 
-000131c0: 2020 2020 2020 224e 6f6e 6c69 6e65 6172        "Nonlinear
-000131d0: 2063 6f6d 706f 6e65 6e74 2061 6e61 6c79   component analy
-000131e0: 7369 7320 6173 2061 206b 6572 6e65 6c20  sis as a kernel 
-000131f0: 6569 6765 6e76 616c 7565 2070 726f 626c  eigenvalue probl
-00013200: 656d 2e22 0a20 2020 2020 2020 4e65 7572  em.".       Neur
-00013210: 616c 2063 6f6d 7075 7461 7469 6f6e 2031  al computation 1
-00013220: 302e 3520 2831 3939 3829 3a20 3132 3939  0.5 (1998): 1299
-00013230: 2d31 3331 392e 0a20 2020 2020 2020 3c68  -1319..       <h
-00013240: 7474 7073 3a2f 2f77 7777 2e6d 6c70 6163  ttps://www.mlpac
-00013250: 6b2e 6f72 672f 7061 7065 7273 2f6b 7063  k.org/papers/kpc
-00013260: 612e 7064 663e 605f 0a0a 2020 2020 4578  a.pdf>`_..    Ex
-00013270: 616d 706c 6573 0a20 2020 202d 2d2d 2d2d  amples.    -----
-00013280: 2d2d 2d0a 2020 2020 3e3e 3e20 6672 6f6d  ---.    >>> from
-00013290: 2073 6b6c 6561 726e 2e70 7265 7072 6f63   sklearn.preproc
-000132a0: 6573 7369 6e67 2069 6d70 6f72 7420 4b65  essing import Ke
-000132b0: 726e 656c 4365 6e74 6572 6572 0a20 2020  rnelCenterer.   
-000132c0: 203e 3e3e 2066 726f 6d20 736b 6c65 6172   >>> from sklear
-000132d0: 6e2e 6d65 7472 6963 732e 7061 6972 7769  n.metrics.pairwi
-000132e0: 7365 2069 6d70 6f72 7420 7061 6972 7769  se import pairwi
-000132f0: 7365 5f6b 6572 6e65 6c73 0a20 2020 203e  se_kernels.    >
-00013300: 3e3e 2058 203d 205b 5b20 312e 2c20 2d32  >> X = [[ 1., -2
-00013310: 2e2c 2020 322e 5d2c 0a20 2020 202e 2e2e  .,  2.],.    ...
-00013320: 2020 2020 2020 5b20 2d32 2e2c 2020 312e        [ -2.,  1.
-00013330: 2c20 2033 2e5d 2c0a 2020 2020 2e2e 2e20  ,  3.],.    ... 
-00013340: 2020 2020 205b 2034 2e2c 2020 312e 2c20       [ 4.,  1., 
-00013350: 2d32 2e5d 5d0a 2020 2020 3e3e 3e20 4b20  -2.]].    >>> K 
-00013360: 3d20 7061 6972 7769 7365 5f6b 6572 6e65  = pairwise_kerne
-00013370: 6c73 2858 2c20 6d65 7472 6963 3d27 6c69  ls(X, metric='li
-00013380: 6e65 6172 2729 0a20 2020 203e 3e3e 204b  near').    >>> K
-00013390: 0a20 2020 2061 7272 6179 285b 5b20 2039  .    array([[  9
-000133a0: 2e2c 2020 2032 2e2c 2020 2d32 2e5d 2c0a  .,   2.,  -2.],.
-000133b0: 2020 2020 2020 2020 2020 205b 2020 322e             [  2.
-000133c0: 2c20 2031 342e 2c20 2d31 332e 5d2c 0a20  ,  14., -13.],. 
-000133d0: 2020 2020 2020 2020 2020 5b20 2d32 2e2c            [ -2.,
-000133e0: 202d 3133 2e2c 2020 3231 2e5d 5d29 0a20   -13.,  21.]]). 
-000133f0: 2020 203e 3e3e 2074 7261 6e73 666f 726d     >>> transform
-00013400: 6572 203d 204b 6572 6e65 6c43 656e 7465  er = KernelCente
-00013410: 7265 7228 292e 6669 7428 4b29 0a20 2020  rer().fit(K).   
-00013420: 203e 3e3e 2074 7261 6e73 666f 726d 6572   >>> transformer
-00013430: 0a20 2020 204b 6572 6e65 6c43 656e 7465  .    KernelCente
-00013440: 7265 7228 290a 2020 2020 3e3e 3e20 7472  rer().    >>> tr
-00013450: 616e 7366 6f72 6d65 722e 7472 616e 7366  ansformer.transf
-00013460: 6f72 6d28 4b29 0a20 2020 2061 7272 6179  orm(K).    array
-00013470: 285b 5b20 2035 2e2c 2020 2030 2e2c 2020  ([[  5.,   0.,  
-00013480: 2d35 2e5d 2c0a 2020 2020 2020 2020 2020  -5.],.          
-00013490: 205b 2020 302e 2c20 2031 342e 2c20 2d31   [  0.,  14., -1
-000134a0: 342e 5d2c 0a20 2020 2020 2020 2020 2020  4.],.           
-000134b0: 5b20 2d35 2e2c 202d 3134 2e2c 2020 3139  [ -5., -14.,  19
-000134c0: 2e5d 5d29 0a20 2020 2022 2222 0a0a 2020  .]]).    """..  
-000134d0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
-000134e0: 656c 6629 3a0a 2020 2020 2020 2020 2320  elf):.        # 
-000134f0: 4e65 6564 6564 2066 6f72 2062 6163 6b70  Needed for backp
-00013500: 6f72 7465 6420 696e 7370 6563 742e 7369  orted inspect.si
-00013510: 676e 6174 7572 6520 636f 6d70 6174 6962  gnature compatib
-00013520: 696c 6974 7920 7769 7468 2050 7950 790a  ility with PyPy.
-00013530: 2020 2020 2020 2020 7061 7373 0a0a 2020          pass..  
-00013540: 2020 6465 6620 6669 7428 7365 6c66 2c20    def fit(self, 
-00013550: 4b2c 2079 3d4e 6f6e 6529 3a0a 2020 2020  K, y=None):.    
-00013560: 2020 2020 2222 2246 6974 204b 6572 6e65      """Fit Kerne
-00013570: 6c43 656e 7465 7265 722e 0a0a 2020 2020  lCenterer...    
-00013580: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
-00013590: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
-000135a0: 2d0a 2020 2020 2020 2020 4b20 3a20 6e64  -.        K : nd
-000135b0: 6172 7261 7920 6f66 2073 6861 7065 2028  array of shape (
-000135c0: 6e5f 7361 6d70 6c65 732c 206e 5f73 616d  n_samples, n_sam
-000135d0: 706c 6573 290a 2020 2020 2020 2020 2020  ples).          
-000135e0: 2020 4b65 726e 656c 206d 6174 7269 782e    Kernel matrix.
-000135f0: 0a0a 2020 2020 2020 2020 7920 3a20 4e6f  ..        y : No
-00013600: 6e65 0a20 2020 2020 2020 2020 2020 2049  ne.            I
-00013610: 676e 6f72 6564 2e0a 0a20 2020 2020 2020  gnored...       
-00013620: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
-00013630: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
-00013640: 2073 656c 6620 3a20 6f62 6a65 6374 0a20   self : object. 
-00013650: 2020 2020 2020 2020 2020 2052 6574 7572             Retur
-00013660: 6e73 2074 6865 2069 6e73 7461 6e63 6520  ns the instance 
-00013670: 6974 7365 6c66 2e0a 2020 2020 2020 2020  itself..        
-00013680: 2222 220a 2020 2020 2020 2020 7870 2c20  """.        xp, 
-00013690: 5f20 3d20 6765 745f 6e61 6d65 7370 6163  _ = get_namespac
-000136a0: 6528 4b29 0a0a 2020 2020 2020 2020 4b20  e(K)..        K 
-000136b0: 3d20 7365 6c66 2e5f 7661 6c69 6461 7465  = self._validate
-000136c0: 5f64 6174 6128 4b2c 2064 7479 7065 3d5f  _data(K, dtype=_
-000136d0: 6172 7261 795f 6170 692e 7375 7070 6f72  array_api.suppor
-000136e0: 7465 645f 666c 6f61 745f 6474 7970 6573  ted_float_dtypes
-000136f0: 2878 7029 290a 0a20 2020 2020 2020 2069  (xp))..        i
-00013700: 6620 4b2e 7368 6170 655b 305d 2021 3d20  f K.shape[0] != 
-00013710: 4b2e 7368 6170 655b 315d 3a0a 2020 2020  K.shape[1]:.    
-00013720: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
-00013730: 6c75 6545 7272 6f72 280a 2020 2020 2020  lueError(.      
-00013740: 2020 2020 2020 2020 2020 224b 6572 6e65            "Kerne
-00013750: 6c20 6d61 7472 6978 206d 7573 7420 6265  l matrix must be
-00013760: 2061 2073 7175 6172 6520 6d61 7472 6978   a square matrix
-00013770: 2e22 0a20 2020 2020 2020 2020 2020 2020  .".             
-00013780: 2020 2022 2049 6e70 7574 2069 7320 6120     " Input is a 
-00013790: 7b7d 787b 7d20 6d61 7472 6978 2e22 2e66  {}x{} matrix.".f
-000137a0: 6f72 6d61 7428 4b2e 7368 6170 655b 305d  ormat(K.shape[0]
-000137b0: 2c20 4b2e 7368 6170 655b 315d 290a 2020  , K.shape[1]).  
-000137c0: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-000137d0: 2020 2020 206e 5f73 616d 706c 6573 203d       n_samples =
-000137e0: 204b 2e73 6861 7065 5b30 5d0a 2020 2020   K.shape[0].    
-000137f0: 2020 2020 7365 6c66 2e4b 5f66 6974 5f72      self.K_fit_r
-00013800: 6f77 735f 203d 2078 702e 7375 6d28 4b2c  ows_ = xp.sum(K,
-00013810: 2061 7869 733d 3029 202f 206e 5f73 616d   axis=0) / n_sam
-00013820: 706c 6573 0a20 2020 2020 2020 2073 656c  ples.        sel
-00013830: 662e 4b5f 6669 745f 616c 6c5f 203d 2078  f.K_fit_all_ = x
-00013840: 702e 7375 6d28 7365 6c66 2e4b 5f66 6974  p.sum(self.K_fit
-00013850: 5f72 6f77 735f 2920 2f20 6e5f 7361 6d70  _rows_) / n_samp
-00013860: 6c65 730a 2020 2020 2020 2020 7265 7475  les.        retu
-00013870: 726e 2073 656c 660a 0a20 2020 2064 6566  rn self..    def
-00013880: 2074 7261 6e73 666f 726d 2873 656c 662c   transform(self,
-00013890: 204b 2c20 636f 7079 3d54 7275 6529 3a0a   K, copy=True):.
-000138a0: 2020 2020 2020 2020 2222 2243 656e 7465          """Cente
-000138b0: 7220 6b65 726e 656c 206d 6174 7269 782e  r kernel matrix.
-000138c0: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
-000138d0: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
-000138e0: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
-000138f0: 4b20 3a20 6e64 6172 7261 7920 6f66 2073  K : ndarray of s
-00013900: 6861 7065 2028 6e5f 7361 6d70 6c65 7331  hape (n_samples1
-00013910: 2c20 6e5f 7361 6d70 6c65 7332 290a 2020  , n_samples2).  
-00013920: 2020 2020 2020 2020 2020 4b65 726e 656c            Kernel
-00013930: 206d 6174 7269 782e 0a0a 2020 2020 2020   matrix...      
-00013940: 2020 636f 7079 203a 2062 6f6f 6c2c 2064    copy : bool, d
-00013950: 6566 6175 6c74 3d54 7275 650a 2020 2020  efault=True.    
-00013960: 2020 2020 2020 2020 5365 7420 746f 2046          Set to F
-00013970: 616c 7365 2074 6f20 7065 7266 6f72 6d20  alse to perform 
-00013980: 696e 706c 6163 6520 636f 6d70 7574 6174  inplace computat
-00013990: 696f 6e2e 0a0a 2020 2020 2020 2020 5265  ion...        Re
-000139a0: 7475 726e 730a 2020 2020 2020 2020 2d2d  turns.        --
-000139b0: 2d2d 2d2d 2d0a 2020 2020 2020 2020 4b5f  -----.        K_
-000139c0: 6e65 7720 3a20 6e64 6172 7261 7920 6f66  new : ndarray of
-000139d0: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
-000139e0: 7331 2c20 6e5f 7361 6d70 6c65 7332 290a  s1, n_samples2).
-000139f0: 2020 2020 2020 2020 2020 2020 5265 7475              Retu
-00013a00: 726e 7320 7468 6520 696e 7374 616e 6365  rns the instance
-00013a10: 2069 7473 656c 662e 0a20 2020 2020 2020   itself..       
-00013a20: 2022 2222 0a20 2020 2020 2020 2063 6865   """.        che
-00013a30: 636b 5f69 735f 6669 7474 6564 2873 656c  ck_is_fitted(sel
-00013a40: 6629 0a0a 2020 2020 2020 2020 7870 2c20  f)..        xp, 
-00013a50: 5f20 3d20 6765 745f 6e61 6d65 7370 6163  _ = get_namespac
-00013a60: 6528 4b29 0a0a 2020 2020 2020 2020 4b20  e(K)..        K 
-00013a70: 3d20 7365 6c66 2e5f 7661 6c69 6461 7465  = self._validate
-00013a80: 5f64 6174 6128 0a20 2020 2020 2020 2020  _data(.         
-00013a90: 2020 204b 2c20 636f 7079 3d63 6f70 792c     K, copy=copy,
-00013aa0: 2064 7479 7065 3d5f 6172 7261 795f 6170   dtype=_array_ap
-00013ab0: 692e 7375 7070 6f72 7465 645f 666c 6f61  i.supported_floa
-00013ac0: 745f 6474 7970 6573 2878 7029 2c20 7265  t_dtypes(xp), re
-00013ad0: 7365 743d 4661 6c73 650a 2020 2020 2020  set=False.      
-00013ae0: 2020 290a 0a20 2020 2020 2020 204b 5f70    )..        K_p
-00013af0: 7265 645f 636f 6c73 203d 2028 7870 2e73  red_cols = (xp.s
-00013b00: 756d 284b 2c20 6178 6973 3d31 2920 2f20  um(K, axis=1) / 
-00013b10: 7365 6c66 2e4b 5f66 6974 5f72 6f77 735f  self.K_fit_rows_
-00013b20: 2e73 6861 7065 5b30 5d29 5b3a 2c20 4e6f  .shape[0])[:, No
-00013b30: 6e65 5d0a 0a20 2020 2020 2020 204b 202d  ne]..        K -
-00013b40: 3d20 7365 6c66 2e4b 5f66 6974 5f72 6f77  = self.K_fit_row
-00013b50: 735f 0a20 2020 2020 2020 204b 202d 3d20  s_.        K -= 
-00013b60: 4b5f 7072 6564 5f63 6f6c 730a 2020 2020  K_pred_cols.    
-00013b70: 2020 2020 4b20 2b3d 2073 656c 662e 4b5f      K += self.K_
-00013b80: 6669 745f 616c 6c5f 0a0a 2020 2020 2020  fit_all_..      
-00013b90: 2020 7265 7475 726e 204b 0a0a 2020 2020    return K..    
-00013ba0: 4070 726f 7065 7274 790a 2020 2020 6465  @property.    de
-00013bb0: 6620 5f6e 5f66 6561 7475 7265 735f 6f75  f _n_features_ou
-00013bc0: 7428 7365 6c66 293a 0a20 2020 2020 2020  t(self):.       
-00013bd0: 2022 2222 4e75 6d62 6572 206f 6620 7472   """Number of tr
-00013be0: 616e 7366 6f72 6d65 6420 6f75 7470 7574  ansformed output
-00013bf0: 2066 6561 7475 7265 732e 2222 220a 2020   features.""".  
-00013c00: 2020 2020 2020 2320 5573 6564 2062 7920        # Used by 
-00013c10: 436c 6173 734e 616d 6550 7265 6669 7846  ClassNamePrefixF
-00013c20: 6561 7475 7265 734f 7574 4d69 7869 6e2e  eaturesOutMixin.
-00013c30: 2054 6869 7320 6d6f 6465 6c20 7072 6573   This model pres
-00013c40: 6572 7665 7320 7468 650a 2020 2020 2020  erves the.      
-00013c50: 2020 2320 6e75 6d62 6572 206f 6620 696e    # number of in
-00013c60: 7075 7420 6665 6174 7572 6573 2062 7574  put features but
-00013c70: 2074 6869 7320 6973 206e 6f74 2061 206f   this is not a o
-00013c80: 6e65 2d74 6f2d 6f6e 6520 6d61 7070 696e  ne-to-one mappin
-00013c90: 6720 696e 2074 6865 0a20 2020 2020 2020  g in the.       
-00013ca0: 2023 2075 7375 616c 2073 656e 7365 2e20   # usual sense. 
-00013cb0: 4865 6e63 6520 7468 6520 6368 6f69 6365  Hence the choice
-00013cc0: 206e 6f74 2074 6f20 7573 6520 4f6e 6554   not to use OneT
-00013cd0: 6f4f 6e65 4665 6174 7572 654d 6978 696e  oOneFeatureMixin
-00013ce0: 2074 6f0a 2020 2020 2020 2020 2320 696d   to.        # im
-00013cf0: 706c 656d 656e 7420 6765 745f 6665 6174  plement get_feat
-00013d00: 7572 655f 6e61 6d65 735f 6f75 7420 666f  ure_names_out fo
-00013d10: 7220 7468 6973 2063 6c61 7373 2e0a 2020  r this class..  
-00013d20: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00013d30: 662e 6e5f 6665 6174 7572 6573 5f69 6e5f  f.n_features_in_
-00013d40: 0a0a 2020 2020 6465 6620 5f6d 6f72 655f  ..    def _more_
-00013d50: 7461 6773 2873 656c 6629 3a0a 2020 2020  tags(self):.    
-00013d60: 2020 2020 7265 7475 726e 207b 2270 6169      return {"pai
-00013d70: 7277 6973 6522 3a20 5472 7565 2c20 2261  rwise": True, "a
-00013d80: 7272 6179 5f61 7069 5f73 7570 706f 7274  rray_api_support
-00013d90: 223a 2054 7275 657d 0a0a 0a40 7661 6c69  ": True}...@vali
-00013da0: 6461 7465 5f70 6172 616d 7328 0a20 2020  date_params(.   
-00013db0: 207b 0a20 2020 2020 2020 2022 5822 3a20   {.        "X": 
-00013dc0: 5b22 6172 7261 792d 6c69 6b65 222c 2022  ["array-like", "
-00013dd0: 7370 6172 7365 206d 6174 7269 7822 5d2c  sparse matrix"],
-00013de0: 0a20 2020 2020 2020 2022 7661 6c75 6522  .        "value"
-00013df0: 3a20 5b49 6e74 6572 7661 6c28 5265 616c  : [Interval(Real
-00013e00: 2c20 4e6f 6e65 2c20 4e6f 6e65 2c20 636c  , None, None, cl
-00013e10: 6f73 6564 3d22 6e65 6974 6865 7222 295d  osed="neither")]
-00013e20: 2c0a 2020 2020 7d2c 0a20 2020 2070 7265  ,.    },.    pre
-00013e30: 6665 725f 736b 6970 5f6e 6573 7465 645f  fer_skip_nested_
-00013e40: 7661 6c69 6461 7469 6f6e 3d54 7275 652c  validation=True,
-00013e50: 0a29 0a64 6566 2061 6464 5f64 756d 6d79  .).def add_dummy
-00013e60: 5f66 6561 7475 7265 2858 2c20 7661 6c75  _feature(X, valu
-00013e70: 653d 312e 3029 3a0a 2020 2020 2222 2241  e=1.0):.    """A
-00013e80: 7567 6d65 6e74 2064 6174 6173 6574 2077  ugment dataset w
-00013e90: 6974 6820 616e 2061 6464 6974 696f 6e61  ith an additiona
-00013ea0: 6c20 6475 6d6d 7920 6665 6174 7572 652e  l dummy feature.
-00013eb0: 0a0a 2020 2020 5468 6973 2069 7320 7573  ..    This is us
-00013ec0: 6566 756c 2066 6f72 2066 6974 7469 6e67  eful for fitting
-00013ed0: 2061 6e20 696e 7465 7263 6570 7420 7465   an intercept te
-00013ee0: 726d 2077 6974 6820 696d 706c 656d 656e  rm with implemen
-00013ef0: 7461 7469 6f6e 7320 7768 6963 680a 2020  tations which.  
-00013f00: 2020 6361 6e6e 6f74 206f 7468 6572 7769    cannot otherwi
-00013f10: 7365 2066 6974 2069 7420 6469 7265 6374  se fit it direct
-00013f20: 6c79 2e0a 0a20 2020 2050 6172 616d 6574  ly...    Paramet
-00013f30: 6572 730a 2020 2020 2d2d 2d2d 2d2d 2d2d  ers.    --------
-00013f40: 2d2d 0a20 2020 2058 203a 207b 6172 7261  --.    X : {arra
-00013f50: 792d 6c69 6b65 2c20 7370 6172 7365 206d  y-like, sparse m
-00013f60: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
-00013f70: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
-00013f80: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-00013f90: 4461 7461 2e0a 0a20 2020 2076 616c 7565  Data...    value
-00013fa0: 203a 2066 6c6f 6174 0a20 2020 2020 2020   : float.       
-00013fb0: 2056 616c 7565 2074 6f20 7573 6520 666f   Value to use fo
-00013fc0: 7220 7468 6520 6475 6d6d 7920 6665 6174  r the dummy feat
-00013fd0: 7572 652e 0a0a 2020 2020 5265 7475 726e  ure...    Return
-00013fe0: 730a 2020 2020 2d2d 2d2d 2d2d 2d0a 2020  s.    -------.  
-00013ff0: 2020 5820 3a20 7b6e 6461 7272 6179 2c20    X : {ndarray, 
-00014000: 7370 6172 7365 206d 6174 7269 787d 206f  sparse matrix} o
-00014010: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
-00014020: 6573 2c20 6e5f 6665 6174 7572 6573 202b  es, n_features +
-00014030: 2031 290a 2020 2020 2020 2020 5361 6d65   1).        Same
-00014040: 2064 6174 6120 7769 7468 2064 756d 6d79   data with dummy
-00014050: 2066 6561 7475 7265 2061 6464 6564 2061   feature added a
-00014060: 7320 6669 7273 7420 636f 6c75 6d6e 2e0a  s first column..
-00014070: 0a20 2020 2045 7861 6d70 6c65 730a 2020  .    Examples.  
-00014080: 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020 203e    --------.    >
-00014090: 3e3e 2066 726f 6d20 736b 6c65 6172 6e2e  >> from sklearn.
-000140a0: 7072 6570 726f 6365 7373 696e 6720 696d  preprocessing im
-000140b0: 706f 7274 2061 6464 5f64 756d 6d79 5f66  port add_dummy_f
-000140c0: 6561 7475 7265 0a20 2020 203e 3e3e 2061  eature.    >>> a
-000140d0: 6464 5f64 756d 6d79 5f66 6561 7475 7265  dd_dummy_feature
-000140e0: 285b 5b30 2c20 315d 2c20 5b31 2c20 305d  ([[0, 1], [1, 0]
-000140f0: 5d29 0a20 2020 2061 7272 6179 285b 5b31  ]).    array([[1
-00014100: 2e2c 2030 2e2c 2031 2e5d 2c0a 2020 2020  ., 0., 1.],.    
-00014110: 2020 2020 2020 205b 312e 2c20 312e 2c20         [1., 1., 
-00014120: 302e 5d5d 290a 2020 2020 2222 220a 2020  0.]]).    """.  
-00014130: 2020 5820 3d20 6368 6563 6b5f 6172 7261    X = check_arra
-00014140: 7928 582c 2061 6363 6570 745f 7370 6172  y(X, accept_spar
-00014150: 7365 3d5b 2263 7363 222c 2022 6373 7222  se=["csc", "csr"
-00014160: 2c20 2263 6f6f 225d 2c20 6474 7970 653d  , "coo"], dtype=
-00014170: 464c 4f41 545f 4454 5950 4553 290a 2020  FLOAT_DTYPES).  
-00014180: 2020 6e5f 7361 6d70 6c65 732c 206e 5f66    n_samples, n_f
-00014190: 6561 7475 7265 7320 3d20 582e 7368 6170  eatures = X.shap
-000141a0: 650a 2020 2020 7368 6170 6520 3d20 286e  e.    shape = (n
-000141b0: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
-000141c0: 7572 6573 202b 2031 290a 2020 2020 6966  ures + 1).    if
-000141d0: 2073 7061 7273 652e 6973 7370 6172 7365   sparse.issparse
-000141e0: 2858 293a 0a20 2020 2020 2020 2069 6620  (X):.        if 
-000141f0: 582e 666f 726d 6174 203d 3d20 2263 6f6f  X.format == "coo
-00014200: 223a 0a20 2020 2020 2020 2020 2020 2023  ":.            #
-00014210: 2053 6869 6674 2063 6f6c 756d 6e73 2074   Shift columns t
-00014220: 6f20 7468 6520 7269 6768 742e 0a20 2020  o the right..   
-00014230: 2020 2020 2020 2020 2063 6f6c 203d 2058           col = X
-00014240: 2e63 6f6c 202b 2031 0a20 2020 2020 2020  .col + 1.       
-00014250: 2020 2020 2023 2043 6f6c 756d 6e20 696e       # Column in
-00014260: 6469 6365 7320 6f66 2064 756d 6d79 2066  dices of dummy f
-00014270: 6561 7475 7265 2061 7265 2030 2065 7665  eature are 0 eve
-00014280: 7279 7768 6572 652e 0a20 2020 2020 2020  rywhere..       
-00014290: 2020 2020 2063 6f6c 203d 206e 702e 636f       col = np.co
-000142a0: 6e63 6174 656e 6174 6528 286e 702e 7a65  ncatenate((np.ze
-000142b0: 726f 7328 6e5f 7361 6d70 6c65 7329 2c20  ros(n_samples), 
-000142c0: 636f 6c29 290a 2020 2020 2020 2020 2020  col)).          
-000142d0: 2020 2320 526f 7720 696e 6469 6365 7320    # Row indices 
-000142e0: 6f66 2064 756d 6d79 2066 6561 7475 7265  of dummy feature
-000142f0: 2061 7265 2030 2c20 2e2e 2e2c 206e 5f73   are 0, ..., n_s
-00014300: 616d 706c 6573 2d31 2e0a 2020 2020 2020  amples-1..      
-00014310: 2020 2020 2020 726f 7720 3d20 6e70 2e63        row = np.c
-00014320: 6f6e 6361 7465 6e61 7465 2828 6e70 2e61  oncatenate((np.a
-00014330: 7261 6e67 6528 6e5f 7361 6d70 6c65 7329  range(n_samples)
-00014340: 2c20 582e 726f 7729 290a 2020 2020 2020  , X.row)).      
-00014350: 2020 2020 2020 2320 5072 6570 656e 6420        # Prepend 
-00014360: 7468 6520 6475 6d6d 7920 6665 6174 7572  the dummy featur
-00014370: 6520 6e5f 7361 6d70 6c65 7320 7469 6d65  e n_samples time
-00014380: 732e 0a20 2020 2020 2020 2020 2020 2064  s..            d
-00014390: 6174 6120 3d20 6e70 2e63 6f6e 6361 7465  ata = np.concate
-000143a0: 6e61 7465 2828 6e70 2e66 756c 6c28 6e5f  nate((np.full(n_
-000143b0: 7361 6d70 6c65 732c 2076 616c 7565 292c  samples, value),
-000143c0: 2058 2e64 6174 6129 290a 2020 2020 2020   X.data)).      
-000143d0: 2020 2020 2020 7265 7475 726e 2073 7061        return spa
-000143e0: 7273 652e 636f 6f5f 6d61 7472 6978 2828  rse.coo_matrix((
-000143f0: 6461 7461 2c20 2872 6f77 2c20 636f 6c29  data, (row, col)
-00014400: 292c 2073 6861 7065 290a 2020 2020 2020  ), shape).      
-00014410: 2020 656c 6966 2058 2e66 6f72 6d61 7420    elif X.format 
-00014420: 3d3d 2022 6373 6322 3a0a 2020 2020 2020  == "csc":.      
-00014430: 2020 2020 2020 2320 5368 6966 7420 696e        # Shift in
-00014440: 6465 7820 706f 696e 7465 7273 2073 696e  dex pointers sin
-00014450: 6365 2077 6520 6e65 6564 2074 6f20 6164  ce we need to ad
-00014460: 6420 6e5f 7361 6d70 6c65 7320 656c 656d  d n_samples elem
-00014470: 656e 7473 2e0a 2020 2020 2020 2020 2020  ents..          
-00014480: 2020 696e 6470 7472 203d 2058 2e69 6e64    indptr = X.ind
-00014490: 7074 7220 2b20 6e5f 7361 6d70 6c65 730a  ptr + n_samples.
-000144a0: 2020 2020 2020 2020 2020 2020 2320 696e              # in
-000144b0: 6470 7472 5b30 5d20 6d75 7374 2062 6520  dptr[0] must be 
-000144c0: 302e 0a20 2020 2020 2020 2020 2020 2069  0..            i
-000144d0: 6e64 7074 7220 3d20 6e70 2e63 6f6e 6361  ndptr = np.conca
-000144e0: 7465 6e61 7465 2828 6e70 2e61 7272 6179  tenate((np.array
-000144f0: 285b 305d 292c 2069 6e64 7074 7229 290a  ([0]), indptr)).
-00014500: 2020 2020 2020 2020 2020 2020 2320 526f              # Ro
-00014510: 7720 696e 6469 6365 7320 6f66 2064 756d  w indices of dum
-00014520: 6d79 2066 6561 7475 7265 2061 7265 2030  my feature are 0
-00014530: 2c20 2e2e 2e2c 206e 5f73 616d 706c 6573  , ..., n_samples
-00014540: 2d31 2e0a 2020 2020 2020 2020 2020 2020  -1..            
-00014550: 696e 6469 6365 7320 3d20 6e70 2e63 6f6e  indices = np.con
-00014560: 6361 7465 6e61 7465 2828 6e70 2e61 7261  catenate((np.ara
-00014570: 6e67 6528 6e5f 7361 6d70 6c65 7329 2c20  nge(n_samples), 
-00014580: 582e 696e 6469 6365 7329 290a 2020 2020  X.indices)).    
-00014590: 2020 2020 2020 2020 2320 5072 6570 656e          # Prepen
-000145a0: 6420 7468 6520 6475 6d6d 7920 6665 6174  d the dummy feat
-000145b0: 7572 6520 6e5f 7361 6d70 6c65 7320 7469  ure n_samples ti
-000145c0: 6d65 732e 0a20 2020 2020 2020 2020 2020  mes..           
-000145d0: 2064 6174 6120 3d20 6e70 2e63 6f6e 6361   data = np.conca
-000145e0: 7465 6e61 7465 2828 6e70 2e66 756c 6c28  tenate((np.full(
-000145f0: 6e5f 7361 6d70 6c65 732c 2076 616c 7565  n_samples, value
-00014600: 292c 2058 2e64 6174 6129 290a 2020 2020  ), X.data)).    
-00014610: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-00014620: 7061 7273 652e 6373 635f 6d61 7472 6978  parse.csc_matrix
-00014630: 2828 6461 7461 2c20 696e 6469 6365 732c  ((data, indices,
-00014640: 2069 6e64 7074 7229 2c20 7368 6170 6529   indptr), shape)
-00014650: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. 
-00014660: 2020 2020 2020 2020 2020 206b 6c61 7373             klass
-00014670: 203d 2058 2e5f 5f63 6c61 7373 5f5f 0a20   = X.__class__. 
-00014680: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-00014690: 6e20 6b6c 6173 7328 6164 645f 6475 6d6d  n klass(add_dumm
-000146a0: 795f 6665 6174 7572 6528 582e 746f 636f  y_feature(X.toco
-000146b0: 6f28 292c 2076 616c 7565 2929 0a20 2020  o(), value)).   
-000146c0: 2065 6c73 653a 0a20 2020 2020 2020 2072   else:.        r
-000146d0: 6574 7572 6e20 6e70 2e68 7374 6163 6b28  eturn np.hstack(
-000146e0: 286e 702e 6675 6c6c 2828 6e5f 7361 6d70  (np.full((n_samp
-000146f0: 6c65 732c 2031 292c 2076 616c 7565 292c  les, 1), value),
-00014700: 2058 2929 0a0a 0a63 6c61 7373 2051 7561   X))...class Qua
-00014710: 6e74 696c 6554 7261 6e73 666f 726d 6572  ntileTransformer
-00014720: 284f 6e65 546f 4f6e 6546 6561 7475 7265  (OneToOneFeature
-00014730: 4d69 7869 6e2c 2054 7261 6e73 666f 726d  Mixin, Transform
-00014740: 6572 4d69 7869 6e2c 2042 6173 6545 7374  erMixin, BaseEst
-00014750: 696d 6174 6f72 293a 0a20 2020 2022 2222  imator):.    """
-00014760: 5472 616e 7366 6f72 6d20 6665 6174 7572  Transform featur
-00014770: 6573 2075 7369 6e67 2071 7561 6e74 696c  es using quantil
-00014780: 6573 2069 6e66 6f72 6d61 7469 6f6e 2e0a  es information..
-00014790: 0a20 2020 2054 6869 7320 6d65 7468 6f64  .    This method
-000147a0: 2074 7261 6e73 666f 726d 7320 7468 6520   transforms the 
-000147b0: 6665 6174 7572 6573 2074 6f20 666f 6c6c  features to foll
-000147c0: 6f77 2061 2075 6e69 666f 726d 206f 7220  ow a uniform or 
-000147d0: 6120 6e6f 726d 616c 0a20 2020 2064 6973  a normal.    dis
-000147e0: 7472 6962 7574 696f 6e2e 2054 6865 7265  tribution. There
-000147f0: 666f 7265 2c20 666f 7220 6120 6769 7665  fore, for a give
-00014800: 6e20 6665 6174 7572 652c 2074 6869 7320  n feature, this 
-00014810: 7472 616e 7366 6f72 6d61 7469 6f6e 2074  transformation t
-00014820: 656e 6473 0a20 2020 2074 6f20 7370 7265  ends.    to spre
-00014830: 6164 206f 7574 2074 6865 206d 6f73 7420  ad out the most 
-00014840: 6672 6571 7565 6e74 2076 616c 7565 732e  frequent values.
-00014850: 2049 7420 616c 736f 2072 6564 7563 6573   It also reduces
-00014860: 2074 6865 2069 6d70 6163 7420 6f66 0a20   the impact of. 
-00014870: 2020 2028 6d61 7267 696e 616c 2920 6f75     (marginal) ou
-00014880: 746c 6965 7273 3a20 7468 6973 2069 7320  tliers: this is 
-00014890: 7468 6572 6566 6f72 6520 6120 726f 6275  therefore a robu
-000148a0: 7374 2070 7265 7072 6f63 6573 7369 6e67  st preprocessing
-000148b0: 2073 6368 656d 652e 0a0a 2020 2020 5468   scheme...    Th
-000148c0: 6520 7472 616e 7366 6f72 6d61 7469 6f6e  e transformation
-000148d0: 2069 7320 6170 706c 6965 6420 6f6e 2065   is applied on e
-000148e0: 6163 6820 6665 6174 7572 6520 696e 6465  ach feature inde
-000148f0: 7065 6e64 656e 746c 792e 2046 6972 7374  pendently. First
-00014900: 2061 6e0a 2020 2020 6573 7469 6d61 7465   an.    estimate
-00014910: 206f 6620 7468 6520 6375 6d75 6c61 7469   of the cumulati
-00014920: 7665 2064 6973 7472 6962 7574 696f 6e20  ve distribution 
-00014930: 6675 6e63 7469 6f6e 206f 6620 6120 6665  function of a fe
-00014940: 6174 7572 6520 6973 0a20 2020 2075 7365  ature is.    use
-00014950: 6420 746f 206d 6170 2074 6865 206f 7269  d to map the ori
-00014960: 6769 6e61 6c20 7661 6c75 6573 2074 6f20  ginal values to 
-00014970: 6120 756e 6966 6f72 6d20 6469 7374 7269  a uniform distri
-00014980: 6275 7469 6f6e 2e20 5468 6520 6f62 7461  bution. The obta
-00014990: 696e 6564 0a20 2020 2076 616c 7565 7320  ined.    values 
-000149a0: 6172 6520 7468 656e 206d 6170 7065 6420  are then mapped 
-000149b0: 746f 2074 6865 2064 6573 6972 6564 206f  to the desired o
-000149c0: 7574 7075 7420 6469 7374 7269 6275 7469  utput distributi
-000149d0: 6f6e 2075 7369 6e67 2074 6865 0a20 2020  on using the.   
-000149e0: 2061 7373 6f63 6961 7465 6420 7175 616e   associated quan
-000149f0: 7469 6c65 2066 756e 6374 696f 6e2e 2046  tile function. F
-00014a00: 6561 7475 7265 7320 7661 6c75 6573 206f  eatures values o
-00014a10: 6620 6e65 772f 756e 7365 656e 2064 6174  f new/unseen dat
-00014a20: 6120 7468 6174 2066 616c 6c0a 2020 2020  a that fall.    
-00014a30: 6265 6c6f 7720 6f72 2061 626f 7665 2074  below or above t
-00014a40: 6865 2066 6974 7465 6420 7261 6e67 6520  he fitted range 
-00014a50: 7769 6c6c 2062 6520 6d61 7070 6564 2074  will be mapped t
-00014a60: 6f20 7468 6520 626f 756e 6473 206f 6620  o the bounds of 
-00014a70: 7468 6520 6f75 7470 7574 0a20 2020 2064  the output.    d
-00014a80: 6973 7472 6962 7574 696f 6e2e 204e 6f74  istribution. Not
-00014a90: 6520 7468 6174 2074 6869 7320 7472 616e  e that this tran
-00014aa0: 7366 6f72 6d20 6973 206e 6f6e 2d6c 696e  sform is non-lin
-00014ab0: 6561 722e 2049 7420 6d61 7920 6469 7374  ear. It may dist
-00014ac0: 6f72 7420 6c69 6e65 6172 0a20 2020 2063  ort linear.    c
-00014ad0: 6f72 7265 6c61 7469 6f6e 7320 6265 7477  orrelations betw
-00014ae0: 6565 6e20 7661 7269 6162 6c65 7320 6d65  een variables me
-00014af0: 6173 7572 6564 2061 7420 7468 6520 7361  asured at the sa
-00014b00: 6d65 2073 6361 6c65 2062 7574 2072 656e  me scale but ren
-00014b10: 6465 7273 0a20 2020 2076 6172 6961 626c  ders.    variabl
-00014b20: 6573 206d 6561 7375 7265 6420 6174 2064  es measured at d
-00014b30: 6966 6665 7265 6e74 2073 6361 6c65 7320  ifferent scales 
-00014b40: 6d6f 7265 2064 6972 6563 746c 7920 636f  more directly co
-00014b50: 6d70 6172 6162 6c65 2e0a 0a20 2020 2046  mparable...    F
-00014b60: 6f72 2065 7861 6d70 6c65 2076 6973 7561  or example visua
-00014b70: 6c69 7a61 7469 6f6e 732c 2072 6566 6572  lizations, refer
-00014b80: 2074 6f20 3a72 6566 3a60 436f 6d70 6172   to :ref:`Compar
-00014b90: 6520 5175 616e 7469 6c65 5472 616e 7366  e QuantileTransf
-00014ba0: 6f72 6d65 7220 7769 7468 0a20 2020 206f  ormer with.    o
-00014bb0: 7468 6572 2073 6361 6c65 7273 203c 706c  ther scalers <pl
-00014bc0: 6f74 5f61 6c6c 5f73 6361 6c69 6e67 5f71  ot_all_scaling_q
-00014bd0: 7561 6e74 696c 655f 7472 616e 7366 6f72  uantile_transfor
-00014be0: 6d65 725f 7365 6374 696f 6e3e 602e 0a0a  mer_section>`...
-00014bf0: 2020 2020 5265 6164 206d 6f72 6520 696e      Read more in
-00014c00: 2074 6865 203a 7265 663a 6055 7365 7220   the :ref:`User 
-00014c10: 4775 6964 6520 3c70 7265 7072 6f63 6573  Guide <preproces
-00014c20: 7369 6e67 5f74 7261 6e73 666f 726d 6572  sing_transformer
-00014c30: 3e60 2e0a 0a20 2020 202e 2e20 7665 7273  >`...    .. vers
-00014c40: 696f 6e61 6464 6564 3a3a 2030 2e31 390a  ionadded:: 0.19.
-00014c50: 0a20 2020 2050 6172 616d 6574 6572 730a  .    Parameters.
-00014c60: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
-00014c70: 2020 206e 5f71 7561 6e74 696c 6573 203a     n_quantiles :
-00014c80: 2069 6e74 2c20 6465 6661 756c 743d 3130   int, default=10
-00014c90: 3030 206f 7220 6e5f 7361 6d70 6c65 730a  00 or n_samples.
-00014ca0: 2020 2020 2020 2020 4e75 6d62 6572 206f          Number o
-00014cb0: 6620 7175 616e 7469 6c65 7320 746f 2062  f quantiles to b
-00014cc0: 6520 636f 6d70 7574 6564 2e20 4974 2063  e computed. It c
-00014cd0: 6f72 7265 7370 6f6e 6473 2074 6f20 7468  orresponds to th
-00014ce0: 6520 6e75 6d62 6572 0a20 2020 2020 2020  e number.       
-00014cf0: 206f 6620 6c61 6e64 6d61 726b 7320 7573   of landmarks us
-00014d00: 6564 2074 6f20 6469 7363 7265 7469 7a65  ed to discretize
-00014d10: 2074 6865 2063 756d 756c 6174 6976 6520   the cumulative 
-00014d20: 6469 7374 7269 6275 7469 6f6e 2066 756e  distribution fun
-00014d30: 6374 696f 6e2e 0a20 2020 2020 2020 2049  ction..        I
-00014d40: 6620 6e5f 7175 616e 7469 6c65 7320 6973  f n_quantiles is
-00014d50: 206c 6172 6765 7220 7468 616e 2074 6865   larger than the
-00014d60: 206e 756d 6265 7220 6f66 2073 616d 706c   number of sampl
-00014d70: 6573 2c20 6e5f 7175 616e 7469 6c65 7320  es, n_quantiles 
-00014d80: 6973 2073 6574 0a20 2020 2020 2020 2074  is set.        t
-00014d90: 6f20 7468 6520 6e75 6d62 6572 206f 6620  o the number of 
-00014da0: 7361 6d70 6c65 7320 6173 2061 206c 6172  samples as a lar
-00014db0: 6765 7220 6e75 6d62 6572 206f 6620 7175  ger number of qu
-00014dc0: 616e 7469 6c65 7320 646f 6573 206e 6f74  antiles does not
-00014dd0: 2067 6976 650a 2020 2020 2020 2020 6120   give.        a 
-00014de0: 6265 7474 6572 2061 7070 726f 7869 6d61  better approxima
-00014df0: 7469 6f6e 206f 6620 7468 6520 6375 6d75  tion of the cumu
-00014e00: 6c61 7469 7665 2064 6973 7472 6962 7574  lative distribut
-00014e10: 696f 6e20 6675 6e63 7469 6f6e 0a20 2020  ion function.   
-00014e20: 2020 2020 2065 7374 696d 6174 6f72 2e0a       estimator..
-00014e30: 0a20 2020 206f 7574 7075 745f 6469 7374  .    output_dist
-00014e40: 7269 6275 7469 6f6e 203a 207b 2775 6e69  ribution : {'uni
-00014e50: 666f 726d 272c 2027 6e6f 726d 616c 277d  form', 'normal'}
-00014e60: 2c20 6465 6661 756c 743d 2775 6e69 666f  , default='unifo
-00014e70: 726d 270a 2020 2020 2020 2020 4d61 7267  rm'.        Marg
-00014e80: 696e 616c 2064 6973 7472 6962 7574 696f  inal distributio
-00014e90: 6e20 666f 7220 7468 6520 7472 616e 7366  n for the transf
-00014ea0: 6f72 6d65 6420 6461 7461 2e20 5468 6520  ormed data. The 
-00014eb0: 6368 6f69 6365 7320 6172 650a 2020 2020  choices are.    
-00014ec0: 2020 2020 2775 6e69 666f 726d 2720 2864      'uniform' (d
-00014ed0: 6566 6175 6c74 2920 6f72 2027 6e6f 726d  efault) or 'norm
-00014ee0: 616c 272e 0a0a 2020 2020 6967 6e6f 7265  al'...    ignore
-00014ef0: 5f69 6d70 6c69 6369 745f 7a65 726f 7320  _implicit_zeros 
-00014f00: 3a20 626f 6f6c 2c20 6465 6661 756c 743d  : bool, default=
-00014f10: 4661 6c73 650a 2020 2020 2020 2020 4f6e  False.        On
-00014f20: 6c79 2061 7070 6c69 6573 2074 6f20 7370  ly applies to sp
-00014f30: 6172 7365 206d 6174 7269 6365 732e 2049  arse matrices. I
-00014f40: 6620 5472 7565 2c20 7468 6520 7370 6172  f True, the spar
-00014f50: 7365 2065 6e74 7269 6573 206f 6620 7468  se entries of th
-00014f60: 650a 2020 2020 2020 2020 6d61 7472 6978  e.        matrix
-00014f70: 2061 7265 2064 6973 6361 7264 6564 2074   are discarded t
-00014f80: 6f20 636f 6d70 7574 6520 7468 6520 7175  o compute the qu
-00014f90: 616e 7469 6c65 2073 7461 7469 7374 6963  antile statistic
-00014fa0: 732e 2049 6620 4661 6c73 652c 0a20 2020  s. If False,.   
-00014fb0: 2020 2020 2074 6865 7365 2065 6e74 7269       these entri
-00014fc0: 6573 2061 7265 2074 7265 6174 6564 2061  es are treated a
-00014fd0: 7320 7a65 726f 732e 0a0a 2020 2020 7375  s zeros...    su
-00014fe0: 6273 616d 706c 6520 3a20 696e 742c 2064  bsample : int, d
-00014ff0: 6566 6175 6c74 3d31 305f 3030 300a 2020  efault=10_000.  
-00015000: 2020 2020 2020 4d61 7869 6d75 6d20 6e75        Maximum nu
-00015010: 6d62 6572 206f 6620 7361 6d70 6c65 7320  mber of samples 
-00015020: 7573 6564 2074 6f20 6573 7469 6d61 7465  used to estimate
-00015030: 2074 6865 2071 7561 6e74 696c 6573 2066   the quantiles f
-00015040: 6f72 0a20 2020 2020 2020 2063 6f6d 7075  or.        compu
-00015050: 7461 7469 6f6e 616c 2065 6666 6963 6965  tational efficie
-00015060: 6e63 792e 204e 6f74 6520 7468 6174 2074  ncy. Note that t
-00015070: 6865 2073 7562 7361 6d70 6c69 6e67 2070  he subsampling p
-00015080: 726f 6365 6475 7265 206d 6179 0a20 2020  rocedure may.   
-00015090: 2020 2020 2064 6966 6665 7220 666f 7220       differ for 
-000150a0: 7661 6c75 652d 6964 656e 7469 6361 6c20  value-identical 
-000150b0: 7370 6172 7365 2061 6e64 2064 656e 7365  sparse and dense
-000150c0: 206d 6174 7269 6365 732e 0a0a 2020 2020   matrices...    
-000150d0: 7261 6e64 6f6d 5f73 7461 7465 203a 2069  random_state : i
-000150e0: 6e74 2c20 5261 6e64 6f6d 5374 6174 6520  nt, RandomState 
-000150f0: 696e 7374 616e 6365 206f 7220 4e6f 6e65  instance or None
-00015100: 2c20 6465 6661 756c 743d 4e6f 6e65 0a20  , default=None. 
-00015110: 2020 2020 2020 2044 6574 6572 6d69 6e65         Determine
-00015120: 7320 7261 6e64 6f6d 206e 756d 6265 7220  s random number 
-00015130: 6765 6e65 7261 7469 6f6e 2066 6f72 2073  generation for s
-00015140: 7562 7361 6d70 6c69 6e67 2061 6e64 2073  ubsampling and s
-00015150: 6d6f 6f74 6869 6e67 0a20 2020 2020 2020  moothing.       
-00015160: 206e 6f69 7365 2e0a 2020 2020 2020 2020   noise..        
-00015170: 506c 6561 7365 2073 6565 2060 6073 7562  Please see ``sub
-00015180: 7361 6d70 6c65 6060 2066 6f72 206d 6f72  sample`` for mor
-00015190: 6520 6465 7461 696c 732e 0a20 2020 2020  e details..     
-000151a0: 2020 2050 6173 7320 616e 2069 6e74 2066     Pass an int f
-000151b0: 6f72 2072 6570 726f 6475 6369 626c 6520  or reproducible 
-000151c0: 7265 7375 6c74 7320 6163 726f 7373 206d  results across m
-000151d0: 756c 7469 706c 6520 6675 6e63 7469 6f6e  ultiple function
-000151e0: 2063 616c 6c73 2e0a 2020 2020 2020 2020   calls..        
-000151f0: 5365 6520 3a74 6572 6d3a 6047 6c6f 7373  See :term:`Gloss
-00015200: 6172 7920 3c72 616e 646f 6d5f 7374 6174  ary <random_stat
-00015210: 653e 602e 0a0a 2020 2020 636f 7079 203a  e>`...    copy :
-00015220: 2062 6f6f 6c2c 2064 6566 6175 6c74 3d54   bool, default=T
-00015230: 7275 650a 2020 2020 2020 2020 5365 7420  rue.        Set 
-00015240: 746f 2046 616c 7365 2074 6f20 7065 7266  to False to perf
-00015250: 6f72 6d20 696e 706c 6163 6520 7472 616e  orm inplace tran
-00015260: 7366 6f72 6d61 7469 6f6e 2061 6e64 2061  sformation and a
-00015270: 766f 6964 2061 2063 6f70 7920 2869 6620  void a copy (if 
-00015280: 7468 650a 2020 2020 2020 2020 696e 7075  the.        inpu
-00015290: 7420 6973 2061 6c72 6561 6479 2061 206e  t is already a n
-000152a0: 756d 7079 2061 7272 6179 292e 0a0a 2020  umpy array)...  
-000152b0: 2020 4174 7472 6962 7574 6573 0a20 2020    Attributes.   
-000152c0: 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020   ----------.    
-000152d0: 6e5f 7175 616e 7469 6c65 735f 203a 2069  n_quantiles_ : i
-000152e0: 6e74 0a20 2020 2020 2020 2054 6865 2061  nt.        The a
-000152f0: 6374 7561 6c20 6e75 6d62 6572 206f 6620  ctual number of 
-00015300: 7175 616e 7469 6c65 7320 7573 6564 2074  quantiles used t
-00015310: 6f20 6469 7363 7265 7469 7a65 2074 6865  o discretize the
-00015320: 2063 756d 756c 6174 6976 650a 2020 2020   cumulative.    
-00015330: 2020 2020 6469 7374 7269 6275 7469 6f6e      distribution
-00015340: 2066 756e 6374 696f 6e2e 0a0a 2020 2020   function...    
-00015350: 7175 616e 7469 6c65 735f 203a 206e 6461  quantiles_ : nda
-00015360: 7272 6179 206f 6620 7368 6170 6520 286e  rray of shape (n
-00015370: 5f71 7561 6e74 696c 6573 2c20 6e5f 6665  _quantiles, n_fe
-00015380: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
-00015390: 5468 6520 7661 6c75 6573 2063 6f72 7265  The values corre
-000153a0: 7370 6f6e 6469 6e67 2074 6865 2071 7561  sponding the qua
-000153b0: 6e74 696c 6573 206f 6620 7265 6665 7265  ntiles of refere
-000153c0: 6e63 652e 0a0a 2020 2020 7265 6665 7265  nce...    refere
-000153d0: 6e63 6573 5f20 3a20 6e64 6172 7261 7920  nces_ : ndarray 
-000153e0: 6f66 2073 6861 7065 2028 6e5f 7175 616e  of shape (n_quan
-000153f0: 7469 6c65 732c 2029 0a20 2020 2020 2020  tiles, ).       
-00015400: 2051 7561 6e74 696c 6573 206f 6620 7265   Quantiles of re
-00015410: 6665 7265 6e63 6573 2e0a 0a20 2020 206e  ferences...    n
-00015420: 5f66 6561 7475 7265 735f 696e 5f20 3a20  _features_in_ : 
-00015430: 696e 740a 2020 2020 2020 2020 4e75 6d62  int.        Numb
-00015440: 6572 206f 6620 6665 6174 7572 6573 2073  er of features s
-00015450: 6565 6e20 6475 7269 6e67 203a 7465 726d  een during :term
-00015460: 3a60 6669 7460 2e0a 0a20 2020 2020 2020  :`fit`...       
-00015470: 202e 2e20 7665 7273 696f 6e61 6464 6564   .. versionadded
-00015480: 3a3a 2030 2e32 340a 0a20 2020 2066 6561  :: 0.24..    fea
-00015490: 7475 7265 5f6e 616d 6573 5f69 6e5f 203a  ture_names_in_ :
-000154a0: 206e 6461 7272 6179 206f 6620 7368 6170   ndarray of shap
-000154b0: 6520 2860 6e5f 6665 6174 7572 6573 5f69  e (`n_features_i
-000154c0: 6e5f 602c 290a 2020 2020 2020 2020 4e61  n_`,).        Na
-000154d0: 6d65 7320 6f66 2066 6561 7475 7265 7320  mes of features 
-000154e0: 7365 656e 2064 7572 696e 6720 3a74 6572  seen during :ter
-000154f0: 6d3a 6066 6974 602e 2044 6566 696e 6564  m:`fit`. Defined
-00015500: 206f 6e6c 7920 7768 656e 2060 5860 0a20   only when `X`. 
-00015510: 2020 2020 2020 2068 6173 2066 6561 7475         has featu
-00015520: 7265 206e 616d 6573 2074 6861 7420 6172  re names that ar
-00015530: 6520 616c 6c20 7374 7269 6e67 732e 0a0a  e all strings...
-00015540: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
-00015550: 6f6e 6164 6465 643a 3a20 312e 300a 0a20  onadded:: 1.0.. 
-00015560: 2020 2053 6565 2041 6c73 6f0a 2020 2020     See Also.    
-00015570: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2071 7561  --------.    qua
-00015580: 6e74 696c 655f 7472 616e 7366 6f72 6d20  ntile_transform 
-00015590: 3a20 4571 7569 7661 6c65 6e74 2066 756e  : Equivalent fun
-000155a0: 6374 696f 6e20 7769 7468 6f75 7420 7468  ction without th
-000155b0: 6520 6573 7469 6d61 746f 7220 4150 492e  e estimator API.
-000155c0: 0a20 2020 2050 6f77 6572 5472 616e 7366  .    PowerTransf
-000155d0: 6f72 6d65 7220 3a20 5065 7266 6f72 6d20  ormer : Perform 
-000155e0: 6d61 7070 696e 6720 746f 2061 206e 6f72  mapping to a nor
-000155f0: 6d61 6c20 6469 7374 7269 6275 7469 6f6e  mal distribution
-00015600: 2075 7369 6e67 2061 2070 6f77 6572 0a20   using a power. 
-00015610: 2020 2020 2020 2074 7261 6e73 666f 726d         transform
-00015620: 2e0a 2020 2020 5374 616e 6461 7264 5363  ..    StandardSc
-00015630: 616c 6572 203a 2050 6572 666f 726d 2073  aler : Perform s
-00015640: 7461 6e64 6172 6469 7a61 7469 6f6e 2074  tandardization t
-00015650: 6861 7420 6973 2066 6173 7465 722c 2062  hat is faster, b
-00015660: 7574 206c 6573 7320 726f 6275 7374 0a20  ut less robust. 
-00015670: 2020 2020 2020 2074 6f20 6f75 746c 6965         to outlie
-00015680: 7273 2e0a 2020 2020 526f 6275 7374 5363  rs..    RobustSc
-00015690: 616c 6572 203a 2050 6572 666f 726d 2072  aler : Perform r
-000156a0: 6f62 7573 7420 7374 616e 6461 7264 697a  obust standardiz
-000156b0: 6174 696f 6e20 7468 6174 2072 656d 6f76  ation that remov
-000156c0: 6573 2074 6865 2069 6e66 6c75 656e 6365  es the influence
-000156d0: 0a20 2020 2020 2020 206f 6620 6f75 746c  .        of outl
-000156e0: 6965 7273 2062 7574 2064 6f65 7320 6e6f  iers but does no
-000156f0: 7420 7075 7420 6f75 746c 6965 7273 2061  t put outliers a
-00015700: 6e64 2069 6e6c 6965 7273 206f 6e20 7468  nd inliers on th
-00015710: 6520 7361 6d65 2073 6361 6c65 2e0a 0a20  e same scale... 
-00015720: 2020 204e 6f74 6573 0a20 2020 202d 2d2d     Notes.    ---
-00015730: 2d2d 0a20 2020 204e 614e 7320 6172 6520  --.    NaNs are 
-00015740: 7472 6561 7465 6420 6173 206d 6973 7369  treated as missi
-00015750: 6e67 2076 616c 7565 733a 2064 6973 7265  ng values: disre
-00015760: 6761 7264 6564 2069 6e20 6669 742c 2061  garded in fit, a
-00015770: 6e64 206d 6169 6e74 6169 6e65 6420 696e  nd maintained in
-00015780: 0a20 2020 2074 7261 6e73 666f 726d 2e0a  .    transform..
-00015790: 0a20 2020 2045 7861 6d70 6c65 730a 2020  .    Examples.  
-000157a0: 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020 203e    --------.    >
-000157b0: 3e3e 2069 6d70 6f72 7420 6e75 6d70 7920  >> import numpy 
-000157c0: 6173 206e 700a 2020 2020 3e3e 3e20 6672  as np.    >>> fr
-000157d0: 6f6d 2073 6b6c 6561 726e 2e70 7265 7072  om sklearn.prepr
-000157e0: 6f63 6573 7369 6e67 2069 6d70 6f72 7420  ocessing import 
-000157f0: 5175 616e 7469 6c65 5472 616e 7366 6f72  QuantileTransfor
-00015800: 6d65 720a 2020 2020 3e3e 3e20 726e 6720  mer.    >>> rng 
-00015810: 3d20 6e70 2e72 616e 646f 6d2e 5261 6e64  = np.random.Rand
-00015820: 6f6d 5374 6174 6528 3029 0a20 2020 203e  omState(0).    >
-00015830: 3e3e 2058 203d 206e 702e 736f 7274 2872  >> X = np.sort(r
-00015840: 6e67 2e6e 6f72 6d61 6c28 6c6f 633d 302e  ng.normal(loc=0.
-00015850: 352c 2073 6361 6c65 3d30 2e32 352c 2073  5, scale=0.25, s
-00015860: 697a 653d 2832 352c 2031 2929 2c20 6178  ize=(25, 1)), ax
-00015870: 6973 3d30 290a 2020 2020 3e3e 3e20 7174  is=0).    >>> qt
-00015880: 203d 2051 7561 6e74 696c 6554 7261 6e73   = QuantileTrans
-00015890: 666f 726d 6572 286e 5f71 7561 6e74 696c  former(n_quantil
-000158a0: 6573 3d31 302c 2072 616e 646f 6d5f 7374  es=10, random_st
-000158b0: 6174 653d 3029 0a20 2020 203e 3e3e 2071  ate=0).    >>> q
-000158c0: 742e 6669 745f 7472 616e 7366 6f72 6d28  t.fit_transform(
-000158d0: 5829 0a20 2020 2061 7272 6179 285b 2e2e  X).    array([..
-000158e0: 2e5d 290a 2020 2020 2222 220a 0a20 2020  .]).    """..   
-000158f0: 205f 7061 7261 6d65 7465 725f 636f 6e73   _parameter_cons
-00015900: 7472 6169 6e74 733a 2064 6963 7420 3d20  traints: dict = 
-00015910: 7b0a 2020 2020 2020 2020 226e 5f71 7561  {.        "n_qua
-00015920: 6e74 696c 6573 223a 205b 496e 7465 7276  ntiles": [Interv
-00015930: 616c 2849 6e74 6567 7261 6c2c 2031 2c20  al(Integral, 1, 
-00015940: 4e6f 6e65 2c20 636c 6f73 6564 3d22 6c65  None, closed="le
-00015950: 6674 2229 5d2c 0a20 2020 2020 2020 2022  ft")],.        "
-00015960: 6f75 7470 7574 5f64 6973 7472 6962 7574  output_distribut
-00015970: 696f 6e22 3a20 5b53 7472 4f70 7469 6f6e  ion": [StrOption
-00015980: 7328 7b22 756e 6966 6f72 6d22 2c20 226e  s({"uniform", "n
-00015990: 6f72 6d61 6c22 7d29 5d2c 0a20 2020 2020  ormal"})],.     
-000159a0: 2020 2022 6967 6e6f 7265 5f69 6d70 6c69     "ignore_impli
-000159b0: 6369 745f 7a65 726f 7322 3a20 5b22 626f  cit_zeros": ["bo
-000159c0: 6f6c 6561 6e22 5d2c 0a20 2020 2020 2020  olean"],.       
-000159d0: 2022 7375 6273 616d 706c 6522 3a20 5b49   "subsample": [I
-000159e0: 6e74 6572 7661 6c28 496e 7465 6772 616c  nterval(Integral
-000159f0: 2c20 312c 204e 6f6e 652c 2063 6c6f 7365  , 1, None, close
-00015a00: 643d 226c 6566 7422 295d 2c0a 2020 2020  d="left")],.    
-00015a10: 2020 2020 2272 616e 646f 6d5f 7374 6174      "random_stat
-00015a20: 6522 3a20 5b22 7261 6e64 6f6d 5f73 7461  e": ["random_sta
-00015a30: 7465 225d 2c0a 2020 2020 2020 2020 2263  te"],.        "c
-00015a40: 6f70 7922 3a20 5b22 626f 6f6c 6561 6e22  opy": ["boolean"
-00015a50: 5d2c 0a20 2020 207d 0a0a 2020 2020 6465  ],.    }..    de
-00015a60: 6620 5f5f 696e 6974 5f5f 280a 2020 2020  f __init__(.    
-00015a70: 2020 2020 7365 6c66 2c0a 2020 2020 2020      self,.      
-00015a80: 2020 2a2c 0a20 2020 2020 2020 206e 5f71    *,.        n_q
-00015a90: 7561 6e74 696c 6573 3d31 3030 302c 0a20  uantiles=1000,. 
-00015aa0: 2020 2020 2020 206f 7574 7075 745f 6469         output_di
-00015ab0: 7374 7269 6275 7469 6f6e 3d22 756e 6966  stribution="unif
-00015ac0: 6f72 6d22 2c0a 2020 2020 2020 2020 6967  orm",.        ig
-00015ad0: 6e6f 7265 5f69 6d70 6c69 6369 745f 7a65  nore_implicit_ze
-00015ae0: 726f 733d 4661 6c73 652c 0a20 2020 2020  ros=False,.     
-00015af0: 2020 2073 7562 7361 6d70 6c65 3d31 305f     subsample=10_
-00015b00: 3030 302c 0a20 2020 2020 2020 2072 616e  000,.        ran
-00015b10: 646f 6d5f 7374 6174 653d 4e6f 6e65 2c0a  dom_state=None,.
-00015b20: 2020 2020 2020 2020 636f 7079 3d54 7275          copy=Tru
-00015b30: 652c 0a20 2020 2029 3a0a 2020 2020 2020  e,.    ):.      
-00015b40: 2020 7365 6c66 2e6e 5f71 7561 6e74 696c    self.n_quantil
-00015b50: 6573 203d 206e 5f71 7561 6e74 696c 6573  es = n_quantiles
-00015b60: 0a20 2020 2020 2020 2073 656c 662e 6f75  .        self.ou
-00015b70: 7470 7574 5f64 6973 7472 6962 7574 696f  tput_distributio
-00015b80: 6e20 3d20 6f75 7470 7574 5f64 6973 7472  n = output_distr
-00015b90: 6962 7574 696f 6e0a 2020 2020 2020 2020  ibution.        
-00015ba0: 7365 6c66 2e69 676e 6f72 655f 696d 706c  self.ignore_impl
-00015bb0: 6963 6974 5f7a 6572 6f73 203d 2069 676e  icit_zeros = ign
-00015bc0: 6f72 655f 696d 706c 6963 6974 5f7a 6572  ore_implicit_zer
-00015bd0: 6f73 0a20 2020 2020 2020 2073 656c 662e  os.        self.
-00015be0: 7375 6273 616d 706c 6520 3d20 7375 6273  subsample = subs
-00015bf0: 616d 706c 650a 2020 2020 2020 2020 7365  ample.        se
-00015c00: 6c66 2e72 616e 646f 6d5f 7374 6174 6520  lf.random_state 
-00015c10: 3d20 7261 6e64 6f6d 5f73 7461 7465 0a20  = random_state. 
-00015c20: 2020 2020 2020 2073 656c 662e 636f 7079         self.copy
-00015c30: 203d 2063 6f70 790a 0a20 2020 2064 6566   = copy..    def
-00015c40: 205f 6465 6e73 655f 6669 7428 7365 6c66   _dense_fit(self
-00015c50: 2c20 582c 2072 616e 646f 6d5f 7374 6174  , X, random_stat
-00015c60: 6529 3a0a 2020 2020 2020 2020 2222 2243  e):.        """C
-00015c70: 6f6d 7075 7465 2070 6572 6365 6e74 696c  ompute percentil
-00015c80: 6573 2066 6f72 2064 656e 7365 206d 6174  es for dense mat
-00015c90: 7269 6365 732e 0a0a 2020 2020 2020 2020  rices...        
-00015ca0: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
-00015cb0: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
-00015cc0: 2020 2020 2020 5820 3a20 6e64 6172 7261        X : ndarra
-00015cd0: 7920 6f66 2073 6861 7065 2028 6e5f 7361  y of shape (n_sa
-00015ce0: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
-00015cf0: 7329 0a20 2020 2020 2020 2020 2020 2054  s).            T
-00015d00: 6865 2064 6174 6120 7573 6564 2074 6f20  he data used to 
-00015d10: 7363 616c 6520 616c 6f6e 6720 7468 6520  scale along the 
-00015d20: 6665 6174 7572 6573 2061 7869 732e 0a20  features axis.. 
-00015d30: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
-00015d40: 2020 2069 6620 7365 6c66 2e69 676e 6f72     if self.ignor
-00015d50: 655f 696d 706c 6963 6974 5f7a 6572 6f73  e_implicit_zeros
-00015d60: 3a0a 2020 2020 2020 2020 2020 2020 7761  :.            wa
-00015d70: 726e 696e 6773 2e77 6172 6e28 0a20 2020  rnings.warn(.   
-00015d80: 2020 2020 2020 2020 2020 2020 2022 2769               "'i
-00015d90: 676e 6f72 655f 696d 706c 6963 6974 5f7a  gnore_implicit_z
-00015da0: 6572 6f73 2720 7461 6b65 7320 6566 6665  eros' takes effe
-00015db0: 6374 206f 6e6c 7920 7769 7468 220a 2020  ct only with".  
-00015dc0: 2020 2020 2020 2020 2020 2020 2020 2220                " 
-00015dd0: 7370 6172 7365 206d 6174 7269 782e 2054  sparse matrix. T
-00015de0: 6869 7320 7061 7261 6d65 7465 7220 6861  his parameter ha
-00015df0: 7320 6e6f 2065 6666 6563 742e 220a 2020  s no effect.".  
-00015e00: 2020 2020 2020 2020 2020 290a 0a20 2020            )..   
-00015e10: 2020 2020 206e 5f73 616d 706c 6573 2c20       n_samples, 
-00015e20: 6e5f 6665 6174 7572 6573 203d 2058 2e73  n_features = X.s
-00015e30: 6861 7065 0a20 2020 2020 2020 2072 6566  hape.        ref
-00015e40: 6572 656e 6365 7320 3d20 7365 6c66 2e72  erences = self.r
-00015e50: 6566 6572 656e 6365 735f 202a 2031 3030  eferences_ * 100
-00015e60: 0a0a 2020 2020 2020 2020 7365 6c66 2e71  ..        self.q
-00015e70: 7561 6e74 696c 6573 5f20 3d20 5b5d 0a20  uantiles_ = []. 
-00015e80: 2020 2020 2020 2066 6f72 2063 6f6c 2069         for col i
-00015e90: 6e20 582e 543a 0a20 2020 2020 2020 2020  n X.T:.         
-00015ea0: 2020 2069 6620 7365 6c66 2e73 7562 7361     if self.subsa
-00015eb0: 6d70 6c65 203c 206e 5f73 616d 706c 6573  mple < n_samples
-00015ec0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-00015ed0: 2020 7375 6273 616d 706c 655f 6964 7820    subsample_idx 
-00015ee0: 3d20 7261 6e64 6f6d 5f73 7461 7465 2e63  = random_state.c
-00015ef0: 686f 6963 6528 0a20 2020 2020 2020 2020  hoice(.         
-00015f00: 2020 2020 2020 2020 2020 206e 5f73 616d             n_sam
-00015f10: 706c 6573 2c20 7369 7a65 3d73 656c 662e  ples, size=self.
-00015f20: 7375 6273 616d 706c 652c 2072 6570 6c61  subsample, repla
-00015f30: 6365 3d46 616c 7365 0a20 2020 2020 2020  ce=False.       
-00015f40: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-00015f50: 2020 2020 2020 2020 2020 2063 6f6c 203d             col =
-00015f60: 2063 6f6c 2e74 616b 6528 7375 6273 616d   col.take(subsam
-00015f70: 706c 655f 6964 782c 206d 6f64 653d 2263  ple_idx, mode="c
-00015f80: 6c69 7022 290a 2020 2020 2020 2020 2020  lip").          
-00015f90: 2020 7365 6c66 2e71 7561 6e74 696c 6573    self.quantiles
-00015fa0: 5f2e 6170 7065 6e64 286e 702e 6e61 6e70  _.append(np.nanp
-00015fb0: 6572 6365 6e74 696c 6528 636f 6c2c 2072  ercentile(col, r
-00015fc0: 6566 6572 656e 6365 7329 290a 2020 2020  eferences)).    
-00015fd0: 2020 2020 7365 6c66 2e71 7561 6e74 696c      self.quantil
-00015fe0: 6573 5f20 3d20 6e70 2e74 7261 6e73 706f  es_ = np.transpo
-00015ff0: 7365 2873 656c 662e 7175 616e 7469 6c65  se(self.quantile
-00016000: 735f 290a 2020 2020 2020 2020 2320 4475  s_).        # Du
-00016010: 6520 746f 2066 6c6f 6174 696e 672d 706f  e to floating-po
-00016020: 696e 7420 7072 6563 6973 696f 6e20 6572  int precision er
-00016030: 726f 7220 696e 2060 6e70 2e6e 616e 7065  ror in `np.nanpe
-00016040: 7263 656e 7469 6c65 602c 0a20 2020 2020  rcentile`,.     
-00016050: 2020 2023 206d 616b 6520 7375 7265 2074     # make sure t
-00016060: 6861 7420 7175 616e 7469 6c65 7320 6172  hat quantiles ar
-00016070: 6520 6d6f 6e6f 746f 6e69 6361 6c6c 7920  e monotonically 
-00016080: 696e 6372 6561 7369 6e67 2e0a 2020 2020  increasing..    
-00016090: 2020 2020 2320 5570 7374 7265 616d 2069      # Upstream i
-000160a0: 7373 7565 2069 6e20 6e75 6d70 793a 0a20  ssue in numpy:. 
-000160b0: 2020 2020 2020 2023 2068 7474 7073 3a2f         # https:/
-000160c0: 2f67 6974 6875 622e 636f 6d2f 6e75 6d70  /github.com/nump
-000160d0: 792f 6e75 6d70 792f 6973 7375 6573 2f31  y/numpy/issues/1
-000160e0: 3436 3835 0a20 2020 2020 2020 2073 656c  4685.        sel
-000160f0: 662e 7175 616e 7469 6c65 735f 203d 206e  f.quantiles_ = n
-00016100: 702e 6d61 7869 6d75 6d2e 6163 6375 6d75  p.maximum.accumu
-00016110: 6c61 7465 2873 656c 662e 7175 616e 7469  late(self.quanti
-00016120: 6c65 735f 290a 0a20 2020 2064 6566 205f  les_)..    def _
-00016130: 7370 6172 7365 5f66 6974 2873 656c 662c  sparse_fit(self,
-00016140: 2058 2c20 7261 6e64 6f6d 5f73 7461 7465   X, random_state
-00016150: 293a 0a20 2020 2020 2020 2022 2222 436f  ):.        """Co
-00016160: 6d70 7574 6520 7065 7263 656e 7469 6c65  mpute percentile
-00016170: 7320 666f 7220 7370 6172 7365 206d 6174  s for sparse mat
-00016180: 7269 6365 732e 0a0a 2020 2020 2020 2020  rices...        
-00016190: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
-000161a0: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
-000161b0: 2020 2020 2020 5820 3a20 7370 6172 7365        X : sparse
-000161c0: 206d 6174 7269 7820 6f66 2073 6861 7065   matrix of shape
-000161d0: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
-000161e0: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
-000161f0: 2020 2020 2054 6865 2064 6174 6120 7573       The data us
-00016200: 6564 2074 6f20 7363 616c 6520 616c 6f6e  ed to scale alon
-00016210: 6720 7468 6520 6665 6174 7572 6573 2061  g the features a
-00016220: 7869 732e 2054 6865 2073 7061 7273 6520  xis. The sparse 
-00016230: 6d61 7472 6978 0a20 2020 2020 2020 2020  matrix.         
-00016240: 2020 206e 6565 6473 2074 6f20 6265 206e     needs to be n
-00016250: 6f6e 6e65 6761 7469 7665 2e20 4966 2061  onnegative. If a
-00016260: 2073 7061 7273 6520 6d61 7472 6978 2069   sparse matrix i
-00016270: 7320 7072 6f76 6964 6564 2c0a 2020 2020  s provided,.    
-00016280: 2020 2020 2020 2020 6974 2077 696c 6c20          it will 
-00016290: 6265 2063 6f6e 7665 7274 6564 2069 6e74  be converted int
-000162a0: 6f20 6120 7370 6172 7365 2060 6063 7363  o a sparse ``csc
-000162b0: 5f6d 6174 7269 7860 602e 0a20 2020 2020  _matrix``..     
-000162c0: 2020 2022 2222 0a20 2020 2020 2020 206e     """.        n
-000162d0: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
-000162e0: 7572 6573 203d 2058 2e73 6861 7065 0a20  ures = X.shape. 
-000162f0: 2020 2020 2020 2072 6566 6572 656e 6365         reference
-00016300: 7320 3d20 7365 6c66 2e72 6566 6572 656e  s = self.referen
-00016310: 6365 735f 202a 2031 3030 0a0a 2020 2020  ces_ * 100..    
-00016320: 2020 2020 7365 6c66 2e71 7561 6e74 696c      self.quantil
-00016330: 6573 5f20 3d20 5b5d 0a20 2020 2020 2020  es_ = [].       
-00016340: 2066 6f72 2066 6561 7475 7265 5f69 6478   for feature_idx
-00016350: 2069 6e20 7261 6e67 6528 6e5f 6665 6174   in range(n_feat
-00016360: 7572 6573 293a 0a20 2020 2020 2020 2020  ures):.         
-00016370: 2020 2063 6f6c 756d 6e5f 6e6e 7a5f 6461     column_nnz_da
-00016380: 7461 203d 2058 2e64 6174 615b 582e 696e  ta = X.data[X.in
-00016390: 6470 7472 5b66 6561 7475 7265 5f69 6478  dptr[feature_idx
-000163a0: 5d20 3a20 582e 696e 6470 7472 5b66 6561  ] : X.indptr[fea
-000163b0: 7475 7265 5f69 6478 202b 2031 5d5d 0a20  ture_idx + 1]]. 
-000163c0: 2020 2020 2020 2020 2020 2069 6620 6c65             if le
-000163d0: 6e28 636f 6c75 6d6e 5f6e 6e7a 5f64 6174  n(column_nnz_dat
-000163e0: 6129 203e 2073 656c 662e 7375 6273 616d  a) > self.subsam
-000163f0: 706c 653a 0a20 2020 2020 2020 2020 2020  ple:.           
-00016400: 2020 2020 2063 6f6c 756d 6e5f 7375 6273       column_subs
-00016410: 616d 706c 6520 3d20 7365 6c66 2e73 7562  ample = self.sub
-00016420: 7361 6d70 6c65 202a 206c 656e 2863 6f6c  sample * len(col
-00016430: 756d 6e5f 6e6e 7a5f 6461 7461 2920 2f2f  umn_nnz_data) //
-00016440: 206e 5f73 616d 706c 6573 0a20 2020 2020   n_samples.     
-00016450: 2020 2020 2020 2020 2020 2069 6620 7365             if se
-00016460: 6c66 2e69 676e 6f72 655f 696d 706c 6963  lf.ignore_implic
-00016470: 6974 5f7a 6572 6f73 3a0a 2020 2020 2020  it_zeros:.      
-00016480: 2020 2020 2020 2020 2020 2020 2020 636f                co
-00016490: 6c75 6d6e 5f64 6174 6120 3d20 6e70 2e7a  lumn_data = np.z
-000164a0: 6572 6f73 2873 6861 7065 3d63 6f6c 756d  eros(shape=colum
-000164b0: 6e5f 7375 6273 616d 706c 652c 2064 7479  n_subsample, dty
-000164c0: 7065 3d58 2e64 7479 7065 290a 2020 2020  pe=X.dtype).    
-000164d0: 2020 2020 2020 2020 2020 2020 656c 7365              else
-000164e0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
-000164f0: 2020 2020 2020 636f 6c75 6d6e 5f64 6174        column_dat
-00016500: 6120 3d20 6e70 2e7a 6572 6f73 2873 6861  a = np.zeros(sha
-00016510: 7065 3d73 656c 662e 7375 6273 616d 706c  pe=self.subsampl
-00016520: 652c 2064 7479 7065 3d58 2e64 7479 7065  e, dtype=X.dtype
-00016530: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00016540: 2020 636f 6c75 6d6e 5f64 6174 615b 3a63    column_data[:c
-00016550: 6f6c 756d 6e5f 7375 6273 616d 706c 655d  olumn_subsample]
-00016560: 203d 2072 616e 646f 6d5f 7374 6174 652e   = random_state.
-00016570: 6368 6f69 6365 280a 2020 2020 2020 2020  choice(.        
-00016580: 2020 2020 2020 2020 2020 2020 636f 6c75              colu
-00016590: 6d6e 5f6e 6e7a 5f64 6174 612c 2073 697a  mn_nnz_data, siz
-000165a0: 653d 636f 6c75 6d6e 5f73 7562 7361 6d70  e=column_subsamp
-000165b0: 6c65 2c20 7265 706c 6163 653d 4661 6c73  le, replace=Fals
-000165c0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-000165d0: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-000165e0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-000165f0: 2020 2020 2020 6966 2073 656c 662e 6967        if self.ig
-00016600: 6e6f 7265 5f69 6d70 6c69 6369 745f 7a65  nore_implicit_ze
-00016610: 726f 733a 0a20 2020 2020 2020 2020 2020  ros:.           
-00016620: 2020 2020 2020 2020 2063 6f6c 756d 6e5f           column_
-00016630: 6461 7461 203d 206e 702e 7a65 726f 7328  data = np.zeros(
-00016640: 7368 6170 653d 6c65 6e28 636f 6c75 6d6e  shape=len(column
-00016650: 5f6e 6e7a 5f64 6174 6129 2c20 6474 7970  _nnz_data), dtyp
-00016660: 653d 582e 6474 7970 6529 0a20 2020 2020  e=X.dtype).     
-00016670: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
-00016680: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00016690: 2020 2020 2063 6f6c 756d 6e5f 6461 7461       column_data
-000166a0: 203d 206e 702e 7a65 726f 7328 7368 6170   = np.zeros(shap
-000166b0: 653d 6e5f 7361 6d70 6c65 732c 2064 7479  e=n_samples, dty
-000166c0: 7065 3d58 2e64 7479 7065 290a 2020 2020  pe=X.dtype).    
-000166d0: 2020 2020 2020 2020 2020 2020 636f 6c75              colu
-000166e0: 6d6e 5f64 6174 615b 3a20 6c65 6e28 636f  mn_data[: len(co
-000166f0: 6c75 6d6e 5f6e 6e7a 5f64 6174 6129 5d20  lumn_nnz_data)] 
-00016700: 3d20 636f 6c75 6d6e 5f6e 6e7a 5f64 6174  = column_nnz_dat
-00016710: 610a 0a20 2020 2020 2020 2020 2020 2069  a..            i
-00016720: 6620 6e6f 7420 636f 6c75 6d6e 5f64 6174  f not column_dat
-00016730: 612e 7369 7a65 3a0a 2020 2020 2020 2020  a.size:.        
-00016740: 2020 2020 2020 2020 2320 6966 206e 6f20          # if no 
-00016750: 6e6e 7a2c 2061 6e20 6572 726f 7220 7769  nnz, an error wi
-00016760: 6c6c 2062 6520 7261 6973 6564 2066 6f72  ll be raised for
-00016770: 2063 6f6d 7075 7469 6e67 2074 6865 0a20   computing the. 
-00016780: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-00016790: 2071 7561 6e74 696c 6573 2e20 466f 7263   quantiles. Forc
-000167a0: 6520 7468 6520 7175 616e 7469 6c65 7320  e the quantiles 
-000167b0: 746f 2062 6520 7a65 726f 732e 0a20 2020  to be zeros..   
-000167c0: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-000167d0: 662e 7175 616e 7469 6c65 735f 2e61 7070  f.quantiles_.app
-000167e0: 656e 6428 5b30 5d20 2a20 6c65 6e28 7265  end([0] * len(re
-000167f0: 6665 7265 6e63 6573 2929 0a20 2020 2020  ferences)).     
-00016800: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   
-00016810: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00016820: 662e 7175 616e 7469 6c65 735f 2e61 7070  f.quantiles_.app
-00016830: 656e 6428 6e70 2e6e 616e 7065 7263 656e  end(np.nanpercen
-00016840: 7469 6c65 2863 6f6c 756d 6e5f 6461 7461  tile(column_data
-00016850: 2c20 7265 6665 7265 6e63 6573 2929 0a20  , references)). 
-00016860: 2020 2020 2020 2073 656c 662e 7175 616e         self.quan
-00016870: 7469 6c65 735f 203d 206e 702e 7472 616e  tiles_ = np.tran
-00016880: 7370 6f73 6528 7365 6c66 2e71 7561 6e74  spose(self.quant
-00016890: 696c 6573 5f29 0a20 2020 2020 2020 2023  iles_).        #
-000168a0: 2064 7565 2074 6f20 666c 6f61 7469 6e67   due to floating
-000168b0: 2d70 6f69 6e74 2070 7265 6369 7369 6f6e  -point precision
-000168c0: 2065 7272 6f72 2069 6e20 606e 702e 6e61   error in `np.na
-000168d0: 6e70 6572 6365 6e74 696c 6560 2c0a 2020  npercentile`,.  
-000168e0: 2020 2020 2020 2320 6d61 6b65 2073 7572        # make sur
-000168f0: 6520 7468 6520 7175 616e 7469 6c65 7320  e the quantiles 
-00016900: 6172 6520 6d6f 6e6f 746f 6e69 6361 6c6c  are monotonicall
-00016910: 7920 696e 6372 6561 7369 6e67 0a20 2020  y increasing.   
-00016920: 2020 2020 2023 2055 7073 7472 6561 6d20       # Upstream 
-00016930: 6973 7375 6520 696e 206e 756d 7079 3a0a  issue in numpy:.
-00016940: 2020 2020 2020 2020 2320 6874 7470 733a          # https:
-00016950: 2f2f 6769 7468 7562 2e63 6f6d 2f6e 756d  //github.com/num
-00016960: 7079 2f6e 756d 7079 2f69 7373 7565 732f  py/numpy/issues/
-00016970: 3134 3638 350a 2020 2020 2020 2020 7365  14685.        se
-00016980: 6c66 2e71 7561 6e74 696c 6573 5f20 3d20  lf.quantiles_ = 
-00016990: 6e70 2e6d 6178 696d 756d 2e61 6363 756d  np.maximum.accum
-000169a0: 756c 6174 6528 7365 6c66 2e71 7561 6e74  ulate(self.quant
-000169b0: 696c 6573 5f29 0a0a 2020 2020 405f 6669  iles_)..    @_fi
-000169c0: 745f 636f 6e74 6578 7428 7072 6566 6572  t_context(prefer
-000169d0: 5f73 6b69 705f 6e65 7374 6564 5f76 616c  _skip_nested_val
-000169e0: 6964 6174 696f 6e3d 5472 7565 290a 2020  idation=True).  
-000169f0: 2020 6465 6620 6669 7428 7365 6c66 2c20    def fit(self, 
-00016a00: 582c 2079 3d4e 6f6e 6529 3a0a 2020 2020  X, y=None):.    
-00016a10: 2020 2020 2222 2243 6f6d 7075 7465 2074      """Compute t
-00016a20: 6865 2071 7561 6e74 696c 6573 2075 7365  he quantiles use
-00016a30: 6420 666f 7220 7472 616e 7366 6f72 6d69  d for transformi
-00016a40: 6e67 2e0a 0a20 2020 2020 2020 2050 6172  ng...        Par
-00016a50: 616d 6574 6572 730a 2020 2020 2020 2020  ameters.        
-00016a60: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
-00016a70: 2020 2058 203a 207b 6172 7261 792d 6c69     X : {array-li
-00016a80: 6b65 2c20 7370 6172 7365 206d 6174 7269  ke, sparse matri
-00016a90: 787d 206f 6620 7368 6170 6520 286e 5f73  x} of shape (n_s
-00016aa0: 616d 706c 6573 2c20 6e5f 6665 6174 7572  amples, n_featur
-00016ab0: 6573 290a 2020 2020 2020 2020 2020 2020  es).            
-00016ac0: 5468 6520 6461 7461 2075 7365 6420 746f  The data used to
-00016ad0: 2073 6361 6c65 2061 6c6f 6e67 2074 6865   scale along the
-00016ae0: 2066 6561 7475 7265 7320 6178 6973 2e20   features axis. 
-00016af0: 4966 2061 2073 7061 7273 650a 2020 2020  If a sparse.    
-00016b00: 2020 2020 2020 2020 6d61 7472 6978 2069          matrix i
-00016b10: 7320 7072 6f76 6964 6564 2c20 6974 2077  s provided, it w
-00016b20: 696c 6c20 6265 2063 6f6e 7665 7274 6564  ill be converted
-00016b30: 2069 6e74 6f20 6120 7370 6172 7365 0a20   into a sparse. 
-00016b40: 2020 2020 2020 2020 2020 2060 6063 7363             ``csc
-00016b50: 5f6d 6174 7269 7860 602e 2041 6464 6974  _matrix``. Addit
-00016b60: 696f 6e61 6c6c 792c 2074 6865 2073 7061  ionally, the spa
-00016b70: 7273 6520 6d61 7472 6978 206e 6565 6473  rse matrix needs
-00016b80: 2074 6f20 6265 0a20 2020 2020 2020 2020   to be.         
-00016b90: 2020 206e 6f6e 6e65 6761 7469 7665 2069     nonnegative i
-00016ba0: 6620 6069 676e 6f72 655f 696d 706c 6963  f `ignore_implic
-00016bb0: 6974 5f7a 6572 6f73 6020 6973 2046 616c  it_zeros` is Fal
-00016bc0: 7365 2e0a 0a20 2020 2020 2020 2079 203a  se...        y :
-00016bd0: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          
-00016be0: 2020 4967 6e6f 7265 642e 0a0a 2020 2020    Ignored...    
-00016bf0: 2020 2020 5265 7475 726e 730a 2020 2020      Returns.    
-00016c00: 2020 2020 2d2d 2d2d 2d2d 2d0a 2020 2020      -------.    
-00016c10: 2020 2020 7365 6c66 203a 206f 626a 6563      self : objec
-00016c20: 740a 2020 2020 2020 2020 2020 2046 6974  t.           Fit
-00016c30: 7465 6420 7472 616e 7366 6f72 6d65 722e  ted transformer.
-00016c40: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-00016c50: 2020 2020 2069 6620 7365 6c66 2e6e 5f71       if self.n_q
-00016c60: 7561 6e74 696c 6573 203e 2073 656c 662e  uantiles > self.
-00016c70: 7375 6273 616d 706c 653a 0a20 2020 2020  subsample:.     
-00016c80: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
-00016c90: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-00016ca0: 2020 2020 2020 2020 2022 5468 6520 6e75           "The nu
-00016cb0: 6d62 6572 206f 6620 7175 616e 7469 6c65  mber of quantile
-00016cc0: 7320 6361 6e6e 6f74 2062 6520 6772 6561  s cannot be grea
-00016cd0: 7465 7220 7468 616e 220a 2020 2020 2020  ter than".      
-00016ce0: 2020 2020 2020 2020 2020 2220 7468 6520            " the 
-00016cf0: 6e75 6d62 6572 206f 6620 7361 6d70 6c65  number of sample
-00016d00: 7320 7573 6564 2e20 476f 7420 7b7d 2071  s used. Got {} q
-00016d10: 7561 6e74 696c 6573 220a 2020 2020 2020  uantiles".      
-00016d20: 2020 2020 2020 2020 2020 2220 616e 6420            " and 
-00016d30: 7b7d 2073 616d 706c 6573 2e22 2e66 6f72  {} samples.".for
-00016d40: 6d61 7428 7365 6c66 2e6e 5f71 7561 6e74  mat(self.n_quant
-00016d50: 696c 6573 2c20 7365 6c66 2e73 7562 7361  iles, self.subsa
-00016d60: 6d70 6c65 290a 2020 2020 2020 2020 2020  mple).          
-00016d70: 2020 290a 0a20 2020 2020 2020 2058 203d    )..        X =
-00016d80: 2073 656c 662e 5f63 6865 636b 5f69 6e70   self._check_inp
-00016d90: 7574 7328 582c 2069 6e5f 6669 743d 5472  uts(X, in_fit=Tr
-00016da0: 7565 2c20 636f 7079 3d46 616c 7365 290a  ue, copy=False).
-00016db0: 2020 2020 2020 2020 6e5f 7361 6d70 6c65          n_sample
-00016dc0: 7320 3d20 582e 7368 6170 655b 305d 0a0a  s = X.shape[0]..
-00016dd0: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
-00016de0: 6e5f 7175 616e 7469 6c65 7320 3e20 6e5f  n_quantiles > n_
-00016df0: 7361 6d70 6c65 733a 0a20 2020 2020 2020  samples:.       
-00016e00: 2020 2020 2077 6172 6e69 6e67 732e 7761       warnings.wa
-00016e10: 726e 280a 2020 2020 2020 2020 2020 2020  rn(.            
-00016e20: 2020 2020 226e 5f71 7561 6e74 696c 6573      "n_quantiles
-00016e30: 2028 2573 2920 6973 2067 7265 6174 6572   (%s) is greater
-00016e40: 2074 6861 6e20 7468 6520 746f 7461 6c20   than the total 
-00016e50: 6e75 6d62 6572 2022 0a20 2020 2020 2020  number ".       
-00016e60: 2020 2020 2020 2020 2022 6f66 2073 616d           "of sam
-00016e70: 706c 6573 2028 2573 292e 206e 5f71 7561  ples (%s). n_qua
-00016e80: 6e74 696c 6573 2069 7320 7365 7420 746f  ntiles is set to
-00016e90: 2022 0a20 2020 2020 2020 2020 2020 2020   ".             
-00016ea0: 2020 2022 6e5f 7361 6d70 6c65 732e 2220     "n_samples." 
-00016eb0: 2520 2873 656c 662e 6e5f 7175 616e 7469  % (self.n_quanti
-00016ec0: 6c65 732c 206e 5f73 616d 706c 6573 290a  les, n_samples).
-00016ed0: 2020 2020 2020 2020 2020 2020 290a 2020              ).  
-00016ee0: 2020 2020 2020 7365 6c66 2e6e 5f71 7561        self.n_qua
-00016ef0: 6e74 696c 6573 5f20 3d20 6d61 7828 312c  ntiles_ = max(1,
-00016f00: 206d 696e 2873 656c 662e 6e5f 7175 616e   min(self.n_quan
-00016f10: 7469 6c65 732c 206e 5f73 616d 706c 6573  tiles, n_samples
-00016f20: 2929 0a0a 2020 2020 2020 2020 726e 6720  ))..        rng 
-00016f30: 3d20 6368 6563 6b5f 7261 6e64 6f6d 5f73  = check_random_s
-00016f40: 7461 7465 2873 656c 662e 7261 6e64 6f6d  tate(self.random
-00016f50: 5f73 7461 7465 290a 0a20 2020 2020 2020  _state)..       
-00016f60: 2023 2043 7265 6174 6520 7468 6520 7175   # Create the qu
-00016f70: 616e 7469 6c65 7320 6f66 2072 6566 6572  antiles of refer
-00016f80: 656e 6365 0a20 2020 2020 2020 2073 656c  ence.        sel
-00016f90: 662e 7265 6665 7265 6e63 6573 5f20 3d20  f.references_ = 
-00016fa0: 6e70 2e6c 696e 7370 6163 6528 302c 2031  np.linspace(0, 1
-00016fb0: 2c20 7365 6c66 2e6e 5f71 7561 6e74 696c  , self.n_quantil
-00016fc0: 6573 5f2c 2065 6e64 706f 696e 743d 5472  es_, endpoint=Tr
-00016fd0: 7565 290a 2020 2020 2020 2020 6966 2073  ue).        if s
-00016fe0: 7061 7273 652e 6973 7370 6172 7365 2858  parse.issparse(X
-00016ff0: 293a 0a20 2020 2020 2020 2020 2020 2073  ):.            s
-00017000: 656c 662e 5f73 7061 7273 655f 6669 7428  elf._sparse_fit(
-00017010: 582c 2072 6e67 290a 2020 2020 2020 2020  X, rng).        
-00017020: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-00017030: 2020 7365 6c66 2e5f 6465 6e73 655f 6669    self._dense_fi
-00017040: 7428 582c 2072 6e67 290a 0a20 2020 2020  t(X, rng)..     
-00017050: 2020 2072 6574 7572 6e20 7365 6c66 0a0a     return self..
-00017060: 2020 2020 6465 6620 5f74 7261 6e73 666f      def _transfo
-00017070: 726d 5f63 6f6c 2873 656c 662c 2058 5f63  rm_col(self, X_c
-00017080: 6f6c 2c20 7175 616e 7469 6c65 732c 2069  ol, quantiles, i
-00017090: 6e76 6572 7365 293a 0a20 2020 2020 2020  nverse):.       
-000170a0: 2022 2222 5072 6976 6174 6520 6675 6e63   """Private func
-000170b0: 7469 6f6e 2074 6f20 7472 616e 7366 6f72  tion to transfor
-000170c0: 6d20 6120 7369 6e67 6c65 2066 6561 7475  m a single featu
-000170d0: 7265 2e22 2222 0a0a 2020 2020 2020 2020  re."""..        
-000170e0: 6f75 7470 7574 5f64 6973 7472 6962 7574  output_distribut
-000170f0: 696f 6e20 3d20 7365 6c66 2e6f 7574 7075  ion = self.outpu
-00017100: 745f 6469 7374 7269 6275 7469 6f6e 0a0a  t_distribution..
-00017110: 2020 2020 2020 2020 6966 206e 6f74 2069          if not i
-00017120: 6e76 6572 7365 3a0a 2020 2020 2020 2020  nverse:.        
-00017130: 2020 2020 6c6f 7765 725f 626f 756e 645f      lower_bound_
-00017140: 7820 3d20 7175 616e 7469 6c65 735b 305d  x = quantiles[0]
-00017150: 0a20 2020 2020 2020 2020 2020 2075 7070  .            upp
-00017160: 6572 5f62 6f75 6e64 5f78 203d 2071 7561  er_bound_x = qua
-00017170: 6e74 696c 6573 5b2d 315d 0a20 2020 2020  ntiles[-1].     
-00017180: 2020 2020 2020 206c 6f77 6572 5f62 6f75         lower_bou
-00017190: 6e64 5f79 203d 2030 0a20 2020 2020 2020  nd_y = 0.       
-000171a0: 2020 2020 2075 7070 6572 5f62 6f75 6e64       upper_bound
-000171b0: 5f79 203d 2031 0a20 2020 2020 2020 2065  _y = 1.        e
-000171c0: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-000171d0: 206c 6f77 6572 5f62 6f75 6e64 5f78 203d   lower_bound_x =
-000171e0: 2030 0a20 2020 2020 2020 2020 2020 2075   0.            u
-000171f0: 7070 6572 5f62 6f75 6e64 5f78 203d 2031  pper_bound_x = 1
-00017200: 0a20 2020 2020 2020 2020 2020 206c 6f77  .            low
-00017210: 6572 5f62 6f75 6e64 5f79 203d 2071 7561  er_bound_y = qua
-00017220: 6e74 696c 6573 5b30 5d0a 2020 2020 2020  ntiles[0].      
-00017230: 2020 2020 2020 7570 7065 725f 626f 756e        upper_boun
-00017240: 645f 7920 3d20 7175 616e 7469 6c65 735b  d_y = quantiles[
-00017250: 2d31 5d0a 2020 2020 2020 2020 2020 2020  -1].            
-00017260: 2320 666f 7220 696e 7665 7273 6520 7472  # for inverse tr
-00017270: 616e 7366 6f72 6d2c 206d 6174 6368 2061  ansform, match a
-00017280: 2075 6e69 666f 726d 2064 6973 7472 6962   uniform distrib
-00017290: 7574 696f 6e0a 2020 2020 2020 2020 2020  ution.          
-000172a0: 2020 7769 7468 206e 702e 6572 7273 7461    with np.errsta
-000172b0: 7465 2869 6e76 616c 6964 3d22 6967 6e6f  te(invalid="igno
-000172c0: 7265 2229 3a20 2023 2068 6964 6520 4e61  re"):  # hide Na
-000172d0: 4e20 636f 6d70 6172 6973 6f6e 2077 6172  N comparison war
-000172e0: 6e69 6e67 730a 2020 2020 2020 2020 2020  nings.          
-000172f0: 2020 2020 2020 6966 206f 7574 7075 745f        if output_
-00017300: 6469 7374 7269 6275 7469 6f6e 203d 3d20  distribution == 
-00017310: 226e 6f72 6d61 6c22 3a0a 2020 2020 2020  "normal":.      
-00017320: 2020 2020 2020 2020 2020 2020 2020 585f                X_
-00017330: 636f 6c20 3d20 7374 6174 732e 6e6f 726d  col = stats.norm
-00017340: 2e63 6466 2858 5f63 6f6c 290a 2020 2020  .cdf(X_col).    
-00017350: 2020 2020 2020 2020 2020 2020 2320 656c              # el
-00017360: 7365 206f 7574 7075 7420 6469 7374 7269  se output distri
-00017370: 6275 7469 6f6e 2069 7320 616c 7265 6164  bution is alread
-00017380: 7920 6120 756e 6966 6f72 6d20 6469 7374  y a uniform dist
-00017390: 7269 6275 7469 6f6e 0a0a 2020 2020 2020  ribution..      
-000173a0: 2020 2320 6669 6e64 2069 6e64 6578 2066    # find index f
-000173b0: 6f72 206c 6f77 6572 2061 6e64 2068 6967  or lower and hig
-000173c0: 6865 7220 626f 756e 6473 0a20 2020 2020  her bounds.     
-000173d0: 2020 2077 6974 6820 6e70 2e65 7272 7374     with np.errst
-000173e0: 6174 6528 696e 7661 6c69 643d 2269 676e  ate(invalid="ign
-000173f0: 6f72 6522 293a 2020 2320 6869 6465 204e  ore"):  # hide N
-00017400: 614e 2063 6f6d 7061 7269 736f 6e20 7761  aN comparison wa
-00017410: 726e 696e 6773 0a20 2020 2020 2020 2020  rnings.         
-00017420: 2020 2069 6620 6f75 7470 7574 5f64 6973     if output_dis
-00017430: 7472 6962 7574 696f 6e20 3d3d 2022 6e6f  tribution == "no
-00017440: 726d 616c 223a 0a20 2020 2020 2020 2020  rmal":.         
-00017450: 2020 2020 2020 206c 6f77 6572 5f62 6f75         lower_bou
-00017460: 6e64 735f 6964 7820 3d20 585f 636f 6c20  nds_idx = X_col 
-00017470: 2d20 424f 554e 4453 5f54 4852 4553 484f  - BOUNDS_THRESHO
-00017480: 4c44 203c 206c 6f77 6572 5f62 6f75 6e64  LD < lower_bound
-00017490: 5f78 0a20 2020 2020 2020 2020 2020 2020  _x.             
-000174a0: 2020 2075 7070 6572 5f62 6f75 6e64 735f     upper_bounds_
-000174b0: 6964 7820 3d20 585f 636f 6c20 2b20 424f  idx = X_col + BO
-000174c0: 554e 4453 5f54 4852 4553 484f 4c44 203e  UNDS_THRESHOLD >
-000174d0: 2075 7070 6572 5f62 6f75 6e64 5f78 0a20   upper_bound_x. 
-000174e0: 2020 2020 2020 2020 2020 2069 6620 6f75             if ou
-000174f0: 7470 7574 5f64 6973 7472 6962 7574 696f  tput_distributio
-00017500: 6e20 3d3d 2022 756e 6966 6f72 6d22 3a0a  n == "uniform":.
-00017510: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017520: 6c6f 7765 725f 626f 756e 6473 5f69 6478  lower_bounds_idx
-00017530: 203d 2058 5f63 6f6c 203d 3d20 6c6f 7765   = X_col == lowe
-00017540: 725f 626f 756e 645f 780a 2020 2020 2020  r_bound_x.      
-00017550: 2020 2020 2020 2020 2020 7570 7065 725f            upper_
-00017560: 626f 756e 6473 5f69 6478 203d 2058 5f63  bounds_idx = X_c
-00017570: 6f6c 203d 3d20 7570 7065 725f 626f 756e  ol == upper_boun
-00017580: 645f 780a 0a20 2020 2020 2020 2069 7366  d_x..        isf
-00017590: 696e 6974 655f 6d61 736b 203d 207e 6e70  inite_mask = ~np
-000175a0: 2e69 736e 616e 2858 5f63 6f6c 290a 2020  .isnan(X_col).  
-000175b0: 2020 2020 2020 585f 636f 6c5f 6669 6e69        X_col_fini
-000175c0: 7465 203d 2058 5f63 6f6c 5b69 7366 696e  te = X_col[isfin
-000175d0: 6974 655f 6d61 736b 5d0a 2020 2020 2020  ite_mask].      
-000175e0: 2020 6966 206e 6f74 2069 6e76 6572 7365    if not inverse
-000175f0: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
-00017600: 496e 7465 7270 6f6c 6174 6520 696e 206f  Interpolate in o
-00017610: 6e65 2064 6972 6563 7469 6f6e 2061 6e64  ne direction and
-00017620: 2069 6e20 7468 6520 6f74 6865 7220 616e   in the other an
-00017630: 6420 7461 6b65 2074 6865 0a20 2020 2020  d take the.     
-00017640: 2020 2020 2020 2023 206d 6561 6e2e 2054         # mean. T
-00017650: 6869 7320 6973 2069 6e20 6361 7365 206f  his is in case o
-00017660: 6620 7265 7065 6174 6564 2076 616c 7565  f repeated value
-00017670: 7320 696e 2074 6865 2066 6561 7475 7265  s in the feature
-00017680: 730a 2020 2020 2020 2020 2020 2020 2320  s.            # 
-00017690: 616e 6420 6865 6e63 6520 7265 7065 6174  and hence repeat
-000176a0: 6564 2071 7561 6e74 696c 6573 0a20 2020  ed quantiles.   
-000176b0: 2020 2020 2020 2020 2023 0a20 2020 2020           #.     
-000176c0: 2020 2020 2020 2023 2049 6620 7765 2064         # If we d
-000176d0: 6f6e 2774 2064 6f20 7468 6973 2c20 6f6e  on't do this, on
-000176e0: 6c79 206f 6e65 2065 7874 7265 6d65 206f  ly one extreme o
-000176f0: 6620 7468 6520 6475 706c 6963 6174 6564  f the duplicated
-00017700: 2069 730a 2020 2020 2020 2020 2020 2020   is.            
-00017710: 2320 7573 6564 2028 7468 6520 7570 7065  # used (the uppe
-00017720: 7220 7768 656e 2077 6520 646f 2061 7363  r when we do asc
-00017730: 656e 6469 6e67 2c20 616e 6420 7468 650a  ending, and the.
-00017740: 2020 2020 2020 2020 2020 2020 2320 6c6f              # lo
-00017750: 7765 7220 666f 7220 6465 7363 656e 6469  wer for descendi
-00017760: 6e67 292e 2057 6520 7461 6b65 2074 6865  ng). We take the
-00017770: 206d 6561 6e20 6f66 2074 6865 7365 2074   mean of these t
-00017780: 776f 0a20 2020 2020 2020 2020 2020 2058  wo.            X
-00017790: 5f63 6f6c 5b69 7366 696e 6974 655f 6d61  _col[isfinite_ma
-000177a0: 736b 5d20 3d20 302e 3520 2a20 280a 2020  sk] = 0.5 * (.  
-000177b0: 2020 2020 2020 2020 2020 2020 2020 6e70                np
-000177c0: 2e69 6e74 6572 7028 585f 636f 6c5f 6669  .interp(X_col_fi
-000177d0: 6e69 7465 2c20 7175 616e 7469 6c65 732c  nite, quantiles,
-000177e0: 2073 656c 662e 7265 6665 7265 6e63 6573   self.references
-000177f0: 5f29 0a20 2020 2020 2020 2020 2020 2020  _).             
-00017800: 2020 202d 206e 702e 696e 7465 7270 282d     - np.interp(-
-00017810: 585f 636f 6c5f 6669 6e69 7465 2c20 2d71  X_col_finite, -q
-00017820: 7561 6e74 696c 6573 5b3a 3a2d 315d 2c20  uantiles[::-1], 
-00017830: 2d73 656c 662e 7265 6665 7265 6e63 6573  -self.references
-00017840: 5f5b 3a3a 2d31 5d29 0a20 2020 2020 2020  _[::-1]).       
-00017850: 2020 2020 2029 0a20 2020 2020 2020 2065       ).        e
-00017860: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-00017870: 2058 5f63 6f6c 5b69 7366 696e 6974 655f   X_col[isfinite_
-00017880: 6d61 736b 5d20 3d20 6e70 2e69 6e74 6572  mask] = np.inter
-00017890: 7028 585f 636f 6c5f 6669 6e69 7465 2c20  p(X_col_finite, 
-000178a0: 7365 6c66 2e72 6566 6572 656e 6365 735f  self.references_
-000178b0: 2c20 7175 616e 7469 6c65 7329 0a0a 2020  , quantiles)..  
-000178c0: 2020 2020 2020 585f 636f 6c5b 7570 7065        X_col[uppe
-000178d0: 725f 626f 756e 6473 5f69 6478 5d20 3d20  r_bounds_idx] = 
-000178e0: 7570 7065 725f 626f 756e 645f 790a 2020  upper_bound_y.  
-000178f0: 2020 2020 2020 585f 636f 6c5b 6c6f 7765        X_col[lowe
-00017900: 725f 626f 756e 6473 5f69 6478 5d20 3d20  r_bounds_idx] = 
-00017910: 6c6f 7765 725f 626f 756e 645f 790a 2020  lower_bound_y.  
-00017920: 2020 2020 2020 2320 666f 7220 666f 7277        # for forw
-00017930: 6172 6420 7472 616e 7366 6f72 6d2c 206d  ard transform, m
-00017940: 6174 6368 2074 6865 206f 7574 7075 7420  atch the output 
-00017950: 6469 7374 7269 6275 7469 6f6e 0a20 2020  distribution.   
-00017960: 2020 2020 2069 6620 6e6f 7420 696e 7665       if not inve
-00017970: 7273 653a 0a20 2020 2020 2020 2020 2020  rse:.           
-00017980: 2077 6974 6820 6e70 2e65 7272 7374 6174   with np.errstat
-00017990: 6528 696e 7661 6c69 643d 2269 676e 6f72  e(invalid="ignor
-000179a0: 6522 293a 2020 2320 6869 6465 204e 614e  e"):  # hide NaN
-000179b0: 2063 6f6d 7061 7269 736f 6e20 7761 726e   comparison warn
-000179c0: 696e 6773 0a20 2020 2020 2020 2020 2020  ings.           
-000179d0: 2020 2020 2069 6620 6f75 7470 7574 5f64       if output_d
-000179e0: 6973 7472 6962 7574 696f 6e20 3d3d 2022  istribution == "
-000179f0: 6e6f 726d 616c 223a 0a20 2020 2020 2020  normal":.       
-00017a00: 2020 2020 2020 2020 2020 2020 2058 5f63               X_c
-00017a10: 6f6c 203d 2073 7461 7473 2e6e 6f72 6d2e  ol = stats.norm.
-00017a20: 7070 6628 585f 636f 6c29 0a20 2020 2020  ppf(X_col).     
-00017a30: 2020 2020 2020 2020 2020 2020 2020 2023                 #
-00017a40: 2066 696e 6420 7468 6520 7661 6c75 6520   find the value 
-00017a50: 746f 2063 6c69 7020 7468 6520 6461 7461  to clip the data
-00017a60: 2074 6f20 6176 6f69 6420 6d61 7070 696e   to avoid mappin
-00017a70: 6720 746f 0a20 2020 2020 2020 2020 2020  g to.           
-00017a80: 2020 2020 2020 2020 2023 2069 6e66 696e           # infin
-00017a90: 6974 792e 2043 6c69 7020 7375 6368 2074  ity. Clip such t
-00017aa0: 6861 7420 7468 6520 696e 7665 7273 6520  hat the inverse 
-00017ab0: 7472 616e 7366 6f72 6d20 7769 6c6c 2062  transform will b
-00017ac0: 650a 2020 2020 2020 2020 2020 2020 2020  e.              
-00017ad0: 2020 2020 2020 2320 636f 6e73 6973 7465        # consiste
-00017ae0: 6e74 0a20 2020 2020 2020 2020 2020 2020  nt.             
-00017af0: 2020 2020 2020 2063 6c69 705f 6d69 6e20         clip_min 
-00017b00: 3d20 7374 6174 732e 6e6f 726d 2e70 7066  = stats.norm.ppf
-00017b10: 2842 4f55 4e44 535f 5448 5245 5348 4f4c  (BOUNDS_THRESHOL
-00017b20: 4420 2d20 6e70 2e73 7061 6369 6e67 2831  D - np.spacing(1
-00017b30: 2929 0a20 2020 2020 2020 2020 2020 2020  )).             
-00017b40: 2020 2020 2020 2063 6c69 705f 6d61 7820         clip_max 
-00017b50: 3d20 7374 6174 732e 6e6f 726d 2e70 7066  = stats.norm.ppf
-00017b60: 2831 202d 2028 424f 554e 4453 5f54 4852  (1 - (BOUNDS_THR
-00017b70: 4553 484f 4c44 202d 206e 702e 7370 6163  ESHOLD - np.spac
-00017b80: 696e 6728 3129 2929 0a20 2020 2020 2020  ing(1))).       
-00017b90: 2020 2020 2020 2020 2020 2020 2058 5f63               X_c
-00017ba0: 6f6c 203d 206e 702e 636c 6970 2858 5f63  ol = np.clip(X_c
-00017bb0: 6f6c 2c20 636c 6970 5f6d 696e 2c20 636c  ol, clip_min, cl
-00017bc0: 6970 5f6d 6178 290a 2020 2020 2020 2020  ip_max).        
-00017bd0: 2020 2020 2020 2020 2320 656c 7365 206f          # else o
-00017be0: 7574 7075 7420 6469 7374 7269 6275 7469  utput distributi
-00017bf0: 6f6e 2069 7320 756e 6966 6f72 6d20 616e  on is uniform an
-00017c00: 6420 7468 6520 7070 6620 6973 2074 6865  d the ppf is the
-00017c10: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017c20: 2023 2069 6465 6e74 6974 7920 6675 6e63   # identity func
-00017c30: 7469 6f6e 2073 6f20 7765 206c 6574 2058  tion so we let X
-00017c40: 5f63 6f6c 2075 6e63 6861 6e67 6564 0a0a  _col unchanged..
-00017c50: 2020 2020 2020 2020 7265 7475 726e 2058          return X
-00017c60: 5f63 6f6c 0a0a 2020 2020 6465 6620 5f63  _col..    def _c
-00017c70: 6865 636b 5f69 6e70 7574 7328 7365 6c66  heck_inputs(self
-00017c80: 2c20 582c 2069 6e5f 6669 742c 2061 6363  , X, in_fit, acc
-00017c90: 6570 745f 7370 6172 7365 5f6e 6567 6174  ept_sparse_negat
-00017ca0: 6976 653d 4661 6c73 652c 2063 6f70 793d  ive=False, copy=
-00017cb0: 4661 6c73 6529 3a0a 2020 2020 2020 2020  False):.        
-00017cc0: 2222 2243 6865 636b 2069 6e70 7574 7320  """Check inputs 
-00017cd0: 6265 666f 7265 2066 6974 2061 6e64 2074  before fit and t
-00017ce0: 7261 6e73 666f 726d 2e22 2222 0a20 2020  ransform.""".   
-00017cf0: 2020 2020 2058 203d 2073 656c 662e 5f76       X = self._v
-00017d00: 616c 6964 6174 655f 6461 7461 280a 2020  alidate_data(.  
-00017d10: 2020 2020 2020 2020 2020 582c 0a20 2020            X,.   
-00017d20: 2020 2020 2020 2020 2072 6573 6574 3d69           reset=i
-00017d30: 6e5f 6669 742c 0a20 2020 2020 2020 2020  n_fit,.         
-00017d40: 2020 2061 6363 6570 745f 7370 6172 7365     accept_sparse
-00017d50: 3d22 6373 6322 2c0a 2020 2020 2020 2020  ="csc",.        
-00017d60: 2020 2020 636f 7079 3d63 6f70 792c 0a20      copy=copy,. 
-00017d70: 2020 2020 2020 2020 2020 2064 7479 7065             dtype
-00017d80: 3d46 4c4f 4154 5f44 5459 5045 532c 0a20  =FLOAT_DTYPES,. 
-00017d90: 2020 2020 2020 2020 2020 2066 6f72 6365             force
-00017da0: 5f61 6c6c 5f66 696e 6974 653d 2261 6c6c  _all_finite="all
-00017db0: 6f77 2d6e 616e 222c 0a20 2020 2020 2020  ow-nan",.       
-00017dc0: 2029 0a20 2020 2020 2020 2023 2077 6520   ).        # we 
-00017dd0: 6f6e 6c79 2061 6363 6570 7420 706f 7369  only accept posi
-00017de0: 7469 7665 2073 7061 7273 6520 6d61 7472  tive sparse matr
-00017df0: 6978 2077 6865 6e20 6967 6e6f 7265 5f69  ix when ignore_i
-00017e00: 6d70 6c69 6369 745f 7a65 726f 7320 6973  mplicit_zeros is
-00017e10: 0a20 2020 2020 2020 2023 2066 616c 7365  .        # false
-00017e20: 2061 6e64 2074 6861 7420 7765 2063 616c   and that we cal
-00017e30: 6c20 6669 7420 6f72 2074 7261 6e73 666f  l fit or transfo
-00017e40: 726d 2e0a 2020 2020 2020 2020 7769 7468  rm..        with
-00017e50: 206e 702e 6572 7273 7461 7465 2869 6e76   np.errstate(inv
-00017e60: 616c 6964 3d22 6967 6e6f 7265 2229 3a20  alid="ignore"): 
-00017e70: 2023 2068 6964 6520 4e61 4e20 636f 6d70   # hide NaN comp
-00017e80: 6172 6973 6f6e 2077 6172 6e69 6e67 730a  arison warnings.
-00017e90: 2020 2020 2020 2020 2020 2020 6966 2028              if (
-00017ea0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00017eb0: 206e 6f74 2061 6363 6570 745f 7370 6172   not accept_spar
-00017ec0: 7365 5f6e 6567 6174 6976 650a 2020 2020  se_negative.    
-00017ed0: 2020 2020 2020 2020 2020 2020 616e 6420              and 
-00017ee0: 6e6f 7420 7365 6c66 2e69 676e 6f72 655f  not self.ignore_
-00017ef0: 696d 706c 6963 6974 5f7a 6572 6f73 0a20  implicit_zeros. 
-00017f00: 2020 2020 2020 2020 2020 2020 2020 2061                 a
-00017f10: 6e64 2028 7370 6172 7365 2e69 7373 7061  nd (sparse.isspa
-00017f20: 7273 6528 5829 2061 6e64 206e 702e 616e  rse(X) and np.an
-00017f30: 7928 582e 6461 7461 203c 2030 2929 0a20  y(X.data < 0)). 
-00017f40: 2020 2020 2020 2020 2020 2029 3a0a 2020             ):.  
-00017f50: 2020 2020 2020 2020 2020 2020 2020 7261                ra
-00017f60: 6973 6520 5661 6c75 6545 7272 6f72 280a  ise ValueError(.
-00017f70: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017f80: 2020 2020 2251 7561 6e74 696c 6554 7261      "QuantileTra
-00017f90: 6e73 666f 726d 6572 206f 6e6c 7920 6163  nsformer only ac
-00017fa0: 6365 7074 7320 6e6f 6e2d 6e65 6761 7469  cepts non-negati
-00017fb0: 7665 2073 7061 7273 6520 6d61 7472 6963  ve sparse matric
-00017fc0: 6573 2e22 0a20 2020 2020 2020 2020 2020  es.".           
-00017fd0: 2020 2020 2029 0a0a 2020 2020 2020 2020       )..        
-00017fe0: 7265 7475 726e 2058 0a0a 2020 2020 6465  return X..    de
-00017ff0: 6620 5f74 7261 6e73 666f 726d 2873 656c  f _transform(sel
-00018000: 662c 2058 2c20 696e 7665 7273 653d 4661  f, X, inverse=Fa
-00018010: 6c73 6529 3a0a 2020 2020 2020 2020 2222  lse):.        ""
-00018020: 2246 6f72 7761 7264 2061 6e64 2069 6e76  "Forward and inv
-00018030: 6572 7365 2074 7261 6e73 666f 726d 2e0a  erse transform..
-00018040: 0a20 2020 2020 2020 2050 6172 616d 6574  .        Paramet
-00018050: 6572 730a 2020 2020 2020 2020 2d2d 2d2d  ers.        ----
-00018060: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2058  ------.        X
-00018070: 203a 206e 6461 7272 6179 206f 6620 7368   : ndarray of sh
-00018080: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
-00018090: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
-000180a0: 2020 2020 2020 2020 5468 6520 6461 7461          The data
-000180b0: 2075 7365 6420 746f 2073 6361 6c65 2061   used to scale a
-000180c0: 6c6f 6e67 2074 6865 2066 6561 7475 7265  long the feature
-000180d0: 7320 6178 6973 2e0a 0a20 2020 2020 2020  s axis...       
-000180e0: 2069 6e76 6572 7365 203a 2062 6f6f 6c2c   inverse : bool,
-000180f0: 2064 6566 6175 6c74 3d46 616c 7365 0a20   default=False. 
-00018100: 2020 2020 2020 2020 2020 2049 6620 4661             If Fa
-00018110: 6c73 652c 2061 7070 6c79 2066 6f72 7761  lse, apply forwa
-00018120: 7264 2074 7261 6e73 666f 726d 2e20 4966  rd transform. If
-00018130: 2054 7275 652c 2061 7070 6c79 0a20 2020   True, apply.   
-00018140: 2020 2020 2020 2020 2069 6e76 6572 7365           inverse
-00018150: 2074 7261 6e73 666f 726d 2e0a 0a20 2020   transform...   
-00018160: 2020 2020 2052 6574 7572 6e73 0a20 2020       Returns.   
-00018170: 2020 2020 202d 2d2d 2d2d 2d2d 0a20 2020       -------.   
-00018180: 2020 2020 2058 203a 206e 6461 7272 6179       X : ndarray
-00018190: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-000181a0: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-000181b0: 290a 2020 2020 2020 2020 2020 2020 5072  ).            Pr
-000181c0: 6f6a 6563 7465 6420 6461 7461 2e0a 2020  ojected data..  
-000181d0: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-000181e0: 2020 6966 2073 7061 7273 652e 6973 7370    if sparse.issp
-000181f0: 6172 7365 2858 293a 0a20 2020 2020 2020  arse(X):.       
-00018200: 2020 2020 2066 6f72 2066 6561 7475 7265       for feature
-00018210: 5f69 6478 2069 6e20 7261 6e67 6528 582e  _idx in range(X.
-00018220: 7368 6170 655b 315d 293a 0a20 2020 2020  shape[1]):.     
-00018230: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
-00018240: 6e5f 736c 6963 6520 3d20 736c 6963 6528  n_slice = slice(
-00018250: 582e 696e 6470 7472 5b66 6561 7475 7265  X.indptr[feature
-00018260: 5f69 6478 5d2c 2058 2e69 6e64 7074 725b  _idx], X.indptr[
-00018270: 6665 6174 7572 655f 6964 7820 2b20 315d  feature_idx + 1]
-00018280: 290a 2020 2020 2020 2020 2020 2020 2020  ).              
-00018290: 2020 582e 6461 7461 5b63 6f6c 756d 6e5f    X.data[column_
-000182a0: 736c 6963 655d 203d 2073 656c 662e 5f74  slice] = self._t
-000182b0: 7261 6e73 666f 726d 5f63 6f6c 280a 2020  ransform_col(.  
-000182c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000182d0: 2020 582e 6461 7461 5b63 6f6c 756d 6e5f    X.data[column_
-000182e0: 736c 6963 655d 2c20 7365 6c66 2e71 7561  slice], self.qua
-000182f0: 6e74 696c 6573 5f5b 3a2c 2066 6561 7475  ntiles_[:, featu
-00018300: 7265 5f69 6478 5d2c 2069 6e76 6572 7365  re_idx], inverse
-00018310: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00018320: 2029 0a20 2020 2020 2020 2065 6c73 653a   ).        else:
-00018330: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-00018340: 2066 6561 7475 7265 5f69 6478 2069 6e20   feature_idx in 
-00018350: 7261 6e67 6528 582e 7368 6170 655b 315d  range(X.shape[1]
-00018360: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-00018370: 2020 2058 5b3a 2c20 6665 6174 7572 655f     X[:, feature_
-00018380: 6964 785d 203d 2073 656c 662e 5f74 7261  idx] = self._tra
-00018390: 6e73 666f 726d 5f63 6f6c 280a 2020 2020  nsform_col(.    
-000183a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000183b0: 585b 3a2c 2066 6561 7475 7265 5f69 6478  X[:, feature_idx
-000183c0: 5d2c 2073 656c 662e 7175 616e 7469 6c65  ], self.quantile
-000183d0: 735f 5b3a 2c20 6665 6174 7572 655f 6964  s_[:, feature_id
-000183e0: 785d 2c20 696e 7665 7273 650a 2020 2020  x], inverse.    
-000183f0: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
-00018400: 2020 2020 2020 2072 6574 7572 6e20 580a         return X.
-00018410: 0a20 2020 2064 6566 2074 7261 6e73 666f  .    def transfo
-00018420: 726d 2873 656c 662c 2058 293a 0a20 2020  rm(self, X):.   
-00018430: 2020 2020 2022 2222 4665 6174 7572 652d       """Feature-
-00018440: 7769 7365 2074 7261 6e73 666f 726d 6174  wise transformat
-00018450: 696f 6e20 6f66 2074 6865 2064 6174 612e  ion of the data.
-00018460: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
-00018470: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
-00018480: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
-00018490: 5820 3a20 7b61 7272 6179 2d6c 696b 652c  X : {array-like,
-000184a0: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
-000184b0: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
-000184c0: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
-000184d0: 0a20 2020 2020 2020 2020 2020 2054 6865  .            The
-000184e0: 2064 6174 6120 7573 6564 2074 6f20 7363   data used to sc
-000184f0: 616c 6520 616c 6f6e 6720 7468 6520 6665  ale along the fe
-00018500: 6174 7572 6573 2061 7869 732e 2049 6620  atures axis. If 
-00018510: 6120 7370 6172 7365 0a20 2020 2020 2020  a sparse.       
-00018520: 2020 2020 206d 6174 7269 7820 6973 2070       matrix is p
-00018530: 726f 7669 6465 642c 2069 7420 7769 6c6c  rovided, it will
-00018540: 2062 6520 636f 6e76 6572 7465 6420 696e   be converted in
-00018550: 746f 2061 2073 7061 7273 650a 2020 2020  to a sparse.    
-00018560: 2020 2020 2020 2020 6060 6373 635f 6d61          ``csc_ma
-00018570: 7472 6978 6060 2e20 4164 6469 7469 6f6e  trix``. Addition
-00018580: 616c 6c79 2c20 7468 6520 7370 6172 7365  ally, the sparse
-00018590: 206d 6174 7269 7820 6e65 6564 7320 746f   matrix needs to
-000185a0: 2062 650a 2020 2020 2020 2020 2020 2020   be.            
-000185b0: 6e6f 6e6e 6567 6174 6976 6520 6966 2060  nonnegative if `
-000185c0: 6967 6e6f 7265 5f69 6d70 6c69 6369 745f  ignore_implicit_
-000185d0: 7a65 726f 7360 2069 7320 4661 6c73 652e  zeros` is False.
-000185e0: 0a0a 2020 2020 2020 2020 5265 7475 726e  ..        Return
-000185f0: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
-00018600: 2d0a 2020 2020 2020 2020 5874 203a 207b  -.        Xt : {
-00018610: 6e64 6172 7261 792c 2073 7061 7273 6520  ndarray, sparse 
-00018620: 6d61 7472 6978 7d20 6f66 2073 6861 7065  matrix} of shape
-00018630: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
-00018640: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
-00018650: 2020 2020 2054 6865 2070 726f 6a65 6374       The project
-00018660: 6564 2064 6174 612e 0a20 2020 2020 2020  ed data..       
-00018670: 2022 2222 0a20 2020 2020 2020 2063 6865   """.        che
-00018680: 636b 5f69 735f 6669 7474 6564 2873 656c  ck_is_fitted(sel
-00018690: 6629 0a20 2020 2020 2020 2058 203d 2073  f).        X = s
-000186a0: 656c 662e 5f63 6865 636b 5f69 6e70 7574  elf._check_input
-000186b0: 7328 582c 2069 6e5f 6669 743d 4661 6c73  s(X, in_fit=Fals
-000186c0: 652c 2063 6f70 793d 7365 6c66 2e63 6f70  e, copy=self.cop
-000186d0: 7929 0a0a 2020 2020 2020 2020 7265 7475  y)..        retu
-000186e0: 726e 2073 656c 662e 5f74 7261 6e73 666f  rn self._transfo
-000186f0: 726d 2858 2c20 696e 7665 7273 653d 4661  rm(X, inverse=Fa
-00018700: 6c73 6529 0a0a 2020 2020 6465 6620 696e  lse)..    def in
-00018710: 7665 7273 655f 7472 616e 7366 6f72 6d28  verse_transform(
-00018720: 7365 6c66 2c20 5829 3a0a 2020 2020 2020  self, X):.      
-00018730: 2020 2222 2242 6163 6b2d 7072 6f6a 6563    """Back-projec
-00018740: 7469 6f6e 2074 6f20 7468 6520 6f72 6967  tion to the orig
-00018750: 696e 616c 2073 7061 6365 2e0a 0a20 2020  inal space...   
-00018760: 2020 2020 2050 6172 616d 6574 6572 730a       Parameters.
-00018770: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d2d          --------
-00018780: 2d2d 0a20 2020 2020 2020 2058 203a 207b  --.        X : {
-00018790: 6172 7261 792d 6c69 6b65 2c20 7370 6172  array-like, spar
-000187a0: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
-000187b0: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
-000187c0: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
-000187d0: 2020 2020 2020 2020 5468 6520 6461 7461          The data
-000187e0: 2075 7365 6420 746f 2073 6361 6c65 2061   used to scale a
-000187f0: 6c6f 6e67 2074 6865 2066 6561 7475 7265  long the feature
-00018800: 7320 6178 6973 2e20 4966 2061 2073 7061  s axis. If a spa
-00018810: 7273 650a 2020 2020 2020 2020 2020 2020  rse.            
-00018820: 6d61 7472 6978 2069 7320 7072 6f76 6964  matrix is provid
-00018830: 6564 2c20 6974 2077 696c 6c20 6265 2063  ed, it will be c
-00018840: 6f6e 7665 7274 6564 2069 6e74 6f20 6120  onverted into a 
-00018850: 7370 6172 7365 0a20 2020 2020 2020 2020  sparse.         
-00018860: 2020 2060 6063 7363 5f6d 6174 7269 7860     ``csc_matrix`
-00018870: 602e 2041 6464 6974 696f 6e61 6c6c 792c  `. Additionally,
-00018880: 2074 6865 2073 7061 7273 6520 6d61 7472   the sparse matr
-00018890: 6978 206e 6565 6473 2074 6f20 6265 0a20  ix needs to be. 
-000188a0: 2020 2020 2020 2020 2020 206e 6f6e 6e65             nonne
-000188b0: 6761 7469 7665 2069 6620 6069 676e 6f72  gative if `ignor
-000188c0: 655f 696d 706c 6963 6974 5f7a 6572 6f73  e_implicit_zeros
-000188d0: 6020 6973 2046 616c 7365 2e0a 0a20 2020  ` is False...   
-000188e0: 2020 2020 2052 6574 7572 6e73 0a20 2020       Returns.   
-000188f0: 2020 2020 202d 2d2d 2d2d 2d2d 0a20 2020       -------.   
-00018900: 2020 2020 2058 7420 3a20 7b6e 6461 7272       Xt : {ndarr
-00018910: 6179 2c20 7370 6172 7365 206d 6174 7269  ay, sparse matri
-00018920: 787d 206f 6620 286e 5f73 616d 706c 6573  x} of (n_samples
-00018930: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
-00018940: 2020 2020 2020 2020 2020 5468 6520 7072            The pr
-00018950: 6f6a 6563 7465 6420 6461 7461 2e0a 2020  ojected data..  
-00018960: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-00018970: 2020 6368 6563 6b5f 6973 5f66 6974 7465    check_is_fitte
-00018980: 6428 7365 6c66 290a 2020 2020 2020 2020  d(self).        
-00018990: 5820 3d20 7365 6c66 2e5f 6368 6563 6b5f  X = self._check_
-000189a0: 696e 7075 7473 280a 2020 2020 2020 2020  inputs(.        
-000189b0: 2020 2020 582c 2069 6e5f 6669 743d 4661      X, in_fit=Fa
-000189c0: 6c73 652c 2061 6363 6570 745f 7370 6172  lse, accept_spar
-000189d0: 7365 5f6e 6567 6174 6976 653d 5472 7565  se_negative=True
-000189e0: 2c20 636f 7079 3d73 656c 662e 636f 7079  , copy=self.copy
-000189f0: 0a20 2020 2020 2020 2029 0a0a 2020 2020  .        )..    
-00018a00: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-00018a10: 5f74 7261 6e73 666f 726d 2858 2c20 696e  _transform(X, in
-00018a20: 7665 7273 653d 5472 7565 290a 0a20 2020  verse=True)..   
-00018a30: 2064 6566 205f 6d6f 7265 5f74 6167 7328   def _more_tags(
-00018a40: 7365 6c66 293a 0a20 2020 2020 2020 2072  self):.        r
-00018a50: 6574 7572 6e20 7b22 616c 6c6f 775f 6e61  eturn {"allow_na
-00018a60: 6e22 3a20 5472 7565 7d0a 0a0a 4076 616c  n": True}...@val
-00018a70: 6964 6174 655f 7061 7261 6d73 280a 2020  idate_params(.  
-00018a80: 2020 7b22 5822 3a20 5b22 6172 7261 792d    {"X": ["array-
-00018a90: 6c69 6b65 222c 2022 7370 6172 7365 206d  like", "sparse m
-00018aa0: 6174 7269 7822 5d2c 2022 6178 6973 223a  atrix"], "axis":
-00018ab0: 205b 4f70 7469 6f6e 7328 496e 7465 6772   [Options(Integr
-00018ac0: 616c 2c20 7b30 2c20 317d 295d 7d2c 0a20  al, {0, 1})]},. 
-00018ad0: 2020 2070 7265 6665 725f 736b 6970 5f6e     prefer_skip_n
-00018ae0: 6573 7465 645f 7661 6c69 6461 7469 6f6e  ested_validation
-00018af0: 3d46 616c 7365 2c0a 290a 6465 6620 7175  =False,.).def qu
-00018b00: 616e 7469 6c65 5f74 7261 6e73 666f 726d  antile_transform
-00018b10: 280a 2020 2020 582c 0a20 2020 202a 2c0a  (.    X,.    *,.
-00018b20: 2020 2020 6178 6973 3d30 2c0a 2020 2020      axis=0,.    
-00018b30: 6e5f 7175 616e 7469 6c65 733d 3130 3030  n_quantiles=1000
-00018b40: 2c0a 2020 2020 6f75 7470 7574 5f64 6973  ,.    output_dis
-00018b50: 7472 6962 7574 696f 6e3d 2275 6e69 666f  tribution="unifo
-00018b60: 726d 222c 0a20 2020 2069 676e 6f72 655f  rm",.    ignore_
-00018b70: 696d 706c 6963 6974 5f7a 6572 6f73 3d46  implicit_zeros=F
-00018b80: 616c 7365 2c0a 2020 2020 7375 6273 616d  alse,.    subsam
-00018b90: 706c 653d 696e 7428 3165 3529 2c0a 2020  ple=int(1e5),.  
-00018ba0: 2020 7261 6e64 6f6d 5f73 7461 7465 3d4e    random_state=N
-00018bb0: 6f6e 652c 0a20 2020 2063 6f70 793d 5472  one,.    copy=Tr
-00018bc0: 7565 2c0a 293a 0a20 2020 2022 2222 5472  ue,.):.    """Tr
-00018bd0: 616e 7366 6f72 6d20 6665 6174 7572 6573  ansform features
-00018be0: 2075 7369 6e67 2071 7561 6e74 696c 6573   using quantiles
-00018bf0: 2069 6e66 6f72 6d61 7469 6f6e 2e0a 0a20   information... 
-00018c00: 2020 2054 6869 7320 6d65 7468 6f64 2074     This method t
-00018c10: 7261 6e73 666f 726d 7320 7468 6520 6665  ransforms the fe
-00018c20: 6174 7572 6573 2074 6f20 666f 6c6c 6f77  atures to follow
-00018c30: 2061 2075 6e69 666f 726d 206f 7220 6120   a uniform or a 
-00018c40: 6e6f 726d 616c 0a20 2020 2064 6973 7472  normal.    distr
-00018c50: 6962 7574 696f 6e2e 2054 6865 7265 666f  ibution. Therefo
-00018c60: 7265 2c20 666f 7220 6120 6769 7665 6e20  re, for a given 
-00018c70: 6665 6174 7572 652c 2074 6869 7320 7472  feature, this tr
-00018c80: 616e 7366 6f72 6d61 7469 6f6e 2074 656e  ansformation ten
-00018c90: 6473 0a20 2020 2074 6f20 7370 7265 6164  ds.    to spread
-00018ca0: 206f 7574 2074 6865 206d 6f73 7420 6672   out the most fr
-00018cb0: 6571 7565 6e74 2076 616c 7565 732e 2049  equent values. I
-00018cc0: 7420 616c 736f 2072 6564 7563 6573 2074  t also reduces t
-00018cd0: 6865 2069 6d70 6163 7420 6f66 0a20 2020  he impact of.   
-00018ce0: 2028 6d61 7267 696e 616c 2920 6f75 746c   (marginal) outl
-00018cf0: 6965 7273 3a20 7468 6973 2069 7320 7468  iers: this is th
-00018d00: 6572 6566 6f72 6520 6120 726f 6275 7374  erefore a robust
-00018d10: 2070 7265 7072 6f63 6573 7369 6e67 2073   preprocessing s
-00018d20: 6368 656d 652e 0a0a 2020 2020 5468 6520  cheme...    The 
-00018d30: 7472 616e 7366 6f72 6d61 7469 6f6e 2069  transformation i
-00018d40: 7320 6170 706c 6965 6420 6f6e 2065 6163  s applied on eac
-00018d50: 6820 6665 6174 7572 6520 696e 6465 7065  h feature indepe
-00018d60: 6e64 656e 746c 792e 2046 6972 7374 2061  ndently. First a
-00018d70: 6e0a 2020 2020 6573 7469 6d61 7465 206f  n.    estimate o
-00018d80: 6620 7468 6520 6375 6d75 6c61 7469 7665  f the cumulative
-00018d90: 2064 6973 7472 6962 7574 696f 6e20 6675   distribution fu
-00018da0: 6e63 7469 6f6e 206f 6620 6120 6665 6174  nction of a feat
-00018db0: 7572 6520 6973 0a20 2020 2075 7365 6420  ure is.    used 
-00018dc0: 746f 206d 6170 2074 6865 206f 7269 6769  to map the origi
-00018dd0: 6e61 6c20 7661 6c75 6573 2074 6f20 6120  nal values to a 
-00018de0: 756e 6966 6f72 6d20 6469 7374 7269 6275  uniform distribu
-00018df0: 7469 6f6e 2e20 5468 6520 6f62 7461 696e  tion. The obtain
-00018e00: 6564 0a20 2020 2076 616c 7565 7320 6172  ed.    values ar
-00018e10: 6520 7468 656e 206d 6170 7065 6420 746f  e then mapped to
-00018e20: 2074 6865 2064 6573 6972 6564 206f 7574   the desired out
-00018e30: 7075 7420 6469 7374 7269 6275 7469 6f6e  put distribution
-00018e40: 2075 7369 6e67 2074 6865 0a20 2020 2061   using the.    a
-00018e50: 7373 6f63 6961 7465 6420 7175 616e 7469  ssociated quanti
-00018e60: 6c65 2066 756e 6374 696f 6e2e 2046 6561  le function. Fea
-00018e70: 7475 7265 7320 7661 6c75 6573 206f 6620  tures values of 
-00018e80: 6e65 772f 756e 7365 656e 2064 6174 6120  new/unseen data 
-00018e90: 7468 6174 2066 616c 6c0a 2020 2020 6265  that fall.    be
-00018ea0: 6c6f 7720 6f72 2061 626f 7665 2074 6865  low or above the
-00018eb0: 2066 6974 7465 6420 7261 6e67 6520 7769   fitted range wi
-00018ec0: 6c6c 2062 6520 6d61 7070 6564 2074 6f20  ll be mapped to 
-00018ed0: 7468 6520 626f 756e 6473 206f 6620 7468  the bounds of th
-00018ee0: 6520 6f75 7470 7574 0a20 2020 2064 6973  e output.    dis
-00018ef0: 7472 6962 7574 696f 6e2e 204e 6f74 6520  tribution. Note 
-00018f00: 7468 6174 2074 6869 7320 7472 616e 7366  that this transf
-00018f10: 6f72 6d20 6973 206e 6f6e 2d6c 696e 6561  orm is non-linea
-00018f20: 722e 2049 7420 6d61 7920 6469 7374 6f72  r. It may distor
-00018f30: 7420 6c69 6e65 6172 0a20 2020 2063 6f72  t linear.    cor
-00018f40: 7265 6c61 7469 6f6e 7320 6265 7477 6565  relations betwee
-00018f50: 6e20 7661 7269 6162 6c65 7320 6d65 6173  n variables meas
-00018f60: 7572 6564 2061 7420 7468 6520 7361 6d65  ured at the same
-00018f70: 2073 6361 6c65 2062 7574 2072 656e 6465   scale but rende
-00018f80: 7273 0a20 2020 2076 6172 6961 626c 6573  rs.    variables
-00018f90: 206d 6561 7375 7265 6420 6174 2064 6966   measured at dif
-00018fa0: 6665 7265 6e74 2073 6361 6c65 7320 6d6f  ferent scales mo
-00018fb0: 7265 2064 6972 6563 746c 7920 636f 6d70  re directly comp
-00018fc0: 6172 6162 6c65 2e0a 0a20 2020 2052 6561  arable...    Rea
-00018fd0: 6420 6d6f 7265 2069 6e20 7468 6520 3a72  d more in the :r
-00018fe0: 6566 3a60 5573 6572 2047 7569 6465 203c  ef:`User Guide <
-00018ff0: 7072 6570 726f 6365 7373 696e 675f 7472  preprocessing_tr
-00019000: 616e 7366 6f72 6d65 723e 602e 0a0a 2020  ansformer>`...  
-00019010: 2020 5061 7261 6d65 7465 7273 0a20 2020    Parameters.   
-00019020: 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020   ----------.    
-00019030: 5820 3a20 7b61 7272 6179 2d6c 696b 652c  X : {array-like,
-00019040: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
-00019050: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
-00019060: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
-00019070: 0a20 2020 2020 2020 2054 6865 2064 6174  .        The dat
-00019080: 6120 746f 2074 7261 6e73 666f 726d 2e0a  a to transform..
-00019090: 0a20 2020 2061 7869 7320 3a20 696e 742c  .    axis : int,
-000190a0: 2064 6566 6175 6c74 3d30 0a20 2020 2020   default=0.     
-000190b0: 2020 2041 7869 7320 7573 6564 2074 6f20     Axis used to 
-000190c0: 636f 6d70 7574 6520 7468 6520 6d65 616e  compute the mean
-000190d0: 7320 616e 6420 7374 616e 6461 7264 2064  s and standard d
-000190e0: 6576 6961 7469 6f6e 7320 616c 6f6e 672e  eviations along.
-000190f0: 2049 6620 302c 0a20 2020 2020 2020 2074   If 0,.        t
-00019100: 7261 6e73 666f 726d 2065 6163 6820 6665  ransform each fe
-00019110: 6174 7572 652c 206f 7468 6572 7769 7365  ature, otherwise
-00019120: 2028 6966 2031 2920 7472 616e 7366 6f72   (if 1) transfor
-00019130: 6d20 6561 6368 2073 616d 706c 652e 0a0a  m each sample...
-00019140: 2020 2020 6e5f 7175 616e 7469 6c65 7320      n_quantiles 
-00019150: 3a20 696e 742c 2064 6566 6175 6c74 3d31  : int, default=1
-00019160: 3030 3020 6f72 206e 5f73 616d 706c 6573  000 or n_samples
-00019170: 0a20 2020 2020 2020 204e 756d 6265 7220  .        Number 
-00019180: 6f66 2071 7561 6e74 696c 6573 2074 6f20  of quantiles to 
-00019190: 6265 2063 6f6d 7075 7465 642e 2049 7420  be computed. It 
-000191a0: 636f 7272 6573 706f 6e64 7320 746f 2074  corresponds to t
-000191b0: 6865 206e 756d 6265 720a 2020 2020 2020  he number.      
-000191c0: 2020 6f66 206c 616e 646d 6172 6b73 2075    of landmarks u
-000191d0: 7365 6420 746f 2064 6973 6372 6574 697a  sed to discretiz
-000191e0: 6520 7468 6520 6375 6d75 6c61 7469 7665  e the cumulative
-000191f0: 2064 6973 7472 6962 7574 696f 6e20 6675   distribution fu
-00019200: 6e63 7469 6f6e 2e0a 2020 2020 2020 2020  nction..        
-00019210: 4966 206e 5f71 7561 6e74 696c 6573 2069  If n_quantiles i
-00019220: 7320 6c61 7267 6572 2074 6861 6e20 7468  s larger than th
-00019230: 6520 6e75 6d62 6572 206f 6620 7361 6d70  e number of samp
-00019240: 6c65 732c 206e 5f71 7561 6e74 696c 6573  les, n_quantiles
-00019250: 2069 7320 7365 740a 2020 2020 2020 2020   is set.        
-00019260: 746f 2074 6865 206e 756d 6265 7220 6f66  to the number of
-00019270: 2073 616d 706c 6573 2061 7320 6120 6c61   samples as a la
-00019280: 7267 6572 206e 756d 6265 7220 6f66 2071  rger number of q
-00019290: 7561 6e74 696c 6573 2064 6f65 7320 6e6f  uantiles does no
-000192a0: 7420 6769 7665 0a20 2020 2020 2020 2061  t give.        a
-000192b0: 2062 6574 7465 7220 6170 7072 6f78 696d   better approxim
-000192c0: 6174 696f 6e20 6f66 2074 6865 2063 756d  ation of the cum
-000192d0: 756c 6174 6976 6520 6469 7374 7269 6275  ulative distribu
-000192e0: 7469 6f6e 2066 756e 6374 696f 6e0a 2020  tion function.  
-000192f0: 2020 2020 2020 6573 7469 6d61 746f 722e        estimator.
-00019300: 0a0a 2020 2020 6f75 7470 7574 5f64 6973  ..    output_dis
-00019310: 7472 6962 7574 696f 6e20 3a20 7b27 756e  tribution : {'un
-00019320: 6966 6f72 6d27 2c20 276e 6f72 6d61 6c27  iform', 'normal'
-00019330: 7d2c 2064 6566 6175 6c74 3d27 756e 6966  }, default='unif
-00019340: 6f72 6d27 0a20 2020 2020 2020 204d 6172  orm'.        Mar
-00019350: 6769 6e61 6c20 6469 7374 7269 6275 7469  ginal distributi
-00019360: 6f6e 2066 6f72 2074 6865 2074 7261 6e73  on for the trans
-00019370: 666f 726d 6564 2064 6174 612e 2054 6865  formed data. The
-00019380: 2063 686f 6963 6573 2061 7265 0a20 2020   choices are.   
-00019390: 2020 2020 2027 756e 6966 6f72 6d27 2028       'uniform' (
-000193a0: 6465 6661 756c 7429 206f 7220 276e 6f72  default) or 'nor
-000193b0: 6d61 6c27 2e0a 0a20 2020 2069 676e 6f72  mal'...    ignor
-000193c0: 655f 696d 706c 6963 6974 5f7a 6572 6f73  e_implicit_zeros
-000193d0: 203a 2062 6f6f 6c2c 2064 6566 6175 6c74   : bool, default
-000193e0: 3d46 616c 7365 0a20 2020 2020 2020 204f  =False.        O
-000193f0: 6e6c 7920 6170 706c 6965 7320 746f 2073  nly applies to s
-00019400: 7061 7273 6520 6d61 7472 6963 6573 2e20  parse matrices. 
-00019410: 4966 2054 7275 652c 2074 6865 2073 7061  If True, the spa
-00019420: 7273 6520 656e 7472 6965 7320 6f66 2074  rse entries of t
-00019430: 6865 0a20 2020 2020 2020 206d 6174 7269  he.        matri
-00019440: 7820 6172 6520 6469 7363 6172 6465 6420  x are discarded 
-00019450: 746f 2063 6f6d 7075 7465 2074 6865 2071  to compute the q
-00019460: 7561 6e74 696c 6520 7374 6174 6973 7469  uantile statisti
-00019470: 6373 2e20 4966 2046 616c 7365 2c0a 2020  cs. If False,.  
-00019480: 2020 2020 2020 7468 6573 6520 656e 7472        these entr
-00019490: 6965 7320 6172 6520 7472 6561 7465 6420  ies are treated 
-000194a0: 6173 207a 6572 6f73 2e0a 0a20 2020 2073  as zeros...    s
-000194b0: 7562 7361 6d70 6c65 203a 2069 6e74 2c20  ubsample : int, 
-000194c0: 6465 6661 756c 743d 3165 350a 2020 2020  default=1e5.    
-000194d0: 2020 2020 4d61 7869 6d75 6d20 6e75 6d62      Maximum numb
-000194e0: 6572 206f 6620 7361 6d70 6c65 7320 7573  er of samples us
-000194f0: 6564 2074 6f20 6573 7469 6d61 7465 2074  ed to estimate t
-00019500: 6865 2071 7561 6e74 696c 6573 2066 6f72  he quantiles for
-00019510: 0a20 2020 2020 2020 2063 6f6d 7075 7461  .        computa
-00019520: 7469 6f6e 616c 2065 6666 6963 6965 6e63  tional efficienc
-00019530: 792e 204e 6f74 6520 7468 6174 2074 6865  y. Note that the
-00019540: 2073 7562 7361 6d70 6c69 6e67 2070 726f   subsampling pro
-00019550: 6365 6475 7265 206d 6179 0a20 2020 2020  cedure may.     
-00019560: 2020 2064 6966 6665 7220 666f 7220 7661     differ for va
-00019570: 6c75 652d 6964 656e 7469 6361 6c20 7370  lue-identical sp
-00019580: 6172 7365 2061 6e64 2064 656e 7365 206d  arse and dense m
-00019590: 6174 7269 6365 732e 0a0a 2020 2020 7261  atrices...    ra
-000195a0: 6e64 6f6d 5f73 7461 7465 203a 2069 6e74  ndom_state : int
-000195b0: 2c20 5261 6e64 6f6d 5374 6174 6520 696e  , RandomState in
-000195c0: 7374 616e 6365 206f 7220 4e6f 6e65 2c20  stance or None, 
-000195d0: 6465 6661 756c 743d 4e6f 6e65 0a20 2020  default=None.   
-000195e0: 2020 2020 2044 6574 6572 6d69 6e65 7320       Determines 
-000195f0: 7261 6e64 6f6d 206e 756d 6265 7220 6765  random number ge
-00019600: 6e65 7261 7469 6f6e 2066 6f72 2073 7562  neration for sub
-00019610: 7361 6d70 6c69 6e67 2061 6e64 2073 6d6f  sampling and smo
-00019620: 6f74 6869 6e67 0a20 2020 2020 2020 206e  othing.        n
-00019630: 6f69 7365 2e0a 2020 2020 2020 2020 506c  oise..        Pl
-00019640: 6561 7365 2073 6565 2060 6073 7562 7361  ease see ``subsa
-00019650: 6d70 6c65 6060 2066 6f72 206d 6f72 6520  mple`` for more 
-00019660: 6465 7461 696c 732e 0a20 2020 2020 2020  details..       
-00019670: 2050 6173 7320 616e 2069 6e74 2066 6f72   Pass an int for
-00019680: 2072 6570 726f 6475 6369 626c 6520 7265   reproducible re
-00019690: 7375 6c74 7320 6163 726f 7373 206d 756c  sults across mul
-000196a0: 7469 706c 6520 6675 6e63 7469 6f6e 2063  tiple function c
-000196b0: 616c 6c73 2e0a 2020 2020 2020 2020 5365  alls..        Se
-000196c0: 6520 3a74 6572 6d3a 6047 6c6f 7373 6172  e :term:`Glossar
-000196d0: 7920 3c72 616e 646f 6d5f 7374 6174 653e  y <random_state>
-000196e0: 602e 0a0a 2020 2020 636f 7079 203a 2062  `...    copy : b
-000196f0: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
-00019700: 650a 2020 2020 2020 2020 4966 2046 616c  e.        If Fal
-00019710: 7365 2c20 7472 7920 746f 2061 766f 6964  se, try to avoid
-00019720: 2061 2063 6f70 7920 616e 6420 7472 616e   a copy and tran
-00019730: 7366 6f72 6d20 696e 2070 6c61 6365 2e0a  sform in place..
-00019740: 2020 2020 2020 2020 5468 6973 2069 7320          This is 
-00019750: 6e6f 7420 6775 6172 616e 7465 6564 2074  not guaranteed t
-00019760: 6f20 616c 7761 7973 2077 6f72 6b20 696e  o always work in
-00019770: 2070 6c61 6365 3b20 652e 672e 2069 6620   place; e.g. if 
-00019780: 7468 6520 6461 7461 2069 730a 2020 2020  the data is.    
-00019790: 2020 2020 6120 6e75 6d70 7920 6172 7261      a numpy arra
-000197a0: 7920 7769 7468 2061 6e20 696e 7420 6474  y with an int dt
-000197b0: 7970 652c 2061 2063 6f70 7920 7769 6c6c  ype, a copy will
-000197c0: 2062 6520 7265 7475 726e 6564 2065 7665   be returned eve
-000197d0: 6e20 7769 7468 0a20 2020 2020 2020 2063  n with.        c
-000197e0: 6f70 793d 4661 6c73 652e 0a0a 2020 2020  opy=False...    
-000197f0: 2020 2020 2e2e 2076 6572 7369 6f6e 6368      .. versionch
-00019800: 616e 6765 643a 3a20 302e 3233 0a20 2020  anged:: 0.23.   
-00019810: 2020 2020 2020 2020 2054 6865 2064 6566           The def
-00019820: 6175 6c74 2076 616c 7565 206f 6620 6063  ault value of `c
-00019830: 6f70 7960 2063 6861 6e67 6564 2066 726f  opy` changed fro
-00019840: 6d20 4661 6c73 6520 746f 2054 7275 6520  m False to True 
-00019850: 696e 2030 2e32 332e 0a0a 2020 2020 5265  in 0.23...    Re
-00019860: 7475 726e 730a 2020 2020 2d2d 2d2d 2d2d  turns.    ------
-00019870: 2d0a 2020 2020 5874 203a 207b 6e64 6172  -.    Xt : {ndar
-00019880: 7261 792c 2073 7061 7273 6520 6d61 7472  ray, sparse matr
-00019890: 6978 7d20 6f66 2073 6861 7065 2028 6e5f  ix} of shape (n_
-000198a0: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
-000198b0: 7265 7329 0a20 2020 2020 2020 2054 6865  res).        The
-000198c0: 2074 7261 6e73 666f 726d 6564 2064 6174   transformed dat
-000198d0: 612e 0a0a 2020 2020 5365 6520 416c 736f  a...    See Also
-000198e0: 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a 2020  .    --------.  
-000198f0: 2020 5175 616e 7469 6c65 5472 616e 7366    QuantileTransf
-00019900: 6f72 6d65 7220 3a20 5065 7266 6f72 6d73  ormer : Performs
-00019910: 2071 7561 6e74 696c 652d 6261 7365 6420   quantile-based 
-00019920: 7363 616c 696e 6720 7573 696e 6720 7468  scaling using th
-00019930: 650a 2020 2020 2020 2020 5472 616e 7366  e.        Transf
-00019940: 6f72 6d65 7220 4150 4920 2865 2e67 2e20  ormer API (e.g. 
-00019950: 6173 2070 6172 7420 6f66 2061 2070 7265  as part of a pre
-00019960: 7072 6f63 6573 7369 6e67 0a20 2020 2020  processing.     
-00019970: 2020 203a 636c 6173 733a 607e 736b 6c65     :class:`~skle
-00019980: 6172 6e2e 7069 7065 6c69 6e65 2e50 6970  arn.pipeline.Pip
-00019990: 656c 696e 6560 292e 0a20 2020 2070 6f77  eline`)..    pow
-000199a0: 6572 5f74 7261 6e73 666f 726d 203a 204d  er_transform : M
-000199b0: 6170 7320 6461 7461 2074 6f20 6120 6e6f  aps data to a no
-000199c0: 726d 616c 2064 6973 7472 6962 7574 696f  rmal distributio
-000199d0: 6e20 7573 696e 6720 610a 2020 2020 2020  n using a.      
-000199e0: 2020 706f 7765 7220 7472 616e 7366 6f72    power transfor
-000199f0: 6d61 7469 6f6e 2e0a 2020 2020 7363 616c  mation..    scal
-00019a00: 6520 3a20 5065 7266 6f72 6d73 2073 7461  e : Performs sta
-00019a10: 6e64 6172 6469 7a61 7469 6f6e 2074 6861  ndardization tha
-00019a20: 7420 6973 2066 6173 7465 722c 2062 7574  t is faster, but
-00019a30: 206c 6573 7320 726f 6275 7374 0a20 2020   less robust.   
-00019a40: 2020 2020 2074 6f20 6f75 746c 6965 7273       to outliers
-00019a50: 2e0a 2020 2020 726f 6275 7374 5f73 6361  ..    robust_sca
-00019a60: 6c65 203a 2050 6572 666f 726d 7320 726f  le : Performs ro
-00019a70: 6275 7374 2073 7461 6e64 6172 6469 7a61  bust standardiza
-00019a80: 7469 6f6e 2074 6861 7420 7265 6d6f 7665  tion that remove
-00019a90: 7320 7468 6520 696e 666c 7565 6e63 650a  s the influence.
-00019aa0: 2020 2020 2020 2020 6f66 206f 7574 6c69          of outli
-00019ab0: 6572 7320 6275 7420 646f 6573 206e 6f74  ers but does not
-00019ac0: 2070 7574 206f 7574 6c69 6572 7320 616e   put outliers an
-00019ad0: 6420 696e 6c69 6572 7320 6f6e 2074 6865  d inliers on the
-00019ae0: 2073 616d 6520 7363 616c 652e 0a0a 2020   same scale...  
-00019af0: 2020 4e6f 7465 730a 2020 2020 2d2d 2d2d    Notes.    ----
-00019b00: 2d0a 2020 2020 4e61 4e73 2061 7265 2074  -.    NaNs are t
-00019b10: 7265 6174 6564 2061 7320 6d69 7373 696e  reated as missin
-00019b20: 6720 7661 6c75 6573 3a20 6469 7372 6567  g values: disreg
-00019b30: 6172 6465 6420 696e 2066 6974 2c20 616e  arded in fit, an
-00019b40: 6420 6d61 696e 7461 696e 6564 2069 6e0a  d maintained in.
-00019b50: 2020 2020 7472 616e 7366 6f72 6d2e 0a0a      transform...
-00019b60: 2020 2020 2e2e 2077 6172 6e69 6e67 3a3a      .. warning::
-00019b70: 2052 6973 6b20 6f66 2064 6174 6120 6c65   Risk of data le
-00019b80: 616b 0a0a 2020 2020 2020 2020 446f 206e  ak..        Do n
-00019b90: 6f74 2075 7365 203a 6675 6e63 3a60 7e73  ot use :func:`~s
-00019ba0: 6b6c 6561 726e 2e70 7265 7072 6f63 6573  klearn.preproces
-00019bb0: 7369 6e67 2e71 7561 6e74 696c 655f 7472  sing.quantile_tr
-00019bc0: 616e 7366 6f72 6d60 2075 6e6c 6573 730a  ansform` unless.
-00019bd0: 2020 2020 2020 2020 796f 7520 6b6e 6f77          you know
-00019be0: 2077 6861 7420 796f 7520 6172 6520 646f   what you are do
-00019bf0: 696e 672e 2041 2063 6f6d 6d6f 6e20 6d69  ing. A common mi
-00019c00: 7374 616b 6520 6973 2074 6f20 6170 706c  stake is to appl
-00019c10: 7920 6974 0a20 2020 2020 2020 2074 6f20  y it.        to 
-00019c20: 7468 6520 656e 7469 7265 2064 6174 6120  the entire data 
-00019c30: 2a62 6566 6f72 652a 2073 706c 6974 7469  *before* splitti
-00019c40: 6e67 2069 6e74 6f20 7472 6169 6e69 6e67  ng into training
-00019c50: 2061 6e64 0a20 2020 2020 2020 2074 6573   and.        tes
-00019c60: 7420 7365 7473 2e20 5468 6973 2077 696c  t sets. This wil
-00019c70: 6c20 6269 6173 2074 6865 206d 6f64 656c  l bias the model
-00019c80: 2065 7661 6c75 6174 696f 6e20 6265 6361   evaluation beca
-00019c90: 7573 650a 2020 2020 2020 2020 696e 666f  use.        info
-00019ca0: 726d 6174 696f 6e20 776f 756c 6420 6861  rmation would ha
-00019cb0: 7665 206c 6561 6b65 6420 6672 6f6d 2074  ve leaked from t
-00019cc0: 6865 2074 6573 7420 7365 7420 746f 2074  he test set to t
-00019cd0: 6865 0a20 2020 2020 2020 2074 7261 696e  he.        train
-00019ce0: 696e 6720 7365 742e 0a20 2020 2020 2020  ing set..       
-00019cf0: 2049 6e20 6765 6e65 7261 6c2c 2077 6520   In general, we 
-00019d00: 7265 636f 6d6d 656e 6420 7573 696e 670a  recommend using.
-00019d10: 2020 2020 2020 2020 3a63 6c61 7373 3a60          :class:`
-00019d20: 7e73 6b6c 6561 726e 2e70 7265 7072 6f63  ~sklearn.preproc
-00019d30: 6573 7369 6e67 2e51 7561 6e74 696c 6554  essing.QuantileT
-00019d40: 7261 6e73 666f 726d 6572 6020 7769 7468  ransformer` with
-00019d50: 696e 2061 0a20 2020 2020 2020 203a 7265  in a.        :re
-00019d60: 663a 6050 6970 656c 696e 6520 3c70 6970  f:`Pipeline <pip
-00019d70: 656c 696e 653e 6020 696e 206f 7264 6572  eline>` in order
-00019d80: 2074 6f20 7072 6576 656e 7420 6d6f 7374   to prevent most
-00019d90: 2072 6973 6b73 206f 6620 6461 7461 0a20   risks of data. 
-00019da0: 2020 2020 2020 206c 6561 6b69 6e67 3a60         leaking:`
-00019db0: 7069 7065 203d 206d 616b 655f 7069 7065  pipe = make_pipe
-00019dc0: 6c69 6e65 2851 7561 6e74 696c 6554 7261  line(QuantileTra
-00019dd0: 6e73 666f 726d 6572 2829 2c0a 2020 2020  nsformer(),.    
-00019de0: 2020 2020 4c6f 6769 7374 6963 5265 6772      LogisticRegr
-00019df0: 6573 7369 6f6e 2829 2960 2e0a 0a20 2020  ession())`...   
-00019e00: 2046 6f72 2061 2063 6f6d 7061 7269 736f   For a compariso
-00019e10: 6e20 6f66 2074 6865 2064 6966 6665 7265  n of the differe
-00019e20: 6e74 2073 6361 6c65 7273 2c20 7472 616e  nt scalers, tran
-00019e30: 7366 6f72 6d65 7273 2c20 616e 6420 6e6f  sformers, and no
-00019e40: 726d 616c 697a 6572 732c 0a20 2020 2073  rmalizers,.    s
-00019e50: 6565 3a20 3a72 6566 3a60 7370 6878 5f67  ee: :ref:`sphx_g
-00019e60: 6c72 5f61 7574 6f5f 6578 616d 706c 6573  lr_auto_examples
-00019e70: 5f70 7265 7072 6f63 6573 7369 6e67 5f70  _preprocessing_p
-00019e80: 6c6f 745f 616c 6c5f 7363 616c 696e 672e  lot_all_scaling.
-00019e90: 7079 602e 0a0a 2020 2020 4578 616d 706c  py`...    Exampl
-00019ea0: 6573 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a  es.    --------.
-00019eb0: 2020 2020 3e3e 3e20 696d 706f 7274 206e      >>> import n
-00019ec0: 756d 7079 2061 7320 6e70 0a20 2020 203e  umpy as np.    >
-00019ed0: 3e3e 2066 726f 6d20 736b 6c65 6172 6e2e  >> from sklearn.
-00019ee0: 7072 6570 726f 6365 7373 696e 6720 696d  preprocessing im
-00019ef0: 706f 7274 2071 7561 6e74 696c 655f 7472  port quantile_tr
-00019f00: 616e 7366 6f72 6d0a 2020 2020 3e3e 3e20  ansform.    >>> 
-00019f10: 726e 6720 3d20 6e70 2e72 616e 646f 6d2e  rng = np.random.
-00019f20: 5261 6e64 6f6d 5374 6174 6528 3029 0a20  RandomState(0). 
-00019f30: 2020 203e 3e3e 2058 203d 206e 702e 736f     >>> X = np.so
-00019f40: 7274 2872 6e67 2e6e 6f72 6d61 6c28 6c6f  rt(rng.normal(lo
-00019f50: 633d 302e 352c 2073 6361 6c65 3d30 2e32  c=0.5, scale=0.2
-00019f60: 352c 2073 697a 653d 2832 352c 2031 2929  5, size=(25, 1))
-00019f70: 2c20 6178 6973 3d30 290a 2020 2020 3e3e  , axis=0).    >>
-00019f80: 3e20 7175 616e 7469 6c65 5f74 7261 6e73  > quantile_trans
-00019f90: 666f 726d 2858 2c20 6e5f 7175 616e 7469  form(X, n_quanti
-00019fa0: 6c65 733d 3130 2c20 7261 6e64 6f6d 5f73  les=10, random_s
-00019fb0: 7461 7465 3d30 2c20 636f 7079 3d54 7275  tate=0, copy=Tru
-00019fc0: 6529 0a20 2020 2061 7272 6179 285b 2e2e  e).    array([..
-00019fd0: 2e5d 290a 2020 2020 2222 220a 2020 2020  .]).    """.    
-00019fe0: 6e20 3d20 5175 616e 7469 6c65 5472 616e  n = QuantileTran
-00019ff0: 7366 6f72 6d65 7228 0a20 2020 2020 2020  sformer(.       
-0001a000: 206e 5f71 7561 6e74 696c 6573 3d6e 5f71   n_quantiles=n_q
-0001a010: 7561 6e74 696c 6573 2c0a 2020 2020 2020  uantiles,.      
-0001a020: 2020 6f75 7470 7574 5f64 6973 7472 6962    output_distrib
-0001a030: 7574 696f 6e3d 6f75 7470 7574 5f64 6973  ution=output_dis
-0001a040: 7472 6962 7574 696f 6e2c 0a20 2020 2020  tribution,.     
-0001a050: 2020 2073 7562 7361 6d70 6c65 3d73 7562     subsample=sub
-0001a060: 7361 6d70 6c65 2c0a 2020 2020 2020 2020  sample,.        
-0001a070: 6967 6e6f 7265 5f69 6d70 6c69 6369 745f  ignore_implicit_
-0001a080: 7a65 726f 733d 6967 6e6f 7265 5f69 6d70  zeros=ignore_imp
-0001a090: 6c69 6369 745f 7a65 726f 732c 0a20 2020  licit_zeros,.   
-0001a0a0: 2020 2020 2072 616e 646f 6d5f 7374 6174       random_stat
-0001a0b0: 653d 7261 6e64 6f6d 5f73 7461 7465 2c0a  e=random_state,.
-0001a0c0: 2020 2020 2020 2020 636f 7079 3d63 6f70          copy=cop
-0001a0d0: 792c 0a20 2020 2029 0a20 2020 2069 6620  y,.    ).    if 
-0001a0e0: 6178 6973 203d 3d20 303a 0a20 2020 2020  axis == 0:.     
-0001a0f0: 2020 2058 203d 206e 2e66 6974 5f74 7261     X = n.fit_tra
-0001a100: 6e73 666f 726d 2858 290a 2020 2020 656c  nsform(X).    el
-0001a110: 7365 3a20 2023 2061 7869 7320 3d3d 2031  se:  # axis == 1
-0001a120: 0a20 2020 2020 2020 2058 203d 206e 2e66  .        X = n.f
-0001a130: 6974 5f74 7261 6e73 666f 726d 2858 2e54  it_transform(X.T
-0001a140: 292e 540a 2020 2020 7265 7475 726e 2058  ).T.    return X
-0001a150: 0a0a 0a63 6c61 7373 2050 6f77 6572 5472  ...class PowerTr
-0001a160: 616e 7366 6f72 6d65 7228 4f6e 6554 6f4f  ansformer(OneToO
-0001a170: 6e65 4665 6174 7572 654d 6978 696e 2c20  neFeatureMixin, 
-0001a180: 5472 616e 7366 6f72 6d65 724d 6978 696e  TransformerMixin
-0001a190: 2c20 4261 7365 4573 7469 6d61 746f 7229  , BaseEstimator)
-0001a1a0: 3a0a 2020 2020 2222 2241 7070 6c79 2061  :.    """Apply a
-0001a1b0: 2070 6f77 6572 2074 7261 6e73 666f 726d   power transform
-0001a1c0: 2066 6561 7475 7265 7769 7365 2074 6f20   featurewise to 
-0001a1d0: 6d61 6b65 2064 6174 6120 6d6f 7265 2047  make data more G
-0001a1e0: 6175 7373 6961 6e2d 6c69 6b65 2e0a 0a20  aussian-like... 
-0001a1f0: 2020 2050 6f77 6572 2074 7261 6e73 666f     Power transfo
-0001a200: 726d 7320 6172 6520 6120 6661 6d69 6c79  rms are a family
-0001a210: 206f 6620 7061 7261 6d65 7472 6963 2c20   of parametric, 
-0001a220: 6d6f 6e6f 746f 6e69 6320 7472 616e 7366  monotonic transf
-0001a230: 6f72 6d61 7469 6f6e 730a 2020 2020 7468  ormations.    th
-0001a240: 6174 2061 7265 2061 7070 6c69 6564 2074  at are applied t
-0001a250: 6f20 6d61 6b65 2064 6174 6120 6d6f 7265  o make data more
-0001a260: 2047 6175 7373 6961 6e2d 6c69 6b65 2e20   Gaussian-like. 
-0001a270: 5468 6973 2069 7320 7573 6566 756c 2066  This is useful f
-0001a280: 6f72 0a20 2020 206d 6f64 656c 696e 6720  or.    modeling 
-0001a290: 6973 7375 6573 2072 656c 6174 6564 2074  issues related t
-0001a2a0: 6f20 6865 7465 726f 7363 6564 6173 7469  o heteroscedasti
-0001a2b0: 6369 7479 2028 6e6f 6e2d 636f 6e73 7461  city (non-consta
-0001a2c0: 6e74 2076 6172 6961 6e63 6529 2c0a 2020  nt variance),.  
-0001a2d0: 2020 6f72 206f 7468 6572 2073 6974 7561    or other situa
-0001a2e0: 7469 6f6e 7320 7768 6572 6520 6e6f 726d  tions where norm
-0001a2f0: 616c 6974 7920 6973 2064 6573 6972 6564  ality is desired
-0001a300: 2e0a 0a20 2020 2043 7572 7265 6e74 6c79  ...    Currently
-0001a310: 2c20 506f 7765 7254 7261 6e73 666f 726d  , PowerTransform
-0001a320: 6572 2073 7570 706f 7274 7320 7468 6520  er supports the 
-0001a330: 426f 782d 436f 7820 7472 616e 7366 6f72  Box-Cox transfor
-0001a340: 6d20 616e 6420 7468 650a 2020 2020 5965  m and the.    Ye
-0001a350: 6f2d 4a6f 686e 736f 6e20 7472 616e 7366  o-Johnson transf
-0001a360: 6f72 6d2e 2054 6865 206f 7074 696d 616c  orm. The optimal
-0001a370: 2070 6172 616d 6574 6572 2066 6f72 2073   parameter for s
-0001a380: 7461 6269 6c69 7a69 6e67 2076 6172 6961  tabilizing varia
-0001a390: 6e63 6520 616e 640a 2020 2020 6d69 6e69  nce and.    mini
-0001a3a0: 6d69 7a69 6e67 2073 6b65 776e 6573 7320  mizing skewness 
-0001a3b0: 6973 2065 7374 696d 6174 6564 2074 6872  is estimated thr
-0001a3c0: 6f75 6768 206d 6178 696d 756d 206c 696b  ough maximum lik
-0001a3d0: 656c 6968 6f6f 642e 0a0a 2020 2020 426f  elihood...    Bo
-0001a3e0: 782d 436f 7820 7265 7175 6972 6573 2069  x-Cox requires i
-0001a3f0: 6e70 7574 2064 6174 6120 746f 2062 6520  nput data to be 
-0001a400: 7374 7269 6374 6c79 2070 6f73 6974 6976  strictly positiv
-0001a410: 652c 2077 6869 6c65 2059 656f 2d4a 6f68  e, while Yeo-Joh
-0001a420: 6e73 6f6e 0a20 2020 2073 7570 706f 7274  nson.    support
-0001a430: 7320 626f 7468 2070 6f73 6974 6976 6520  s both positive 
-0001a440: 6f72 206e 6567 6174 6976 6520 6461 7461  or negative data
-0001a450: 2e0a 0a20 2020 2042 7920 6465 6661 756c  ...    By defaul
-0001a460: 742c 207a 6572 6f2d 6d65 616e 2c20 756e  t, zero-mean, un
-0001a470: 6974 2d76 6172 6961 6e63 6520 6e6f 726d  it-variance norm
-0001a480: 616c 697a 6174 696f 6e20 6973 2061 7070  alization is app
-0001a490: 6c69 6564 2074 6f20 7468 650a 2020 2020  lied to the.    
-0001a4a0: 7472 616e 7366 6f72 6d65 6420 6461 7461  transformed data
-0001a4b0: 2e0a 0a20 2020 2046 6f72 2061 6e20 6578  ...    For an ex
-0001a4c0: 616d 706c 6520 7669 7375 616c 697a 6174  ample visualizat
-0001a4d0: 696f 6e2c 2072 6566 6572 2074 6f20 3a72  ion, refer to :r
-0001a4e0: 6566 3a60 436f 6d70 6172 6520 506f 7765  ef:`Compare Powe
-0001a4f0: 7254 7261 6e73 666f 726d 6572 2077 6974  rTransformer wit
-0001a500: 680a 2020 2020 6f74 6865 7220 7363 616c  h.    other scal
-0001a510: 6572 7320 3c70 6c6f 745f 616c 6c5f 7363  ers <plot_all_sc
-0001a520: 616c 696e 675f 706f 7765 725f 7472 616e  aling_power_tran
-0001a530: 7366 6f72 6d65 725f 7365 6374 696f 6e3e  sformer_section>
-0001a540: 602e 2054 6f20 7365 6520 7468 650a 2020  `. To see the.  
-0001a550: 2020 6566 6665 6374 206f 6620 426f 782d    effect of Box-
-0001a560: 436f 7820 616e 6420 5965 6f2d 4a6f 686e  Cox and Yeo-John
-0001a570: 736f 6e20 7472 616e 7366 6f72 6d61 7469  son transformati
-0001a580: 6f6e 7320 6f6e 2064 6966 6665 7265 6e74  ons on different
-0001a590: 0a20 2020 2064 6973 7472 6962 7574 696f  .    distributio
-0001a5a0: 6e73 2c20 7365 653a 0a20 2020 203a 7265  ns, see:.    :re
-0001a5b0: 663a 6073 7068 785f 676c 725f 6175 746f  f:`sphx_glr_auto
-0001a5c0: 5f65 7861 6d70 6c65 735f 7072 6570 726f  _examples_prepro
-0001a5d0: 6365 7373 696e 675f 706c 6f74 5f6d 6170  cessing_plot_map
-0001a5e0: 5f64 6174 615f 746f 5f6e 6f72 6d61 6c2e  _data_to_normal.
-0001a5f0: 7079 602e 0a0a 2020 2020 5265 6164 206d  py`...    Read m
-0001a600: 6f72 6520 696e 2074 6865 203a 7265 663a  ore in the :ref:
-0001a610: 6055 7365 7220 4775 6964 6520 3c70 7265  `User Guide <pre
-0001a620: 7072 6f63 6573 7369 6e67 5f74 7261 6e73  processing_trans
-0001a630: 666f 726d 6572 3e60 2e0a 0a20 2020 202e  former>`...    .
-0001a640: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
-0001a650: 2030 2e32 300a 0a20 2020 2050 6172 616d   0.20..    Param
-0001a660: 6574 6572 730a 2020 2020 2d2d 2d2d 2d2d  eters.    ------
-0001a670: 2d2d 2d2d 0a20 2020 206d 6574 686f 6420  ----.    method 
-0001a680: 3a20 7b27 7965 6f2d 6a6f 686e 736f 6e27  : {'yeo-johnson'
-0001a690: 2c20 2762 6f78 2d63 6f78 277d 2c20 6465  , 'box-cox'}, de
-0001a6a0: 6661 756c 743d 2779 656f 2d6a 6f68 6e73  fault='yeo-johns
-0001a6b0: 6f6e 270a 2020 2020 2020 2020 5468 6520  on'.        The 
-0001a6c0: 706f 7765 7220 7472 616e 7366 6f72 6d20  power transform 
-0001a6d0: 6d65 7468 6f64 2e20 4176 6169 6c61 626c  method. Availabl
-0001a6e0: 6520 6d65 7468 6f64 7320 6172 653a 0a0a  e methods are:..
-0001a6f0: 2020 2020 2020 2020 2d20 2779 656f 2d6a          - 'yeo-j
-0001a700: 6f68 6e73 6f6e 2720 5b31 5d5f 2c20 776f  ohnson' [1]_, wo
-0001a710: 726b 7320 7769 7468 2070 6f73 6974 6976  rks with positiv
-0001a720: 6520 616e 6420 6e65 6761 7469 7665 2076  e and negative v
-0001a730: 616c 7565 730a 2020 2020 2020 2020 2d20  alues.        - 
-0001a740: 2762 6f78 2d63 6f78 2720 5b32 5d5f 2c20  'box-cox' [2]_, 
-0001a750: 6f6e 6c79 2077 6f72 6b73 2077 6974 6820  only works with 
-0001a760: 7374 7269 6374 6c79 2070 6f73 6974 6976  strictly positiv
-0001a770: 6520 7661 6c75 6573 0a0a 2020 2020 7374  e values..    st
-0001a780: 616e 6461 7264 697a 6520 3a20 626f 6f6c  andardize : bool
-0001a790: 2c20 6465 6661 756c 743d 5472 7565 0a20  , default=True. 
-0001a7a0: 2020 2020 2020 2053 6574 2074 6f20 5472         Set to Tr
-0001a7b0: 7565 2074 6f20 6170 706c 7920 7a65 726f  ue to apply zero
-0001a7c0: 2d6d 6561 6e2c 2075 6e69 742d 7661 7269  -mean, unit-vari
-0001a7d0: 616e 6365 206e 6f72 6d61 6c69 7a61 7469  ance normalizati
-0001a7e0: 6f6e 2074 6f20 7468 650a 2020 2020 2020  on to the.      
-0001a7f0: 2020 7472 616e 7366 6f72 6d65 6420 6f75    transformed ou
-0001a800: 7470 7574 2e0a 0a20 2020 2063 6f70 7920  tput...    copy 
-0001a810: 3a20 626f 6f6c 2c20 6465 6661 756c 743d  : bool, default=
-0001a820: 5472 7565 0a20 2020 2020 2020 2053 6574  True.        Set
-0001a830: 2074 6f20 4661 6c73 6520 746f 2070 6572   to False to per
-0001a840: 666f 726d 2069 6e70 6c61 6365 2063 6f6d  form inplace com
-0001a850: 7075 7461 7469 6f6e 2064 7572 696e 6720  putation during 
-0001a860: 7472 616e 7366 6f72 6d61 7469 6f6e 2e0a  transformation..
-0001a870: 0a20 2020 2041 7474 7269 6275 7465 730a  .    Attributes.
-0001a880: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
-0001a890: 2020 206c 616d 6264 6173 5f20 3a20 6e64     lambdas_ : nd
-0001a8a0: 6172 7261 7920 6f66 2066 6c6f 6174 206f  array of float o
-0001a8b0: 6620 7368 6170 6520 286e 5f66 6561 7475  f shape (n_featu
-0001a8c0: 7265 732c 290a 2020 2020 2020 2020 5468  res,).        Th
-0001a8d0: 6520 7061 7261 6d65 7465 7273 206f 6620  e parameters of 
-0001a8e0: 7468 6520 706f 7765 7220 7472 616e 7366  the power transf
-0001a8f0: 6f72 6d61 7469 6f6e 2066 6f72 2074 6865  ormation for the
-0001a900: 2073 656c 6563 7465 6420 6665 6174 7572   selected featur
-0001a910: 6573 2e0a 0a20 2020 206e 5f66 6561 7475  es...    n_featu
-0001a920: 7265 735f 696e 5f20 3a20 696e 740a 2020  res_in_ : int.  
-0001a930: 2020 2020 2020 4e75 6d62 6572 206f 6620        Number of 
-0001a940: 6665 6174 7572 6573 2073 6565 6e20 6475  features seen du
-0001a950: 7269 6e67 203a 7465 726d 3a60 6669 7460  ring :term:`fit`
-0001a960: 2e0a 0a20 2020 2020 2020 202e 2e20 7665  ...        .. ve
-0001a970: 7273 696f 6e61 6464 6564 3a3a 2030 2e32  rsionadded:: 0.2
-0001a980: 340a 0a20 2020 2066 6561 7475 7265 5f6e  4..    feature_n
-0001a990: 616d 6573 5f69 6e5f 203a 206e 6461 7272  ames_in_ : ndarr
-0001a9a0: 6179 206f 6620 7368 6170 6520 2860 6e5f  ay of shape (`n_
-0001a9b0: 6665 6174 7572 6573 5f69 6e5f 602c 290a  features_in_`,).
-0001a9c0: 2020 2020 2020 2020 4e61 6d65 7320 6f66          Names of
-0001a9d0: 2066 6561 7475 7265 7320 7365 656e 2064   features seen d
-0001a9e0: 7572 696e 6720 3a74 6572 6d3a 6066 6974  uring :term:`fit
-0001a9f0: 602e 2044 6566 696e 6564 206f 6e6c 7920  `. Defined only 
-0001aa00: 7768 656e 2060 5860 0a20 2020 2020 2020  when `X`.       
-0001aa10: 2068 6173 2066 6561 7475 7265 206e 616d   has feature nam
-0001aa20: 6573 2074 6861 7420 6172 6520 616c 6c20  es that are all 
-0001aa30: 7374 7269 6e67 732e 0a0a 2020 2020 2020  strings...      
-0001aa40: 2020 2e2e 2076 6572 7369 6f6e 6164 6465    .. versionadde
-0001aa50: 643a 3a20 312e 300a 0a20 2020 2053 6565  d:: 1.0..    See
-0001aa60: 2041 6c73 6f0a 2020 2020 2d2d 2d2d 2d2d   Also.    ------
-0001aa70: 2d2d 0a20 2020 2070 6f77 6572 5f74 7261  --.    power_tra
-0001aa80: 6e73 666f 726d 203a 2045 7175 6976 616c  nsform : Equival
-0001aa90: 656e 7420 6675 6e63 7469 6f6e 2077 6974  ent function wit
-0001aaa0: 686f 7574 2074 6865 2065 7374 696d 6174  hout the estimat
-0001aab0: 6f72 2041 5049 2e0a 0a20 2020 2051 7561  or API...    Qua
-0001aac0: 6e74 696c 6554 7261 6e73 666f 726d 6572  ntileTransformer
-0001aad0: 203a 204d 6170 7320 6461 7461 2074 6f20   : Maps data to 
-0001aae0: 6120 7374 616e 6461 7264 206e 6f72 6d61  a standard norma
-0001aaf0: 6c20 6469 7374 7269 6275 7469 6f6e 2077  l distribution w
-0001ab00: 6974 680a 2020 2020 2020 2020 7468 6520  ith.        the 
-0001ab10: 7061 7261 6d65 7465 7220 606f 7574 7075  parameter `outpu
-0001ab20: 745f 6469 7374 7269 6275 7469 6f6e 3d27  t_distribution='
-0001ab30: 6e6f 726d 616c 2760 2e0a 0a20 2020 204e  normal'`...    N
-0001ab40: 6f74 6573 0a20 2020 202d 2d2d 2d2d 0a20  otes.    -----. 
-0001ab50: 2020 204e 614e 7320 6172 6520 7472 6561     NaNs are trea
-0001ab60: 7465 6420 6173 206d 6973 7369 6e67 2076  ted as missing v
-0001ab70: 616c 7565 733a 2064 6973 7265 6761 7264  alues: disregard
-0001ab80: 6564 2069 6e20 6060 6669 7460 602c 2061  ed in ``fit``, a
-0001ab90: 6e64 206d 6169 6e74 6169 6e65 640a 2020  nd maintained.  
-0001aba0: 2020 696e 2060 6074 7261 6e73 666f 726d    in ``transform
-0001abb0: 6060 2e0a 0a20 2020 2052 6566 6572 656e  ``...    Referen
-0001abc0: 6365 730a 2020 2020 2d2d 2d2d 2d2d 2d2d  ces.    --------
-0001abd0: 2d2d 0a0a 2020 2020 2e2e 205b 315d 203a  --..    .. [1] :
-0001abe0: 646f 693a 6049 2e4b 2e20 5965 6f20 616e  doi:`I.K. Yeo an
-0001abf0: 6420 522e 412e 204a 6f68 6e73 6f6e 2c20  d R.A. Johnson, 
-0001ac00: 2241 206e 6577 2066 616d 696c 7920 6f66  "A new family of
-0001ac10: 2070 6f77 6572 0a20 2020 2020 2020 2020   power.         
-0001ac20: 2020 7472 616e 7366 6f72 6d61 7469 6f6e    transformation
-0001ac30: 7320 746f 2069 6d70 726f 7665 206e 6f72  s to improve nor
-0001ac40: 6d61 6c69 7479 206f 7220 7379 6d6d 6574  mality or symmet
-0001ac50: 7279 2e22 2042 696f 6d65 7472 696b 612c  ry." Biometrika,
-0001ac60: 0a20 2020 2020 2020 2020 2020 3837 2834  .           87(4
-0001ac70: 292c 2070 702e 3935 342d 3935 392c 2028  ), pp.954-959, (
-0001ac80: 3230 3030 292e 203c 3130 2e31 3039 332f  2000). <10.1093/
-0001ac90: 6269 6f6d 6574 2f38 372e 342e 3935 343e  biomet/87.4.954>
-0001aca0: 600a 0a20 2020 202e 2e20 5b32 5d20 3a64  `..    .. [2] :d
-0001acb0: 6f69 3a60 472e 452e 502e 2042 6f78 2061  oi:`G.E.P. Box a
-0001acc0: 6e64 2044 2e52 2e20 436f 782c 2022 416e  nd D.R. Cox, "An
-0001acd0: 2041 6e61 6c79 7369 7320 6f66 2054 7261   Analysis of Tra
-0001ace0: 6e73 666f 726d 6174 696f 6e73 222c 0a20  nsformations",. 
-0001acf0: 2020 2020 2020 2020 2020 4a6f 7572 6e61            Journa
-0001ad00: 6c20 6f66 2074 6865 2052 6f79 616c 2053  l of the Royal S
-0001ad10: 7461 7469 7374 6963 616c 2053 6f63 6965  tatistical Socie
-0001ad20: 7479 2042 2c20 3236 2c20 3231 312d 3235  ty B, 26, 211-25
-0001ad30: 3220 2831 3936 3429 2e0a 2020 2020 2020  2 (1964)..      
-0001ad40: 2020 2020 203c 3130 2e31 3131 312f 6a2e       <10.1111/j.
-0001ad50: 3235 3137 2d36 3136 312e 3139 3634 2e74  2517-6161.1964.t
-0001ad60: 6230 3035 3533 2e78 3e60 0a0a 2020 2020  b00553.x>`..    
-0001ad70: 4578 616d 706c 6573 0a20 2020 202d 2d2d  Examples.    ---
-0001ad80: 2d2d 2d2d 2d0a 2020 2020 3e3e 3e20 696d  -----.    >>> im
-0001ad90: 706f 7274 206e 756d 7079 2061 7320 6e70  port numpy as np
-0001ada0: 0a20 2020 203e 3e3e 2066 726f 6d20 736b  .    >>> from sk
-0001adb0: 6c65 6172 6e2e 7072 6570 726f 6365 7373  learn.preprocess
-0001adc0: 696e 6720 696d 706f 7274 2050 6f77 6572  ing import Power
-0001add0: 5472 616e 7366 6f72 6d65 720a 2020 2020  Transformer.    
-0001ade0: 3e3e 3e20 7074 203d 2050 6f77 6572 5472  >>> pt = PowerTr
-0001adf0: 616e 7366 6f72 6d65 7228 290a 2020 2020  ansformer().    
-0001ae00: 3e3e 3e20 6461 7461 203d 205b 5b31 2c20  >>> data = [[1, 
-0001ae10: 325d 2c20 5b33 2c20 325d 2c20 5b34 2c20  2], [3, 2], [4, 
-0001ae20: 355d 5d0a 2020 2020 3e3e 3e20 7072 696e  5]].    >>> prin
-0001ae30: 7428 7074 2e66 6974 2864 6174 6129 290a  t(pt.fit(data)).
-0001ae40: 2020 2020 506f 7765 7254 7261 6e73 666f      PowerTransfo
-0001ae50: 726d 6572 2829 0a20 2020 203e 3e3e 2070  rmer().    >>> p
-0001ae60: 7269 6e74 2870 742e 6c61 6d62 6461 735f  rint(pt.lambdas_
-0001ae70: 290a 2020 2020 5b20 312e 3338 362e 2e2e  ).    [ 1.386...
-0001ae80: 202d 332e 3130 302e 2e2e 5d0a 2020 2020   -3.100...].    
-0001ae90: 3e3e 3e20 7072 696e 7428 7074 2e74 7261  >>> print(pt.tra
-0001aea0: 6e73 666f 726d 2864 6174 6129 290a 2020  nsform(data)).  
-0001aeb0: 2020 5b5b 2d31 2e33 3136 2e2e 2e20 2d30    [[-1.316... -0
-0001aec0: 2e37 3037 2e2e 2e5d 0a20 2020 2020 5b20  .707...].     [ 
-0001aed0: 302e 3230 392e 2e2e 202d 302e 3730 372e  0.209... -0.707.
-0001aee0: 2e2e 5d0a 2020 2020 205b 2031 2e31 3036  ..].     [ 1.106
-0001aef0: 2e2e 2e20 2031 2e34 3134 2e2e 2e5d 5d0a  ...  1.414...]].
-0001af00: 2020 2020 2222 220a 0a20 2020 205f 7061      """..    _pa
-0001af10: 7261 6d65 7465 725f 636f 6e73 7472 6169  rameter_constrai
-0001af20: 6e74 733a 2064 6963 7420 3d20 7b0a 2020  nts: dict = {.  
-0001af30: 2020 2020 2020 226d 6574 686f 6422 3a20        "method": 
-0001af40: 5b53 7472 4f70 7469 6f6e 7328 7b22 7965  [StrOptions({"ye
-0001af50: 6f2d 6a6f 686e 736f 6e22 2c20 2262 6f78  o-johnson", "box
-0001af60: 2d63 6f78 227d 295d 2c0a 2020 2020 2020  -cox"})],.      
-0001af70: 2020 2273 7461 6e64 6172 6469 7a65 223a    "standardize":
-0001af80: 205b 2262 6f6f 6c65 616e 225d 2c0a 2020   ["boolean"],.  
-0001af90: 2020 2020 2020 2263 6f70 7922 3a20 5b22        "copy": ["
-0001afa0: 626f 6f6c 6561 6e22 5d2c 0a20 2020 207d  boolean"],.    }
-0001afb0: 0a0a 2020 2020 6465 6620 5f5f 696e 6974  ..    def __init
-0001afc0: 5f5f 2873 656c 662c 206d 6574 686f 643d  __(self, method=
-0001afd0: 2279 656f 2d6a 6f68 6e73 6f6e 222c 202a  "yeo-johnson", *
-0001afe0: 2c20 7374 616e 6461 7264 697a 653d 5472  , standardize=Tr
-0001aff0: 7565 2c20 636f 7079 3d54 7275 6529 3a0a  ue, copy=True):.
-0001b000: 2020 2020 2020 2020 7365 6c66 2e6d 6574          self.met
-0001b010: 686f 6420 3d20 6d65 7468 6f64 0a20 2020  hod = method.   
-0001b020: 2020 2020 2073 656c 662e 7374 616e 6461       self.standa
-0001b030: 7264 697a 6520 3d20 7374 616e 6461 7264  rdize = standard
-0001b040: 697a 650a 2020 2020 2020 2020 7365 6c66  ize.        self
-0001b050: 2e63 6f70 7920 3d20 636f 7079 0a0a 2020  .copy = copy..  
-0001b060: 2020 405f 6669 745f 636f 6e74 6578 7428    @_fit_context(
-0001b070: 7072 6566 6572 5f73 6b69 705f 6e65 7374  prefer_skip_nest
-0001b080: 6564 5f76 616c 6964 6174 696f 6e3d 5472  ed_validation=Tr
-0001b090: 7565 290a 2020 2020 6465 6620 6669 7428  ue).    def fit(
-0001b0a0: 7365 6c66 2c20 582c 2079 3d4e 6f6e 6529  self, X, y=None)
-0001b0b0: 3a0a 2020 2020 2020 2020 2222 2245 7374  :.        """Est
-0001b0c0: 696d 6174 6520 7468 6520 6f70 7469 6d61  imate the optima
-0001b0d0: 6c20 7061 7261 6d65 7465 7220 6c61 6d62  l parameter lamb
-0001b0e0: 6461 2066 6f72 2065 6163 6820 6665 6174  da for each feat
-0001b0f0: 7572 652e 0a0a 2020 2020 2020 2020 5468  ure...        Th
-0001b100: 6520 6f70 7469 6d61 6c20 6c61 6d62 6461  e optimal lambda
-0001b110: 2070 6172 616d 6574 6572 2066 6f72 206d   parameter for m
-0001b120: 696e 696d 697a 696e 6720 736b 6577 6e65  inimizing skewne
-0001b130: 7373 2069 7320 6573 7469 6d61 7465 6420  ss is estimated 
-0001b140: 6f6e 0a20 2020 2020 2020 2065 6163 6820  on.        each 
-0001b150: 6665 6174 7572 6520 696e 6465 7065 6e64  feature independ
-0001b160: 656e 746c 7920 7573 696e 6720 6d61 7869  ently using maxi
-0001b170: 6d75 6d20 6c69 6b65 6c69 686f 6f64 2e0a  mum likelihood..
-0001b180: 0a20 2020 2020 2020 2050 6172 616d 6574  .        Paramet
-0001b190: 6572 730a 2020 2020 2020 2020 2d2d 2d2d  ers.        ----
-0001b1a0: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2058  ------.        X
-0001b1b0: 203a 2061 7272 6179 2d6c 696b 6520 6f66   : array-like of
-0001b1c0: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
-0001b1d0: 732c 206e 5f66 6561 7475 7265 7329 0a20  s, n_features). 
-0001b1e0: 2020 2020 2020 2020 2020 2054 6865 2064             The d
-0001b1f0: 6174 6120 7573 6564 2074 6f20 6573 7469  ata used to esti
-0001b200: 6d61 7465 2074 6865 206f 7074 696d 616c  mate the optimal
-0001b210: 2074 7261 6e73 666f 726d 6174 696f 6e20   transformation 
-0001b220: 7061 7261 6d65 7465 7273 2e0a 0a20 2020  parameters...   
-0001b230: 2020 2020 2079 203a 204e 6f6e 650a 2020       y : None.  
-0001b240: 2020 2020 2020 2020 2020 4967 6e6f 7265            Ignore
-0001b250: 642e 0a0a 2020 2020 2020 2020 5265 7475  d...        Retu
-0001b260: 726e 730a 2020 2020 2020 2020 2d2d 2d2d  rns.        ----
-0001b270: 2d2d 2d0a 2020 2020 2020 2020 7365 6c66  ---.        self
-0001b280: 203a 206f 626a 6563 740a 2020 2020 2020   : object.      
-0001b290: 2020 2020 2020 4669 7474 6564 2074 7261        Fitted tra
-0001b2a0: 6e73 666f 726d 6572 2e0a 2020 2020 2020  nsformer..      
-0001b2b0: 2020 2222 220a 2020 2020 2020 2020 7365    """.        se
-0001b2c0: 6c66 2e5f 6669 7428 582c 2079 3d79 2c20  lf._fit(X, y=y, 
-0001b2d0: 666f 7263 655f 7472 616e 7366 6f72 6d3d  force_transform=
-0001b2e0: 4661 6c73 6529 0a20 2020 2020 2020 2072  False).        r
-0001b2f0: 6574 7572 6e20 7365 6c66 0a0a 2020 2020  eturn self..    
-0001b300: 405f 6669 745f 636f 6e74 6578 7428 7072  @_fit_context(pr
-0001b310: 6566 6572 5f73 6b69 705f 6e65 7374 6564  efer_skip_nested
-0001b320: 5f76 616c 6964 6174 696f 6e3d 5472 7565  _validation=True
-0001b330: 290a 2020 2020 6465 6620 6669 745f 7472  ).    def fit_tr
-0001b340: 616e 7366 6f72 6d28 7365 6c66 2c20 582c  ansform(self, X,
-0001b350: 2079 3d4e 6f6e 6529 3a0a 2020 2020 2020   y=None):.      
-0001b360: 2020 2222 2246 6974 2060 506f 7765 7254    """Fit `PowerT
-0001b370: 7261 6e73 666f 726d 6572 6020 746f 2060  ransformer` to `
-0001b380: 5860 2c20 7468 656e 2074 7261 6e73 666f  X`, then transfo
-0001b390: 726d 2060 5860 2e0a 0a20 2020 2020 2020  rm `X`...       
-0001b3a0: 2050 6172 616d 6574 6572 730a 2020 2020   Parameters.    
-0001b3b0: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20      ----------. 
-0001b3c0: 2020 2020 2020 2058 203a 2061 7272 6179         X : array
-0001b3d0: 2d6c 696b 6520 6f66 2073 6861 7065 2028  -like of shape (
-0001b3e0: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
-0001b3f0: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
-0001b400: 2020 2054 6865 2064 6174 6120 7573 6564     The data used
-0001b410: 2074 6f20 6573 7469 6d61 7465 2074 6865   to estimate the
-0001b420: 206f 7074 696d 616c 2074 7261 6e73 666f   optimal transfo
-0001b430: 726d 6174 696f 6e20 7061 7261 6d65 7465  rmation paramete
-0001b440: 7273 0a20 2020 2020 2020 2020 2020 2061  rs.            a
-0001b450: 6e64 2074 6f20 6265 2074 7261 6e73 666f  nd to be transfo
-0001b460: 726d 6564 2075 7369 6e67 2061 2070 6f77  rmed using a pow
-0001b470: 6572 2074 7261 6e73 666f 726d 6174 696f  er transformatio
-0001b480: 6e2e 0a0a 2020 2020 2020 2020 7920 3a20  n...        y : 
-0001b490: 4967 6e6f 7265 640a 2020 2020 2020 2020  Ignored.        
-0001b4a0: 2020 2020 4e6f 7420 7573 6564 2c20 7072      Not used, pr
-0001b4b0: 6573 656e 7420 666f 7220 4150 4920 636f  esent for API co
-0001b4c0: 6e73 6973 7465 6e63 7920 6279 2063 6f6e  nsistency by con
-0001b4d0: 7665 6e74 696f 6e2e 0a0a 2020 2020 2020  vention...      
-0001b4e0: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
-0001b4f0: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
-0001b500: 2020 585f 6e65 7720 3a20 6e64 6172 7261    X_new : ndarra
-0001b510: 7920 6f66 2073 6861 7065 2028 6e5f 7361  y of shape (n_sa
-0001b520: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
-0001b530: 7329 0a20 2020 2020 2020 2020 2020 2054  s).            T
-0001b540: 7261 6e73 666f 726d 6564 2064 6174 612e  ransformed data.
-0001b550: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-0001b560: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-0001b570: 2e5f 6669 7428 582c 2079 2c20 666f 7263  ._fit(X, y, forc
-0001b580: 655f 7472 616e 7366 6f72 6d3d 5472 7565  e_transform=True
-0001b590: 290a 0a20 2020 2064 6566 205f 6669 7428  )..    def _fit(
-0001b5a0: 7365 6c66 2c20 582c 2079 3d4e 6f6e 652c  self, X, y=None,
-0001b5b0: 2066 6f72 6365 5f74 7261 6e73 666f 726d   force_transform
-0001b5c0: 3d46 616c 7365 293a 0a20 2020 2020 2020  =False):.       
-0001b5d0: 2058 203d 2073 656c 662e 5f63 6865 636b   X = self._check
-0001b5e0: 5f69 6e70 7574 2858 2c20 696e 5f66 6974  _input(X, in_fit
-0001b5f0: 3d54 7275 652c 2063 6865 636b 5f70 6f73  =True, check_pos
-0001b600: 6974 6976 653d 5472 7565 290a 0a20 2020  itive=True)..   
-0001b610: 2020 2020 2069 6620 6e6f 7420 7365 6c66       if not self
-0001b620: 2e63 6f70 7920 616e 6420 6e6f 7420 666f  .copy and not fo
-0001b630: 7263 655f 7472 616e 7366 6f72 6d3a 2020  rce_transform:  
-0001b640: 2320 6966 2063 616c 6c20 6672 6f6d 2066  # if call from f
-0001b650: 6974 2829 0a20 2020 2020 2020 2020 2020  it().           
-0001b660: 2058 203d 2058 2e63 6f70 7928 2920 2023   X = X.copy()  #
-0001b670: 2066 6f72 6365 2063 6f70 7920 736f 2074   force copy so t
-0001b680: 6861 7420 6669 7420 646f 6573 206e 6f74  hat fit does not
-0001b690: 2063 6861 6e67 6520 5820 696e 706c 6163   change X inplac
-0001b6a0: 650a 0a20 2020 2020 2020 206e 5f73 616d  e..        n_sam
-0001b6b0: 706c 6573 203d 2058 2e73 6861 7065 5b30  ples = X.shape[0
-0001b6c0: 5d0a 2020 2020 2020 2020 6d65 616e 203d  ].        mean =
-0001b6d0: 206e 702e 6d65 616e 2858 2c20 6178 6973   np.mean(X, axis
-0001b6e0: 3d30 2c20 6474 7970 653d 6e70 2e66 6c6f  =0, dtype=np.flo
-0001b6f0: 6174 3634 290a 2020 2020 2020 2020 7661  at64).        va
-0001b700: 7220 3d20 6e70 2e76 6172 2858 2c20 6178  r = np.var(X, ax
-0001b710: 6973 3d30 2c20 6474 7970 653d 6e70 2e66  is=0, dtype=np.f
-0001b720: 6c6f 6174 3634 290a 0a20 2020 2020 2020  loat64)..       
-0001b730: 206f 7074 696d 5f66 756e 6374 696f 6e20   optim_function 
-0001b740: 3d20 7b0a 2020 2020 2020 2020 2020 2020  = {.            
-0001b750: 2262 6f78 2d63 6f78 223a 2073 656c 662e  "box-cox": self.
-0001b760: 5f62 6f78 5f63 6f78 5f6f 7074 696d 697a  _box_cox_optimiz
-0001b770: 652c 0a20 2020 2020 2020 2020 2020 2022  e,.            "
-0001b780: 7965 6f2d 6a6f 686e 736f 6e22 3a20 7365  yeo-johnson": se
-0001b790: 6c66 2e5f 7965 6f5f 6a6f 686e 736f 6e5f  lf._yeo_johnson_
-0001b7a0: 6f70 7469 6d69 7a65 2c0a 2020 2020 2020  optimize,.      
-0001b7b0: 2020 7d5b 7365 6c66 2e6d 6574 686f 645d    }[self.method]
-0001b7c0: 0a0a 2020 2020 2020 2020 7472 616e 7366  ..        transf
-0001b7d0: 6f72 6d5f 6675 6e63 7469 6f6e 203d 207b  orm_function = {
-0001b7e0: 0a20 2020 2020 2020 2020 2020 2022 626f  .            "bo
-0001b7f0: 782d 636f 7822 3a20 626f 7863 6f78 2c0a  x-cox": boxcox,.
-0001b800: 2020 2020 2020 2020 2020 2020 2279 656f              "yeo
-0001b810: 2d6a 6f68 6e73 6f6e 223a 2073 656c 662e  -johnson": self.
-0001b820: 5f79 656f 5f6a 6f68 6e73 6f6e 5f74 7261  _yeo_johnson_tra
-0001b830: 6e73 666f 726d 2c0a 2020 2020 2020 2020  nsform,.        
-0001b840: 7d5b 7365 6c66 2e6d 6574 686f 645d 0a0a  }[self.method]..
-0001b850: 2020 2020 2020 2020 7769 7468 206e 702e          with np.
-0001b860: 6572 7273 7461 7465 2869 6e76 616c 6964  errstate(invalid
-0001b870: 3d22 6967 6e6f 7265 2229 3a20 2023 2068  ="ignore"):  # h
-0001b880: 6964 6520 4e61 4e20 7761 726e 696e 6773  ide NaN warnings
-0001b890: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0001b8a0: 662e 6c61 6d62 6461 735f 203d 206e 702e  f.lambdas_ = np.
-0001b8b0: 656d 7074 7928 582e 7368 6170 655b 315d  empty(X.shape[1]
-0001b8c0: 2c20 6474 7970 653d 582e 6474 7970 6529  , dtype=X.dtype)
-0001b8d0: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
-0001b8e0: 2069 2c20 636f 6c20 696e 2065 6e75 6d65   i, col in enume
-0001b8f0: 7261 7465 2858 2e54 293a 0a20 2020 2020  rate(X.T):.     
-0001b900: 2020 2020 2020 2020 2020 2023 2046 6f72             # For
-0001b910: 2079 656f 2d6a 6f68 6e73 6f6e 2c20 6c65   yeo-johnson, le
-0001b920: 6176 6520 636f 6e73 7461 6e74 2066 6561  ave constant fea
-0001b930: 7475 7265 7320 756e 6368 616e 6765 640a  tures unchanged.
-0001b940: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001b950: 2320 6c61 6d62 6461 3d31 2063 6f72 7265  # lambda=1 corre
-0001b960: 7370 6f6e 6473 2074 6f20 7468 6520 6964  sponds to the id
-0001b970: 656e 7469 7479 2074 7261 6e73 666f 726d  entity transform
-0001b980: 6174 696f 6e0a 2020 2020 2020 2020 2020  ation.          
-0001b990: 2020 2020 2020 6973 5f63 6f6e 7374 616e        is_constan
-0001b9a0: 745f 6665 6174 7572 6520 3d20 5f69 735f  t_feature = _is_
-0001b9b0: 636f 6e73 7461 6e74 5f66 6561 7475 7265  constant_feature
-0001b9c0: 2876 6172 5b69 5d2c 206d 6561 6e5b 695d  (var[i], mean[i]
-0001b9d0: 2c20 6e5f 7361 6d70 6c65 7329 0a20 2020  , n_samples).   
-0001b9e0: 2020 2020 2020 2020 2020 2020 2069 6620               if 
-0001b9f0: 7365 6c66 2e6d 6574 686f 6420 3d3d 2022  self.method == "
-0001ba00: 7965 6f2d 6a6f 686e 736f 6e22 2061 6e64  yeo-johnson" and
-0001ba10: 2069 735f 636f 6e73 7461 6e74 5f66 6561   is_constant_fea
-0001ba20: 7475 7265 3a0a 2020 2020 2020 2020 2020  ture:.          
-0001ba30: 2020 2020 2020 2020 2020 7365 6c66 2e6c            self.l
-0001ba40: 616d 6264 6173 5f5b 695d 203d 2031 2e30  ambdas_[i] = 1.0
-0001ba50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001ba60: 2020 2020 2063 6f6e 7469 6e75 650a 0a20       continue.. 
-0001ba70: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-0001ba80: 656c 662e 6c61 6d62 6461 735f 5b69 5d20  elf.lambdas_[i] 
-0001ba90: 3d20 6f70 7469 6d5f 6675 6e63 7469 6f6e  = optim_function
-0001baa0: 2863 6f6c 290a 0a20 2020 2020 2020 2020  (col)..         
-0001bab0: 2020 2020 2020 2069 6620 7365 6c66 2e73         if self.s
-0001bac0: 7461 6e64 6172 6469 7a65 206f 7220 666f  tandardize or fo
-0001bad0: 7263 655f 7472 616e 7366 6f72 6d3a 0a20  rce_transform:. 
-0001bae0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001baf0: 2020 2058 5b3a 2c20 695d 203d 2074 7261     X[:, i] = tra
-0001bb00: 6e73 666f 726d 5f66 756e 6374 696f 6e28  nsform_function(
-0001bb10: 585b 3a2c 2069 5d2c 2073 656c 662e 6c61  X[:, i], self.la
-0001bb20: 6d62 6461 735f 5b69 5d29 0a0a 2020 2020  mbdas_[i])..    
-0001bb30: 2020 2020 6966 2073 656c 662e 7374 616e      if self.stan
-0001bb40: 6461 7264 697a 653a 0a20 2020 2020 2020  dardize:.       
-0001bb50: 2020 2020 2073 656c 662e 5f73 6361 6c65       self._scale
-0001bb60: 7220 3d20 5374 616e 6461 7264 5363 616c  r = StandardScal
-0001bb70: 6572 2863 6f70 793d 4661 6c73 6529 2e73  er(copy=False).s
-0001bb80: 6574 5f6f 7574 7075 7428 7472 616e 7366  et_output(transf
-0001bb90: 6f72 6d3d 2264 6566 6175 6c74 2229 0a20  orm="default"). 
-0001bba0: 2020 2020 2020 2020 2020 2069 6620 666f             if fo
-0001bbb0: 7263 655f 7472 616e 7366 6f72 6d3a 0a20  rce_transform:. 
-0001bbc0: 2020 2020 2020 2020 2020 2020 2020 2058                 X
-0001bbd0: 203d 2073 656c 662e 5f73 6361 6c65 722e   = self._scaler.
-0001bbe0: 6669 745f 7472 616e 7366 6f72 6d28 5829  fit_transform(X)
-0001bbf0: 0a20 2020 2020 2020 2020 2020 2065 6c73  .            els
-0001bc00: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-0001bc10: 2020 2073 656c 662e 5f73 6361 6c65 722e     self._scaler.
-0001bc20: 6669 7428 5829 0a0a 2020 2020 2020 2020  fit(X)..        
-0001bc30: 7265 7475 726e 2058 0a0a 2020 2020 6465  return X..    de
-0001bc40: 6620 7472 616e 7366 6f72 6d28 7365 6c66  f transform(self
-0001bc50: 2c20 5829 3a0a 2020 2020 2020 2020 2222  , X):.        ""
-0001bc60: 2241 7070 6c79 2074 6865 2070 6f77 6572  "Apply the power
-0001bc70: 2074 7261 6e73 666f 726d 2074 6f20 6561   transform to ea
-0001bc80: 6368 2066 6561 7475 7265 2075 7369 6e67  ch feature using
-0001bc90: 2074 6865 2066 6974 7465 6420 6c61 6d62   the fitted lamb
-0001bca0: 6461 732e 0a0a 2020 2020 2020 2020 5061  das...        Pa
-0001bcb0: 7261 6d65 7465 7273 0a20 2020 2020 2020  rameters.       
-0001bcc0: 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020   ----------.    
-0001bcd0: 2020 2020 5820 3a20 6172 7261 792d 6c69      X : array-li
-0001bce0: 6b65 206f 6620 7368 6170 6520 286e 5f73  ke of shape (n_s
-0001bcf0: 616d 706c 6573 2c20 6e5f 6665 6174 7572  amples, n_featur
-0001bd00: 6573 290a 2020 2020 2020 2020 2020 2020  es).            
-0001bd10: 5468 6520 6461 7461 2074 6f20 6265 2074  The data to be t
-0001bd20: 7261 6e73 666f 726d 6564 2075 7369 6e67  ransformed using
-0001bd30: 2061 2070 6f77 6572 2074 7261 6e73 666f   a power transfo
-0001bd40: 726d 6174 696f 6e2e 0a0a 2020 2020 2020  rmation...      
-0001bd50: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
-0001bd60: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
-0001bd70: 2020 585f 7472 616e 7320 3a20 6e64 6172    X_trans : ndar
-0001bd80: 7261 7920 6f66 2073 6861 7065 2028 6e5f  ray of shape (n_
-0001bd90: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
-0001bda0: 7265 7329 0a20 2020 2020 2020 2020 2020  res).           
-0001bdb0: 2054 6865 2074 7261 6e73 666f 726d 6564   The transformed
-0001bdc0: 2064 6174 612e 0a20 2020 2020 2020 2022   data..        "
-0001bdd0: 2222 0a20 2020 2020 2020 2063 6865 636b  "".        check
-0001bde0: 5f69 735f 6669 7474 6564 2873 656c 6629  _is_fitted(self)
-0001bdf0: 0a20 2020 2020 2020 2058 203d 2073 656c  .        X = sel
-0001be00: 662e 5f63 6865 636b 5f69 6e70 7574 2858  f._check_input(X
-0001be10: 2c20 696e 5f66 6974 3d46 616c 7365 2c20  , in_fit=False, 
-0001be20: 6368 6563 6b5f 706f 7369 7469 7665 3d54  check_positive=T
-0001be30: 7275 652c 2063 6865 636b 5f73 6861 7065  rue, check_shape
-0001be40: 3d54 7275 6529 0a0a 2020 2020 2020 2020  =True)..        
-0001be50: 7472 616e 7366 6f72 6d5f 6675 6e63 7469  transform_functi
-0001be60: 6f6e 203d 207b 0a20 2020 2020 2020 2020  on = {.         
-0001be70: 2020 2022 626f 782d 636f 7822 3a20 626f     "box-cox": bo
-0001be80: 7863 6f78 2c0a 2020 2020 2020 2020 2020  xcox,.          
-0001be90: 2020 2279 656f 2d6a 6f68 6e73 6f6e 223a    "yeo-johnson":
-0001bea0: 2073 656c 662e 5f79 656f 5f6a 6f68 6e73   self._yeo_johns
-0001beb0: 6f6e 5f74 7261 6e73 666f 726d 2c0a 2020  on_transform,.  
-0001bec0: 2020 2020 2020 7d5b 7365 6c66 2e6d 6574        }[self.met
-0001bed0: 686f 645d 0a20 2020 2020 2020 2066 6f72  hod].        for
-0001bee0: 2069 2c20 6c6d 6264 6120 696e 2065 6e75   i, lmbda in enu
-0001bef0: 6d65 7261 7465 2873 656c 662e 6c61 6d62  merate(self.lamb
-0001bf00: 6461 735f 293a 0a20 2020 2020 2020 2020  das_):.         
-0001bf10: 2020 2077 6974 6820 6e70 2e65 7272 7374     with np.errst
-0001bf20: 6174 6528 696e 7661 6c69 643d 2269 676e  ate(invalid="ign
-0001bf30: 6f72 6522 293a 2020 2320 6869 6465 204e  ore"):  # hide N
-0001bf40: 614e 2077 6172 6e69 6e67 730a 2020 2020  aN warnings.    
-0001bf50: 2020 2020 2020 2020 2020 2020 585b 3a2c              X[:,
-0001bf60: 2069 5d20 3d20 7472 616e 7366 6f72 6d5f   i] = transform_
-0001bf70: 6675 6e63 7469 6f6e 2858 5b3a 2c20 695d  function(X[:, i]
-0001bf80: 2c20 6c6d 6264 6129 0a0a 2020 2020 2020  , lmbda)..      
-0001bf90: 2020 6966 2073 656c 662e 7374 616e 6461    if self.standa
-0001bfa0: 7264 697a 653a 0a20 2020 2020 2020 2020  rdize:.         
-0001bfb0: 2020 2058 203d 2073 656c 662e 5f73 6361     X = self._sca
-0001bfc0: 6c65 722e 7472 616e 7366 6f72 6d28 5829  ler.transform(X)
-0001bfd0: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-0001bfe0: 2058 0a0a 2020 2020 6465 6620 696e 7665   X..    def inve
-0001bff0: 7273 655f 7472 616e 7366 6f72 6d28 7365  rse_transform(se
-0001c000: 6c66 2c20 5829 3a0a 2020 2020 2020 2020  lf, X):.        
-0001c010: 2222 2241 7070 6c79 2074 6865 2069 6e76  """Apply the inv
-0001c020: 6572 7365 2070 6f77 6572 2074 7261 6e73  erse power trans
-0001c030: 666f 726d 6174 696f 6e20 7573 696e 6720  formation using 
-0001c040: 7468 6520 6669 7474 6564 206c 616d 6264  the fitted lambd
-0001c050: 6173 2e0a 0a20 2020 2020 2020 2054 6865  as...        The
-0001c060: 2069 6e76 6572 7365 206f 6620 7468 6520   inverse of the 
-0001c070: 426f 782d 436f 7820 7472 616e 7366 6f72  Box-Cox transfor
-0001c080: 6d61 7469 6f6e 2069 7320 6769 7665 6e20  mation is given 
-0001c090: 6279 3a3a 0a0a 2020 2020 2020 2020 2020  by::..          
-0001c0a0: 2020 6966 206c 616d 6264 615f 203d 3d20    if lambda_ == 
-0001c0b0: 303a 0a20 2020 2020 2020 2020 2020 2020  0:.             
-0001c0c0: 2020 2058 203d 2065 7870 2858 5f74 7261     X = exp(X_tra
-0001c0d0: 6e73 290a 2020 2020 2020 2020 2020 2020  ns).            
-0001c0e0: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
-0001c0f0: 2020 2020 2020 5820 3d20 2858 5f74 7261        X = (X_tra
-0001c100: 6e73 202a 206c 616d 6264 615f 202b 2031  ns * lambda_ + 1
-0001c110: 2920 2a2a 2028 3120 2f20 6c61 6d62 6461  ) ** (1 / lambda
-0001c120: 5f29 0a0a 2020 2020 2020 2020 5468 6520  _)..        The 
-0001c130: 696e 7665 7273 6520 6f66 2074 6865 2059  inverse of the Y
-0001c140: 656f 2d4a 6f68 6e73 6f6e 2074 7261 6e73  eo-Johnson trans
-0001c150: 666f 726d 6174 696f 6e20 6973 2067 6976  formation is giv
-0001c160: 656e 2062 793a 3a0a 0a20 2020 2020 2020  en by::..       
-0001c170: 2020 2020 2069 6620 5820 3e3d 2030 2061       if X >= 0 a
-0001c180: 6e64 206c 616d 6264 615f 203d 3d20 303a  nd lambda_ == 0:
-0001c190: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c1a0: 2058 203d 2065 7870 2858 5f74 7261 6e73   X = exp(X_trans
-0001c1b0: 2920 2d20 310a 2020 2020 2020 2020 2020  ) - 1.          
-0001c1c0: 2020 656c 6966 2058 203e 3d20 3020 616e    elif X >= 0 an
-0001c1d0: 6420 6c61 6d62 6461 5f20 213d 2030 3a0a  d lambda_ != 0:.
-0001c1e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001c1f0: 5820 3d20 2858 5f74 7261 6e73 202a 206c  X = (X_trans * l
-0001c200: 616d 6264 615f 202b 2031 2920 2a2a 2028  ambda_ + 1) ** (
-0001c210: 3120 2f20 6c61 6d62 6461 5f29 202d 2031  1 / lambda_) - 1
-0001c220: 0a20 2020 2020 2020 2020 2020 2065 6c69  .            eli
-0001c230: 6620 5820 3c20 3020 616e 6420 6c61 6d62  f X < 0 and lamb
-0001c240: 6461 5f20 213d 2032 3a0a 2020 2020 2020  da_ != 2:.      
-0001c250: 2020 2020 2020 2020 2020 5820 3d20 3120            X = 1 
-0001c260: 2d20 282d 2832 202d 206c 616d 6264 615f  - (-(2 - lambda_
-0001c270: 2920 2a20 585f 7472 616e 7320 2b20 3129  ) * X_trans + 1)
-0001c280: 202a 2a20 2831 202f 2028 3220 2d20 6c61   ** (1 / (2 - la
-0001c290: 6d62 6461 5f29 290a 2020 2020 2020 2020  mbda_)).        
-0001c2a0: 2020 2020 656c 6966 2058 203c 2030 2061      elif X < 0 a
-0001c2b0: 6e64 206c 616d 6264 615f 203d 3d20 323a  nd lambda_ == 2:
-0001c2c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-0001c2d0: 2058 203d 2031 202d 2065 7870 282d 585f   X = 1 - exp(-X_
-0001c2e0: 7472 616e 7329 0a0a 2020 2020 2020 2020  trans)..        
-0001c2f0: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
-0001c300: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
-0001c310: 2020 2020 2020 5820 3a20 6172 7261 792d        X : array-
-0001c320: 6c69 6b65 206f 6620 7368 6170 6520 286e  like of shape (n
-0001c330: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
-0001c340: 7572 6573 290a 2020 2020 2020 2020 2020  ures).          
-0001c350: 2020 5468 6520 7472 616e 7366 6f72 6d65    The transforme
-0001c360: 6420 6461 7461 2e0a 0a20 2020 2020 2020  d data...       
-0001c370: 2052 6574 7572 6e73 0a20 2020 2020 2020   Returns.       
-0001c380: 202d 2d2d 2d2d 2d2d 0a20 2020 2020 2020   -------.       
-0001c390: 2058 203a 206e 6461 7272 6179 206f 6620   X : ndarray of 
-0001c3a0: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
-0001c3b0: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
-0001c3c0: 2020 2020 2020 2020 2020 5468 6520 6f72            The or
-0001c3d0: 6967 696e 616c 2064 6174 612e 0a20 2020  iginal data..   
-0001c3e0: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
-0001c3f0: 2063 6865 636b 5f69 735f 6669 7474 6564   check_is_fitted
-0001c400: 2873 656c 6629 0a20 2020 2020 2020 2058  (self).        X
-0001c410: 203d 2073 656c 662e 5f63 6865 636b 5f69   = self._check_i
-0001c420: 6e70 7574 2858 2c20 696e 5f66 6974 3d46  nput(X, in_fit=F
-0001c430: 616c 7365 2c20 6368 6563 6b5f 7368 6170  alse, check_shap
-0001c440: 653d 5472 7565 290a 0a20 2020 2020 2020  e=True)..       
-0001c450: 2069 6620 7365 6c66 2e73 7461 6e64 6172   if self.standar
-0001c460: 6469 7a65 3a0a 2020 2020 2020 2020 2020  dize:.          
-0001c470: 2020 5820 3d20 7365 6c66 2e5f 7363 616c    X = self._scal
-0001c480: 6572 2e69 6e76 6572 7365 5f74 7261 6e73  er.inverse_trans
-0001c490: 666f 726d 2858 290a 0a20 2020 2020 2020  form(X)..       
-0001c4a0: 2069 6e76 5f66 756e 203d 207b 0a20 2020   inv_fun = {.   
-0001c4b0: 2020 2020 2020 2020 2022 626f 782d 636f           "box-co
-0001c4c0: 7822 3a20 7365 6c66 2e5f 626f 785f 636f  x": self._box_co
-0001c4d0: 785f 696e 7665 7273 655f 7472 616e 666f  x_inverse_tranfo
-0001c4e0: 726d 2c0a 2020 2020 2020 2020 2020 2020  rm,.            
-0001c4f0: 2279 656f 2d6a 6f68 6e73 6f6e 223a 2073  "yeo-johnson": s
-0001c500: 656c 662e 5f79 656f 5f6a 6f68 6e73 6f6e  elf._yeo_johnson
-0001c510: 5f69 6e76 6572 7365 5f74 7261 6e73 666f  _inverse_transfo
-0001c520: 726d 2c0a 2020 2020 2020 2020 7d5b 7365  rm,.        }[se
-0001c530: 6c66 2e6d 6574 686f 645d 0a20 2020 2020  lf.method].     
-0001c540: 2020 2066 6f72 2069 2c20 6c6d 6264 6120     for i, lmbda 
-0001c550: 696e 2065 6e75 6d65 7261 7465 2873 656c  in enumerate(sel
-0001c560: 662e 6c61 6d62 6461 735f 293a 0a20 2020  f.lambdas_):.   
-0001c570: 2020 2020 2020 2020 2077 6974 6820 6e70           with np
-0001c580: 2e65 7272 7374 6174 6528 696e 7661 6c69  .errstate(invali
-0001c590: 643d 2269 676e 6f72 6522 293a 2020 2320  d="ignore"):  # 
-0001c5a0: 6869 6465 204e 614e 2077 6172 6e69 6e67  hide NaN warning
-0001c5b0: 730a 2020 2020 2020 2020 2020 2020 2020  s.              
-0001c5c0: 2020 585b 3a2c 2069 5d20 3d20 696e 765f    X[:, i] = inv_
-0001c5d0: 6675 6e28 585b 3a2c 2069 5d2c 206c 6d62  fun(X[:, i], lmb
-0001c5e0: 6461 290a 0a20 2020 2020 2020 2072 6574  da)..        ret
-0001c5f0: 7572 6e20 580a 0a20 2020 2064 6566 205f  urn X..    def _
-0001c600: 626f 785f 636f 785f 696e 7665 7273 655f  box_cox_inverse_
-0001c610: 7472 616e 666f 726d 2873 656c 662c 2078  tranform(self, x
-0001c620: 2c20 6c6d 6264 6129 3a0a 2020 2020 2020  , lmbda):.      
-0001c630: 2020 2222 2252 6574 7572 6e20 696e 7665    """Return inve
-0001c640: 7273 652d 7472 616e 7366 6f72 6d65 6420  rse-transformed 
-0001c650: 696e 7075 7420 7820 666f 6c6c 6f77 696e  input x followin
-0001c660: 6720 426f 782d 436f 7820 696e 7665 7273  g Box-Cox invers
-0001c670: 650a 2020 2020 2020 2020 7472 616e 7366  e.        transf
-0001c680: 6f72 6d20 7769 7468 2070 6172 616d 6574  orm with paramet
-0001c690: 6572 206c 616d 6264 612e 0a20 2020 2020  er lambda..     
-0001c6a0: 2020 2022 2222 0a20 2020 2020 2020 2069     """.        i
-0001c6b0: 6620 6c6d 6264 6120 3d3d 2030 3a0a 2020  f lmbda == 0:.  
-0001c6c0: 2020 2020 2020 2020 2020 785f 696e 7620            x_inv 
-0001c6d0: 3d20 6e70 2e65 7870 2878 290a 2020 2020  = np.exp(x).    
-0001c6e0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0001c6f0: 2020 2020 2020 785f 696e 7620 3d20 2878        x_inv = (x
-0001c700: 202a 206c 6d62 6461 202b 2031 2920 2a2a   * lmbda + 1) **
-0001c710: 2028 3120 2f20 6c6d 6264 6129 0a0a 2020   (1 / lmbda)..  
-0001c720: 2020 2020 2020 7265 7475 726e 2078 5f69        return x_i
-0001c730: 6e76 0a0a 2020 2020 6465 6620 5f79 656f  nv..    def _yeo
-0001c740: 5f6a 6f68 6e73 6f6e 5f69 6e76 6572 7365  _johnson_inverse
-0001c750: 5f74 7261 6e73 666f 726d 2873 656c 662c  _transform(self,
-0001c760: 2078 2c20 6c6d 6264 6129 3a0a 2020 2020   x, lmbda):.    
-0001c770: 2020 2020 2222 2252 6574 7572 6e20 696e      """Return in
-0001c780: 7665 7273 652d 7472 616e 7366 6f72 6d65  verse-transforme
-0001c790: 6420 696e 7075 7420 7820 666f 6c6c 6f77  d input x follow
-0001c7a0: 696e 6720 5965 6f2d 4a6f 686e 736f 6e20  ing Yeo-Johnson 
-0001c7b0: 696e 7665 7273 650a 2020 2020 2020 2020  inverse.        
-0001c7c0: 7472 616e 7366 6f72 6d20 7769 7468 2070  transform with p
-0001c7d0: 6172 616d 6574 6572 206c 616d 6264 612e  arameter lambda.
-0001c7e0: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-0001c7f0: 2020 2020 2078 5f69 6e76 203d 206e 702e       x_inv = np.
-0001c800: 7a65 726f 735f 6c69 6b65 2878 290a 2020  zeros_like(x).  
-0001c810: 2020 2020 2020 706f 7320 3d20 7820 3e3d        pos = x >=
-0001c820: 2030 0a0a 2020 2020 2020 2020 2320 7768   0..        # wh
-0001c830: 656e 2078 203e 3d20 300a 2020 2020 2020  en x >= 0.      
-0001c840: 2020 6966 2061 6273 286c 6d62 6461 2920    if abs(lmbda) 
-0001c850: 3c20 6e70 2e73 7061 6369 6e67 2831 2e30  < np.spacing(1.0
-0001c860: 293a 0a20 2020 2020 2020 2020 2020 2078  ):.            x
-0001c870: 5f69 6e76 5b70 6f73 5d20 3d20 6e70 2e65  _inv[pos] = np.e
-0001c880: 7870 2878 5b70 6f73 5d29 202d 2031 0a20  xp(x[pos]) - 1. 
-0001c890: 2020 2020 2020 2065 6c73 653a 2020 2320         else:  # 
-0001c8a0: 6c6d 6264 6120 213d 2030 0a20 2020 2020  lmbda != 0.     
-0001c8b0: 2020 2020 2020 2078 5f69 6e76 5b70 6f73         x_inv[pos
-0001c8c0: 5d20 3d20 6e70 2e70 6f77 6572 2878 5b70  ] = np.power(x[p
-0001c8d0: 6f73 5d20 2a20 6c6d 6264 6120 2b20 312c  os] * lmbda + 1,
-0001c8e0: 2031 202f 206c 6d62 6461 2920 2d20 310a   1 / lmbda) - 1.
-0001c8f0: 0a20 2020 2020 2020 2023 2077 6865 6e20  .        # when 
-0001c900: 7820 3c20 300a 2020 2020 2020 2020 6966  x < 0.        if
-0001c910: 2061 6273 286c 6d62 6461 202d 2032 2920   abs(lmbda - 2) 
-0001c920: 3e20 6e70 2e73 7061 6369 6e67 2831 2e30  > np.spacing(1.0
-0001c930: 293a 0a20 2020 2020 2020 2020 2020 2078  ):.            x
-0001c940: 5f69 6e76 5b7e 706f 735d 203d 2031 202d  _inv[~pos] = 1 -
-0001c950: 206e 702e 706f 7765 7228 2d28 3220 2d20   np.power(-(2 - 
-0001c960: 6c6d 6264 6129 202a 2078 5b7e 706f 735d  lmbda) * x[~pos]
-0001c970: 202b 2031 2c20 3120 2f20 2832 202d 206c   + 1, 1 / (2 - l
-0001c980: 6d62 6461 2929 0a20 2020 2020 2020 2065  mbda)).        e
-0001c990: 6c73 653a 2020 2320 6c6d 6264 6120 3d3d  lse:  # lmbda ==
-0001c9a0: 2032 0a20 2020 2020 2020 2020 2020 2078   2.            x
-0001c9b0: 5f69 6e76 5b7e 706f 735d 203d 2031 202d  _inv[~pos] = 1 -
-0001c9c0: 206e 702e 6578 7028 2d78 5b7e 706f 735d   np.exp(-x[~pos]
-0001c9d0: 290a 0a20 2020 2020 2020 2072 6574 7572  )..        retur
-0001c9e0: 6e20 785f 696e 760a 0a20 2020 2064 6566  n x_inv..    def
-0001c9f0: 205f 7965 6f5f 6a6f 686e 736f 6e5f 7472   _yeo_johnson_tr
-0001ca00: 616e 7366 6f72 6d28 7365 6c66 2c20 782c  ansform(self, x,
-0001ca10: 206c 6d62 6461 293a 0a20 2020 2020 2020   lmbda):.       
-0001ca20: 2022 2222 5265 7475 726e 2074 7261 6e73   """Return trans
-0001ca30: 666f 726d 6564 2069 6e70 7574 2078 2066  formed input x f
-0001ca40: 6f6c 6c6f 7769 6e67 2059 656f 2d4a 6f68  ollowing Yeo-Joh
-0001ca50: 6e73 6f6e 2074 7261 6e73 666f 726d 2077  nson transform w
-0001ca60: 6974 680a 2020 2020 2020 2020 7061 7261  ith.        para
-0001ca70: 6d65 7465 7220 6c61 6d62 6461 2e0a 2020  meter lambda..  
-0001ca80: 2020 2020 2020 2222 220a 0a20 2020 2020        """..     
-0001ca90: 2020 206f 7574 203d 206e 702e 7a65 726f     out = np.zero
-0001caa0: 735f 6c69 6b65 2878 290a 2020 2020 2020  s_like(x).      
-0001cab0: 2020 706f 7320 3d20 7820 3e3d 2030 2020    pos = x >= 0  
-0001cac0: 2320 6269 6e61 7279 206d 6173 6b0a 0a20  # binary mask.. 
-0001cad0: 2020 2020 2020 2023 2077 6865 6e20 7820         # when x 
-0001cae0: 3e3d 2030 0a20 2020 2020 2020 2069 6620  >= 0.        if 
-0001caf0: 6162 7328 6c6d 6264 6129 203c 206e 702e  abs(lmbda) < np.
-0001cb00: 7370 6163 696e 6728 312e 3029 3a0a 2020  spacing(1.0):.  
-0001cb10: 2020 2020 2020 2020 2020 6f75 745b 706f            out[po
-0001cb20: 735d 203d 206e 702e 6c6f 6731 7028 785b  s] = np.log1p(x[
-0001cb30: 706f 735d 290a 2020 2020 2020 2020 656c  pos]).        el
-0001cb40: 7365 3a20 2023 206c 6d62 6461 2021 3d20  se:  # lmbda != 
-0001cb50: 300a 2020 2020 2020 2020 2020 2020 6f75  0.            ou
-0001cb60: 745b 706f 735d 203d 2028 6e70 2e70 6f77  t[pos] = (np.pow
-0001cb70: 6572 2878 5b70 6f73 5d20 2b20 312c 206c  er(x[pos] + 1, l
-0001cb80: 6d62 6461 2920 2d20 3129 202f 206c 6d62  mbda) - 1) / lmb
-0001cb90: 6461 0a0a 2020 2020 2020 2020 2320 7768  da..        # wh
-0001cba0: 656e 2078 203c 2030 0a20 2020 2020 2020  en x < 0.       
-0001cbb0: 2069 6620 6162 7328 6c6d 6264 6120 2d20   if abs(lmbda - 
-0001cbc0: 3229 203e 206e 702e 7370 6163 696e 6728  2) > np.spacing(
-0001cbd0: 312e 3029 3a0a 2020 2020 2020 2020 2020  1.0):.          
-0001cbe0: 2020 6f75 745b 7e70 6f73 5d20 3d20 2d28    out[~pos] = -(
-0001cbf0: 6e70 2e70 6f77 6572 282d 785b 7e70 6f73  np.power(-x[~pos
-0001cc00: 5d20 2b20 312c 2032 202d 206c 6d62 6461  ] + 1, 2 - lmbda
-0001cc10: 2920 2d20 3129 202f 2028 3220 2d20 6c6d  ) - 1) / (2 - lm
-0001cc20: 6264 6129 0a20 2020 2020 2020 2065 6c73  bda).        els
-0001cc30: 653a 2020 2320 6c6d 6264 6120 3d3d 2032  e:  # lmbda == 2
-0001cc40: 0a20 2020 2020 2020 2020 2020 206f 7574  .            out
-0001cc50: 5b7e 706f 735d 203d 202d 6e70 2e6c 6f67  [~pos] = -np.log
-0001cc60: 3170 282d 785b 7e70 6f73 5d29 0a0a 2020  1p(-x[~pos])..  
-0001cc70: 2020 2020 2020 7265 7475 726e 206f 7574        return out
-0001cc80: 0a0a 2020 2020 6465 6620 5f62 6f78 5f63  ..    def _box_c
-0001cc90: 6f78 5f6f 7074 696d 697a 6528 7365 6c66  ox_optimize(self
-0001cca0: 2c20 7829 3a0a 2020 2020 2020 2020 2222  , x):.        ""
-0001ccb0: 2246 696e 6420 616e 6420 7265 7475 726e  "Find and return
-0001ccc0: 206f 7074 696d 616c 206c 616d 6264 6120   optimal lambda 
-0001ccd0: 7061 7261 6d65 7465 7220 6f66 2074 6865  parameter of the
-0001cce0: 2042 6f78 2d43 6f78 2074 7261 6e73 666f   Box-Cox transfo
-0001ccf0: 726d 2062 790a 2020 2020 2020 2020 4d4c  rm by.        ML
-0001cd00: 452c 2066 6f72 206f 6273 6572 7665 6420  E, for observed 
-0001cd10: 6461 7461 2078 2e0a 0a20 2020 2020 2020  data x...       
-0001cd20: 2057 6520 6865 7265 2075 7365 2073 6369   We here use sci
-0001cd30: 7079 2062 7569 6c74 696e 7320 7768 6963  py builtins whic
-0001cd40: 6820 7573 6573 2074 6865 2062 7265 6e74  h uses the brent
-0001cd50: 206f 7074 696d 697a 6572 2e0a 2020 2020   optimizer..    
-0001cd60: 2020 2020 2222 220a 2020 2020 2020 2020      """.        
-0001cd70: 6d61 736b 203d 206e 702e 6973 6e61 6e28  mask = np.isnan(
-0001cd80: 7829 0a20 2020 2020 2020 2069 6620 6e70  x).        if np
-0001cd90: 2e61 6c6c 286d 6173 6b29 3a0a 2020 2020  .all(mask):.    
-0001cda0: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
-0001cdb0: 6c75 6545 7272 6f72 2822 436f 6c75 6d6e  lueError("Column
-0001cdc0: 206d 7573 7420 6e6f 7420 6265 2061 6c6c   must not be all
-0001cdd0: 206e 616e 2e22 290a 0a20 2020 2020 2020   nan.")..       
-0001cde0: 2023 2074 6865 2063 6f6d 7075 7461 7469   # the computati
-0001cdf0: 6f6e 206f 6620 6c61 6d62 6461 2069 7320  on of lambda is 
-0001ce00: 696e 666c 7565 6e63 6564 2062 7920 4e61  influenced by Na
-0001ce10: 4e73 2073 6f20 7765 206e 6565 6420 746f  Ns so we need to
-0001ce20: 0a20 2020 2020 2020 2023 2067 6574 2072  .        # get r
-0001ce30: 6964 206f 6620 7468 656d 0a20 2020 2020  id of them.     
-0001ce40: 2020 205f 2c20 6c6d 6264 6120 3d20 7374     _, lmbda = st
-0001ce50: 6174 732e 626f 7863 6f78 2878 5b7e 6d61  ats.boxcox(x[~ma
-0001ce60: 736b 5d2c 206c 6d62 6461 3d4e 6f6e 6529  sk], lmbda=None)
-0001ce70: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-0001ce80: 206c 6d62 6461 0a0a 2020 2020 6465 6620   lmbda..    def 
-0001ce90: 5f79 656f 5f6a 6f68 6e73 6f6e 5f6f 7074  _yeo_johnson_opt
-0001cea0: 696d 697a 6528 7365 6c66 2c20 7829 3a0a  imize(self, x):.
-0001ceb0: 2020 2020 2020 2020 2222 2246 696e 6420          """Find 
-0001cec0: 616e 6420 7265 7475 726e 206f 7074 696d  and return optim
-0001ced0: 616c 206c 616d 6264 6120 7061 7261 6d65  al lambda parame
-0001cee0: 7465 7220 6f66 2074 6865 2059 656f 2d4a  ter of the Yeo-J
-0001cef0: 6f68 6e73 6f6e 0a20 2020 2020 2020 2074  ohnson.        t
-0001cf00: 7261 6e73 666f 726d 2062 7920 4d4c 452c  ransform by MLE,
-0001cf10: 2066 6f72 206f 6273 6572 7665 6420 6461   for observed da
-0001cf20: 7461 2078 2e0a 0a20 2020 2020 2020 204c  ta x...        L
-0001cf30: 696b 6520 666f 7220 426f 782d 436f 782c  ike for Box-Cox,
-0001cf40: 204d 4c45 2069 7320 646f 6e65 2076 6961   MLE is done via
-0001cf50: 2074 6865 2062 7265 6e74 206f 7074 696d   the brent optim
-0001cf60: 697a 6572 2e0a 2020 2020 2020 2020 2222  izer..        ""
-0001cf70: 220a 2020 2020 2020 2020 785f 7469 6e79  ".        x_tiny
-0001cf80: 203d 206e 702e 6669 6e66 6f28 6e70 2e66   = np.finfo(np.f
-0001cf90: 6c6f 6174 3634 292e 7469 6e79 0a0a 2020  loat64).tiny..  
-0001cfa0: 2020 2020 2020 6465 6620 5f6e 6567 5f6c        def _neg_l
-0001cfb0: 6f67 5f6c 696b 656c 6968 6f6f 6428 6c6d  og_likelihood(lm
-0001cfc0: 6264 6129 3a0a 2020 2020 2020 2020 2020  bda):.          
-0001cfd0: 2020 2222 2252 6574 7572 6e20 7468 6520    """Return the 
-0001cfe0: 6e65 6761 7469 7665 206c 6f67 206c 696b  negative log lik
-0001cff0: 656c 6968 6f6f 6420 6f66 2074 6865 206f  elihood of the o
-0001d000: 6273 6572 7665 6420 6461 7461 2078 2061  bserved data x a
-0001d010: 7320 610a 2020 2020 2020 2020 2020 2020  s a.            
-0001d020: 6675 6e63 7469 6f6e 206f 6620 6c61 6d62  function of lamb
-0001d030: 6461 2e22 2222 0a20 2020 2020 2020 2020  da.""".         
-0001d040: 2020 2078 5f74 7261 6e73 203d 2073 656c     x_trans = sel
-0001d050: 662e 5f79 656f 5f6a 6f68 6e73 6f6e 5f74  f._yeo_johnson_t
-0001d060: 7261 6e73 666f 726d 2878 2c20 6c6d 6264  ransform(x, lmbd
-0001d070: 6129 0a20 2020 2020 2020 2020 2020 206e  a).            n
-0001d080: 5f73 616d 706c 6573 203d 2078 2e73 6861  _samples = x.sha
-0001d090: 7065 5b30 5d0a 2020 2020 2020 2020 2020  pe[0].          
-0001d0a0: 2020 785f 7472 616e 735f 7661 7220 3d20    x_trans_var = 
-0001d0b0: 785f 7472 616e 732e 7661 7228 290a 0a20  x_trans.var().. 
-0001d0c0: 2020 2020 2020 2020 2020 2023 2052 656a             # Rej
-0001d0d0: 6563 7420 7472 616e 7366 6f72 6d65 6420  ect transformed 
-0001d0e0: 6461 7461 2074 6861 7420 776f 756c 6420  data that would 
-0001d0f0: 7261 6973 6520 6120 5275 6e74 696d 6557  raise a RuntimeW
-0001d100: 6172 6e69 6e67 2069 6e20 6e70 2e6c 6f67  arning in np.log
-0001d110: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-0001d120: 785f 7472 616e 735f 7661 7220 3c20 785f  x_trans_var < x_
-0001d130: 7469 6e79 3a0a 2020 2020 2020 2020 2020  tiny:.          
-0001d140: 2020 2020 2020 7265 7475 726e 206e 702e        return np.
-0001d150: 696e 660a 0a20 2020 2020 2020 2020 2020  inf..           
-0001d160: 206c 6f67 5f76 6172 203d 206e 702e 6c6f   log_var = np.lo
-0001d170: 6728 785f 7472 616e 735f 7661 7229 0a20  g(x_trans_var). 
-0001d180: 2020 2020 2020 2020 2020 206c 6f67 6c69             logli
-0001d190: 6b65 203d 202d 6e5f 7361 6d70 6c65 7320  ke = -n_samples 
-0001d1a0: 2f20 3220 2a20 6c6f 675f 7661 720a 2020  / 2 * log_var.  
-0001d1b0: 2020 2020 2020 2020 2020 6c6f 676c 696b            loglik
-0001d1c0: 6520 2b3d 2028 6c6d 6264 6120 2d20 3129  e += (lmbda - 1)
-0001d1d0: 202a 2028 6e70 2e73 6967 6e28 7829 202a   * (np.sign(x) *
-0001d1e0: 206e 702e 6c6f 6731 7028 6e70 2e61 6273   np.log1p(np.abs
-0001d1f0: 2878 2929 292e 7375 6d28 290a 0a20 2020  (x))).sum()..   
-0001d200: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
-0001d210: 2d6c 6f67 6c69 6b65 0a0a 2020 2020 2020  -loglike..      
-0001d220: 2020 2320 7468 6520 636f 6d70 7574 6174    # the computat
-0001d230: 696f 6e20 6f66 206c 616d 6264 6120 6973  ion of lambda is
-0001d240: 2069 6e66 6c75 656e 6365 6420 6279 204e   influenced by N
-0001d250: 614e 7320 736f 2077 6520 6e65 6564 2074  aNs so we need t
-0001d260: 6f0a 2020 2020 2020 2020 2320 6765 7420  o.        # get 
-0001d270: 7269 6420 6f66 2074 6865 6d0a 2020 2020  rid of them.    
-0001d280: 2020 2020 7820 3d20 785b 7e6e 702e 6973      x = x[~np.is
-0001d290: 6e61 6e28 7829 5d0a 2020 2020 2020 2020  nan(x)].        
-0001d2a0: 2320 6368 6f6f 7369 6e67 2062 7261 636b  # choosing brack
-0001d2b0: 6574 202d 322c 2032 206c 696b 6520 666f  et -2, 2 like fo
-0001d2c0: 7220 626f 7863 6f78 0a20 2020 2020 2020  r boxcox.       
-0001d2d0: 2072 6574 7572 6e20 6f70 7469 6d69 7a65   return optimize
-0001d2e0: 2e62 7265 6e74 285f 6e65 675f 6c6f 675f  .brent(_neg_log_
-0001d2f0: 6c69 6b65 6c69 686f 6f64 2c20 6272 6163  likelihood, brac
-0001d300: 6b3d 282d 322c 2032 2929 0a0a 2020 2020  k=(-2, 2))..    
-0001d310: 6465 6620 5f63 6865 636b 5f69 6e70 7574  def _check_input
-0001d320: 2873 656c 662c 2058 2c20 696e 5f66 6974  (self, X, in_fit
-0001d330: 2c20 6368 6563 6b5f 706f 7369 7469 7665  , check_positive
-0001d340: 3d46 616c 7365 2c20 6368 6563 6b5f 7368  =False, check_sh
-0001d350: 6170 653d 4661 6c73 6529 3a0a 2020 2020  ape=False):.    
-0001d360: 2020 2020 2222 2256 616c 6964 6174 6520      """Validate 
-0001d370: 7468 6520 696e 7075 7420 6265 666f 7265  the input before
-0001d380: 2066 6974 2061 6e64 2074 7261 6e73 666f   fit and transfo
-0001d390: 726d 2e0a 0a20 2020 2020 2020 2050 6172  rm...        Par
-0001d3a0: 616d 6574 6572 730a 2020 2020 2020 2020  ameters.        
-0001d3b0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
-0001d3c0: 2020 2058 203a 2061 7272 6179 2d6c 696b     X : array-lik
-0001d3d0: 6520 6f66 2073 6861 7065 2028 6e5f 7361  e of shape (n_sa
-0001d3e0: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
-0001d3f0: 7329 0a0a 2020 2020 2020 2020 696e 5f66  s)..        in_f
-0001d400: 6974 203a 2062 6f6f 6c0a 2020 2020 2020  it : bool.      
-0001d410: 2020 2020 2020 5768 6574 6865 7220 6f72        Whether or
-0001d420: 206e 6f74 2060 5f63 6865 636b 5f69 6e70   not `_check_inp
-0001d430: 7574 6020 6973 2063 616c 6c65 6420 6672  ut` is called fr
-0001d440: 6f6d 2060 6669 7460 206f 7220 6f74 6865  om `fit` or othe
-0001d450: 720a 2020 2020 2020 2020 2020 2020 6d65  r.            me
-0001d460: 7468 6f64 732c 2065 2e67 2e20 6070 7265  thods, e.g. `pre
-0001d470: 6469 6374 602c 2060 7472 616e 7366 6f72  dict`, `transfor
-0001d480: 6d60 2c20 6574 632e 0a0a 2020 2020 2020  m`, etc...      
-0001d490: 2020 6368 6563 6b5f 706f 7369 7469 7665    check_positive
-0001d4a0: 203a 2062 6f6f 6c2c 2064 6566 6175 6c74   : bool, default
-0001d4b0: 3d46 616c 7365 0a20 2020 2020 2020 2020  =False.         
-0001d4c0: 2020 2049 6620 5472 7565 2c20 6368 6563     If True, chec
-0001d4d0: 6b20 7468 6174 2061 6c6c 2064 6174 6120  k that all data 
-0001d4e0: 6973 2070 6f73 6974 6976 6520 616e 6420  is positive and 
-0001d4f0: 6e6f 6e2d 7a65 726f 2028 6f6e 6c79 2069  non-zero (only i
-0001d500: 660a 2020 2020 2020 2020 2020 2020 6060  f.            ``
-0001d510: 7365 6c66 2e6d 6574 686f 643d 3d27 626f  self.method=='bo
-0001d520: 782d 636f 7827 6060 292e 0a0a 2020 2020  x-cox'``)...    
-0001d530: 2020 2020 6368 6563 6b5f 7368 6170 6520      check_shape 
-0001d540: 3a20 626f 6f6c 2c20 6465 6661 756c 743d  : bool, default=
-0001d550: 4661 6c73 650a 2020 2020 2020 2020 2020  False.          
-0001d560: 2020 4966 2054 7275 652c 2063 6865 636b    If True, check
-0001d570: 2074 6861 7420 6e5f 6665 6174 7572 6573   that n_features
-0001d580: 206d 6174 6368 6573 2074 6865 206c 656e   matches the len
-0001d590: 6774 6820 6f66 2073 656c 662e 6c61 6d62  gth of self.lamb
-0001d5a0: 6461 735f 0a20 2020 2020 2020 2022 2222  das_.        """
-0001d5b0: 0a20 2020 2020 2020 2058 203d 2073 656c  .        X = sel
-0001d5c0: 662e 5f76 616c 6964 6174 655f 6461 7461  f._validate_data
-0001d5d0: 280a 2020 2020 2020 2020 2020 2020 582c  (.            X,
-0001d5e0: 0a20 2020 2020 2020 2020 2020 2065 6e73  .            ens
-0001d5f0: 7572 655f 3264 3d54 7275 652c 0a20 2020  ure_2d=True,.   
-0001d600: 2020 2020 2020 2020 2064 7479 7065 3d46           dtype=F
-0001d610: 4c4f 4154 5f44 5459 5045 532c 0a20 2020  LOAT_DTYPES,.   
-0001d620: 2020 2020 2020 2020 2063 6f70 793d 7365           copy=se
-0001d630: 6c66 2e63 6f70 792c 0a20 2020 2020 2020  lf.copy,.       
-0001d640: 2020 2020 2066 6f72 6365 5f61 6c6c 5f66       force_all_f
-0001d650: 696e 6974 653d 2261 6c6c 6f77 2d6e 616e  inite="allow-nan
-0001d660: 222c 0a20 2020 2020 2020 2020 2020 2072  ",.            r
-0001d670: 6573 6574 3d69 6e5f 6669 742c 0a20 2020  eset=in_fit,.   
-0001d680: 2020 2020 2029 0a0a 2020 2020 2020 2020       )..        
-0001d690: 7769 7468 2077 6172 6e69 6e67 732e 6361  with warnings.ca
-0001d6a0: 7463 685f 7761 726e 696e 6773 2829 3a0a  tch_warnings():.
-0001d6b0: 2020 2020 2020 2020 2020 2020 7761 726e              warn
-0001d6c0: 696e 6773 2e66 696c 7465 7277 6172 6e69  ings.filterwarni
-0001d6d0: 6e67 7328 2269 676e 6f72 6522 2c20 7222  ngs("ignore", r"
-0001d6e0: 416c 6c2d 4e61 4e20 2873 6c69 6365 7c61  All-NaN (slice|a
-0001d6f0: 7869 7329 2065 6e63 6f75 6e74 6572 6564  xis) encountered
-0001d700: 2229 0a20 2020 2020 2020 2020 2020 2069  ").            i
-0001d710: 6620 6368 6563 6b5f 706f 7369 7469 7665  f check_positive
-0001d720: 2061 6e64 2073 656c 662e 6d65 7468 6f64   and self.method
-0001d730: 203d 3d20 2262 6f78 2d63 6f78 2220 616e   == "box-cox" an
-0001d740: 6420 6e70 2e6e 616e 6d69 6e28 5829 203c  d np.nanmin(X) <
-0001d750: 3d20 303a 0a20 2020 2020 2020 2020 2020  = 0:.           
-0001d760: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-0001d770: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
-0001d780: 2020 2020 2020 2020 2020 2022 5468 6520             "The 
-0001d790: 426f 782d 436f 7820 7472 616e 7366 6f72  Box-Cox transfor
-0001d7a0: 6d61 7469 6f6e 2063 616e 206f 6e6c 7920  mation can only 
-0001d7b0: 6265 2022 0a20 2020 2020 2020 2020 2020  be ".           
-0001d7c0: 2020 2020 2020 2020 2022 6170 706c 6965           "applie
-0001d7d0: 6420 746f 2073 7472 6963 746c 7920 706f  d to strictly po
-0001d7e0: 7369 7469 7665 2064 6174 6122 0a20 2020  sitive data".   
-0001d7f0: 2020 2020 2020 2020 2020 2020 2029 0a0a               )..
-0001d800: 2020 2020 2020 2020 6966 2063 6865 636b          if check
-0001d810: 5f73 6861 7065 2061 6e64 206e 6f74 2058  _shape and not X
-0001d820: 2e73 6861 7065 5b31 5d20 3d3d 206c 656e  .shape[1] == len
-0001d830: 2873 656c 662e 6c61 6d62 6461 735f 293a  (self.lambdas_):
-0001d840: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
-0001d850: 7365 2056 616c 7565 4572 726f 7228 0a20  se ValueError(. 
-0001d860: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-0001d870: 496e 7075 7420 6461 7461 2068 6173 2061  Input data has a
-0001d880: 2064 6966 6665 7265 6e74 206e 756d 6265   different numbe
-0001d890: 7220 6f66 2066 6561 7475 7265 7320 220a  r of features ".
-0001d8a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-0001d8b0: 2274 6861 6e20 6669 7474 696e 6720 6461  "than fitting da
-0001d8c0: 7461 2e20 5368 6f75 6c64 2068 6176 6520  ta. Should have 
-0001d8d0: 7b6e 7d2c 2064 6174 6120 6861 7320 7b6d  {n}, data has {m
-0001d8e0: 7d22 2e66 6f72 6d61 7428 0a20 2020 2020  }".format(.     
-0001d8f0: 2020 2020 2020 2020 2020 2020 2020 206e                 n
-0001d900: 3d6c 656e 2873 656c 662e 6c61 6d62 6461  =len(self.lambda
-0001d910: 735f 292c 206d 3d58 2e73 6861 7065 5b31  s_), m=X.shape[1
-0001d920: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              
-0001d930: 2020 290a 2020 2020 2020 2020 2020 2020    ).            
-0001d940: 290a 0a20 2020 2020 2020 2072 6574 7572  )..        retur
-0001d950: 6e20 580a 0a20 2020 2064 6566 205f 6d6f  n X..    def _mo
-0001d960: 7265 5f74 6167 7328 7365 6c66 293a 0a20  re_tags(self):. 
-0001d970: 2020 2020 2020 2072 6574 7572 6e20 7b22         return {"
-0001d980: 616c 6c6f 775f 6e61 6e22 3a20 5472 7565  allow_nan": True
-0001d990: 7d0a 0a0a 4076 616c 6964 6174 655f 7061  }...@validate_pa
-0001d9a0: 7261 6d73 280a 2020 2020 7b22 5822 3a20  rams(.    {"X": 
-0001d9b0: 5b22 6172 7261 792d 6c69 6b65 225d 7d2c  ["array-like"]},
-0001d9c0: 0a20 2020 2070 7265 6665 725f 736b 6970  .    prefer_skip
-0001d9d0: 5f6e 6573 7465 645f 7661 6c69 6461 7469  _nested_validati
-0001d9e0: 6f6e 3d46 616c 7365 2c0a 290a 6465 6620  on=False,.).def 
-0001d9f0: 706f 7765 725f 7472 616e 7366 6f72 6d28  power_transform(
-0001da00: 582c 206d 6574 686f 643d 2279 656f 2d6a  X, method="yeo-j
-0001da10: 6f68 6e73 6f6e 222c 202a 2c20 7374 616e  ohnson", *, stan
-0001da20: 6461 7264 697a 653d 5472 7565 2c20 636f  dardize=True, co
-0001da30: 7079 3d54 7275 6529 3a0a 2020 2020 2222  py=True):.    ""
-0001da40: 2250 6172 616d 6574 7269 632c 206d 6f6e  "Parametric, mon
-0001da50: 6f74 6f6e 6963 2074 7261 6e73 666f 726d  otonic transform
-0001da60: 6174 696f 6e20 746f 206d 616b 6520 6461  ation to make da
-0001da70: 7461 206d 6f72 6520 4761 7573 7369 616e  ta more Gaussian
-0001da80: 2d6c 696b 652e 0a0a 2020 2020 506f 7765  -like...    Powe
-0001da90: 7220 7472 616e 7366 6f72 6d73 2061 7265  r transforms are
-0001daa0: 2061 2066 616d 696c 7920 6f66 2070 6172   a family of par
-0001dab0: 616d 6574 7269 632c 206d 6f6e 6f74 6f6e  ametric, monoton
-0001dac0: 6963 2074 7261 6e73 666f 726d 6174 696f  ic transformatio
-0001dad0: 6e73 0a20 2020 2074 6861 7420 6172 6520  ns.    that are 
-0001dae0: 6170 706c 6965 6420 746f 206d 616b 6520  applied to make 
-0001daf0: 6461 7461 206d 6f72 6520 4761 7573 7369  data more Gaussi
-0001db00: 616e 2d6c 696b 652e 2054 6869 7320 6973  an-like. This is
-0001db10: 2075 7365 6675 6c20 666f 720a 2020 2020   useful for.    
-0001db20: 6d6f 6465 6c69 6e67 2069 7373 7565 7320  modeling issues 
-0001db30: 7265 6c61 7465 6420 746f 2068 6574 6572  related to heter
-0001db40: 6f73 6365 6461 7374 6963 6974 7920 286e  oscedasticity (n
-0001db50: 6f6e 2d63 6f6e 7374 616e 7420 7661 7269  on-constant vari
-0001db60: 616e 6365 292c 0a20 2020 206f 7220 6f74  ance),.    or ot
-0001db70: 6865 7220 7369 7475 6174 696f 6e73 2077  her situations w
-0001db80: 6865 7265 206e 6f72 6d61 6c69 7479 2069  here normality i
-0001db90: 7320 6465 7369 7265 642e 0a0a 2020 2020  s desired...    
-0001dba0: 4375 7272 656e 746c 792c 2070 6f77 6572  Currently, power
-0001dbb0: 5f74 7261 6e73 666f 726d 2073 7570 706f  _transform suppo
-0001dbc0: 7274 7320 7468 6520 426f 782d 436f 7820  rts the Box-Cox 
-0001dbd0: 7472 616e 7366 6f72 6d20 616e 6420 7468  transform and th
-0001dbe0: 650a 2020 2020 5965 6f2d 4a6f 686e 736f  e.    Yeo-Johnso
-0001dbf0: 6e20 7472 616e 7366 6f72 6d2e 2054 6865  n transform. The
-0001dc00: 206f 7074 696d 616c 2070 6172 616d 6574   optimal paramet
-0001dc10: 6572 2066 6f72 2073 7461 6269 6c69 7a69  er for stabilizi
-0001dc20: 6e67 2076 6172 6961 6e63 6520 616e 640a  ng variance and.
-0001dc30: 2020 2020 6d69 6e69 6d69 7a69 6e67 2073      minimizing s
-0001dc40: 6b65 776e 6573 7320 6973 2065 7374 696d  kewness is estim
-0001dc50: 6174 6564 2074 6872 6f75 6768 206d 6178  ated through max
-0001dc60: 696d 756d 206c 696b 656c 6968 6f6f 642e  imum likelihood.
-0001dc70: 0a0a 2020 2020 426f 782d 436f 7820 7265  ..    Box-Cox re
-0001dc80: 7175 6972 6573 2069 6e70 7574 2064 6174  quires input dat
-0001dc90: 6120 746f 2062 6520 7374 7269 6374 6c79  a to be strictly
-0001dca0: 2070 6f73 6974 6976 652c 2077 6869 6c65   positive, while
-0001dcb0: 2059 656f 2d4a 6f68 6e73 6f6e 0a20 2020   Yeo-Johnson.   
-0001dcc0: 2073 7570 706f 7274 7320 626f 7468 2070   supports both p
-0001dcd0: 6f73 6974 6976 6520 6f72 206e 6567 6174  ositive or negat
-0001dce0: 6976 6520 6461 7461 2e0a 0a20 2020 2042  ive data...    B
-0001dcf0: 7920 6465 6661 756c 742c 207a 6572 6f2d  y default, zero-
-0001dd00: 6d65 616e 2c20 756e 6974 2d76 6172 6961  mean, unit-varia
-0001dd10: 6e63 6520 6e6f 726d 616c 697a 6174 696f  nce normalizatio
-0001dd20: 6e20 6973 2061 7070 6c69 6564 2074 6f20  n is applied to 
-0001dd30: 7468 650a 2020 2020 7472 616e 7366 6f72  the.    transfor
-0001dd40: 6d65 6420 6461 7461 2e0a 0a20 2020 2052  med data...    R
-0001dd50: 6561 6420 6d6f 7265 2069 6e20 7468 6520  ead more in the 
-0001dd60: 3a72 6566 3a60 5573 6572 2047 7569 6465  :ref:`User Guide
-0001dd70: 203c 7072 6570 726f 6365 7373 696e 675f   <preprocessing_
-0001dd80: 7472 616e 7366 6f72 6d65 723e 602e 0a0a  transformer>`...
-0001dd90: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
-0001dda0: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
-0001ddb0: 2020 5820 3a20 6172 7261 792d 6c69 6b65    X : array-like
-0001ddc0: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
-0001ddd0: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
-0001dde0: 290a 2020 2020 2020 2020 5468 6520 6461  ).        The da
-0001ddf0: 7461 2074 6f20 6265 2074 7261 6e73 666f  ta to be transfo
-0001de00: 726d 6564 2075 7369 6e67 2061 2070 6f77  rmed using a pow
-0001de10: 6572 2074 7261 6e73 666f 726d 6174 696f  er transformatio
-0001de20: 6e2e 0a0a 2020 2020 6d65 7468 6f64 203a  n...    method :
-0001de30: 207b 2779 656f 2d6a 6f68 6e73 6f6e 272c   {'yeo-johnson',
-0001de40: 2027 626f 782d 636f 7827 7d2c 2064 6566   'box-cox'}, def
-0001de50: 6175 6c74 3d27 7965 6f2d 6a6f 686e 736f  ault='yeo-johnso
-0001de60: 6e27 0a20 2020 2020 2020 2054 6865 2070  n'.        The p
-0001de70: 6f77 6572 2074 7261 6e73 666f 726d 206d  ower transform m
-0001de80: 6574 686f 642e 2041 7661 696c 6162 6c65  ethod. Available
-0001de90: 206d 6574 686f 6473 2061 7265 3a0a 0a20   methods are:.. 
-0001dea0: 2020 2020 2020 202d 2027 7965 6f2d 6a6f         - 'yeo-jo
-0001deb0: 686e 736f 6e27 205b 315d 5f2c 2077 6f72  hnson' [1]_, wor
-0001dec0: 6b73 2077 6974 6820 706f 7369 7469 7665  ks with positive
-0001ded0: 2061 6e64 206e 6567 6174 6976 6520 7661   and negative va
-0001dee0: 6c75 6573 0a20 2020 2020 2020 202d 2027  lues.        - '
-0001def0: 626f 782d 636f 7827 205b 325d 5f2c 206f  box-cox' [2]_, o
-0001df00: 6e6c 7920 776f 726b 7320 7769 7468 2073  nly works with s
-0001df10: 7472 6963 746c 7920 706f 7369 7469 7665  trictly positive
-0001df20: 2076 616c 7565 730a 0a20 2020 2020 2020   values..       
-0001df30: 202e 2e20 7665 7273 696f 6e63 6861 6e67   .. versionchang
-0001df40: 6564 3a3a 2030 2e32 330a 2020 2020 2020  ed:: 0.23.      
-0001df50: 2020 2020 2020 5468 6520 6465 6661 756c        The defaul
-0001df60: 7420 7661 6c75 6520 6f66 2074 6865 2060  t value of the `
-0001df70: 6d65 7468 6f64 6020 7061 7261 6d65 7465  method` paramete
-0001df80: 7220 6368 616e 6765 6420 6672 6f6d 0a20  r changed from. 
-0001df90: 2020 2020 2020 2020 2020 2027 626f 782d             'box-
-0001dfa0: 636f 7827 2074 6f20 2779 656f 2d6a 6f68  cox' to 'yeo-joh
-0001dfb0: 6e73 6f6e 2720 696e 2030 2e32 332e 0a0a  nson' in 0.23...
-0001dfc0: 2020 2020 7374 616e 6461 7264 697a 6520      standardize 
-0001dfd0: 3a20 626f 6f6c 2c20 6465 6661 756c 743d  : bool, default=
-0001dfe0: 5472 7565 0a20 2020 2020 2020 2053 6574  True.        Set
-0001dff0: 2074 6f20 5472 7565 2074 6f20 6170 706c   to True to appl
-0001e000: 7920 7a65 726f 2d6d 6561 6e2c 2075 6e69  y zero-mean, uni
-0001e010: 742d 7661 7269 616e 6365 206e 6f72 6d61  t-variance norma
-0001e020: 6c69 7a61 7469 6f6e 2074 6f20 7468 650a  lization to the.
-0001e030: 2020 2020 2020 2020 7472 616e 7366 6f72          transfor
-0001e040: 6d65 6420 6f75 7470 7574 2e0a 0a20 2020  med output...   
-0001e050: 2063 6f70 7920 3a20 626f 6f6c 2c20 6465   copy : bool, de
-0001e060: 6661 756c 743d 5472 7565 0a20 2020 2020  fault=True.     
-0001e070: 2020 2049 6620 4661 6c73 652c 2074 7279     If False, try
-0001e080: 2074 6f20 6176 6f69 6420 6120 636f 7079   to avoid a copy
-0001e090: 2061 6e64 2074 7261 6e73 666f 726d 2069   and transform i
-0001e0a0: 6e20 706c 6163 652e 0a20 2020 2020 2020  n place..       
-0001e0b0: 2054 6869 7320 6973 206e 6f74 2067 7561   This is not gua
-0001e0c0: 7261 6e74 6565 6420 746f 2061 6c77 6179  ranteed to alway
-0001e0d0: 7320 776f 726b 2069 6e20 706c 6163 653b  s work in place;
-0001e0e0: 2065 2e67 2e20 6966 2074 6865 2064 6174   e.g. if the dat
-0001e0f0: 6120 6973 0a20 2020 2020 2020 2061 206e  a is.        a n
-0001e100: 756d 7079 2061 7272 6179 2077 6974 6820  umpy array with 
-0001e110: 616e 2069 6e74 2064 7479 7065 2c20 6120  an int dtype, a 
-0001e120: 636f 7079 2077 696c 6c20 6265 2072 6574  copy will be ret
-0001e130: 7572 6e65 6420 6576 656e 2077 6974 680a  urned even with.
-0001e140: 2020 2020 2020 2020 636f 7079 3d46 616c          copy=Fal
-0001e150: 7365 2e0a 0a20 2020 2052 6574 7572 6e73  se...    Returns
-0001e160: 0a20 2020 202d 2d2d 2d2d 2d2d 0a20 2020  .    -------.   
-0001e170: 2058 5f74 7261 6e73 203a 206e 6461 7272   X_trans : ndarr
-0001e180: 6179 206f 6620 7368 6170 6520 286e 5f73  ay of shape (n_s
-0001e190: 616d 706c 6573 2c20 6e5f 6665 6174 7572  amples, n_featur
-0001e1a0: 6573 290a 2020 2020 2020 2020 5468 6520  es).        The 
-0001e1b0: 7472 616e 7366 6f72 6d65 6420 6461 7461  transformed data
-0001e1c0: 2e0a 0a20 2020 2053 6565 2041 6c73 6f0a  ...    See Also.
-0001e1d0: 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020      --------.   
-0001e1e0: 2050 6f77 6572 5472 616e 7366 6f72 6d65   PowerTransforme
-0001e1f0: 7220 3a20 4571 7569 7661 6c65 6e74 2074  r : Equivalent t
-0001e200: 7261 6e73 666f 726d 6174 696f 6e20 7769  ransformation wi
-0001e210: 7468 2074 6865 0a20 2020 2020 2020 2054  th the.        T
-0001e220: 7261 6e73 666f 726d 6572 2041 5049 2028  ransformer API (
-0001e230: 652e 672e 2061 7320 7061 7274 206f 6620  e.g. as part of 
-0001e240: 6120 7072 6570 726f 6365 7373 696e 670a  a preprocessing.
-0001e250: 2020 2020 2020 2020 3a63 6c61 7373 3a60          :class:`
-0001e260: 7e73 6b6c 6561 726e 2e70 6970 656c 696e  ~sklearn.pipelin
-0001e270: 652e 5069 7065 6c69 6e65 6029 2e0a 0a20  e.Pipeline`)... 
-0001e280: 2020 2071 7561 6e74 696c 655f 7472 616e     quantile_tran
-0001e290: 7366 6f72 6d20 3a20 4d61 7073 2064 6174  sform : Maps dat
-0001e2a0: 6120 746f 2061 2073 7461 6e64 6172 6420  a to a standard 
-0001e2b0: 6e6f 726d 616c 2064 6973 7472 6962 7574  normal distribut
-0001e2c0: 696f 6e20 7769 7468 0a20 2020 2020 2020  ion with.       
-0001e2d0: 2074 6865 2070 6172 616d 6574 6572 2060   the parameter `
-0001e2e0: 6f75 7470 7574 5f64 6973 7472 6962 7574  output_distribut
-0001e2f0: 696f 6e3d 276e 6f72 6d61 6c27 602e 0a0a  ion='normal'`...
-0001e300: 2020 2020 4e6f 7465 730a 2020 2020 2d2d      Notes.    --
-0001e310: 2d2d 2d0a 2020 2020 4e61 4e73 2061 7265  ---.    NaNs are
-0001e320: 2074 7265 6174 6564 2061 7320 6d69 7373   treated as miss
-0001e330: 696e 6720 7661 6c75 6573 3a20 6469 7372  ing values: disr
-0001e340: 6567 6172 6465 6420 696e 2060 6066 6974  egarded in ``fit
-0001e350: 6060 2c20 616e 6420 6d61 696e 7461 696e  ``, and maintain
-0001e360: 6564 0a20 2020 2069 6e20 6060 7472 616e  ed.    in ``tran
-0001e370: 7366 6f72 6d60 602e 0a0a 2020 2020 466f  sform``...    Fo
-0001e380: 7220 6120 636f 6d70 6172 6973 6f6e 206f  r a comparison o
-0001e390: 6620 7468 6520 6469 6666 6572 656e 7420  f the different 
-0001e3a0: 7363 616c 6572 732c 2074 7261 6e73 666f  scalers, transfo
-0001e3b0: 726d 6572 732c 2061 6e64 206e 6f72 6d61  rmers, and norma
-0001e3c0: 6c69 7a65 7273 2c0a 2020 2020 7365 653a  lizers,.    see:
-0001e3d0: 203a 7265 663a 6073 7068 785f 676c 725f   :ref:`sphx_glr_
-0001e3e0: 6175 746f 5f65 7861 6d70 6c65 735f 7072  auto_examples_pr
-0001e3f0: 6570 726f 6365 7373 696e 675f 706c 6f74  eprocessing_plot
-0001e400: 5f61 6c6c 5f73 6361 6c69 6e67 2e70 7960  _all_scaling.py`
-0001e410: 2e0a 0a20 2020 2052 6566 6572 656e 6365  ...    Reference
-0001e420: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
-0001e430: 0a0a 2020 2020 2e2e 205b 315d 2049 2e4b  ..    .. [1] I.K
-0001e440: 2e20 5965 6f20 616e 6420 522e 412e 204a  . Yeo and R.A. J
-0001e450: 6f68 6e73 6f6e 2c20 2241 206e 6577 2066  ohnson, "A new f
-0001e460: 616d 696c 7920 6f66 2070 6f77 6572 2074  amily of power t
-0001e470: 7261 6e73 666f 726d 6174 696f 6e73 2074  ransformations t
-0001e480: 6f0a 2020 2020 2020 2020 2020 2069 6d70  o.           imp
-0001e490: 726f 7665 206e 6f72 6d61 6c69 7479 206f  rove normality o
-0001e4a0: 7220 7379 6d6d 6574 7279 2e22 2042 696f  r symmetry." Bio
-0001e4b0: 6d65 7472 696b 612c 2038 3728 3429 2c20  metrika, 87(4), 
-0001e4c0: 7070 2e39 3534 2d39 3539 2c0a 2020 2020  pp.954-959,.    
-0001e4d0: 2020 2020 2020 2028 3230 3030 292e 0a0a         (2000)...
-0001e4e0: 2020 2020 2e2e 205b 325d 2047 2e45 2e50      .. [2] G.E.P
-0001e4f0: 2e20 426f 7820 616e 6420 442e 522e 2043  . Box and D.R. C
-0001e500: 6f78 2c20 2241 6e20 416e 616c 7973 6973  ox, "An Analysis
-0001e510: 206f 6620 5472 616e 7366 6f72 6d61 7469   of Transformati
-0001e520: 6f6e 7322 2c20 4a6f 7572 6e61 6c0a 2020  ons", Journal.  
-0001e530: 2020 2020 2020 2020 206f 6620 7468 6520           of the 
-0001e540: 526f 7961 6c20 5374 6174 6973 7469 6361  Royal Statistica
-0001e550: 6c20 536f 6369 6574 7920 422c 2032 362c  l Society B, 26,
-0001e560: 2032 3131 2d32 3532 2028 3139 3634 292e   211-252 (1964).
-0001e570: 0a0a 2020 2020 4578 616d 706c 6573 0a20  ..    Examples. 
-0001e580: 2020 202d 2d2d 2d2d 2d2d 2d0a 2020 2020     --------.    
-0001e590: 3e3e 3e20 696d 706f 7274 206e 756d 7079  >>> import numpy
-0001e5a0: 2061 7320 6e70 0a20 2020 203e 3e3e 2066   as np.    >>> f
-0001e5b0: 726f 6d20 736b 6c65 6172 6e2e 7072 6570  rom sklearn.prep
-0001e5c0: 726f 6365 7373 696e 6720 696d 706f 7274  rocessing import
-0001e5d0: 2070 6f77 6572 5f74 7261 6e73 666f 726d   power_transform
-0001e5e0: 0a20 2020 203e 3e3e 2064 6174 6120 3d20  .    >>> data = 
-0001e5f0: 5b5b 312c 2032 5d2c 205b 332c 2032 5d2c  [[1, 2], [3, 2],
-0001e600: 205b 342c 2035 5d5d 0a20 2020 203e 3e3e   [4, 5]].    >>>
-0001e610: 2070 7269 6e74 2870 6f77 6572 5f74 7261   print(power_tra
-0001e620: 6e73 666f 726d 2864 6174 612c 206d 6574  nsform(data, met
-0001e630: 686f 643d 2762 6f78 2d63 6f78 2729 290a  hod='box-cox')).
-0001e640: 2020 2020 5b5b 2d31 2e33 3332 2e2e 2e20      [[-1.332... 
-0001e650: 2d30 2e37 3037 2e2e 2e5d 0a20 2020 2020  -0.707...].     
-0001e660: 5b20 302e 3235 362e 2e2e 202d 302e 3730  [ 0.256... -0.70
-0001e670: 372e 2e2e 5d0a 2020 2020 205b 2031 2e30  7...].     [ 1.0
-0001e680: 3736 2e2e 2e20 2031 2e34 3134 2e2e 2e5d  76...  1.414...]
-0001e690: 5d0a 0a20 2020 202e 2e20 7761 726e 696e  ]..    .. warnin
-0001e6a0: 673a 3a20 5269 736b 206f 6620 6461 7461  g:: Risk of data
-0001e6b0: 206c 6561 6b2e 0a20 2020 2020 2020 2044   leak..        D
-0001e6c0: 6f20 6e6f 7420 7573 6520 3a66 756e 633a  o not use :func:
-0001e6d0: 607e 736b 6c65 6172 6e2e 7072 6570 726f  `~sklearn.prepro
-0001e6e0: 6365 7373 696e 672e 706f 7765 725f 7472  cessing.power_tr
-0001e6f0: 616e 7366 6f72 6d60 2075 6e6c 6573 7320  ansform` unless 
-0001e700: 796f 750a 2020 2020 2020 2020 6b6e 6f77  you.        know
-0001e710: 2077 6861 7420 796f 7520 6172 6520 646f   what you are do
-0001e720: 696e 672e 2041 2063 6f6d 6d6f 6e20 6d69  ing. A common mi
-0001e730: 7374 616b 6520 6973 2074 6f20 6170 706c  stake is to appl
-0001e740: 7920 6974 2074 6f20 7468 6520 656e 7469  y it to the enti
-0001e750: 7265 0a20 2020 2020 2020 2064 6174 6120  re.        data 
-0001e760: 2a62 6566 6f72 652a 2073 706c 6974 7469  *before* splitti
-0001e770: 6e67 2069 6e74 6f20 7472 6169 6e69 6e67  ng into training
-0001e780: 2061 6e64 2074 6573 7420 7365 7473 2e20   and test sets. 
-0001e790: 5468 6973 2077 696c 6c20 6269 6173 2074  This will bias t
-0001e7a0: 6865 0a20 2020 2020 2020 206d 6f64 656c  he.        model
-0001e7b0: 2065 7661 6c75 6174 696f 6e20 6265 6361   evaluation beca
-0001e7c0: 7573 6520 696e 666f 726d 6174 696f 6e20  use information 
-0001e7d0: 776f 756c 6420 6861 7665 206c 6561 6b65  would have leake
-0001e7e0: 6420 6672 6f6d 2074 6865 2074 6573 740a  d from the test.
-0001e7f0: 2020 2020 2020 2020 7365 7420 746f 2074          set to t
-0001e800: 6865 2074 7261 696e 696e 6720 7365 742e  he training set.
-0001e810: 0a20 2020 2020 2020 2049 6e20 6765 6e65  .        In gene
-0001e820: 7261 6c2c 2077 6520 7265 636f 6d6d 656e  ral, we recommen
-0001e830: 6420 7573 696e 670a 2020 2020 2020 2020  d using.        
-0001e840: 3a63 6c61 7373 3a60 7e73 6b6c 6561 726e  :class:`~sklearn
-0001e850: 2e70 7265 7072 6f63 6573 7369 6e67 2e50  .preprocessing.P
-0001e860: 6f77 6572 5472 616e 7366 6f72 6d65 7260  owerTransformer`
-0001e870: 2077 6974 6869 6e20 610a 2020 2020 2020   within a.      
-0001e880: 2020 3a72 6566 3a60 5069 7065 6c69 6e65    :ref:`Pipeline
-0001e890: 203c 7069 7065 6c69 6e65 3e60 2069 6e20   <pipeline>` in 
-0001e8a0: 6f72 6465 7220 746f 2070 7265 7665 6e74  order to prevent
-0001e8b0: 206d 6f73 7420 7269 736b 7320 6f66 2064   most risks of d
-0001e8c0: 6174 610a 2020 2020 2020 2020 6c65 616b  ata.        leak
-0001e8d0: 696e 672c 2065 2e67 2e3a 2060 7069 7065  ing, e.g.: `pipe
-0001e8e0: 203d 206d 616b 655f 7069 7065 6c69 6e65   = make_pipeline
-0001e8f0: 2850 6f77 6572 5472 616e 7366 6f72 6d65  (PowerTransforme
-0001e900: 7228 292c 0a20 2020 2020 2020 204c 6f67  r(),.        Log
-0001e910: 6973 7469 6352 6567 7265 7373 696f 6e28  isticRegression(
-0001e920: 2929 602e 0a20 2020 2022 2222 0a20 2020  ))`..    """.   
-0001e930: 2070 7420 3d20 506f 7765 7254 7261 6e73   pt = PowerTrans
-0001e940: 666f 726d 6572 286d 6574 686f 643d 6d65  former(method=me
-0001e950: 7468 6f64 2c20 7374 616e 6461 7264 697a  thod, standardiz
-0001e960: 653d 7374 616e 6461 7264 697a 652c 2063  e=standardize, c
-0001e970: 6f70 793d 636f 7079 290a 2020 2020 7265  opy=copy).    re
-0001e980: 7475 726e 2070 742e 6669 745f 7472 616e  turn pt.fit_tran
-0001e990: 7366 6f72 6d28 5829 0a                   sform(X).
+0000cb10: 202d 322e 2c20 2031 2e2c 2020 332e 5d2c   -2.,  1.,  3.],
+0000cb20: 0a20 2020 202e 2e2e 2020 2020 2020 5b20  .    ...      [ 
+0000cb30: 342e 2c20 2031 2e2c 202d 322e 5d5d 0a20  4.,  1., -2.]]. 
+0000cb40: 2020 203e 3e3e 2074 7261 6e73 666f 726d     >>> transform
+0000cb50: 6572 203d 2052 6f62 7573 7453 6361 6c65  er = RobustScale
+0000cb60: 7228 292e 6669 7428 5829 0a20 2020 203e  r().fit(X).    >
+0000cb70: 3e3e 2074 7261 6e73 666f 726d 6572 0a20  >> transformer. 
+0000cb80: 2020 2052 6f62 7573 7453 6361 6c65 7228     RobustScaler(
+0000cb90: 290a 2020 2020 3e3e 3e20 7472 616e 7366  ).    >>> transf
+0000cba0: 6f72 6d65 722e 7472 616e 7366 6f72 6d28  ormer.transform(
+0000cbb0: 5829 0a20 2020 2061 7272 6179 285b 5b20  X).    array([[ 
+0000cbc0: 302e 202c 202d 322e 202c 2020 302e 205d  0. , -2. ,  0. ]
+0000cbd0: 2c0a 2020 2020 2020 2020 2020 205b 2d31  ,.           [-1
+0000cbe0: 2e20 2c20 2030 2e20 2c20 2030 2e34 5d2c  . ,  0. ,  0.4],
+0000cbf0: 0a20 2020 2020 2020 2020 2020 5b20 312e  .           [ 1.
+0000cc00: 202c 2020 302e 202c 202d 312e 365d 5d29   ,  0. , -1.6]])
+0000cc10: 0a20 2020 2022 2222 0a0a 2020 2020 5f70  .    """..    _p
+0000cc20: 6172 616d 6574 6572 5f63 6f6e 7374 7261  arameter_constra
+0000cc30: 696e 7473 3a20 6469 6374 203d 207b 0a20  ints: dict = {. 
+0000cc40: 2020 2020 2020 2022 7769 7468 5f63 656e         "with_cen
+0000cc50: 7465 7269 6e67 223a 205b 2262 6f6f 6c65  tering": ["boole
+0000cc60: 616e 225d 2c0a 2020 2020 2020 2020 2277  an"],.        "w
+0000cc70: 6974 685f 7363 616c 696e 6722 3a20 5b22  ith_scaling": ["
+0000cc80: 626f 6f6c 6561 6e22 5d2c 0a20 2020 2020  boolean"],.     
+0000cc90: 2020 2022 7175 616e 7469 6c65 5f72 616e     "quantile_ran
+0000cca0: 6765 223a 205b 7475 706c 655d 2c0a 2020  ge": [tuple],.  
+0000ccb0: 2020 2020 2020 2263 6f70 7922 3a20 5b22        "copy": ["
+0000ccc0: 626f 6f6c 6561 6e22 5d2c 0a20 2020 2020  boolean"],.     
+0000ccd0: 2020 2022 756e 6974 5f76 6172 6961 6e63     "unit_varianc
+0000cce0: 6522 3a20 5b22 626f 6f6c 6561 6e22 5d2c  e": ["boolean"],
+0000ccf0: 0a20 2020 207d 0a0a 2020 2020 6465 6620  .    }..    def 
+0000cd00: 5f5f 696e 6974 5f5f 280a 2020 2020 2020  __init__(.      
+0000cd10: 2020 7365 6c66 2c0a 2020 2020 2020 2020    self,.        
+0000cd20: 2a2c 0a20 2020 2020 2020 2077 6974 685f  *,.        with_
+0000cd30: 6365 6e74 6572 696e 673d 5472 7565 2c0a  centering=True,.
+0000cd40: 2020 2020 2020 2020 7769 7468 5f73 6361          with_sca
+0000cd50: 6c69 6e67 3d54 7275 652c 0a20 2020 2020  ling=True,.     
+0000cd60: 2020 2071 7561 6e74 696c 655f 7261 6e67     quantile_rang
+0000cd70: 653d 2832 352e 302c 2037 352e 3029 2c0a  e=(25.0, 75.0),.
+0000cd80: 2020 2020 2020 2020 636f 7079 3d54 7275          copy=Tru
+0000cd90: 652c 0a20 2020 2020 2020 2075 6e69 745f  e,.        unit_
+0000cda0: 7661 7269 616e 6365 3d46 616c 7365 2c0a  variance=False,.
+0000cdb0: 2020 2020 293a 0a20 2020 2020 2020 2073      ):.        s
+0000cdc0: 656c 662e 7769 7468 5f63 656e 7465 7269  elf.with_centeri
+0000cdd0: 6e67 203d 2077 6974 685f 6365 6e74 6572  ng = with_center
+0000cde0: 696e 670a 2020 2020 2020 2020 7365 6c66  ing.        self
+0000cdf0: 2e77 6974 685f 7363 616c 696e 6720 3d20  .with_scaling = 
+0000ce00: 7769 7468 5f73 6361 6c69 6e67 0a20 2020  with_scaling.   
+0000ce10: 2020 2020 2073 656c 662e 7175 616e 7469       self.quanti
+0000ce20: 6c65 5f72 616e 6765 203d 2071 7561 6e74  le_range = quant
+0000ce30: 696c 655f 7261 6e67 650a 2020 2020 2020  ile_range.      
+0000ce40: 2020 7365 6c66 2e75 6e69 745f 7661 7269    self.unit_vari
+0000ce50: 616e 6365 203d 2075 6e69 745f 7661 7269  ance = unit_vari
+0000ce60: 616e 6365 0a20 2020 2020 2020 2073 656c  ance.        sel
+0000ce70: 662e 636f 7079 203d 2063 6f70 790a 0a20  f.copy = copy.. 
+0000ce80: 2020 2040 5f66 6974 5f63 6f6e 7465 7874     @_fit_context
+0000ce90: 2870 7265 6665 725f 736b 6970 5f6e 6573  (prefer_skip_nes
+0000cea0: 7465 645f 7661 6c69 6461 7469 6f6e 3d54  ted_validation=T
+0000ceb0: 7275 6529 0a20 2020 2064 6566 2066 6974  rue).    def fit
+0000cec0: 2873 656c 662c 2058 2c20 793d 4e6f 6e65  (self, X, y=None
+0000ced0: 293a 0a20 2020 2020 2020 2022 2222 436f  ):.        """Co
+0000cee0: 6d70 7574 6520 7468 6520 6d65 6469 616e  mpute the median
+0000cef0: 2061 6e64 2071 7561 6e74 696c 6573 2074   and quantiles t
+0000cf00: 6f20 6265 2075 7365 6420 666f 7220 7363  o be used for sc
+0000cf10: 616c 696e 672e 0a0a 2020 2020 2020 2020  aling...        
+0000cf20: 5061 7261 6d65 7465 7273 0a20 2020 2020  Parameters.     
+0000cf30: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
+0000cf40: 2020 2020 2020 5820 3a20 7b61 7272 6179        X : {array
+0000cf50: 2d6c 696b 652c 2073 7061 7273 6520 6d61  -like, sparse ma
+0000cf60: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
+0000cf70: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+0000cf80: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
+0000cf90: 2020 2054 6865 2064 6174 6120 7573 6564     The data used
+0000cfa0: 2074 6f20 636f 6d70 7574 6520 7468 6520   to compute the 
+0000cfb0: 6d65 6469 616e 2061 6e64 2071 7561 6e74  median and quant
+0000cfc0: 696c 6573 0a20 2020 2020 2020 2020 2020  iles.           
+0000cfd0: 2075 7365 6420 666f 7220 6c61 7465 7220   used for later 
+0000cfe0: 7363 616c 696e 6720 616c 6f6e 6720 7468  scaling along th
+0000cff0: 6520 6665 6174 7572 6573 2061 7869 732e  e features axis.
+0000d000: 0a0a 2020 2020 2020 2020 7920 3a20 4967  ..        y : Ig
+0000d010: 6e6f 7265 640a 2020 2020 2020 2020 2020  nored.          
+0000d020: 2020 4e6f 7420 7573 6564 2c20 7072 6573    Not used, pres
+0000d030: 656e 7420 6865 7265 2066 6f72 2041 5049  ent here for API
+0000d040: 2063 6f6e 7369 7374 656e 6379 2062 7920   consistency by 
+0000d050: 636f 6e76 656e 7469 6f6e 2e0a 0a20 2020  convention...   
+0000d060: 2020 2020 2052 6574 7572 6e73 0a20 2020       Returns.   
+0000d070: 2020 2020 202d 2d2d 2d2d 2d2d 0a20 2020       -------.   
+0000d080: 2020 2020 2073 656c 6620 3a20 6f62 6a65       self : obje
+0000d090: 6374 0a20 2020 2020 2020 2020 2020 2046  ct.            F
+0000d0a0: 6974 7465 6420 7363 616c 6572 2e0a 2020  itted scaler..  
+0000d0b0: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
+0000d0c0: 2020 2320 6174 2066 6974 2c20 636f 6e76    # at fit, conv
+0000d0d0: 6572 7420 7370 6172 7365 206d 6174 7269  ert sparse matri
+0000d0e0: 6365 7320 746f 2063 7363 2066 6f72 206f  ces to csc for o
+0000d0f0: 7074 696d 697a 6564 2063 6f6d 7075 7461  ptimized computa
+0000d100: 7469 6f6e 206f 660a 2020 2020 2020 2020  tion of.        
+0000d110: 2320 7468 6520 7175 616e 7469 6c65 730a  # the quantiles.
+0000d120: 2020 2020 2020 2020 5820 3d20 7365 6c66          X = self
+0000d130: 2e5f 7661 6c69 6461 7465 5f64 6174 6128  ._validate_data(
+0000d140: 0a20 2020 2020 2020 2020 2020 2058 2c0a  .            X,.
+0000d150: 2020 2020 2020 2020 2020 2020 6163 6365              acce
+0000d160: 7074 5f73 7061 7273 653d 2263 7363 222c  pt_sparse="csc",
+0000d170: 0a20 2020 2020 2020 2020 2020 2064 7479  .            dty
+0000d180: 7065 3d46 4c4f 4154 5f44 5459 5045 532c  pe=FLOAT_DTYPES,
+0000d190: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
+0000d1a0: 6365 5f61 6c6c 5f66 696e 6974 653d 2261  ce_all_finite="a
+0000d1b0: 6c6c 6f77 2d6e 616e 222c 0a20 2020 2020  llow-nan",.     
+0000d1c0: 2020 2029 0a0a 2020 2020 2020 2020 715f     )..        q_
+0000d1d0: 6d69 6e2c 2071 5f6d 6178 203d 2073 656c  min, q_max = sel
+0000d1e0: 662e 7175 616e 7469 6c65 5f72 616e 6765  f.quantile_range
+0000d1f0: 0a20 2020 2020 2020 2069 6620 6e6f 7420  .        if not 
+0000d200: 3020 3c3d 2071 5f6d 696e 203c 3d20 715f  0 <= q_min <= q_
+0000d210: 6d61 7820 3c3d 2031 3030 3a0a 2020 2020  max <= 100:.    
+0000d220: 2020 2020 2020 2020 7261 6973 6520 5661          raise Va
+0000d230: 6c75 6545 7272 6f72 2822 496e 7661 6c69  lueError("Invali
+0000d240: 6420 7175 616e 7469 6c65 2072 616e 6765  d quantile range
+0000d250: 3a20 2573 2220 2520 7374 7228 7365 6c66  : %s" % str(self
+0000d260: 2e71 7561 6e74 696c 655f 7261 6e67 6529  .quantile_range)
+0000d270: 290a 0a20 2020 2020 2020 2069 6620 7365  )..        if se
+0000d280: 6c66 2e77 6974 685f 6365 6e74 6572 696e  lf.with_centerin
+0000d290: 673a 0a20 2020 2020 2020 2020 2020 2069  g:.            i
+0000d2a0: 6620 7370 6172 7365 2e69 7373 7061 7273  f sparse.isspars
+0000d2b0: 6528 5829 3a0a 2020 2020 2020 2020 2020  e(X):.          
+0000d2c0: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+0000d2d0: 6545 7272 6f72 280a 2020 2020 2020 2020  eError(.        
+0000d2e0: 2020 2020 2020 2020 2020 2020 2243 616e              "Can
+0000d2f0: 6e6f 7420 6365 6e74 6572 2073 7061 7273  not center spars
+0000d300: 6520 6d61 7472 6963 6573 3a20 7573 6520  e matrices: use 
+0000d310: 6077 6974 685f 6365 6e74 6572 696e 673d  `with_centering=
+0000d320: 4661 6c73 6560 220a 2020 2020 2020 2020  False`".        
+0000d330: 2020 2020 2020 2020 2020 2020 2220 696e              " in
+0000d340: 7374 6561 642e 2053 6565 2064 6f63 7374  stead. See docst
+0000d350: 7269 6e67 2066 6f72 206d 6f74 6976 6174  ring for motivat
+0000d360: 696f 6e20 616e 6420 616c 7465 726e 6174  ion and alternat
+0000d370: 6976 6573 2e22 0a20 2020 2020 2020 2020  ives.".         
+0000d380: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+0000d390: 2020 2020 2073 656c 662e 6365 6e74 6572       self.center
+0000d3a0: 5f20 3d20 6e70 2e6e 616e 6d65 6469 616e  _ = np.nanmedian
+0000d3b0: 2858 2c20 6178 6973 3d30 290a 2020 2020  (X, axis=0).    
+0000d3c0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+0000d3d0: 2020 2020 2020 7365 6c66 2e63 656e 7465        self.cente
+0000d3e0: 725f 203d 204e 6f6e 650a 0a20 2020 2020  r_ = None..     
+0000d3f0: 2020 2069 6620 7365 6c66 2e77 6974 685f     if self.with_
+0000d400: 7363 616c 696e 673a 0a20 2020 2020 2020  scaling:.       
+0000d410: 2020 2020 2071 7561 6e74 696c 6573 203d       quantiles =
+0000d420: 205b 5d0a 2020 2020 2020 2020 2020 2020   [].            
+0000d430: 666f 7220 6665 6174 7572 655f 6964 7820  for feature_idx 
+0000d440: 696e 2072 616e 6765 2858 2e73 6861 7065  in range(X.shape
+0000d450: 5b31 5d29 3a0a 2020 2020 2020 2020 2020  [1]):.          
+0000d460: 2020 2020 2020 6966 2073 7061 7273 652e        if sparse.
+0000d470: 6973 7370 6172 7365 2858 293a 0a20 2020  issparse(X):.   
+0000d480: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d490: 2063 6f6c 756d 6e5f 6e6e 7a5f 6461 7461   column_nnz_data
+0000d4a0: 203d 2058 2e64 6174 615b 0a20 2020 2020   = X.data[.     
+0000d4b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0000d4c0: 2020 2058 2e69 6e64 7074 725b 6665 6174     X.indptr[feat
+0000d4d0: 7572 655f 6964 785d 203a 2058 2e69 6e64  ure_idx] : X.ind
+0000d4e0: 7074 725b 6665 6174 7572 655f 6964 7820  ptr[feature_idx 
+0000d4f0: 2b20 315d 0a20 2020 2020 2020 2020 2020  + 1].           
+0000d500: 2020 2020 2020 2020 205d 0a20 2020 2020           ].     
+0000d510: 2020 2020 2020 2020 2020 2020 2020 2063                 c
+0000d520: 6f6c 756d 6e5f 6461 7461 203d 206e 702e  olumn_data = np.
+0000d530: 7a65 726f 7328 7368 6170 653d 582e 7368  zeros(shape=X.sh
+0000d540: 6170 655b 305d 2c20 6474 7970 653d 582e  ape[0], dtype=X.
+0000d550: 6474 7970 6529 0a20 2020 2020 2020 2020  dtype).         
+0000d560: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
+0000d570: 6e5f 6461 7461 5b3a 206c 656e 2863 6f6c  n_data[: len(col
+0000d580: 756d 6e5f 6e6e 7a5f 6461 7461 295d 203d  umn_nnz_data)] =
+0000d590: 2063 6f6c 756d 6e5f 6e6e 7a5f 6461 7461   column_nnz_data
+0000d5a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0000d5b0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
+0000d5c0: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
+0000d5d0: 6e5f 6461 7461 203d 2058 5b3a 2c20 6665  n_data = X[:, fe
+0000d5e0: 6174 7572 655f 6964 785d 0a0a 2020 2020  ature_idx]..    
+0000d5f0: 2020 2020 2020 2020 2020 2020 7175 616e              quan
+0000d600: 7469 6c65 732e 6170 7065 6e64 286e 702e  tiles.append(np.
+0000d610: 6e61 6e70 6572 6365 6e74 696c 6528 636f  nanpercentile(co
+0000d620: 6c75 6d6e 5f64 6174 612c 2073 656c 662e  lumn_data, self.
+0000d630: 7175 616e 7469 6c65 5f72 616e 6765 2929  quantile_range))
+0000d640: 0a0a 2020 2020 2020 2020 2020 2020 7175  ..            qu
+0000d650: 616e 7469 6c65 7320 3d20 6e70 2e74 7261  antiles = np.tra
+0000d660: 6e73 706f 7365 2871 7561 6e74 696c 6573  nspose(quantiles
+0000d670: 290a 0a20 2020 2020 2020 2020 2020 2073  )..            s
+0000d680: 656c 662e 7363 616c 655f 203d 2071 7561  elf.scale_ = qua
+0000d690: 6e74 696c 6573 5b31 5d20 2d20 7175 616e  ntiles[1] - quan
+0000d6a0: 7469 6c65 735b 305d 0a20 2020 2020 2020  tiles[0].       
+0000d6b0: 2020 2020 2073 656c 662e 7363 616c 655f       self.scale_
+0000d6c0: 203d 205f 6861 6e64 6c65 5f7a 6572 6f73   = _handle_zeros
+0000d6d0: 5f69 6e5f 7363 616c 6528 7365 6c66 2e73  _in_scale(self.s
+0000d6e0: 6361 6c65 5f2c 2063 6f70 793d 4661 6c73  cale_, copy=Fals
+0000d6f0: 6529 0a20 2020 2020 2020 2020 2020 2069  e).            i
+0000d700: 6620 7365 6c66 2e75 6e69 745f 7661 7269  f self.unit_vari
+0000d710: 616e 6365 3a0a 2020 2020 2020 2020 2020  ance:.          
+0000d720: 2020 2020 2020 6164 6a75 7374 203d 2073        adjust = s
+0000d730: 7461 7473 2e6e 6f72 6d2e 7070 6628 715f  tats.norm.ppf(q_
+0000d740: 6d61 7820 2f20 3130 302e 3029 202d 2073  max / 100.0) - s
+0000d750: 7461 7473 2e6e 6f72 6d2e 7070 6628 715f  tats.norm.ppf(q_
+0000d760: 6d69 6e20 2f20 3130 302e 3029 0a20 2020  min / 100.0).   
+0000d770: 2020 2020 2020 2020 2020 2020 2073 656c               sel
+0000d780: 662e 7363 616c 655f 203d 2073 656c 662e  f.scale_ = self.
+0000d790: 7363 616c 655f 202f 2061 646a 7573 740a  scale_ / adjust.
+0000d7a0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+0000d7b0: 2020 2020 2020 2020 2020 7365 6c66 2e73            self.s
+0000d7c0: 6361 6c65 5f20 3d20 4e6f 6e65 0a0a 2020  cale_ = None..  
+0000d7d0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+0000d7e0: 660a 0a20 2020 2064 6566 2074 7261 6e73  f..    def trans
+0000d7f0: 666f 726d 2873 656c 662c 2058 293a 0a20  form(self, X):. 
+0000d800: 2020 2020 2020 2022 2222 4365 6e74 6572         """Center
+0000d810: 2061 6e64 2073 6361 6c65 2074 6865 2064   and scale the d
+0000d820: 6174 612e 0a0a 2020 2020 2020 2020 5061  ata...        Pa
+0000d830: 7261 6d65 7465 7273 0a20 2020 2020 2020  rameters.       
+0000d840: 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020   ----------.    
+0000d850: 2020 2020 5820 3a20 7b61 7272 6179 2d6c      X : {array-l
+0000d860: 696b 652c 2073 7061 7273 6520 6d61 7472  ike, sparse matr
+0000d870: 6978 7d20 6f66 2073 6861 7065 2028 6e5f  ix} of shape (n_
+0000d880: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
+0000d890: 7265 7329 0a20 2020 2020 2020 2020 2020  res).           
+0000d8a0: 2054 6865 2064 6174 6120 7573 6564 2074   The data used t
+0000d8b0: 6f20 7363 616c 6520 616c 6f6e 6720 7468  o scale along th
+0000d8c0: 6520 7370 6563 6966 6965 6420 6178 6973  e specified axis
+0000d8d0: 2e0a 0a20 2020 2020 2020 2052 6574 7572  ...        Retur
+0000d8e0: 6e73 0a20 2020 2020 2020 202d 2d2d 2d2d  ns.        -----
+0000d8f0: 2d2d 0a20 2020 2020 2020 2058 5f74 7220  --.        X_tr 
+0000d900: 3a20 7b6e 6461 7272 6179 2c20 7370 6172  : {ndarray, spar
+0000d910: 7365 206d 6174 7269 787d 206f 6620 7368  se matrix} of sh
+0000d920: 6170 6520 286e 5f73 616d 706c 6573 2c20  ape (n_samples, 
+0000d930: 6e5f 6665 6174 7572 6573 290a 2020 2020  n_features).    
+0000d940: 2020 2020 2020 2020 5472 616e 7366 6f72          Transfor
+0000d950: 6d65 6420 6172 7261 792e 0a20 2020 2020  med array..     
+0000d960: 2020 2022 2222 0a20 2020 2020 2020 2063     """.        c
+0000d970: 6865 636b 5f69 735f 6669 7474 6564 2873  heck_is_fitted(s
+0000d980: 656c 6629 0a20 2020 2020 2020 2058 203d  elf).        X =
+0000d990: 2073 656c 662e 5f76 616c 6964 6174 655f   self._validate_
+0000d9a0: 6461 7461 280a 2020 2020 2020 2020 2020  data(.          
+0000d9b0: 2020 582c 0a20 2020 2020 2020 2020 2020    X,.           
+0000d9c0: 2061 6363 6570 745f 7370 6172 7365 3d28   accept_sparse=(
+0000d9d0: 2263 7372 222c 2022 6373 6322 292c 0a20  "csr", "csc"),. 
+0000d9e0: 2020 2020 2020 2020 2020 2063 6f70 793d             copy=
+0000d9f0: 7365 6c66 2e63 6f70 792c 0a20 2020 2020  self.copy,.     
+0000da00: 2020 2020 2020 2064 7479 7065 3d46 4c4f         dtype=FLO
+0000da10: 4154 5f44 5459 5045 532c 0a20 2020 2020  AT_DTYPES,.     
+0000da20: 2020 2020 2020 2072 6573 6574 3d46 616c         reset=Fal
+0000da30: 7365 2c0a 2020 2020 2020 2020 2020 2020  se,.            
+0000da40: 666f 7263 655f 616c 6c5f 6669 6e69 7465  force_all_finite
+0000da50: 3d22 616c 6c6f 772d 6e61 6e22 2c0a 2020  ="allow-nan",.  
+0000da60: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       
+0000da70: 2069 6620 7370 6172 7365 2e69 7373 7061   if sparse.isspa
+0000da80: 7273 6528 5829 3a0a 2020 2020 2020 2020  rse(X):.        
+0000da90: 2020 2020 6966 2073 656c 662e 7769 7468      if self.with
+0000daa0: 5f73 6361 6c69 6e67 3a0a 2020 2020 2020  _scaling:.      
+0000dab0: 2020 2020 2020 2020 2020 696e 706c 6163            inplac
+0000dac0: 655f 636f 6c75 6d6e 5f73 6361 6c65 2858  e_column_scale(X
+0000dad0: 2c20 312e 3020 2f20 7365 6c66 2e73 6361  , 1.0 / self.sca
+0000dae0: 6c65 5f29 0a20 2020 2020 2020 2065 6c73  le_).        els
+0000daf0: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
+0000db00: 6620 7365 6c66 2e77 6974 685f 6365 6e74  f self.with_cent
+0000db10: 6572 696e 673a 0a20 2020 2020 2020 2020  ering:.         
+0000db20: 2020 2020 2020 2058 202d 3d20 7365 6c66         X -= self
+0000db30: 2e63 656e 7465 725f 0a20 2020 2020 2020  .center_.       
+0000db40: 2020 2020 2069 6620 7365 6c66 2e77 6974       if self.wit
+0000db50: 685f 7363 616c 696e 673a 0a20 2020 2020  h_scaling:.     
+0000db60: 2020 2020 2020 2020 2020 2058 202f 3d20             X /= 
+0000db70: 7365 6c66 2e73 6361 6c65 5f0a 2020 2020  self.scale_.    
+0000db80: 2020 2020 7265 7475 726e 2058 0a0a 2020      return X..  
+0000db90: 2020 6465 6620 696e 7665 7273 655f 7472    def inverse_tr
+0000dba0: 616e 7366 6f72 6d28 7365 6c66 2c20 5829  ansform(self, X)
+0000dbb0: 3a0a 2020 2020 2020 2020 2222 2253 6361  :.        """Sca
+0000dbc0: 6c65 2062 6163 6b20 7468 6520 6461 7461  le back the data
+0000dbd0: 2074 6f20 7468 6520 6f72 6967 696e 616c   to the original
+0000dbe0: 2072 6570 7265 7365 6e74 6174 696f 6e2e   representation.
+0000dbf0: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
+0000dc00: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
+0000dc10: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+0000dc20: 5820 3a20 7b61 7272 6179 2d6c 696b 652c  X : {array-like,
+0000dc30: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
+0000dc40: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
+0000dc50: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
+0000dc60: 0a20 2020 2020 2020 2020 2020 2054 6865  .            The
+0000dc70: 2072 6573 6361 6c65 6420 6461 7461 2074   rescaled data t
+0000dc80: 6f20 6265 2074 7261 6e73 666f 726d 6564  o be transformed
+0000dc90: 2062 6163 6b2e 0a0a 2020 2020 2020 2020   back...        
+0000dca0: 5265 7475 726e 730a 2020 2020 2020 2020  Returns.        
+0000dcb0: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+0000dcc0: 585f 7472 203a 207b 6e64 6172 7261 792c  X_tr : {ndarray,
+0000dcd0: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
+0000dce0: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
+0000dcf0: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
+0000dd00: 0a20 2020 2020 2020 2020 2020 2054 7261  .            Tra
+0000dd10: 6e73 666f 726d 6564 2061 7272 6179 2e0a  nsformed array..
+0000dd20: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
+0000dd30: 2020 2020 6368 6563 6b5f 6973 5f66 6974      check_is_fit
+0000dd40: 7465 6428 7365 6c66 290a 2020 2020 2020  ted(self).      
+0000dd50: 2020 5820 3d20 6368 6563 6b5f 6172 7261    X = check_arra
+0000dd60: 7928 0a20 2020 2020 2020 2020 2020 2058  y(.            X
+0000dd70: 2c0a 2020 2020 2020 2020 2020 2020 6163  ,.            ac
+0000dd80: 6365 7074 5f73 7061 7273 653d 2822 6373  cept_sparse=("cs
+0000dd90: 7222 2c20 2263 7363 2229 2c0a 2020 2020  r", "csc"),.    
+0000dda0: 2020 2020 2020 2020 636f 7079 3d73 656c          copy=sel
+0000ddb0: 662e 636f 7079 2c0a 2020 2020 2020 2020  f.copy,.        
+0000ddc0: 2020 2020 6474 7970 653d 464c 4f41 545f      dtype=FLOAT_
+0000ddd0: 4454 5950 4553 2c0a 2020 2020 2020 2020  DTYPES,.        
+0000dde0: 2020 2020 666f 7263 655f 616c 6c5f 6669      force_all_fi
+0000ddf0: 6e69 7465 3d22 616c 6c6f 772d 6e61 6e22  nite="allow-nan"
+0000de00: 2c0a 2020 2020 2020 2020 290a 0a20 2020  ,.        )..   
+0000de10: 2020 2020 2069 6620 7370 6172 7365 2e69       if sparse.i
+0000de20: 7373 7061 7273 6528 5829 3a0a 2020 2020  ssparse(X):.    
+0000de30: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+0000de40: 7769 7468 5f73 6361 6c69 6e67 3a0a 2020  with_scaling:.  
+0000de50: 2020 2020 2020 2020 2020 2020 2020 696e                in
+0000de60: 706c 6163 655f 636f 6c75 6d6e 5f73 6361  place_column_sca
+0000de70: 6c65 2858 2c20 7365 6c66 2e73 6361 6c65  le(X, self.scale
+0000de80: 5f29 0a20 2020 2020 2020 2065 6c73 653a  _).        else:
+0000de90: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0000dea0: 7365 6c66 2e77 6974 685f 7363 616c 696e  self.with_scalin
+0000deb0: 673a 0a20 2020 2020 2020 2020 2020 2020  g:.             
+0000dec0: 2020 2058 202a 3d20 7365 6c66 2e73 6361     X *= self.sca
+0000ded0: 6c65 5f0a 2020 2020 2020 2020 2020 2020  le_.            
+0000dee0: 6966 2073 656c 662e 7769 7468 5f63 656e  if self.with_cen
+0000def0: 7465 7269 6e67 3a0a 2020 2020 2020 2020  tering:.        
+0000df00: 2020 2020 2020 2020 5820 2b3d 2073 656c          X += sel
+0000df10: 662e 6365 6e74 6572 5f0a 2020 2020 2020  f.center_.      
+0000df20: 2020 7265 7475 726e 2058 0a0a 2020 2020    return X..    
+0000df30: 6465 6620 5f6d 6f72 655f 7461 6773 2873  def _more_tags(s
+0000df40: 656c 6629 3a0a 2020 2020 2020 2020 7265  elf):.        re
+0000df50: 7475 726e 207b 2261 6c6c 6f77 5f6e 616e  turn {"allow_nan
+0000df60: 223a 2054 7275 657d 0a0a 0a40 7661 6c69  ": True}...@vali
+0000df70: 6461 7465 5f70 6172 616d 7328 0a20 2020  date_params(.   
+0000df80: 207b 2258 223a 205b 2261 7272 6179 2d6c   {"X": ["array-l
+0000df90: 696b 6522 2c20 2273 7061 7273 6520 6d61  ike", "sparse ma
+0000dfa0: 7472 6978 225d 2c20 2261 7869 7322 3a20  trix"], "axis": 
+0000dfb0: 5b4f 7074 696f 6e73 2849 6e74 6567 7261  [Options(Integra
+0000dfc0: 6c2c 207b 302c 2031 7d29 5d7d 2c0a 2020  l, {0, 1})]},.  
+0000dfd0: 2020 7072 6566 6572 5f73 6b69 705f 6e65    prefer_skip_ne
+0000dfe0: 7374 6564 5f76 616c 6964 6174 696f 6e3d  sted_validation=
+0000dff0: 4661 6c73 652c 0a29 0a64 6566 2072 6f62  False,.).def rob
+0000e000: 7573 745f 7363 616c 6528 0a20 2020 2058  ust_scale(.    X
+0000e010: 2c0a 2020 2020 2a2c 0a20 2020 2061 7869  ,.    *,.    axi
+0000e020: 733d 302c 0a20 2020 2077 6974 685f 6365  s=0,.    with_ce
+0000e030: 6e74 6572 696e 673d 5472 7565 2c0a 2020  ntering=True,.  
+0000e040: 2020 7769 7468 5f73 6361 6c69 6e67 3d54    with_scaling=T
+0000e050: 7275 652c 0a20 2020 2071 7561 6e74 696c  rue,.    quantil
+0000e060: 655f 7261 6e67 653d 2832 352e 302c 2037  e_range=(25.0, 7
+0000e070: 352e 3029 2c0a 2020 2020 636f 7079 3d54  5.0),.    copy=T
+0000e080: 7275 652c 0a20 2020 2075 6e69 745f 7661  rue,.    unit_va
+0000e090: 7269 616e 6365 3d46 616c 7365 2c0a 293a  riance=False,.):
+0000e0a0: 0a20 2020 2022 2222 5374 616e 6461 7264  .    """Standard
+0000e0b0: 697a 6520 6120 6461 7461 7365 7420 616c  ize a dataset al
+0000e0c0: 6f6e 6720 616e 7920 6178 6973 2e0a 0a20  ong any axis... 
+0000e0d0: 2020 2043 656e 7465 7220 746f 2074 6865     Center to the
+0000e0e0: 206d 6564 6961 6e20 616e 6420 636f 6d70   median and comp
+0000e0f0: 6f6e 656e 7420 7769 7365 2073 6361 6c65  onent wise scale
+0000e100: 0a20 2020 2061 6363 6f72 6469 6e67 2074  .    according t
+0000e110: 6f20 7468 6520 696e 7465 7271 7561 7274  o the interquart
+0000e120: 696c 6520 7261 6e67 652e 0a0a 2020 2020  ile range...    
+0000e130: 5265 6164 206d 6f72 6520 696e 2074 6865  Read more in the
+0000e140: 203a 7265 663a 6055 7365 7220 4775 6964   :ref:`User Guid
+0000e150: 6520 3c70 7265 7072 6f63 6573 7369 6e67  e <preprocessing
+0000e160: 5f73 6361 6c65 723e 602e 0a0a 2020 2020  _scaler>`...    
+0000e170: 5061 7261 6d65 7465 7273 0a20 2020 202d  Parameters.    -
+0000e180: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 5820  ---------.    X 
+0000e190: 3a20 7b61 7272 6179 2d6c 696b 652c 2073  : {array-like, s
+0000e1a0: 7061 7273 6520 6d61 7472 6978 7d20 6f66  parse matrix} of
+0000e1b0: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
+0000e1c0: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
+0000e1d0: 2020 2020 2020 5468 6520 6461 7461 2074        The data t
+0000e1e0: 6f20 6365 6e74 6572 2061 6e64 2073 6361  o center and sca
+0000e1f0: 6c65 2e0a 0a20 2020 2061 7869 7320 3a20  le...    axis : 
+0000e200: 696e 742c 2064 6566 6175 6c74 3d30 0a20  int, default=0. 
+0000e210: 2020 2020 2020 2041 7869 7320 7573 6564         Axis used
+0000e220: 2074 6f20 636f 6d70 7574 6520 7468 6520   to compute the 
+0000e230: 6d65 6469 616e 7320 616e 6420 4951 5220  medians and IQR 
+0000e240: 616c 6f6e 672e 2049 6620 302c 0a20 2020  along. If 0,.   
+0000e250: 2020 2020 2069 6e64 6570 656e 6465 6e74       independent
+0000e260: 6c79 2073 6361 6c65 2065 6163 6820 6665  ly scale each fe
+0000e270: 6174 7572 652c 206f 7468 6572 7769 7365  ature, otherwise
+0000e280: 2028 6966 2031 2920 7363 616c 650a 2020   (if 1) scale.  
+0000e290: 2020 2020 2020 6561 6368 2073 616d 706c        each sampl
+0000e2a0: 652e 0a0a 2020 2020 7769 7468 5f63 656e  e...    with_cen
+0000e2b0: 7465 7269 6e67 203a 2062 6f6f 6c2c 2064  tering : bool, d
+0000e2c0: 6566 6175 6c74 3d54 7275 650a 2020 2020  efault=True.    
+0000e2d0: 2020 2020 4966 2060 5472 7565 602c 2063      If `True`, c
+0000e2e0: 656e 7465 7220 7468 6520 6461 7461 2062  enter the data b
+0000e2f0: 6566 6f72 6520 7363 616c 696e 672e 0a0a  efore scaling...
+0000e300: 2020 2020 7769 7468 5f73 6361 6c69 6e67      with_scaling
+0000e310: 203a 2062 6f6f 6c2c 2064 6566 6175 6c74   : bool, default
+0000e320: 3d54 7275 650a 2020 2020 2020 2020 4966  =True.        If
+0000e330: 2060 5472 7565 602c 2073 6361 6c65 2074   `True`, scale t
+0000e340: 6865 2064 6174 6120 746f 2075 6e69 7420  he data to unit 
+0000e350: 7661 7269 616e 6365 2028 6f72 2065 7175  variance (or equ
+0000e360: 6976 616c 656e 746c 792c 0a20 2020 2020  ivalently,.     
+0000e370: 2020 2075 6e69 7420 7374 616e 6461 7264     unit standard
+0000e380: 2064 6576 6961 7469 6f6e 292e 0a0a 2020   deviation)...  
+0000e390: 2020 7175 616e 7469 6c65 5f72 616e 6765    quantile_range
+0000e3a0: 203a 2074 7570 6c65 2028 715f 6d69 6e2c   : tuple (q_min,
+0000e3b0: 2071 5f6d 6178 292c 2030 2e30 203c 2071   q_max), 0.0 < q
+0000e3c0: 5f6d 696e 203c 2071 5f6d 6178 203c 2031  _min < q_max < 1
+0000e3d0: 3030 2e30 2c5c 0a20 2020 2020 2020 2064  00.0,\.        d
+0000e3e0: 6566 6175 6c74 3d28 3235 2e30 2c20 3735  efault=(25.0, 75
+0000e3f0: 2e30 290a 2020 2020 2020 2020 5175 616e  .0).        Quan
+0000e400: 7469 6c65 2072 616e 6765 2075 7365 6420  tile range used 
+0000e410: 746f 2063 616c 6375 6c61 7465 2060 7363  to calculate `sc
+0000e420: 616c 655f 602e 2042 7920 6465 6661 756c  ale_`. By defaul
+0000e430: 7420 7468 6973 2069 7320 6571 7561 6c20  t this is equal 
+0000e440: 746f 0a20 2020 2020 2020 2074 6865 2049  to.        the I
+0000e450: 5152 2c20 692e 652e 2c20 6071 5f6d 696e  QR, i.e., `q_min
+0000e460: 6020 6973 2074 6865 2066 6972 7374 2071  ` is the first q
+0000e470: 7561 6e74 696c 6520 616e 6420 6071 5f6d  uantile and `q_m
+0000e480: 6178 6020 6973 2074 6865 2074 6869 7264  ax` is the third
+0000e490: 0a20 2020 2020 2020 2071 7561 6e74 696c  .        quantil
+0000e4a0: 652e 0a0a 2020 2020 2020 2020 2e2e 2076  e...        .. v
+0000e4b0: 6572 7369 6f6e 6164 6465 643a 3a20 302e  ersionadded:: 0.
+0000e4c0: 3138 0a0a 2020 2020 636f 7079 203a 2062  18..    copy : b
+0000e4d0: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
+0000e4e0: 650a 2020 2020 2020 2020 4966 2046 616c  e.        If Fal
+0000e4f0: 7365 2c20 7472 7920 746f 2061 766f 6964  se, try to avoid
+0000e500: 2061 2063 6f70 7920 616e 6420 7363 616c   a copy and scal
+0000e510: 6520 696e 2070 6c61 6365 2e0a 2020 2020  e in place..    
+0000e520: 2020 2020 5468 6973 2069 7320 6e6f 7420      This is not 
+0000e530: 6775 6172 616e 7465 6564 2074 6f20 616c  guaranteed to al
+0000e540: 7761 7973 2077 6f72 6b20 696e 2070 6c61  ways work in pla
+0000e550: 6365 3b20 652e 672e 2069 6620 7468 6520  ce; e.g. if the 
+0000e560: 6461 7461 2069 730a 2020 2020 2020 2020  data is.        
+0000e570: 6120 6e75 6d70 7920 6172 7261 7920 7769  a numpy array wi
+0000e580: 7468 2061 6e20 696e 7420 6474 7970 652c  th an int dtype,
+0000e590: 2061 2063 6f70 7920 7769 6c6c 2062 6520   a copy will be 
+0000e5a0: 7265 7475 726e 6564 2065 7665 6e20 7769  returned even wi
+0000e5b0: 7468 0a20 2020 2020 2020 2063 6f70 793d  th.        copy=
+0000e5c0: 4661 6c73 652e 0a0a 2020 2020 756e 6974  False...    unit
+0000e5d0: 5f76 6172 6961 6e63 6520 3a20 626f 6f6c  _variance : bool
+0000e5e0: 2c20 6465 6661 756c 743d 4661 6c73 650a  , default=False.
+0000e5f0: 2020 2020 2020 2020 4966 2060 5472 7565          If `True
+0000e600: 602c 2073 6361 6c65 2064 6174 6120 736f  `, scale data so
+0000e610: 2074 6861 7420 6e6f 726d 616c 6c79 2064   that normally d
+0000e620: 6973 7472 6962 7574 6564 2066 6561 7475  istributed featu
+0000e630: 7265 7320 6861 7665 2061 0a20 2020 2020  res have a.     
+0000e640: 2020 2076 6172 6961 6e63 6520 6f66 2031     variance of 1
+0000e650: 2e20 496e 2067 656e 6572 616c 2c20 6966  . In general, if
+0000e660: 2074 6865 2064 6966 6665 7265 6e63 6520   the difference 
+0000e670: 6265 7477 6565 6e20 7468 6520 782d 7661  between the x-va
+0000e680: 6c75 6573 206f 660a 2020 2020 2020 2020  lues of.        
+0000e690: 6071 5f6d 6178 6020 616e 6420 6071 5f6d  `q_max` and `q_m
+0000e6a0: 696e 6020 666f 7220 6120 7374 616e 6461  in` for a standa
+0000e6b0: 7264 206e 6f72 6d61 6c20 6469 7374 7269  rd normal distri
+0000e6c0: 6275 7469 6f6e 2069 7320 6772 6561 7465  bution is greate
+0000e6d0: 720a 2020 2020 2020 2020 7468 616e 2031  r.        than 1
+0000e6e0: 2c20 7468 6520 6461 7461 7365 7420 7769  , the dataset wi
+0000e6f0: 6c6c 2062 6520 7363 616c 6564 2064 6f77  ll be scaled dow
+0000e700: 6e2e 2049 6620 6c65 7373 2074 6861 6e20  n. If less than 
+0000e710: 312c 2074 6865 2064 6174 6173 6574 0a20  1, the dataset. 
+0000e720: 2020 2020 2020 2077 696c 6c20 6265 2073         will be s
+0000e730: 6361 6c65 6420 7570 2e0a 0a20 2020 2020  caled up...     
+0000e740: 2020 202e 2e20 7665 7273 696f 6e61 6464     .. versionadd
+0000e750: 6564 3a3a 2030 2e32 340a 0a20 2020 2052  ed:: 0.24..    R
+0000e760: 6574 7572 6e73 0a20 2020 202d 2d2d 2d2d  eturns.    -----
+0000e770: 2d2d 0a20 2020 2058 5f74 7220 3a20 7b6e  --.    X_tr : {n
+0000e780: 6461 7272 6179 2c20 7370 6172 7365 206d  darray, sparse m
+0000e790: 6174 7269 787d 206f 6620 7368 6170 6520  atrix} of shape 
+0000e7a0: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
+0000e7b0: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
+0000e7c0: 5468 6520 7472 616e 7366 6f72 6d65 6420  The transformed 
+0000e7d0: 6461 7461 2e0a 0a20 2020 2053 6565 2041  data...    See A
+0000e7e0: 6c73 6f0a 2020 2020 2d2d 2d2d 2d2d 2d2d  lso.    --------
+0000e7f0: 0a20 2020 2052 6f62 7573 7453 6361 6c65  .    RobustScale
+0000e800: 7220 3a20 5065 7266 6f72 6d73 2063 656e  r : Performs cen
+0000e810: 7465 7269 6e67 2061 6e64 2073 6361 6c69  tering and scali
+0000e820: 6e67 2075 7369 6e67 2074 6865 2054 7261  ng using the Tra
+0000e830: 6e73 666f 726d 6572 2041 5049 0a20 2020  nsformer API.   
+0000e840: 2020 2020 2028 652e 672e 2061 7320 7061       (e.g. as pa
+0000e850: 7274 206f 6620 6120 7072 6570 726f 6365  rt of a preproce
+0000e860: 7373 696e 6720 3a63 6c61 7373 3a60 7e73  ssing :class:`~s
+0000e870: 6b6c 6561 726e 2e70 6970 656c 696e 652e  klearn.pipeline.
+0000e880: 5069 7065 6c69 6e65 6029 2e0a 0a20 2020  Pipeline`)...   
+0000e890: 204e 6f74 6573 0a20 2020 202d 2d2d 2d2d   Notes.    -----
+0000e8a0: 0a20 2020 2054 6869 7320 696d 706c 656d  .    This implem
+0000e8b0: 656e 7461 7469 6f6e 2077 696c 6c20 7265  entation will re
+0000e8c0: 6675 7365 2074 6f20 6365 6e74 6572 2073  fuse to center s
+0000e8d0: 6369 7079 2e73 7061 7273 6520 6d61 7472  cipy.sparse matr
+0000e8e0: 6963 6573 0a20 2020 2073 696e 6365 2069  ices.    since i
+0000e8f0: 7420 776f 756c 6420 6d61 6b65 2074 6865  t would make the
+0000e900: 6d20 6e6f 6e2d 7370 6172 7365 2061 6e64  m non-sparse and
+0000e910: 2077 6f75 6c64 2070 6f74 656e 7469 616c   would potential
+0000e920: 6c79 2063 7261 7368 2074 6865 0a20 2020  ly crash the.   
+0000e930: 2070 726f 6772 616d 2077 6974 6820 6d65   program with me
+0000e940: 6d6f 7279 2065 7868 6175 7374 696f 6e20  mory exhaustion 
+0000e950: 7072 6f62 6c65 6d73 2e0a 0a20 2020 2049  problems...    I
+0000e960: 6e73 7465 6164 2074 6865 2063 616c 6c65  nstead the calle
+0000e970: 7220 6973 2065 7870 6563 7465 6420 746f  r is expected to
+0000e980: 2065 6974 6865 7220 7365 7420 6578 706c   either set expl
+0000e990: 6963 6974 6c79 0a20 2020 2060 7769 7468  icitly.    `with
+0000e9a0: 5f63 656e 7465 7269 6e67 3d46 616c 7365  _centering=False
+0000e9b0: 6020 2869 6e20 7468 6174 2063 6173 652c  ` (in that case,
+0000e9c0: 206f 6e6c 7920 7661 7269 616e 6365 2073   only variance s
+0000e9d0: 6361 6c69 6e67 2077 696c 6c20 6265 0a20  caling will be. 
+0000e9e0: 2020 2070 6572 666f 726d 6564 206f 6e20     performed on 
+0000e9f0: 7468 6520 6665 6174 7572 6573 206f 6620  the features of 
+0000ea00: 7468 6520 4353 5220 6d61 7472 6978 2920  the CSR matrix) 
+0000ea10: 6f72 2074 6f20 6361 6c6c 2060 582e 746f  or to call `X.to
+0000ea20: 6172 7261 7928 2960 0a20 2020 2069 6620  array()`.    if 
+0000ea30: 6865 2f73 6865 2065 7870 6563 7473 2074  he/she expects t
+0000ea40: 6865 206d 6174 6572 6961 6c69 7a65 6420  he materialized 
+0000ea50: 6465 6e73 6520 6172 7261 7920 746f 2066  dense array to f
+0000ea60: 6974 2069 6e20 6d65 6d6f 7279 2e0a 0a20  it in memory... 
+0000ea70: 2020 2054 6f20 6176 6f69 6420 6d65 6d6f     To avoid memo
+0000ea80: 7279 2063 6f70 7920 7468 6520 6361 6c6c  ry copy the call
+0000ea90: 6572 2073 686f 756c 6420 7061 7373 2061  er should pass a
+0000eaa0: 2043 5352 206d 6174 7269 782e 0a0a 2020   CSR matrix...  
+0000eab0: 2020 466f 7220 6120 636f 6d70 6172 6973    For a comparis
+0000eac0: 6f6e 206f 6620 7468 6520 6469 6666 6572  on of the differ
+0000ead0: 656e 7420 7363 616c 6572 732c 2074 7261  ent scalers, tra
+0000eae0: 6e73 666f 726d 6572 732c 2061 6e64 206e  nsformers, and n
+0000eaf0: 6f72 6d61 6c69 7a65 7273 2c0a 2020 2020  ormalizers,.    
+0000eb00: 7365 653a 203a 7265 663a 6073 7068 785f  see: :ref:`sphx_
+0000eb10: 676c 725f 6175 746f 5f65 7861 6d70 6c65  glr_auto_example
+0000eb20: 735f 7072 6570 726f 6365 7373 696e 675f  s_preprocessing_
+0000eb30: 706c 6f74 5f61 6c6c 5f73 6361 6c69 6e67  plot_all_scaling
+0000eb40: 2e70 7960 2e0a 0a20 2020 202e 2e20 7761  .py`...    .. wa
+0000eb50: 726e 696e 673a 3a20 5269 736b 206f 6620  rning:: Risk of 
+0000eb60: 6461 7461 206c 6561 6b0a 0a20 2020 2020  data leak..     
+0000eb70: 2020 2044 6f20 6e6f 7420 7573 6520 3a66     Do not use :f
+0000eb80: 756e 633a 607e 736b 6c65 6172 6e2e 7072  unc:`~sklearn.pr
+0000eb90: 6570 726f 6365 7373 696e 672e 726f 6275  eprocessing.robu
+0000eba0: 7374 5f73 6361 6c65 6020 756e 6c65 7373  st_scale` unless
+0000ebb0: 2079 6f75 206b 6e6f 770a 2020 2020 2020   you know.      
+0000ebc0: 2020 7768 6174 2079 6f75 2061 7265 2064    what you are d
+0000ebd0: 6f69 6e67 2e20 4120 636f 6d6d 6f6e 206d  oing. A common m
+0000ebe0: 6973 7461 6b65 2069 7320 746f 2061 7070  istake is to app
+0000ebf0: 6c79 2069 7420 746f 2074 6865 2065 6e74  ly it to the ent
+0000ec00: 6972 6520 6461 7461 0a20 2020 2020 2020  ire data.       
+0000ec10: 202a 6265 666f 7265 2a20 7370 6c69 7474   *before* splitt
+0000ec20: 696e 6720 696e 746f 2074 7261 696e 696e  ing into trainin
+0000ec30: 6720 616e 6420 7465 7374 2073 6574 732e  g and test sets.
+0000ec40: 2054 6869 7320 7769 6c6c 2062 6961 7320   This will bias 
+0000ec50: 7468 650a 2020 2020 2020 2020 6d6f 6465  the.        mode
+0000ec60: 6c20 6576 616c 7561 7469 6f6e 2062 6563  l evaluation bec
+0000ec70: 6175 7365 2069 6e66 6f72 6d61 7469 6f6e  ause information
+0000ec80: 2077 6f75 6c64 2068 6176 6520 6c65 616b   would have leak
+0000ec90: 6564 2066 726f 6d20 7468 6520 7465 7374  ed from the test
+0000eca0: 0a20 2020 2020 2020 2073 6574 2074 6f20  .        set to 
+0000ecb0: 7468 6520 7472 6169 6e69 6e67 2073 6574  the training set
+0000ecc0: 2e0a 2020 2020 2020 2020 496e 2067 656e  ..        In gen
+0000ecd0: 6572 616c 2c20 7765 2072 6563 6f6d 6d65  eral, we recomme
+0000ece0: 6e64 2075 7369 6e67 0a20 2020 2020 2020  nd using.       
+0000ecf0: 203a 636c 6173 733a 607e 736b 6c65 6172   :class:`~sklear
+0000ed00: 6e2e 7072 6570 726f 6365 7373 696e 672e  n.preprocessing.
+0000ed10: 526f 6275 7374 5363 616c 6572 6020 7769  RobustScaler` wi
+0000ed20: 7468 696e 2061 0a20 2020 2020 2020 203a  thin a.        :
+0000ed30: 7265 663a 6050 6970 656c 696e 6520 3c70  ref:`Pipeline <p
+0000ed40: 6970 656c 696e 653e 6020 696e 206f 7264  ipeline>` in ord
+0000ed50: 6572 2074 6f20 7072 6576 656e 7420 6d6f  er to prevent mo
+0000ed60: 7374 2072 6973 6b73 206f 6620 6461 7461  st risks of data
+0000ed70: 0a20 2020 2020 2020 206c 6561 6b69 6e67  .        leaking
+0000ed80: 3a20 6070 6970 6520 3d20 6d61 6b65 5f70  : `pipe = make_p
+0000ed90: 6970 656c 696e 6528 526f 6275 7374 5363  ipeline(RobustSc
+0000eda0: 616c 6572 2829 2c20 4c6f 6769 7374 6963  aler(), Logistic
+0000edb0: 5265 6772 6573 7369 6f6e 2829 2960 2e0a  Regression())`..
+0000edc0: 0a20 2020 2045 7861 6d70 6c65 730a 2020  .    Examples.  
+0000edd0: 2020 2d2d 2d2d 2d2d 2d2d 0a20 2020 203e    --------.    >
+0000ede0: 3e3e 2066 726f 6d20 736b 6c65 6172 6e2e  >> from sklearn.
+0000edf0: 7072 6570 726f 6365 7373 696e 6720 696d  preprocessing im
+0000ee00: 706f 7274 2072 6f62 7573 745f 7363 616c  port robust_scal
+0000ee10: 650a 2020 2020 3e3e 3e20 5820 3d20 5b5b  e.    >>> X = [[
+0000ee20: 2d32 2c20 312c 2032 5d2c 205b 2d31 2c20  -2, 1, 2], [-1, 
+0000ee30: 302c 2031 5d5d 0a20 2020 203e 3e3e 2072  0, 1]].    >>> r
+0000ee40: 6f62 7573 745f 7363 616c 6528 582c 2061  obust_scale(X, a
+0000ee50: 7869 733d 3029 2020 2320 7363 616c 6520  xis=0)  # scale 
+0000ee60: 6561 6368 2063 6f6c 756d 6e20 696e 6465  each column inde
+0000ee70: 7065 6e64 656e 746c 790a 2020 2020 6172  pendently.    ar
+0000ee80: 7261 7928 5b5b 2d31 2e2c 2020 312e 2c20  ray([[-1.,  1., 
+0000ee90: 2031 2e5d 2c0a 2020 2020 2020 2020 2020   1.],.          
+0000eea0: 205b 2031 2e2c 202d 312e 2c20 2d31 2e5d   [ 1., -1., -1.]
+0000eeb0: 5d29 0a20 2020 203e 3e3e 2072 6f62 7573  ]).    >>> robus
+0000eec0: 745f 7363 616c 6528 582c 2061 7869 733d  t_scale(X, axis=
+0000eed0: 3129 2020 2320 7363 616c 6520 6561 6368  1)  # scale each
+0000eee0: 2072 6f77 2069 6e64 6570 656e 6465 6e74   row independent
+0000eef0: 6c79 0a20 2020 2061 7272 6179 285b 5b2d  ly.    array([[-
+0000ef00: 312e 352c 2020 302e 202c 2020 302e 355d  1.5,  0. ,  0.5]
+0000ef10: 2c0a 2020 2020 2020 2020 2020 205b 2d31  ,.           [-1
+0000ef20: 2e20 2c20 2030 2e20 2c20 2031 2e20 5d5d  . ,  0. ,  1. ]]
+0000ef30: 290a 2020 2020 2222 220a 2020 2020 5820  ).    """.    X 
+0000ef40: 3d20 6368 6563 6b5f 6172 7261 7928 0a20  = check_array(. 
+0000ef50: 2020 2020 2020 2058 2c0a 2020 2020 2020         X,.      
+0000ef60: 2020 6163 6365 7074 5f73 7061 7273 653d    accept_sparse=
+0000ef70: 2822 6373 7222 2c20 2263 7363 2229 2c0a  ("csr", "csc"),.
+0000ef80: 2020 2020 2020 2020 636f 7079 3d46 616c          copy=Fal
+0000ef90: 7365 2c0a 2020 2020 2020 2020 656e 7375  se,.        ensu
+0000efa0: 7265 5f32 643d 4661 6c73 652c 0a20 2020  re_2d=False,.   
+0000efb0: 2020 2020 2064 7479 7065 3d46 4c4f 4154       dtype=FLOAT
+0000efc0: 5f44 5459 5045 532c 0a20 2020 2020 2020  _DTYPES,.       
+0000efd0: 2066 6f72 6365 5f61 6c6c 5f66 696e 6974   force_all_finit
+0000efe0: 653d 2261 6c6c 6f77 2d6e 616e 222c 0a20  e="allow-nan",. 
+0000eff0: 2020 2029 0a20 2020 206f 7269 6769 6e61     ).    origina
+0000f000: 6c5f 6e64 696d 203d 2058 2e6e 6469 6d0a  l_ndim = X.ndim.
+0000f010: 0a20 2020 2069 6620 6f72 6967 696e 616c  .    if original
+0000f020: 5f6e 6469 6d20 3d3d 2031 3a0a 2020 2020  _ndim == 1:.    
+0000f030: 2020 2020 5820 3d20 582e 7265 7368 6170      X = X.reshap
+0000f040: 6528 582e 7368 6170 655b 305d 2c20 3129  e(X.shape[0], 1)
+0000f050: 0a0a 2020 2020 7320 3d20 526f 6275 7374  ..    s = Robust
+0000f060: 5363 616c 6572 280a 2020 2020 2020 2020  Scaler(.        
+0000f070: 7769 7468 5f63 656e 7465 7269 6e67 3d77  with_centering=w
+0000f080: 6974 685f 6365 6e74 6572 696e 672c 0a20  ith_centering,. 
+0000f090: 2020 2020 2020 2077 6974 685f 7363 616c         with_scal
+0000f0a0: 696e 673d 7769 7468 5f73 6361 6c69 6e67  ing=with_scaling
+0000f0b0: 2c0a 2020 2020 2020 2020 7175 616e 7469  ,.        quanti
+0000f0c0: 6c65 5f72 616e 6765 3d71 7561 6e74 696c  le_range=quantil
+0000f0d0: 655f 7261 6e67 652c 0a20 2020 2020 2020  e_range,.       
+0000f0e0: 2075 6e69 745f 7661 7269 616e 6365 3d75   unit_variance=u
+0000f0f0: 6e69 745f 7661 7269 616e 6365 2c0a 2020  nit_variance,.  
+0000f100: 2020 2020 2020 636f 7079 3d63 6f70 792c        copy=copy,
+0000f110: 0a20 2020 2029 0a20 2020 2069 6620 6178  .    ).    if ax
+0000f120: 6973 203d 3d20 303a 0a20 2020 2020 2020  is == 0:.       
+0000f130: 2058 203d 2073 2e66 6974 5f74 7261 6e73   X = s.fit_trans
+0000f140: 666f 726d 2858 290a 2020 2020 656c 7365  form(X).    else
+0000f150: 3a0a 2020 2020 2020 2020 5820 3d20 732e  :.        X = s.
+0000f160: 6669 745f 7472 616e 7366 6f72 6d28 582e  fit_transform(X.
+0000f170: 5429 2e54 0a0a 2020 2020 6966 206f 7269  T).T..    if ori
+0000f180: 6769 6e61 6c5f 6e64 696d 203d 3d20 313a  ginal_ndim == 1:
+0000f190: 0a20 2020 2020 2020 2058 203d 2058 2e72  .        X = X.r
+0000f1a0: 6176 656c 2829 0a0a 2020 2020 7265 7475  avel()..    retu
+0000f1b0: 726e 2058 0a0a 0a40 7661 6c69 6461 7465  rn X...@validate
+0000f1c0: 5f70 6172 616d 7328 0a20 2020 207b 0a20  _params(.    {. 
+0000f1d0: 2020 2020 2020 2022 5822 3a20 5b22 6172         "X": ["ar
+0000f1e0: 7261 792d 6c69 6b65 222c 2022 7370 6172  ray-like", "spar
+0000f1f0: 7365 206d 6174 7269 7822 5d2c 0a20 2020  se matrix"],.   
+0000f200: 2020 2020 2022 6e6f 726d 223a 205b 5374       "norm": [St
+0000f210: 724f 7074 696f 6e73 287b 226c 3122 2c20  rOptions({"l1", 
+0000f220: 226c 3222 2c20 226d 6178 227d 295d 2c0a  "l2", "max"})],.
+0000f230: 2020 2020 2020 2020 2261 7869 7322 3a20          "axis": 
+0000f240: 5b4f 7074 696f 6e73 2849 6e74 6567 7261  [Options(Integra
+0000f250: 6c2c 207b 302c 2031 7d29 5d2c 0a20 2020  l, {0, 1})],.   
+0000f260: 2020 2020 2022 636f 7079 223a 205b 2262       "copy": ["b
+0000f270: 6f6f 6c65 616e 225d 2c0a 2020 2020 2020  oolean"],.      
+0000f280: 2020 2272 6574 7572 6e5f 6e6f 726d 223a    "return_norm":
+0000f290: 205b 2262 6f6f 6c65 616e 225d 2c0a 2020   ["boolean"],.  
+0000f2a0: 2020 7d2c 0a20 2020 2070 7265 6665 725f    },.    prefer_
+0000f2b0: 736b 6970 5f6e 6573 7465 645f 7661 6c69  skip_nested_vali
+0000f2c0: 6461 7469 6f6e 3d54 7275 652c 0a29 0a64  dation=True,.).d
+0000f2d0: 6566 206e 6f72 6d61 6c69 7a65 2858 2c20  ef normalize(X, 
+0000f2e0: 6e6f 726d 3d22 6c32 222c 202a 2c20 6178  norm="l2", *, ax
+0000f2f0: 6973 3d31 2c20 636f 7079 3d54 7275 652c  is=1, copy=True,
+0000f300: 2072 6574 7572 6e5f 6e6f 726d 3d46 616c   return_norm=Fal
+0000f310: 7365 293a 0a20 2020 2022 2222 5363 616c  se):.    """Scal
+0000f320: 6520 696e 7075 7420 7665 6374 6f72 7320  e input vectors 
+0000f330: 696e 6469 7669 6475 616c 6c79 2074 6f20  individually to 
+0000f340: 756e 6974 206e 6f72 6d20 2876 6563 746f  unit norm (vecto
+0000f350: 7220 6c65 6e67 7468 292e 0a0a 2020 2020  r length)...    
+0000f360: 5265 6164 206d 6f72 6520 696e 2074 6865  Read more in the
+0000f370: 203a 7265 663a 6055 7365 7220 4775 6964   :ref:`User Guid
+0000f380: 6520 3c70 7265 7072 6f63 6573 7369 6e67  e <preprocessing
+0000f390: 5f6e 6f72 6d61 6c69 7a61 7469 6f6e 3e60  _normalization>`
+0000f3a0: 2e0a 0a20 2020 2050 6172 616d 6574 6572  ...    Parameter
+0000f3b0: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
+0000f3c0: 0a20 2020 2058 203a 207b 6172 7261 792d  .    X : {array-
+0000f3d0: 6c69 6b65 2c20 7370 6172 7365 206d 6174  like, sparse mat
+0000f3e0: 7269 787d 206f 6620 7368 6170 6520 286e  rix} of shape (n
+0000f3f0: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
+0000f400: 7572 6573 290a 2020 2020 2020 2020 5468  ures).        Th
+0000f410: 6520 6461 7461 2074 6f20 6e6f 726d 616c  e data to normal
+0000f420: 697a 652c 2065 6c65 6d65 6e74 2062 7920  ize, element by 
+0000f430: 656c 656d 656e 742e 0a20 2020 2020 2020  element..       
+0000f440: 2073 6369 7079 2e73 7061 7273 6520 6d61   scipy.sparse ma
+0000f450: 7472 6963 6573 2073 686f 756c 6420 6265  trices should be
+0000f460: 2069 6e20 4353 5220 666f 726d 6174 2074   in CSR format t
+0000f470: 6f20 6176 6f69 6420 616e 0a20 2020 2020  o avoid an.     
+0000f480: 2020 2075 6e2d 6e65 6365 7373 6172 7920     un-necessary 
+0000f490: 636f 7079 2e0a 0a20 2020 206e 6f72 6d20  copy...    norm 
+0000f4a0: 3a20 7b27 6c31 272c 2027 6c32 272c 2027  : {'l1', 'l2', '
+0000f4b0: 6d61 7827 7d2c 2064 6566 6175 6c74 3d27  max'}, default='
+0000f4c0: 6c32 270a 2020 2020 2020 2020 5468 6520  l2'.        The 
+0000f4d0: 6e6f 726d 2074 6f20 7573 6520 746f 206e  norm to use to n
+0000f4e0: 6f72 6d61 6c69 7a65 2065 6163 6820 6e6f  ormalize each no
+0000f4f0: 6e20 7a65 726f 2073 616d 706c 6520 286f  n zero sample (o
+0000f500: 7220 6561 6368 206e 6f6e 2d7a 6572 6f0a  r each non-zero.
+0000f510: 2020 2020 2020 2020 6665 6174 7572 6520          feature 
+0000f520: 6966 2061 7869 7320 6973 2030 292e 0a0a  if axis is 0)...
+0000f530: 2020 2020 6178 6973 203a 207b 302c 2031      axis : {0, 1
+0000f540: 7d2c 2064 6566 6175 6c74 3d31 0a20 2020  }, default=1.   
+0000f550: 2020 2020 2044 6566 696e 6520 6178 6973       Define axis
+0000f560: 2075 7365 6420 746f 206e 6f72 6d61 6c69   used to normali
+0000f570: 7a65 2074 6865 2064 6174 6120 616c 6f6e  ze the data alon
+0000f580: 672e 2049 6620 312c 2069 6e64 6570 656e  g. If 1, indepen
+0000f590: 6465 6e74 6c79 0a20 2020 2020 2020 206e  dently.        n
+0000f5a0: 6f72 6d61 6c69 7a65 2065 6163 6820 7361  ormalize each sa
+0000f5b0: 6d70 6c65 2c20 6f74 6865 7277 6973 6520  mple, otherwise 
+0000f5c0: 2869 6620 3029 206e 6f72 6d61 6c69 7a65  (if 0) normalize
+0000f5d0: 2065 6163 6820 6665 6174 7572 652e 0a0a   each feature...
+0000f5e0: 2020 2020 636f 7079 203a 2062 6f6f 6c2c      copy : bool,
+0000f5f0: 2064 6566 6175 6c74 3d54 7275 650a 2020   default=True.  
+0000f600: 2020 2020 2020 4966 2046 616c 7365 2c20        If False, 
+0000f610: 7472 7920 746f 2061 766f 6964 2061 2063  try to avoid a c
+0000f620: 6f70 7920 616e 6420 6e6f 726d 616c 697a  opy and normaliz
+0000f630: 6520 696e 2070 6c61 6365 2e0a 2020 2020  e in place..    
+0000f640: 2020 2020 5468 6973 2069 7320 6e6f 7420      This is not 
+0000f650: 6775 6172 616e 7465 6564 2074 6f20 616c  guaranteed to al
+0000f660: 7761 7973 2077 6f72 6b20 696e 2070 6c61  ways work in pla
+0000f670: 6365 3b20 652e 672e 2069 6620 7468 6520  ce; e.g. if the 
+0000f680: 6461 7461 2069 730a 2020 2020 2020 2020  data is.        
+0000f690: 6120 6e75 6d70 7920 6172 7261 7920 7769  a numpy array wi
+0000f6a0: 7468 2061 6e20 696e 7420 6474 7970 652c  th an int dtype,
+0000f6b0: 2061 2063 6f70 7920 7769 6c6c 2062 6520   a copy will be 
+0000f6c0: 7265 7475 726e 6564 2065 7665 6e20 7769  returned even wi
+0000f6d0: 7468 0a20 2020 2020 2020 2063 6f70 793d  th.        copy=
+0000f6e0: 4661 6c73 652e 0a0a 2020 2020 7265 7475  False...    retu
+0000f6f0: 726e 5f6e 6f72 6d20 3a20 626f 6f6c 2c20  rn_norm : bool, 
+0000f700: 6465 6661 756c 743d 4661 6c73 650a 2020  default=False.  
+0000f710: 2020 2020 2020 5768 6574 6865 7220 746f        Whether to
+0000f720: 2072 6574 7572 6e20 7468 6520 636f 6d70   return the comp
+0000f730: 7574 6564 206e 6f72 6d73 2e0a 0a20 2020  uted norms...   
+0000f740: 2052 6574 7572 6e73 0a20 2020 202d 2d2d   Returns.    ---
+0000f750: 2d2d 2d2d 0a20 2020 2058 203a 207b 6e64  ----.    X : {nd
+0000f760: 6172 7261 792c 2073 7061 7273 6520 6d61  array, sparse ma
+0000f770: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
+0000f780: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+0000f790: 7475 7265 7329 0a20 2020 2020 2020 204e  tures).        N
+0000f7a0: 6f72 6d61 6c69 7a65 6420 696e 7075 7420  ormalized input 
+0000f7b0: 582e 0a0a 2020 2020 6e6f 726d 7320 3a20  X...    norms : 
+0000f7c0: 6e64 6172 7261 7920 6f66 2073 6861 7065  ndarray of shape
+0000f7d0: 2028 6e5f 7361 6d70 6c65 732c 2029 2069   (n_samples, ) i
+0000f7e0: 6620 6178 6973 3d31 2065 6c73 6520 286e  f axis=1 else (n
+0000f7f0: 5f66 6561 7475 7265 732c 2029 0a20 2020  _features, ).   
+0000f800: 2020 2020 2041 6e20 6172 7261 7920 6f66       An array of
+0000f810: 206e 6f72 6d73 2061 6c6f 6e67 2067 6976   norms along giv
+0000f820: 656e 2061 7869 7320 666f 7220 582e 0a20  en axis for X.. 
+0000f830: 2020 2020 2020 2057 6865 6e20 5820 6973         When X is
+0000f840: 2073 7061 7273 652c 2061 204e 6f74 496d   sparse, a NotIm
+0000f850: 706c 656d 656e 7465 6445 7272 6f72 2077  plementedError w
+0000f860: 696c 6c20 6265 2072 6169 7365 640a 2020  ill be raised.  
+0000f870: 2020 2020 2020 666f 7220 6e6f 726d 2027        for norm '
+0000f880: 6c31 2720 6f72 2027 6c32 272e 0a0a 2020  l1' or 'l2'...  
+0000f890: 2020 5365 6520 416c 736f 0a20 2020 202d    See Also.    -
+0000f8a0: 2d2d 2d2d 2d2d 2d0a 2020 2020 4e6f 726d  -------.    Norm
+0000f8b0: 616c 697a 6572 203a 2050 6572 666f 726d  alizer : Perform
+0000f8c0: 7320 6e6f 726d 616c 697a 6174 696f 6e20  s normalization 
+0000f8d0: 7573 696e 6720 7468 6520 5472 616e 7366  using the Transf
+0000f8e0: 6f72 6d65 7220 4150 490a 2020 2020 2020  ormer API.      
+0000f8f0: 2020 2865 2e67 2e20 6173 2070 6172 7420    (e.g. as part 
+0000f900: 6f66 2061 2070 7265 7072 6f63 6573 7369  of a preprocessi
+0000f910: 6e67 203a 636c 6173 733a 607e 736b 6c65  ng :class:`~skle
+0000f920: 6172 6e2e 7069 7065 6c69 6e65 2e50 6970  arn.pipeline.Pip
+0000f930: 656c 696e 6560 292e 0a0a 2020 2020 4e6f  eline`)...    No
+0000f940: 7465 730a 2020 2020 2d2d 2d2d 2d0a 2020  tes.    -----.  
+0000f950: 2020 466f 7220 6120 636f 6d70 6172 6973    For a comparis
+0000f960: 6f6e 206f 6620 7468 6520 6469 6666 6572  on of the differ
+0000f970: 656e 7420 7363 616c 6572 732c 2074 7261  ent scalers, tra
+0000f980: 6e73 666f 726d 6572 732c 2061 6e64 206e  nsformers, and n
+0000f990: 6f72 6d61 6c69 7a65 7273 2c0a 2020 2020  ormalizers,.    
+0000f9a0: 7365 653a 203a 7265 663a 6073 7068 785f  see: :ref:`sphx_
+0000f9b0: 676c 725f 6175 746f 5f65 7861 6d70 6c65  glr_auto_example
+0000f9c0: 735f 7072 6570 726f 6365 7373 696e 675f  s_preprocessing_
+0000f9d0: 706c 6f74 5f61 6c6c 5f73 6361 6c69 6e67  plot_all_scaling
+0000f9e0: 2e70 7960 2e0a 0a20 2020 2045 7861 6d70  .py`...    Examp
+0000f9f0: 6c65 730a 2020 2020 2d2d 2d2d 2d2d 2d2d  les.    --------
+0000fa00: 0a20 2020 203e 3e3e 2066 726f 6d20 736b  .    >>> from sk
+0000fa10: 6c65 6172 6e2e 7072 6570 726f 6365 7373  learn.preprocess
+0000fa20: 696e 6720 696d 706f 7274 206e 6f72 6d61  ing import norma
+0000fa30: 6c69 7a65 0a20 2020 203e 3e3e 2058 203d  lize.    >>> X =
+0000fa40: 205b 5b2d 322c 2031 2c20 325d 2c20 5b2d   [[-2, 1, 2], [-
+0000fa50: 312c 2030 2c20 315d 5d0a 2020 2020 3e3e  1, 0, 1]].    >>
+0000fa60: 3e20 6e6f 726d 616c 697a 6528 582c 206e  > normalize(X, n
+0000fa70: 6f72 6d3d 226c 3122 2920 2023 204c 3120  orm="l1")  # L1 
+0000fa80: 6e6f 726d 616c 697a 6174 696f 6e20 6561  normalization ea
+0000fa90: 6368 2072 6f77 2069 6e64 6570 656e 6465  ch row independe
+0000faa0: 6e74 6c79 0a20 2020 2061 7272 6179 285b  ntly.    array([
+0000fab0: 5b2d 302e 342c 2020 302e 322c 2020 302e  [-0.4,  0.2,  0.
+0000fac0: 345d 2c0a 2020 2020 2020 2020 2020 205b  4],.           [
+0000fad0: 2d30 2e35 2c20 2030 2e20 2c20 2030 2e35  -0.5,  0. ,  0.5
+0000fae0: 5d5d 290a 2020 2020 3e3e 3e20 6e6f 726d  ]]).    >>> norm
+0000faf0: 616c 697a 6528 582c 206e 6f72 6d3d 226c  alize(X, norm="l
+0000fb00: 3222 2920 2023 204c 3220 6e6f 726d 616c  2")  # L2 normal
+0000fb10: 697a 6174 696f 6e20 6561 6368 2072 6f77  ization each row
+0000fb20: 2069 6e64 6570 656e 6465 6e74 6c79 0a20   independently. 
+0000fb30: 2020 2061 7272 6179 285b 5b2d 302e 3636     array([[-0.66
+0000fb40: 2e2e 2e2c 2020 302e 3333 2e2e 2e2c 2020  ...,  0.33...,  
+0000fb50: 302e 3636 2e2e 2e5d 2c0a 2020 2020 2020  0.66...],.      
+0000fb60: 2020 2020 205b 2d30 2e37 302e 2e2e 2c20       [-0.70..., 
+0000fb70: 2030 2e20 2020 2020 2c20 2030 2e37 302e   0.     ,  0.70.
+0000fb80: 2e2e 5d5d 290a 2020 2020 2222 220a 2020  ..]]).    """.  
+0000fb90: 2020 6966 2061 7869 7320 3d3d 2030 3a0a    if axis == 0:.
+0000fba0: 2020 2020 2020 2020 7370 6172 7365 5f66          sparse_f
+0000fbb0: 6f72 6d61 7420 3d20 2263 7363 220a 2020  ormat = "csc".  
+0000fbc0: 2020 656c 7365 3a20 2023 2061 7869 7320    else:  # axis 
+0000fbd0: 3d3d 2031 3a0a 2020 2020 2020 2020 7370  == 1:.        sp
+0000fbe0: 6172 7365 5f66 6f72 6d61 7420 3d20 2263  arse_format = "c
+0000fbf0: 7372 220a 0a20 2020 2078 702c 205f 203d  sr"..    xp, _ =
+0000fc00: 2067 6574 5f6e 616d 6573 7061 6365 2858   get_namespace(X
+0000fc10: 290a 0a20 2020 2058 203d 2063 6865 636b  )..    X = check
+0000fc20: 5f61 7272 6179 280a 2020 2020 2020 2020  _array(.        
+0000fc30: 582c 0a20 2020 2020 2020 2061 6363 6570  X,.        accep
+0000fc40: 745f 7370 6172 7365 3d73 7061 7273 655f  t_sparse=sparse_
+0000fc50: 666f 726d 6174 2c0a 2020 2020 2020 2020  format,.        
+0000fc60: 636f 7079 3d63 6f70 792c 0a20 2020 2020  copy=copy,.     
+0000fc70: 2020 2065 7374 696d 6174 6f72 3d22 7468     estimator="th
+0000fc80: 6520 6e6f 726d 616c 697a 6520 6675 6e63  e normalize func
+0000fc90: 7469 6f6e 222c 0a20 2020 2020 2020 2064  tion",.        d
+0000fca0: 7479 7065 3d5f 6172 7261 795f 6170 692e  type=_array_api.
+0000fcb0: 7375 7070 6f72 7465 645f 666c 6f61 745f  supported_float_
+0000fcc0: 6474 7970 6573 2878 7029 2c0a 2020 2020  dtypes(xp),.    
+0000fcd0: 290a 2020 2020 6966 2061 7869 7320 3d3d  ).    if axis ==
+0000fce0: 2030 3a0a 2020 2020 2020 2020 5820 3d20   0:.        X = 
+0000fcf0: 582e 540a 0a20 2020 2069 6620 7370 6172  X.T..    if spar
+0000fd00: 7365 2e69 7373 7061 7273 6528 5829 3a0a  se.issparse(X):.
+0000fd10: 2020 2020 2020 2020 6966 2072 6574 7572          if retur
+0000fd20: 6e5f 6e6f 726d 2061 6e64 206e 6f72 6d20  n_norm and norm 
+0000fd30: 696e 2028 226c 3122 2c20 226c 3222 293a  in ("l1", "l2"):
+0000fd40: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
+0000fd50: 7365 204e 6f74 496d 706c 656d 656e 7465  se NotImplemente
+0000fd60: 6445 7272 6f72 280a 2020 2020 2020 2020  dError(.        
+0000fd70: 2020 2020 2020 2020 2272 6574 7572 6e5f          "return_
+0000fd80: 6e6f 726d 3d54 7275 6520 6973 206e 6f74  norm=True is not
+0000fd90: 2069 6d70 6c65 6d65 6e74 6564 2022 0a20   implemented ". 
+0000fda0: 2020 2020 2020 2020 2020 2020 2020 2022                 "
+0000fdb0: 666f 7220 7370 6172 7365 206d 6174 7269  for sparse matri
+0000fdc0: 6365 7320 7769 7468 206e 6f72 6d20 276c  ces with norm 'l
+0000fdd0: 3127 2022 0a20 2020 2020 2020 2020 2020  1' ".           
+0000fde0: 2020 2020 2022 6f72 206e 6f72 6d20 276c       "or norm 'l
+0000fdf0: 3227 220a 2020 2020 2020 2020 2020 2020  2'".            
+0000fe00: 290a 2020 2020 2020 2020 6966 206e 6f72  ).        if nor
+0000fe10: 6d20 3d3d 2022 6c31 223a 0a20 2020 2020  m == "l1":.     
+0000fe20: 2020 2020 2020 2069 6e70 6c61 6365 5f63         inplace_c
+0000fe30: 7372 5f72 6f77 5f6e 6f72 6d61 6c69 7a65  sr_row_normalize
+0000fe40: 5f6c 3128 5829 0a20 2020 2020 2020 2065  _l1(X).        e
+0000fe50: 6c69 6620 6e6f 726d 203d 3d20 226c 3222  lif norm == "l2"
+0000fe60: 3a0a 2020 2020 2020 2020 2020 2020 696e  :.            in
+0000fe70: 706c 6163 655f 6373 725f 726f 775f 6e6f  place_csr_row_no
+0000fe80: 726d 616c 697a 655f 6c32 2858 290a 2020  rmalize_l2(X).  
+0000fe90: 2020 2020 2020 656c 6966 206e 6f72 6d20        elif norm 
+0000fea0: 3d3d 2022 6d61 7822 3a0a 2020 2020 2020  == "max":.      
+0000feb0: 2020 2020 2020 6d69 6e73 2c20 6d61 7865        mins, maxe
+0000fec0: 7320 3d20 6d69 6e5f 6d61 785f 6178 6973  s = min_max_axis
+0000fed0: 2858 2c20 3129 0a20 2020 2020 2020 2020  (X, 1).         
+0000fee0: 2020 206e 6f72 6d73 203d 206e 702e 6d61     norms = np.ma
+0000fef0: 7869 6d75 6d28 6162 7328 6d69 6e73 292c  ximum(abs(mins),
+0000ff00: 206d 6178 6573 290a 2020 2020 2020 2020   maxes).        
+0000ff10: 2020 2020 6e6f 726d 735f 656c 656d 656e      norms_elemen
+0000ff20: 7477 6973 6520 3d20 6e6f 726d 732e 7265  twise = norms.re
+0000ff30: 7065 6174 286e 702e 6469 6666 2858 2e69  peat(np.diff(X.i
+0000ff40: 6e64 7074 7229 290a 2020 2020 2020 2020  ndptr)).        
+0000ff50: 2020 2020 6d61 736b 203d 206e 6f72 6d73      mask = norms
+0000ff60: 5f65 6c65 6d65 6e74 7769 7365 2021 3d20  _elementwise != 
+0000ff70: 300a 2020 2020 2020 2020 2020 2020 582e  0.            X.
+0000ff80: 6461 7461 5b6d 6173 6b5d 202f 3d20 6e6f  data[mask] /= no
+0000ff90: 726d 735f 656c 656d 656e 7477 6973 655b  rms_elementwise[
+0000ffa0: 6d61 736b 5d0a 2020 2020 656c 7365 3a0a  mask].    else:.
+0000ffb0: 2020 2020 2020 2020 6966 206e 6f72 6d20          if norm 
+0000ffc0: 3d3d 2022 6c31 223a 0a20 2020 2020 2020  == "l1":.       
+0000ffd0: 2020 2020 206e 6f72 6d73 203d 2078 702e       norms = xp.
+0000ffe0: 7375 6d28 7870 2e61 6273 2858 292c 2061  sum(xp.abs(X), a
+0000fff0: 7869 733d 3129 0a20 2020 2020 2020 2065  xis=1).        e
+00010000: 6c69 6620 6e6f 726d 203d 3d20 226c 3222  lif norm == "l2"
+00010010: 3a0a 2020 2020 2020 2020 2020 2020 6e6f  :.            no
+00010020: 726d 7320 3d20 726f 775f 6e6f 726d 7328  rms = row_norms(
+00010030: 5829 0a20 2020 2020 2020 2065 6c69 6620  X).        elif 
+00010040: 6e6f 726d 203d 3d20 226d 6178 223a 0a20  norm == "max":. 
+00010050: 2020 2020 2020 2020 2020 206e 6f72 6d73             norms
+00010060: 203d 2078 702e 6d61 7828 7870 2e61 6273   = xp.max(xp.abs
+00010070: 2858 292c 2061 7869 733d 3129 0a20 2020  (X), axis=1).   
+00010080: 2020 2020 206e 6f72 6d73 203d 205f 6861       norms = _ha
+00010090: 6e64 6c65 5f7a 6572 6f73 5f69 6e5f 7363  ndle_zeros_in_sc
+000100a0: 616c 6528 6e6f 726d 732c 2063 6f70 793d  ale(norms, copy=
+000100b0: 4661 6c73 6529 0a20 2020 2020 2020 2058  False).        X
+000100c0: 202f 3d20 6e6f 726d 735b 3a2c 204e 6f6e   /= norms[:, Non
+000100d0: 655d 0a0a 2020 2020 6966 2061 7869 7320  e]..    if axis 
+000100e0: 3d3d 2030 3a0a 2020 2020 2020 2020 5820  == 0:.        X 
+000100f0: 3d20 582e 540a 0a20 2020 2069 6620 7265  = X.T..    if re
+00010100: 7475 726e 5f6e 6f72 6d3a 0a20 2020 2020  turn_norm:.     
+00010110: 2020 2072 6574 7572 6e20 582c 206e 6f72     return X, nor
+00010120: 6d73 0a20 2020 2065 6c73 653a 0a20 2020  ms.    else:.   
+00010130: 2020 2020 2072 6574 7572 6e20 580a 0a0a       return X...
+00010140: 636c 6173 7320 4e6f 726d 616c 697a 6572  class Normalizer
+00010150: 284f 6e65 546f 4f6e 6546 6561 7475 7265  (OneToOneFeature
+00010160: 4d69 7869 6e2c 2054 7261 6e73 666f 726d  Mixin, Transform
+00010170: 6572 4d69 7869 6e2c 2042 6173 6545 7374  erMixin, BaseEst
+00010180: 696d 6174 6f72 293a 0a20 2020 2022 2222  imator):.    """
+00010190: 4e6f 726d 616c 697a 6520 7361 6d70 6c65  Normalize sample
+000101a0: 7320 696e 6469 7669 6475 616c 6c79 2074  s individually t
+000101b0: 6f20 756e 6974 206e 6f72 6d2e 0a0a 2020  o unit norm...  
+000101c0: 2020 4561 6368 2073 616d 706c 6520 2869    Each sample (i
+000101d0: 2e65 2e20 6561 6368 2072 6f77 206f 6620  .e. each row of 
+000101e0: 7468 6520 6461 7461 206d 6174 7269 7829  the data matrix)
+000101f0: 2077 6974 6820 6174 206c 6561 7374 206f   with at least o
+00010200: 6e65 0a20 2020 206e 6f6e 207a 6572 6f20  ne.    non zero 
+00010210: 636f 6d70 6f6e 656e 7420 6973 2072 6573  component is res
+00010220: 6361 6c65 6420 696e 6465 7065 6e64 656e  caled independen
+00010230: 746c 7920 6f66 206f 7468 6572 2073 616d  tly of other sam
+00010240: 706c 6573 2073 6f0a 2020 2020 7468 6174  ples so.    that
+00010250: 2069 7473 206e 6f72 6d20 286c 312c 206c   its norm (l1, l
+00010260: 3220 6f72 2069 6e66 2920 6571 7561 6c73  2 or inf) equals
+00010270: 206f 6e65 2e0a 0a20 2020 2054 6869 7320   one...    This 
+00010280: 7472 616e 7366 6f72 6d65 7220 6973 2061  transformer is a
+00010290: 626c 6520 746f 2077 6f72 6b20 626f 7468  ble to work both
+000102a0: 2077 6974 6820 6465 6e73 6520 6e75 6d70   with dense nump
+000102b0: 7920 6172 7261 7973 2061 6e64 0a20 2020  y arrays and.   
+000102c0: 2073 6369 7079 2e73 7061 7273 6520 6d61   scipy.sparse ma
+000102d0: 7472 6978 2028 7573 6520 4353 5220 666f  trix (use CSR fo
+000102e0: 726d 6174 2069 6620 796f 7520 7761 6e74  rmat if you want
+000102f0: 2074 6f20 6176 6f69 6420 7468 6520 6275   to avoid the bu
+00010300: 7264 656e 206f 660a 2020 2020 6120 636f  rden of.    a co
+00010310: 7079 202f 2063 6f6e 7665 7273 696f 6e29  py / conversion)
+00010320: 2e0a 0a20 2020 2053 6361 6c69 6e67 2069  ...    Scaling i
+00010330: 6e70 7574 7320 746f 2075 6e69 7420 6e6f  nputs to unit no
+00010340: 726d 7320 6973 2061 2063 6f6d 6d6f 6e20  rms is a common 
+00010350: 6f70 6572 6174 696f 6e20 666f 7220 7465  operation for te
+00010360: 7874 0a20 2020 2063 6c61 7373 6966 6963  xt.    classific
+00010370: 6174 696f 6e20 6f72 2063 6c75 7374 6572  ation or cluster
+00010380: 696e 6720 666f 7220 696e 7374 616e 6365  ing for instance
+00010390: 2e20 466f 7220 696e 7374 616e 6365 2074  . For instance t
+000103a0: 6865 2064 6f74 0a20 2020 2070 726f 6475  he dot.    produ
+000103b0: 6374 206f 6620 7477 6f20 6c32 2d6e 6f72  ct of two l2-nor
+000103c0: 6d61 6c69 7a65 6420 5446 2d49 4446 2076  malized TF-IDF v
+000103d0: 6563 746f 7273 2069 7320 7468 6520 636f  ectors is the co
+000103e0: 7369 6e65 2073 696d 696c 6172 6974 790a  sine similarity.
+000103f0: 2020 2020 6f66 2074 6865 2076 6563 746f      of the vecto
+00010400: 7273 2061 6e64 2069 7320 7468 6520 6261  rs and is the ba
+00010410: 7365 2073 696d 696c 6172 6974 7920 6d65  se similarity me
+00010420: 7472 6963 2066 6f72 2074 6865 2056 6563  tric for the Vec
+00010430: 746f 720a 2020 2020 5370 6163 6520 4d6f  tor.    Space Mo
+00010440: 6465 6c20 636f 6d6d 6f6e 6c79 2075 7365  del commonly use
+00010450: 6420 6279 2074 6865 2049 6e66 6f72 6d61  d by the Informa
+00010460: 7469 6f6e 2052 6574 7269 6576 616c 2063  tion Retrieval c
+00010470: 6f6d 6d75 6e69 7479 2e0a 0a20 2020 2046  ommunity...    F
+00010480: 6f72 2061 6e20 6578 616d 706c 6520 7669  or an example vi
+00010490: 7375 616c 697a 6174 696f 6e2c 2072 6566  sualization, ref
+000104a0: 6572 2074 6f20 3a72 6566 3a60 436f 6d70  er to :ref:`Comp
+000104b0: 6172 6520 4e6f 726d 616c 697a 6572 2077  are Normalizer w
+000104c0: 6974 6820 6f74 6865 720a 2020 2020 7363  ith other.    sc
+000104d0: 616c 6572 7320 3c70 6c6f 745f 616c 6c5f  alers <plot_all_
+000104e0: 7363 616c 696e 675f 6e6f 726d 616c 697a  scaling_normaliz
+000104f0: 6572 5f73 6563 7469 6f6e 3e60 2e0a 0a20  er_section>`... 
+00010500: 2020 2052 6561 6420 6d6f 7265 2069 6e20     Read more in 
+00010510: 7468 6520 3a72 6566 3a60 5573 6572 2047  the :ref:`User G
+00010520: 7569 6465 203c 7072 6570 726f 6365 7373  uide <preprocess
+00010530: 696e 675f 6e6f 726d 616c 697a 6174 696f  ing_normalizatio
+00010540: 6e3e 602e 0a0a 2020 2020 5061 7261 6d65  n>`...    Parame
+00010550: 7465 7273 0a20 2020 202d 2d2d 2d2d 2d2d  ters.    -------
+00010560: 2d2d 2d0a 2020 2020 6e6f 726d 203a 207b  ---.    norm : {
+00010570: 276c 3127 2c20 276c 3227 2c20 276d 6178  'l1', 'l2', 'max
+00010580: 277d 2c20 6465 6661 756c 743d 276c 3227  '}, default='l2'
+00010590: 0a20 2020 2020 2020 2054 6865 206e 6f72  .        The nor
+000105a0: 6d20 746f 2075 7365 2074 6f20 6e6f 726d  m to use to norm
+000105b0: 616c 697a 6520 6561 6368 206e 6f6e 207a  alize each non z
+000105c0: 6572 6f20 7361 6d70 6c65 2e20 4966 206e  ero sample. If n
+000105d0: 6f72 6d3d 276d 6178 270a 2020 2020 2020  orm='max'.      
+000105e0: 2020 6973 2075 7365 642c 2076 616c 7565    is used, value
+000105f0: 7320 7769 6c6c 2062 6520 7265 7363 616c  s will be rescal
+00010600: 6564 2062 7920 7468 6520 6d61 7869 6d75  ed by the maximu
+00010610: 6d20 6f66 2074 6865 2061 6273 6f6c 7574  m of the absolut
+00010620: 650a 2020 2020 2020 2020 7661 6c75 6573  e.        values
+00010630: 2e0a 0a20 2020 2063 6f70 7920 3a20 626f  ...    copy : bo
+00010640: 6f6c 2c20 6465 6661 756c 743d 5472 7565  ol, default=True
+00010650: 0a20 2020 2020 2020 2053 6574 2074 6f20  .        Set to 
+00010660: 4661 6c73 6520 746f 2070 6572 666f 726d  False to perform
+00010670: 2069 6e70 6c61 6365 2072 6f77 206e 6f72   inplace row nor
+00010680: 6d61 6c69 7a61 7469 6f6e 2061 6e64 2061  malization and a
+00010690: 766f 6964 2061 0a20 2020 2020 2020 2063  void a.        c
+000106a0: 6f70 7920 2869 6620 7468 6520 696e 7075  opy (if the inpu
+000106b0: 7420 6973 2061 6c72 6561 6479 2061 206e  t is already a n
+000106c0: 756d 7079 2061 7272 6179 206f 7220 6120  umpy array or a 
+000106d0: 7363 6970 792e 7370 6172 7365 0a20 2020  scipy.sparse.   
+000106e0: 2020 2020 2043 5352 206d 6174 7269 7829       CSR matrix)
+000106f0: 2e0a 0a20 2020 2041 7474 7269 6275 7465  ...    Attribute
+00010700: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
+00010710: 0a20 2020 206e 5f66 6561 7475 7265 735f  .    n_features_
+00010720: 696e 5f20 3a20 696e 740a 2020 2020 2020  in_ : int.      
+00010730: 2020 4e75 6d62 6572 206f 6620 6665 6174    Number of feat
+00010740: 7572 6573 2073 6565 6e20 6475 7269 6e67  ures seen during
+00010750: 203a 7465 726d 3a60 6669 7460 2e0a 0a20   :term:`fit`... 
+00010760: 2020 2020 2020 202e 2e20 7665 7273 696f         .. versio
+00010770: 6e61 6464 6564 3a3a 2030 2e32 340a 0a20  nadded:: 0.24.. 
+00010780: 2020 2066 6561 7475 7265 5f6e 616d 6573     feature_names
+00010790: 5f69 6e5f 203a 206e 6461 7272 6179 206f  _in_ : ndarray o
+000107a0: 6620 7368 6170 6520 2860 6e5f 6665 6174  f shape (`n_feat
+000107b0: 7572 6573 5f69 6e5f 602c 290a 2020 2020  ures_in_`,).    
+000107c0: 2020 2020 4e61 6d65 7320 6f66 2066 6561      Names of fea
+000107d0: 7475 7265 7320 7365 656e 2064 7572 696e  tures seen durin
+000107e0: 6720 3a74 6572 6d3a 6066 6974 602e 2044  g :term:`fit`. D
+000107f0: 6566 696e 6564 206f 6e6c 7920 7768 656e  efined only when
+00010800: 2060 5860 0a20 2020 2020 2020 2068 6173   `X`.        has
+00010810: 2066 6561 7475 7265 206e 616d 6573 2074   feature names t
+00010820: 6861 7420 6172 6520 616c 6c20 7374 7269  hat are all stri
+00010830: 6e67 732e 0a0a 2020 2020 2020 2020 2e2e  ngs...        ..
+00010840: 2076 6572 7369 6f6e 6164 6465 643a 3a20   versionadded:: 
+00010850: 312e 300a 0a20 2020 2053 6565 2041 6c73  1.0..    See Als
+00010860: 6f0a 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20  o.    --------. 
+00010870: 2020 206e 6f72 6d61 6c69 7a65 203a 2045     normalize : E
+00010880: 7175 6976 616c 656e 7420 6675 6e63 7469  quivalent functi
+00010890: 6f6e 2077 6974 686f 7574 2074 6865 2065  on without the e
+000108a0: 7374 696d 6174 6f72 2041 5049 2e0a 0a20  stimator API... 
+000108b0: 2020 204e 6f74 6573 0a20 2020 202d 2d2d     Notes.    ---
+000108c0: 2d2d 0a20 2020 2054 6869 7320 6573 7469  --.    This esti
+000108d0: 6d61 746f 7220 6973 203a 7465 726d 3a60  mator is :term:`
+000108e0: 7374 6174 656c 6573 7360 2061 6e64 2064  stateless` and d
+000108f0: 6f65 7320 6e6f 7420 6e65 6564 2074 6f20  oes not need to 
+00010900: 6265 2066 6974 7465 642e 0a20 2020 2048  be fitted..    H
+00010910: 6f77 6576 6572 2c20 7765 2072 6563 6f6d  owever, we recom
+00010920: 6d65 6e64 2074 6f20 6361 6c6c 203a 6d65  mend to call :me
+00010930: 7468 3a60 6669 745f 7472 616e 7366 6f72  th:`fit_transfor
+00010940: 6d60 2069 6e73 7465 6164 206f 660a 2020  m` instead of.  
+00010950: 2020 3a6d 6574 683a 6074 7261 6e73 666f    :meth:`transfo
+00010960: 726d 602c 2061 7320 7061 7261 6d65 7465  rm`, as paramete
+00010970: 7220 7661 6c69 6461 7469 6f6e 2069 7320  r validation is 
+00010980: 6f6e 6c79 2070 6572 666f 726d 6564 2069  only performed i
+00010990: 6e0a 2020 2020 3a6d 6574 683a 6066 6974  n.    :meth:`fit
+000109a0: 602e 0a0a 2020 2020 4578 616d 706c 6573  `...    Examples
+000109b0: 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a 2020  .    --------.  
+000109c0: 2020 3e3e 3e20 6672 6f6d 2073 6b6c 6561    >>> from sklea
+000109d0: 726e 2e70 7265 7072 6f63 6573 7369 6e67  rn.preprocessing
+000109e0: 2069 6d70 6f72 7420 4e6f 726d 616c 697a   import Normaliz
+000109f0: 6572 0a20 2020 203e 3e3e 2058 203d 205b  er.    >>> X = [
+00010a00: 5b34 2c20 312c 2032 2c20 325d 2c0a 2020  [4, 1, 2, 2],.  
+00010a10: 2020 2e2e 2e20 2020 2020 205b 312c 2033    ...      [1, 3
+00010a20: 2c20 392c 2033 5d2c 0a20 2020 202e 2e2e  , 9, 3],.    ...
+00010a30: 2020 2020 2020 5b35 2c20 372c 2035 2c20        [5, 7, 5, 
+00010a40: 315d 5d0a 2020 2020 3e3e 3e20 7472 616e  1]].    >>> tran
+00010a50: 7366 6f72 6d65 7220 3d20 4e6f 726d 616c  sformer = Normal
+00010a60: 697a 6572 2829 2e66 6974 2858 2920 2023  izer().fit(X)  #
+00010a70: 2066 6974 2064 6f65 7320 6e6f 7468 696e   fit does nothin
+00010a80: 672e 0a20 2020 203e 3e3e 2074 7261 6e73  g..    >>> trans
+00010a90: 666f 726d 6572 0a20 2020 204e 6f72 6d61  former.    Norma
+00010aa0: 6c69 7a65 7228 290a 2020 2020 3e3e 3e20  lizer().    >>> 
+00010ab0: 7472 616e 7366 6f72 6d65 722e 7472 616e  transformer.tran
+00010ac0: 7366 6f72 6d28 5829 0a20 2020 2061 7272  sform(X).    arr
+00010ad0: 6179 285b 5b30 2e38 2c20 302e 322c 2030  ay([[0.8, 0.2, 0
+00010ae0: 2e34 2c20 302e 345d 2c0a 2020 2020 2020  .4, 0.4],.      
+00010af0: 2020 2020 205b 302e 312c 2030 2e33 2c20       [0.1, 0.3, 
+00010b00: 302e 392c 2030 2e33 5d2c 0a20 2020 2020  0.9, 0.3],.     
+00010b10: 2020 2020 2020 5b30 2e35 2c20 302e 372c        [0.5, 0.7,
+00010b20: 2030 2e35 2c20 302e 315d 5d29 0a20 2020   0.5, 0.1]]).   
+00010b30: 2022 2222 0a0a 2020 2020 5f70 6172 616d   """..    _param
+00010b40: 6574 6572 5f63 6f6e 7374 7261 696e 7473  eter_constraints
+00010b50: 3a20 6469 6374 203d 207b 0a20 2020 2020  : dict = {.     
+00010b60: 2020 2022 6e6f 726d 223a 205b 5374 724f     "norm": [StrO
+00010b70: 7074 696f 6e73 287b 226c 3122 2c20 226c  ptions({"l1", "l
+00010b80: 3222 2c20 226d 6178 227d 295d 2c0a 2020  2", "max"})],.  
+00010b90: 2020 2020 2020 2263 6f70 7922 3a20 5b22        "copy": ["
+00010ba0: 626f 6f6c 6561 6e22 5d2c 0a20 2020 207d  boolean"],.    }
+00010bb0: 0a0a 2020 2020 6465 6620 5f5f 696e 6974  ..    def __init
+00010bc0: 5f5f 2873 656c 662c 206e 6f72 6d3d 226c  __(self, norm="l
+00010bd0: 3222 2c20 2a2c 2063 6f70 793d 5472 7565  2", *, copy=True
+00010be0: 293a 0a20 2020 2020 2020 2073 656c 662e  ):.        self.
+00010bf0: 6e6f 726d 203d 206e 6f72 6d0a 2020 2020  norm = norm.    
+00010c00: 2020 2020 7365 6c66 2e63 6f70 7920 3d20      self.copy = 
+00010c10: 636f 7079 0a0a 2020 2020 405f 6669 745f  copy..    @_fit_
+00010c20: 636f 6e74 6578 7428 7072 6566 6572 5f73  context(prefer_s
+00010c30: 6b69 705f 6e65 7374 6564 5f76 616c 6964  kip_nested_valid
+00010c40: 6174 696f 6e3d 5472 7565 290a 2020 2020  ation=True).    
+00010c50: 6465 6620 6669 7428 7365 6c66 2c20 582c  def fit(self, X,
+00010c60: 2079 3d4e 6f6e 6529 3a0a 2020 2020 2020   y=None):.      
+00010c70: 2020 2222 224f 6e6c 7920 7661 6c69 6461    """Only valida
+00010c80: 7465 7320 6573 7469 6d61 746f 7227 7320  tes estimator's 
+00010c90: 7061 7261 6d65 7465 7273 2e0a 0a20 2020  parameters...   
+00010ca0: 2020 2020 2054 6869 7320 6d65 7468 6f64       This method
+00010cb0: 2061 6c6c 6f77 7320 746f 3a20 2869 2920   allows to: (i) 
+00010cc0: 7661 6c69 6461 7465 2074 6865 2065 7374  validate the est
+00010cd0: 696d 6174 6f72 2773 2070 6172 616d 6574  imator's paramet
+00010ce0: 6572 7320 616e 640a 2020 2020 2020 2020  ers and.        
+00010cf0: 2869 6929 2062 6520 636f 6e73 6973 7465  (ii) be consiste
+00010d00: 6e74 2077 6974 6820 7468 6520 7363 696b  nt with the scik
+00010d10: 6974 2d6c 6561 726e 2074 7261 6e73 666f  it-learn transfo
+00010d20: 726d 6572 2041 5049 2e0a 0a20 2020 2020  rmer API...     
+00010d30: 2020 2050 6172 616d 6574 6572 730a 2020     Parameters.  
+00010d40: 2020 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d        ----------
+00010d50: 0a20 2020 2020 2020 2058 203a 207b 6172  .        X : {ar
+00010d60: 7261 792d 6c69 6b65 2c20 7370 6172 7365  ray-like, sparse
+00010d70: 206d 6174 7269 787d 206f 6620 7368 6170   matrix} of shap
+00010d80: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
+00010d90: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
+00010da0: 2020 2020 2020 5468 6520 6461 7461 2074        The data t
+00010db0: 6f20 6573 7469 6d61 7465 2074 6865 206e  o estimate the n
+00010dc0: 6f72 6d61 6c69 7a61 7469 6f6e 2070 6172  ormalization par
+00010dd0: 616d 6574 6572 732e 0a0a 2020 2020 2020  ameters...      
+00010de0: 2020 7920 3a20 4967 6e6f 7265 640a 2020    y : Ignored.  
+00010df0: 2020 2020 2020 2020 2020 4e6f 7420 7573            Not us
+00010e00: 6564 2c20 7072 6573 656e 7420 6865 7265  ed, present here
+00010e10: 2066 6f72 2041 5049 2063 6f6e 7369 7374   for API consist
+00010e20: 656e 6379 2062 7920 636f 6e76 656e 7469  ency by conventi
+00010e30: 6f6e 2e0a 0a20 2020 2020 2020 2052 6574  on...        Ret
+00010e40: 7572 6e73 0a20 2020 2020 2020 202d 2d2d  urns.        ---
+00010e50: 2d2d 2d2d 0a20 2020 2020 2020 2073 656c  ----.        sel
+00010e60: 6620 3a20 6f62 6a65 6374 0a20 2020 2020  f : object.     
+00010e70: 2020 2020 2020 2046 6974 7465 6420 7472         Fitted tr
+00010e80: 616e 7366 6f72 6d65 722e 0a20 2020 2020  ansformer..     
+00010e90: 2020 2022 2222 0a20 2020 2020 2020 2073     """.        s
+00010ea0: 656c 662e 5f76 616c 6964 6174 655f 6461  elf._validate_da
+00010eb0: 7461 2858 2c20 6163 6365 7074 5f73 7061  ta(X, accept_spa
+00010ec0: 7273 653d 2263 7372 2229 0a20 2020 2020  rse="csr").     
+00010ed0: 2020 2072 6574 7572 6e20 7365 6c66 0a0a     return self..
+00010ee0: 2020 2020 6465 6620 7472 616e 7366 6f72      def transfor
+00010ef0: 6d28 7365 6c66 2c20 582c 2063 6f70 793d  m(self, X, copy=
+00010f00: 4e6f 6e65 293a 0a20 2020 2020 2020 2022  None):.        "
+00010f10: 2222 5363 616c 6520 6561 6368 206e 6f6e  ""Scale each non
+00010f20: 207a 6572 6f20 726f 7720 6f66 2058 2074   zero row of X t
+00010f30: 6f20 756e 6974 206e 6f72 6d2e 0a0a 2020  o unit norm...  
+00010f40: 2020 2020 2020 5061 7261 6d65 7465 7273        Parameters
+00010f50: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+00010f60: 2d2d 2d0a 2020 2020 2020 2020 5820 3a20  ---.        X : 
+00010f70: 7b61 7272 6179 2d6c 696b 652c 2073 7061  {array-like, spa
+00010f80: 7273 6520 6d61 7472 6978 7d20 6f66 2073  rse matrix} of s
+00010f90: 6861 7065 2028 6e5f 7361 6d70 6c65 732c  hape (n_samples,
+00010fa0: 206e 5f66 6561 7475 7265 7329 0a20 2020   n_features).   
+00010fb0: 2020 2020 2020 2020 2054 6865 2064 6174           The dat
+00010fc0: 6120 746f 206e 6f72 6d61 6c69 7a65 2c20  a to normalize, 
+00010fd0: 726f 7720 6279 2072 6f77 2e20 7363 6970  row by row. scip
+00010fe0: 792e 7370 6172 7365 206d 6174 7269 6365  y.sparse matrice
+00010ff0: 7320 7368 6f75 6c64 2062 650a 2020 2020  s should be.    
+00011000: 2020 2020 2020 2020 696e 2043 5352 2066          in CSR f
+00011010: 6f72 6d61 7420 746f 2061 766f 6964 2061  ormat to avoid a
+00011020: 6e20 756e 2d6e 6563 6573 7361 7279 2063  n un-necessary c
+00011030: 6f70 792e 0a0a 2020 2020 2020 2020 636f  opy...        co
+00011040: 7079 203a 2062 6f6f 6c2c 2064 6566 6175  py : bool, defau
+00011050: 6c74 3d4e 6f6e 650a 2020 2020 2020 2020  lt=None.        
+00011060: 2020 2020 436f 7079 2074 6865 2069 6e70      Copy the inp
+00011070: 7574 2058 206f 7220 6e6f 742e 0a0a 2020  ut X or not...  
+00011080: 2020 2020 2020 5265 7475 726e 730a 2020        Returns.  
+00011090: 2020 2020 2020 2d2d 2d2d 2d2d 2d0a 2020        -------.  
+000110a0: 2020 2020 2020 585f 7472 203a 207b 6e64        X_tr : {nd
+000110b0: 6172 7261 792c 2073 7061 7273 6520 6d61  array, sparse ma
+000110c0: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
+000110d0: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+000110e0: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
+000110f0: 2020 2054 7261 6e73 666f 726d 6564 2061     Transformed a
+00011100: 7272 6179 2e0a 2020 2020 2020 2020 2222  rray..        ""
+00011110: 220a 2020 2020 2020 2020 636f 7079 203d  ".        copy =
+00011120: 2063 6f70 7920 6966 2063 6f70 7920 6973   copy if copy is
+00011130: 206e 6f74 204e 6f6e 6520 656c 7365 2073   not None else s
+00011140: 656c 662e 636f 7079 0a20 2020 2020 2020  elf.copy.       
+00011150: 2058 203d 2073 656c 662e 5f76 616c 6964   X = self._valid
+00011160: 6174 655f 6461 7461 2858 2c20 6163 6365  ate_data(X, acce
+00011170: 7074 5f73 7061 7273 653d 2263 7372 222c  pt_sparse="csr",
+00011180: 2072 6573 6574 3d46 616c 7365 290a 2020   reset=False).  
+00011190: 2020 2020 2020 7265 7475 726e 206e 6f72        return nor
+000111a0: 6d61 6c69 7a65 2858 2c20 6e6f 726d 3d73  malize(X, norm=s
+000111b0: 656c 662e 6e6f 726d 2c20 6178 6973 3d31  elf.norm, axis=1
+000111c0: 2c20 636f 7079 3d63 6f70 7929 0a0a 2020  , copy=copy)..  
+000111d0: 2020 6465 6620 5f6d 6f72 655f 7461 6773    def _more_tags
+000111e0: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+000111f0: 7265 7475 726e 207b 2273 7461 7465 6c65  return {"statele
+00011200: 7373 223a 2054 7275 652c 2022 6172 7261  ss": True, "arra
+00011210: 795f 6170 695f 7375 7070 6f72 7422 3a20  y_api_support": 
+00011220: 5472 7565 7d0a 0a0a 4076 616c 6964 6174  True}...@validat
+00011230: 655f 7061 7261 6d73 280a 2020 2020 7b0a  e_params(.    {.
+00011240: 2020 2020 2020 2020 2258 223a 205b 2261          "X": ["a
+00011250: 7272 6179 2d6c 696b 6522 2c20 2273 7061  rray-like", "spa
+00011260: 7273 6520 6d61 7472 6978 225d 2c0a 2020  rse matrix"],.  
+00011270: 2020 2020 2020 2274 6872 6573 686f 6c64        "threshold
+00011280: 223a 205b 496e 7465 7276 616c 2852 6561  ": [Interval(Rea
+00011290: 6c2c 204e 6f6e 652c 204e 6f6e 652c 2063  l, None, None, c
+000112a0: 6c6f 7365 643d 226e 6569 7468 6572 2229  losed="neither")
+000112b0: 5d2c 0a20 2020 2020 2020 2022 636f 7079  ],.        "copy
+000112c0: 223a 205b 2262 6f6f 6c65 616e 225d 2c0a  ": ["boolean"],.
+000112d0: 2020 2020 7d2c 0a20 2020 2070 7265 6665      },.    prefe
+000112e0: 725f 736b 6970 5f6e 6573 7465 645f 7661  r_skip_nested_va
+000112f0: 6c69 6461 7469 6f6e 3d54 7275 652c 0a29  lidation=True,.)
+00011300: 0a64 6566 2062 696e 6172 697a 6528 582c  .def binarize(X,
+00011310: 202a 2c20 7468 7265 7368 6f6c 643d 302e   *, threshold=0.
+00011320: 302c 2063 6f70 793d 5472 7565 293a 0a20  0, copy=True):. 
+00011330: 2020 2022 2222 426f 6f6c 6561 6e20 7468     """Boolean th
+00011340: 7265 7368 6f6c 6469 6e67 206f 6620 6172  resholding of ar
+00011350: 7261 792d 6c69 6b65 206f 7220 7363 6970  ray-like or scip
+00011360: 792e 7370 6172 7365 206d 6174 7269 782e  y.sparse matrix.
+00011370: 0a0a 2020 2020 5265 6164 206d 6f72 6520  ..    Read more 
+00011380: 696e 2074 6865 203a 7265 663a 6055 7365  in the :ref:`Use
+00011390: 7220 4775 6964 6520 3c70 7265 7072 6f63  r Guide <preproc
+000113a0: 6573 7369 6e67 5f62 696e 6172 697a 6174  essing_binarizat
+000113b0: 696f 6e3e 602e 0a0a 2020 2020 5061 7261  ion>`...    Para
+000113c0: 6d65 7465 7273 0a20 2020 202d 2d2d 2d2d  meters.    -----
+000113d0: 2d2d 2d2d 2d0a 2020 2020 5820 3a20 7b61  -----.    X : {a
+000113e0: 7272 6179 2d6c 696b 652c 2073 7061 7273  rray-like, spars
+000113f0: 6520 6d61 7472 6978 7d20 6f66 2073 6861  e matrix} of sha
+00011400: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
+00011410: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
+00011420: 2020 2054 6865 2064 6174 6120 746f 2062     The data to b
+00011430: 696e 6172 697a 652c 2065 6c65 6d65 6e74  inarize, element
+00011440: 2062 7920 656c 656d 656e 742e 0a20 2020   by element..   
+00011450: 2020 2020 2073 6369 7079 2e73 7061 7273       scipy.spars
+00011460: 6520 6d61 7472 6963 6573 2073 686f 756c  e matrices shoul
+00011470: 6420 6265 2069 6e20 4353 5220 6f72 2043  d be in CSR or C
+00011480: 5343 2066 6f72 6d61 7420 746f 2061 766f  SC format to avo
+00011490: 6964 2061 6e0a 2020 2020 2020 2020 756e  id an.        un
+000114a0: 2d6e 6563 6573 7361 7279 2063 6f70 792e  -necessary copy.
+000114b0: 0a0a 2020 2020 7468 7265 7368 6f6c 6420  ..    threshold 
+000114c0: 3a20 666c 6f61 742c 2064 6566 6175 6c74  : float, default
+000114d0: 3d30 2e30 0a20 2020 2020 2020 2046 6561  =0.0.        Fea
+000114e0: 7475 7265 2076 616c 7565 7320 6265 6c6f  ture values belo
+000114f0: 7720 6f72 2065 7175 616c 2074 6f20 7468  w or equal to th
+00011500: 6973 2061 7265 2072 6570 6c61 6365 6420  is are replaced 
+00011510: 6279 2030 2c20 6162 6f76 6520 6974 2062  by 0, above it b
+00011520: 7920 312e 0a20 2020 2020 2020 2054 6872  y 1..        Thr
+00011530: 6573 686f 6c64 206d 6179 206e 6f74 2062  eshold may not b
+00011540: 6520 6c65 7373 2074 6861 6e20 3020 666f  e less than 0 fo
+00011550: 7220 6f70 6572 6174 696f 6e73 206f 6e20  r operations on 
+00011560: 7370 6172 7365 206d 6174 7269 6365 732e  sparse matrices.
+00011570: 0a0a 2020 2020 636f 7079 203a 2062 6f6f  ..    copy : boo
+00011580: 6c2c 2064 6566 6175 6c74 3d54 7275 650a  l, default=True.
+00011590: 2020 2020 2020 2020 4966 2046 616c 7365          If False
+000115a0: 2c20 7472 7920 746f 2061 766f 6964 2061  , try to avoid a
+000115b0: 2063 6f70 7920 616e 6420 6269 6e61 7269   copy and binari
+000115c0: 7a65 2069 6e20 706c 6163 652e 0a20 2020  ze in place..   
+000115d0: 2020 2020 2054 6869 7320 6973 206e 6f74       This is not
+000115e0: 2067 7561 7261 6e74 6565 6420 746f 2061   guaranteed to a
+000115f0: 6c77 6179 7320 776f 726b 2069 6e20 706c  lways work in pl
+00011600: 6163 653b 2065 2e67 2e20 6966 2074 6865  ace; e.g. if the
+00011610: 2064 6174 6120 6973 0a20 2020 2020 2020   data is.       
+00011620: 2061 206e 756d 7079 2061 7272 6179 2077   a numpy array w
+00011630: 6974 6820 616e 206f 626a 6563 7420 6474  ith an object dt
+00011640: 7970 652c 2061 2063 6f70 7920 7769 6c6c  ype, a copy will
+00011650: 2062 6520 7265 7475 726e 6564 2065 7665   be returned eve
+00011660: 6e20 7769 7468 0a20 2020 2020 2020 2063  n with.        c
+00011670: 6f70 793d 4661 6c73 652e 0a0a 2020 2020  opy=False...    
+00011680: 5265 7475 726e 730a 2020 2020 2d2d 2d2d  Returns.    ----
+00011690: 2d2d 2d0a 2020 2020 585f 7472 203a 207b  ---.    X_tr : {
+000116a0: 6e64 6172 7261 792c 2073 7061 7273 6520  ndarray, sparse 
+000116b0: 6d61 7472 6978 7d20 6f66 2073 6861 7065  matrix} of shape
+000116c0: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
+000116d0: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
+000116e0: 2054 6865 2074 7261 6e73 666f 726d 6564   The transformed
+000116f0: 2064 6174 612e 0a0a 2020 2020 5365 6520   data...    See 
+00011700: 416c 736f 0a20 2020 202d 2d2d 2d2d 2d2d  Also.    -------
+00011710: 2d0a 2020 2020 4269 6e61 7269 7a65 7220  -.    Binarizer 
+00011720: 3a20 5065 7266 6f72 6d73 2062 696e 6172  : Performs binar
+00011730: 697a 6174 696f 6e20 7573 696e 6720 7468  ization using th
+00011740: 6520 5472 616e 7366 6f72 6d65 7220 4150  e Transformer AP
+00011750: 490a 2020 2020 2020 2020 2865 2e67 2e20  I.        (e.g. 
+00011760: 6173 2070 6172 7420 6f66 2061 2070 7265  as part of a pre
+00011770: 7072 6f63 6573 7369 6e67 203a 636c 6173  processing :clas
+00011780: 733a 607e 736b 6c65 6172 6e2e 7069 7065  s:`~sklearn.pipe
+00011790: 6c69 6e65 2e50 6970 656c 696e 6560 292e  line.Pipeline`).
+000117a0: 0a0a 2020 2020 4578 616d 706c 6573 0a20  ..    Examples. 
+000117b0: 2020 202d 2d2d 2d2d 2d2d 2d0a 2020 2020     --------.    
+000117c0: 3e3e 3e20 6672 6f6d 2073 6b6c 6561 726e  >>> from sklearn
+000117d0: 2e70 7265 7072 6f63 6573 7369 6e67 2069  .preprocessing i
+000117e0: 6d70 6f72 7420 6269 6e61 7269 7a65 0a20  mport binarize. 
+000117f0: 2020 203e 3e3e 2058 203d 205b 5b30 2e34     >>> X = [[0.4
+00011800: 2c20 302e 362c 2030 2e35 5d2c 205b 302e  , 0.6, 0.5], [0.
+00011810: 362c 2030 2e31 2c20 302e 325d 5d0a 2020  6, 0.1, 0.2]].  
+00011820: 2020 3e3e 3e20 6269 6e61 7269 7a65 2858    >>> binarize(X
+00011830: 2c20 7468 7265 7368 6f6c 643d 302e 3529  , threshold=0.5)
+00011840: 0a20 2020 2061 7272 6179 285b 5b30 2e2c  .    array([[0.,
+00011850: 2031 2e2c 2030 2e5d 2c0a 2020 2020 2020   1., 0.],.      
+00011860: 2020 2020 205b 312e 2c20 302e 2c20 302e       [1., 0., 0.
+00011870: 5d5d 290a 2020 2020 2222 220a 2020 2020  ]]).    """.    
+00011880: 5820 3d20 6368 6563 6b5f 6172 7261 7928  X = check_array(
+00011890: 582c 2061 6363 6570 745f 7370 6172 7365  X, accept_sparse
+000118a0: 3d5b 2263 7372 222c 2022 6373 6322 5d2c  =["csr", "csc"],
+000118b0: 2063 6f70 793d 636f 7079 290a 2020 2020   copy=copy).    
+000118c0: 6966 2073 7061 7273 652e 6973 7370 6172  if sparse.isspar
+000118d0: 7365 2858 293a 0a20 2020 2020 2020 2069  se(X):.        i
+000118e0: 6620 7468 7265 7368 6f6c 6420 3c20 303a  f threshold < 0:
+000118f0: 0a20 2020 2020 2020 2020 2020 2072 6169  .            rai
+00011900: 7365 2056 616c 7565 4572 726f 7228 2243  se ValueError("C
+00011910: 616e 6e6f 7420 6269 6e61 7269 7a65 2061  annot binarize a
+00011920: 2073 7061 7273 6520 6d61 7472 6978 2077   sparse matrix w
+00011930: 6974 6820 7468 7265 7368 6f6c 6420 3c20  ith threshold < 
+00011940: 3022 290a 2020 2020 2020 2020 636f 6e64  0").        cond
+00011950: 203d 2058 2e64 6174 6120 3e20 7468 7265   = X.data > thre
+00011960: 7368 6f6c 640a 2020 2020 2020 2020 6e6f  shold.        no
+00011970: 745f 636f 6e64 203d 206e 702e 6c6f 6769  t_cond = np.logi
+00011980: 6361 6c5f 6e6f 7428 636f 6e64 290a 2020  cal_not(cond).  
+00011990: 2020 2020 2020 582e 6461 7461 5b63 6f6e        X.data[con
+000119a0: 645d 203d 2031 0a20 2020 2020 2020 2058  d] = 1.        X
+000119b0: 2e64 6174 615b 6e6f 745f 636f 6e64 5d20  .data[not_cond] 
+000119c0: 3d20 300a 2020 2020 2020 2020 582e 656c  = 0.        X.el
+000119d0: 696d 696e 6174 655f 7a65 726f 7328 290a  iminate_zeros().
+000119e0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
+000119f0: 2020 636f 6e64 203d 2058 203e 2074 6872    cond = X > thr
+00011a00: 6573 686f 6c64 0a20 2020 2020 2020 206e  eshold.        n
+00011a10: 6f74 5f63 6f6e 6420 3d20 6e70 2e6c 6f67  ot_cond = np.log
+00011a20: 6963 616c 5f6e 6f74 2863 6f6e 6429 0a20  ical_not(cond). 
+00011a30: 2020 2020 2020 2058 5b63 6f6e 645d 203d         X[cond] =
+00011a40: 2031 0a20 2020 2020 2020 2058 5b6e 6f74   1.        X[not
+00011a50: 5f63 6f6e 645d 203d 2030 0a20 2020 2072  _cond] = 0.    r
+00011a60: 6574 7572 6e20 580a 0a0a 636c 6173 7320  eturn X...class 
+00011a70: 4269 6e61 7269 7a65 7228 4f6e 6554 6f4f  Binarizer(OneToO
+00011a80: 6e65 4665 6174 7572 654d 6978 696e 2c20  neFeatureMixin, 
+00011a90: 5472 616e 7366 6f72 6d65 724d 6978 696e  TransformerMixin
+00011aa0: 2c20 4261 7365 4573 7469 6d61 746f 7229  , BaseEstimator)
+00011ab0: 3a0a 2020 2020 2222 2242 696e 6172 697a  :.    """Binariz
+00011ac0: 6520 6461 7461 2028 7365 7420 6665 6174  e data (set feat
+00011ad0: 7572 6520 7661 6c75 6573 2074 6f20 3020  ure values to 0 
+00011ae0: 6f72 2031 2920 6163 636f 7264 696e 6720  or 1) according 
+00011af0: 746f 2061 2074 6872 6573 686f 6c64 2e0a  to a threshold..
+00011b00: 0a20 2020 2056 616c 7565 7320 6772 6561  .    Values grea
+00011b10: 7465 7220 7468 616e 2074 6865 2074 6872  ter than the thr
+00011b20: 6573 686f 6c64 206d 6170 2074 6f20 312c  eshold map to 1,
+00011b30: 2077 6869 6c65 2076 616c 7565 7320 6c65   while values le
+00011b40: 7373 2074 6861 6e0a 2020 2020 6f72 2065  ss than.    or e
+00011b50: 7175 616c 2074 6f20 7468 6520 7468 7265  qual to the thre
+00011b60: 7368 6f6c 6420 6d61 7020 746f 2030 2e20  shold map to 0. 
+00011b70: 5769 7468 2074 6865 2064 6566 6175 6c74  With the default
+00011b80: 2074 6872 6573 686f 6c64 206f 6620 302c   threshold of 0,
+00011b90: 0a20 2020 206f 6e6c 7920 706f 7369 7469  .    only positi
+00011ba0: 7665 2076 616c 7565 7320 6d61 7020 746f  ve values map to
+00011bb0: 2031 2e0a 0a20 2020 2042 696e 6172 697a   1...    Binariz
+00011bc0: 6174 696f 6e20 6973 2061 2063 6f6d 6d6f  ation is a commo
+00011bd0: 6e20 6f70 6572 6174 696f 6e20 6f6e 2074  n operation on t
+00011be0: 6578 7420 636f 756e 7420 6461 7461 2077  ext count data w
+00011bf0: 6865 7265 2074 6865 0a20 2020 2061 6e61  here the.    ana
+00011c00: 6c79 7374 2063 616e 2064 6563 6964 6520  lyst can decide 
+00011c10: 746f 206f 6e6c 7920 636f 6e73 6964 6572  to only consider
+00011c20: 2074 6865 2070 7265 7365 6e63 6520 6f72   the presence or
+00011c30: 2061 6273 656e 6365 206f 6620 610a 2020   absence of a.  
+00011c40: 2020 6665 6174 7572 6520 7261 7468 6572    feature rather
+00011c50: 2074 6861 6e20 6120 7175 616e 7469 6669   than a quantifi
+00011c60: 6564 206e 756d 6265 7220 6f66 206f 6363  ed number of occ
+00011c70: 7572 7265 6e63 6573 2066 6f72 2069 6e73  urrences for ins
+00011c80: 7461 6e63 652e 0a0a 2020 2020 4974 2063  tance...    It c
+00011c90: 616e 2061 6c73 6f20 6265 2075 7365 6420  an also be used 
+00011ca0: 6173 2061 2070 7265 2d70 726f 6365 7373  as a pre-process
+00011cb0: 696e 6720 7374 6570 2066 6f72 2065 7374  ing step for est
+00011cc0: 696d 6174 6f72 7320 7468 6174 0a20 2020  imators that.   
+00011cd0: 2063 6f6e 7369 6465 7220 626f 6f6c 6561   consider boolea
+00011ce0: 6e20 7261 6e64 6f6d 2076 6172 6961 626c  n random variabl
+00011cf0: 6573 2028 652e 672e 206d 6f64 656c 6c65  es (e.g. modelle
+00011d00: 6420 7573 696e 6720 7468 6520 4265 726e  d using the Bern
+00011d10: 6f75 6c6c 690a 2020 2020 6469 7374 7269  oulli.    distri
+00011d20: 6275 7469 6f6e 2069 6e20 6120 4261 7965  bution in a Baye
+00011d30: 7369 616e 2073 6574 7469 6e67 292e 0a0a  sian setting)...
+00011d40: 2020 2020 5265 6164 206d 6f72 6520 696e      Read more in
+00011d50: 2074 6865 203a 7265 663a 6055 7365 7220   the :ref:`User 
+00011d60: 4775 6964 6520 3c70 7265 7072 6f63 6573  Guide <preproces
+00011d70: 7369 6e67 5f62 696e 6172 697a 6174 696f  sing_binarizatio
+00011d80: 6e3e 602e 0a0a 2020 2020 5061 7261 6d65  n>`...    Parame
+00011d90: 7465 7273 0a20 2020 202d 2d2d 2d2d 2d2d  ters.    -------
+00011da0: 2d2d 2d0a 2020 2020 7468 7265 7368 6f6c  ---.    threshol
+00011db0: 6420 3a20 666c 6f61 742c 2064 6566 6175  d : float, defau
+00011dc0: 6c74 3d30 2e30 0a20 2020 2020 2020 2046  lt=0.0.        F
+00011dd0: 6561 7475 7265 2076 616c 7565 7320 6265  eature values be
+00011de0: 6c6f 7720 6f72 2065 7175 616c 2074 6f20  low or equal to 
+00011df0: 7468 6973 2061 7265 2072 6570 6c61 6365  this are replace
+00011e00: 6420 6279 2030 2c20 6162 6f76 6520 6974  d by 0, above it
+00011e10: 2062 7920 312e 0a20 2020 2020 2020 2054   by 1..        T
+00011e20: 6872 6573 686f 6c64 206d 6179 206e 6f74  hreshold may not
+00011e30: 2062 6520 6c65 7373 2074 6861 6e20 3020   be less than 0 
+00011e40: 666f 7220 6f70 6572 6174 696f 6e73 206f  for operations o
+00011e50: 6e20 7370 6172 7365 206d 6174 7269 6365  n sparse matrice
+00011e60: 732e 0a0a 2020 2020 636f 7079 203a 2062  s...    copy : b
+00011e70: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
+00011e80: 650a 2020 2020 2020 2020 5365 7420 746f  e.        Set to
+00011e90: 2046 616c 7365 2074 6f20 7065 7266 6f72   False to perfor
+00011ea0: 6d20 696e 706c 6163 6520 6269 6e61 7269  m inplace binari
+00011eb0: 7a61 7469 6f6e 2061 6e64 2061 766f 6964  zation and avoid
+00011ec0: 2061 2063 6f70 7920 2869 660a 2020 2020   a copy (if.    
+00011ed0: 2020 2020 7468 6520 696e 7075 7420 6973      the input is
+00011ee0: 2061 6c72 6561 6479 2061 206e 756d 7079   already a numpy
+00011ef0: 2061 7272 6179 206f 7220 6120 7363 6970   array or a scip
+00011f00: 792e 7370 6172 7365 2043 5352 206d 6174  y.sparse CSR mat
+00011f10: 7269 7829 2e0a 0a20 2020 2041 7474 7269  rix)...    Attri
+00011f20: 6275 7465 730a 2020 2020 2d2d 2d2d 2d2d  butes.    ------
+00011f30: 2d2d 2d2d 0a20 2020 206e 5f66 6561 7475  ----.    n_featu
+00011f40: 7265 735f 696e 5f20 3a20 696e 740a 2020  res_in_ : int.  
+00011f50: 2020 2020 2020 4e75 6d62 6572 206f 6620        Number of 
+00011f60: 6665 6174 7572 6573 2073 6565 6e20 6475  features seen du
+00011f70: 7269 6e67 203a 7465 726d 3a60 6669 7460  ring :term:`fit`
+00011f80: 2e0a 0a20 2020 2020 2020 202e 2e20 7665  ...        .. ve
+00011f90: 7273 696f 6e61 6464 6564 3a3a 2030 2e32  rsionadded:: 0.2
+00011fa0: 340a 0a20 2020 2066 6561 7475 7265 5f6e  4..    feature_n
+00011fb0: 616d 6573 5f69 6e5f 203a 206e 6461 7272  ames_in_ : ndarr
+00011fc0: 6179 206f 6620 7368 6170 6520 2860 6e5f  ay of shape (`n_
+00011fd0: 6665 6174 7572 6573 5f69 6e5f 602c 290a  features_in_`,).
+00011fe0: 2020 2020 2020 2020 4e61 6d65 7320 6f66          Names of
+00011ff0: 2066 6561 7475 7265 7320 7365 656e 2064   features seen d
+00012000: 7572 696e 6720 3a74 6572 6d3a 6066 6974  uring :term:`fit
+00012010: 602e 2044 6566 696e 6564 206f 6e6c 7920  `. Defined only 
+00012020: 7768 656e 2060 5860 0a20 2020 2020 2020  when `X`.       
+00012030: 2068 6173 2066 6561 7475 7265 206e 616d   has feature nam
+00012040: 6573 2074 6861 7420 6172 6520 616c 6c20  es that are all 
+00012050: 7374 7269 6e67 732e 0a0a 2020 2020 2020  strings...      
+00012060: 2020 2e2e 2076 6572 7369 6f6e 6164 6465    .. versionadde
+00012070: 643a 3a20 312e 300a 0a20 2020 2053 6565  d:: 1.0..    See
+00012080: 2041 6c73 6f0a 2020 2020 2d2d 2d2d 2d2d   Also.    ------
+00012090: 2d2d 0a20 2020 2062 696e 6172 697a 6520  --.    binarize 
+000120a0: 3a20 4571 7569 7661 6c65 6e74 2066 756e  : Equivalent fun
+000120b0: 6374 696f 6e20 7769 7468 6f75 7420 7468  ction without th
+000120c0: 6520 6573 7469 6d61 746f 7220 4150 492e  e estimator API.
+000120d0: 0a20 2020 204b 4269 6e73 4469 7363 7265  .    KBinsDiscre
+000120e0: 7469 7a65 7220 3a20 4269 6e20 636f 6e74  tizer : Bin cont
+000120f0: 696e 756f 7573 2064 6174 6120 696e 746f  inuous data into
+00012100: 2069 6e74 6572 7661 6c73 2e0a 2020 2020   intervals..    
+00012110: 4f6e 6548 6f74 456e 636f 6465 7220 3a20  OneHotEncoder : 
+00012120: 456e 636f 6465 2063 6174 6567 6f72 6963  Encode categoric
+00012130: 616c 2066 6561 7475 7265 7320 6173 2061  al features as a
+00012140: 206f 6e65 2d68 6f74 206e 756d 6572 6963   one-hot numeric
+00012150: 2061 7272 6179 2e0a 0a20 2020 204e 6f74   array...    Not
+00012160: 6573 0a20 2020 202d 2d2d 2d2d 0a20 2020  es.    -----.   
+00012170: 2049 6620 7468 6520 696e 7075 7420 6973   If the input is
+00012180: 2061 2073 7061 7273 6520 6d61 7472 6978   a sparse matrix
+00012190: 2c20 6f6e 6c79 2074 6865 206e 6f6e 2d7a  , only the non-z
+000121a0: 6572 6f20 7661 6c75 6573 2061 7265 2073  ero values are s
+000121b0: 7562 6a65 6374 0a20 2020 2074 6f20 7570  ubject.    to up
+000121c0: 6461 7465 2062 7920 7468 6520 3a63 6c61  date by the :cla
+000121d0: 7373 3a60 4269 6e61 7269 7a65 7260 2063  ss:`Binarizer` c
+000121e0: 6c61 7373 2e0a 0a20 2020 2054 6869 7320  lass...    This 
+000121f0: 6573 7469 6d61 746f 7220 6973 203a 7465  estimator is :te
+00012200: 726d 3a60 7374 6174 656c 6573 7360 2061  rm:`stateless` a
+00012210: 6e64 2064 6f65 7320 6e6f 7420 6e65 6564  nd does not need
+00012220: 2074 6f20 6265 2066 6974 7465 642e 0a20   to be fitted.. 
+00012230: 2020 2048 6f77 6576 6572 2c20 7765 2072     However, we r
+00012240: 6563 6f6d 6d65 6e64 2074 6f20 6361 6c6c  ecommend to call
+00012250: 203a 6d65 7468 3a60 6669 745f 7472 616e   :meth:`fit_tran
+00012260: 7366 6f72 6d60 2069 6e73 7465 6164 206f  sform` instead o
+00012270: 660a 2020 2020 3a6d 6574 683a 6074 7261  f.    :meth:`tra
+00012280: 6e73 666f 726d 602c 2061 7320 7061 7261  nsform`, as para
+00012290: 6d65 7465 7220 7661 6c69 6461 7469 6f6e  meter validation
+000122a0: 2069 7320 6f6e 6c79 2070 6572 666f 726d   is only perform
+000122b0: 6564 2069 6e0a 2020 2020 3a6d 6574 683a  ed in.    :meth:
+000122c0: 6066 6974 602e 0a0a 2020 2020 4578 616d  `fit`...    Exam
+000122d0: 706c 6573 0a20 2020 202d 2d2d 2d2d 2d2d  ples.    -------
+000122e0: 2d0a 2020 2020 3e3e 3e20 6672 6f6d 2073  -.    >>> from s
+000122f0: 6b6c 6561 726e 2e70 7265 7072 6f63 6573  klearn.preproces
+00012300: 7369 6e67 2069 6d70 6f72 7420 4269 6e61  sing import Bina
+00012310: 7269 7a65 720a 2020 2020 3e3e 3e20 5820  rizer.    >>> X 
+00012320: 3d20 5b5b 2031 2e2c 202d 312e 2c20 2032  = [[ 1., -1.,  2
+00012330: 2e5d 2c0a 2020 2020 2e2e 2e20 2020 2020  .],.    ...     
+00012340: 205b 2032 2e2c 2020 302e 2c20 2030 2e5d   [ 2.,  0.,  0.]
+00012350: 2c0a 2020 2020 2e2e 2e20 2020 2020 205b  ,.    ...      [
+00012360: 2030 2e2c 2020 312e 2c20 2d31 2e5d 5d0a   0.,  1., -1.]].
+00012370: 2020 2020 3e3e 3e20 7472 616e 7366 6f72      >>> transfor
+00012380: 6d65 7220 3d20 4269 6e61 7269 7a65 7228  mer = Binarizer(
+00012390: 292e 6669 7428 5829 2020 2320 6669 7420  ).fit(X)  # fit 
+000123a0: 646f 6573 206e 6f74 6869 6e67 2e0a 2020  does nothing..  
+000123b0: 2020 3e3e 3e20 7472 616e 7366 6f72 6d65    >>> transforme
+000123c0: 720a 2020 2020 4269 6e61 7269 7a65 7228  r.    Binarizer(
+000123d0: 290a 2020 2020 3e3e 3e20 7472 616e 7366  ).    >>> transf
+000123e0: 6f72 6d65 722e 7472 616e 7366 6f72 6d28  ormer.transform(
+000123f0: 5829 0a20 2020 2061 7272 6179 285b 5b31  X).    array([[1
+00012400: 2e2c 2030 2e2c 2031 2e5d 2c0a 2020 2020  ., 0., 1.],.    
+00012410: 2020 2020 2020 205b 312e 2c20 302e 2c20         [1., 0., 
+00012420: 302e 5d2c 0a20 2020 2020 2020 2020 2020  0.],.           
+00012430: 5b30 2e2c 2031 2e2c 2030 2e5d 5d29 0a20  [0., 1., 0.]]). 
+00012440: 2020 2022 2222 0a0a 2020 2020 5f70 6172     """..    _par
+00012450: 616d 6574 6572 5f63 6f6e 7374 7261 696e  ameter_constrain
+00012460: 7473 3a20 6469 6374 203d 207b 0a20 2020  ts: dict = {.   
+00012470: 2020 2020 2022 7468 7265 7368 6f6c 6422       "threshold"
+00012480: 3a20 5b52 6561 6c5d 2c0a 2020 2020 2020  : [Real],.      
+00012490: 2020 2263 6f70 7922 3a20 5b22 626f 6f6c    "copy": ["bool
+000124a0: 6561 6e22 5d2c 0a20 2020 207d 0a0a 2020  ean"],.    }..  
+000124b0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+000124c0: 656c 662c 202a 2c20 7468 7265 7368 6f6c  elf, *, threshol
+000124d0: 643d 302e 302c 2063 6f70 793d 5472 7565  d=0.0, copy=True
+000124e0: 293a 0a20 2020 2020 2020 2073 656c 662e  ):.        self.
+000124f0: 7468 7265 7368 6f6c 6420 3d20 7468 7265  threshold = thre
+00012500: 7368 6f6c 640a 2020 2020 2020 2020 7365  shold.        se
+00012510: 6c66 2e63 6f70 7920 3d20 636f 7079 0a0a  lf.copy = copy..
+00012520: 2020 2020 405f 6669 745f 636f 6e74 6578      @_fit_contex
+00012530: 7428 7072 6566 6572 5f73 6b69 705f 6e65  t(prefer_skip_ne
+00012540: 7374 6564 5f76 616c 6964 6174 696f 6e3d  sted_validation=
+00012550: 5472 7565 290a 2020 2020 6465 6620 6669  True).    def fi
+00012560: 7428 7365 6c66 2c20 582c 2079 3d4e 6f6e  t(self, X, y=Non
+00012570: 6529 3a0a 2020 2020 2020 2020 2222 224f  e):.        """O
+00012580: 6e6c 7920 7661 6c69 6461 7465 7320 6573  nly validates es
+00012590: 7469 6d61 746f 7227 7320 7061 7261 6d65  timator's parame
+000125a0: 7465 7273 2e0a 0a20 2020 2020 2020 2054  ters...        T
+000125b0: 6869 7320 6d65 7468 6f64 2061 6c6c 6f77  his method allow
+000125c0: 7320 746f 3a20 2869 2920 7661 6c69 6461  s to: (i) valida
+000125d0: 7465 2074 6865 2065 7374 696d 6174 6f72  te the estimator
+000125e0: 2773 2070 6172 616d 6574 6572 7320 616e  's parameters an
+000125f0: 640a 2020 2020 2020 2020 2869 6929 2062  d.        (ii) b
+00012600: 6520 636f 6e73 6973 7465 6e74 2077 6974  e consistent wit
+00012610: 6820 7468 6520 7363 696b 6974 2d6c 6561  h the scikit-lea
+00012620: 726e 2074 7261 6e73 666f 726d 6572 2041  rn transformer A
+00012630: 5049 2e0a 0a20 2020 2020 2020 2050 6172  PI...        Par
+00012640: 616d 6574 6572 730a 2020 2020 2020 2020  ameters.        
+00012650: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
+00012660: 2020 2058 203a 207b 6172 7261 792d 6c69     X : {array-li
+00012670: 6b65 2c20 7370 6172 7365 206d 6174 7269  ke, sparse matri
+00012680: 787d 206f 6620 7368 6170 6520 286e 5f73  x} of shape (n_s
+00012690: 616d 706c 6573 2c20 6e5f 6665 6174 7572  amples, n_featur
+000126a0: 6573 290a 2020 2020 2020 2020 2020 2020  es).            
+000126b0: 5468 6520 6461 7461 2e0a 0a20 2020 2020  The data...     
+000126c0: 2020 2079 203a 204e 6f6e 650a 2020 2020     y : None.    
+000126d0: 2020 2020 2020 2020 4967 6e6f 7265 642e          Ignored.
+000126e0: 0a0a 2020 2020 2020 2020 5265 7475 726e  ..        Return
+000126f0: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
+00012700: 2d0a 2020 2020 2020 2020 7365 6c66 203a  -.        self :
+00012710: 206f 626a 6563 740a 2020 2020 2020 2020   object.        
+00012720: 2020 2020 4669 7474 6564 2074 7261 6e73      Fitted trans
+00012730: 666f 726d 6572 2e0a 2020 2020 2020 2020  former..        
+00012740: 2222 220a 2020 2020 2020 2020 7365 6c66  """.        self
+00012750: 2e5f 7661 6c69 6461 7465 5f64 6174 6128  ._validate_data(
+00012760: 582c 2061 6363 6570 745f 7370 6172 7365  X, accept_sparse
+00012770: 3d22 6373 7222 290a 2020 2020 2020 2020  ="csr").        
+00012780: 7265 7475 726e 2073 656c 660a 0a20 2020  return self..   
+00012790: 2064 6566 2074 7261 6e73 666f 726d 2873   def transform(s
+000127a0: 656c 662c 2058 2c20 636f 7079 3d4e 6f6e  elf, X, copy=Non
+000127b0: 6529 3a0a 2020 2020 2020 2020 2222 2242  e):.        """B
+000127c0: 696e 6172 697a 6520 6561 6368 2065 6c65  inarize each ele
+000127d0: 6d65 6e74 206f 6620 582e 0a0a 2020 2020  ment of X...    
+000127e0: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
+000127f0: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
+00012800: 2d0a 2020 2020 2020 2020 5820 3a20 7b61  -.        X : {a
+00012810: 7272 6179 2d6c 696b 652c 2073 7061 7273  rray-like, spars
+00012820: 6520 6d61 7472 6978 7d20 6f66 2073 6861  e matrix} of sha
+00012830: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
+00012840: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
+00012850: 2020 2020 2020 2054 6865 2064 6174 6120         The data 
+00012860: 746f 2062 696e 6172 697a 652c 2065 6c65  to binarize, ele
+00012870: 6d65 6e74 2062 7920 656c 656d 656e 742e  ment by element.
+00012880: 0a20 2020 2020 2020 2020 2020 2073 6369  .            sci
+00012890: 7079 2e73 7061 7273 6520 6d61 7472 6963  py.sparse matric
+000128a0: 6573 2073 686f 756c 6420 6265 2069 6e20  es should be in 
+000128b0: 4353 5220 666f 726d 6174 2074 6f20 6176  CSR format to av
+000128c0: 6f69 6420 616e 0a20 2020 2020 2020 2020  oid an.         
+000128d0: 2020 2075 6e2d 6e65 6365 7373 6172 7920     un-necessary 
+000128e0: 636f 7079 2e0a 0a20 2020 2020 2020 2063  copy...        c
+000128f0: 6f70 7920 3a20 626f 6f6c 0a20 2020 2020  opy : bool.     
+00012900: 2020 2020 2020 2043 6f70 7920 7468 6520         Copy the 
+00012910: 696e 7075 7420 5820 6f72 206e 6f74 2e0a  input X or not..
+00012920: 0a20 2020 2020 2020 2052 6574 7572 6e73  .        Returns
+00012930: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+00012940: 0a20 2020 2020 2020 2058 5f74 7220 3a20  .        X_tr : 
+00012950: 7b6e 6461 7272 6179 2c20 7370 6172 7365  {ndarray, sparse
+00012960: 206d 6174 7269 787d 206f 6620 7368 6170   matrix} of shap
+00012970: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
+00012980: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
+00012990: 2020 2020 2020 5472 616e 7366 6f72 6d65        Transforme
+000129a0: 6420 6172 7261 792e 0a20 2020 2020 2020  d array..       
+000129b0: 2022 2222 0a20 2020 2020 2020 2063 6f70   """.        cop
+000129c0: 7920 3d20 636f 7079 2069 6620 636f 7079  y = copy if copy
+000129d0: 2069 7320 6e6f 7420 4e6f 6e65 2065 6c73   is not None els
+000129e0: 6520 7365 6c66 2e63 6f70 790a 2020 2020  e self.copy.    
+000129f0: 2020 2020 2320 544f 444f 3a20 5468 6973      # TODO: This
+00012a00: 2073 686f 756c 6420 6265 2072 6566 6163   should be refac
+00012a10: 746f 7265 6420 6265 6361 7573 6520 6269  tored because bi
+00012a20: 6e61 7269 7a65 2061 6c73 6f20 6361 6c6c  narize also call
+00012a30: 730a 2020 2020 2020 2020 2320 6368 6563  s.        # chec
+00012a40: 6b5f 6172 7261 790a 2020 2020 2020 2020  k_array.        
+00012a50: 5820 3d20 7365 6c66 2e5f 7661 6c69 6461  X = self._valida
+00012a60: 7465 5f64 6174 6128 582c 2061 6363 6570  te_data(X, accep
+00012a70: 745f 7370 6172 7365 3d5b 2263 7372 222c  t_sparse=["csr",
+00012a80: 2022 6373 6322 5d2c 2063 6f70 793d 636f   "csc"], copy=co
+00012a90: 7079 2c20 7265 7365 743d 4661 6c73 6529  py, reset=False)
+00012aa0: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00012ab0: 6269 6e61 7269 7a65 2858 2c20 7468 7265  binarize(X, thre
+00012ac0: 7368 6f6c 643d 7365 6c66 2e74 6872 6573  shold=self.thres
+00012ad0: 686f 6c64 2c20 636f 7079 3d46 616c 7365  hold, copy=False
+00012ae0: 290a 0a20 2020 2064 6566 205f 6d6f 7265  )..    def _more
+00012af0: 5f74 6167 7328 7365 6c66 293a 0a20 2020  _tags(self):.   
+00012b00: 2020 2020 2072 6574 7572 6e20 7b22 7374       return {"st
+00012b10: 6174 656c 6573 7322 3a20 5472 7565 7d0a  ateless": True}.
+00012b20: 0a0a 636c 6173 7320 4b65 726e 656c 4365  ..class KernelCe
+00012b30: 6e74 6572 6572 2843 6c61 7373 4e61 6d65  nterer(ClassName
+00012b40: 5072 6566 6978 4665 6174 7572 6573 4f75  PrefixFeaturesOu
+00012b50: 744d 6978 696e 2c20 5472 616e 7366 6f72  tMixin, Transfor
+00012b60: 6d65 724d 6978 696e 2c20 4261 7365 4573  merMixin, BaseEs
+00012b70: 7469 6d61 746f 7229 3a0a 2020 2020 7222  timator):.    r"
+00012b80: 2222 4365 6e74 6572 2061 6e20 6172 6269  ""Center an arbi
+00012b90: 7472 6172 7920 6b65 726e 656c 206d 6174  trary kernel mat
+00012ba0: 7269 7820 3a6d 6174 683a 604b 602e 0a0a  rix :math:`K`...
+00012bb0: 2020 2020 4c65 7420 6465 6669 6e65 2061      Let define a
+00012bc0: 206b 6572 6e65 6c20 3a6d 6174 683a 604b   kernel :math:`K
+00012bd0: 6020 7375 6368 2074 6861 743a 0a0a 2020  ` such that:..  
+00012be0: 2020 2e2e 206d 6174 683a 3a0a 2020 2020    .. math::.    
+00012bf0: 2020 2020 4b28 582c 2059 2920 3d20 5c70      K(X, Y) = \p
+00012c00: 6869 2858 2920 2e20 5c70 6869 2859 295e  hi(X) . \phi(Y)^
+00012c10: 7b54 7d0a 0a20 2020 203a 6d61 7468 3a60  {T}..    :math:`
+00012c20: 5c70 6869 2858 2960 2069 7320 6120 6675  \phi(X)` is a fu
+00012c30: 6e63 7469 6f6e 206d 6170 7069 6e67 206f  nction mapping o
+00012c40: 6620 726f 7773 206f 6620 3a6d 6174 683a  f rows of :math:
+00012c50: 6058 6020 746f 2061 0a20 2020 2048 696c  `X` to a.    Hil
+00012c60: 6265 7274 2073 7061 6365 2061 6e64 203a  bert space and :
+00012c70: 6d61 7468 3a60 4b60 2069 7320 6f66 2073  math:`K` is of s
+00012c80: 6861 7065 2060 286e 5f73 616d 706c 6573  hape `(n_samples
+00012c90: 2c20 6e5f 7361 6d70 6c65 7329 602e 0a0a  , n_samples)`...
+00012ca0: 2020 2020 5468 6973 2063 6c61 7373 2061      This class a
+00012cb0: 6c6c 6f77 7320 746f 2063 6f6d 7075 7465  llows to compute
+00012cc0: 203a 6d61 7468 3a60 5c74 696c 6465 7b4b   :math:`\tilde{K
+00012cd0: 7d28 582c 2059 2960 2073 7563 6820 7468  }(X, Y)` such th
+00012ce0: 6174 3a0a 0a20 2020 202e 2e20 6d61 7468  at:..    .. math
+00012cf0: 3a3a 0a20 2020 2020 2020 205c 7469 6c64  ::.        \tild
+00012d00: 657b 4b28 582c 2059 297d 203d 205c 7469  e{K(X, Y)} = \ti
+00012d10: 6c64 657b 5c70 6869 7d28 5829 202e 205c  lde{\phi}(X) . \
+00012d20: 7469 6c64 657b 5c70 6869 7d28 5929 5e7b  tilde{\phi}(Y)^{
+00012d30: 547d 0a0a 2020 2020 3a6d 6174 683a 605c  T}..    :math:`\
+00012d40: 7469 6c64 657b 5c70 6869 7d28 5829 6020  tilde{\phi}(X)` 
+00012d50: 6973 2074 6865 2063 656e 7465 7265 6420  is the centered 
+00012d60: 6d61 7070 6564 2064 6174 6120 696e 2074  mapped data in t
+00012d70: 6865 2048 696c 6265 7274 0a20 2020 2073  he Hilbert.    s
+00012d80: 7061 6365 2e0a 0a20 2020 2060 4b65 726e  pace...    `Kern
+00012d90: 656c 4365 6e74 6572 6572 6020 6365 6e74  elCenterer` cent
+00012da0: 6572 7320 7468 6520 6665 6174 7572 6573  ers the features
+00012db0: 2077 6974 686f 7574 2065 7870 6c69 6369   without explici
+00012dc0: 746c 7920 636f 6d70 7574 696e 6720 7468  tly computing th
+00012dd0: 650a 2020 2020 6d61 7070 696e 6720 3a6d  e.    mapping :m
+00012de0: 6174 683a 605c 7068 6928 5c63 646f 7429  ath:`\phi(\cdot)
+00012df0: 602e 2057 6f72 6b69 6e67 2077 6974 6820  `. Working with 
+00012e00: 6365 6e74 6572 6564 206b 6572 6e65 6c73  centered kernels
+00012e10: 2069 7320 736f 6d65 7469 6d65 0a20 2020   is sometime.   
+00012e20: 2065 7870 6563 7465 6420 7768 656e 2064   expected when d
+00012e30: 6561 6c69 6e67 2077 6974 6820 616c 6765  ealing with alge
+00012e40: 6272 6120 636f 6d70 7574 6174 696f 6e20  bra computation 
+00012e50: 7375 6368 2061 7320 6569 6765 6e64 6563  such as eigendec
+00012e60: 6f6d 706f 7369 7469 6f6e 0a20 2020 2066  omposition.    f
+00012e70: 6f72 203a 636c 6173 733a 607e 736b 6c65  or :class:`~skle
+00012e80: 6172 6e2e 6465 636f 6d70 6f73 6974 696f  arn.decompositio
+00012e90: 6e2e 4b65 726e 656c 5043 4160 2066 6f72  n.KernelPCA` for
+00012ea0: 2069 6e73 7461 6e63 652e 0a0a 2020 2020   instance...    
+00012eb0: 5265 6164 206d 6f72 6520 696e 2074 6865  Read more in the
+00012ec0: 203a 7265 663a 6055 7365 7220 4775 6964   :ref:`User Guid
+00012ed0: 6520 3c6b 6572 6e65 6c5f 6365 6e74 6572  e <kernel_center
+00012ee0: 696e 673e 602e 0a0a 2020 2020 4174 7472  ing>`...    Attr
+00012ef0: 6962 7574 6573 0a20 2020 202d 2d2d 2d2d  ibutes.    -----
+00012f00: 2d2d 2d2d 2d0a 2020 2020 4b5f 6669 745f  -----.    K_fit_
+00012f10: 726f 7773 5f20 3a20 6e64 6172 7261 7920  rows_ : ndarray 
+00012f20: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
+00012f30: 6c65 732c 290a 2020 2020 2020 2020 4176  les,).        Av
+00012f40: 6572 6167 6520 6f66 2065 6163 6820 636f  erage of each co
+00012f50: 6c75 6d6e 206f 6620 6b65 726e 656c 206d  lumn of kernel m
+00012f60: 6174 7269 782e 0a0a 2020 2020 4b5f 6669  atrix...    K_fi
+00012f70: 745f 616c 6c5f 203a 2066 6c6f 6174 0a20  t_all_ : float. 
+00012f80: 2020 2020 2020 2041 7665 7261 6765 206f         Average o
+00012f90: 6620 6b65 726e 656c 206d 6174 7269 782e  f kernel matrix.
+00012fa0: 0a0a 2020 2020 6e5f 6665 6174 7572 6573  ..    n_features
+00012fb0: 5f69 6e5f 203a 2069 6e74 0a20 2020 2020  _in_ : int.     
+00012fc0: 2020 204e 756d 6265 7220 6f66 2066 6561     Number of fea
+00012fd0: 7475 7265 7320 7365 656e 2064 7572 696e  tures seen durin
+00012fe0: 6720 3a74 6572 6d3a 6066 6974 602e 0a0a  g :term:`fit`...
+00012ff0: 2020 2020 2020 2020 2e2e 2076 6572 7369          .. versi
+00013000: 6f6e 6164 6465 643a 3a20 302e 3234 0a0a  onadded:: 0.24..
+00013010: 2020 2020 6665 6174 7572 655f 6e61 6d65      feature_name
+00013020: 735f 696e 5f20 3a20 6e64 6172 7261 7920  s_in_ : ndarray 
+00013030: 6f66 2073 6861 7065 2028 606e 5f66 6561  of shape (`n_fea
+00013040: 7475 7265 735f 696e 5f60 2c29 0a20 2020  tures_in_`,).   
+00013050: 2020 2020 204e 616d 6573 206f 6620 6665       Names of fe
+00013060: 6174 7572 6573 2073 6565 6e20 6475 7269  atures seen duri
+00013070: 6e67 203a 7465 726d 3a60 6669 7460 2e20  ng :term:`fit`. 
+00013080: 4465 6669 6e65 6420 6f6e 6c79 2077 6865  Defined only whe
+00013090: 6e20 6058 600a 2020 2020 2020 2020 6861  n `X`.        ha
+000130a0: 7320 6665 6174 7572 6520 6e61 6d65 7320  s feature names 
+000130b0: 7468 6174 2061 7265 2061 6c6c 2073 7472  that are all str
+000130c0: 696e 6773 2e0a 0a20 2020 2020 2020 202e  ings...        .
+000130d0: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
+000130e0: 2031 2e30 0a0a 2020 2020 5365 6520 416c   1.0..    See Al
+000130f0: 736f 0a20 2020 202d 2d2d 2d2d 2d2d 2d0a  so.    --------.
+00013100: 2020 2020 736b 6c65 6172 6e2e 6b65 726e      sklearn.kern
+00013110: 656c 5f61 7070 726f 7869 6d61 7469 6f6e  el_approximation
+00013120: 2e4e 7973 7472 6f65 6d20 3a20 4170 7072  .Nystroem : Appr
+00013130: 6f78 696d 6174 6520 6120 6b65 726e 656c  oximate a kernel
+00013140: 206d 6170 0a20 2020 2020 2020 2075 7369   map.        usi
+00013150: 6e67 2061 2073 7562 7365 7420 6f66 2074  ng a subset of t
+00013160: 6865 2074 7261 696e 696e 6720 6461 7461  he training data
+00013170: 2e0a 0a20 2020 2052 6566 6572 656e 6365  ...    Reference
+00013180: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
+00013190: 0a20 2020 202e 2e20 5b31 5d20 6053 6368  .    .. [1] `Sch
+000131a0: c3b6 6c6b 6f70 662c 2042 6572 6e68 6172  ..lkopf, Bernhar
+000131b0: 642c 2041 6c65 7861 6e64 6572 2053 6d6f  d, Alexander Smo
+000131c0: 6c61 2c20 616e 6420 4b6c 6175 732d 526f  la, and Klaus-Ro
+000131d0: 6265 7274 204d c3bc 6c6c 6572 2e0a 2020  bert M..ller..  
+000131e0: 2020 2020 2022 4e6f 6e6c 696e 6561 7220       "Nonlinear 
+000131f0: 636f 6d70 6f6e 656e 7420 616e 616c 7973  component analys
+00013200: 6973 2061 7320 6120 6b65 726e 656c 2065  is as a kernel e
+00013210: 6967 656e 7661 6c75 6520 7072 6f62 6c65  igenvalue proble
+00013220: 6d2e 220a 2020 2020 2020 204e 6575 7261  m.".       Neura
+00013230: 6c20 636f 6d70 7574 6174 696f 6e20 3130  l computation 10
+00013240: 2e35 2028 3139 3938 293a 2031 3239 392d  .5 (1998): 1299-
+00013250: 3133 3139 2e0a 2020 2020 2020 203c 6874  1319..       <ht
+00013260: 7470 733a 2f2f 7777 772e 6d6c 7061 636b  tps://www.mlpack
+00013270: 2e6f 7267 2f70 6170 6572 732f 6b70 6361  .org/papers/kpca
+00013280: 2e70 6466 3e60 5f0a 0a20 2020 2045 7861  .pdf>`_..    Exa
+00013290: 6d70 6c65 730a 2020 2020 2d2d 2d2d 2d2d  mples.    ------
+000132a0: 2d2d 0a20 2020 203e 3e3e 2066 726f 6d20  --.    >>> from 
+000132b0: 736b 6c65 6172 6e2e 7072 6570 726f 6365  sklearn.preproce
+000132c0: 7373 696e 6720 696d 706f 7274 204b 6572  ssing import Ker
+000132d0: 6e65 6c43 656e 7465 7265 720a 2020 2020  nelCenterer.    
+000132e0: 3e3e 3e20 6672 6f6d 2073 6b6c 6561 726e  >>> from sklearn
+000132f0: 2e6d 6574 7269 6373 2e70 6169 7277 6973  .metrics.pairwis
+00013300: 6520 696d 706f 7274 2070 6169 7277 6973  e import pairwis
+00013310: 655f 6b65 726e 656c 730a 2020 2020 3e3e  e_kernels.    >>
+00013320: 3e20 5820 3d20 5b5b 2031 2e2c 202d 322e  > X = [[ 1., -2.
+00013330: 2c20 2032 2e5d 2c0a 2020 2020 2e2e 2e20  ,  2.],.    ... 
+00013340: 2020 2020 205b 202d 322e 2c20 2031 2e2c       [ -2.,  1.,
+00013350: 2020 332e 5d2c 0a20 2020 202e 2e2e 2020    3.],.    ...  
+00013360: 2020 2020 5b20 342e 2c20 2031 2e2c 202d      [ 4.,  1., -
+00013370: 322e 5d5d 0a20 2020 203e 3e3e 204b 203d  2.]].    >>> K =
+00013380: 2070 6169 7277 6973 655f 6b65 726e 656c   pairwise_kernel
+00013390: 7328 582c 206d 6574 7269 633d 276c 696e  s(X, metric='lin
+000133a0: 6561 7227 290a 2020 2020 3e3e 3e20 4b0a  ear').    >>> K.
+000133b0: 2020 2020 6172 7261 7928 5b5b 2020 392e      array([[  9.
+000133c0: 2c20 2020 322e 2c20 202d 322e 5d2c 0a20  ,   2.,  -2.],. 
+000133d0: 2020 2020 2020 2020 2020 5b20 2032 2e2c            [  2.,
+000133e0: 2020 3134 2e2c 202d 3133 2e5d 2c0a 2020    14., -13.],.  
+000133f0: 2020 2020 2020 2020 205b 202d 322e 2c20           [ -2., 
+00013400: 2d31 332e 2c20 2032 312e 5d5d 290a 2020  -13.,  21.]]).  
+00013410: 2020 3e3e 3e20 7472 616e 7366 6f72 6d65    >>> transforme
+00013420: 7220 3d20 4b65 726e 656c 4365 6e74 6572  r = KernelCenter
+00013430: 6572 2829 2e66 6974 284b 290a 2020 2020  er().fit(K).    
+00013440: 3e3e 3e20 7472 616e 7366 6f72 6d65 720a  >>> transformer.
+00013450: 2020 2020 4b65 726e 656c 4365 6e74 6572      KernelCenter
+00013460: 6572 2829 0a20 2020 203e 3e3e 2074 7261  er().    >>> tra
+00013470: 6e73 666f 726d 6572 2e74 7261 6e73 666f  nsformer.transfo
+00013480: 726d 284b 290a 2020 2020 6172 7261 7928  rm(K).    array(
+00013490: 5b5b 2020 352e 2c20 2020 302e 2c20 202d  [[  5.,   0.,  -
+000134a0: 352e 5d2c 0a20 2020 2020 2020 2020 2020  5.],.           
+000134b0: 5b20 2030 2e2c 2020 3134 2e2c 202d 3134  [  0.,  14., -14
+000134c0: 2e5d 2c0a 2020 2020 2020 2020 2020 205b  .],.           [
+000134d0: 202d 352e 2c20 2d31 342e 2c20 2031 392e   -5., -14.,  19.
+000134e0: 5d5d 290a 2020 2020 2222 220a 0a20 2020  ]]).    """..   
+000134f0: 2064 6566 205f 5f69 6e69 745f 5f28 7365   def __init__(se
+00013500: 6c66 293a 0a20 2020 2020 2020 2023 204e  lf):.        # N
+00013510: 6565 6465 6420 666f 7220 6261 636b 706f  eeded for backpo
+00013520: 7274 6564 2069 6e73 7065 6374 2e73 6967  rted inspect.sig
+00013530: 6e61 7475 7265 2063 6f6d 7061 7469 6269  nature compatibi
+00013540: 6c69 7479 2077 6974 6820 5079 5079 0a20  lity with PyPy. 
+00013550: 2020 2020 2020 2070 6173 730a 0a20 2020         pass..   
+00013560: 2064 6566 2066 6974 2873 656c 662c 204b   def fit(self, K
+00013570: 2c20 793d 4e6f 6e65 293a 0a20 2020 2020  , y=None):.     
+00013580: 2020 2022 2222 4669 7420 4b65 726e 656c     """Fit Kernel
+00013590: 4365 6e74 6572 6572 2e0a 0a20 2020 2020  Centerer...     
+000135a0: 2020 2050 6172 616d 6574 6572 730a 2020     Parameters.  
+000135b0: 2020 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d        ----------
+000135c0: 0a20 2020 2020 2020 204b 203a 206e 6461  .        K : nda
+000135d0: 7272 6179 206f 6620 7368 6170 6520 286e  rray of shape (n
+000135e0: 5f73 616d 706c 6573 2c20 6e5f 7361 6d70  _samples, n_samp
+000135f0: 6c65 7329 0a20 2020 2020 2020 2020 2020  les).           
+00013600: 204b 6572 6e65 6c20 6d61 7472 6978 2e0a   Kernel matrix..
+00013610: 0a20 2020 2020 2020 2079 203a 204e 6f6e  .        y : Non
+00013620: 650a 2020 2020 2020 2020 2020 2020 4967  e.            Ig
+00013630: 6e6f 7265 642e 0a0a 2020 2020 2020 2020  nored...        
+00013640: 5265 7475 726e 730a 2020 2020 2020 2020  Returns.        
+00013650: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+00013660: 7365 6c66 203a 206f 626a 6563 740a 2020  self : object.  
+00013670: 2020 2020 2020 2020 2020 5265 7475 726e            Return
+00013680: 7320 7468 6520 696e 7374 616e 6365 2069  s the instance i
+00013690: 7473 656c 662e 0a20 2020 2020 2020 2022  tself..        "
+000136a0: 2222 0a20 2020 2020 2020 2078 702c 205f  "".        xp, _
+000136b0: 203d 2067 6574 5f6e 616d 6573 7061 6365   = get_namespace
+000136c0: 284b 290a 0a20 2020 2020 2020 204b 203d  (K)..        K =
+000136d0: 2073 656c 662e 5f76 616c 6964 6174 655f   self._validate_
+000136e0: 6461 7461 284b 2c20 6474 7970 653d 5f61  data(K, dtype=_a
+000136f0: 7272 6179 5f61 7069 2e73 7570 706f 7274  rray_api.support
+00013700: 6564 5f66 6c6f 6174 5f64 7479 7065 7328  ed_float_dtypes(
+00013710: 7870 2929 0a0a 2020 2020 2020 2020 6966  xp))..        if
+00013720: 204b 2e73 6861 7065 5b30 5d20 213d 204b   K.shape[0] != K
+00013730: 2e73 6861 7065 5b31 5d3a 0a20 2020 2020  .shape[1]:.     
+00013740: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+00013750: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
+00013760: 2020 2020 2020 2020 2022 4b65 726e 656c           "Kernel
+00013770: 206d 6174 7269 7820 6d75 7374 2062 6520   matrix must be 
+00013780: 6120 7371 7561 7265 206d 6174 7269 782e  a square matrix.
+00013790: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+000137a0: 2020 2220 496e 7075 7420 6973 2061 207b    " Input is a {
+000137b0: 7d78 7b7d 206d 6174 7269 782e 222e 666f  }x{} matrix.".fo
+000137c0: 726d 6174 284b 2e73 6861 7065 5b30 5d2c  rmat(K.shape[0],
+000137d0: 204b 2e73 6861 7065 5b31 5d29 0a20 2020   K.shape[1]).   
+000137e0: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+000137f0: 2020 2020 6e5f 7361 6d70 6c65 7320 3d20      n_samples = 
+00013800: 4b2e 7368 6170 655b 305d 0a20 2020 2020  K.shape[0].     
+00013810: 2020 2073 656c 662e 4b5f 6669 745f 726f     self.K_fit_ro
+00013820: 7773 5f20 3d20 7870 2e73 756d 284b 2c20  ws_ = xp.sum(K, 
+00013830: 6178 6973 3d30 2920 2f20 6e5f 7361 6d70  axis=0) / n_samp
+00013840: 6c65 730a 2020 2020 2020 2020 7365 6c66  les.        self
+00013850: 2e4b 5f66 6974 5f61 6c6c 5f20 3d20 7870  .K_fit_all_ = xp
+00013860: 2e73 756d 2873 656c 662e 4b5f 6669 745f  .sum(self.K_fit_
+00013870: 726f 7773 5f29 202f 206e 5f73 616d 706c  rows_) / n_sampl
+00013880: 6573 0a20 2020 2020 2020 2072 6574 7572  es.        retur
+00013890: 6e20 7365 6c66 0a0a 2020 2020 6465 6620  n self..    def 
+000138a0: 7472 616e 7366 6f72 6d28 7365 6c66 2c20  transform(self, 
+000138b0: 4b2c 2063 6f70 793d 5472 7565 293a 0a20  K, copy=True):. 
+000138c0: 2020 2020 2020 2022 2222 4365 6e74 6572         """Center
+000138d0: 206b 6572 6e65 6c20 6d61 7472 6978 2e0a   kernel matrix..
+000138e0: 0a20 2020 2020 2020 2050 6172 616d 6574  .        Paramet
+000138f0: 6572 730a 2020 2020 2020 2020 2d2d 2d2d  ers.        ----
+00013900: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 204b  ------.        K
+00013910: 203a 206e 6461 7272 6179 206f 6620 7368   : ndarray of sh
+00013920: 6170 6520 286e 5f73 616d 706c 6573 312c  ape (n_samples1,
+00013930: 206e 5f73 616d 706c 6573 3229 0a20 2020   n_samples2).   
+00013940: 2020 2020 2020 2020 204b 6572 6e65 6c20           Kernel 
+00013950: 6d61 7472 6978 2e0a 0a20 2020 2020 2020  matrix...       
+00013960: 2063 6f70 7920 3a20 626f 6f6c 2c20 6465   copy : bool, de
+00013970: 6661 756c 743d 5472 7565 0a20 2020 2020  fault=True.     
+00013980: 2020 2020 2020 2053 6574 2074 6f20 4661         Set to Fa
+00013990: 6c73 6520 746f 2070 6572 666f 726d 2069  lse to perform i
+000139a0: 6e70 6c61 6365 2063 6f6d 7075 7461 7469  nplace computati
+000139b0: 6f6e 2e0a 0a20 2020 2020 2020 2052 6574  on...        Ret
+000139c0: 7572 6e73 0a20 2020 2020 2020 202d 2d2d  urns.        ---
+000139d0: 2d2d 2d2d 0a20 2020 2020 2020 204b 5f6e  ----.        K_n
+000139e0: 6577 203a 206e 6461 7272 6179 206f 6620  ew : ndarray of 
+000139f0: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
+00013a00: 312c 206e 5f73 616d 706c 6573 3229 0a20  1, n_samples2). 
+00013a10: 2020 2020 2020 2020 2020 2052 6574 7572             Retur
+00013a20: 6e73 2074 6865 2069 6e73 7461 6e63 6520  ns the instance 
+00013a30: 6974 7365 6c66 2e0a 2020 2020 2020 2020  itself..        
+00013a40: 2222 220a 2020 2020 2020 2020 6368 6563  """.        chec
+00013a50: 6b5f 6973 5f66 6974 7465 6428 7365 6c66  k_is_fitted(self
+00013a60: 290a 0a20 2020 2020 2020 2078 702c 205f  )..        xp, _
+00013a70: 203d 2067 6574 5f6e 616d 6573 7061 6365   = get_namespace
+00013a80: 284b 290a 0a20 2020 2020 2020 204b 203d  (K)..        K =
+00013a90: 2073 656c 662e 5f76 616c 6964 6174 655f   self._validate_
+00013aa0: 6461 7461 280a 2020 2020 2020 2020 2020  data(.          
+00013ab0: 2020 4b2c 2063 6f70 793d 636f 7079 2c20    K, copy=copy, 
+00013ac0: 6474 7970 653d 5f61 7272 6179 5f61 7069  dtype=_array_api
+00013ad0: 2e73 7570 706f 7274 6564 5f66 6c6f 6174  .supported_float
+00013ae0: 5f64 7479 7065 7328 7870 292c 2072 6573  _dtypes(xp), res
+00013af0: 6574 3d46 616c 7365 0a20 2020 2020 2020  et=False.       
+00013b00: 2029 0a0a 2020 2020 2020 2020 4b5f 7072   )..        K_pr
+00013b10: 6564 5f63 6f6c 7320 3d20 2878 702e 7375  ed_cols = (xp.su
+00013b20: 6d28 4b2c 2061 7869 733d 3129 202f 2073  m(K, axis=1) / s
+00013b30: 656c 662e 4b5f 6669 745f 726f 7773 5f2e  elf.K_fit_rows_.
+00013b40: 7368 6170 655b 305d 295b 3a2c 204e 6f6e  shape[0])[:, Non
+00013b50: 655d 0a0a 2020 2020 2020 2020 4b20 2d3d  e]..        K -=
+00013b60: 2073 656c 662e 4b5f 6669 745f 726f 7773   self.K_fit_rows
+00013b70: 5f0a 2020 2020 2020 2020 4b20 2d3d 204b  _.        K -= K
+00013b80: 5f70 7265 645f 636f 6c73 0a20 2020 2020  _pred_cols.     
+00013b90: 2020 204b 202b 3d20 7365 6c66 2e4b 5f66     K += self.K_f
+00013ba0: 6974 5f61 6c6c 5f0a 0a20 2020 2020 2020  it_all_..       
+00013bb0: 2072 6574 7572 6e20 4b0a 0a20 2020 2040   return K..    @
+00013bc0: 7072 6f70 6572 7479 0a20 2020 2064 6566  property.    def
+00013bd0: 205f 6e5f 6665 6174 7572 6573 5f6f 7574   _n_features_out
+00013be0: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+00013bf0: 2222 224e 756d 6265 7220 6f66 2074 7261  """Number of tra
+00013c00: 6e73 666f 726d 6564 206f 7574 7075 7420  nsformed output 
+00013c10: 6665 6174 7572 6573 2e22 2222 0a20 2020  features.""".   
+00013c20: 2020 2020 2023 2055 7365 6420 6279 2043       # Used by C
+00013c30: 6c61 7373 4e61 6d65 5072 6566 6978 4665  lassNamePrefixFe
+00013c40: 6174 7572 6573 4f75 744d 6978 696e 2e20  aturesOutMixin. 
+00013c50: 5468 6973 206d 6f64 656c 2070 7265 7365  This model prese
+00013c60: 7276 6573 2074 6865 0a20 2020 2020 2020  rves the.       
+00013c70: 2023 206e 756d 6265 7220 6f66 2069 6e70   # number of inp
+00013c80: 7574 2066 6561 7475 7265 7320 6275 7420  ut features but 
+00013c90: 7468 6973 2069 7320 6e6f 7420 6120 6f6e  this is not a on
+00013ca0: 652d 746f 2d6f 6e65 206d 6170 7069 6e67  e-to-one mapping
+00013cb0: 2069 6e20 7468 650a 2020 2020 2020 2020   in the.        
+00013cc0: 2320 7573 7561 6c20 7365 6e73 652e 2048  # usual sense. H
+00013cd0: 656e 6365 2074 6865 2063 686f 6963 6520  ence the choice 
+00013ce0: 6e6f 7420 746f 2075 7365 204f 6e65 546f  not to use OneTo
+00013cf0: 4f6e 6546 6561 7475 7265 4d69 7869 6e20  OneFeatureMixin 
+00013d00: 746f 0a20 2020 2020 2020 2023 2069 6d70  to.        # imp
+00013d10: 6c65 6d65 6e74 2067 6574 5f66 6561 7475  lement get_featu
+00013d20: 7265 5f6e 616d 6573 5f6f 7574 2066 6f72  re_names_out for
+00013d30: 2074 6869 7320 636c 6173 732e 0a20 2020   this class..   
+00013d40: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00013d50: 2e6e 5f66 6561 7475 7265 735f 696e 5f0a  .n_features_in_.
+00013d60: 0a20 2020 2064 6566 205f 6d6f 7265 5f74  .    def _more_t
+00013d70: 6167 7328 7365 6c66 293a 0a20 2020 2020  ags(self):.     
+00013d80: 2020 2072 6574 7572 6e20 7b22 7061 6972     return {"pair
+00013d90: 7769 7365 223a 2054 7275 652c 2022 6172  wise": True, "ar
+00013da0: 7261 795f 6170 695f 7375 7070 6f72 7422  ray_api_support"
+00013db0: 3a20 5472 7565 7d0a 0a0a 4076 616c 6964  : True}...@valid
+00013dc0: 6174 655f 7061 7261 6d73 280a 2020 2020  ate_params(.    
+00013dd0: 7b0a 2020 2020 2020 2020 2258 223a 205b  {.        "X": [
+00013de0: 2261 7272 6179 2d6c 696b 6522 2c20 2273  "array-like", "s
+00013df0: 7061 7273 6520 6d61 7472 6978 225d 2c0a  parse matrix"],.
+00013e00: 2020 2020 2020 2020 2276 616c 7565 223a          "value":
+00013e10: 205b 496e 7465 7276 616c 2852 6561 6c2c   [Interval(Real,
+00013e20: 204e 6f6e 652c 204e 6f6e 652c 2063 6c6f   None, None, clo
+00013e30: 7365 643d 226e 6569 7468 6572 2229 5d2c  sed="neither")],
+00013e40: 0a20 2020 207d 2c0a 2020 2020 7072 6566  .    },.    pref
+00013e50: 6572 5f73 6b69 705f 6e65 7374 6564 5f76  er_skip_nested_v
+00013e60: 616c 6964 6174 696f 6e3d 5472 7565 2c0a  alidation=True,.
+00013e70: 290a 6465 6620 6164 645f 6475 6d6d 795f  ).def add_dummy_
+00013e80: 6665 6174 7572 6528 582c 2076 616c 7565  feature(X, value
+00013e90: 3d31 2e30 293a 0a20 2020 2022 2222 4175  =1.0):.    """Au
+00013ea0: 676d 656e 7420 6461 7461 7365 7420 7769  gment dataset wi
+00013eb0: 7468 2061 6e20 6164 6469 7469 6f6e 616c  th an additional
+00013ec0: 2064 756d 6d79 2066 6561 7475 7265 2e0a   dummy feature..
+00013ed0: 0a20 2020 2054 6869 7320 6973 2075 7365  .    This is use
+00013ee0: 6675 6c20 666f 7220 6669 7474 696e 6720  ful for fitting 
+00013ef0: 616e 2069 6e74 6572 6365 7074 2074 6572  an intercept ter
+00013f00: 6d20 7769 7468 2069 6d70 6c65 6d65 6e74  m with implement
+00013f10: 6174 696f 6e73 2077 6869 6368 0a20 2020  ations which.   
+00013f20: 2063 616e 6e6f 7420 6f74 6865 7277 6973   cannot otherwis
+00013f30: 6520 6669 7420 6974 2064 6972 6563 746c  e fit it directl
+00013f40: 792e 0a0a 2020 2020 5061 7261 6d65 7465  y...    Paramete
+00013f50: 7273 0a20 2020 202d 2d2d 2d2d 2d2d 2d2d  rs.    ---------
+00013f60: 2d0a 2020 2020 5820 3a20 7b61 7272 6179  -.    X : {array
+00013f70: 2d6c 696b 652c 2073 7061 7273 6520 6d61  -like, sparse ma
+00013f80: 7472 6978 7d20 6f66 2073 6861 7065 2028  trix} of shape (
+00013f90: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+00013fa0: 7475 7265 7329 0a20 2020 2020 2020 2044  tures).        D
+00013fb0: 6174 612e 0a0a 2020 2020 7661 6c75 6520  ata...    value 
+00013fc0: 3a20 666c 6f61 740a 2020 2020 2020 2020  : float.        
+00013fd0: 5661 6c75 6520 746f 2075 7365 2066 6f72  Value to use for
+00013fe0: 2074 6865 2064 756d 6d79 2066 6561 7475   the dummy featu
+00013ff0: 7265 2e0a 0a20 2020 2052 6574 7572 6e73  re...    Returns
+00014000: 0a20 2020 202d 2d2d 2d2d 2d2d 0a20 2020  .    -------.   
+00014010: 2058 203a 207b 6e64 6172 7261 792c 2073   X : {ndarray, s
+00014020: 7061 7273 6520 6d61 7472 6978 7d20 6f66  parse matrix} of
+00014030: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
+00014040: 732c 206e 5f66 6561 7475 7265 7320 2b20  s, n_features + 
+00014050: 3129 0a20 2020 2020 2020 2053 616d 6520  1).        Same 
+00014060: 6461 7461 2077 6974 6820 6475 6d6d 7920  data with dummy 
+00014070: 6665 6174 7572 6520 6164 6465 6420 6173  feature added as
+00014080: 2066 6972 7374 2063 6f6c 756d 6e2e 0a0a   first column...
+00014090: 2020 2020 4578 616d 706c 6573 0a20 2020      Examples.   
+000140a0: 202d 2d2d 2d2d 2d2d 2d0a 2020 2020 3e3e   --------.    >>
+000140b0: 3e20 6672 6f6d 2073 6b6c 6561 726e 2e70  > from sklearn.p
+000140c0: 7265 7072 6f63 6573 7369 6e67 2069 6d70  reprocessing imp
+000140d0: 6f72 7420 6164 645f 6475 6d6d 795f 6665  ort add_dummy_fe
+000140e0: 6174 7572 650a 2020 2020 3e3e 3e20 6164  ature.    >>> ad
+000140f0: 645f 6475 6d6d 795f 6665 6174 7572 6528  d_dummy_feature(
+00014100: 5b5b 302c 2031 5d2c 205b 312c 2030 5d5d  [[0, 1], [1, 0]]
+00014110: 290a 2020 2020 6172 7261 7928 5b5b 312e  ).    array([[1.
+00014120: 2c20 302e 2c20 312e 5d2c 0a20 2020 2020  , 0., 1.],.     
+00014130: 2020 2020 2020 5b31 2e2c 2031 2e2c 2030        [1., 1., 0
+00014140: 2e5d 5d29 0a20 2020 2022 2222 0a20 2020  .]]).    """.   
+00014150: 2058 203d 2063 6865 636b 5f61 7272 6179   X = check_array
+00014160: 2858 2c20 6163 6365 7074 5f73 7061 7273  (X, accept_spars
+00014170: 653d 5b22 6373 6322 2c20 2263 7372 222c  e=["csc", "csr",
+00014180: 2022 636f 6f22 5d2c 2064 7479 7065 3d46   "coo"], dtype=F
+00014190: 4c4f 4154 5f44 5459 5045 5329 0a20 2020  LOAT_DTYPES).   
+000141a0: 206e 5f73 616d 706c 6573 2c20 6e5f 6665   n_samples, n_fe
+000141b0: 6174 7572 6573 203d 2058 2e73 6861 7065  atures = X.shape
+000141c0: 0a20 2020 2073 6861 7065 203d 2028 6e5f  .    shape = (n_
+000141d0: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
+000141e0: 7265 7320 2b20 3129 0a20 2020 2069 6620  res + 1).    if 
+000141f0: 7370 6172 7365 2e69 7373 7061 7273 6528  sparse.issparse(
+00014200: 5829 3a0a 2020 2020 2020 2020 6966 2058  X):.        if X
+00014210: 2e66 6f72 6d61 7420 3d3d 2022 636f 6f22  .format == "coo"
+00014220: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # 
+00014230: 5368 6966 7420 636f 6c75 6d6e 7320 746f  Shift columns to
+00014240: 2074 6865 2072 6967 6874 2e0a 2020 2020   the right..    
+00014250: 2020 2020 2020 2020 636f 6c20 3d20 582e          col = X.
+00014260: 636f 6c20 2b20 310a 2020 2020 2020 2020  col + 1.        
+00014270: 2020 2020 2320 436f 6c75 6d6e 2069 6e64      # Column ind
+00014280: 6963 6573 206f 6620 6475 6d6d 7920 6665  ices of dummy fe
+00014290: 6174 7572 6520 6172 6520 3020 6576 6572  ature are 0 ever
+000142a0: 7977 6865 7265 2e0a 2020 2020 2020 2020  ywhere..        
+000142b0: 2020 2020 636f 6c20 3d20 6e70 2e63 6f6e      col = np.con
+000142c0: 6361 7465 6e61 7465 2828 6e70 2e7a 6572  catenate((np.zer
+000142d0: 6f73 286e 5f73 616d 706c 6573 292c 2063  os(n_samples), c
+000142e0: 6f6c 2929 0a20 2020 2020 2020 2020 2020  ol)).           
+000142f0: 2023 2052 6f77 2069 6e64 6963 6573 206f   # Row indices o
+00014300: 6620 6475 6d6d 7920 6665 6174 7572 6520  f dummy feature 
+00014310: 6172 6520 302c 202e 2e2e 2c20 6e5f 7361  are 0, ..., n_sa
+00014320: 6d70 6c65 732d 312e 0a20 2020 2020 2020  mples-1..       
+00014330: 2020 2020 2072 6f77 203d 206e 702e 636f       row = np.co
+00014340: 6e63 6174 656e 6174 6528 286e 702e 6172  ncatenate((np.ar
+00014350: 616e 6765 286e 5f73 616d 706c 6573 292c  ange(n_samples),
+00014360: 2058 2e72 6f77 2929 0a20 2020 2020 2020   X.row)).       
+00014370: 2020 2020 2023 2050 7265 7065 6e64 2074       # Prepend t
+00014380: 6865 2064 756d 6d79 2066 6561 7475 7265  he dummy feature
+00014390: 206e 5f73 616d 706c 6573 2074 696d 6573   n_samples times
+000143a0: 2e0a 2020 2020 2020 2020 2020 2020 6461  ..            da
+000143b0: 7461 203d 206e 702e 636f 6e63 6174 656e  ta = np.concaten
+000143c0: 6174 6528 286e 702e 6675 6c6c 286e 5f73  ate((np.full(n_s
+000143d0: 616d 706c 6573 2c20 7661 6c75 6529 2c20  amples, value), 
+000143e0: 582e 6461 7461 2929 0a20 2020 2020 2020  X.data)).       
+000143f0: 2020 2020 2072 6574 7572 6e20 7370 6172       return spar
+00014400: 7365 2e63 6f6f 5f6d 6174 7269 7828 2864  se.coo_matrix((d
+00014410: 6174 612c 2028 726f 772c 2063 6f6c 2929  ata, (row, col))
+00014420: 2c20 7368 6170 6529 0a20 2020 2020 2020  , shape).       
+00014430: 2065 6c69 6620 582e 666f 726d 6174 203d   elif X.format =
+00014440: 3d20 2263 7363 223a 0a20 2020 2020 2020  = "csc":.       
+00014450: 2020 2020 2023 2053 6869 6674 2069 6e64       # Shift ind
+00014460: 6578 2070 6f69 6e74 6572 7320 7369 6e63  ex pointers sinc
+00014470: 6520 7765 206e 6565 6420 746f 2061 6464  e we need to add
+00014480: 206e 5f73 616d 706c 6573 2065 6c65 6d65   n_samples eleme
+00014490: 6e74 732e 0a20 2020 2020 2020 2020 2020  nts..           
+000144a0: 2069 6e64 7074 7220 3d20 582e 696e 6470   indptr = X.indp
+000144b0: 7472 202b 206e 5f73 616d 706c 6573 0a20  tr + n_samples. 
+000144c0: 2020 2020 2020 2020 2020 2023 2069 6e64             # ind
+000144d0: 7074 725b 305d 206d 7573 7420 6265 2030  ptr[0] must be 0
+000144e0: 2e0a 2020 2020 2020 2020 2020 2020 696e  ..            in
+000144f0: 6470 7472 203d 206e 702e 636f 6e63 6174  dptr = np.concat
+00014500: 656e 6174 6528 286e 702e 6172 7261 7928  enate((np.array(
+00014510: 5b30 5d29 2c20 696e 6470 7472 2929 0a20  [0]), indptr)). 
+00014520: 2020 2020 2020 2020 2020 2023 2052 6f77             # Row
+00014530: 2069 6e64 6963 6573 206f 6620 6475 6d6d   indices of dumm
+00014540: 7920 6665 6174 7572 6520 6172 6520 302c  y feature are 0,
+00014550: 202e 2e2e 2c20 6e5f 7361 6d70 6c65 732d   ..., n_samples-
+00014560: 312e 0a20 2020 2020 2020 2020 2020 2069  1..            i
+00014570: 6e64 6963 6573 203d 206e 702e 636f 6e63  ndices = np.conc
+00014580: 6174 656e 6174 6528 286e 702e 6172 616e  atenate((np.aran
+00014590: 6765 286e 5f73 616d 706c 6573 292c 2058  ge(n_samples), X
+000145a0: 2e69 6e64 6963 6573 2929 0a20 2020 2020  .indices)).     
+000145b0: 2020 2020 2020 2023 2050 7265 7065 6e64         # Prepend
+000145c0: 2074 6865 2064 756d 6d79 2066 6561 7475   the dummy featu
+000145d0: 7265 206e 5f73 616d 706c 6573 2074 696d  re n_samples tim
+000145e0: 6573 2e0a 2020 2020 2020 2020 2020 2020  es..            
+000145f0: 6461 7461 203d 206e 702e 636f 6e63 6174  data = np.concat
+00014600: 656e 6174 6528 286e 702e 6675 6c6c 286e  enate((np.full(n
+00014610: 5f73 616d 706c 6573 2c20 7661 6c75 6529  _samples, value)
+00014620: 2c20 582e 6461 7461 2929 0a20 2020 2020  , X.data)).     
+00014630: 2020 2020 2020 2072 6574 7572 6e20 7370         return sp
+00014640: 6172 7365 2e63 7363 5f6d 6174 7269 7828  arse.csc_matrix(
+00014650: 2864 6174 612c 2069 6e64 6963 6573 2c20  (data, indices, 
+00014660: 696e 6470 7472 292c 2073 6861 7065 290a  indptr), shape).
+00014670: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+00014680: 2020 2020 2020 2020 2020 6b6c 6173 7320            klass 
+00014690: 3d20 582e 5f5f 636c 6173 735f 5f0a 2020  = X.__class__.  
+000146a0: 2020 2020 2020 2020 2020 7265 7475 726e            return
+000146b0: 206b 6c61 7373 2861 6464 5f64 756d 6d79   klass(add_dummy
+000146c0: 5f66 6561 7475 7265 2858 2e74 6f63 6f6f  _feature(X.tocoo
+000146d0: 2829 2c20 7661 6c75 6529 290a 2020 2020  (), value)).    
+000146e0: 656c 7365 3a0a 2020 2020 2020 2020 7265  else:.        re
+000146f0: 7475 726e 206e 702e 6873 7461 636b 2828  turn np.hstack((
+00014700: 6e70 2e66 756c 6c28 286e 5f73 616d 706c  np.full((n_sampl
+00014710: 6573 2c20 3129 2c20 7661 6c75 6529 2c20  es, 1), value), 
+00014720: 5829 290a 0a0a 636c 6173 7320 5175 616e  X))...class Quan
+00014730: 7469 6c65 5472 616e 7366 6f72 6d65 7228  tileTransformer(
+00014740: 4f6e 6554 6f4f 6e65 4665 6174 7572 654d  OneToOneFeatureM
+00014750: 6978 696e 2c20 5472 616e 7366 6f72 6d65  ixin, Transforme
+00014760: 724d 6978 696e 2c20 4261 7365 4573 7469  rMixin, BaseEsti
+00014770: 6d61 746f 7229 3a0a 2020 2020 2222 2254  mator):.    """T
+00014780: 7261 6e73 666f 726d 2066 6561 7475 7265  ransform feature
+00014790: 7320 7573 696e 6720 7175 616e 7469 6c65  s using quantile
+000147a0: 7320 696e 666f 726d 6174 696f 6e2e 0a0a  s information...
+000147b0: 2020 2020 5468 6973 206d 6574 686f 6420      This method 
+000147c0: 7472 616e 7366 6f72 6d73 2074 6865 2066  transforms the f
+000147d0: 6561 7475 7265 7320 746f 2066 6f6c 6c6f  eatures to follo
+000147e0: 7720 6120 756e 6966 6f72 6d20 6f72 2061  w a uniform or a
+000147f0: 206e 6f72 6d61 6c0a 2020 2020 6469 7374   normal.    dist
+00014800: 7269 6275 7469 6f6e 2e20 5468 6572 6566  ribution. Theref
+00014810: 6f72 652c 2066 6f72 2061 2067 6976 656e  ore, for a given
+00014820: 2066 6561 7475 7265 2c20 7468 6973 2074   feature, this t
+00014830: 7261 6e73 666f 726d 6174 696f 6e20 7465  ransformation te
+00014840: 6e64 730a 2020 2020 746f 2073 7072 6561  nds.    to sprea
+00014850: 6420 6f75 7420 7468 6520 6d6f 7374 2066  d out the most f
+00014860: 7265 7175 656e 7420 7661 6c75 6573 2e20  requent values. 
+00014870: 4974 2061 6c73 6f20 7265 6475 6365 7320  It also reduces 
+00014880: 7468 6520 696d 7061 6374 206f 660a 2020  the impact of.  
+00014890: 2020 286d 6172 6769 6e61 6c29 206f 7574    (marginal) out
+000148a0: 6c69 6572 733a 2074 6869 7320 6973 2074  liers: this is t
+000148b0: 6865 7265 666f 7265 2061 2072 6f62 7573  herefore a robus
+000148c0: 7420 7072 6570 726f 6365 7373 696e 6720  t preprocessing 
+000148d0: 7363 6865 6d65 2e0a 0a20 2020 2054 6865  scheme...    The
+000148e0: 2074 7261 6e73 666f 726d 6174 696f 6e20   transformation 
+000148f0: 6973 2061 7070 6c69 6564 206f 6e20 6561  is applied on ea
+00014900: 6368 2066 6561 7475 7265 2069 6e64 6570  ch feature indep
+00014910: 656e 6465 6e74 6c79 2e20 4669 7273 7420  endently. First 
+00014920: 616e 0a20 2020 2065 7374 696d 6174 6520  an.    estimate 
+00014930: 6f66 2074 6865 2063 756d 756c 6174 6976  of the cumulativ
+00014940: 6520 6469 7374 7269 6275 7469 6f6e 2066  e distribution f
+00014950: 756e 6374 696f 6e20 6f66 2061 2066 6561  unction of a fea
+00014960: 7475 7265 2069 730a 2020 2020 7573 6564  ture is.    used
+00014970: 2074 6f20 6d61 7020 7468 6520 6f72 6967   to map the orig
+00014980: 696e 616c 2076 616c 7565 7320 746f 2061  inal values to a
+00014990: 2075 6e69 666f 726d 2064 6973 7472 6962   uniform distrib
+000149a0: 7574 696f 6e2e 2054 6865 206f 6274 6169  ution. The obtai
+000149b0: 6e65 640a 2020 2020 7661 6c75 6573 2061  ned.    values a
+000149c0: 7265 2074 6865 6e20 6d61 7070 6564 2074  re then mapped t
+000149d0: 6f20 7468 6520 6465 7369 7265 6420 6f75  o the desired ou
+000149e0: 7470 7574 2064 6973 7472 6962 7574 696f  tput distributio
+000149f0: 6e20 7573 696e 6720 7468 650a 2020 2020  n using the.    
+00014a00: 6173 736f 6369 6174 6564 2071 7561 6e74  associated quant
+00014a10: 696c 6520 6675 6e63 7469 6f6e 2e20 4665  ile function. Fe
+00014a20: 6174 7572 6573 2076 616c 7565 7320 6f66  atures values of
+00014a30: 206e 6577 2f75 6e73 6565 6e20 6461 7461   new/unseen data
+00014a40: 2074 6861 7420 6661 6c6c 0a20 2020 2062   that fall.    b
+00014a50: 656c 6f77 206f 7220 6162 6f76 6520 7468  elow or above th
+00014a60: 6520 6669 7474 6564 2072 616e 6765 2077  e fitted range w
+00014a70: 696c 6c20 6265 206d 6170 7065 6420 746f  ill be mapped to
+00014a80: 2074 6865 2062 6f75 6e64 7320 6f66 2074   the bounds of t
+00014a90: 6865 206f 7574 7075 740a 2020 2020 6469  he output.    di
+00014aa0: 7374 7269 6275 7469 6f6e 2e20 4e6f 7465  stribution. Note
+00014ab0: 2074 6861 7420 7468 6973 2074 7261 6e73   that this trans
+00014ac0: 666f 726d 2069 7320 6e6f 6e2d 6c69 6e65  form is non-line
+00014ad0: 6172 2e20 4974 206d 6179 2064 6973 746f  ar. It may disto
+00014ae0: 7274 206c 696e 6561 720a 2020 2020 636f  rt linear.    co
+00014af0: 7272 656c 6174 696f 6e73 2062 6574 7765  rrelations betwe
+00014b00: 656e 2076 6172 6961 626c 6573 206d 6561  en variables mea
+00014b10: 7375 7265 6420 6174 2074 6865 2073 616d  sured at the sam
+00014b20: 6520 7363 616c 6520 6275 7420 7265 6e64  e scale but rend
+00014b30: 6572 730a 2020 2020 7661 7269 6162 6c65  ers.    variable
+00014b40: 7320 6d65 6173 7572 6564 2061 7420 6469  s measured at di
+00014b50: 6666 6572 656e 7420 7363 616c 6573 206d  fferent scales m
+00014b60: 6f72 6520 6469 7265 6374 6c79 2063 6f6d  ore directly com
+00014b70: 7061 7261 626c 652e 0a0a 2020 2020 466f  parable...    Fo
+00014b80: 7220 6578 616d 706c 6520 7669 7375 616c  r example visual
+00014b90: 697a 6174 696f 6e73 2c20 7265 6665 7220  izations, refer 
+00014ba0: 746f 203a 7265 663a 6043 6f6d 7061 7265  to :ref:`Compare
+00014bb0: 2051 7561 6e74 696c 6554 7261 6e73 666f   QuantileTransfo
+00014bc0: 726d 6572 2077 6974 680a 2020 2020 6f74  rmer with.    ot
+00014bd0: 6865 7220 7363 616c 6572 7320 3c70 6c6f  her scalers <plo
+00014be0: 745f 616c 6c5f 7363 616c 696e 675f 7175  t_all_scaling_qu
+00014bf0: 616e 7469 6c65 5f74 7261 6e73 666f 726d  antile_transform
+00014c00: 6572 5f73 6563 7469 6f6e 3e60 2e0a 0a20  er_section>`... 
+00014c10: 2020 2052 6561 6420 6d6f 7265 2069 6e20     Read more in 
+00014c20: 7468 6520 3a72 6566 3a60 5573 6572 2047  the :ref:`User G
+00014c30: 7569 6465 203c 7072 6570 726f 6365 7373  uide <preprocess
+00014c40: 696e 675f 7472 616e 7366 6f72 6d65 723e  ing_transformer>
+00014c50: 602e 0a0a 2020 2020 2e2e 2076 6572 7369  `...    .. versi
+00014c60: 6f6e 6164 6465 643a 3a20 302e 3139 0a0a  onadded:: 0.19..
+00014c70: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
+00014c80: 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a 2020     ----------.  
+00014c90: 2020 6e5f 7175 616e 7469 6c65 7320 3a20    n_quantiles : 
+00014ca0: 696e 742c 2064 6566 6175 6c74 3d31 3030  int, default=100
+00014cb0: 3020 6f72 206e 5f73 616d 706c 6573 0a20  0 or n_samples. 
+00014cc0: 2020 2020 2020 204e 756d 6265 7220 6f66         Number of
+00014cd0: 2071 7561 6e74 696c 6573 2074 6f20 6265   quantiles to be
+00014ce0: 2063 6f6d 7075 7465 642e 2049 7420 636f   computed. It co
+00014cf0: 7272 6573 706f 6e64 7320 746f 2074 6865  rresponds to the
+00014d00: 206e 756d 6265 720a 2020 2020 2020 2020   number.        
+00014d10: 6f66 206c 616e 646d 6172 6b73 2075 7365  of landmarks use
+00014d20: 6420 746f 2064 6973 6372 6574 697a 6520  d to discretize 
+00014d30: 7468 6520 6375 6d75 6c61 7469 7665 2064  the cumulative d
+00014d40: 6973 7472 6962 7574 696f 6e20 6675 6e63  istribution func
+00014d50: 7469 6f6e 2e0a 2020 2020 2020 2020 4966  tion..        If
+00014d60: 206e 5f71 7561 6e74 696c 6573 2069 7320   n_quantiles is 
+00014d70: 6c61 7267 6572 2074 6861 6e20 7468 6520  larger than the 
+00014d80: 6e75 6d62 6572 206f 6620 7361 6d70 6c65  number of sample
+00014d90: 732c 206e 5f71 7561 6e74 696c 6573 2069  s, n_quantiles i
+00014da0: 7320 7365 740a 2020 2020 2020 2020 746f  s set.        to
+00014db0: 2074 6865 206e 756d 6265 7220 6f66 2073   the number of s
+00014dc0: 616d 706c 6573 2061 7320 6120 6c61 7267  amples as a larg
+00014dd0: 6572 206e 756d 6265 7220 6f66 2071 7561  er number of qua
+00014de0: 6e74 696c 6573 2064 6f65 7320 6e6f 7420  ntiles does not 
+00014df0: 6769 7665 0a20 2020 2020 2020 2061 2062  give.        a b
+00014e00: 6574 7465 7220 6170 7072 6f78 696d 6174  etter approximat
+00014e10: 696f 6e20 6f66 2074 6865 2063 756d 756c  ion of the cumul
+00014e20: 6174 6976 6520 6469 7374 7269 6275 7469  ative distributi
+00014e30: 6f6e 2066 756e 6374 696f 6e0a 2020 2020  on function.    
+00014e40: 2020 2020 6573 7469 6d61 746f 722e 0a0a      estimator...
+00014e50: 2020 2020 6f75 7470 7574 5f64 6973 7472      output_distr
+00014e60: 6962 7574 696f 6e20 3a20 7b27 756e 6966  ibution : {'unif
+00014e70: 6f72 6d27 2c20 276e 6f72 6d61 6c27 7d2c  orm', 'normal'},
+00014e80: 2064 6566 6175 6c74 3d27 756e 6966 6f72   default='unifor
+00014e90: 6d27 0a20 2020 2020 2020 204d 6172 6769  m'.        Margi
+00014ea0: 6e61 6c20 6469 7374 7269 6275 7469 6f6e  nal distribution
+00014eb0: 2066 6f72 2074 6865 2074 7261 6e73 666f   for the transfo
+00014ec0: 726d 6564 2064 6174 612e 2054 6865 2063  rmed data. The c
+00014ed0: 686f 6963 6573 2061 7265 0a20 2020 2020  hoices are.     
+00014ee0: 2020 2027 756e 6966 6f72 6d27 2028 6465     'uniform' (de
+00014ef0: 6661 756c 7429 206f 7220 276e 6f72 6d61  fault) or 'norma
+00014f00: 6c27 2e0a 0a20 2020 2069 676e 6f72 655f  l'...    ignore_
+00014f10: 696d 706c 6963 6974 5f7a 6572 6f73 203a  implicit_zeros :
+00014f20: 2062 6f6f 6c2c 2064 6566 6175 6c74 3d46   bool, default=F
+00014f30: 616c 7365 0a20 2020 2020 2020 204f 6e6c  alse.        Onl
+00014f40: 7920 6170 706c 6965 7320 746f 2073 7061  y applies to spa
+00014f50: 7273 6520 6d61 7472 6963 6573 2e20 4966  rse matrices. If
+00014f60: 2054 7275 652c 2074 6865 2073 7061 7273   True, the spars
+00014f70: 6520 656e 7472 6965 7320 6f66 2074 6865  e entries of the
+00014f80: 0a20 2020 2020 2020 206d 6174 7269 7820  .        matrix 
+00014f90: 6172 6520 6469 7363 6172 6465 6420 746f  are discarded to
+00014fa0: 2063 6f6d 7075 7465 2074 6865 2071 7561   compute the qua
+00014fb0: 6e74 696c 6520 7374 6174 6973 7469 6373  ntile statistics
+00014fc0: 2e20 4966 2046 616c 7365 2c0a 2020 2020  . If False,.    
+00014fd0: 2020 2020 7468 6573 6520 656e 7472 6965      these entrie
+00014fe0: 7320 6172 6520 7472 6561 7465 6420 6173  s are treated as
+00014ff0: 207a 6572 6f73 2e0a 0a20 2020 2073 7562   zeros...    sub
+00015000: 7361 6d70 6c65 203a 2069 6e74 206f 7220  sample : int or 
+00015010: 4e6f 6e65 2c20 6465 6661 756c 743d 3130  None, default=10
+00015020: 5f30 3030 0a20 2020 2020 2020 204d 6178  _000.        Max
+00015030: 696d 756d 206e 756d 6265 7220 6f66 2073  imum number of s
+00015040: 616d 706c 6573 2075 7365 6420 746f 2065  amples used to e
+00015050: 7374 696d 6174 6520 7468 6520 7175 616e  stimate the quan
+00015060: 7469 6c65 7320 666f 720a 2020 2020 2020  tiles for.      
+00015070: 2020 636f 6d70 7574 6174 696f 6e61 6c20    computational 
+00015080: 6566 6669 6369 656e 6379 2e20 4e6f 7465  efficiency. Note
+00015090: 2074 6861 7420 7468 6520 7375 6273 616d   that the subsam
+000150a0: 706c 696e 6720 7072 6f63 6564 7572 6520  pling procedure 
+000150b0: 6d61 790a 2020 2020 2020 2020 6469 6666  may.        diff
+000150c0: 6572 2066 6f72 2076 616c 7565 2d69 6465  er for value-ide
+000150d0: 6e74 6963 616c 2073 7061 7273 6520 616e  ntical sparse an
+000150e0: 6420 6465 6e73 6520 6d61 7472 6963 6573  d dense matrices
+000150f0: 2e0a 2020 2020 2020 2020 4469 7361 626c  ..        Disabl
+00015100: 6520 7375 6273 616d 706c 696e 6720 6279  e subsampling by
+00015110: 2073 6574 7469 6e67 2060 7375 6273 616d   setting `subsam
+00015120: 706c 653d 4e6f 6e65 602e 0a0a 2020 2020  ple=None`...    
+00015130: 2020 2020 2e2e 2076 6572 7369 6f6e 6164      .. versionad
+00015140: 6465 643a 3a20 312e 350a 2020 2020 2020  ded:: 1.5.      
+00015150: 2020 2020 2054 6865 206f 7074 696f 6e20       The option 
+00015160: 604e 6f6e 6560 2074 6f20 6469 7361 626c  `None` to disabl
+00015170: 6520 7375 6273 616d 706c 696e 6720 7761  e subsampling wa
+00015180: 7320 6164 6465 642e 0a0a 2020 2020 7261  s added...    ra
+00015190: 6e64 6f6d 5f73 7461 7465 203a 2069 6e74  ndom_state : int
+000151a0: 2c20 5261 6e64 6f6d 5374 6174 6520 696e  , RandomState in
+000151b0: 7374 616e 6365 206f 7220 4e6f 6e65 2c20  stance or None, 
+000151c0: 6465 6661 756c 743d 4e6f 6e65 0a20 2020  default=None.   
+000151d0: 2020 2020 2044 6574 6572 6d69 6e65 7320       Determines 
+000151e0: 7261 6e64 6f6d 206e 756d 6265 7220 6765  random number ge
+000151f0: 6e65 7261 7469 6f6e 2066 6f72 2073 7562  neration for sub
+00015200: 7361 6d70 6c69 6e67 2061 6e64 2073 6d6f  sampling and smo
+00015210: 6f74 6869 6e67 0a20 2020 2020 2020 206e  othing.        n
+00015220: 6f69 7365 2e0a 2020 2020 2020 2020 506c  oise..        Pl
+00015230: 6561 7365 2073 6565 2060 6073 7562 7361  ease see ``subsa
+00015240: 6d70 6c65 6060 2066 6f72 206d 6f72 6520  mple`` for more 
+00015250: 6465 7461 696c 732e 0a20 2020 2020 2020  details..       
+00015260: 2050 6173 7320 616e 2069 6e74 2066 6f72   Pass an int for
+00015270: 2072 6570 726f 6475 6369 626c 6520 7265   reproducible re
+00015280: 7375 6c74 7320 6163 726f 7373 206d 756c  sults across mul
+00015290: 7469 706c 6520 6675 6e63 7469 6f6e 2063  tiple function c
+000152a0: 616c 6c73 2e0a 2020 2020 2020 2020 5365  alls..        Se
+000152b0: 6520 3a74 6572 6d3a 6047 6c6f 7373 6172  e :term:`Glossar
+000152c0: 7920 3c72 616e 646f 6d5f 7374 6174 653e  y <random_state>
+000152d0: 602e 0a0a 2020 2020 636f 7079 203a 2062  `...    copy : b
+000152e0: 6f6f 6c2c 2064 6566 6175 6c74 3d54 7275  ool, default=Tru
+000152f0: 650a 2020 2020 2020 2020 5365 7420 746f  e.        Set to
+00015300: 2046 616c 7365 2074 6f20 7065 7266 6f72   False to perfor
+00015310: 6d20 696e 706c 6163 6520 7472 616e 7366  m inplace transf
+00015320: 6f72 6d61 7469 6f6e 2061 6e64 2061 766f  ormation and avo
+00015330: 6964 2061 2063 6f70 7920 2869 6620 7468  id a copy (if th
+00015340: 650a 2020 2020 2020 2020 696e 7075 7420  e.        input 
+00015350: 6973 2061 6c72 6561 6479 2061 206e 756d  is already a num
+00015360: 7079 2061 7272 6179 292e 0a0a 2020 2020  py array)...    
+00015370: 4174 7472 6962 7574 6573 0a20 2020 202d  Attributes.    -
+00015380: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 6e5f  ---------.    n_
+00015390: 7175 616e 7469 6c65 735f 203a 2069 6e74  quantiles_ : int
+000153a0: 0a20 2020 2020 2020 2054 6865 2061 6374  .        The act
+000153b0: 7561 6c20 6e75 6d62 6572 206f 6620 7175  ual number of qu
+000153c0: 616e 7469 6c65 7320 7573 6564 2074 6f20  antiles used to 
+000153d0: 6469 7363 7265 7469 7a65 2074 6865 2063  discretize the c
+000153e0: 756d 756c 6174 6976 650a 2020 2020 2020  umulative.      
+000153f0: 2020 6469 7374 7269 6275 7469 6f6e 2066    distribution f
+00015400: 756e 6374 696f 6e2e 0a0a 2020 2020 7175  unction...    qu
+00015410: 616e 7469 6c65 735f 203a 206e 6461 7272  antiles_ : ndarr
+00015420: 6179 206f 6620 7368 6170 6520 286e 5f71  ay of shape (n_q
+00015430: 7561 6e74 696c 6573 2c20 6e5f 6665 6174  uantiles, n_feat
+00015440: 7572 6573 290a 2020 2020 2020 2020 5468  ures).        Th
+00015450: 6520 7661 6c75 6573 2063 6f72 7265 7370  e values corresp
+00015460: 6f6e 6469 6e67 2074 6865 2071 7561 6e74  onding the quant
+00015470: 696c 6573 206f 6620 7265 6665 7265 6e63  iles of referenc
+00015480: 652e 0a0a 2020 2020 7265 6665 7265 6e63  e...    referenc
+00015490: 6573 5f20 3a20 6e64 6172 7261 7920 6f66  es_ : ndarray of
+000154a0: 2073 6861 7065 2028 6e5f 7175 616e 7469   shape (n_quanti
+000154b0: 6c65 732c 2029 0a20 2020 2020 2020 2051  les, ).        Q
+000154c0: 7561 6e74 696c 6573 206f 6620 7265 6665  uantiles of refe
+000154d0: 7265 6e63 6573 2e0a 0a20 2020 206e 5f66  rences...    n_f
+000154e0: 6561 7475 7265 735f 696e 5f20 3a20 696e  eatures_in_ : in
+000154f0: 740a 2020 2020 2020 2020 4e75 6d62 6572  t.        Number
+00015500: 206f 6620 6665 6174 7572 6573 2073 6565   of features see
+00015510: 6e20 6475 7269 6e67 203a 7465 726d 3a60  n during :term:`
+00015520: 6669 7460 2e0a 0a20 2020 2020 2020 202e  fit`...        .
+00015530: 2e20 7665 7273 696f 6e61 6464 6564 3a3a  . versionadded::
+00015540: 2030 2e32 340a 0a20 2020 2066 6561 7475   0.24..    featu
+00015550: 7265 5f6e 616d 6573 5f69 6e5f 203a 206e  re_names_in_ : n
+00015560: 6461 7272 6179 206f 6620 7368 6170 6520  darray of shape 
+00015570: 2860 6e5f 6665 6174 7572 6573 5f69 6e5f  (`n_features_in_
+00015580: 602c 290a 2020 2020 2020 2020 4e61 6d65  `,).        Name
+00015590: 7320 6f66 2066 6561 7475 7265 7320 7365  s of features se
+000155a0: 656e 2064 7572 696e 6720 3a74 6572 6d3a  en during :term:
+000155b0: 6066 6974 602e 2044 6566 696e 6564 206f  `fit`. Defined o
+000155c0: 6e6c 7920 7768 656e 2060 5860 0a20 2020  nly when `X`.   
+000155d0: 2020 2020 2068 6173 2066 6561 7475 7265       has feature
+000155e0: 206e 616d 6573 2074 6861 7420 6172 6520   names that are 
+000155f0: 616c 6c20 7374 7269 6e67 732e 0a0a 2020  all strings...  
+00015600: 2020 2020 2020 2e2e 2076 6572 7369 6f6e        .. version
+00015610: 6164 6465 643a 3a20 312e 300a 0a20 2020  added:: 1.0..   
+00015620: 2053 6565 2041 6c73 6f0a 2020 2020 2d2d   See Also.    --
+00015630: 2d2d 2d2d 2d2d 0a20 2020 2071 7561 6e74  ------.    quant
+00015640: 696c 655f 7472 616e 7366 6f72 6d20 3a20  ile_transform : 
+00015650: 4571 7569 7661 6c65 6e74 2066 756e 6374  Equivalent funct
+00015660: 696f 6e20 7769 7468 6f75 7420 7468 6520  ion without the 
+00015670: 6573 7469 6d61 746f 7220 4150 492e 0a20  estimator API.. 
+00015680: 2020 2050 6f77 6572 5472 616e 7366 6f72     PowerTransfor
+00015690: 6d65 7220 3a20 5065 7266 6f72 6d20 6d61  mer : Perform ma
+000156a0: 7070 696e 6720 746f 2061 206e 6f72 6d61  pping to a norma
+000156b0: 6c20 6469 7374 7269 6275 7469 6f6e 2075  l distribution u
+000156c0: 7369 6e67 2061 2070 6f77 6572 0a20 2020  sing a power.   
+000156d0: 2020 2020 2074 7261 6e73 666f 726d 2e0a       transform..
+000156e0: 2020 2020 5374 616e 6461 7264 5363 616c      StandardScal
+000156f0: 6572 203a 2050 6572 666f 726d 2073 7461  er : Perform sta
+00015700: 6e64 6172 6469 7a61 7469 6f6e 2074 6861  ndardization tha
+00015710: 7420 6973 2066 6173 7465 722c 2062 7574  t is faster, but
+00015720: 206c 6573 7320 726f 6275 7374 0a20 2020   less robust.   
+00015730: 2020 2020 2074 6f20 6f75 746c 6965 7273       to outliers
+00015740: 2e0a 2020 2020 526f 6275 7374 5363 616c  ..    RobustScal
+00015750: 6572 203a 2050 6572 666f 726d 2072 6f62  er : Perform rob
+00015760: 7573 7420 7374 616e 6461 7264 697a 6174  ust standardizat
+00015770: 696f 6e20 7468 6174 2072 656d 6f76 6573  ion that removes
+00015780: 2074 6865 2069 6e66 6c75 656e 6365 0a20   the influence. 
+00015790: 2020 2020 2020 206f 6620 6f75 746c 6965         of outlie
+000157a0: 7273 2062 7574 2064 6f65 7320 6e6f 7420  rs but does not 
+000157b0: 7075 7420 6f75 746c 6965 7273 2061 6e64  put outliers and
+000157c0: 2069 6e6c 6965 7273 206f 6e20 7468 6520   inliers on the 
+000157d0: 7361 6d65 2073 6361 6c65 2e0a 0a20 2020  same scale...   
+000157e0: 204e 6f74 6573 0a20 2020 202d 2d2d 2d2d   Notes.    -----
+000157f0: 0a20 2020 204e 614e 7320 6172 6520 7472  .    NaNs are tr
+00015800: 6561 7465 6420 6173 206d 6973 7369 6e67  eated as missing
+00015810: 2076 616c 7565 733a 2064 6973 7265 6761   values: disrega
+00015820: 7264 6564 2069 6e20 6669 742c 2061 6e64  rded in fit, and
+00015830: 206d 6169 6e74 6169 6e65 6420 696e 0a20   maintained in. 
+00015840: 2020 2074 7261 6e73 666f 726d 2e0a 0a20     transform... 
+00015850: 2020 2045 7861 6d70 6c65 730a 2020 2020     Examples.    
+00015860: 2d2d 2d2d 2d2d 2d2d 0a20 2020 203e 3e3e  --------.    >>>
+00015870: 2069 6d70 6f72 7420 6e75 6d70 7920 6173   import numpy as
+00015880: 206e 700a 2020 2020 3e3e 3e20 6672 6f6d   np.    >>> from
+00015890: 2073 6b6c 6561 726e 2e70 7265 7072 6f63   sklearn.preproc
+000158a0: 6573 7369 6e67 2069 6d70 6f72 7420 5175  essing import Qu
+000158b0: 616e 7469 6c65 5472 616e 7366 6f72 6d65  antileTransforme
+000158c0: 720a 2020 2020 3e3e 3e20 726e 6720 3d20  r.    >>> rng = 
+000158d0: 6e70 2e72 616e 646f 6d2e 5261 6e64 6f6d  np.random.Random
+000158e0: 5374 6174 6528 3029 0a20 2020 203e 3e3e  State(0).    >>>
+000158f0: 2058 203d 206e 702e 736f 7274 2872 6e67   X = np.sort(rng
+00015900: 2e6e 6f72 6d61 6c28 6c6f 633d 302e 352c  .normal(loc=0.5,
+00015910: 2073 6361 6c65 3d30 2e32 352c 2073 697a   scale=0.25, siz
+00015920: 653d 2832 352c 2031 2929 2c20 6178 6973  e=(25, 1)), axis
+00015930: 3d30 290a 2020 2020 3e3e 3e20 7174 203d  =0).    >>> qt =
+00015940: 2051 7561 6e74 696c 6554 7261 6e73 666f   QuantileTransfo
+00015950: 726d 6572 286e 5f71 7561 6e74 696c 6573  rmer(n_quantiles
+00015960: 3d31 302c 2072 616e 646f 6d5f 7374 6174  =10, random_stat
+00015970: 653d 3029 0a20 2020 203e 3e3e 2071 742e  e=0).    >>> qt.
+00015980: 6669 745f 7472 616e 7366 6f72 6d28 5829  fit_transform(X)
+00015990: 0a20 2020 2061 7272 6179 285b 2e2e 2e5d  .    array([...]
+000159a0: 290a 2020 2020 2222 220a 0a20 2020 205f  ).    """..    _
+000159b0: 7061 7261 6d65 7465 725f 636f 6e73 7472  parameter_constr
+000159c0: 6169 6e74 733a 2064 6963 7420 3d20 7b0a  aints: dict = {.
+000159d0: 2020 2020 2020 2020 226e 5f71 7561 6e74          "n_quant
+000159e0: 696c 6573 223a 205b 496e 7465 7276 616c  iles": [Interval
+000159f0: 2849 6e74 6567 7261 6c2c 2031 2c20 4e6f  (Integral, 1, No
+00015a00: 6e65 2c20 636c 6f73 6564 3d22 6c65 6674  ne, closed="left
+00015a10: 2229 5d2c 0a20 2020 2020 2020 2022 6f75  ")],.        "ou
+00015a20: 7470 7574 5f64 6973 7472 6962 7574 696f  tput_distributio
+00015a30: 6e22 3a20 5b53 7472 4f70 7469 6f6e 7328  n": [StrOptions(
+00015a40: 7b22 756e 6966 6f72 6d22 2c20 226e 6f72  {"uniform", "nor
+00015a50: 6d61 6c22 7d29 5d2c 0a20 2020 2020 2020  mal"})],.       
+00015a60: 2022 6967 6e6f 7265 5f69 6d70 6c69 6369   "ignore_implici
+00015a70: 745f 7a65 726f 7322 3a20 5b22 626f 6f6c  t_zeros": ["bool
+00015a80: 6561 6e22 5d2c 0a20 2020 2020 2020 2022  ean"],.        "
+00015a90: 7375 6273 616d 706c 6522 3a20 5b49 6e74  subsample": [Int
+00015aa0: 6572 7661 6c28 496e 7465 6772 616c 2c20  erval(Integral, 
+00015ab0: 312c 204e 6f6e 652c 2063 6c6f 7365 643d  1, None, closed=
+00015ac0: 226c 6566 7422 292c 204e 6f6e 655d 2c0a  "left"), None],.
+00015ad0: 2020 2020 2020 2020 2272 616e 646f 6d5f          "random_
+00015ae0: 7374 6174 6522 3a20 5b22 7261 6e64 6f6d  state": ["random
+00015af0: 5f73 7461 7465 225d 2c0a 2020 2020 2020  _state"],.      
+00015b00: 2020 2263 6f70 7922 3a20 5b22 626f 6f6c    "copy": ["bool
+00015b10: 6561 6e22 5d2c 0a20 2020 207d 0a0a 2020  ean"],.    }..  
+00015b20: 2020 6465 6620 5f5f 696e 6974 5f5f 280a    def __init__(.
+00015b30: 2020 2020 2020 2020 7365 6c66 2c0a 2020          self,.  
+00015b40: 2020 2020 2020 2a2c 0a20 2020 2020 2020        *,.       
+00015b50: 206e 5f71 7561 6e74 696c 6573 3d31 3030   n_quantiles=100
+00015b60: 302c 0a20 2020 2020 2020 206f 7574 7075  0,.        outpu
+00015b70: 745f 6469 7374 7269 6275 7469 6f6e 3d22  t_distribution="
+00015b80: 756e 6966 6f72 6d22 2c0a 2020 2020 2020  uniform",.      
+00015b90: 2020 6967 6e6f 7265 5f69 6d70 6c69 6369    ignore_implici
+00015ba0: 745f 7a65 726f 733d 4661 6c73 652c 0a20  t_zeros=False,. 
+00015bb0: 2020 2020 2020 2073 7562 7361 6d70 6c65         subsample
+00015bc0: 3d31 305f 3030 302c 0a20 2020 2020 2020  =10_000,.       
+00015bd0: 2072 616e 646f 6d5f 7374 6174 653d 4e6f   random_state=No
+00015be0: 6e65 2c0a 2020 2020 2020 2020 636f 7079  ne,.        copy
+00015bf0: 3d54 7275 652c 0a20 2020 2029 3a0a 2020  =True,.    ):.  
+00015c00: 2020 2020 2020 7365 6c66 2e6e 5f71 7561        self.n_qua
+00015c10: 6e74 696c 6573 203d 206e 5f71 7561 6e74  ntiles = n_quant
+00015c20: 696c 6573 0a20 2020 2020 2020 2073 656c  iles.        sel
+00015c30: 662e 6f75 7470 7574 5f64 6973 7472 6962  f.output_distrib
+00015c40: 7574 696f 6e20 3d20 6f75 7470 7574 5f64  ution = output_d
+00015c50: 6973 7472 6962 7574 696f 6e0a 2020 2020  istribution.    
+00015c60: 2020 2020 7365 6c66 2e69 676e 6f72 655f      self.ignore_
+00015c70: 696d 706c 6963 6974 5f7a 6572 6f73 203d  implicit_zeros =
+00015c80: 2069 676e 6f72 655f 696d 706c 6963 6974   ignore_implicit
+00015c90: 5f7a 6572 6f73 0a20 2020 2020 2020 2073  _zeros.        s
+00015ca0: 656c 662e 7375 6273 616d 706c 6520 3d20  elf.subsample = 
+00015cb0: 7375 6273 616d 706c 650a 2020 2020 2020  subsample.      
+00015cc0: 2020 7365 6c66 2e72 616e 646f 6d5f 7374    self.random_st
+00015cd0: 6174 6520 3d20 7261 6e64 6f6d 5f73 7461  ate = random_sta
+00015ce0: 7465 0a20 2020 2020 2020 2073 656c 662e  te.        self.
+00015cf0: 636f 7079 203d 2063 6f70 790a 0a20 2020  copy = copy..   
+00015d00: 2064 6566 205f 6465 6e73 655f 6669 7428   def _dense_fit(
+00015d10: 7365 6c66 2c20 582c 2072 616e 646f 6d5f  self, X, random_
+00015d20: 7374 6174 6529 3a0a 2020 2020 2020 2020  state):.        
+00015d30: 2222 2243 6f6d 7075 7465 2070 6572 6365  """Compute perce
+00015d40: 6e74 696c 6573 2066 6f72 2064 656e 7365  ntiles for dense
+00015d50: 206d 6174 7269 6365 732e 0a0a 2020 2020   matrices...    
+00015d60: 2020 2020 5061 7261 6d65 7465 7273 0a20      Parameters. 
+00015d70: 2020 2020 2020 202d 2d2d 2d2d 2d2d 2d2d         ---------
+00015d80: 2d0a 2020 2020 2020 2020 5820 3a20 6e64  -.        X : nd
+00015d90: 6172 7261 7920 6f66 2073 6861 7065 2028  array of shape (
+00015da0: 6e5f 7361 6d70 6c65 732c 206e 5f66 6561  n_samples, n_fea
+00015db0: 7475 7265 7329 0a20 2020 2020 2020 2020  tures).         
+00015dc0: 2020 2054 6865 2064 6174 6120 7573 6564     The data used
+00015dd0: 2074 6f20 7363 616c 6520 616c 6f6e 6720   to scale along 
+00015de0: 7468 6520 6665 6174 7572 6573 2061 7869  the features axi
+00015df0: 732e 0a20 2020 2020 2020 2022 2222 0a20  s..        """. 
+00015e00: 2020 2020 2020 2069 6620 7365 6c66 2e69         if self.i
+00015e10: 676e 6f72 655f 696d 706c 6963 6974 5f7a  gnore_implicit_z
+00015e20: 6572 6f73 3a0a 2020 2020 2020 2020 2020  eros:.          
+00015e30: 2020 7761 726e 696e 6773 2e77 6172 6e28    warnings.warn(
+00015e40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00015e50: 2022 2769 676e 6f72 655f 696d 706c 6963   "'ignore_implic
+00015e60: 6974 5f7a 6572 6f73 2720 7461 6b65 7320  it_zeros' takes 
+00015e70: 6566 6665 6374 206f 6e6c 7920 7769 7468  effect only with
+00015e80: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00015e90: 2020 2220 7370 6172 7365 206d 6174 7269    " sparse matri
+00015ea0: 782e 2054 6869 7320 7061 7261 6d65 7465  x. This paramete
+00015eb0: 7220 6861 7320 6e6f 2065 6666 6563 742e  r has no effect.
+00015ec0: 220a 2020 2020 2020 2020 2020 2020 290a  ".            ).
+00015ed0: 0a20 2020 2020 2020 206e 5f73 616d 706c  .        n_sampl
+00015ee0: 6573 2c20 6e5f 6665 6174 7572 6573 203d  es, n_features =
+00015ef0: 2058 2e73 6861 7065 0a20 2020 2020 2020   X.shape.       
+00015f00: 2072 6566 6572 656e 6365 7320 3d20 7365   references = se
+00015f10: 6c66 2e72 6566 6572 656e 6365 735f 202a  lf.references_ *
+00015f20: 2031 3030 0a0a 2020 2020 2020 2020 6966   100..        if
+00015f30: 2073 656c 662e 7375 6273 616d 706c 6520   self.subsample 
+00015f40: 6973 206e 6f74 204e 6f6e 6520 616e 6420  is not None and 
+00015f50: 7365 6c66 2e73 7562 7361 6d70 6c65 203c  self.subsample <
+00015f60: 206e 5f73 616d 706c 6573 3a0a 2020 2020   n_samples:.    
+00015f70: 2020 2020 2020 2020 2320 5461 6b65 2061          # Take a
+00015f80: 2073 7562 7361 6d70 6c65 206f 6620 6058   subsample of `X
+00015f90: 600a 2020 2020 2020 2020 2020 2020 5820  `.            X 
+00015fa0: 3d20 7265 7361 6d70 6c65 280a 2020 2020  = resample(.    
+00015fb0: 2020 2020 2020 2020 2020 2020 582c 2072              X, r
+00015fc0: 6570 6c61 6365 3d46 616c 7365 2c20 6e5f  eplace=False, n_
+00015fd0: 7361 6d70 6c65 733d 7365 6c66 2e73 7562  samples=self.sub
+00015fe0: 7361 6d70 6c65 2c20 7261 6e64 6f6d 5f73  sample, random_s
+00015ff0: 7461 7465 3d72 616e 646f 6d5f 7374 6174  tate=random_stat
+00016000: 650a 2020 2020 2020 2020 2020 2020 290a  e.            ).
+00016010: 0a20 2020 2020 2020 2073 656c 662e 7175  .        self.qu
+00016020: 616e 7469 6c65 735f 203d 206e 702e 6e61  antiles_ = np.na
+00016030: 6e70 6572 6365 6e74 696c 6528 582c 2072  npercentile(X, r
+00016040: 6566 6572 656e 6365 732c 2061 7869 733d  eferences, axis=
+00016050: 3029 0a20 2020 2020 2020 2023 2044 7565  0).        # Due
+00016060: 2074 6f20 666c 6f61 7469 6e67 2d70 6f69   to floating-poi
+00016070: 6e74 2070 7265 6369 7369 6f6e 2065 7272  nt precision err
+00016080: 6f72 2069 6e20 606e 702e 6e61 6e70 6572  or in `np.nanper
+00016090: 6365 6e74 696c 6560 2c0a 2020 2020 2020  centile`,.      
+000160a0: 2020 2320 6d61 6b65 2073 7572 6520 7468    # make sure th
+000160b0: 6174 2071 7561 6e74 696c 6573 2061 7265  at quantiles are
+000160c0: 206d 6f6e 6f74 6f6e 6963 616c 6c79 2069   monotonically i
+000160d0: 6e63 7265 6173 696e 672e 0a20 2020 2020  ncreasing..     
+000160e0: 2020 2023 2055 7073 7472 6561 6d20 6973     # Upstream is
+000160f0: 7375 6520 696e 206e 756d 7079 3a0a 2020  sue in numpy:.  
+00016100: 2020 2020 2020 2320 6874 7470 733a 2f2f        # https://
+00016110: 6769 7468 7562 2e63 6f6d 2f6e 756d 7079  github.com/numpy
+00016120: 2f6e 756d 7079 2f69 7373 7565 732f 3134  /numpy/issues/14
+00016130: 3638 350a 2020 2020 2020 2020 7365 6c66  685.        self
+00016140: 2e71 7561 6e74 696c 6573 5f20 3d20 6e70  .quantiles_ = np
+00016150: 2e6d 6178 696d 756d 2e61 6363 756d 756c  .maximum.accumul
+00016160: 6174 6528 7365 6c66 2e71 7561 6e74 696c  ate(self.quantil
+00016170: 6573 5f29 0a0a 2020 2020 6465 6620 5f73  es_)..    def _s
+00016180: 7061 7273 655f 6669 7428 7365 6c66 2c20  parse_fit(self, 
+00016190: 582c 2072 616e 646f 6d5f 7374 6174 6529  X, random_state)
+000161a0: 3a0a 2020 2020 2020 2020 2222 2243 6f6d  :.        """Com
+000161b0: 7075 7465 2070 6572 6365 6e74 696c 6573  pute percentiles
+000161c0: 2066 6f72 2073 7061 7273 6520 6d61 7472   for sparse matr
+000161d0: 6963 6573 2e0a 0a20 2020 2020 2020 2050  ices...        P
+000161e0: 6172 616d 6574 6572 730a 2020 2020 2020  arameters.      
+000161f0: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020    ----------.   
+00016200: 2020 2020 2058 203a 2073 7061 7273 6520       X : sparse 
+00016210: 6d61 7472 6978 206f 6620 7368 6170 6520  matrix of shape 
+00016220: 286e 5f73 616d 706c 6573 2c20 6e5f 6665  (n_samples, n_fe
+00016230: 6174 7572 6573 290a 2020 2020 2020 2020  atures).        
+00016240: 2020 2020 5468 6520 6461 7461 2075 7365      The data use
+00016250: 6420 746f 2073 6361 6c65 2061 6c6f 6e67  d to scale along
+00016260: 2074 6865 2066 6561 7475 7265 7320 6178   the features ax
+00016270: 6973 2e20 5468 6520 7370 6172 7365 206d  is. The sparse m
+00016280: 6174 7269 780a 2020 2020 2020 2020 2020  atrix.          
+00016290: 2020 6e65 6564 7320 746f 2062 6520 6e6f    needs to be no
+000162a0: 6e6e 6567 6174 6976 652e 2049 6620 6120  nnegative. If a 
+000162b0: 7370 6172 7365 206d 6174 7269 7820 6973  sparse matrix is
+000162c0: 2070 726f 7669 6465 642c 0a20 2020 2020   provided,.     
+000162d0: 2020 2020 2020 2069 7420 7769 6c6c 2062         it will b
+000162e0: 6520 636f 6e76 6572 7465 6420 696e 746f  e converted into
+000162f0: 2061 2073 7061 7273 6520 6060 6373 635f   a sparse ``csc_
+00016300: 6d61 7472 6978 6060 2e0a 2020 2020 2020  matrix``..      
+00016310: 2020 2222 220a 2020 2020 2020 2020 6e5f    """.        n_
+00016320: 7361 6d70 6c65 732c 206e 5f66 6561 7475  samples, n_featu
+00016330: 7265 7320 3d20 582e 7368 6170 650a 2020  res = X.shape.  
+00016340: 2020 2020 2020 7265 6665 7265 6e63 6573        references
+00016350: 203d 2073 656c 662e 7265 6665 7265 6e63   = self.referenc
+00016360: 6573 5f20 2a20 3130 300a 0a20 2020 2020  es_ * 100..     
+00016370: 2020 2073 656c 662e 7175 616e 7469 6c65     self.quantile
+00016380: 735f 203d 205b 5d0a 2020 2020 2020 2020  s_ = [].        
+00016390: 666f 7220 6665 6174 7572 655f 6964 7820  for feature_idx 
+000163a0: 696e 2072 616e 6765 286e 5f66 6561 7475  in range(n_featu
+000163b0: 7265 7329 3a0a 2020 2020 2020 2020 2020  res):.          
+000163c0: 2020 636f 6c75 6d6e 5f6e 6e7a 5f64 6174    column_nnz_dat
+000163d0: 6120 3d20 582e 6461 7461 5b58 2e69 6e64  a = X.data[X.ind
+000163e0: 7074 725b 6665 6174 7572 655f 6964 785d  ptr[feature_idx]
+000163f0: 203a 2058 2e69 6e64 7074 725b 6665 6174   : X.indptr[feat
+00016400: 7572 655f 6964 7820 2b20 315d 5d0a 2020  ure_idx + 1]].  
+00016410: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+00016420: 662e 7375 6273 616d 706c 6520 6973 206e  f.subsample is n
+00016430: 6f74 204e 6f6e 6520 616e 6420 6c65 6e28  ot None and len(
+00016440: 636f 6c75 6d6e 5f6e 6e7a 5f64 6174 6129  column_nnz_data)
+00016450: 203e 2073 656c 662e 7375 6273 616d 706c   > self.subsampl
+00016460: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
+00016470: 2020 2063 6f6c 756d 6e5f 7375 6273 616d     column_subsam
+00016480: 706c 6520 3d20 7365 6c66 2e73 7562 7361  ple = self.subsa
+00016490: 6d70 6c65 202a 206c 656e 2863 6f6c 756d  mple * len(colum
+000164a0: 6e5f 6e6e 7a5f 6461 7461 2920 2f2f 206e  n_nnz_data) // n
+000164b0: 5f73 616d 706c 6573 0a20 2020 2020 2020  _samples.       
+000164c0: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+000164d0: 2e69 676e 6f72 655f 696d 706c 6963 6974  .ignore_implicit
+000164e0: 5f7a 6572 6f73 3a0a 2020 2020 2020 2020  _zeros:.        
+000164f0: 2020 2020 2020 2020 2020 2020 636f 6c75              colu
+00016500: 6d6e 5f64 6174 6120 3d20 6e70 2e7a 6572  mn_data = np.zer
+00016510: 6f73 2873 6861 7065 3d63 6f6c 756d 6e5f  os(shape=column_
+00016520: 7375 6273 616d 706c 652c 2064 7479 7065  subsample, dtype
+00016530: 3d58 2e64 7479 7065 290a 2020 2020 2020  =X.dtype).      
+00016540: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.
+00016550: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016560: 2020 2020 636f 6c75 6d6e 5f64 6174 6120      column_data 
+00016570: 3d20 6e70 2e7a 6572 6f73 2873 6861 7065  = np.zeros(shape
+00016580: 3d73 656c 662e 7375 6273 616d 706c 652c  =self.subsample,
+00016590: 2064 7479 7065 3d58 2e64 7479 7065 290a   dtype=X.dtype).
+000165a0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000165b0: 636f 6c75 6d6e 5f64 6174 615b 3a63 6f6c  column_data[:col
+000165c0: 756d 6e5f 7375 6273 616d 706c 655d 203d  umn_subsample] =
+000165d0: 2072 616e 646f 6d5f 7374 6174 652e 6368   random_state.ch
+000165e0: 6f69 6365 280a 2020 2020 2020 2020 2020  oice(.          
+000165f0: 2020 2020 2020 2020 2020 636f 6c75 6d6e            column
+00016600: 5f6e 6e7a 5f64 6174 612c 2073 697a 653d  _nnz_data, size=
+00016610: 636f 6c75 6d6e 5f73 7562 7361 6d70 6c65  column_subsample
+00016620: 2c20 7265 706c 6163 653d 4661 6c73 650a  , replace=False.
+00016630: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016640: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el
+00016650: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
+00016660: 2020 2020 6966 2073 656c 662e 6967 6e6f      if self.igno
+00016670: 7265 5f69 6d70 6c69 6369 745f 7a65 726f  re_implicit_zero
+00016680: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             
+00016690: 2020 2020 2020 2063 6f6c 756d 6e5f 6461         column_da
+000166a0: 7461 203d 206e 702e 7a65 726f 7328 7368  ta = np.zeros(sh
+000166b0: 6170 653d 6c65 6e28 636f 6c75 6d6e 5f6e  ape=len(column_n
+000166c0: 6e7a 5f64 6174 6129 2c20 6474 7970 653d  nz_data), dtype=
+000166d0: 582e 6474 7970 6529 0a20 2020 2020 2020  X.dtype).       
+000166e0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+000166f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016700: 2020 2063 6f6c 756d 6e5f 6461 7461 203d     column_data =
+00016710: 206e 702e 7a65 726f 7328 7368 6170 653d   np.zeros(shape=
+00016720: 6e5f 7361 6d70 6c65 732c 2064 7479 7065  n_samples, dtype
+00016730: 3d58 2e64 7479 7065 290a 2020 2020 2020  =X.dtype).      
+00016740: 2020 2020 2020 2020 2020 636f 6c75 6d6e            column
+00016750: 5f64 6174 615b 3a20 6c65 6e28 636f 6c75  _data[: len(colu
+00016760: 6d6e 5f6e 6e7a 5f64 6174 6129 5d20 3d20  mn_nnz_data)] = 
+00016770: 636f 6c75 6d6e 5f6e 6e7a 5f64 6174 610a  column_nnz_data.
+00016780: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+00016790: 6e6f 7420 636f 6c75 6d6e 5f64 6174 612e  not column_data.
+000167a0: 7369 7a65 3a0a 2020 2020 2020 2020 2020  size:.          
+000167b0: 2020 2020 2020 2320 6966 206e 6f20 6e6e        # if no nn
+000167c0: 7a2c 2061 6e20 6572 726f 7220 7769 6c6c  z, an error will
+000167d0: 2062 6520 7261 6973 6564 2066 6f72 2063   be raised for c
+000167e0: 6f6d 7075 7469 6e67 2074 6865 0a20 2020  omputing the.   
+000167f0: 2020 2020 2020 2020 2020 2020 2023 2071               # q
+00016800: 7561 6e74 696c 6573 2e20 466f 7263 6520  uantiles. Force 
+00016810: 7468 6520 7175 616e 7469 6c65 7320 746f  the quantiles to
+00016820: 2062 6520 7a65 726f 732e 0a20 2020 2020   be zeros..     
+00016830: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00016840: 7175 616e 7469 6c65 735f 2e61 7070 656e  quantiles_.appen
+00016850: 6428 5b30 5d20 2a20 6c65 6e28 7265 6665  d([0] * len(refe
+00016860: 7265 6e63 6573 2929 0a20 2020 2020 2020  rences)).       
+00016870: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+00016880: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+00016890: 7175 616e 7469 6c65 735f 2e61 7070 656e  quantiles_.appen
+000168a0: 6428 6e70 2e6e 616e 7065 7263 656e 7469  d(np.nanpercenti
+000168b0: 6c65 2863 6f6c 756d 6e5f 6461 7461 2c20  le(column_data, 
+000168c0: 7265 6665 7265 6e63 6573 2929 0a20 2020  references)).   
+000168d0: 2020 2020 2073 656c 662e 7175 616e 7469       self.quanti
+000168e0: 6c65 735f 203d 206e 702e 7472 616e 7370  les_ = np.transp
+000168f0: 6f73 6528 7365 6c66 2e71 7561 6e74 696c  ose(self.quantil
+00016900: 6573 5f29 0a20 2020 2020 2020 2023 2064  es_).        # d
+00016910: 7565 2074 6f20 666c 6f61 7469 6e67 2d70  ue to floating-p
+00016920: 6f69 6e74 2070 7265 6369 7369 6f6e 2065  oint precision e
+00016930: 7272 6f72 2069 6e20 606e 702e 6e61 6e70  rror in `np.nanp
+00016940: 6572 6365 6e74 696c 6560 2c0a 2020 2020  ercentile`,.    
+00016950: 2020 2020 2320 6d61 6b65 2073 7572 6520      # make sure 
+00016960: 7468 6520 7175 616e 7469 6c65 7320 6172  the quantiles ar
+00016970: 6520 6d6f 6e6f 746f 6e69 6361 6c6c 7920  e monotonically 
+00016980: 696e 6372 6561 7369 6e67 0a20 2020 2020  increasing.     
+00016990: 2020 2023 2055 7073 7472 6561 6d20 6973     # Upstream is
+000169a0: 7375 6520 696e 206e 756d 7079 3a0a 2020  sue in numpy:.  
+000169b0: 2020 2020 2020 2320 6874 7470 733a 2f2f        # https://
+000169c0: 6769 7468 7562 2e63 6f6d 2f6e 756d 7079  github.com/numpy
+000169d0: 2f6e 756d 7079 2f69 7373 7565 732f 3134  /numpy/issues/14
+000169e0: 3638 350a 2020 2020 2020 2020 7365 6c66  685.        self
+000169f0: 2e71 7561 6e74 696c 6573 5f20 3d20 6e70  .quantiles_ = np
+00016a00: 2e6d 6178 696d 756d 2e61 6363 756d 756c  .maximum.accumul
+00016a10: 6174 6528 7365 6c66 2e71 7561 6e74 696c  ate(self.quantil
+00016a20: 6573 5f29 0a0a 2020 2020 405f 6669 745f  es_)..    @_fit_
+00016a30: 636f 6e74 6578 7428 7072 6566 6572 5f73  context(prefer_s
+00016a40: 6b69 705f 6e65 7374 6564 5f76 616c 6964  kip_nested_valid
+00016a50: 6174 696f 6e3d 5472 7565 290a 2020 2020  ation=True).    
+00016a60: 6465 6620 6669 7428 7365 6c66 2c20 582c  def fit(self, X,
+00016a70: 2079 3d4e 6f6e 6529 3a0a 2020 2020 2020   y=None):.      
+00016a80: 2020 2222 2243 6f6d 7075 7465 2074 6865    """Compute the
+00016a90: 2071 7561 6e74 696c 6573 2075 7365 6420   quantiles used 
+00016aa0: 666f 7220 7472 616e 7366 6f72 6d69 6e67  for transforming
+00016ab0: 2e0a 0a20 2020 2020 2020 2050 6172 616d  ...        Param
+00016ac0: 6574 6572 730a 2020 2020 2020 2020 2d2d  eters.        --
+00016ad0: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020 2020  --------.       
+00016ae0: 2058 203a 207b 6172 7261 792d 6c69 6b65   X : {array-like
+00016af0: 2c20 7370 6172 7365 206d 6174 7269 787d  , sparse matrix}
+00016b00: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
+00016b10: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
+00016b20: 290a 2020 2020 2020 2020 2020 2020 5468  ).            Th
+00016b30: 6520 6461 7461 2075 7365 6420 746f 2073  e data used to s
+00016b40: 6361 6c65 2061 6c6f 6e67 2074 6865 2066  cale along the f
+00016b50: 6561 7475 7265 7320 6178 6973 2e20 4966  eatures axis. If
+00016b60: 2061 2073 7061 7273 650a 2020 2020 2020   a sparse.      
+00016b70: 2020 2020 2020 6d61 7472 6978 2069 7320        matrix is 
+00016b80: 7072 6f76 6964 6564 2c20 6974 2077 696c  provided, it wil
+00016b90: 6c20 6265 2063 6f6e 7665 7274 6564 2069  l be converted i
+00016ba0: 6e74 6f20 6120 7370 6172 7365 0a20 2020  nto a sparse.   
+00016bb0: 2020 2020 2020 2020 2060 6063 7363 5f6d           ``csc_m
+00016bc0: 6174 7269 7860 602e 2041 6464 6974 696f  atrix``. Additio
+00016bd0: 6e61 6c6c 792c 2074 6865 2073 7061 7273  nally, the spars
+00016be0: 6520 6d61 7472 6978 206e 6565 6473 2074  e matrix needs t
+00016bf0: 6f20 6265 0a20 2020 2020 2020 2020 2020  o be.           
+00016c00: 206e 6f6e 6e65 6761 7469 7665 2069 6620   nonnegative if 
+00016c10: 6069 676e 6f72 655f 696d 706c 6963 6974  `ignore_implicit
+00016c20: 5f7a 6572 6f73 6020 6973 2046 616c 7365  _zeros` is False
+00016c30: 2e0a 0a20 2020 2020 2020 2079 203a 204e  ...        y : N
+00016c40: 6f6e 650a 2020 2020 2020 2020 2020 2020  one.            
+00016c50: 4967 6e6f 7265 642e 0a0a 2020 2020 2020  Ignored...      
+00016c60: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
+00016c70: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
+00016c80: 2020 7365 6c66 203a 206f 626a 6563 740a    self : object.
+00016c90: 2020 2020 2020 2020 2020 2046 6974 7465             Fitte
+00016ca0: 6420 7472 616e 7366 6f72 6d65 722e 0a20  d transformer.. 
+00016cb0: 2020 2020 2020 2022 2222 0a20 2020 2020         """.     
+00016cc0: 2020 2069 6620 7365 6c66 2e73 7562 7361     if self.subsa
+00016cd0: 6d70 6c65 2069 7320 6e6f 7420 4e6f 6e65  mple is not None
+00016ce0: 2061 6e64 2073 656c 662e 6e5f 7175 616e   and self.n_quan
+00016cf0: 7469 6c65 7320 3e20 7365 6c66 2e73 7562  tiles > self.sub
+00016d00: 7361 6d70 6c65 3a0a 2020 2020 2020 2020  sample:.        
+00016d10: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00016d20: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
+00016d30: 2020 2020 2020 2254 6865 206e 756d 6265        "The numbe
+00016d40: 7220 6f66 2071 7561 6e74 696c 6573 2063  r of quantiles c
+00016d50: 616e 6e6f 7420 6265 2067 7265 6174 6572  annot be greater
+00016d60: 2074 6861 6e22 0a20 2020 2020 2020 2020   than".         
+00016d70: 2020 2020 2020 2022 2074 6865 206e 756d         " the num
+00016d80: 6265 7220 6f66 2073 616d 706c 6573 2075  ber of samples u
+00016d90: 7365 642e 2047 6f74 207b 7d20 7175 616e  sed. Got {} quan
+00016da0: 7469 6c65 7322 0a20 2020 2020 2020 2020  tiles".         
+00016db0: 2020 2020 2020 2022 2061 6e64 207b 7d20         " and {} 
+00016dc0: 7361 6d70 6c65 732e 222e 666f 726d 6174  samples.".format
+00016dd0: 2873 656c 662e 6e5f 7175 616e 7469 6c65  (self.n_quantile
+00016de0: 732c 2073 656c 662e 7375 6273 616d 706c  s, self.subsampl
+00016df0: 6529 0a20 2020 2020 2020 2020 2020 2029  e).            )
+00016e00: 0a0a 2020 2020 2020 2020 5820 3d20 7365  ..        X = se
+00016e10: 6c66 2e5f 6368 6563 6b5f 696e 7075 7473  lf._check_inputs
+00016e20: 2858 2c20 696e 5f66 6974 3d54 7275 652c  (X, in_fit=True,
+00016e30: 2063 6f70 793d 4661 6c73 6529 0a20 2020   copy=False).   
+00016e40: 2020 2020 206e 5f73 616d 706c 6573 203d       n_samples =
+00016e50: 2058 2e73 6861 7065 5b30 5d0a 0a20 2020   X.shape[0]..   
+00016e60: 2020 2020 2069 6620 7365 6c66 2e6e 5f71       if self.n_q
+00016e70: 7561 6e74 696c 6573 203e 206e 5f73 616d  uantiles > n_sam
+00016e80: 706c 6573 3a0a 2020 2020 2020 2020 2020  ples:.          
+00016e90: 2020 7761 726e 696e 6773 2e77 6172 6e28    warnings.warn(
+00016ea0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00016eb0: 2022 6e5f 7175 616e 7469 6c65 7320 2825   "n_quantiles (%
+00016ec0: 7329 2069 7320 6772 6561 7465 7220 7468  s) is greater th
+00016ed0: 616e 2074 6865 2074 6f74 616c 206e 756d  an the total num
+00016ee0: 6265 7220 220a 2020 2020 2020 2020 2020  ber ".          
+00016ef0: 2020 2020 2020 226f 6620 7361 6d70 6c65        "of sample
+00016f00: 7320 2825 7329 2e20 6e5f 7175 616e 7469  s (%s). n_quanti
+00016f10: 6c65 7320 6973 2073 6574 2074 6f20 220a  les is set to ".
+00016f20: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016f30: 226e 5f73 616d 706c 6573 2e22 2025 2028  "n_samples." % (
+00016f40: 7365 6c66 2e6e 5f71 7561 6e74 696c 6573  self.n_quantiles
+00016f50: 2c20 6e5f 7361 6d70 6c65 7329 0a20 2020  , n_samples).   
+00016f60: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
+00016f70: 2020 2073 656c 662e 6e5f 7175 616e 7469     self.n_quanti
+00016f80: 6c65 735f 203d 206d 6178 2831 2c20 6d69  les_ = max(1, mi
+00016f90: 6e28 7365 6c66 2e6e 5f71 7561 6e74 696c  n(self.n_quantil
+00016fa0: 6573 2c20 6e5f 7361 6d70 6c65 7329 290a  es, n_samples)).
+00016fb0: 0a20 2020 2020 2020 2072 6e67 203d 2063  .        rng = c
+00016fc0: 6865 636b 5f72 616e 646f 6d5f 7374 6174  heck_random_stat
+00016fd0: 6528 7365 6c66 2e72 616e 646f 6d5f 7374  e(self.random_st
+00016fe0: 6174 6529 0a0a 2020 2020 2020 2020 2320  ate)..        # 
+00016ff0: 4372 6561 7465 2074 6865 2071 7561 6e74  Create the quant
+00017000: 696c 6573 206f 6620 7265 6665 7265 6e63  iles of referenc
+00017010: 650a 2020 2020 2020 2020 7365 6c66 2e72  e.        self.r
+00017020: 6566 6572 656e 6365 735f 203d 206e 702e  eferences_ = np.
+00017030: 6c69 6e73 7061 6365 2830 2c20 312c 2073  linspace(0, 1, s
+00017040: 656c 662e 6e5f 7175 616e 7469 6c65 735f  elf.n_quantiles_
+00017050: 2c20 656e 6470 6f69 6e74 3d54 7275 6529  , endpoint=True)
+00017060: 0a20 2020 2020 2020 2069 6620 7370 6172  .        if spar
+00017070: 7365 2e69 7373 7061 7273 6528 5829 3a0a  se.issparse(X):.
+00017080: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00017090: 2e5f 7370 6172 7365 5f66 6974 2858 2c20  ._sparse_fit(X, 
+000170a0: 726e 6729 0a20 2020 2020 2020 2065 6c73  rng).        els
+000170b0: 653a 0a20 2020 2020 2020 2020 2020 2073  e:.            s
+000170c0: 656c 662e 5f64 656e 7365 5f66 6974 2858  elf._dense_fit(X
+000170d0: 2c20 726e 6729 0a0a 2020 2020 2020 2020  , rng)..        
+000170e0: 7265 7475 726e 2073 656c 660a 0a20 2020  return self..   
+000170f0: 2064 6566 205f 7472 616e 7366 6f72 6d5f   def _transform_
+00017100: 636f 6c28 7365 6c66 2c20 585f 636f 6c2c  col(self, X_col,
+00017110: 2071 7561 6e74 696c 6573 2c20 696e 7665   quantiles, inve
+00017120: 7273 6529 3a0a 2020 2020 2020 2020 2222  rse):.        ""
+00017130: 2250 7269 7661 7465 2066 756e 6374 696f  "Private functio
+00017140: 6e20 746f 2074 7261 6e73 666f 726d 2061  n to transform a
+00017150: 2073 696e 676c 6520 6665 6174 7572 652e   single feature.
+00017160: 2222 220a 0a20 2020 2020 2020 206f 7574  """..        out
+00017170: 7075 745f 6469 7374 7269 6275 7469 6f6e  put_distribution
+00017180: 203d 2073 656c 662e 6f75 7470 7574 5f64   = self.output_d
+00017190: 6973 7472 6962 7574 696f 6e0a 0a20 2020  istribution..   
+000171a0: 2020 2020 2069 6620 6e6f 7420 696e 7665       if not inve
+000171b0: 7273 653a 0a20 2020 2020 2020 2020 2020  rse:.           
+000171c0: 206c 6f77 6572 5f62 6f75 6e64 5f78 203d   lower_bound_x =
+000171d0: 2071 7561 6e74 696c 6573 5b30 5d0a 2020   quantiles[0].  
+000171e0: 2020 2020 2020 2020 2020 7570 7065 725f            upper_
+000171f0: 626f 756e 645f 7820 3d20 7175 616e 7469  bound_x = quanti
+00017200: 6c65 735b 2d31 5d0a 2020 2020 2020 2020  les[-1].        
+00017210: 2020 2020 6c6f 7765 725f 626f 756e 645f      lower_bound_
+00017220: 7920 3d20 300a 2020 2020 2020 2020 2020  y = 0.          
+00017230: 2020 7570 7065 725f 626f 756e 645f 7920    upper_bound_y 
+00017240: 3d20 310a 2020 2020 2020 2020 656c 7365  = 1.        else
+00017250: 3a0a 2020 2020 2020 2020 2020 2020 6c6f  :.            lo
+00017260: 7765 725f 626f 756e 645f 7820 3d20 300a  wer_bound_x = 0.
+00017270: 2020 2020 2020 2020 2020 2020 7570 7065              uppe
+00017280: 725f 626f 756e 645f 7820 3d20 310a 2020  r_bound_x = 1.  
+00017290: 2020 2020 2020 2020 2020 6c6f 7765 725f            lower_
+000172a0: 626f 756e 645f 7920 3d20 7175 616e 7469  bound_y = quanti
+000172b0: 6c65 735b 305d 0a20 2020 2020 2020 2020  les[0].         
+000172c0: 2020 2075 7070 6572 5f62 6f75 6e64 5f79     upper_bound_y
+000172d0: 203d 2071 7561 6e74 696c 6573 5b2d 315d   = quantiles[-1]
+000172e0: 0a20 2020 2020 2020 2020 2020 2023 2066  .            # f
+000172f0: 6f72 2069 6e76 6572 7365 2074 7261 6e73  or inverse trans
+00017300: 666f 726d 2c20 6d61 7463 6820 6120 756e  form, match a un
+00017310: 6966 6f72 6d20 6469 7374 7269 6275 7469  iform distributi
+00017320: 6f6e 0a20 2020 2020 2020 2020 2020 2077  on.            w
+00017330: 6974 6820 6e70 2e65 7272 7374 6174 6528  ith np.errstate(
+00017340: 696e 7661 6c69 643d 2269 676e 6f72 6522  invalid="ignore"
+00017350: 293a 2020 2320 6869 6465 204e 614e 2063  ):  # hide NaN c
+00017360: 6f6d 7061 7269 736f 6e20 7761 726e 696e  omparison warnin
+00017370: 6773 0a20 2020 2020 2020 2020 2020 2020  gs.             
+00017380: 2020 2069 6620 6f75 7470 7574 5f64 6973     if output_dis
+00017390: 7472 6962 7574 696f 6e20 3d3d 2022 6e6f  tribution == "no
+000173a0: 726d 616c 223a 0a20 2020 2020 2020 2020  rmal":.         
+000173b0: 2020 2020 2020 2020 2020 2058 5f63 6f6c             X_col
+000173c0: 203d 2073 7461 7473 2e6e 6f72 6d2e 6364   = stats.norm.cd
+000173d0: 6628 585f 636f 6c29 0a20 2020 2020 2020  f(X_col).       
+000173e0: 2020 2020 2020 2020 2023 2065 6c73 6520           # else 
+000173f0: 6f75 7470 7574 2064 6973 7472 6962 7574  output distribut
+00017400: 696f 6e20 6973 2061 6c72 6561 6479 2061  ion is already a
+00017410: 2075 6e69 666f 726d 2064 6973 7472 6962   uniform distrib
+00017420: 7574 696f 6e0a 0a20 2020 2020 2020 2023  ution..        #
+00017430: 2066 696e 6420 696e 6465 7820 666f 7220   find index for 
+00017440: 6c6f 7765 7220 616e 6420 6869 6768 6572  lower and higher
+00017450: 2062 6f75 6e64 730a 2020 2020 2020 2020   bounds.        
+00017460: 7769 7468 206e 702e 6572 7273 7461 7465  with np.errstate
+00017470: 2869 6e76 616c 6964 3d22 6967 6e6f 7265  (invalid="ignore
+00017480: 2229 3a20 2023 2068 6964 6520 4e61 4e20  "):  # hide NaN 
+00017490: 636f 6d70 6172 6973 6f6e 2077 6172 6e69  comparison warni
+000174a0: 6e67 730a 2020 2020 2020 2020 2020 2020  ngs.            
+000174b0: 6966 206f 7574 7075 745f 6469 7374 7269  if output_distri
+000174c0: 6275 7469 6f6e 203d 3d20 226e 6f72 6d61  bution == "norma
+000174d0: 6c22 3a0a 2020 2020 2020 2020 2020 2020  l":.            
+000174e0: 2020 2020 6c6f 7765 725f 626f 756e 6473      lower_bounds
+000174f0: 5f69 6478 203d 2058 5f63 6f6c 202d 2042  _idx = X_col - B
+00017500: 4f55 4e44 535f 5448 5245 5348 4f4c 4420  OUNDS_THRESHOLD 
+00017510: 3c20 6c6f 7765 725f 626f 756e 645f 780a  < lower_bound_x.
+00017520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017530: 7570 7065 725f 626f 756e 6473 5f69 6478  upper_bounds_idx
+00017540: 203d 2058 5f63 6f6c 202b 2042 4f55 4e44   = X_col + BOUND
+00017550: 535f 5448 5245 5348 4f4c 4420 3e20 7570  S_THRESHOLD > up
+00017560: 7065 725f 626f 756e 645f 780a 2020 2020  per_bound_x.    
+00017570: 2020 2020 2020 2020 6966 206f 7574 7075          if outpu
+00017580: 745f 6469 7374 7269 6275 7469 6f6e 203d  t_distribution =
+00017590: 3d20 2275 6e69 666f 726d 223a 0a20 2020  = "uniform":.   
+000175a0: 2020 2020 2020 2020 2020 2020 206c 6f77               low
+000175b0: 6572 5f62 6f75 6e64 735f 6964 7820 3d20  er_bounds_idx = 
+000175c0: 585f 636f 6c20 3d3d 206c 6f77 6572 5f62  X_col == lower_b
+000175d0: 6f75 6e64 5f78 0a20 2020 2020 2020 2020  ound_x.         
+000175e0: 2020 2020 2020 2075 7070 6572 5f62 6f75         upper_bou
+000175f0: 6e64 735f 6964 7820 3d20 585f 636f 6c20  nds_idx = X_col 
+00017600: 3d3d 2075 7070 6572 5f62 6f75 6e64 5f78  == upper_bound_x
+00017610: 0a0a 2020 2020 2020 2020 6973 6669 6e69  ..        isfini
+00017620: 7465 5f6d 6173 6b20 3d20 7e6e 702e 6973  te_mask = ~np.is
+00017630: 6e61 6e28 585f 636f 6c29 0a20 2020 2020  nan(X_col).     
+00017640: 2020 2058 5f63 6f6c 5f66 696e 6974 6520     X_col_finite 
+00017650: 3d20 585f 636f 6c5b 6973 6669 6e69 7465  = X_col[isfinite
+00017660: 5f6d 6173 6b5d 0a20 2020 2020 2020 2069  _mask].        i
+00017670: 6620 6e6f 7420 696e 7665 7273 653a 0a20  f not inverse:. 
+00017680: 2020 2020 2020 2020 2020 2023 2049 6e74             # Int
+00017690: 6572 706f 6c61 7465 2069 6e20 6f6e 6520  erpolate in one 
+000176a0: 6469 7265 6374 696f 6e20 616e 6420 696e  direction and in
+000176b0: 2074 6865 206f 7468 6572 2061 6e64 2074   the other and t
+000176c0: 616b 6520 7468 650a 2020 2020 2020 2020  ake the.        
+000176d0: 2020 2020 2320 6d65 616e 2e20 5468 6973      # mean. This
+000176e0: 2069 7320 696e 2063 6173 6520 6f66 2072   is in case of r
+000176f0: 6570 6561 7465 6420 7661 6c75 6573 2069  epeated values i
+00017700: 6e20 7468 6520 6665 6174 7572 6573 0a20  n the features. 
+00017710: 2020 2020 2020 2020 2020 2023 2061 6e64             # and
+00017720: 2068 656e 6365 2072 6570 6561 7465 6420   hence repeated 
+00017730: 7175 616e 7469 6c65 730a 2020 2020 2020  quantiles.      
+00017740: 2020 2020 2020 230a 2020 2020 2020 2020        #.        
+00017750: 2020 2020 2320 4966 2077 6520 646f 6e27      # If we don'
+00017760: 7420 646f 2074 6869 732c 206f 6e6c 7920  t do this, only 
+00017770: 6f6e 6520 6578 7472 656d 6520 6f66 2074  one extreme of t
+00017780: 6865 2064 7570 6c69 6361 7465 6420 6973  he duplicated is
+00017790: 0a20 2020 2020 2020 2020 2020 2023 2075  .            # u
+000177a0: 7365 6420 2874 6865 2075 7070 6572 2077  sed (the upper w
+000177b0: 6865 6e20 7765 2064 6f20 6173 6365 6e64  hen we do ascend
+000177c0: 696e 672c 2061 6e64 2074 6865 0a20 2020  ing, and the.   
+000177d0: 2020 2020 2020 2020 2023 206c 6f77 6572           # lower
+000177e0: 2066 6f72 2064 6573 6365 6e64 696e 6729   for descending)
+000177f0: 2e20 5765 2074 616b 6520 7468 6520 6d65  . We take the me
+00017800: 616e 206f 6620 7468 6573 6520 7477 6f0a  an of these two.
+00017810: 2020 2020 2020 2020 2020 2020 585f 636f              X_co
+00017820: 6c5b 6973 6669 6e69 7465 5f6d 6173 6b5d  l[isfinite_mask]
+00017830: 203d 2030 2e35 202a 2028 0a20 2020 2020   = 0.5 * (.     
+00017840: 2020 2020 2020 2020 2020 206e 702e 696e             np.in
+00017850: 7465 7270 2858 5f63 6f6c 5f66 696e 6974  terp(X_col_finit
+00017860: 652c 2071 7561 6e74 696c 6573 2c20 7365  e, quantiles, se
+00017870: 6c66 2e72 6566 6572 656e 6365 735f 290a  lf.references_).
+00017880: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017890: 2d20 6e70 2e69 6e74 6572 7028 2d58 5f63  - np.interp(-X_c
+000178a0: 6f6c 5f66 696e 6974 652c 202d 7175 616e  ol_finite, -quan
+000178b0: 7469 6c65 735b 3a3a 2d31 5d2c 202d 7365  tiles[::-1], -se
+000178c0: 6c66 2e72 6566 6572 656e 6365 735f 5b3a  lf.references_[:
+000178d0: 3a2d 315d 290a 2020 2020 2020 2020 2020  :-1]).          
+000178e0: 2020 290a 2020 2020 2020 2020 656c 7365    ).        else
+000178f0: 3a0a 2020 2020 2020 2020 2020 2020 585f  :.            X_
+00017900: 636f 6c5b 6973 6669 6e69 7465 5f6d 6173  col[isfinite_mas
+00017910: 6b5d 203d 206e 702e 696e 7465 7270 2858  k] = np.interp(X
+00017920: 5f63 6f6c 5f66 696e 6974 652c 2073 656c  _col_finite, sel
+00017930: 662e 7265 6665 7265 6e63 6573 5f2c 2071  f.references_, q
+00017940: 7561 6e74 696c 6573 290a 0a20 2020 2020  uantiles)..     
+00017950: 2020 2058 5f63 6f6c 5b75 7070 6572 5f62     X_col[upper_b
+00017960: 6f75 6e64 735f 6964 785d 203d 2075 7070  ounds_idx] = upp
+00017970: 6572 5f62 6f75 6e64 5f79 0a20 2020 2020  er_bound_y.     
+00017980: 2020 2058 5f63 6f6c 5b6c 6f77 6572 5f62     X_col[lower_b
+00017990: 6f75 6e64 735f 6964 785d 203d 206c 6f77  ounds_idx] = low
+000179a0: 6572 5f62 6f75 6e64 5f79 0a20 2020 2020  er_bound_y.     
+000179b0: 2020 2023 2066 6f72 2066 6f72 7761 7264     # for forward
+000179c0: 2074 7261 6e73 666f 726d 2c20 6d61 7463   transform, matc
+000179d0: 6820 7468 6520 6f75 7470 7574 2064 6973  h the output dis
+000179e0: 7472 6962 7574 696f 6e0a 2020 2020 2020  tribution.      
+000179f0: 2020 6966 206e 6f74 2069 6e76 6572 7365    if not inverse
+00017a00: 3a0a 2020 2020 2020 2020 2020 2020 7769  :.            wi
+00017a10: 7468 206e 702e 6572 7273 7461 7465 2869  th np.errstate(i
+00017a20: 6e76 616c 6964 3d22 6967 6e6f 7265 2229  nvalid="ignore")
+00017a30: 3a20 2023 2068 6964 6520 4e61 4e20 636f  :  # hide NaN co
+00017a40: 6d70 6172 6973 6f6e 2077 6172 6e69 6e67  mparison warning
+00017a50: 730a 2020 2020 2020 2020 2020 2020 2020  s.              
+00017a60: 2020 6966 206f 7574 7075 745f 6469 7374    if output_dist
+00017a70: 7269 6275 7469 6f6e 203d 3d20 226e 6f72  ribution == "nor
+00017a80: 6d61 6c22 3a0a 2020 2020 2020 2020 2020  mal":.          
+00017a90: 2020 2020 2020 2020 2020 585f 636f 6c20            X_col 
+00017aa0: 3d20 7374 6174 732e 6e6f 726d 2e70 7066  = stats.norm.ppf
+00017ab0: 2858 5f63 6f6c 290a 2020 2020 2020 2020  (X_col).        
+00017ac0: 2020 2020 2020 2020 2020 2020 2320 6669              # fi
+00017ad0: 6e64 2074 6865 2076 616c 7565 2074 6f20  nd the value to 
+00017ae0: 636c 6970 2074 6865 2064 6174 6120 746f  clip the data to
+00017af0: 2061 766f 6964 206d 6170 7069 6e67 2074   avoid mapping t
+00017b00: 6f0a 2020 2020 2020 2020 2020 2020 2020  o.              
+00017b10: 2020 2020 2020 2320 696e 6669 6e69 7479        # infinity
+00017b20: 2e20 436c 6970 2073 7563 6820 7468 6174  . Clip such that
+00017b30: 2074 6865 2069 6e76 6572 7365 2074 7261   the inverse tra
+00017b40: 6e73 666f 726d 2077 696c 6c20 6265 0a20  nsform will be. 
+00017b50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017b60: 2020 2023 2063 6f6e 7369 7374 656e 740a     # consistent.
+00017b70: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017b80: 2020 2020 636c 6970 5f6d 696e 203d 2073      clip_min = s
+00017b90: 7461 7473 2e6e 6f72 6d2e 7070 6628 424f  tats.norm.ppf(BO
+00017ba0: 554e 4453 5f54 4852 4553 484f 4c44 202d  UNDS_THRESHOLD -
+00017bb0: 206e 702e 7370 6163 696e 6728 3129 290a   np.spacing(1)).
+00017bc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017bd0: 2020 2020 636c 6970 5f6d 6178 203d 2073      clip_max = s
+00017be0: 7461 7473 2e6e 6f72 6d2e 7070 6628 3120  tats.norm.ppf(1 
+00017bf0: 2d20 2842 4f55 4e44 535f 5448 5245 5348  - (BOUNDS_THRESH
+00017c00: 4f4c 4420 2d20 6e70 2e73 7061 6369 6e67  OLD - np.spacing
+00017c10: 2831 2929 290a 2020 2020 2020 2020 2020  (1))).          
+00017c20: 2020 2020 2020 2020 2020 585f 636f 6c20            X_col 
+00017c30: 3d20 6e70 2e63 6c69 7028 585f 636f 6c2c  = np.clip(X_col,
+00017c40: 2063 6c69 705f 6d69 6e2c 2063 6c69 705f   clip_min, clip_
+00017c50: 6d61 7829 0a20 2020 2020 2020 2020 2020  max).           
+00017c60: 2020 2020 2023 2065 6c73 6520 6f75 7470       # else outp
+00017c70: 7574 2064 6973 7472 6962 7574 696f 6e20  ut distribution 
+00017c80: 6973 2075 6e69 666f 726d 2061 6e64 2074  is uniform and t
+00017c90: 6865 2070 7066 2069 7320 7468 650a 2020  he ppf is the.  
+00017ca0: 2020 2020 2020 2020 2020 2020 2020 2320                # 
+00017cb0: 6964 656e 7469 7479 2066 756e 6374 696f  identity functio
+00017cc0: 6e20 736f 2077 6520 6c65 7420 585f 636f  n so we let X_co
+00017cd0: 6c20 756e 6368 616e 6765 640a 0a20 2020  l unchanged..   
+00017ce0: 2020 2020 2072 6574 7572 6e20 585f 636f       return X_co
+00017cf0: 6c0a 0a20 2020 2064 6566 205f 6368 6563  l..    def _chec
+00017d00: 6b5f 696e 7075 7473 2873 656c 662c 2058  k_inputs(self, X
+00017d10: 2c20 696e 5f66 6974 2c20 6163 6365 7074  , in_fit, accept
+00017d20: 5f73 7061 7273 655f 6e65 6761 7469 7665  _sparse_negative
+00017d30: 3d46 616c 7365 2c20 636f 7079 3d46 616c  =False, copy=Fal
+00017d40: 7365 293a 0a20 2020 2020 2020 2022 2222  se):.        """
+00017d50: 4368 6563 6b20 696e 7075 7473 2062 6566  Check inputs bef
+00017d60: 6f72 6520 6669 7420 616e 6420 7472 616e  ore fit and tran
+00017d70: 7366 6f72 6d2e 2222 220a 2020 2020 2020  sform.""".      
+00017d80: 2020 5820 3d20 7365 6c66 2e5f 7661 6c69    X = self._vali
+00017d90: 6461 7465 5f64 6174 6128 0a20 2020 2020  date_data(.     
+00017da0: 2020 2020 2020 2058 2c0a 2020 2020 2020         X,.      
+00017db0: 2020 2020 2020 7265 7365 743d 696e 5f66        reset=in_f
+00017dc0: 6974 2c0a 2020 2020 2020 2020 2020 2020  it,.            
+00017dd0: 6163 6365 7074 5f73 7061 7273 653d 2263  accept_sparse="c
+00017de0: 7363 222c 0a20 2020 2020 2020 2020 2020  sc",.           
+00017df0: 2063 6f70 793d 636f 7079 2c0a 2020 2020   copy=copy,.    
+00017e00: 2020 2020 2020 2020 6474 7970 653d 464c          dtype=FL
+00017e10: 4f41 545f 4454 5950 4553 2c0a 2020 2020  OAT_DTYPES,.    
+00017e20: 2020 2020 2020 2020 666f 7263 655f 616c          force_al
+00017e30: 6c5f 6669 6e69 7465 3d22 616c 6c6f 772d  l_finite="allow-
+00017e40: 6e61 6e22 2c0a 2020 2020 2020 2020 290a  nan",.        ).
+00017e50: 2020 2020 2020 2020 2320 7765 206f 6e6c          # we onl
+00017e60: 7920 6163 6365 7074 2070 6f73 6974 6976  y accept positiv
+00017e70: 6520 7370 6172 7365 206d 6174 7269 7820  e sparse matrix 
+00017e80: 7768 656e 2069 676e 6f72 655f 696d 706c  when ignore_impl
+00017e90: 6963 6974 5f7a 6572 6f73 2069 730a 2020  icit_zeros is.  
+00017ea0: 2020 2020 2020 2320 6661 6c73 6520 616e        # false an
+00017eb0: 6420 7468 6174 2077 6520 6361 6c6c 2066  d that we call f
+00017ec0: 6974 206f 7220 7472 616e 7366 6f72 6d2e  it or transform.
+00017ed0: 0a20 2020 2020 2020 2077 6974 6820 6e70  .        with np
+00017ee0: 2e65 7272 7374 6174 6528 696e 7661 6c69  .errstate(invali
+00017ef0: 643d 2269 676e 6f72 6522 293a 2020 2320  d="ignore"):  # 
+00017f00: 6869 6465 204e 614e 2063 6f6d 7061 7269  hide NaN compari
+00017f10: 736f 6e20 7761 726e 696e 6773 0a20 2020  son warnings.   
+00017f20: 2020 2020 2020 2020 2069 6620 280a 2020           if (.  
+00017f30: 2020 2020 2020 2020 2020 2020 2020 6e6f                no
+00017f40: 7420 6163 6365 7074 5f73 7061 7273 655f  t accept_sparse_
+00017f50: 6e65 6761 7469 7665 0a20 2020 2020 2020  negative.       
+00017f60: 2020 2020 2020 2020 2061 6e64 206e 6f74           and not
+00017f70: 2073 656c 662e 6967 6e6f 7265 5f69 6d70   self.ignore_imp
+00017f80: 6c69 6369 745f 7a65 726f 730a 2020 2020  licit_zeros.    
+00017f90: 2020 2020 2020 2020 2020 2020 616e 6420              and 
+00017fa0: 2873 7061 7273 652e 6973 7370 6172 7365  (sparse.issparse
+00017fb0: 2858 2920 616e 6420 6e70 2e61 6e79 2858  (X) and np.any(X
+00017fc0: 2e64 6174 6120 3c20 3029 290a 2020 2020  .data < 0)).    
+00017fd0: 2020 2020 2020 2020 293a 0a20 2020 2020          ):.     
+00017fe0: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+00017ff0: 2056 616c 7565 4572 726f 7228 0a20 2020   ValueError(.   
+00018000: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018010: 2022 5175 616e 7469 6c65 5472 616e 7366   "QuantileTransf
+00018020: 6f72 6d65 7220 6f6e 6c79 2061 6363 6570  ormer only accep
+00018030: 7473 206e 6f6e 2d6e 6567 6174 6976 6520  ts non-negative 
+00018040: 7370 6172 7365 206d 6174 7269 6365 732e  sparse matrices.
+00018050: 220a 2020 2020 2020 2020 2020 2020 2020  ".              
+00018060: 2020 290a 0a20 2020 2020 2020 2072 6574    )..        ret
+00018070: 7572 6e20 580a 0a20 2020 2064 6566 205f  urn X..    def _
+00018080: 7472 616e 7366 6f72 6d28 7365 6c66 2c20  transform(self, 
+00018090: 582c 2069 6e76 6572 7365 3d46 616c 7365  X, inverse=False
+000180a0: 293a 0a20 2020 2020 2020 2022 2222 466f  ):.        """Fo
+000180b0: 7277 6172 6420 616e 6420 696e 7665 7273  rward and invers
+000180c0: 6520 7472 616e 7366 6f72 6d2e 0a0a 2020  e transform...  
+000180d0: 2020 2020 2020 5061 7261 6d65 7465 7273        Parameters
+000180e0: 0a20 2020 2020 2020 202d 2d2d 2d2d 2d2d  .        -------
+000180f0: 2d2d 2d0a 2020 2020 2020 2020 5820 3a20  ---.        X : 
+00018100: 6e64 6172 7261 7920 6f66 2073 6861 7065  ndarray of shape
+00018110: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
+00018120: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
+00018130: 2020 2020 2054 6865 2064 6174 6120 7573       The data us
+00018140: 6564 2074 6f20 7363 616c 6520 616c 6f6e  ed to scale alon
+00018150: 6720 7468 6520 6665 6174 7572 6573 2061  g the features a
+00018160: 7869 732e 0a0a 2020 2020 2020 2020 696e  xis...        in
+00018170: 7665 7273 6520 3a20 626f 6f6c 2c20 6465  verse : bool, de
+00018180: 6661 756c 743d 4661 6c73 650a 2020 2020  fault=False.    
+00018190: 2020 2020 2020 2020 4966 2046 616c 7365          If False
+000181a0: 2c20 6170 706c 7920 666f 7277 6172 6420  , apply forward 
+000181b0: 7472 616e 7366 6f72 6d2e 2049 6620 5472  transform. If Tr
+000181c0: 7565 2c20 6170 706c 790a 2020 2020 2020  ue, apply.      
+000181d0: 2020 2020 2020 696e 7665 7273 6520 7472        inverse tr
+000181e0: 616e 7366 6f72 6d2e 0a0a 2020 2020 2020  ansform...      
+000181f0: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
+00018200: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
+00018210: 2020 5820 3a20 6e64 6172 7261 7920 6f66    X : ndarray of
+00018220: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
+00018230: 732c 206e 5f66 6561 7475 7265 7329 0a20  s, n_features). 
+00018240: 2020 2020 2020 2020 2020 2050 726f 6a65             Proje
+00018250: 6374 6564 2064 6174 612e 0a20 2020 2020  cted data..     
+00018260: 2020 2022 2222 0a20 2020 2020 2020 2069     """.        i
+00018270: 6620 7370 6172 7365 2e69 7373 7061 7273  f sparse.isspars
+00018280: 6528 5829 3a0a 2020 2020 2020 2020 2020  e(X):.          
+00018290: 2020 666f 7220 6665 6174 7572 655f 6964    for feature_id
+000182a0: 7820 696e 2072 616e 6765 2858 2e73 6861  x in range(X.sha
+000182b0: 7065 5b31 5d29 3a0a 2020 2020 2020 2020  pe[1]):.        
+000182c0: 2020 2020 2020 2020 636f 6c75 6d6e 5f73          column_s
+000182d0: 6c69 6365 203d 2073 6c69 6365 2858 2e69  lice = slice(X.i
+000182e0: 6e64 7074 725b 6665 6174 7572 655f 6964  ndptr[feature_id
+000182f0: 785d 2c20 582e 696e 6470 7472 5b66 6561  x], X.indptr[fea
+00018300: 7475 7265 5f69 6478 202b 2031 5d29 0a20  ture_idx + 1]). 
+00018310: 2020 2020 2020 2020 2020 2020 2020 2058                 X
+00018320: 2e64 6174 615b 636f 6c75 6d6e 5f73 6c69  .data[column_sli
+00018330: 6365 5d20 3d20 7365 6c66 2e5f 7472 616e  ce] = self._tran
+00018340: 7366 6f72 6d5f 636f 6c28 0a20 2020 2020  sform_col(.     
+00018350: 2020 2020 2020 2020 2020 2020 2020 2058                 X
+00018360: 2e64 6174 615b 636f 6c75 6d6e 5f73 6c69  .data[column_sli
+00018370: 6365 5d2c 2073 656c 662e 7175 616e 7469  ce], self.quanti
+00018380: 6c65 735f 5b3a 2c20 6665 6174 7572 655f  les_[:, feature_
+00018390: 6964 785d 2c20 696e 7665 7273 650a 2020  idx], inverse.  
+000183a0: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+000183b0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+000183c0: 2020 2020 2020 2020 2020 666f 7220 6665            for fe
+000183d0: 6174 7572 655f 6964 7820 696e 2072 616e  ature_idx in ran
+000183e0: 6765 2858 2e73 6861 7065 5b31 5d29 3a0a  ge(X.shape[1]):.
+000183f0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00018400: 585b 3a2c 2066 6561 7475 7265 5f69 6478  X[:, feature_idx
+00018410: 5d20 3d20 7365 6c66 2e5f 7472 616e 7366  ] = self._transf
+00018420: 6f72 6d5f 636f 6c28 0a20 2020 2020 2020  orm_col(.       
+00018430: 2020 2020 2020 2020 2020 2020 2058 5b3a               X[:
+00018440: 2c20 6665 6174 7572 655f 6964 785d 2c20  , feature_idx], 
+00018450: 7365 6c66 2e71 7561 6e74 696c 6573 5f5b  self.quantiles_[
+00018460: 3a2c 2066 6561 7475 7265 5f69 6478 5d2c  :, feature_idx],
+00018470: 2069 6e76 6572 7365 0a20 2020 2020 2020   inverse.       
+00018480: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+00018490: 2020 2020 7265 7475 726e 2058 0a0a 2020      return X..  
+000184a0: 2020 6465 6620 7472 616e 7366 6f72 6d28    def transform(
+000184b0: 7365 6c66 2c20 5829 3a0a 2020 2020 2020  self, X):.      
+000184c0: 2020 2222 2246 6561 7475 7265 2d77 6973    """Feature-wis
+000184d0: 6520 7472 616e 7366 6f72 6d61 7469 6f6e  e transformation
+000184e0: 206f 6620 7468 6520 6461 7461 2e0a 0a20   of the data... 
+000184f0: 2020 2020 2020 2050 6172 616d 6574 6572         Parameter
+00018500: 730a 2020 2020 2020 2020 2d2d 2d2d 2d2d  s.        ------
+00018510: 2d2d 2d2d 0a20 2020 2020 2020 2058 203a  ----.        X :
+00018520: 207b 6172 7261 792d 6c69 6b65 2c20 7370   {array-like, sp
+00018530: 6172 7365 206d 6174 7269 787d 206f 6620  arse matrix} of 
+00018540: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
+00018550: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
+00018560: 2020 2020 2020 2020 2020 5468 6520 6461            The da
+00018570: 7461 2075 7365 6420 746f 2073 6361 6c65  ta used to scale
+00018580: 2061 6c6f 6e67 2074 6865 2066 6561 7475   along the featu
+00018590: 7265 7320 6178 6973 2e20 4966 2061 2073  res axis. If a s
+000185a0: 7061 7273 650a 2020 2020 2020 2020 2020  parse.          
+000185b0: 2020 6d61 7472 6978 2069 7320 7072 6f76    matrix is prov
+000185c0: 6964 6564 2c20 6974 2077 696c 6c20 6265  ided, it will be
+000185d0: 2063 6f6e 7665 7274 6564 2069 6e74 6f20   converted into 
+000185e0: 6120 7370 6172 7365 0a20 2020 2020 2020  a sparse.       
+000185f0: 2020 2020 2060 6063 7363 5f6d 6174 7269       ``csc_matri
+00018600: 7860 602e 2041 6464 6974 696f 6e61 6c6c  x``. Additionall
+00018610: 792c 2074 6865 2073 7061 7273 6520 6d61  y, the sparse ma
+00018620: 7472 6978 206e 6565 6473 2074 6f20 6265  trix needs to be
+00018630: 0a20 2020 2020 2020 2020 2020 206e 6f6e  .            non
+00018640: 6e65 6761 7469 7665 2069 6620 6069 676e  negative if `ign
+00018650: 6f72 655f 696d 706c 6963 6974 5f7a 6572  ore_implicit_zer
+00018660: 6f73 6020 6973 2046 616c 7365 2e0a 0a20  os` is False... 
+00018670: 2020 2020 2020 2052 6574 7572 6e73 0a20         Returns. 
+00018680: 2020 2020 2020 202d 2d2d 2d2d 2d2d 0a20         -------. 
+00018690: 2020 2020 2020 2058 7420 3a20 7b6e 6461         Xt : {nda
+000186a0: 7272 6179 2c20 7370 6172 7365 206d 6174  rray, sparse mat
+000186b0: 7269 787d 206f 6620 7368 6170 6520 286e  rix} of shape (n
+000186c0: 5f73 616d 706c 6573 2c20 6e5f 6665 6174  _samples, n_feat
+000186d0: 7572 6573 290a 2020 2020 2020 2020 2020  ures).          
+000186e0: 2020 5468 6520 7072 6f6a 6563 7465 6420    The projected 
+000186f0: 6461 7461 2e0a 2020 2020 2020 2020 2222  data..        ""
+00018700: 220a 2020 2020 2020 2020 6368 6563 6b5f  ".        check_
+00018710: 6973 5f66 6974 7465 6428 7365 6c66 290a  is_fitted(self).
+00018720: 2020 2020 2020 2020 5820 3d20 7365 6c66          X = self
+00018730: 2e5f 6368 6563 6b5f 696e 7075 7473 2858  ._check_inputs(X
+00018740: 2c20 696e 5f66 6974 3d46 616c 7365 2c20  , in_fit=False, 
+00018750: 636f 7079 3d73 656c 662e 636f 7079 290a  copy=self.copy).
+00018760: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+00018770: 7365 6c66 2e5f 7472 616e 7366 6f72 6d28  self._transform(
+00018780: 582c 2069 6e76 6572 7365 3d46 616c 7365  X, inverse=False
+00018790: 290a 0a20 2020 2064 6566 2069 6e76 6572  )..    def inver
+000187a0: 7365 5f74 7261 6e73 666f 726d 2873 656c  se_transform(sel
+000187b0: 662c 2058 293a 0a20 2020 2020 2020 2022  f, X):.        "
+000187c0: 2222 4261 636b 2d70 726f 6a65 6374 696f  ""Back-projectio
+000187d0: 6e20 746f 2074 6865 206f 7269 6769 6e61  n to the origina
+000187e0: 6c20 7370 6163 652e 0a0a 2020 2020 2020  l space...      
+000187f0: 2020 5061 7261 6d65 7465 7273 0a20 2020    Parameters.   
+00018800: 2020 2020 202d 2d2d 2d2d 2d2d 2d2d 2d0a       ----------.
+00018810: 2020 2020 2020 2020 5820 3a20 7b61 7272          X : {arr
+00018820: 6179 2d6c 696b 652c 2073 7061 7273 6520  ay-like, sparse 
+00018830: 6d61 7472 6978 7d20 6f66 2073 6861 7065  matrix} of shape
+00018840: 2028 6e5f 7361 6d70 6c65 732c 206e 5f66   (n_samples, n_f
+00018850: 6561 7475 7265 7329 0a20 2020 2020 2020  eatures).       
+00018860: 2020 2020 2054 6865 2064 6174 6120 7573       The data us
+00018870: 6564 2074 6f20 7363 616c 6520 616c 6f6e  ed to scale alon
+00018880: 6720 7468 6520 6665 6174 7572 6573 2061  g the features a
+00018890: 7869 732e 2049 6620 6120 7370 6172 7365  xis. If a sparse
+000188a0: 0a20 2020 2020 2020 2020 2020 206d 6174  .            mat
+000188b0: 7269 7820 6973 2070 726f 7669 6465 642c  rix is provided,
+000188c0: 2069 7420 7769 6c6c 2062 6520 636f 6e76   it will be conv
+000188d0: 6572 7465 6420 696e 746f 2061 2073 7061  erted into a spa
+000188e0: 7273 650a 2020 2020 2020 2020 2020 2020  rse.            
+000188f0: 6060 6373 635f 6d61 7472 6978 6060 2e20  ``csc_matrix``. 
+00018900: 4164 6469 7469 6f6e 616c 6c79 2c20 7468  Additionally, th
+00018910: 6520 7370 6172 7365 206d 6174 7269 7820  e sparse matrix 
+00018920: 6e65 6564 7320 746f 2062 650a 2020 2020  needs to be.    
+00018930: 2020 2020 2020 2020 6e6f 6e6e 6567 6174          nonnegat
+00018940: 6976 6520 6966 2060 6967 6e6f 7265 5f69  ive if `ignore_i
+00018950: 6d70 6c69 6369 745f 7a65 726f 7360 2069  mplicit_zeros` i
+00018960: 7320 4661 6c73 652e 0a0a 2020 2020 2020  s False...      
+00018970: 2020 5265 7475 726e 730a 2020 2020 2020    Returns.      
+00018980: 2020 2d2d 2d2d 2d2d 2d0a 2020 2020 2020    -------.      
+00018990: 2020 5874 203a 207b 6e64 6172 7261 792c    Xt : {ndarray,
+000189a0: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
+000189b0: 6f66 2028 6e5f 7361 6d70 6c65 732c 206e  of (n_samples, n
+000189c0: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
+000189d0: 2020 2020 2020 2054 6865 2070 726f 6a65         The proje
+000189e0: 6374 6564 2064 6174 612e 0a20 2020 2020  cted data..     
+000189f0: 2020 2022 2222 0a20 2020 2020 2020 2063     """.        c
+00018a00: 6865 636b 5f69 735f 6669 7474 6564 2873  heck_is_fitted(s
+00018a10: 656c 6629 0a20 2020 2020 2020 2058 203d  elf).        X =
+00018a20: 2073 656c 662e 5f63 6865 636b 5f69 6e70   self._check_inp
+00018a30: 7574 7328 0a20 2020 2020 2020 2020 2020  uts(.           
+00018a40: 2058 2c20 696e 5f66 6974 3d46 616c 7365   X, in_fit=False
+00018a50: 2c20 6163 6365 7074 5f73 7061 7273 655f  , accept_sparse_
+00018a60: 6e65 6761 7469 7665 3d54 7275 652c 2063  negative=True, c
+00018a70: 6f70 793d 7365 6c66 2e63 6f70 790a 2020  opy=self.copy.  
+00018a80: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       
+00018a90: 2072 6574 7572 6e20 7365 6c66 2e5f 7472   return self._tr
+00018aa0: 616e 7366 6f72 6d28 582c 2069 6e76 6572  ansform(X, inver
+00018ab0: 7365 3d54 7275 6529 0a0a 2020 2020 6465  se=True)..    de
+00018ac0: 6620 5f6d 6f72 655f 7461 6773 2873 656c  f _more_tags(sel
+00018ad0: 6629 3a0a 2020 2020 2020 2020 7265 7475  f):.        retu
+00018ae0: 726e 207b 2261 6c6c 6f77 5f6e 616e 223a  rn {"allow_nan":
+00018af0: 2054 7275 657d 0a0a 0a40 7661 6c69 6461   True}...@valida
+00018b00: 7465 5f70 6172 616d 7328 0a20 2020 207b  te_params(.    {
+00018b10: 2258 223a 205b 2261 7272 6179 2d6c 696b  "X": ["array-lik
+00018b20: 6522 2c20 2273 7061 7273 6520 6d61 7472  e", "sparse matr
+00018b30: 6978 225d 2c20 2261 7869 7322 3a20 5b4f  ix"], "axis": [O
+00018b40: 7074 696f 6e73 2849 6e74 6567 7261 6c2c  ptions(Integral,
+00018b50: 207b 302c 2031 7d29 5d7d 2c0a 2020 2020   {0, 1})]},.    
+00018b60: 7072 6566 6572 5f73 6b69 705f 6e65 7374  prefer_skip_nest
+00018b70: 6564 5f76 616c 6964 6174 696f 6e3d 4661  ed_validation=Fa
+00018b80: 6c73 652c 0a29 0a64 6566 2071 7561 6e74  lse,.).def quant
+00018b90: 696c 655f 7472 616e 7366 6f72 6d28 0a20  ile_transform(. 
+00018ba0: 2020 2058 2c0a 2020 2020 2a2c 0a20 2020     X,.    *,.   
+00018bb0: 2061 7869 733d 302c 0a20 2020 206e 5f71   axis=0,.    n_q
+00018bc0: 7561 6e74 696c 6573 3d31 3030 302c 0a20  uantiles=1000,. 
+00018bd0: 2020 206f 7574 7075 745f 6469 7374 7269     output_distri
+00018be0: 6275 7469 6f6e 3d22 756e 6966 6f72 6d22  bution="uniform"
+00018bf0: 2c0a 2020 2020 6967 6e6f 7265 5f69 6d70  ,.    ignore_imp
+00018c00: 6c69 6369 745f 7a65 726f 733d 4661 6c73  licit_zeros=Fals
+00018c10: 652c 0a20 2020 2073 7562 7361 6d70 6c65  e,.    subsample
+00018c20: 3d69 6e74 2831 6535 292c 0a20 2020 2072  =int(1e5),.    r
+00018c30: 616e 646f 6d5f 7374 6174 653d 4e6f 6e65  andom_state=None
+00018c40: 2c0a 2020 2020 636f 7079 3d54 7275 652c  ,.    copy=True,
+00018c50: 0a29 3a0a 2020 2020 2222 2254 7261 6e73  .):.    """Trans
+00018c60: 666f 726d 2066 6561 7475 7265 7320 7573  form features us
+00018c70: 696e 6720 7175 616e 7469 6c65 7320 696e  ing quantiles in
+00018c80: 666f 726d 6174 696f 6e2e 0a0a 2020 2020  formation...    
+00018c90: 5468 6973 206d 6574 686f 6420 7472 616e  This method tran
+00018ca0: 7366 6f72 6d73 2074 6865 2066 6561 7475  sforms the featu
+00018cb0: 7265 7320 746f 2066 6f6c 6c6f 7720 6120  res to follow a 
+00018cc0: 756e 6966 6f72 6d20 6f72 2061 206e 6f72  uniform or a nor
+00018cd0: 6d61 6c0a 2020 2020 6469 7374 7269 6275  mal.    distribu
+00018ce0: 7469 6f6e 2e20 5468 6572 6566 6f72 652c  tion. Therefore,
+00018cf0: 2066 6f72 2061 2067 6976 656e 2066 6561   for a given fea
+00018d00: 7475 7265 2c20 7468 6973 2074 7261 6e73  ture, this trans
+00018d10: 666f 726d 6174 696f 6e20 7465 6e64 730a  formation tends.
+00018d20: 2020 2020 746f 2073 7072 6561 6420 6f75      to spread ou
+00018d30: 7420 7468 6520 6d6f 7374 2066 7265 7175  t the most frequ
+00018d40: 656e 7420 7661 6c75 6573 2e20 4974 2061  ent values. It a
+00018d50: 6c73 6f20 7265 6475 6365 7320 7468 6520  lso reduces the 
+00018d60: 696d 7061 6374 206f 660a 2020 2020 286d  impact of.    (m
+00018d70: 6172 6769 6e61 6c29 206f 7574 6c69 6572  arginal) outlier
+00018d80: 733a 2074 6869 7320 6973 2074 6865 7265  s: this is there
+00018d90: 666f 7265 2061 2072 6f62 7573 7420 7072  fore a robust pr
+00018da0: 6570 726f 6365 7373 696e 6720 7363 6865  eprocessing sche
+00018db0: 6d65 2e0a 0a20 2020 2054 6865 2074 7261  me...    The tra
+00018dc0: 6e73 666f 726d 6174 696f 6e20 6973 2061  nsformation is a
+00018dd0: 7070 6c69 6564 206f 6e20 6561 6368 2066  pplied on each f
+00018de0: 6561 7475 7265 2069 6e64 6570 656e 6465  eature independe
+00018df0: 6e74 6c79 2e20 4669 7273 7420 616e 0a20  ntly. First an. 
+00018e00: 2020 2065 7374 696d 6174 6520 6f66 2074     estimate of t
+00018e10: 6865 2063 756d 756c 6174 6976 6520 6469  he cumulative di
+00018e20: 7374 7269 6275 7469 6f6e 2066 756e 6374  stribution funct
+00018e30: 696f 6e20 6f66 2061 2066 6561 7475 7265  ion of a feature
+00018e40: 2069 730a 2020 2020 7573 6564 2074 6f20   is.    used to 
+00018e50: 6d61 7020 7468 6520 6f72 6967 696e 616c  map the original
+00018e60: 2076 616c 7565 7320 746f 2061 2075 6e69   values to a uni
+00018e70: 666f 726d 2064 6973 7472 6962 7574 696f  form distributio
+00018e80: 6e2e 2054 6865 206f 6274 6169 6e65 640a  n. The obtained.
+00018e90: 2020 2020 7661 6c75 6573 2061 7265 2074      values are t
+00018ea0: 6865 6e20 6d61 7070 6564 2074 6f20 7468  hen mapped to th
+00018eb0: 6520 6465 7369 7265 6420 6f75 7470 7574  e desired output
+00018ec0: 2064 6973 7472 6962 7574 696f 6e20 7573   distribution us
+00018ed0: 696e 6720 7468 650a 2020 2020 6173 736f  ing the.    asso
+00018ee0: 6369 6174 6564 2071 7561 6e74 696c 6520  ciated quantile 
+00018ef0: 6675 6e63 7469 6f6e 2e20 4665 6174 7572  function. Featur
+00018f00: 6573 2076 616c 7565 7320 6f66 206e 6577  es values of new
+00018f10: 2f75 6e73 6565 6e20 6461 7461 2074 6861  /unseen data tha
+00018f20: 7420 6661 6c6c 0a20 2020 2062 656c 6f77  t fall.    below
+00018f30: 206f 7220 6162 6f76 6520 7468 6520 6669   or above the fi
+00018f40: 7474 6564 2072 616e 6765 2077 696c 6c20  tted range will 
+00018f50: 6265 206d 6170 7065 6420 746f 2074 6865  be mapped to the
+00018f60: 2062 6f75 6e64 7320 6f66 2074 6865 206f   bounds of the o
+00018f70: 7574 7075 740a 2020 2020 6469 7374 7269  utput.    distri
+00018f80: 6275 7469 6f6e 2e20 4e6f 7465 2074 6861  bution. Note tha
+00018f90: 7420 7468 6973 2074 7261 6e73 666f 726d  t this transform
+00018fa0: 2069 7320 6e6f 6e2d 6c69 6e65 6172 2e20   is non-linear. 
+00018fb0: 4974 206d 6179 2064 6973 746f 7274 206c  It may distort l
+00018fc0: 696e 6561 720a 2020 2020 636f 7272 656c  inear.    correl
+00018fd0: 6174 696f 6e73 2062 6574 7765 656e 2076  ations between v
+00018fe0: 6172 6961 626c 6573 206d 6561 7375 7265  ariables measure
+00018ff0: 6420 6174 2074 6865 2073 616d 6520 7363  d at the same sc
+00019000: 616c 6520 6275 7420 7265 6e64 6572 730a  ale but renders.
+00019010: 2020 2020 7661 7269 6162 6c65 7320 6d65      variables me
+00019020: 6173 7572 6564 2061 7420 6469 6666 6572  asured at differ
+00019030: 656e 7420 7363 616c 6573 206d 6f72 6520  ent scales more 
+00019040: 6469 7265 6374 6c79 2063 6f6d 7061 7261  directly compara
+00019050: 626c 652e 0a0a 2020 2020 5265 6164 206d  ble...    Read m
+00019060: 6f72 6520 696e 2074 6865 203a 7265 663a  ore in the :ref:
+00019070: 6055 7365 7220 4775 6964 6520 3c70 7265  `User Guide <pre
+00019080: 7072 6f63 6573 7369 6e67 5f74 7261 6e73  processing_trans
+00019090: 666f 726d 6572 3e60 2e0a 0a20 2020 2050  former>`...    P
+000190a0: 6172 616d 6574 6572 730a 2020 2020 2d2d  arameters.    --
+000190b0: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2058 203a  --------.    X :
+000190c0: 207b 6172 7261 792d 6c69 6b65 2c20 7370   {array-like, sp
+000190d0: 6172 7365 206d 6174 7269 787d 206f 6620  arse matrix} of 
+000190e0: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
+000190f0: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
+00019100: 2020 2020 2020 5468 6520 6461 7461 2074        The data t
+00019110: 6f20 7472 616e 7366 6f72 6d2e 0a0a 2020  o transform...  
+00019120: 2020 6178 6973 203a 2069 6e74 2c20 6465    axis : int, de
+00019130: 6661 756c 743d 300a 2020 2020 2020 2020  fault=0.        
+00019140: 4178 6973 2075 7365 6420 746f 2063 6f6d  Axis used to com
+00019150: 7075 7465 2074 6865 206d 6561 6e73 2061  pute the means a
+00019160: 6e64 2073 7461 6e64 6172 6420 6465 7669  nd standard devi
+00019170: 6174 696f 6e73 2061 6c6f 6e67 2e20 4966  ations along. If
+00019180: 2030 2c0a 2020 2020 2020 2020 7472 616e   0,.        tran
+00019190: 7366 6f72 6d20 6561 6368 2066 6561 7475  sform each featu
+000191a0: 7265 2c20 6f74 6865 7277 6973 6520 2869  re, otherwise (i
+000191b0: 6620 3129 2074 7261 6e73 666f 726d 2065  f 1) transform e
+000191c0: 6163 6820 7361 6d70 6c65 2e0a 0a20 2020  ach sample...   
+000191d0: 206e 5f71 7561 6e74 696c 6573 203a 2069   n_quantiles : i
+000191e0: 6e74 2c20 6465 6661 756c 743d 3130 3030  nt, default=1000
+000191f0: 206f 7220 6e5f 7361 6d70 6c65 730a 2020   or n_samples.  
+00019200: 2020 2020 2020 4e75 6d62 6572 206f 6620        Number of 
+00019210: 7175 616e 7469 6c65 7320 746f 2062 6520  quantiles to be 
+00019220: 636f 6d70 7574 6564 2e20 4974 2063 6f72  computed. It cor
+00019230: 7265 7370 6f6e 6473 2074 6f20 7468 6520  responds to the 
+00019240: 6e75 6d62 6572 0a20 2020 2020 2020 206f  number.        o
+00019250: 6620 6c61 6e64 6d61 726b 7320 7573 6564  f landmarks used
+00019260: 2074 6f20 6469 7363 7265 7469 7a65 2074   to discretize t
+00019270: 6865 2063 756d 756c 6174 6976 6520 6469  he cumulative di
+00019280: 7374 7269 6275 7469 6f6e 2066 756e 6374  stribution funct
+00019290: 696f 6e2e 0a20 2020 2020 2020 2049 6620  ion..        If 
+000192a0: 6e5f 7175 616e 7469 6c65 7320 6973 206c  n_quantiles is l
+000192b0: 6172 6765 7220 7468 616e 2074 6865 206e  arger than the n
+000192c0: 756d 6265 7220 6f66 2073 616d 706c 6573  umber of samples
+000192d0: 2c20 6e5f 7175 616e 7469 6c65 7320 6973  , n_quantiles is
+000192e0: 2073 6574 0a20 2020 2020 2020 2074 6f20   set.        to 
+000192f0: 7468 6520 6e75 6d62 6572 206f 6620 7361  the number of sa
+00019300: 6d70 6c65 7320 6173 2061 206c 6172 6765  mples as a large
+00019310: 7220 6e75 6d62 6572 206f 6620 7175 616e  r number of quan
+00019320: 7469 6c65 7320 646f 6573 206e 6f74 2067  tiles does not g
+00019330: 6976 650a 2020 2020 2020 2020 6120 6265  ive.        a be
+00019340: 7474 6572 2061 7070 726f 7869 6d61 7469  tter approximati
+00019350: 6f6e 206f 6620 7468 6520 6375 6d75 6c61  on of the cumula
+00019360: 7469 7665 2064 6973 7472 6962 7574 696f  tive distributio
+00019370: 6e20 6675 6e63 7469 6f6e 0a20 2020 2020  n function.     
+00019380: 2020 2065 7374 696d 6174 6f72 2e0a 0a20     estimator... 
+00019390: 2020 206f 7574 7075 745f 6469 7374 7269     output_distri
+000193a0: 6275 7469 6f6e 203a 207b 2775 6e69 666f  bution : {'unifo
+000193b0: 726d 272c 2027 6e6f 726d 616c 277d 2c20  rm', 'normal'}, 
+000193c0: 6465 6661 756c 743d 2775 6e69 666f 726d  default='uniform
+000193d0: 270a 2020 2020 2020 2020 4d61 7267 696e  '.        Margin
+000193e0: 616c 2064 6973 7472 6962 7574 696f 6e20  al distribution 
+000193f0: 666f 7220 7468 6520 7472 616e 7366 6f72  for the transfor
+00019400: 6d65 6420 6461 7461 2e20 5468 6520 6368  med data. The ch
+00019410: 6f69 6365 7320 6172 650a 2020 2020 2020  oices are.      
+00019420: 2020 2775 6e69 666f 726d 2720 2864 6566    'uniform' (def
+00019430: 6175 6c74 2920 6f72 2027 6e6f 726d 616c  ault) or 'normal
+00019440: 272e 0a0a 2020 2020 6967 6e6f 7265 5f69  '...    ignore_i
+00019450: 6d70 6c69 6369 745f 7a65 726f 7320 3a20  mplicit_zeros : 
+00019460: 626f 6f6c 2c20 6465 6661 756c 743d 4661  bool, default=Fa
+00019470: 6c73 650a 2020 2020 2020 2020 4f6e 6c79  lse.        Only
+00019480: 2061 7070 6c69 6573 2074 6f20 7370 6172   applies to spar
+00019490: 7365 206d 6174 7269 6365 732e 2049 6620  se matrices. If 
+000194a0: 5472 7565 2c20 7468 6520 7370 6172 7365  True, the sparse
+000194b0: 2065 6e74 7269 6573 206f 6620 7468 650a   entries of the.
+000194c0: 2020 2020 2020 2020 6d61 7472 6978 2061          matrix a
+000194d0: 7265 2064 6973 6361 7264 6564 2074 6f20  re discarded to 
+000194e0: 636f 6d70 7574 6520 7468 6520 7175 616e  compute the quan
+000194f0: 7469 6c65 2073 7461 7469 7374 6963 732e  tile statistics.
+00019500: 2049 6620 4661 6c73 652c 0a20 2020 2020   If False,.     
+00019510: 2020 2074 6865 7365 2065 6e74 7269 6573     these entries
+00019520: 2061 7265 2074 7265 6174 6564 2061 7320   are treated as 
+00019530: 7a65 726f 732e 0a0a 2020 2020 7375 6273  zeros...    subs
+00019540: 616d 706c 6520 3a20 696e 7420 6f72 204e  ample : int or N
+00019550: 6f6e 652c 2064 6566 6175 6c74 3d31 6535  one, default=1e5
+00019560: 0a20 2020 2020 2020 204d 6178 696d 756d  .        Maximum
+00019570: 206e 756d 6265 7220 6f66 2073 616d 706c   number of sampl
+00019580: 6573 2075 7365 6420 746f 2065 7374 696d  es used to estim
+00019590: 6174 6520 7468 6520 7175 616e 7469 6c65  ate the quantile
+000195a0: 7320 666f 720a 2020 2020 2020 2020 636f  s for.        co
+000195b0: 6d70 7574 6174 696f 6e61 6c20 6566 6669  mputational effi
+000195c0: 6369 656e 6379 2e20 4e6f 7465 2074 6861  ciency. Note tha
+000195d0: 7420 7468 6520 7375 6273 616d 706c 696e  t the subsamplin
+000195e0: 6720 7072 6f63 6564 7572 6520 6d61 790a  g procedure may.
+000195f0: 2020 2020 2020 2020 6469 6666 6572 2066          differ f
+00019600: 6f72 2076 616c 7565 2d69 6465 6e74 6963  or value-identic
+00019610: 616c 2073 7061 7273 6520 616e 6420 6465  al sparse and de
+00019620: 6e73 6520 6d61 7472 6963 6573 2e0a 2020  nse matrices..  
+00019630: 2020 2020 2020 4469 7361 626c 6520 7375        Disable su
+00019640: 6273 616d 706c 696e 6720 6279 2073 6574  bsampling by set
+00019650: 7469 6e67 2060 7375 6273 616d 706c 653d  ting `subsample=
+00019660: 4e6f 6e65 602e 0a0a 2020 2020 2020 2020  None`...        
+00019670: 2e2e 2076 6572 7369 6f6e 6164 6465 643a  .. versionadded:
+00019680: 3a20 312e 350a 2020 2020 2020 2020 2020  : 1.5.          
+00019690: 2054 6865 206f 7074 696f 6e20 604e 6f6e   The option `Non
+000196a0: 6560 2074 6f20 6469 7361 626c 6520 7375  e` to disable su
+000196b0: 6273 616d 706c 696e 6720 7761 7320 6164  bsampling was ad
+000196c0: 6465 642e 0a0a 2020 2020 7261 6e64 6f6d  ded...    random
+000196d0: 5f73 7461 7465 203a 2069 6e74 2c20 5261  _state : int, Ra
+000196e0: 6e64 6f6d 5374 6174 6520 696e 7374 616e  ndomState instan
+000196f0: 6365 206f 7220 4e6f 6e65 2c20 6465 6661  ce or None, defa
+00019700: 756c 743d 4e6f 6e65 0a20 2020 2020 2020  ult=None.       
+00019710: 2044 6574 6572 6d69 6e65 7320 7261 6e64   Determines rand
+00019720: 6f6d 206e 756d 6265 7220 6765 6e65 7261  om number genera
+00019730: 7469 6f6e 2066 6f72 2073 7562 7361 6d70  tion for subsamp
+00019740: 6c69 6e67 2061 6e64 2073 6d6f 6f74 6869  ling and smoothi
+00019750: 6e67 0a20 2020 2020 2020 206e 6f69 7365  ng.        noise
+00019760: 2e0a 2020 2020 2020 2020 506c 6561 7365  ..        Please
+00019770: 2073 6565 2060 6073 7562 7361 6d70 6c65   see ``subsample
+00019780: 6060 2066 6f72 206d 6f72 6520 6465 7461  `` for more deta
+00019790: 696c 732e 0a20 2020 2020 2020 2050 6173  ils..        Pas
+000197a0: 7320 616e 2069 6e74 2066 6f72 2072 6570  s an int for rep
+000197b0: 726f 6475 6369 626c 6520 7265 7375 6c74  roducible result
+000197c0: 7320 6163 726f 7373 206d 756c 7469 706c  s across multipl
+000197d0: 6520 6675 6e63 7469 6f6e 2063 616c 6c73  e function calls
+000197e0: 2e0a 2020 2020 2020 2020 5365 6520 3a74  ..        See :t
+000197f0: 6572 6d3a 6047 6c6f 7373 6172 7920 3c72  erm:`Glossary <r
+00019800: 616e 646f 6d5f 7374 6174 653e 602e 0a0a  andom_state>`...
+00019810: 2020 2020 636f 7079 203a 2062 6f6f 6c2c      copy : bool,
+00019820: 2064 6566 6175 6c74 3d54 7275 650a 2020   default=True.  
+00019830: 2020 2020 2020 4966 2046 616c 7365 2c20        If False, 
+00019840: 7472 7920 746f 2061 766f 6964 2061 2063  try to avoid a c
+00019850: 6f70 7920 616e 6420 7472 616e 7366 6f72  opy and transfor
+00019860: 6d20 696e 2070 6c61 6365 2e0a 2020 2020  m in place..    
+00019870: 2020 2020 5468 6973 2069 7320 6e6f 7420      This is not 
+00019880: 6775 6172 616e 7465 6564 2074 6f20 616c  guaranteed to al
+00019890: 7761 7973 2077 6f72 6b20 696e 2070 6c61  ways work in pla
+000198a0: 6365 3b20 652e 672e 2069 6620 7468 6520  ce; e.g. if the 
+000198b0: 6461 7461 2069 730a 2020 2020 2020 2020  data is.        
+000198c0: 6120 6e75 6d70 7920 6172 7261 7920 7769  a numpy array wi
+000198d0: 7468 2061 6e20 696e 7420 6474 7970 652c  th an int dtype,
+000198e0: 2061 2063 6f70 7920 7769 6c6c 2062 6520   a copy will be 
+000198f0: 7265 7475 726e 6564 2065 7665 6e20 7769  returned even wi
+00019900: 7468 0a20 2020 2020 2020 2063 6f70 793d  th.        copy=
+00019910: 4661 6c73 652e 0a0a 2020 2020 2020 2020  False...        
+00019920: 2e2e 2076 6572 7369 6f6e 6368 616e 6765  .. versionchange
+00019930: 643a 3a20 302e 3233 0a20 2020 2020 2020  d:: 0.23.       
+00019940: 2020 2020 2054 6865 2064 6566 6175 6c74       The default
+00019950: 2076 616c 7565 206f 6620 6063 6f70 7960   value of `copy`
+00019960: 2063 6861 6e67 6564 2066 726f 6d20 4661   changed from Fa
+00019970: 6c73 6520 746f 2054 7275 6520 696e 2030  lse to True in 0
+00019980: 2e32 332e 0a0a 2020 2020 5265 7475 726e  .23...    Return
+00019990: 730a 2020 2020 2d2d 2d2d 2d2d 2d0a 2020  s.    -------.  
+000199a0: 2020 5874 203a 207b 6e64 6172 7261 792c    Xt : {ndarray,
+000199b0: 2073 7061 7273 6520 6d61 7472 6978 7d20   sparse matrix} 
+000199c0: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
+000199d0: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
+000199e0: 0a20 2020 2020 2020 2054 6865 2074 7261  .        The tra
+000199f0: 6e73 666f 726d 6564 2064 6174 612e 0a0a  nsformed data...
+00019a00: 2020 2020 5365 6520 416c 736f 0a20 2020      See Also.   
+00019a10: 202d 2d2d 2d2d 2d2d 2d0a 2020 2020 5175   --------.    Qu
+00019a20: 616e 7469 6c65 5472 616e 7366 6f72 6d65  antileTransforme
+00019a30: 7220 3a20 5065 7266 6f72 6d73 2071 7561  r : Performs qua
+00019a40: 6e74 696c 652d 6261 7365 6420 7363 616c  ntile-based scal
+00019a50: 696e 6720 7573 696e 6720 7468 650a 2020  ing using the.  
+00019a60: 2020 2020 2020 5472 616e 7366 6f72 6d65        Transforme
+00019a70: 7220 4150 4920 2865 2e67 2e20 6173 2070  r API (e.g. as p
+00019a80: 6172 7420 6f66 2061 2070 7265 7072 6f63  art of a preproc
+00019a90: 6573 7369 6e67 0a20 2020 2020 2020 203a  essing.        :
+00019aa0: 636c 6173 733a 607e 736b 6c65 6172 6e2e  class:`~sklearn.
+00019ab0: 7069 7065 6c69 6e65 2e50 6970 656c 696e  pipeline.Pipelin
+00019ac0: 6560 292e 0a20 2020 2070 6f77 6572 5f74  e`)..    power_t
+00019ad0: 7261 6e73 666f 726d 203a 204d 6170 7320  ransform : Maps 
+00019ae0: 6461 7461 2074 6f20 6120 6e6f 726d 616c  data to a normal
+00019af0: 2064 6973 7472 6962 7574 696f 6e20 7573   distribution us
+00019b00: 696e 6720 610a 2020 2020 2020 2020 706f  ing a.        po
+00019b10: 7765 7220 7472 616e 7366 6f72 6d61 7469  wer transformati
+00019b20: 6f6e 2e0a 2020 2020 7363 616c 6520 3a20  on..    scale : 
+00019b30: 5065 7266 6f72 6d73 2073 7461 6e64 6172  Performs standar
+00019b40: 6469 7a61 7469 6f6e 2074 6861 7420 6973  dization that is
+00019b50: 2066 6173 7465 722c 2062 7574 206c 6573   faster, but les
+00019b60: 7320 726f 6275 7374 0a20 2020 2020 2020  s robust.       
+00019b70: 2074 6f20 6f75 746c 6965 7273 2e0a 2020   to outliers..  
+00019b80: 2020 726f 6275 7374 5f73 6361 6c65 203a    robust_scale :
+00019b90: 2050 6572 666f 726d 7320 726f 6275 7374   Performs robust
+00019ba0: 2073 7461 6e64 6172 6469 7a61 7469 6f6e   standardization
+00019bb0: 2074 6861 7420 7265 6d6f 7665 7320 7468   that removes th
+00019bc0: 6520 696e 666c 7565 6e63 650a 2020 2020  e influence.    
+00019bd0: 2020 2020 6f66 206f 7574 6c69 6572 7320      of outliers 
+00019be0: 6275 7420 646f 6573 206e 6f74 2070 7574  but does not put
+00019bf0: 206f 7574 6c69 6572 7320 616e 6420 696e   outliers and in
+00019c00: 6c69 6572 7320 6f6e 2074 6865 2073 616d  liers on the sam
+00019c10: 6520 7363 616c 652e 0a0a 2020 2020 4e6f  e scale...    No
+00019c20: 7465 730a 2020 2020 2d2d 2d2d 2d0a 2020  tes.    -----.  
+00019c30: 2020 4e61 4e73 2061 7265 2074 7265 6174    NaNs are treat
+00019c40: 6564 2061 7320 6d69 7373 696e 6720 7661  ed as missing va
+00019c50: 6c75 6573 3a20 6469 7372 6567 6172 6465  lues: disregarde
+00019c60: 6420 696e 2066 6974 2c20 616e 6420 6d61  d in fit, and ma
+00019c70: 696e 7461 696e 6564 2069 6e0a 2020 2020  intained in.    
+00019c80: 7472 616e 7366 6f72 6d2e 0a0a 2020 2020  transform...    
+00019c90: 2e2e 2077 6172 6e69 6e67 3a3a 2052 6973  .. warning:: Ris
+00019ca0: 6b20 6f66 2064 6174 6120 6c65 616b 0a0a  k of data leak..
+00019cb0: 2020 2020 2020 2020 446f 206e 6f74 2075          Do not u
+00019cc0: 7365 203a 6675 6e63 3a60 7e73 6b6c 6561  se :func:`~sklea
+00019cd0: 726e 2e70 7265 7072 6f63 6573 7369 6e67  rn.preprocessing
+00019ce0: 2e71 7561 6e74 696c 655f 7472 616e 7366  .quantile_transf
+00019cf0: 6f72 6d60 2075 6e6c 6573 730a 2020 2020  orm` unless.    
+00019d00: 2020 2020 796f 7520 6b6e 6f77 2077 6861      you know wha
+00019d10: 7420 796f 7520 6172 6520 646f 696e 672e  t you are doing.
+00019d20: 2041 2063 6f6d 6d6f 6e20 6d69 7374 616b   A common mistak
+00019d30: 6520 6973 2074 6f20 6170 706c 7920 6974  e is to apply it
+00019d40: 0a20 2020 2020 2020 2074 6f20 7468 6520  .        to the 
+00019d50: 656e 7469 7265 2064 6174 6120 2a62 6566  entire data *bef
+00019d60: 6f72 652a 2073 706c 6974 7469 6e67 2069  ore* splitting i
+00019d70: 6e74 6f20 7472 6169 6e69 6e67 2061 6e64  nto training and
+00019d80: 0a20 2020 2020 2020 2074 6573 7420 7365  .        test se
+00019d90: 7473 2e20 5468 6973 2077 696c 6c20 6269  ts. This will bi
+00019da0: 6173 2074 6865 206d 6f64 656c 2065 7661  as the model eva
+00019db0: 6c75 6174 696f 6e20 6265 6361 7573 650a  luation because.
+00019dc0: 2020 2020 2020 2020 696e 666f 726d 6174          informat
+00019dd0: 696f 6e20 776f 756c 6420 6861 7665 206c  ion would have l
+00019de0: 6561 6b65 6420 6672 6f6d 2074 6865 2074  eaked from the t
+00019df0: 6573 7420 7365 7420 746f 2074 6865 0a20  est set to the. 
+00019e00: 2020 2020 2020 2074 7261 696e 696e 6720         training 
+00019e10: 7365 742e 0a20 2020 2020 2020 2049 6e20  set..        In 
+00019e20: 6765 6e65 7261 6c2c 2077 6520 7265 636f  general, we reco
+00019e30: 6d6d 656e 6420 7573 696e 670a 2020 2020  mmend using.    
+00019e40: 2020 2020 3a63 6c61 7373 3a60 7e73 6b6c      :class:`~skl
+00019e50: 6561 726e 2e70 7265 7072 6f63 6573 7369  earn.preprocessi
+00019e60: 6e67 2e51 7561 6e74 696c 6554 7261 6e73  ng.QuantileTrans
+00019e70: 666f 726d 6572 6020 7769 7468 696e 2061  former` within a
+00019e80: 0a20 2020 2020 2020 203a 7265 663a 6050  .        :ref:`P
+00019e90: 6970 656c 696e 6520 3c70 6970 656c 696e  ipeline <pipelin
+00019ea0: 653e 6020 696e 206f 7264 6572 2074 6f20  e>` in order to 
+00019eb0: 7072 6576 656e 7420 6d6f 7374 2072 6973  prevent most ris
+00019ec0: 6b73 206f 6620 6461 7461 0a20 2020 2020  ks of data.     
+00019ed0: 2020 206c 6561 6b69 6e67 3a60 7069 7065     leaking:`pipe
+00019ee0: 203d 206d 616b 655f 7069 7065 6c69 6e65   = make_pipeline
+00019ef0: 2851 7561 6e74 696c 6554 7261 6e73 666f  (QuantileTransfo
+00019f00: 726d 6572 2829 2c0a 2020 2020 2020 2020  rmer(),.        
+00019f10: 4c6f 6769 7374 6963 5265 6772 6573 7369  LogisticRegressi
+00019f20: 6f6e 2829 2960 2e0a 0a20 2020 2046 6f72  on())`...    For
+00019f30: 2061 2063 6f6d 7061 7269 736f 6e20 6f66   a comparison of
+00019f40: 2074 6865 2064 6966 6665 7265 6e74 2073   the different s
+00019f50: 6361 6c65 7273 2c20 7472 616e 7366 6f72  calers, transfor
+00019f60: 6d65 7273 2c20 616e 6420 6e6f 726d 616c  mers, and normal
+00019f70: 697a 6572 732c 0a20 2020 2073 6565 3a20  izers,.    see: 
+00019f80: 3a72 6566 3a60 7370 6878 5f67 6c72 5f61  :ref:`sphx_glr_a
+00019f90: 7574 6f5f 6578 616d 706c 6573 5f70 7265  uto_examples_pre
+00019fa0: 7072 6f63 6573 7369 6e67 5f70 6c6f 745f  processing_plot_
+00019fb0: 616c 6c5f 7363 616c 696e 672e 7079 602e  all_scaling.py`.
+00019fc0: 0a0a 2020 2020 4578 616d 706c 6573 0a20  ..    Examples. 
+00019fd0: 2020 202d 2d2d 2d2d 2d2d 2d0a 2020 2020     --------.    
+00019fe0: 3e3e 3e20 696d 706f 7274 206e 756d 7079  >>> import numpy
+00019ff0: 2061 7320 6e70 0a20 2020 203e 3e3e 2066   as np.    >>> f
+0001a000: 726f 6d20 736b 6c65 6172 6e2e 7072 6570  rom sklearn.prep
+0001a010: 726f 6365 7373 696e 6720 696d 706f 7274  rocessing import
+0001a020: 2071 7561 6e74 696c 655f 7472 616e 7366   quantile_transf
+0001a030: 6f72 6d0a 2020 2020 3e3e 3e20 726e 6720  orm.    >>> rng 
+0001a040: 3d20 6e70 2e72 616e 646f 6d2e 5261 6e64  = np.random.Rand
+0001a050: 6f6d 5374 6174 6528 3029 0a20 2020 203e  omState(0).    >
+0001a060: 3e3e 2058 203d 206e 702e 736f 7274 2872  >> X = np.sort(r
+0001a070: 6e67 2e6e 6f72 6d61 6c28 6c6f 633d 302e  ng.normal(loc=0.
+0001a080: 352c 2073 6361 6c65 3d30 2e32 352c 2073  5, scale=0.25, s
+0001a090: 697a 653d 2832 352c 2031 2929 2c20 6178  ize=(25, 1)), ax
+0001a0a0: 6973 3d30 290a 2020 2020 3e3e 3e20 7175  is=0).    >>> qu
+0001a0b0: 616e 7469 6c65 5f74 7261 6e73 666f 726d  antile_transform
+0001a0c0: 2858 2c20 6e5f 7175 616e 7469 6c65 733d  (X, n_quantiles=
+0001a0d0: 3130 2c20 7261 6e64 6f6d 5f73 7461 7465  10, random_state
+0001a0e0: 3d30 2c20 636f 7079 3d54 7275 6529 0a20  =0, copy=True). 
+0001a0f0: 2020 2061 7272 6179 285b 2e2e 2e5d 290a     array([...]).
+0001a100: 2020 2020 2222 220a 2020 2020 6e20 3d20      """.    n = 
+0001a110: 5175 616e 7469 6c65 5472 616e 7366 6f72  QuantileTransfor
+0001a120: 6d65 7228 0a20 2020 2020 2020 206e 5f71  mer(.        n_q
+0001a130: 7561 6e74 696c 6573 3d6e 5f71 7561 6e74  uantiles=n_quant
+0001a140: 696c 6573 2c0a 2020 2020 2020 2020 6f75  iles,.        ou
+0001a150: 7470 7574 5f64 6973 7472 6962 7574 696f  tput_distributio
+0001a160: 6e3d 6f75 7470 7574 5f64 6973 7472 6962  n=output_distrib
+0001a170: 7574 696f 6e2c 0a20 2020 2020 2020 2073  ution,.        s
+0001a180: 7562 7361 6d70 6c65 3d73 7562 7361 6d70  ubsample=subsamp
+0001a190: 6c65 2c0a 2020 2020 2020 2020 6967 6e6f  le,.        igno
+0001a1a0: 7265 5f69 6d70 6c69 6369 745f 7a65 726f  re_implicit_zero
+0001a1b0: 733d 6967 6e6f 7265 5f69 6d70 6c69 6369  s=ignore_implici
+0001a1c0: 745f 7a65 726f 732c 0a20 2020 2020 2020  t_zeros,.       
+0001a1d0: 2072 616e 646f 6d5f 7374 6174 653d 7261   random_state=ra
+0001a1e0: 6e64 6f6d 5f73 7461 7465 2c0a 2020 2020  ndom_state,.    
+0001a1f0: 2020 2020 636f 7079 3d63 6f70 792c 0a20      copy=copy,. 
+0001a200: 2020 2029 0a20 2020 2069 6620 6178 6973     ).    if axis
+0001a210: 203d 3d20 303a 0a20 2020 2020 2020 2058   == 0:.        X
+0001a220: 203d 206e 2e66 6974 5f74 7261 6e73 666f   = n.fit_transfo
+0001a230: 726d 2858 290a 2020 2020 656c 7365 3a20  rm(X).    else: 
+0001a240: 2023 2061 7869 7320 3d3d 2031 0a20 2020   # axis == 1.   
+0001a250: 2020 2020 2058 203d 206e 2e66 6974 5f74       X = n.fit_t
+0001a260: 7261 6e73 666f 726d 2858 2e54 292e 540a  ransform(X.T).T.
+0001a270: 2020 2020 7265 7475 726e 2058 0a0a 0a63      return X...c
+0001a280: 6c61 7373 2050 6f77 6572 5472 616e 7366  lass PowerTransf
+0001a290: 6f72 6d65 7228 4f6e 6554 6f4f 6e65 4665  ormer(OneToOneFe
+0001a2a0: 6174 7572 654d 6978 696e 2c20 5472 616e  atureMixin, Tran
+0001a2b0: 7366 6f72 6d65 724d 6978 696e 2c20 4261  sformerMixin, Ba
+0001a2c0: 7365 4573 7469 6d61 746f 7229 3a0a 2020  seEstimator):.  
+0001a2d0: 2020 2222 2241 7070 6c79 2061 2070 6f77    """Apply a pow
+0001a2e0: 6572 2074 7261 6e73 666f 726d 2066 6561  er transform fea
+0001a2f0: 7475 7265 7769 7365 2074 6f20 6d61 6b65  turewise to make
+0001a300: 2064 6174 6120 6d6f 7265 2047 6175 7373   data more Gauss
+0001a310: 6961 6e2d 6c69 6b65 2e0a 0a20 2020 2050  ian-like...    P
+0001a320: 6f77 6572 2074 7261 6e73 666f 726d 7320  ower transforms 
+0001a330: 6172 6520 6120 6661 6d69 6c79 206f 6620  are a family of 
+0001a340: 7061 7261 6d65 7472 6963 2c20 6d6f 6e6f  parametric, mono
+0001a350: 746f 6e69 6320 7472 616e 7366 6f72 6d61  tonic transforma
+0001a360: 7469 6f6e 730a 2020 2020 7468 6174 2061  tions.    that a
+0001a370: 7265 2061 7070 6c69 6564 2074 6f20 6d61  re applied to ma
+0001a380: 6b65 2064 6174 6120 6d6f 7265 2047 6175  ke data more Gau
+0001a390: 7373 6961 6e2d 6c69 6b65 2e20 5468 6973  ssian-like. This
+0001a3a0: 2069 7320 7573 6566 756c 2066 6f72 0a20   is useful for. 
+0001a3b0: 2020 206d 6f64 656c 696e 6720 6973 7375     modeling issu
+0001a3c0: 6573 2072 656c 6174 6564 2074 6f20 6865  es related to he
+0001a3d0: 7465 726f 7363 6564 6173 7469 6369 7479  teroscedasticity
+0001a3e0: 2028 6e6f 6e2d 636f 6e73 7461 6e74 2076   (non-constant v
+0001a3f0: 6172 6961 6e63 6529 2c0a 2020 2020 6f72  ariance),.    or
+0001a400: 206f 7468 6572 2073 6974 7561 7469 6f6e   other situation
+0001a410: 7320 7768 6572 6520 6e6f 726d 616c 6974  s where normalit
+0001a420: 7920 6973 2064 6573 6972 6564 2e0a 0a20  y is desired... 
+0001a430: 2020 2043 7572 7265 6e74 6c79 2c20 506f     Currently, Po
+0001a440: 7765 7254 7261 6e73 666f 726d 6572 2073  werTransformer s
+0001a450: 7570 706f 7274 7320 7468 6520 426f 782d  upports the Box-
+0001a460: 436f 7820 7472 616e 7366 6f72 6d20 616e  Cox transform an
+0001a470: 6420 7468 650a 2020 2020 5965 6f2d 4a6f  d the.    Yeo-Jo
+0001a480: 686e 736f 6e20 7472 616e 7366 6f72 6d2e  hnson transform.
+0001a490: 2054 6865 206f 7074 696d 616c 2070 6172   The optimal par
+0001a4a0: 616d 6574 6572 2066 6f72 2073 7461 6269  ameter for stabi
+0001a4b0: 6c69 7a69 6e67 2076 6172 6961 6e63 6520  lizing variance 
+0001a4c0: 616e 640a 2020 2020 6d69 6e69 6d69 7a69  and.    minimizi
+0001a4d0: 6e67 2073 6b65 776e 6573 7320 6973 2065  ng skewness is e
+0001a4e0: 7374 696d 6174 6564 2074 6872 6f75 6768  stimated through
+0001a4f0: 206d 6178 696d 756d 206c 696b 656c 6968   maximum likelih
+0001a500: 6f6f 642e 0a0a 2020 2020 426f 782d 436f  ood...    Box-Co
+0001a510: 7820 7265 7175 6972 6573 2069 6e70 7574  x requires input
+0001a520: 2064 6174 6120 746f 2062 6520 7374 7269   data to be stri
+0001a530: 6374 6c79 2070 6f73 6974 6976 652c 2077  ctly positive, w
+0001a540: 6869 6c65 2059 656f 2d4a 6f68 6e73 6f6e  hile Yeo-Johnson
+0001a550: 0a20 2020 2073 7570 706f 7274 7320 626f  .    supports bo
+0001a560: 7468 2070 6f73 6974 6976 6520 6f72 206e  th positive or n
+0001a570: 6567 6174 6976 6520 6461 7461 2e0a 0a20  egative data... 
+0001a580: 2020 2042 7920 6465 6661 756c 742c 207a     By default, z
+0001a590: 6572 6f2d 6d65 616e 2c20 756e 6974 2d76  ero-mean, unit-v
+0001a5a0: 6172 6961 6e63 6520 6e6f 726d 616c 697a  ariance normaliz
+0001a5b0: 6174 696f 6e20 6973 2061 7070 6c69 6564  ation is applied
+0001a5c0: 2074 6f20 7468 650a 2020 2020 7472 616e   to the.    tran
+0001a5d0: 7366 6f72 6d65 6420 6461 7461 2e0a 0a20  sformed data... 
+0001a5e0: 2020 2046 6f72 2061 6e20 6578 616d 706c     For an exampl
+0001a5f0: 6520 7669 7375 616c 697a 6174 696f 6e2c  e visualization,
+0001a600: 2072 6566 6572 2074 6f20 3a72 6566 3a60   refer to :ref:`
+0001a610: 436f 6d70 6172 6520 506f 7765 7254 7261  Compare PowerTra
+0001a620: 6e73 666f 726d 6572 2077 6974 680a 2020  nsformer with.  
+0001a630: 2020 6f74 6865 7220 7363 616c 6572 7320    other scalers 
+0001a640: 3c70 6c6f 745f 616c 6c5f 7363 616c 696e  <plot_all_scalin
+0001a650: 675f 706f 7765 725f 7472 616e 7366 6f72  g_power_transfor
+0001a660: 6d65 725f 7365 6374 696f 6e3e 602e 2054  mer_section>`. T
+0001a670: 6f20 7365 6520 7468 650a 2020 2020 6566  o see the.    ef
+0001a680: 6665 6374 206f 6620 426f 782d 436f 7820  fect of Box-Cox 
+0001a690: 616e 6420 5965 6f2d 4a6f 686e 736f 6e20  and Yeo-Johnson 
+0001a6a0: 7472 616e 7366 6f72 6d61 7469 6f6e 7320  transformations 
+0001a6b0: 6f6e 2064 6966 6665 7265 6e74 0a20 2020  on different.   
+0001a6c0: 2064 6973 7472 6962 7574 696f 6e73 2c20   distributions, 
+0001a6d0: 7365 653a 0a20 2020 203a 7265 663a 6073  see:.    :ref:`s
+0001a6e0: 7068 785f 676c 725f 6175 746f 5f65 7861  phx_glr_auto_exa
+0001a6f0: 6d70 6c65 735f 7072 6570 726f 6365 7373  mples_preprocess
+0001a700: 696e 675f 706c 6f74 5f6d 6170 5f64 6174  ing_plot_map_dat
+0001a710: 615f 746f 5f6e 6f72 6d61 6c2e 7079 602e  a_to_normal.py`.
+0001a720: 0a0a 2020 2020 5265 6164 206d 6f72 6520  ..    Read more 
+0001a730: 696e 2074 6865 203a 7265 663a 6055 7365  in the :ref:`Use
+0001a740: 7220 4775 6964 6520 3c70 7265 7072 6f63  r Guide <preproc
+0001a750: 6573 7369 6e67 5f74 7261 6e73 666f 726d  essing_transform
+0001a760: 6572 3e60 2e0a 0a20 2020 202e 2e20 7665  er>`...    .. ve
+0001a770: 7273 696f 6e61 6464 6564 3a3a 2030 2e32  rsionadded:: 0.2
+0001a780: 300a 0a20 2020 2050 6172 616d 6574 6572  0..    Parameter
+0001a790: 730a 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d  s.    ----------
+0001a7a0: 0a20 2020 206d 6574 686f 6420 3a20 7b27  .    method : {'
+0001a7b0: 7965 6f2d 6a6f 686e 736f 6e27 2c20 2762  yeo-johnson', 'b
+0001a7c0: 6f78 2d63 6f78 277d 2c20 6465 6661 756c  ox-cox'}, defaul
+0001a7d0: 743d 2779 656f 2d6a 6f68 6e73 6f6e 270a  t='yeo-johnson'.
+0001a7e0: 2020 2020 2020 2020 5468 6520 706f 7765          The powe
+0001a7f0: 7220 7472 616e 7366 6f72 6d20 6d65 7468  r transform meth
+0001a800: 6f64 2e20 4176 6169 6c61 626c 6520 6d65  od. Available me
+0001a810: 7468 6f64 7320 6172 653a 0a0a 2020 2020  thods are:..    
+0001a820: 2020 2020 2d20 2779 656f 2d6a 6f68 6e73      - 'yeo-johns
+0001a830: 6f6e 2720 5b31 5d5f 2c20 776f 726b 7320  on' [1]_, works 
+0001a840: 7769 7468 2070 6f73 6974 6976 6520 616e  with positive an
+0001a850: 6420 6e65 6761 7469 7665 2076 616c 7565  d negative value
+0001a860: 730a 2020 2020 2020 2020 2d20 2762 6f78  s.        - 'box
+0001a870: 2d63 6f78 2720 5b32 5d5f 2c20 6f6e 6c79  -cox' [2]_, only
+0001a880: 2077 6f72 6b73 2077 6974 6820 7374 7269   works with stri
+0001a890: 6374 6c79 2070 6f73 6974 6976 6520 7661  ctly positive va
+0001a8a0: 6c75 6573 0a0a 2020 2020 7374 616e 6461  lues..    standa
+0001a8b0: 7264 697a 6520 3a20 626f 6f6c 2c20 6465  rdize : bool, de
+0001a8c0: 6661 756c 743d 5472 7565 0a20 2020 2020  fault=True.     
+0001a8d0: 2020 2053 6574 2074 6f20 5472 7565 2074     Set to True t
+0001a8e0: 6f20 6170 706c 7920 7a65 726f 2d6d 6561  o apply zero-mea
+0001a8f0: 6e2c 2075 6e69 742d 7661 7269 616e 6365  n, unit-variance
+0001a900: 206e 6f72 6d61 6c69 7a61 7469 6f6e 2074   normalization t
+0001a910: 6f20 7468 650a 2020 2020 2020 2020 7472  o the.        tr
+0001a920: 616e 7366 6f72 6d65 6420 6f75 7470 7574  ansformed output
+0001a930: 2e0a 0a20 2020 2063 6f70 7920 3a20 626f  ...    copy : bo
+0001a940: 6f6c 2c20 6465 6661 756c 743d 5472 7565  ol, default=True
+0001a950: 0a20 2020 2020 2020 2053 6574 2074 6f20  .        Set to 
+0001a960: 4661 6c73 6520 746f 2070 6572 666f 726d  False to perform
+0001a970: 2069 6e70 6c61 6365 2063 6f6d 7075 7461   inplace computa
+0001a980: 7469 6f6e 2064 7572 696e 6720 7472 616e  tion during tran
+0001a990: 7366 6f72 6d61 7469 6f6e 2e0a 0a20 2020  sformation...   
+0001a9a0: 2041 7474 7269 6275 7465 730a 2020 2020   Attributes.    
+0001a9b0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 206c  ----------.    l
+0001a9c0: 616d 6264 6173 5f20 3a20 6e64 6172 7261  ambdas_ : ndarra
+0001a9d0: 7920 6f66 2066 6c6f 6174 206f 6620 7368  y of float of sh
+0001a9e0: 6170 6520 286e 5f66 6561 7475 7265 732c  ape (n_features,
+0001a9f0: 290a 2020 2020 2020 2020 5468 6520 7061  ).        The pa
+0001aa00: 7261 6d65 7465 7273 206f 6620 7468 6520  rameters of the 
+0001aa10: 706f 7765 7220 7472 616e 7366 6f72 6d61  power transforma
+0001aa20: 7469 6f6e 2066 6f72 2074 6865 2073 656c  tion for the sel
+0001aa30: 6563 7465 6420 6665 6174 7572 6573 2e0a  ected features..
+0001aa40: 0a20 2020 206e 5f66 6561 7475 7265 735f  .    n_features_
+0001aa50: 696e 5f20 3a20 696e 740a 2020 2020 2020  in_ : int.      
+0001aa60: 2020 4e75 6d62 6572 206f 6620 6665 6174    Number of feat
+0001aa70: 7572 6573 2073 6565 6e20 6475 7269 6e67  ures seen during
+0001aa80: 203a 7465 726d 3a60 6669 7460 2e0a 0a20   :term:`fit`... 
+0001aa90: 2020 2020 2020 202e 2e20 7665 7273 696f         .. versio
+0001aaa0: 6e61 6464 6564 3a3a 2030 2e32 340a 0a20  nadded:: 0.24.. 
+0001aab0: 2020 2066 6561 7475 7265 5f6e 616d 6573     feature_names
+0001aac0: 5f69 6e5f 203a 206e 6461 7272 6179 206f  _in_ : ndarray o
+0001aad0: 6620 7368 6170 6520 2860 6e5f 6665 6174  f shape (`n_feat
+0001aae0: 7572 6573 5f69 6e5f 602c 290a 2020 2020  ures_in_`,).    
+0001aaf0: 2020 2020 4e61 6d65 7320 6f66 2066 6561      Names of fea
+0001ab00: 7475 7265 7320 7365 656e 2064 7572 696e  tures seen durin
+0001ab10: 6720 3a74 6572 6d3a 6066 6974 602e 2044  g :term:`fit`. D
+0001ab20: 6566 696e 6564 206f 6e6c 7920 7768 656e  efined only when
+0001ab30: 2060 5860 0a20 2020 2020 2020 2068 6173   `X`.        has
+0001ab40: 2066 6561 7475 7265 206e 616d 6573 2074   feature names t
+0001ab50: 6861 7420 6172 6520 616c 6c20 7374 7269  hat are all stri
+0001ab60: 6e67 732e 0a0a 2020 2020 2020 2020 2e2e  ngs...        ..
+0001ab70: 2076 6572 7369 6f6e 6164 6465 643a 3a20   versionadded:: 
+0001ab80: 312e 300a 0a20 2020 2053 6565 2041 6c73  1.0..    See Als
+0001ab90: 6f0a 2020 2020 2d2d 2d2d 2d2d 2d2d 0a20  o.    --------. 
+0001aba0: 2020 2070 6f77 6572 5f74 7261 6e73 666f     power_transfo
+0001abb0: 726d 203a 2045 7175 6976 616c 656e 7420  rm : Equivalent 
+0001abc0: 6675 6e63 7469 6f6e 2077 6974 686f 7574  function without
+0001abd0: 2074 6865 2065 7374 696d 6174 6f72 2041   the estimator A
+0001abe0: 5049 2e0a 0a20 2020 2051 7561 6e74 696c  PI...    Quantil
+0001abf0: 6554 7261 6e73 666f 726d 6572 203a 204d  eTransformer : M
+0001ac00: 6170 7320 6461 7461 2074 6f20 6120 7374  aps data to a st
+0001ac10: 616e 6461 7264 206e 6f72 6d61 6c20 6469  andard normal di
+0001ac20: 7374 7269 6275 7469 6f6e 2077 6974 680a  stribution with.
+0001ac30: 2020 2020 2020 2020 7468 6520 7061 7261          the para
+0001ac40: 6d65 7465 7220 606f 7574 7075 745f 6469  meter `output_di
+0001ac50: 7374 7269 6275 7469 6f6e 3d27 6e6f 726d  stribution='norm
+0001ac60: 616c 2760 2e0a 0a20 2020 204e 6f74 6573  al'`...    Notes
+0001ac70: 0a20 2020 202d 2d2d 2d2d 0a20 2020 204e  .    -----.    N
+0001ac80: 614e 7320 6172 6520 7472 6561 7465 6420  aNs are treated 
+0001ac90: 6173 206d 6973 7369 6e67 2076 616c 7565  as missing value
+0001aca0: 733a 2064 6973 7265 6761 7264 6564 2069  s: disregarded i
+0001acb0: 6e20 6060 6669 7460 602c 2061 6e64 206d  n ``fit``, and m
+0001acc0: 6169 6e74 6169 6e65 640a 2020 2020 696e  aintained.    in
+0001acd0: 2060 6074 7261 6e73 666f 726d 6060 2e0a   ``transform``..
+0001ace0: 0a20 2020 2052 6566 6572 656e 6365 730a  .    References.
+0001acf0: 2020 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a0a      ----------..
+0001ad00: 2020 2020 2e2e 205b 315d 203a 646f 693a      .. [1] :doi:
+0001ad10: 6049 2e4b 2e20 5965 6f20 616e 6420 522e  `I.K. Yeo and R.
+0001ad20: 412e 204a 6f68 6e73 6f6e 2c20 2241 206e  A. Johnson, "A n
+0001ad30: 6577 2066 616d 696c 7920 6f66 2070 6f77  ew family of pow
+0001ad40: 6572 0a20 2020 2020 2020 2020 2020 7472  er.           tr
+0001ad50: 616e 7366 6f72 6d61 7469 6f6e 7320 746f  ansformations to
+0001ad60: 2069 6d70 726f 7665 206e 6f72 6d61 6c69   improve normali
+0001ad70: 7479 206f 7220 7379 6d6d 6574 7279 2e22  ty or symmetry."
+0001ad80: 2042 696f 6d65 7472 696b 612c 0a20 2020   Biometrika,.   
+0001ad90: 2020 2020 2020 2020 3837 2834 292c 2070          87(4), p
+0001ada0: 702e 3935 342d 3935 392c 2028 3230 3030  p.954-959, (2000
+0001adb0: 292e 203c 3130 2e31 3039 332f 6269 6f6d  ). <10.1093/biom
+0001adc0: 6574 2f38 372e 342e 3935 343e 600a 0a20  et/87.4.954>`.. 
+0001add0: 2020 202e 2e20 5b32 5d20 3a64 6f69 3a60     .. [2] :doi:`
+0001ade0: 472e 452e 502e 2042 6f78 2061 6e64 2044  G.E.P. Box and D
+0001adf0: 2e52 2e20 436f 782c 2022 416e 2041 6e61  .R. Cox, "An Ana
+0001ae00: 6c79 7369 7320 6f66 2054 7261 6e73 666f  lysis of Transfo
+0001ae10: 726d 6174 696f 6e73 222c 0a20 2020 2020  rmations",.     
+0001ae20: 2020 2020 2020 4a6f 7572 6e61 6c20 6f66        Journal of
+0001ae30: 2074 6865 2052 6f79 616c 2053 7461 7469   the Royal Stati
+0001ae40: 7374 6963 616c 2053 6f63 6965 7479 2042  stical Society B
+0001ae50: 2c20 3236 2c20 3231 312d 3235 3220 2831  , 26, 211-252 (1
+0001ae60: 3936 3429 2e0a 2020 2020 2020 2020 2020  964)..          
+0001ae70: 203c 3130 2e31 3131 312f 6a2e 3235 3137   <10.1111/j.2517
+0001ae80: 2d36 3136 312e 3139 3634 2e74 6230 3035  -6161.1964.tb005
+0001ae90: 3533 2e78 3e60 0a0a 2020 2020 4578 616d  53.x>`..    Exam
+0001aea0: 706c 6573 0a20 2020 202d 2d2d 2d2d 2d2d  ples.    -------
+0001aeb0: 2d0a 2020 2020 3e3e 3e20 696d 706f 7274  -.    >>> import
+0001aec0: 206e 756d 7079 2061 7320 6e70 0a20 2020   numpy as np.   
+0001aed0: 203e 3e3e 2066 726f 6d20 736b 6c65 6172   >>> from sklear
+0001aee0: 6e2e 7072 6570 726f 6365 7373 696e 6720  n.preprocessing 
+0001aef0: 696d 706f 7274 2050 6f77 6572 5472 616e  import PowerTran
+0001af00: 7366 6f72 6d65 720a 2020 2020 3e3e 3e20  sformer.    >>> 
+0001af10: 7074 203d 2050 6f77 6572 5472 616e 7366  pt = PowerTransf
+0001af20: 6f72 6d65 7228 290a 2020 2020 3e3e 3e20  ormer().    >>> 
+0001af30: 6461 7461 203d 205b 5b31 2c20 325d 2c20  data = [[1, 2], 
+0001af40: 5b33 2c20 325d 2c20 5b34 2c20 355d 5d0a  [3, 2], [4, 5]].
+0001af50: 2020 2020 3e3e 3e20 7072 696e 7428 7074      >>> print(pt
+0001af60: 2e66 6974 2864 6174 6129 290a 2020 2020  .fit(data)).    
+0001af70: 506f 7765 7254 7261 6e73 666f 726d 6572  PowerTransformer
+0001af80: 2829 0a20 2020 203e 3e3e 2070 7269 6e74  ().    >>> print
+0001af90: 2870 742e 6c61 6d62 6461 735f 290a 2020  (pt.lambdas_).  
+0001afa0: 2020 5b20 312e 3338 362e 2e2e 202d 332e    [ 1.386... -3.
+0001afb0: 3130 302e 2e2e 5d0a 2020 2020 3e3e 3e20  100...].    >>> 
+0001afc0: 7072 696e 7428 7074 2e74 7261 6e73 666f  print(pt.transfo
+0001afd0: 726d 2864 6174 6129 290a 2020 2020 5b5b  rm(data)).    [[
+0001afe0: 2d31 2e33 3136 2e2e 2e20 2d30 2e37 3037  -1.316... -0.707
+0001aff0: 2e2e 2e5d 0a20 2020 2020 5b20 302e 3230  ...].     [ 0.20
+0001b000: 392e 2e2e 202d 302e 3730 372e 2e2e 5d0a  9... -0.707...].
+0001b010: 2020 2020 205b 2031 2e31 3036 2e2e 2e20       [ 1.106... 
+0001b020: 2031 2e34 3134 2e2e 2e5d 5d0a 2020 2020   1.414...]].    
+0001b030: 2222 220a 0a20 2020 205f 7061 7261 6d65  """..    _parame
+0001b040: 7465 725f 636f 6e73 7472 6169 6e74 733a  ter_constraints:
+0001b050: 2064 6963 7420 3d20 7b0a 2020 2020 2020   dict = {.      
+0001b060: 2020 226d 6574 686f 6422 3a20 5b53 7472    "method": [Str
+0001b070: 4f70 7469 6f6e 7328 7b22 7965 6f2d 6a6f  Options({"yeo-jo
+0001b080: 686e 736f 6e22 2c20 2262 6f78 2d63 6f78  hnson", "box-cox
+0001b090: 227d 295d 2c0a 2020 2020 2020 2020 2273  "})],.        "s
+0001b0a0: 7461 6e64 6172 6469 7a65 223a 205b 2262  tandardize": ["b
+0001b0b0: 6f6f 6c65 616e 225d 2c0a 2020 2020 2020  oolean"],.      
+0001b0c0: 2020 2263 6f70 7922 3a20 5b22 626f 6f6c    "copy": ["bool
+0001b0d0: 6561 6e22 5d2c 0a20 2020 207d 0a0a 2020  ean"],.    }..  
+0001b0e0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+0001b0f0: 656c 662c 206d 6574 686f 643d 2279 656f  elf, method="yeo
+0001b100: 2d6a 6f68 6e73 6f6e 222c 202a 2c20 7374  -johnson", *, st
+0001b110: 616e 6461 7264 697a 653d 5472 7565 2c20  andardize=True, 
+0001b120: 636f 7079 3d54 7275 6529 3a0a 2020 2020  copy=True):.    
+0001b130: 2020 2020 7365 6c66 2e6d 6574 686f 6420      self.method 
+0001b140: 3d20 6d65 7468 6f64 0a20 2020 2020 2020  = method.       
+0001b150: 2073 656c 662e 7374 616e 6461 7264 697a   self.standardiz
+0001b160: 6520 3d20 7374 616e 6461 7264 697a 650a  e = standardize.
+0001b170: 2020 2020 2020 2020 7365 6c66 2e63 6f70          self.cop
+0001b180: 7920 3d20 636f 7079 0a0a 2020 2020 405f  y = copy..    @_
+0001b190: 6669 745f 636f 6e74 6578 7428 7072 6566  fit_context(pref
+0001b1a0: 6572 5f73 6b69 705f 6e65 7374 6564 5f76  er_skip_nested_v
+0001b1b0: 616c 6964 6174 696f 6e3d 5472 7565 290a  alidation=True).
+0001b1c0: 2020 2020 6465 6620 6669 7428 7365 6c66      def fit(self
+0001b1d0: 2c20 582c 2079 3d4e 6f6e 6529 3a0a 2020  , X, y=None):.  
+0001b1e0: 2020 2020 2020 2222 2245 7374 696d 6174        """Estimat
+0001b1f0: 6520 7468 6520 6f70 7469 6d61 6c20 7061  e the optimal pa
+0001b200: 7261 6d65 7465 7220 6c61 6d62 6461 2066  rameter lambda f
+0001b210: 6f72 2065 6163 6820 6665 6174 7572 652e  or each feature.
+0001b220: 0a0a 2020 2020 2020 2020 5468 6520 6f70  ..        The op
+0001b230: 7469 6d61 6c20 6c61 6d62 6461 2070 6172  timal lambda par
+0001b240: 616d 6574 6572 2066 6f72 206d 696e 696d  ameter for minim
+0001b250: 697a 696e 6720 736b 6577 6e65 7373 2069  izing skewness i
+0001b260: 7320 6573 7469 6d61 7465 6420 6f6e 0a20  s estimated on. 
+0001b270: 2020 2020 2020 2065 6163 6820 6665 6174         each feat
+0001b280: 7572 6520 696e 6465 7065 6e64 656e 746c  ure independentl
+0001b290: 7920 7573 696e 6720 6d61 7869 6d75 6d20  y using maximum 
+0001b2a0: 6c69 6b65 6c69 686f 6f64 2e0a 0a20 2020  likelihood...   
+0001b2b0: 2020 2020 2050 6172 616d 6574 6572 730a       Parameters.
+0001b2c0: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d2d          --------
+0001b2d0: 2d2d 0a20 2020 2020 2020 2058 203a 2061  --.        X : a
+0001b2e0: 7272 6179 2d6c 696b 6520 6f66 2073 6861  rray-like of sha
+0001b2f0: 7065 2028 6e5f 7361 6d70 6c65 732c 206e  pe (n_samples, n
+0001b300: 5f66 6561 7475 7265 7329 0a20 2020 2020  _features).     
+0001b310: 2020 2020 2020 2054 6865 2064 6174 6120         The data 
+0001b320: 7573 6564 2074 6f20 6573 7469 6d61 7465  used to estimate
+0001b330: 2074 6865 206f 7074 696d 616c 2074 7261   the optimal tra
+0001b340: 6e73 666f 726d 6174 696f 6e20 7061 7261  nsformation para
+0001b350: 6d65 7465 7273 2e0a 0a20 2020 2020 2020  meters...       
+0001b360: 2079 203a 204e 6f6e 650a 2020 2020 2020   y : None.      
+0001b370: 2020 2020 2020 4967 6e6f 7265 642e 0a0a        Ignored...
+0001b380: 2020 2020 2020 2020 5265 7475 726e 730a          Returns.
+0001b390: 2020 2020 2020 2020 2d2d 2d2d 2d2d 2d0a          -------.
+0001b3a0: 2020 2020 2020 2020 7365 6c66 203a 206f          self : o
+0001b3b0: 626a 6563 740a 2020 2020 2020 2020 2020  bject.          
+0001b3c0: 2020 4669 7474 6564 2074 7261 6e73 666f    Fitted transfo
+0001b3d0: 726d 6572 2e0a 2020 2020 2020 2020 2222  rmer..        ""
+0001b3e0: 220a 2020 2020 2020 2020 7365 6c66 2e5f  ".        self._
+0001b3f0: 6669 7428 582c 2079 3d79 2c20 666f 7263  fit(X, y=y, forc
+0001b400: 655f 7472 616e 7366 6f72 6d3d 4661 6c73  e_transform=Fals
+0001b410: 6529 0a20 2020 2020 2020 2072 6574 7572  e).        retur
+0001b420: 6e20 7365 6c66 0a0a 2020 2020 405f 6669  n self..    @_fi
+0001b430: 745f 636f 6e74 6578 7428 7072 6566 6572  t_context(prefer
+0001b440: 5f73 6b69 705f 6e65 7374 6564 5f76 616c  _skip_nested_val
+0001b450: 6964 6174 696f 6e3d 5472 7565 290a 2020  idation=True).  
+0001b460: 2020 6465 6620 6669 745f 7472 616e 7366    def fit_transf
+0001b470: 6f72 6d28 7365 6c66 2c20 582c 2079 3d4e  orm(self, X, y=N
+0001b480: 6f6e 6529 3a0a 2020 2020 2020 2020 2222  one):.        ""
+0001b490: 2246 6974 2060 506f 7765 7254 7261 6e73  "Fit `PowerTrans
+0001b4a0: 666f 726d 6572 6020 746f 2060 5860 2c20  former` to `X`, 
+0001b4b0: 7468 656e 2074 7261 6e73 666f 726d 2060  then transform `
+0001b4c0: 5860 2e0a 0a20 2020 2020 2020 2050 6172  X`...        Par
+0001b4d0: 616d 6574 6572 730a 2020 2020 2020 2020  ameters.        
+0001b4e0: 2d2d 2d2d 2d2d 2d2d 2d2d 0a20 2020 2020  ----------.     
+0001b4f0: 2020 2058 203a 2061 7272 6179 2d6c 696b     X : array-lik
+0001b500: 6520 6f66 2073 6861 7065 2028 6e5f 7361  e of shape (n_sa
+0001b510: 6d70 6c65 732c 206e 5f66 6561 7475 7265  mples, n_feature
+0001b520: 7329 0a20 2020 2020 2020 2020 2020 2054  s).            T
+0001b530: 6865 2064 6174 6120 7573 6564 2074 6f20  he data used to 
+0001b540: 6573 7469 6d61 7465 2074 6865 206f 7074  estimate the opt
+0001b550: 696d 616c 2074 7261 6e73 666f 726d 6174  imal transformat
+0001b560: 696f 6e20 7061 7261 6d65 7465 7273 0a20  ion parameters. 
+0001b570: 2020 2020 2020 2020 2020 2061 6e64 2074             and t
+0001b580: 6f20 6265 2074 7261 6e73 666f 726d 6564  o be transformed
+0001b590: 2075 7369 6e67 2061 2070 6f77 6572 2074   using a power t
+0001b5a0: 7261 6e73 666f 726d 6174 696f 6e2e 0a0a  ransformation...
+0001b5b0: 2020 2020 2020 2020 7920 3a20 4967 6e6f          y : Igno
+0001b5c0: 7265 640a 2020 2020 2020 2020 2020 2020  red.            
+0001b5d0: 4e6f 7420 7573 6564 2c20 7072 6573 656e  Not used, presen
+0001b5e0: 7420 666f 7220 4150 4920 636f 6e73 6973  t for API consis
+0001b5f0: 7465 6e63 7920 6279 2063 6f6e 7665 6e74  tency by convent
+0001b600: 696f 6e2e 0a0a 2020 2020 2020 2020 5265  ion...        Re
+0001b610: 7475 726e 730a 2020 2020 2020 2020 2d2d  turns.        --
+0001b620: 2d2d 2d2d 2d0a 2020 2020 2020 2020 585f  -----.        X_
+0001b630: 6e65 7720 3a20 6e64 6172 7261 7920 6f66  new : ndarray of
+0001b640: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
+0001b650: 732c 206e 5f66 6561 7475 7265 7329 0a20  s, n_features). 
+0001b660: 2020 2020 2020 2020 2020 2054 7261 6e73             Trans
+0001b670: 666f 726d 6564 2064 6174 612e 0a20 2020  formed data..   
+0001b680: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
+0001b690: 2072 6574 7572 6e20 7365 6c66 2e5f 6669   return self._fi
+0001b6a0: 7428 582c 2079 2c20 666f 7263 655f 7472  t(X, y, force_tr
+0001b6b0: 616e 7366 6f72 6d3d 5472 7565 290a 0a20  ansform=True).. 
+0001b6c0: 2020 2064 6566 205f 6669 7428 7365 6c66     def _fit(self
+0001b6d0: 2c20 582c 2079 3d4e 6f6e 652c 2066 6f72  , X, y=None, for
+0001b6e0: 6365 5f74 7261 6e73 666f 726d 3d46 616c  ce_transform=Fal
+0001b6f0: 7365 293a 0a20 2020 2020 2020 2058 203d  se):.        X =
+0001b700: 2073 656c 662e 5f63 6865 636b 5f69 6e70   self._check_inp
+0001b710: 7574 2858 2c20 696e 5f66 6974 3d54 7275  ut(X, in_fit=Tru
+0001b720: 652c 2063 6865 636b 5f70 6f73 6974 6976  e, check_positiv
+0001b730: 653d 5472 7565 290a 0a20 2020 2020 2020  e=True)..       
+0001b740: 2069 6620 6e6f 7420 7365 6c66 2e63 6f70   if not self.cop
+0001b750: 7920 616e 6420 6e6f 7420 666f 7263 655f  y and not force_
+0001b760: 7472 616e 7366 6f72 6d3a 2020 2320 6966  transform:  # if
+0001b770: 2063 616c 6c20 6672 6f6d 2066 6974 2829   call from fit()
+0001b780: 0a20 2020 2020 2020 2020 2020 2058 203d  .            X =
+0001b790: 2058 2e63 6f70 7928 2920 2023 2066 6f72   X.copy()  # for
+0001b7a0: 6365 2063 6f70 7920 736f 2074 6861 7420  ce copy so that 
+0001b7b0: 6669 7420 646f 6573 206e 6f74 2063 6861  fit does not cha
+0001b7c0: 6e67 6520 5820 696e 706c 6163 650a 0a20  nge X inplace.. 
+0001b7d0: 2020 2020 2020 206e 5f73 616d 706c 6573         n_samples
+0001b7e0: 203d 2058 2e73 6861 7065 5b30 5d0a 2020   = X.shape[0].  
+0001b7f0: 2020 2020 2020 6d65 616e 203d 206e 702e        mean = np.
+0001b800: 6d65 616e 2858 2c20 6178 6973 3d30 2c20  mean(X, axis=0, 
+0001b810: 6474 7970 653d 6e70 2e66 6c6f 6174 3634  dtype=np.float64
+0001b820: 290a 2020 2020 2020 2020 7661 7220 3d20  ).        var = 
+0001b830: 6e70 2e76 6172 2858 2c20 6178 6973 3d30  np.var(X, axis=0
+0001b840: 2c20 6474 7970 653d 6e70 2e66 6c6f 6174  , dtype=np.float
+0001b850: 3634 290a 0a20 2020 2020 2020 206f 7074  64)..        opt
+0001b860: 696d 5f66 756e 6374 696f 6e20 3d20 7b0a  im_function = {.
+0001b870: 2020 2020 2020 2020 2020 2020 2262 6f78              "box
+0001b880: 2d63 6f78 223a 2073 656c 662e 5f62 6f78  -cox": self._box
+0001b890: 5f63 6f78 5f6f 7074 696d 697a 652c 0a20  _cox_optimize,. 
+0001b8a0: 2020 2020 2020 2020 2020 2022 7965 6f2d             "yeo-
+0001b8b0: 6a6f 686e 736f 6e22 3a20 7365 6c66 2e5f  johnson": self._
+0001b8c0: 7965 6f5f 6a6f 686e 736f 6e5f 6f70 7469  yeo_johnson_opti
+0001b8d0: 6d69 7a65 2c0a 2020 2020 2020 2020 7d5b  mize,.        }[
+0001b8e0: 7365 6c66 2e6d 6574 686f 645d 0a0a 2020  self.method]..  
+0001b8f0: 2020 2020 2020 7472 616e 7366 6f72 6d5f        transform_
+0001b900: 6675 6e63 7469 6f6e 203d 207b 0a20 2020  function = {.   
+0001b910: 2020 2020 2020 2020 2022 626f 782d 636f           "box-co
+0001b920: 7822 3a20 626f 7863 6f78 2c0a 2020 2020  x": boxcox,.    
+0001b930: 2020 2020 2020 2020 2279 656f 2d6a 6f68          "yeo-joh
+0001b940: 6e73 6f6e 223a 2073 656c 662e 5f79 656f  nson": self._yeo
+0001b950: 5f6a 6f68 6e73 6f6e 5f74 7261 6e73 666f  _johnson_transfo
+0001b960: 726d 2c0a 2020 2020 2020 2020 7d5b 7365  rm,.        }[se
+0001b970: 6c66 2e6d 6574 686f 645d 0a0a 2020 2020  lf.method]..    
+0001b980: 2020 2020 7769 7468 206e 702e 6572 7273      with np.errs
+0001b990: 7461 7465 2869 6e76 616c 6964 3d22 6967  tate(invalid="ig
+0001b9a0: 6e6f 7265 2229 3a20 2023 2068 6964 6520  nore"):  # hide 
+0001b9b0: 4e61 4e20 7761 726e 696e 6773 0a20 2020  NaN warnings.   
+0001b9c0: 2020 2020 2020 2020 2073 656c 662e 6c61           self.la
+0001b9d0: 6d62 6461 735f 203d 206e 702e 656d 7074  mbdas_ = np.empt
+0001b9e0: 7928 582e 7368 6170 655b 315d 2c20 6474  y(X.shape[1], dt
+0001b9f0: 7970 653d 582e 6474 7970 6529 0a20 2020  ype=X.dtype).   
+0001ba00: 2020 2020 2020 2020 2066 6f72 2069 2c20           for i, 
+0001ba10: 636f 6c20 696e 2065 6e75 6d65 7261 7465  col in enumerate
+0001ba20: 2858 2e54 293a 0a20 2020 2020 2020 2020  (X.T):.         
+0001ba30: 2020 2020 2020 2023 2046 6f72 2079 656f         # For yeo
+0001ba40: 2d6a 6f68 6e73 6f6e 2c20 6c65 6176 6520  -johnson, leave 
+0001ba50: 636f 6e73 7461 6e74 2066 6561 7475 7265  constant feature
+0001ba60: 7320 756e 6368 616e 6765 640a 2020 2020  s unchanged.    
+0001ba70: 2020 2020 2020 2020 2020 2020 2320 6c61              # la
+0001ba80: 6d62 6461 3d31 2063 6f72 7265 7370 6f6e  mbda=1 correspon
+0001ba90: 6473 2074 6f20 7468 6520 6964 656e 7469  ds to the identi
+0001baa0: 7479 2074 7261 6e73 666f 726d 6174 696f  ty transformatio
+0001bab0: 6e0a 2020 2020 2020 2020 2020 2020 2020  n.              
+0001bac0: 2020 6973 5f63 6f6e 7374 616e 745f 6665    is_constant_fe
+0001bad0: 6174 7572 6520 3d20 5f69 735f 636f 6e73  ature = _is_cons
+0001bae0: 7461 6e74 5f66 6561 7475 7265 2876 6172  tant_feature(var
+0001baf0: 5b69 5d2c 206d 6561 6e5b 695d 2c20 6e5f  [i], mean[i], n_
+0001bb00: 7361 6d70 6c65 7329 0a20 2020 2020 2020  samples).       
+0001bb10: 2020 2020 2020 2020 2069 6620 7365 6c66           if self
+0001bb20: 2e6d 6574 686f 6420 3d3d 2022 7965 6f2d  .method == "yeo-
+0001bb30: 6a6f 686e 736f 6e22 2061 6e64 2069 735f  johnson" and is_
+0001bb40: 636f 6e73 7461 6e74 5f66 6561 7475 7265  constant_feature
+0001bb50: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001bb60: 2020 2020 2020 7365 6c66 2e6c 616d 6264        self.lambd
+0001bb70: 6173 5f5b 695d 203d 2031 2e30 0a20 2020  as_[i] = 1.0.   
+0001bb80: 2020 2020 2020 2020 2020 2020 2020 2020                  
+0001bb90: 2063 6f6e 7469 6e75 650a 0a20 2020 2020   continue..     
+0001bba0: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0001bbb0: 6c61 6d62 6461 735f 5b69 5d20 3d20 6f70  lambdas_[i] = op
+0001bbc0: 7469 6d5f 6675 6e63 7469 6f6e 2863 6f6c  tim_function(col
+0001bbd0: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             
+0001bbe0: 2020 2069 6620 7365 6c66 2e73 7461 6e64     if self.stand
+0001bbf0: 6172 6469 7a65 206f 7220 666f 7263 655f  ardize or force_
+0001bc00: 7472 616e 7366 6f72 6d3a 0a20 2020 2020  transform:.     
+0001bc10: 2020 2020 2020 2020 2020 2020 2020 2058                 X
+0001bc20: 5b3a 2c20 695d 203d 2074 7261 6e73 666f  [:, i] = transfo
+0001bc30: 726d 5f66 756e 6374 696f 6e28 585b 3a2c  rm_function(X[:,
+0001bc40: 2069 5d2c 2073 656c 662e 6c61 6d62 6461   i], self.lambda
+0001bc50: 735f 5b69 5d29 0a0a 2020 2020 2020 2020  s_[i])..        
+0001bc60: 6966 2073 656c 662e 7374 616e 6461 7264  if self.standard
+0001bc70: 697a 653a 0a20 2020 2020 2020 2020 2020  ize:.           
+0001bc80: 2073 656c 662e 5f73 6361 6c65 7220 3d20   self._scaler = 
+0001bc90: 5374 616e 6461 7264 5363 616c 6572 2863  StandardScaler(c
+0001bca0: 6f70 793d 4661 6c73 6529 2e73 6574 5f6f  opy=False).set_o
+0001bcb0: 7574 7075 7428 7472 616e 7366 6f72 6d3d  utput(transform=
+0001bcc0: 2264 6566 6175 6c74 2229 0a20 2020 2020  "default").     
+0001bcd0: 2020 2020 2020 2069 6620 666f 7263 655f         if force_
+0001bce0: 7472 616e 7366 6f72 6d3a 0a20 2020 2020  transform:.     
+0001bcf0: 2020 2020 2020 2020 2020 2058 203d 2073             X = s
+0001bd00: 656c 662e 5f73 6361 6c65 722e 6669 745f  elf._scaler.fit_
+0001bd10: 7472 616e 7366 6f72 6d28 5829 0a20 2020  transform(X).   
+0001bd20: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. 
+0001bd30: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+0001bd40: 656c 662e 5f73 6361 6c65 722e 6669 7428  elf._scaler.fit(
+0001bd50: 5829 0a0a 2020 2020 2020 2020 7265 7475  X)..        retu
+0001bd60: 726e 2058 0a0a 2020 2020 6465 6620 7472  rn X..    def tr
+0001bd70: 616e 7366 6f72 6d28 7365 6c66 2c20 5829  ansform(self, X)
+0001bd80: 3a0a 2020 2020 2020 2020 2222 2241 7070  :.        """App
+0001bd90: 6c79 2074 6865 2070 6f77 6572 2074 7261  ly the power tra
+0001bda0: 6e73 666f 726d 2074 6f20 6561 6368 2066  nsform to each f
+0001bdb0: 6561 7475 7265 2075 7369 6e67 2074 6865  eature using the
+0001bdc0: 2066 6974 7465 6420 6c61 6d62 6461 732e   fitted lambdas.
+0001bdd0: 0a0a 2020 2020 2020 2020 5061 7261 6d65  ..        Parame
+0001bde0: 7465 7273 0a20 2020 2020 2020 202d 2d2d  ters.        ---
+0001bdf0: 2d2d 2d2d 2d2d 2d0a 2020 2020 2020 2020  -------.        
+0001be00: 5820 3a20 6172 7261 792d 6c69 6b65 206f  X : array-like o
+0001be10: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
+0001be20: 6573 2c20 6e5f 6665 6174 7572 6573 290a  es, n_features).
+0001be30: 2020 2020 2020 2020 2020 2020 5468 6520              The 
+0001be40: 6461 7461 2074 6f20 6265 2074 7261 6e73  data to be trans
+0001be50: 666f 726d 6564 2075 7369 6e67 2061 2070  formed using a p
+0001be60: 6f77 6572 2074 7261 6e73 666f 726d 6174  ower transformat
+0001be70: 696f 6e2e 0a0a 2020 2020 2020 2020 5265  ion...        Re
+0001be80: 7475 726e 730a 2020 2020 2020 2020 2d2d  turns.        --
+0001be90: 2d2d 2d2d 2d0a 2020 2020 2020 2020 585f  -----.        X_
+0001bea0: 7472 616e 7320 3a20 6e64 6172 7261 7920  trans : ndarray 
+0001beb0: 6f66 2073 6861 7065 2028 6e5f 7361 6d70  of shape (n_samp
+0001bec0: 6c65 732c 206e 5f66 6561 7475 7265 7329  les, n_features)
+0001bed0: 0a20 2020 2020 2020 2020 2020 2054 6865  .            The
+0001bee0: 2074 7261 6e73 666f 726d 6564 2064 6174   transformed dat
+0001bef0: 612e 0a20 2020 2020 2020 2022 2222 0a20  a..        """. 
+0001bf00: 2020 2020 2020 2063 6865 636b 5f69 735f         check_is_
+0001bf10: 6669 7474 6564 2873 656c 6629 0a20 2020  fitted(self).   
+0001bf20: 2020 2020 2058 203d 2073 656c 662e 5f63       X = self._c
+0001bf30: 6865 636b 5f69 6e70 7574 2858 2c20 696e  heck_input(X, in
+0001bf40: 5f66 6974 3d46 616c 7365 2c20 6368 6563  _fit=False, chec
+0001bf50: 6b5f 706f 7369 7469 7665 3d54 7275 652c  k_positive=True,
+0001bf60: 2063 6865 636b 5f73 6861 7065 3d54 7275   check_shape=Tru
+0001bf70: 6529 0a0a 2020 2020 2020 2020 7472 616e  e)..        tran
+0001bf80: 7366 6f72 6d5f 6675 6e63 7469 6f6e 203d  sform_function =
+0001bf90: 207b 0a20 2020 2020 2020 2020 2020 2022   {.            "
+0001bfa0: 626f 782d 636f 7822 3a20 626f 7863 6f78  box-cox": boxcox
+0001bfb0: 2c0a 2020 2020 2020 2020 2020 2020 2279  ,.            "y
+0001bfc0: 656f 2d6a 6f68 6e73 6f6e 223a 2073 656c  eo-johnson": sel
+0001bfd0: 662e 5f79 656f 5f6a 6f68 6e73 6f6e 5f74  f._yeo_johnson_t
+0001bfe0: 7261 6e73 666f 726d 2c0a 2020 2020 2020  ransform,.      
+0001bff0: 2020 7d5b 7365 6c66 2e6d 6574 686f 645d    }[self.method]
+0001c000: 0a20 2020 2020 2020 2066 6f72 2069 2c20  .        for i, 
+0001c010: 6c6d 6264 6120 696e 2065 6e75 6d65 7261  lmbda in enumera
+0001c020: 7465 2873 656c 662e 6c61 6d62 6461 735f  te(self.lambdas_
+0001c030: 293a 0a20 2020 2020 2020 2020 2020 2077  ):.            w
+0001c040: 6974 6820 6e70 2e65 7272 7374 6174 6528  ith np.errstate(
+0001c050: 696e 7661 6c69 643d 2269 676e 6f72 6522  invalid="ignore"
+0001c060: 293a 2020 2320 6869 6465 204e 614e 2077  ):  # hide NaN w
+0001c070: 6172 6e69 6e67 730a 2020 2020 2020 2020  arnings.        
+0001c080: 2020 2020 2020 2020 585b 3a2c 2069 5d20          X[:, i] 
+0001c090: 3d20 7472 616e 7366 6f72 6d5f 6675 6e63  = transform_func
+0001c0a0: 7469 6f6e 2858 5b3a 2c20 695d 2c20 6c6d  tion(X[:, i], lm
+0001c0b0: 6264 6129 0a0a 2020 2020 2020 2020 6966  bda)..        if
+0001c0c0: 2073 656c 662e 7374 616e 6461 7264 697a   self.standardiz
+0001c0d0: 653a 0a20 2020 2020 2020 2020 2020 2058  e:.            X
+0001c0e0: 203d 2073 656c 662e 5f73 6361 6c65 722e   = self._scaler.
+0001c0f0: 7472 616e 7366 6f72 6d28 5829 0a0a 2020  transform(X)..  
+0001c100: 2020 2020 2020 7265 7475 726e 2058 0a0a        return X..
+0001c110: 2020 2020 6465 6620 696e 7665 7273 655f      def inverse_
+0001c120: 7472 616e 7366 6f72 6d28 7365 6c66 2c20  transform(self, 
+0001c130: 5829 3a0a 2020 2020 2020 2020 2222 2241  X):.        """A
+0001c140: 7070 6c79 2074 6865 2069 6e76 6572 7365  pply the inverse
+0001c150: 2070 6f77 6572 2074 7261 6e73 666f 726d   power transform
+0001c160: 6174 696f 6e20 7573 696e 6720 7468 6520  ation using the 
+0001c170: 6669 7474 6564 206c 616d 6264 6173 2e0a  fitted lambdas..
+0001c180: 0a20 2020 2020 2020 2054 6865 2069 6e76  .        The inv
+0001c190: 6572 7365 206f 6620 7468 6520 426f 782d  erse of the Box-
+0001c1a0: 436f 7820 7472 616e 7366 6f72 6d61 7469  Cox transformati
+0001c1b0: 6f6e 2069 7320 6769 7665 6e20 6279 3a3a  on is given by::
+0001c1c0: 0a0a 2020 2020 2020 2020 2020 2020 6966  ..            if
+0001c1d0: 206c 616d 6264 615f 203d 3d20 303a 0a20   lambda_ == 0:. 
+0001c1e0: 2020 2020 2020 2020 2020 2020 2020 2058                 X
+0001c1f0: 203d 2065 7870 2858 5f74 7261 6e73 290a   = exp(X_trans).
+0001c200: 2020 2020 2020 2020 2020 2020 656c 7365              else
+0001c210: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001c220: 2020 5820 3d20 2858 5f74 7261 6e73 202a    X = (X_trans *
+0001c230: 206c 616d 6264 615f 202b 2031 2920 2a2a   lambda_ + 1) **
+0001c240: 2028 3120 2f20 6c61 6d62 6461 5f29 0a0a   (1 / lambda_)..
+0001c250: 2020 2020 2020 2020 5468 6520 696e 7665          The inve
+0001c260: 7273 6520 6f66 2074 6865 2059 656f 2d4a  rse of the Yeo-J
+0001c270: 6f68 6e73 6f6e 2074 7261 6e73 666f 726d  ohnson transform
+0001c280: 6174 696f 6e20 6973 2067 6976 656e 2062  ation is given b
+0001c290: 793a 3a0a 0a20 2020 2020 2020 2020 2020  y::..           
+0001c2a0: 2069 6620 5820 3e3d 2030 2061 6e64 206c   if X >= 0 and l
+0001c2b0: 616d 6264 615f 203d 3d20 303a 0a20 2020  ambda_ == 0:.   
+0001c2c0: 2020 2020 2020 2020 2020 2020 2058 203d               X =
+0001c2d0: 2065 7870 2858 5f74 7261 6e73 2920 2d20   exp(X_trans) - 
+0001c2e0: 310a 2020 2020 2020 2020 2020 2020 656c  1.            el
+0001c2f0: 6966 2058 203e 3d20 3020 616e 6420 6c61  if X >= 0 and la
+0001c300: 6d62 6461 5f20 213d 2030 3a0a 2020 2020  mbda_ != 0:.    
+0001c310: 2020 2020 2020 2020 2020 2020 5820 3d20              X = 
+0001c320: 2858 5f74 7261 6e73 202a 206c 616d 6264  (X_trans * lambd
+0001c330: 615f 202b 2031 2920 2a2a 2028 3120 2f20  a_ + 1) ** (1 / 
+0001c340: 6c61 6d62 6461 5f29 202d 2031 0a20 2020  lambda_) - 1.   
+0001c350: 2020 2020 2020 2020 2065 6c69 6620 5820           elif X 
+0001c360: 3c20 3020 616e 6420 6c61 6d62 6461 5f20  < 0 and lambda_ 
+0001c370: 213d 2032 3a0a 2020 2020 2020 2020 2020  != 2:.          
+0001c380: 2020 2020 2020 5820 3d20 3120 2d20 282d        X = 1 - (-
+0001c390: 2832 202d 206c 616d 6264 615f 2920 2a20  (2 - lambda_) * 
+0001c3a0: 585f 7472 616e 7320 2b20 3129 202a 2a20  X_trans + 1) ** 
+0001c3b0: 2831 202f 2028 3220 2d20 6c61 6d62 6461  (1 / (2 - lambda
+0001c3c0: 5f29 290a 2020 2020 2020 2020 2020 2020  _)).            
+0001c3d0: 656c 6966 2058 203c 2030 2061 6e64 206c  elif X < 0 and l
+0001c3e0: 616d 6264 615f 203d 3d20 323a 0a20 2020  ambda_ == 2:.   
+0001c3f0: 2020 2020 2020 2020 2020 2020 2058 203d               X =
+0001c400: 2031 202d 2065 7870 282d 585f 7472 616e   1 - exp(-X_tran
+0001c410: 7329 0a0a 2020 2020 2020 2020 5061 7261  s)..        Para
+0001c420: 6d65 7465 7273 0a20 2020 2020 2020 202d  meters.        -
+0001c430: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 2020  ---------.      
+0001c440: 2020 5820 3a20 6172 7261 792d 6c69 6b65    X : array-like
+0001c450: 206f 6620 7368 6170 6520 286e 5f73 616d   of shape (n_sam
+0001c460: 706c 6573 2c20 6e5f 6665 6174 7572 6573  ples, n_features
+0001c470: 290a 2020 2020 2020 2020 2020 2020 5468  ).            Th
+0001c480: 6520 7472 616e 7366 6f72 6d65 6420 6461  e transformed da
+0001c490: 7461 2e0a 0a20 2020 2020 2020 2052 6574  ta...        Ret
+0001c4a0: 7572 6e73 0a20 2020 2020 2020 202d 2d2d  urns.        ---
+0001c4b0: 2d2d 2d2d 0a20 2020 2020 2020 2058 203a  ----.        X :
+0001c4c0: 206e 6461 7272 6179 206f 6620 7368 6170   ndarray of shap
+0001c4d0: 6520 286e 5f73 616d 706c 6573 2c20 6e5f  e (n_samples, n_
+0001c4e0: 6665 6174 7572 6573 290a 2020 2020 2020  features).      
+0001c4f0: 2020 2020 2020 5468 6520 6f72 6967 696e        The origin
+0001c500: 616c 2064 6174 612e 0a20 2020 2020 2020  al data..       
+0001c510: 2022 2222 0a20 2020 2020 2020 2063 6865   """.        che
+0001c520: 636b 5f69 735f 6669 7474 6564 2873 656c  ck_is_fitted(sel
+0001c530: 6629 0a20 2020 2020 2020 2058 203d 2073  f).        X = s
+0001c540: 656c 662e 5f63 6865 636b 5f69 6e70 7574  elf._check_input
+0001c550: 2858 2c20 696e 5f66 6974 3d46 616c 7365  (X, in_fit=False
+0001c560: 2c20 6368 6563 6b5f 7368 6170 653d 5472  , check_shape=Tr
+0001c570: 7565 290a 0a20 2020 2020 2020 2069 6620  ue)..        if 
+0001c580: 7365 6c66 2e73 7461 6e64 6172 6469 7a65  self.standardize
+0001c590: 3a0a 2020 2020 2020 2020 2020 2020 5820  :.            X 
+0001c5a0: 3d20 7365 6c66 2e5f 7363 616c 6572 2e69  = self._scaler.i
+0001c5b0: 6e76 6572 7365 5f74 7261 6e73 666f 726d  nverse_transform
+0001c5c0: 2858 290a 0a20 2020 2020 2020 2069 6e76  (X)..        inv
+0001c5d0: 5f66 756e 203d 207b 0a20 2020 2020 2020  _fun = {.       
+0001c5e0: 2020 2020 2022 626f 782d 636f 7822 3a20       "box-cox": 
+0001c5f0: 7365 6c66 2e5f 626f 785f 636f 785f 696e  self._box_cox_in
+0001c600: 7665 7273 655f 7472 616e 666f 726d 2c0a  verse_tranform,.
+0001c610: 2020 2020 2020 2020 2020 2020 2279 656f              "yeo
+0001c620: 2d6a 6f68 6e73 6f6e 223a 2073 656c 662e  -johnson": self.
+0001c630: 5f79 656f 5f6a 6f68 6e73 6f6e 5f69 6e76  _yeo_johnson_inv
+0001c640: 6572 7365 5f74 7261 6e73 666f 726d 2c0a  erse_transform,.
+0001c650: 2020 2020 2020 2020 7d5b 7365 6c66 2e6d          }[self.m
+0001c660: 6574 686f 645d 0a20 2020 2020 2020 2066  ethod].        f
+0001c670: 6f72 2069 2c20 6c6d 6264 6120 696e 2065  or i, lmbda in e
+0001c680: 6e75 6d65 7261 7465 2873 656c 662e 6c61  numerate(self.la
+0001c690: 6d62 6461 735f 293a 0a20 2020 2020 2020  mbdas_):.       
+0001c6a0: 2020 2020 2077 6974 6820 6e70 2e65 7272       with np.err
+0001c6b0: 7374 6174 6528 696e 7661 6c69 643d 2269  state(invalid="i
+0001c6c0: 676e 6f72 6522 293a 2020 2320 6869 6465  gnore"):  # hide
+0001c6d0: 204e 614e 2077 6172 6e69 6e67 730a 2020   NaN warnings.  
+0001c6e0: 2020 2020 2020 2020 2020 2020 2020 585b                X[
+0001c6f0: 3a2c 2069 5d20 3d20 696e 765f 6675 6e28  :, i] = inv_fun(
+0001c700: 585b 3a2c 2069 5d2c 206c 6d62 6461 290a  X[:, i], lmbda).
+0001c710: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return 
+0001c720: 580a 0a20 2020 2064 6566 205f 626f 785f  X..    def _box_
+0001c730: 636f 785f 696e 7665 7273 655f 7472 616e  cox_inverse_tran
+0001c740: 666f 726d 2873 656c 662c 2078 2c20 6c6d  form(self, x, lm
+0001c750: 6264 6129 3a0a 2020 2020 2020 2020 2222  bda):.        ""
+0001c760: 2252 6574 7572 6e20 696e 7665 7273 652d  "Return inverse-
+0001c770: 7472 616e 7366 6f72 6d65 6420 696e 7075  transformed inpu
+0001c780: 7420 7820 666f 6c6c 6f77 696e 6720 426f  t x following Bo
+0001c790: 782d 436f 7820 696e 7665 7273 650a 2020  x-Cox inverse.  
+0001c7a0: 2020 2020 2020 7472 616e 7366 6f72 6d20        transform 
+0001c7b0: 7769 7468 2070 6172 616d 6574 6572 206c  with parameter l
+0001c7c0: 616d 6264 612e 0a20 2020 2020 2020 2022  ambda..        "
+0001c7d0: 2222 0a20 2020 2020 2020 2069 6620 6c6d  "".        if lm
+0001c7e0: 6264 6120 3d3d 2030 3a0a 2020 2020 2020  bda == 0:.      
+0001c7f0: 2020 2020 2020 785f 696e 7620 3d20 6e70        x_inv = np
+0001c800: 2e65 7870 2878 290a 2020 2020 2020 2020  .exp(x).        
+0001c810: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          
+0001c820: 2020 785f 696e 7620 3d20 2878 202a 206c    x_inv = (x * l
+0001c830: 6d62 6461 202b 2031 2920 2a2a 2028 3120  mbda + 1) ** (1 
+0001c840: 2f20 6c6d 6264 6129 0a0a 2020 2020 2020  / lmbda)..      
+0001c850: 2020 7265 7475 726e 2078 5f69 6e76 0a0a    return x_inv..
+0001c860: 2020 2020 6465 6620 5f79 656f 5f6a 6f68      def _yeo_joh
+0001c870: 6e73 6f6e 5f69 6e76 6572 7365 5f74 7261  nson_inverse_tra
+0001c880: 6e73 666f 726d 2873 656c 662c 2078 2c20  nsform(self, x, 
+0001c890: 6c6d 6264 6129 3a0a 2020 2020 2020 2020  lmbda):.        
+0001c8a0: 2222 2252 6574 7572 6e20 696e 7665 7273  """Return invers
+0001c8b0: 652d 7472 616e 7366 6f72 6d65 6420 696e  e-transformed in
+0001c8c0: 7075 7420 7820 666f 6c6c 6f77 696e 6720  put x following 
+0001c8d0: 5965 6f2d 4a6f 686e 736f 6e20 696e 7665  Yeo-Johnson inve
+0001c8e0: 7273 650a 2020 2020 2020 2020 7472 616e  rse.        tran
+0001c8f0: 7366 6f72 6d20 7769 7468 2070 6172 616d  sform with param
+0001c900: 6574 6572 206c 616d 6264 612e 0a20 2020  eter lambda..   
+0001c910: 2020 2020 2022 2222 0a20 2020 2020 2020       """.       
+0001c920: 2078 5f69 6e76 203d 206e 702e 7a65 726f   x_inv = np.zero
+0001c930: 735f 6c69 6b65 2878 290a 2020 2020 2020  s_like(x).      
+0001c940: 2020 706f 7320 3d20 7820 3e3d 2030 0a0a    pos = x >= 0..
+0001c950: 2020 2020 2020 2020 2320 7768 656e 2078          # when x
+0001c960: 203e 3d20 300a 2020 2020 2020 2020 6966   >= 0.        if
+0001c970: 2061 6273 286c 6d62 6461 2920 3c20 6e70   abs(lmbda) < np
+0001c980: 2e73 7061 6369 6e67 2831 2e30 293a 0a20  .spacing(1.0):. 
+0001c990: 2020 2020 2020 2020 2020 2078 5f69 6e76             x_inv
+0001c9a0: 5b70 6f73 5d20 3d20 6e70 2e65 7870 2878  [pos] = np.exp(x
+0001c9b0: 5b70 6f73 5d29 202d 2031 0a20 2020 2020  [pos]) - 1.     
+0001c9c0: 2020 2065 6c73 653a 2020 2320 6c6d 6264     else:  # lmbd
+0001c9d0: 6120 213d 2030 0a20 2020 2020 2020 2020  a != 0.         
+0001c9e0: 2020 2078 5f69 6e76 5b70 6f73 5d20 3d20     x_inv[pos] = 
+0001c9f0: 6e70 2e70 6f77 6572 2878 5b70 6f73 5d20  np.power(x[pos] 
+0001ca00: 2a20 6c6d 6264 6120 2b20 312c 2031 202f  * lmbda + 1, 1 /
+0001ca10: 206c 6d62 6461 2920 2d20 310a 0a20 2020   lmbda) - 1..   
+0001ca20: 2020 2020 2023 2077 6865 6e20 7820 3c20       # when x < 
+0001ca30: 300a 2020 2020 2020 2020 6966 2061 6273  0.        if abs
+0001ca40: 286c 6d62 6461 202d 2032 2920 3e20 6e70  (lmbda - 2) > np
+0001ca50: 2e73 7061 6369 6e67 2831 2e30 293a 0a20  .spacing(1.0):. 
+0001ca60: 2020 2020 2020 2020 2020 2078 5f69 6e76             x_inv
+0001ca70: 5b7e 706f 735d 203d 2031 202d 206e 702e  [~pos] = 1 - np.
+0001ca80: 706f 7765 7228 2d28 3220 2d20 6c6d 6264  power(-(2 - lmbd
+0001ca90: 6129 202a 2078 5b7e 706f 735d 202b 2031  a) * x[~pos] + 1
+0001caa0: 2c20 3120 2f20 2832 202d 206c 6d62 6461  , 1 / (2 - lmbda
+0001cab0: 2929 0a20 2020 2020 2020 2065 6c73 653a  )).        else:
+0001cac0: 2020 2320 6c6d 6264 6120 3d3d 2032 0a20    # lmbda == 2. 
+0001cad0: 2020 2020 2020 2020 2020 2078 5f69 6e76             x_inv
+0001cae0: 5b7e 706f 735d 203d 2031 202d 206e 702e  [~pos] = 1 - np.
+0001caf0: 6578 7028 2d78 5b7e 706f 735d 290a 0a20  exp(-x[~pos]).. 
+0001cb00: 2020 2020 2020 2072 6574 7572 6e20 785f         return x_
+0001cb10: 696e 760a 0a20 2020 2064 6566 205f 7965  inv..    def _ye
+0001cb20: 6f5f 6a6f 686e 736f 6e5f 7472 616e 7366  o_johnson_transf
+0001cb30: 6f72 6d28 7365 6c66 2c20 782c 206c 6d62  orm(self, x, lmb
+0001cb40: 6461 293a 0a20 2020 2020 2020 2022 2222  da):.        """
+0001cb50: 5265 7475 726e 2074 7261 6e73 666f 726d  Return transform
+0001cb60: 6564 2069 6e70 7574 2078 2066 6f6c 6c6f  ed input x follo
+0001cb70: 7769 6e67 2059 656f 2d4a 6f68 6e73 6f6e  wing Yeo-Johnson
+0001cb80: 2074 7261 6e73 666f 726d 2077 6974 680a   transform with.
+0001cb90: 2020 2020 2020 2020 7061 7261 6d65 7465          paramete
+0001cba0: 7220 6c61 6d62 6461 2e0a 2020 2020 2020  r lambda..      
+0001cbb0: 2020 2222 220a 0a20 2020 2020 2020 206f    """..        o
+0001cbc0: 7574 203d 206e 702e 7a65 726f 735f 6c69  ut = np.zeros_li
+0001cbd0: 6b65 2878 290a 2020 2020 2020 2020 706f  ke(x).        po
+0001cbe0: 7320 3d20 7820 3e3d 2030 2020 2320 6269  s = x >= 0  # bi
+0001cbf0: 6e61 7279 206d 6173 6b0a 0a20 2020 2020  nary mask..     
+0001cc00: 2020 2023 2077 6865 6e20 7820 3e3d 2030     # when x >= 0
+0001cc10: 0a20 2020 2020 2020 2069 6620 6162 7328  .        if abs(
+0001cc20: 6c6d 6264 6129 203c 206e 702e 7370 6163  lmbda) < np.spac
+0001cc30: 696e 6728 312e 3029 3a0a 2020 2020 2020  ing(1.0):.      
+0001cc40: 2020 2020 2020 6f75 745b 706f 735d 203d        out[pos] =
+0001cc50: 206e 702e 6c6f 6731 7028 785b 706f 735d   np.log1p(x[pos]
+0001cc60: 290a 2020 2020 2020 2020 656c 7365 3a20  ).        else: 
+0001cc70: 2023 206c 6d62 6461 2021 3d20 300a 2020   # lmbda != 0.  
+0001cc80: 2020 2020 2020 2020 2020 6f75 745b 706f            out[po
+0001cc90: 735d 203d 2028 6e70 2e70 6f77 6572 2878  s] = (np.power(x
+0001cca0: 5b70 6f73 5d20 2b20 312c 206c 6d62 6461  [pos] + 1, lmbda
+0001ccb0: 2920 2d20 3129 202f 206c 6d62 6461 0a0a  ) - 1) / lmbda..
+0001ccc0: 2020 2020 2020 2020 2320 7768 656e 2078          # when x
+0001ccd0: 203c 2030 0a20 2020 2020 2020 2069 6620   < 0.        if 
+0001cce0: 6162 7328 6c6d 6264 6120 2d20 3229 203e  abs(lmbda - 2) >
+0001ccf0: 206e 702e 7370 6163 696e 6728 312e 3029   np.spacing(1.0)
+0001cd00: 3a0a 2020 2020 2020 2020 2020 2020 6f75  :.            ou
+0001cd10: 745b 7e70 6f73 5d20 3d20 2d28 6e70 2e70  t[~pos] = -(np.p
+0001cd20: 6f77 6572 282d 785b 7e70 6f73 5d20 2b20  ower(-x[~pos] + 
+0001cd30: 312c 2032 202d 206c 6d62 6461 2920 2d20  1, 2 - lmbda) - 
+0001cd40: 3129 202f 2028 3220 2d20 6c6d 6264 6129  1) / (2 - lmbda)
+0001cd50: 0a20 2020 2020 2020 2065 6c73 653a 2020  .        else:  
+0001cd60: 2320 6c6d 6264 6120 3d3d 2032 0a20 2020  # lmbda == 2.   
+0001cd70: 2020 2020 2020 2020 206f 7574 5b7e 706f           out[~po
+0001cd80: 735d 203d 202d 6e70 2e6c 6f67 3170 282d  s] = -np.log1p(-
+0001cd90: 785b 7e70 6f73 5d29 0a0a 2020 2020 2020  x[~pos])..      
+0001cda0: 2020 7265 7475 726e 206f 7574 0a0a 2020    return out..  
+0001cdb0: 2020 6465 6620 5f62 6f78 5f63 6f78 5f6f    def _box_cox_o
+0001cdc0: 7074 696d 697a 6528 7365 6c66 2c20 7829  ptimize(self, x)
+0001cdd0: 3a0a 2020 2020 2020 2020 2222 2246 696e  :.        """Fin
+0001cde0: 6420 616e 6420 7265 7475 726e 206f 7074  d and return opt
+0001cdf0: 696d 616c 206c 616d 6264 6120 7061 7261  imal lambda para
+0001ce00: 6d65 7465 7220 6f66 2074 6865 2042 6f78  meter of the Box
+0001ce10: 2d43 6f78 2074 7261 6e73 666f 726d 2062  -Cox transform b
+0001ce20: 790a 2020 2020 2020 2020 4d4c 452c 2066  y.        MLE, f
+0001ce30: 6f72 206f 6273 6572 7665 6420 6461 7461  or observed data
+0001ce40: 2078 2e0a 0a20 2020 2020 2020 2057 6520   x...        We 
+0001ce50: 6865 7265 2075 7365 2073 6369 7079 2062  here use scipy b
+0001ce60: 7569 6c74 696e 7320 7768 6963 6820 7573  uiltins which us
+0001ce70: 6573 2074 6865 2062 7265 6e74 206f 7074  es the brent opt
+0001ce80: 696d 697a 6572 2e0a 2020 2020 2020 2020  imizer..        
+0001ce90: 2222 220a 2020 2020 2020 2020 6d61 736b  """.        mask
+0001cea0: 203d 206e 702e 6973 6e61 6e28 7829 0a20   = np.isnan(x). 
+0001ceb0: 2020 2020 2020 2069 6620 6e70 2e61 6c6c         if np.all
+0001cec0: 286d 6173 6b29 3a0a 2020 2020 2020 2020  (mask):.        
+0001ced0: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+0001cee0: 7272 6f72 2822 436f 6c75 6d6e 206d 7573  rror("Column mus
+0001cef0: 7420 6e6f 7420 6265 2061 6c6c 206e 616e  t not be all nan
+0001cf00: 2e22 290a 0a20 2020 2020 2020 2023 2074  .")..        # t
+0001cf10: 6865 2063 6f6d 7075 7461 7469 6f6e 206f  he computation o
+0001cf20: 6620 6c61 6d62 6461 2069 7320 696e 666c  f lambda is infl
+0001cf30: 7565 6e63 6564 2062 7920 4e61 4e73 2073  uenced by NaNs s
+0001cf40: 6f20 7765 206e 6565 6420 746f 0a20 2020  o we need to.   
+0001cf50: 2020 2020 2023 2067 6574 2072 6964 206f       # get rid o
+0001cf60: 6620 7468 656d 0a20 2020 2020 2020 205f  f them.        _
+0001cf70: 2c20 6c6d 6264 6120 3d20 7374 6174 732e  , lmbda = stats.
+0001cf80: 626f 7863 6f78 2878 5b7e 6d61 736b 5d2c  boxcox(x[~mask],
+0001cf90: 206c 6d62 6461 3d4e 6f6e 6529 0a0a 2020   lmbda=None)..  
+0001cfa0: 2020 2020 2020 7265 7475 726e 206c 6d62        return lmb
+0001cfb0: 6461 0a0a 2020 2020 6465 6620 5f79 656f  da..    def _yeo
+0001cfc0: 5f6a 6f68 6e73 6f6e 5f6f 7074 696d 697a  _johnson_optimiz
+0001cfd0: 6528 7365 6c66 2c20 7829 3a0a 2020 2020  e(self, x):.    
+0001cfe0: 2020 2020 2222 2246 696e 6420 616e 6420      """Find and 
+0001cff0: 7265 7475 726e 206f 7074 696d 616c 206c  return optimal l
+0001d000: 616d 6264 6120 7061 7261 6d65 7465 7220  ambda parameter 
+0001d010: 6f66 2074 6865 2059 656f 2d4a 6f68 6e73  of the Yeo-Johns
+0001d020: 6f6e 0a20 2020 2020 2020 2074 7261 6e73  on.        trans
+0001d030: 666f 726d 2062 7920 4d4c 452c 2066 6f72  form by MLE, for
+0001d040: 206f 6273 6572 7665 6420 6461 7461 2078   observed data x
+0001d050: 2e0a 0a20 2020 2020 2020 204c 696b 6520  ...        Like 
+0001d060: 666f 7220 426f 782d 436f 782c 204d 4c45  for Box-Cox, MLE
+0001d070: 2069 7320 646f 6e65 2076 6961 2074 6865   is done via the
+0001d080: 2062 7265 6e74 206f 7074 696d 697a 6572   brent optimizer
+0001d090: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
+0001d0a0: 2020 2020 2020 785f 7469 6e79 203d 206e        x_tiny = n
+0001d0b0: 702e 6669 6e66 6f28 6e70 2e66 6c6f 6174  p.finfo(np.float
+0001d0c0: 3634 292e 7469 6e79 0a0a 2020 2020 2020  64).tiny..      
+0001d0d0: 2020 6465 6620 5f6e 6567 5f6c 6f67 5f6c    def _neg_log_l
+0001d0e0: 696b 656c 6968 6f6f 6428 6c6d 6264 6129  ikelihood(lmbda)
+0001d0f0: 3a0a 2020 2020 2020 2020 2020 2020 2222  :.            ""
+0001d100: 2252 6574 7572 6e20 7468 6520 6e65 6761  "Return the nega
+0001d110: 7469 7665 206c 6f67 206c 696b 656c 6968  tive log likelih
+0001d120: 6f6f 6420 6f66 2074 6865 206f 6273 6572  ood of the obser
+0001d130: 7665 6420 6461 7461 2078 2061 7320 610a  ved data x as a.
+0001d140: 2020 2020 2020 2020 2020 2020 6675 6e63              func
+0001d150: 7469 6f6e 206f 6620 6c61 6d62 6461 2e22  tion of lambda."
+0001d160: 2222 0a20 2020 2020 2020 2020 2020 2078  "".            x
+0001d170: 5f74 7261 6e73 203d 2073 656c 662e 5f79  _trans = self._y
+0001d180: 656f 5f6a 6f68 6e73 6f6e 5f74 7261 6e73  eo_johnson_trans
+0001d190: 666f 726d 2878 2c20 6c6d 6264 6129 0a20  form(x, lmbda). 
+0001d1a0: 2020 2020 2020 2020 2020 206e 5f73 616d             n_sam
+0001d1b0: 706c 6573 203d 2078 2e73 6861 7065 5b30  ples = x.shape[0
+0001d1c0: 5d0a 2020 2020 2020 2020 2020 2020 785f  ].            x_
+0001d1d0: 7472 616e 735f 7661 7220 3d20 785f 7472  trans_var = x_tr
+0001d1e0: 616e 732e 7661 7228 290a 0a20 2020 2020  ans.var()..     
+0001d1f0: 2020 2020 2020 2023 2052 656a 6563 7420         # Reject 
+0001d200: 7472 616e 7366 6f72 6d65 6420 6461 7461  transformed data
+0001d210: 2074 6861 7420 776f 756c 6420 7261 6973   that would rais
+0001d220: 6520 6120 5275 6e74 696d 6557 6172 6e69  e a RuntimeWarni
+0001d230: 6e67 2069 6e20 6e70 2e6c 6f67 0a20 2020  ng in np.log.   
+0001d240: 2020 2020 2020 2020 2069 6620 785f 7472           if x_tr
+0001d250: 616e 735f 7661 7220 3c20 785f 7469 6e79  ans_var < x_tiny
+0001d260: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              
+0001d270: 2020 7265 7475 726e 206e 702e 696e 660a    return np.inf.
+0001d280: 0a20 2020 2020 2020 2020 2020 206c 6f67  .            log
+0001d290: 5f76 6172 203d 206e 702e 6c6f 6728 785f  _var = np.log(x_
+0001d2a0: 7472 616e 735f 7661 7229 0a20 2020 2020  trans_var).     
+0001d2b0: 2020 2020 2020 206c 6f67 6c69 6b65 203d         loglike =
+0001d2c0: 202d 6e5f 7361 6d70 6c65 7320 2f20 3220   -n_samples / 2 
+0001d2d0: 2a20 6c6f 675f 7661 720a 2020 2020 2020  * log_var.      
+0001d2e0: 2020 2020 2020 6c6f 676c 696b 6520 2b3d        loglike +=
+0001d2f0: 2028 6c6d 6264 6120 2d20 3129 202a 2028   (lmbda - 1) * (
+0001d300: 6e70 2e73 6967 6e28 7829 202a 206e 702e  np.sign(x) * np.
+0001d310: 6c6f 6731 7028 6e70 2e61 6273 2878 2929  log1p(np.abs(x))
+0001d320: 292e 7375 6d28 290a 0a20 2020 2020 2020  ).sum()..       
+0001d330: 2020 2020 2072 6574 7572 6e20 2d6c 6f67       return -log
+0001d340: 6c69 6b65 0a0a 2020 2020 2020 2020 2320  like..        # 
+0001d350: 7468 6520 636f 6d70 7574 6174 696f 6e20  the computation 
+0001d360: 6f66 206c 616d 6264 6120 6973 2069 6e66  of lambda is inf
+0001d370: 6c75 656e 6365 6420 6279 204e 614e 7320  luenced by NaNs 
+0001d380: 736f 2077 6520 6e65 6564 2074 6f0a 2020  so we need to.  
+0001d390: 2020 2020 2020 2320 6765 7420 7269 6420        # get rid 
+0001d3a0: 6f66 2074 6865 6d0a 2020 2020 2020 2020  of them.        
+0001d3b0: 7820 3d20 785b 7e6e 702e 6973 6e61 6e28  x = x[~np.isnan(
+0001d3c0: 7829 5d0a 2020 2020 2020 2020 2320 6368  x)].        # ch
+0001d3d0: 6f6f 7369 6e67 2062 7261 636b 6574 202d  oosing bracket -
+0001d3e0: 322c 2032 206c 696b 6520 666f 7220 626f  2, 2 like for bo
+0001d3f0: 7863 6f78 0a20 2020 2020 2020 2072 6574  xcox.        ret
+0001d400: 7572 6e20 6f70 7469 6d69 7a65 2e62 7265  urn optimize.bre
+0001d410: 6e74 285f 6e65 675f 6c6f 675f 6c69 6b65  nt(_neg_log_like
+0001d420: 6c69 686f 6f64 2c20 6272 6163 6b3d 282d  lihood, brack=(-
+0001d430: 322c 2032 2929 0a0a 2020 2020 6465 6620  2, 2))..    def 
+0001d440: 5f63 6865 636b 5f69 6e70 7574 2873 656c  _check_input(sel
+0001d450: 662c 2058 2c20 696e 5f66 6974 2c20 6368  f, X, in_fit, ch
+0001d460: 6563 6b5f 706f 7369 7469 7665 3d46 616c  eck_positive=Fal
+0001d470: 7365 2c20 6368 6563 6b5f 7368 6170 653d  se, check_shape=
+0001d480: 4661 6c73 6529 3a0a 2020 2020 2020 2020  False):.        
+0001d490: 2222 2256 616c 6964 6174 6520 7468 6520  """Validate the 
+0001d4a0: 696e 7075 7420 6265 666f 7265 2066 6974  input before fit
+0001d4b0: 2061 6e64 2074 7261 6e73 666f 726d 2e0a   and transform..
+0001d4c0: 0a20 2020 2020 2020 2050 6172 616d 6574  .        Paramet
+0001d4d0: 6572 730a 2020 2020 2020 2020 2d2d 2d2d  ers.        ----
+0001d4e0: 2d2d 2d2d 2d2d 0a20 2020 2020 2020 2058  ------.        X
+0001d4f0: 203a 2061 7272 6179 2d6c 696b 6520 6f66   : array-like of
+0001d500: 2073 6861 7065 2028 6e5f 7361 6d70 6c65   shape (n_sample
+0001d510: 732c 206e 5f66 6561 7475 7265 7329 0a0a  s, n_features)..
+0001d520: 2020 2020 2020 2020 696e 5f66 6974 203a          in_fit :
+0001d530: 2062 6f6f 6c0a 2020 2020 2020 2020 2020   bool.          
+0001d540: 2020 5768 6574 6865 7220 6f72 206e 6f74    Whether or not
+0001d550: 2060 5f63 6865 636b 5f69 6e70 7574 6020   `_check_input` 
+0001d560: 6973 2063 616c 6c65 6420 6672 6f6d 2060  is called from `
+0001d570: 6669 7460 206f 7220 6f74 6865 720a 2020  fit` or other.  
+0001d580: 2020 2020 2020 2020 2020 6d65 7468 6f64            method
+0001d590: 732c 2065 2e67 2e20 6070 7265 6469 6374  s, e.g. `predict
+0001d5a0: 602c 2060 7472 616e 7366 6f72 6d60 2c20  `, `transform`, 
+0001d5b0: 6574 632e 0a0a 2020 2020 2020 2020 6368  etc...        ch
+0001d5c0: 6563 6b5f 706f 7369 7469 7665 203a 2062  eck_positive : b
+0001d5d0: 6f6f 6c2c 2064 6566 6175 6c74 3d46 616c  ool, default=Fal
+0001d5e0: 7365 0a20 2020 2020 2020 2020 2020 2049  se.            I
+0001d5f0: 6620 5472 7565 2c20 6368 6563 6b20 7468  f True, check th
+0001d600: 6174 2061 6c6c 2064 6174 6120 6973 2070  at all data is p
+0001d610: 6f73 6974 6976 6520 616e 6420 6e6f 6e2d  ositive and non-
+0001d620: 7a65 726f 2028 6f6e 6c79 2069 660a 2020  zero (only if.  
+0001d630: 2020 2020 2020 2020 2020 6060 7365 6c66            ``self
+0001d640: 2e6d 6574 686f 643d 3d27 626f 782d 636f  .method=='box-co
+0001d650: 7827 6060 292e 0a0a 2020 2020 2020 2020  x'``)...        
+0001d660: 6368 6563 6b5f 7368 6170 6520 3a20 626f  check_shape : bo
+0001d670: 6f6c 2c20 6465 6661 756c 743d 4661 6c73  ol, default=Fals
+0001d680: 650a 2020 2020 2020 2020 2020 2020 4966  e.            If
+0001d690: 2054 7275 652c 2063 6865 636b 2074 6861   True, check tha
+0001d6a0: 7420 6e5f 6665 6174 7572 6573 206d 6174  t n_features mat
+0001d6b0: 6368 6573 2074 6865 206c 656e 6774 6820  ches the length 
+0001d6c0: 6f66 2073 656c 662e 6c61 6d62 6461 735f  of self.lambdas_
+0001d6d0: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
+0001d6e0: 2020 2020 2058 203d 2073 656c 662e 5f76       X = self._v
+0001d6f0: 616c 6964 6174 655f 6461 7461 280a 2020  alidate_data(.  
+0001d700: 2020 2020 2020 2020 2020 582c 0a20 2020            X,.   
+0001d710: 2020 2020 2020 2020 2065 6e73 7572 655f           ensure_
+0001d720: 3264 3d54 7275 652c 0a20 2020 2020 2020  2d=True,.       
+0001d730: 2020 2020 2064 7479 7065 3d46 4c4f 4154       dtype=FLOAT
+0001d740: 5f44 5459 5045 532c 0a20 2020 2020 2020  _DTYPES,.       
+0001d750: 2020 2020 2063 6f70 793d 7365 6c66 2e63       copy=self.c
+0001d760: 6f70 792c 0a20 2020 2020 2020 2020 2020  opy,.           
+0001d770: 2066 6f72 6365 5f61 6c6c 5f66 696e 6974   force_all_finit
+0001d780: 653d 2261 6c6c 6f77 2d6e 616e 222c 0a20  e="allow-nan",. 
+0001d790: 2020 2020 2020 2020 2020 2072 6573 6574             reset
+0001d7a0: 3d69 6e5f 6669 742c 0a20 2020 2020 2020  =in_fit,.       
+0001d7b0: 2029 0a0a 2020 2020 2020 2020 7769 7468   )..        with
+0001d7c0: 2077 6172 6e69 6e67 732e 6361 7463 685f   warnings.catch_
+0001d7d0: 7761 726e 696e 6773 2829 3a0a 2020 2020  warnings():.    
+0001d7e0: 2020 2020 2020 2020 7761 726e 696e 6773          warnings
+0001d7f0: 2e66 696c 7465 7277 6172 6e69 6e67 7328  .filterwarnings(
+0001d800: 2269 676e 6f72 6522 2c20 7222 416c 6c2d  "ignore", r"All-
+0001d810: 4e61 4e20 2873 6c69 6365 7c61 7869 7329  NaN (slice|axis)
+0001d820: 2065 6e63 6f75 6e74 6572 6564 2229 0a20   encountered"). 
+0001d830: 2020 2020 2020 2020 2020 2069 6620 6368             if ch
+0001d840: 6563 6b5f 706f 7369 7469 7665 2061 6e64  eck_positive and
+0001d850: 2073 656c 662e 6d65 7468 6f64 203d 3d20   self.method == 
+0001d860: 2262 6f78 2d63 6f78 2220 616e 6420 6e70  "box-cox" and np
+0001d870: 2e6e 616e 6d69 6e28 5829 203c 3d20 303a  .nanmin(X) <= 0:
+0001d880: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001d890: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+0001d8a0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             
+0001d8b0: 2020 2020 2020 2022 5468 6520 426f 782d         "The Box-
+0001d8c0: 436f 7820 7472 616e 7366 6f72 6d61 7469  Cox transformati
+0001d8d0: 6f6e 2063 616e 206f 6e6c 7920 6265 2022  on can only be "
+0001d8e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+0001d8f0: 2020 2020 2022 6170 706c 6965 6420 746f       "applied to
+0001d900: 2073 7472 6963 746c 7920 706f 7369 7469   strictly positi
+0001d910: 7665 2064 6174 6122 0a20 2020 2020 2020  ve data".       
+0001d920: 2020 2020 2020 2020 2029 0a0a 2020 2020           )..    
+0001d930: 2020 2020 6966 2063 6865 636b 5f73 6861      if check_sha
+0001d940: 7065 2061 6e64 206e 6f74 2058 2e73 6861  pe and not X.sha
+0001d950: 7065 5b31 5d20 3d3d 206c 656e 2873 656c  pe[1] == len(sel
+0001d960: 662e 6c61 6d62 6461 735f 293a 0a20 2020  f.lambdas_):.   
+0001d970: 2020 2020 2020 2020 2072 6169 7365 2056           raise V
+0001d980: 616c 7565 4572 726f 7228 0a20 2020 2020  alueError(.     
+0001d990: 2020 2020 2020 2020 2020 2022 496e 7075             "Inpu
+0001d9a0: 7420 6461 7461 2068 6173 2061 2064 6966  t data has a dif
+0001d9b0: 6665 7265 6e74 206e 756d 6265 7220 6f66  ferent number of
+0001d9c0: 2066 6561 7475 7265 7320 220a 2020 2020   features ".    
+0001d9d0: 2020 2020 2020 2020 2020 2020 2274 6861              "tha
+0001d9e0: 6e20 6669 7474 696e 6720 6461 7461 2e20  n fitting data. 
+0001d9f0: 5368 6f75 6c64 2068 6176 6520 7b6e 7d2c  Should have {n},
+0001da00: 2064 6174 6120 6861 7320 7b6d 7d22 2e66   data has {m}".f
+0001da10: 6f72 6d61 7428 0a20 2020 2020 2020 2020  ormat(.         
+0001da20: 2020 2020 2020 2020 2020 206e 3d6c 656e             n=len
+0001da30: 2873 656c 662e 6c61 6d62 6461 735f 292c  (self.lambdas_),
+0001da40: 206d 3d58 2e73 6861 7065 5b31 5d0a 2020   m=X.shape[1].  
+0001da50: 2020 2020 2020 2020 2020 2020 2020 290a                ).
+0001da60: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. 
+0001da70: 2020 2020 2020 2072 6574 7572 6e20 580a         return X.
+0001da80: 0a20 2020 2064 6566 205f 6d6f 7265 5f74  .    def _more_t
+0001da90: 6167 7328 7365 6c66 293a 0a20 2020 2020  ags(self):.     
+0001daa0: 2020 2072 6574 7572 6e20 7b22 616c 6c6f     return {"allo
+0001dab0: 775f 6e61 6e22 3a20 5472 7565 7d0a 0a0a  w_nan": True}...
+0001dac0: 4076 616c 6964 6174 655f 7061 7261 6d73  @validate_params
+0001dad0: 280a 2020 2020 7b22 5822 3a20 5b22 6172  (.    {"X": ["ar
+0001dae0: 7261 792d 6c69 6b65 225d 7d2c 0a20 2020  ray-like"]},.   
+0001daf0: 2070 7265 6665 725f 736b 6970 5f6e 6573   prefer_skip_nes
+0001db00: 7465 645f 7661 6c69 6461 7469 6f6e 3d46  ted_validation=F
+0001db10: 616c 7365 2c0a 290a 6465 6620 706f 7765  alse,.).def powe
+0001db20: 725f 7472 616e 7366 6f72 6d28 582c 206d  r_transform(X, m
+0001db30: 6574 686f 643d 2279 656f 2d6a 6f68 6e73  ethod="yeo-johns
+0001db40: 6f6e 222c 202a 2c20 7374 616e 6461 7264  on", *, standard
+0001db50: 697a 653d 5472 7565 2c20 636f 7079 3d54  ize=True, copy=T
+0001db60: 7275 6529 3a0a 2020 2020 2222 2250 6172  rue):.    """Par
+0001db70: 616d 6574 7269 632c 206d 6f6e 6f74 6f6e  ametric, monoton
+0001db80: 6963 2074 7261 6e73 666f 726d 6174 696f  ic transformatio
+0001db90: 6e20 746f 206d 616b 6520 6461 7461 206d  n to make data m
+0001dba0: 6f72 6520 4761 7573 7369 616e 2d6c 696b  ore Gaussian-lik
+0001dbb0: 652e 0a0a 2020 2020 506f 7765 7220 7472  e...    Power tr
+0001dbc0: 616e 7366 6f72 6d73 2061 7265 2061 2066  ansforms are a f
+0001dbd0: 616d 696c 7920 6f66 2070 6172 616d 6574  amily of paramet
+0001dbe0: 7269 632c 206d 6f6e 6f74 6f6e 6963 2074  ric, monotonic t
+0001dbf0: 7261 6e73 666f 726d 6174 696f 6e73 0a20  ransformations. 
+0001dc00: 2020 2074 6861 7420 6172 6520 6170 706c     that are appl
+0001dc10: 6965 6420 746f 206d 616b 6520 6461 7461  ied to make data
+0001dc20: 206d 6f72 6520 4761 7573 7369 616e 2d6c   more Gaussian-l
+0001dc30: 696b 652e 2054 6869 7320 6973 2075 7365  ike. This is use
+0001dc40: 6675 6c20 666f 720a 2020 2020 6d6f 6465  ful for.    mode
+0001dc50: 6c69 6e67 2069 7373 7565 7320 7265 6c61  ling issues rela
+0001dc60: 7465 6420 746f 2068 6574 6572 6f73 6365  ted to heterosce
+0001dc70: 6461 7374 6963 6974 7920 286e 6f6e 2d63  dasticity (non-c
+0001dc80: 6f6e 7374 616e 7420 7661 7269 616e 6365  onstant variance
+0001dc90: 292c 0a20 2020 206f 7220 6f74 6865 7220  ),.    or other 
+0001dca0: 7369 7475 6174 696f 6e73 2077 6865 7265  situations where
+0001dcb0: 206e 6f72 6d61 6c69 7479 2069 7320 6465   normality is de
+0001dcc0: 7369 7265 642e 0a0a 2020 2020 4375 7272  sired...    Curr
+0001dcd0: 656e 746c 792c 2070 6f77 6572 5f74 7261  ently, power_tra
+0001dce0: 6e73 666f 726d 2073 7570 706f 7274 7320  nsform supports 
+0001dcf0: 7468 6520 426f 782d 436f 7820 7472 616e  the Box-Cox tran
+0001dd00: 7366 6f72 6d20 616e 6420 7468 650a 2020  sform and the.  
+0001dd10: 2020 5965 6f2d 4a6f 686e 736f 6e20 7472    Yeo-Johnson tr
+0001dd20: 616e 7366 6f72 6d2e 2054 6865 206f 7074  ansform. The opt
+0001dd30: 696d 616c 2070 6172 616d 6574 6572 2066  imal parameter f
+0001dd40: 6f72 2073 7461 6269 6c69 7a69 6e67 2076  or stabilizing v
+0001dd50: 6172 6961 6e63 6520 616e 640a 2020 2020  ariance and.    
+0001dd60: 6d69 6e69 6d69 7a69 6e67 2073 6b65 776e  minimizing skewn
+0001dd70: 6573 7320 6973 2065 7374 696d 6174 6564  ess is estimated
+0001dd80: 2074 6872 6f75 6768 206d 6178 696d 756d   through maximum
+0001dd90: 206c 696b 656c 6968 6f6f 642e 0a0a 2020   likelihood...  
+0001dda0: 2020 426f 782d 436f 7820 7265 7175 6972    Box-Cox requir
+0001ddb0: 6573 2069 6e70 7574 2064 6174 6120 746f  es input data to
+0001ddc0: 2062 6520 7374 7269 6374 6c79 2070 6f73   be strictly pos
+0001ddd0: 6974 6976 652c 2077 6869 6c65 2059 656f  itive, while Yeo
+0001dde0: 2d4a 6f68 6e73 6f6e 0a20 2020 2073 7570  -Johnson.    sup
+0001ddf0: 706f 7274 7320 626f 7468 2070 6f73 6974  ports both posit
+0001de00: 6976 6520 6f72 206e 6567 6174 6976 6520  ive or negative 
+0001de10: 6461 7461 2e0a 0a20 2020 2042 7920 6465  data...    By de
+0001de20: 6661 756c 742c 207a 6572 6f2d 6d65 616e  fault, zero-mean
+0001de30: 2c20 756e 6974 2d76 6172 6961 6e63 6520  , unit-variance 
+0001de40: 6e6f 726d 616c 697a 6174 696f 6e20 6973  normalization is
+0001de50: 2061 7070 6c69 6564 2074 6f20 7468 650a   applied to the.
+0001de60: 2020 2020 7472 616e 7366 6f72 6d65 6420      transformed 
+0001de70: 6461 7461 2e0a 0a20 2020 2052 6561 6420  data...    Read 
+0001de80: 6d6f 7265 2069 6e20 7468 6520 3a72 6566  more in the :ref
+0001de90: 3a60 5573 6572 2047 7569 6465 203c 7072  :`User Guide <pr
+0001dea0: 6570 726f 6365 7373 696e 675f 7472 616e  eprocessing_tran
+0001deb0: 7366 6f72 6d65 723e 602e 0a0a 2020 2020  sformer>`...    
+0001dec0: 5061 7261 6d65 7465 7273 0a20 2020 202d  Parameters.    -
+0001ded0: 2d2d 2d2d 2d2d 2d2d 2d0a 2020 2020 5820  ---------.    X 
+0001dee0: 3a20 6172 7261 792d 6c69 6b65 206f 6620  : array-like of 
+0001def0: 7368 6170 6520 286e 5f73 616d 706c 6573  shape (n_samples
+0001df00: 2c20 6e5f 6665 6174 7572 6573 290a 2020  , n_features).  
+0001df10: 2020 2020 2020 5468 6520 6461 7461 2074        The data t
+0001df20: 6f20 6265 2074 7261 6e73 666f 726d 6564  o be transformed
+0001df30: 2075 7369 6e67 2061 2070 6f77 6572 2074   using a power t
+0001df40: 7261 6e73 666f 726d 6174 696f 6e2e 0a0a  ransformation...
+0001df50: 2020 2020 6d65 7468 6f64 203a 207b 2779      method : {'y
+0001df60: 656f 2d6a 6f68 6e73 6f6e 272c 2027 626f  eo-johnson', 'bo
+0001df70: 782d 636f 7827 7d2c 2064 6566 6175 6c74  x-cox'}, default
+0001df80: 3d27 7965 6f2d 6a6f 686e 736f 6e27 0a20  ='yeo-johnson'. 
+0001df90: 2020 2020 2020 2054 6865 2070 6f77 6572         The power
+0001dfa0: 2074 7261 6e73 666f 726d 206d 6574 686f   transform metho
+0001dfb0: 642e 2041 7661 696c 6162 6c65 206d 6574  d. Available met
+0001dfc0: 686f 6473 2061 7265 3a0a 0a20 2020 2020  hods are:..     
+0001dfd0: 2020 202d 2027 7965 6f2d 6a6f 686e 736f     - 'yeo-johnso
+0001dfe0: 6e27 205b 315d 5f2c 2077 6f72 6b73 2077  n' [1]_, works w
+0001dff0: 6974 6820 706f 7369 7469 7665 2061 6e64  ith positive and
+0001e000: 206e 6567 6174 6976 6520 7661 6c75 6573   negative values
+0001e010: 0a20 2020 2020 2020 202d 2027 626f 782d  .        - 'box-
+0001e020: 636f 7827 205b 325d 5f2c 206f 6e6c 7920  cox' [2]_, only 
+0001e030: 776f 726b 7320 7769 7468 2073 7472 6963  works with stric
+0001e040: 746c 7920 706f 7369 7469 7665 2076 616c  tly positive val
+0001e050: 7565 730a 0a20 2020 2020 2020 202e 2e20  ues..        .. 
+0001e060: 7665 7273 696f 6e63 6861 6e67 6564 3a3a  versionchanged::
+0001e070: 2030 2e32 330a 2020 2020 2020 2020 2020   0.23.          
+0001e080: 2020 5468 6520 6465 6661 756c 7420 7661    The default va
+0001e090: 6c75 6520 6f66 2074 6865 2060 6d65 7468  lue of the `meth
+0001e0a0: 6f64 6020 7061 7261 6d65 7465 7220 6368  od` parameter ch
+0001e0b0: 616e 6765 6420 6672 6f6d 0a20 2020 2020  anged from.     
+0001e0c0: 2020 2020 2020 2027 626f 782d 636f 7827         'box-cox'
+0001e0d0: 2074 6f20 2779 656f 2d6a 6f68 6e73 6f6e   to 'yeo-johnson
+0001e0e0: 2720 696e 2030 2e32 332e 0a0a 2020 2020  ' in 0.23...    
+0001e0f0: 7374 616e 6461 7264 697a 6520 3a20 626f  standardize : bo
+0001e100: 6f6c 2c20 6465 6661 756c 743d 5472 7565  ol, default=True
+0001e110: 0a20 2020 2020 2020 2053 6574 2074 6f20  .        Set to 
+0001e120: 5472 7565 2074 6f20 6170 706c 7920 7a65  True to apply ze
+0001e130: 726f 2d6d 6561 6e2c 2075 6e69 742d 7661  ro-mean, unit-va
+0001e140: 7269 616e 6365 206e 6f72 6d61 6c69 7a61  riance normaliza
+0001e150: 7469 6f6e 2074 6f20 7468 650a 2020 2020  tion to the.    
+0001e160: 2020 2020 7472 616e 7366 6f72 6d65 6420      transformed 
+0001e170: 6f75 7470 7574 2e0a 0a20 2020 2063 6f70  output...    cop
+0001e180: 7920 3a20 626f 6f6c 2c20 6465 6661 756c  y : bool, defaul
+0001e190: 743d 5472 7565 0a20 2020 2020 2020 2049  t=True.        I
+0001e1a0: 6620 4661 6c73 652c 2074 7279 2074 6f20  f False, try to 
+0001e1b0: 6176 6f69 6420 6120 636f 7079 2061 6e64  avoid a copy and
+0001e1c0: 2074 7261 6e73 666f 726d 2069 6e20 706c   transform in pl
+0001e1d0: 6163 652e 0a20 2020 2020 2020 2054 6869  ace..        Thi
+0001e1e0: 7320 6973 206e 6f74 2067 7561 7261 6e74  s is not guarant
+0001e1f0: 6565 6420 746f 2061 6c77 6179 7320 776f  eed to always wo
+0001e200: 726b 2069 6e20 706c 6163 653b 2065 2e67  rk in place; e.g
+0001e210: 2e20 6966 2074 6865 2064 6174 6120 6973  . if the data is
+0001e220: 0a20 2020 2020 2020 2061 206e 756d 7079  .        a numpy
+0001e230: 2061 7272 6179 2077 6974 6820 616e 2069   array with an i
+0001e240: 6e74 2064 7479 7065 2c20 6120 636f 7079  nt dtype, a copy
+0001e250: 2077 696c 6c20 6265 2072 6574 7572 6e65   will be returne
+0001e260: 6420 6576 656e 2077 6974 680a 2020 2020  d even with.    
+0001e270: 2020 2020 636f 7079 3d46 616c 7365 2e0a      copy=False..
+0001e280: 0a20 2020 2052 6574 7572 6e73 0a20 2020  .    Returns.   
+0001e290: 202d 2d2d 2d2d 2d2d 0a20 2020 2058 5f74   -------.    X_t
+0001e2a0: 7261 6e73 203a 206e 6461 7272 6179 206f  rans : ndarray o
+0001e2b0: 6620 7368 6170 6520 286e 5f73 616d 706c  f shape (n_sampl
+0001e2c0: 6573 2c20 6e5f 6665 6174 7572 6573 290a  es, n_features).
+0001e2d0: 2020 2020 2020 2020 5468 6520 7472 616e          The tran
+0001e2e0: 7366 6f72 6d65 6420 6461 7461 2e0a 0a20  sformed data... 
+0001e2f0: 2020 2053 6565 2041 6c73 6f0a 2020 2020     See Also.    
+0001e300: 2d2d 2d2d 2d2d 2d2d 0a20 2020 2050 6f77  --------.    Pow
+0001e310: 6572 5472 616e 7366 6f72 6d65 7220 3a20  erTransformer : 
+0001e320: 4571 7569 7661 6c65 6e74 2074 7261 6e73  Equivalent trans
+0001e330: 666f 726d 6174 696f 6e20 7769 7468 2074  formation with t
+0001e340: 6865 0a20 2020 2020 2020 2054 7261 6e73  he.        Trans
+0001e350: 666f 726d 6572 2041 5049 2028 652e 672e  former API (e.g.
+0001e360: 2061 7320 7061 7274 206f 6620 6120 7072   as part of a pr
+0001e370: 6570 726f 6365 7373 696e 670a 2020 2020  eprocessing.    
+0001e380: 2020 2020 3a63 6c61 7373 3a60 7e73 6b6c      :class:`~skl
+0001e390: 6561 726e 2e70 6970 656c 696e 652e 5069  earn.pipeline.Pi
+0001e3a0: 7065 6c69 6e65 6029 2e0a 0a20 2020 2071  peline`)...    q
+0001e3b0: 7561 6e74 696c 655f 7472 616e 7366 6f72  uantile_transfor
+0001e3c0: 6d20 3a20 4d61 7073 2064 6174 6120 746f  m : Maps data to
+0001e3d0: 2061 2073 7461 6e64 6172 6420 6e6f 726d   a standard norm
+0001e3e0: 616c 2064 6973 7472 6962 7574 696f 6e20  al distribution 
+0001e3f0: 7769 7468 0a20 2020 2020 2020 2074 6865  with.        the
+0001e400: 2070 6172 616d 6574 6572 2060 6f75 7470   parameter `outp
+0001e410: 7574 5f64 6973 7472 6962 7574 696f 6e3d  ut_distribution=
+0001e420: 276e 6f72 6d61 6c27 602e 0a0a 2020 2020  'normal'`...    
+0001e430: 4e6f 7465 730a 2020 2020 2d2d 2d2d 2d0a  Notes.    -----.
+0001e440: 2020 2020 4e61 4e73 2061 7265 2074 7265      NaNs are tre
+0001e450: 6174 6564 2061 7320 6d69 7373 696e 6720  ated as missing 
+0001e460: 7661 6c75 6573 3a20 6469 7372 6567 6172  values: disregar
+0001e470: 6465 6420 696e 2060 6066 6974 6060 2c20  ded in ``fit``, 
+0001e480: 616e 6420 6d61 696e 7461 696e 6564 0a20  and maintained. 
+0001e490: 2020 2069 6e20 6060 7472 616e 7366 6f72     in ``transfor
+0001e4a0: 6d60 602e 0a0a 2020 2020 466f 7220 6120  m``...    For a 
+0001e4b0: 636f 6d70 6172 6973 6f6e 206f 6620 7468  comparison of th
+0001e4c0: 6520 6469 6666 6572 656e 7420 7363 616c  e different scal
+0001e4d0: 6572 732c 2074 7261 6e73 666f 726d 6572  ers, transformer
+0001e4e0: 732c 2061 6e64 206e 6f72 6d61 6c69 7a65  s, and normalize
+0001e4f0: 7273 2c0a 2020 2020 7365 653a 203a 7265  rs,.    see: :re
+0001e500: 663a 6073 7068 785f 676c 725f 6175 746f  f:`sphx_glr_auto
+0001e510: 5f65 7861 6d70 6c65 735f 7072 6570 726f  _examples_prepro
+0001e520: 6365 7373 696e 675f 706c 6f74 5f61 6c6c  cessing_plot_all
+0001e530: 5f73 6361 6c69 6e67 2e70 7960 2e0a 0a20  _scaling.py`... 
+0001e540: 2020 2052 6566 6572 656e 6365 730a 2020     References.  
+0001e550: 2020 2d2d 2d2d 2d2d 2d2d 2d2d 0a0a 2020    ----------..  
+0001e560: 2020 2e2e 205b 315d 2049 2e4b 2e20 5965    .. [1] I.K. Ye
+0001e570: 6f20 616e 6420 522e 412e 204a 6f68 6e73  o and R.A. Johns
+0001e580: 6f6e 2c20 2241 206e 6577 2066 616d 696c  on, "A new famil
+0001e590: 7920 6f66 2070 6f77 6572 2074 7261 6e73  y of power trans
+0001e5a0: 666f 726d 6174 696f 6e73 2074 6f0a 2020  formations to.  
+0001e5b0: 2020 2020 2020 2020 2069 6d70 726f 7665           improve
+0001e5c0: 206e 6f72 6d61 6c69 7479 206f 7220 7379   normality or sy
+0001e5d0: 6d6d 6574 7279 2e22 2042 696f 6d65 7472  mmetry." Biometr
+0001e5e0: 696b 612c 2038 3728 3429 2c20 7070 2e39  ika, 87(4), pp.9
+0001e5f0: 3534 2d39 3539 2c0a 2020 2020 2020 2020  54-959,.        
+0001e600: 2020 2028 3230 3030 292e 0a0a 2020 2020     (2000)...    
+0001e610: 2e2e 205b 325d 2047 2e45 2e50 2e20 426f  .. [2] G.E.P. Bo
+0001e620: 7820 616e 6420 442e 522e 2043 6f78 2c20  x and D.R. Cox, 
+0001e630: 2241 6e20 416e 616c 7973 6973 206f 6620  "An Analysis of 
+0001e640: 5472 616e 7366 6f72 6d61 7469 6f6e 7322  Transformations"
+0001e650: 2c20 4a6f 7572 6e61 6c0a 2020 2020 2020  , Journal.      
+0001e660: 2020 2020 206f 6620 7468 6520 526f 7961       of the Roya
+0001e670: 6c20 5374 6174 6973 7469 6361 6c20 536f  l Statistical So
+0001e680: 6369 6574 7920 422c 2032 362c 2032 3131  ciety B, 26, 211
+0001e690: 2d32 3532 2028 3139 3634 292e 0a0a 2020  -252 (1964)...  
+0001e6a0: 2020 4578 616d 706c 6573 0a20 2020 202d    Examples.    -
+0001e6b0: 2d2d 2d2d 2d2d 2d0a 2020 2020 3e3e 3e20  -------.    >>> 
+0001e6c0: 696d 706f 7274 206e 756d 7079 2061 7320  import numpy as 
+0001e6d0: 6e70 0a20 2020 203e 3e3e 2066 726f 6d20  np.    >>> from 
+0001e6e0: 736b 6c65 6172 6e2e 7072 6570 726f 6365  sklearn.preproce
+0001e6f0: 7373 696e 6720 696d 706f 7274 2070 6f77  ssing import pow
+0001e700: 6572 5f74 7261 6e73 666f 726d 0a20 2020  er_transform.   
+0001e710: 203e 3e3e 2064 6174 6120 3d20 5b5b 312c   >>> data = [[1,
+0001e720: 2032 5d2c 205b 332c 2032 5d2c 205b 342c   2], [3, 2], [4,
+0001e730: 2035 5d5d 0a20 2020 203e 3e3e 2070 7269   5]].    >>> pri
+0001e740: 6e74 2870 6f77 6572 5f74 7261 6e73 666f  nt(power_transfo
+0001e750: 726d 2864 6174 612c 206d 6574 686f 643d  rm(data, method=
+0001e760: 2762 6f78 2d63 6f78 2729 290a 2020 2020  'box-cox')).    
+0001e770: 5b5b 2d31 2e33 3332 2e2e 2e20 2d30 2e37  [[-1.332... -0.7
+0001e780: 3037 2e2e 2e5d 0a20 2020 2020 5b20 302e  07...].     [ 0.
+0001e790: 3235 362e 2e2e 202d 302e 3730 372e 2e2e  256... -0.707...
+0001e7a0: 5d0a 2020 2020 205b 2031 2e30 3736 2e2e  ].     [ 1.076..
+0001e7b0: 2e20 2031 2e34 3134 2e2e 2e5d 5d0a 0a20  .  1.414...]].. 
+0001e7c0: 2020 202e 2e20 7761 726e 696e 673a 3a20     .. warning:: 
+0001e7d0: 5269 736b 206f 6620 6461 7461 206c 6561  Risk of data lea
+0001e7e0: 6b2e 0a20 2020 2020 2020 2044 6f20 6e6f  k..        Do no
+0001e7f0: 7420 7573 6520 3a66 756e 633a 607e 736b  t use :func:`~sk
+0001e800: 6c65 6172 6e2e 7072 6570 726f 6365 7373  learn.preprocess
+0001e810: 696e 672e 706f 7765 725f 7472 616e 7366  ing.power_transf
+0001e820: 6f72 6d60 2075 6e6c 6573 7320 796f 750a  orm` unless you.
+0001e830: 2020 2020 2020 2020 6b6e 6f77 2077 6861          know wha
+0001e840: 7420 796f 7520 6172 6520 646f 696e 672e  t you are doing.
+0001e850: 2041 2063 6f6d 6d6f 6e20 6d69 7374 616b   A common mistak
+0001e860: 6520 6973 2074 6f20 6170 706c 7920 6974  e is to apply it
+0001e870: 2074 6f20 7468 6520 656e 7469 7265 0a20   to the entire. 
+0001e880: 2020 2020 2020 2064 6174 6120 2a62 6566         data *bef
+0001e890: 6f72 652a 2073 706c 6974 7469 6e67 2069  ore* splitting i
+0001e8a0: 6e74 6f20 7472 6169 6e69 6e67 2061 6e64  nto training and
+0001e8b0: 2074 6573 7420 7365 7473 2e20 5468 6973   test sets. This
+0001e8c0: 2077 696c 6c20 6269 6173 2074 6865 0a20   will bias the. 
+0001e8d0: 2020 2020 2020 206d 6f64 656c 2065 7661         model eva
+0001e8e0: 6c75 6174 696f 6e20 6265 6361 7573 6520  luation because 
+0001e8f0: 696e 666f 726d 6174 696f 6e20 776f 756c  information woul
+0001e900: 6420 6861 7665 206c 6561 6b65 6420 6672  d have leaked fr
+0001e910: 6f6d 2074 6865 2074 6573 740a 2020 2020  om the test.    
+0001e920: 2020 2020 7365 7420 746f 2074 6865 2074      set to the t
+0001e930: 7261 696e 696e 6720 7365 742e 0a20 2020  raining set..   
+0001e940: 2020 2020 2049 6e20 6765 6e65 7261 6c2c       In general,
+0001e950: 2077 6520 7265 636f 6d6d 656e 6420 7573   we recommend us
+0001e960: 696e 670a 2020 2020 2020 2020 3a63 6c61  ing.        :cla
+0001e970: 7373 3a60 7e73 6b6c 6561 726e 2e70 7265  ss:`~sklearn.pre
+0001e980: 7072 6f63 6573 7369 6e67 2e50 6f77 6572  processing.Power
+0001e990: 5472 616e 7366 6f72 6d65 7260 2077 6974  Transformer` wit
+0001e9a0: 6869 6e20 610a 2020 2020 2020 2020 3a72  hin a.        :r
+0001e9b0: 6566 3a60 5069 7065 6c69 6e65 203c 7069  ef:`Pipeline <pi
+0001e9c0: 7065 6c69 6e65 3e60 2069 6e20 6f72 6465  peline>` in orde
+0001e9d0: 7220 746f 2070 7265 7665 6e74 206d 6f73  r to prevent mos
+0001e9e0: 7420 7269 736b 7320 6f66 2064 6174 610a  t risks of data.
+0001e9f0: 2020 2020 2020 2020 6c65 616b 696e 672c          leaking,
+0001ea00: 2065 2e67 2e3a 2060 7069 7065 203d 206d   e.g.: `pipe = m
+0001ea10: 616b 655f 7069 7065 6c69 6e65 2850 6f77  ake_pipeline(Pow
+0001ea20: 6572 5472 616e 7366 6f72 6d65 7228 292c  erTransformer(),
+0001ea30: 0a20 2020 2020 2020 204c 6f67 6973 7469  .        Logisti
+0001ea40: 6352 6567 7265 7373 696f 6e28 2929 602e  cRegression())`.
+0001ea50: 0a20 2020 2022 2222 0a20 2020 2070 7420  .    """.    pt 
+0001ea60: 3d20 506f 7765 7254 7261 6e73 666f 726d  = PowerTransform
+0001ea70: 6572 286d 6574 686f 643d 6d65 7468 6f64  er(method=method
+0001ea80: 2c20 7374 616e 6461 7264 697a 653d 7374  , standardize=st
+0001ea90: 616e 6461 7264 697a 652c 2063 6f70 793d  andardize, copy=
+0001eaa0: 636f 7079 290a 2020 2020 7265 7475 726e  copy).    return
+0001eab0: 2070 742e 6669 745f 7472 616e 7366 6f72   pt.fit_transfor
+0001eac0: 6d28 5829 0a                             m(X).
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_discretization.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_discretization.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,23 +6,23 @@
 
 import warnings
 from numbers import Integral
 
 import numpy as np
 
 from ..base import BaseEstimator, TransformerMixin, _fit_context
-from ..utils import _safe_indexing
-from ..utils._param_validation import Hidden, Interval, Options, StrOptions
+from ..utils import resample
+from ..utils._param_validation import Interval, Options, StrOptions
+from ..utils.deprecation import _deprecate_Xt_in_inverse_transform
 from ..utils.stats import _weighted_percentile
 from ..utils.validation import (
     _check_feature_names_in,
     _check_sample_weight,
     check_array,
     check_is_fitted,
-    check_random_state,
 )
 from ._encoders import OneHotEncoder
 
 
 class KBinsDiscretizer(TransformerMixin, BaseEstimator):
     """
     Bin continuous data into intervals.
@@ -61,18 +61,17 @@
     dtype : {np.float32, np.float64}, default=None
         The desired data-type for the output. If None, output dtype is
         consistent with input dtype. Only np.float32 and np.float64 are
         supported.
 
         .. versionadded:: 0.24
 
-    subsample : int or None, default='warn'
+    subsample : int or None, default=200_000
         Maximum number of samples, used to fit the model, for computational
-        efficiency. Defaults to 200_000 when `strategy='quantile'` and to `None`
-        when `strategy='uniform'` or `strategy='kmeans'`.
+        efficiency.
         `subsample=None` means that all the training samples are used when
         computing the quantiles that determine the binning thresholds.
         Since quantile computation relies on sorting each column of `X` and
         that sorting has an `n log(n)` time complexity,
         it is recommended to use subsampling on datasets with a
         very large number of samples.
 
@@ -144,15 +143,15 @@
     --------
     >>> from sklearn.preprocessing import KBinsDiscretizer
     >>> X = [[-2, 1, -4,   -1],
     ...      [-1, 2, -3, -0.5],
     ...      [ 0, 3, -2,  0.5],
     ...      [ 1, 4, -1,    2]]
     >>> est = KBinsDiscretizer(
-    ...     n_bins=3, encode='ordinal', strategy='uniform', subsample=None
+    ...     n_bins=3, encode='ordinal', strategy='uniform'
     ... )
     >>> est.fit(X)
     KBinsDiscretizer(...)
     >>> Xt = est.transform(X)
     >>> Xt  # doctest: +SKIP
     array([[ 0., 0., 0., 0.],
            [ 1., 1., 1., 0.],
@@ -174,30 +173,26 @@
     """
 
     _parameter_constraints: dict = {
         "n_bins": [Interval(Integral, 2, None, closed="left"), "array-like"],
         "encode": [StrOptions({"onehot", "onehot-dense", "ordinal"})],
         "strategy": [StrOptions({"uniform", "quantile", "kmeans"})],
         "dtype": [Options(type, {np.float64, np.float32}), None],
-        "subsample": [
-            Interval(Integral, 1, None, closed="left"),
-            None,
-            Hidden(StrOptions({"warn"})),
-        ],
+        "subsample": [Interval(Integral, 1, None, closed="left"), None],
         "random_state": ["random_state"],
     }
 
     def __init__(
         self,
         n_bins=5,
         *,
         encode="onehot",
         strategy="quantile",
         dtype=None,
-        subsample="warn",
+        subsample=200_000,
         random_state=None,
     ):
         self.n_bins = n_bins
         self.encode = encode
         self.strategy = strategy
         self.dtype = dtype
         self.subsample = subsample
@@ -215,15 +210,15 @@
 
         y : None
             Ignored. This parameter exists only for compatibility with
             :class:`~sklearn.pipeline.Pipeline`.
 
         sample_weight : ndarray of shape (n_samples,)
             Contains weight values to be associated with each sample.
-            Only possible when `strategy` is set to `"quantile"`.
+            Cannot be used when `strategy` is set to `"uniform"`.
 
             .. versionadded:: 1.3
 
         Returns
         -------
         self : object
             Returns the instance itself.
@@ -240,33 +235,23 @@
         if sample_weight is not None and self.strategy == "uniform":
             raise ValueError(
                 "`sample_weight` was provided but it cannot be "
                 "used with strategy='uniform'. Got strategy="
                 f"{self.strategy!r} instead."
             )
 
-        if self.strategy in ("uniform", "kmeans") and self.subsample == "warn":
-            warnings.warn(
-                (
-                    "In version 1.5 onwards, subsample=200_000 "
-                    "will be used by default. Set subsample explicitly to "
-                    "silence this warning in the mean time. Set "
-                    "subsample=None to disable subsampling explicitly."
-                ),
-                FutureWarning,
+        if self.subsample is not None and n_samples > self.subsample:
+            # Take a subsample of `X`
+            X = resample(
+                X,
+                replace=False,
+                n_samples=self.subsample,
+                random_state=self.random_state,
             )
 
-        subsample = self.subsample
-        if subsample == "warn":
-            subsample = 200000 if self.strategy == "quantile" else None
-        if subsample is not None and n_samples > subsample:
-            rng = check_random_state(self.random_state)
-            subsample_idx = rng.choice(n_samples, size=subsample, replace=False)
-            X = _safe_indexing(X, subsample_idx)
-
         n_features = X.shape[1]
         n_bins = self._validate_n_bins(n_features)
 
         if sample_weight is not None:
             sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
 
         bin_edges = np.zeros(n_features, dtype=object)
@@ -401,37 +386,45 @@
         try:
             Xt_enc = self._encoder.transform(Xt)
         finally:
             # revert the initial dtype to avoid modifying self.
             self._encoder.dtype = dtype_init
         return Xt_enc
 
-    def inverse_transform(self, Xt):
+    def inverse_transform(self, X=None, *, Xt=None):
         """
         Transform discretized data back to original feature space.
 
         Note that this function does not regenerate the original data
         due to discretization rounding.
 
         Parameters
         ----------
+        X : array-like of shape (n_samples, n_features)
+            Transformed data in the binned space.
+
         Xt : array-like of shape (n_samples, n_features)
             Transformed data in the binned space.
 
+            .. deprecated:: 1.5
+                `Xt` was deprecated in 1.5 and will be removed in 1.7. Use `X` instead.
+
         Returns
         -------
         Xinv : ndarray, dtype={np.float32, np.float64}
             Data in the original feature space.
         """
+        X = _deprecate_Xt_in_inverse_transform(X, Xt)
+
         check_is_fitted(self)
 
         if "onehot" in self.encode:
-            Xt = self._encoder.inverse_transform(Xt)
+            X = self._encoder.inverse_transform(X)
 
-        Xinv = check_array(Xt, copy=True, dtype=(np.float64, np.float32))
+        Xinv = check_array(X, copy=True, dtype=(np.float64, np.float32))
         n_features = self.n_bins_.shape[0]
         if Xinv.shape[1] != n_features:
             raise ValueError(
                 "Incorrect number of features. Expecting {}, received {}.".format(
                     n_features, Xinv.shape[1]
                 )
             )
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_encoders.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_encoders.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,17 +6,18 @@
 import warnings
 from numbers import Integral
 
 import numpy as np
 from scipy import sparse
 
 from ..base import BaseEstimator, OneToOneFeatureMixin, TransformerMixin, _fit_context
-from ..utils import _safe_indexing, check_array, is_scalar_nan
+from ..utils import _safe_indexing, check_array
 from ..utils._encode import _check_unknown, _encode, _get_counts, _unique
 from ..utils._mask import _get_mask
+from ..utils._missing import is_scalar_nan
 from ..utils._param_validation import Interval, RealNotInt, StrOptions
 from ..utils._set_output import _get_output_config
 from ..utils.validation import _check_feature_names_in, check_is_fitted
 
 __all__ = ["OneHotEncoder", "OrdinalEncoder"]
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_function_transformer.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_function_transformer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,43 +1,28 @@
 import warnings
 
 import numpy as np
 
 from ..base import BaseEstimator, TransformerMixin, _fit_context
 from ..utils._param_validation import StrOptions
-from ..utils._set_output import ADAPTERS_MANAGER, _get_output_config
+from ..utils._set_output import (
+    _get_adapter_from_container,
+    _get_output_config,
+)
 from ..utils.metaestimators import available_if
 from ..utils.validation import (
     _allclose_dense_sparse,
     _check_feature_names_in,
     _get_feature_names,
     _is_pandas_df,
     _is_polars_df,
     check_array,
 )
 
 
-def _get_adapter_from_container(container):
-    """Get the adapter that nows how to handle such container.
-
-    See :class:`sklearn.utils._set_output.ContainerAdapterProtocol` for more
-    details.
-    """
-    module_name = container.__class__.__module__.split(".")[0]
-    try:
-        return ADAPTERS_MANAGER.adapters[module_name]
-    except KeyError as exc:
-        available_adapters = list(ADAPTERS_MANAGER.adapters.keys())
-        raise ValueError(
-            "The container does not have a registered adapter in scikit-learn. "
-            f"Available adapters are: {available_adapters} while the container "
-            f"provided is: {container!r}."
-        ) from exc
-
-
 def _identity(X):
     """The identity function."""
     return X
 
 
 class FunctionTransformer(TransformerMixin, BaseEstimator):
     """Constructs a transformer from an arbitrary callable.
@@ -404,15 +389,15 @@
         """Set output container.
 
         See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
         for an example on how to use the API.
 
         Parameters
         ----------
-        transform : {"default", "pandas"}, default=None
+        transform : {"default", "pandas", "polars"}, default=None
             Configure output of `transform` and `fit_transform`.
 
             - `"default"`: Default output format of a transformer
             - `"pandas"`: DataFrame output
             - `"polars"`: Polars output
             - `None`: Transform configuration is unchanged
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_label.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_label.py`

 * *Files 0% similar despite different names*

```diff
@@ -415,15 +415,15 @@
 
     def _more_tags(self):
         return {"X_types": ["1dlabels"]}
 
 
 @validate_params(
     {
-        "y": ["array-like"],
+        "y": ["array-like", "sparse matrix"],
         "classes": ["array-like"],
         "neg_label": [Interval(Integral, None, None, closed="neither")],
         "pos_label": [Interval(Integral, None, None, closed="neither")],
         "sparse_output": ["boolean"],
     },
     prefer_skip_nested_validation=True,
 )
@@ -436,15 +436,15 @@
     one-vs-all scheme.
 
     This function makes it possible to compute this transformation for a
     fixed set of class labels known ahead of time.
 
     Parameters
     ----------
-    y : array-like
+    y : array-like or sparse matrix
         Sequence of integer labels or multilabel data to encode.
 
     classes : array-like of shape (n_classes,)
         Uniquely holds the label for each class.
 
     neg_label : int, default=0
         Value with which negative labels must be encoded.
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_polynomial.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_polynomial.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 This file contains preprocessing tools based on polynomials.
 """
+
 import collections
 from itertools import chain, combinations
 from itertools import combinations_with_replacement as combinations_w_r
 from numbers import Integral
 
 import numpy as np
 from scipy import sparse
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_target_encoder.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_target_encoder.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/_target_encoder_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/_target_encoder_fast.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -1,48 +1,48 @@
 from libc.math cimport isnan
 from libcpp.vector cimport vector
 
-cimport numpy as cnp
+from ..utils._typedefs cimport float32_t, float64_t, int32_t, int64_t
+
 import numpy as np
 
-cnp.import_array()
 
 ctypedef fused INT_DTYPE:
-    cnp.int64_t
-    cnp.int32_t
+    int64_t
+    int32_t
 
 ctypedef fused Y_DTYPE:
-    cnp.int64_t
-    cnp.int32_t
-    cnp.float64_t
-    cnp.float32_t
+    int64_t
+    int32_t
+    float64_t
+    float32_t
 
 
 def _fit_encoding_fast(
     INT_DTYPE[:, ::1] X_int,
     const Y_DTYPE[:] y,
-    cnp.int64_t[::1] n_categories,
+    int64_t[::1] n_categories,
     double smooth,
     double y_mean,
 ):
     """Fit a target encoding on X_int and y.
 
     This implementation uses Eq 7 from [1] to compute the encoding.
     As stated in the paper, Eq 7 is the same as Eq 3.
 
     [1]: Micci-Barreca, Daniele. "A preprocessing scheme for high-cardinality
          categorical attributes in classification and prediction problems"
     """
     cdef:
-        cnp.int64_t sample_idx, feat_idx, cat_idx, n_cats
+        int64_t sample_idx, feat_idx, cat_idx, n_cats
         INT_DTYPE X_int_tmp
         int n_samples = X_int.shape[0]
         int n_features = X_int.shape[1]
         double smooth_sum = smooth * y_mean
-        cnp.int64_t max_n_cats = np.max(n_categories)
+        int64_t max_n_cats = np.max(n_categories)
         double[::1] sums = np.empty(max_n_cats, dtype=np.float64)
         double[::1] counts = np.empty(max_n_cats, dtype=np.float64)
         list encodings = []
         double[::1] current_encoding
         # Gives access to encodings without gil
         vector[double*] encoding_vec
 
@@ -76,34 +76,34 @@
 
     return encodings
 
 
 def _fit_encoding_fast_auto_smooth(
     INT_DTYPE[:, ::1] X_int,
     const Y_DTYPE[:] y,
-    cnp.int64_t[::1] n_categories,
+    int64_t[::1] n_categories,
     double y_mean,
     double y_variance,
 ):
     """Fit a target encoding on X_int and y with auto smoothing.
 
     This implementation uses Eq 5 and 6 from [1].
 
     [1]: Micci-Barreca, Daniele. "A preprocessing scheme for high-cardinality
          categorical attributes in classification and prediction problems"
     """
     cdef:
-        cnp.int64_t sample_idx, feat_idx, cat_idx, n_cats
+        int64_t sample_idx, feat_idx, cat_idx, n_cats
         INT_DTYPE X_int_tmp
         double diff
         int n_samples = X_int.shape[0]
         int n_features = X_int.shape[1]
-        cnp.int64_t max_n_cats = np.max(n_categories)
+        int64_t max_n_cats = np.max(n_categories)
         double[::1] means = np.empty(max_n_cats, dtype=np.float64)
-        cnp.int64_t[::1] counts = np.empty(max_n_cats, dtype=np.int64)
+        int64_t[::1] counts = np.empty(max_n_cats, dtype=np.int64)
         double[::1] sum_of_squared_diffs = np.empty(max_n_cats, dtype=np.float64)
         double lambda_
         list encodings = []
         double[::1] current_encoding
         # Gives access to encodings without gil
         vector[double*] encoding_vec
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_common.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_data.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_data.py`

 * *Files 1% similar despite different names*

```diff
@@ -1379,14 +1379,27 @@
         assert inf_norm < 1e-1
         inf_norm_arr.append(inf_norm)
     # each random subsampling yield a unique approximation to the expected
     # linspace CDF
     assert len(np.unique(inf_norm_arr)) == len(inf_norm_arr)
 
 
+def test_quantile_transform_subsampling_disabled():
+    """Check the behaviour of `QuantileTransformer` when `subsample=None`."""
+    X = np.random.RandomState(0).normal(size=(200, 1))
+
+    n_quantiles = 5
+    transformer = QuantileTransformer(n_quantiles=n_quantiles, subsample=None).fit(X)
+
+    expected_references = np.linspace(0, 1, n_quantiles)
+    assert_allclose(transformer.references_, expected_references)
+    expected_quantiles = np.quantile(X.ravel(), expected_references)
+    assert_allclose(transformer.quantiles_.ravel(), expected_quantiles)
+
+
 @pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
 def test_quantile_transform_sparse_toy(csc_container):
     X = np.array(
         [
             [0.0, 2.0, 0.0],
             [25.0, 4.0, 0.0],
             [50.0, 0.0, 2.6],
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_discretization.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_discretization.py`

 * *Files 9% similar despite different names*

```diff
@@ -45,16 +45,14 @@
         (
             "kmeans",
             [[0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2]],
             [1, 1, 1, 1],
         ),
     ],
 )
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 def test_fit_transform(strategy, expected, sample_weight):
     est = KBinsDiscretizer(n_bins=3, encode="ordinal", strategy=strategy)
     est.fit(X, sample_weight=sample_weight)
     assert_array_equal(expected, est.transform(X))
 
 
 def test_valid_n_bins():
@@ -145,16 +143,14 @@
         (
             "kmeans",
             [[0, 0, 0, 0], [0, 1, 1, 0], [1, 1, 1, 1], [1, 2, 2, 2]],
             [1, 0, 3, 1],
         ),
     ],
 )
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 def test_fit_transform_n_bins_array(strategy, expected, sample_weight):
     est = KBinsDiscretizer(
         n_bins=[2, 3, 3, 3], encode="ordinal", strategy=strategy
     ).fit(X, sample_weight=sample_weight)
     assert_array_equal(expected, est.transform(X))
 
     # test the shape of bin_edges_
@@ -172,16 +168,14 @@
     # will be used as bin edge
     est = KBinsDiscretizer(n_bins=10, encode="ordinal", strategy="quantile")
     est.fit(X, sample_weight=[1, 1, 1, 1, 0, 0])
     assert_allclose(est.bin_edges_[0], [-2, -1, 1, 3])
     assert_allclose(est.transform(X), [[0.0], [1.0], [2.0], [2.0], [2.0], [2.0]])
 
 
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 @pytest.mark.parametrize("strategy", ["kmeans", "quantile"])
 def test_kbinsdiscretizer_no_mutating_sample_weight(strategy):
     """Make sure that `sample_weight` is not changed in place."""
     est = KBinsDiscretizer(n_bins=3, encode="ordinal", strategy=strategy)
     sample_weight = np.array([1, 3, 1, 2], dtype=np.float64)
     sample_weight_copy = np.copy(sample_weight)
     est.fit(X, sample_weight=sample_weight)
@@ -254,16 +248,14 @@
     "strategy, expected_2bins, expected_3bins, expected_5bins",
     [
         ("uniform", [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2], [0, 0, 1, 1, 4, 4]),
         ("kmeans", [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 3, 4]),
         ("quantile", [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2], [0, 1, 2, 3, 4, 4]),
     ],
 )
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 def test_nonuniform_strategies(
     strategy, expected_2bins, expected_3bins, expected_5bins
 ):
     X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
 
     # with 2 bins
     est = KBinsDiscretizer(n_bins=2, strategy=strategy, encode="ordinal")
@@ -309,26 +301,22 @@
                 [-0.5, 3.0, -2.5, 0.0],
                 [0.5, 4.0, -1.5, 1.25],
                 [0.5, 4.0, -1.5, 1.25],
             ],
         ),
     ],
 )
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 @pytest.mark.parametrize("encode", ["ordinal", "onehot", "onehot-dense"])
 def test_inverse_transform(strategy, encode, expected_inv):
     kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode=encode)
     Xt = kbd.fit_transform(X)
     Xinv = kbd.inverse_transform(Xt)
     assert_array_almost_equal(expected_inv, Xinv)
 
 
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 @pytest.mark.parametrize("strategy", ["uniform", "kmeans", "quantile"])
 def test_transform_outside_fit_range(strategy):
     X = np.array([0, 1, 2, 3])[:, None]
     kbd = KBinsDiscretizer(n_bins=4, strategy=strategy, encode="ordinal")
     kbd.fit(X)
 
     X2 = np.array([-2, 5])[:, None]
@@ -488,16 +476,25 @@
     # We use a large tolerance because we can't expect the bin edges to be exactly the
     # same when subsampling is used.
     assert_allclose(
         kbd_subsampling.bin_edges_[0], kbd_no_subsampling.bin_edges_[0], rtol=1e-2
     )
 
 
-# TODO(1.5) remove this test
-@pytest.mark.parametrize("strategy", ["uniform", "kmeans"])
-def test_kbd_subsample_warning(strategy):
-    # Check the future warning for the change of default of subsample
-    X = np.random.RandomState(0).random_sample((100, 1))
+# TODO(1.7): remove this test
+def test_KBD_inverse_transform_Xt_deprecation():
+    X = np.arange(10)[:, None]
+    kbd = KBinsDiscretizer()
+    X = kbd.fit_transform(X)
+
+    with pytest.raises(TypeError, match="Missing required positional argument"):
+        kbd.inverse_transform()
+
+    with pytest.raises(TypeError, match="Cannot use both X and Xt. Use X only"):
+        kbd.inverse_transform(X=X, Xt=X)
+
+    with warnings.catch_warnings(record=True):
+        warnings.simplefilter("error")
+        kbd.inverse_transform(X)
 
-    kbd = KBinsDiscretizer(strategy=strategy, random_state=0)
-    with pytest.warns(FutureWarning, match="subsample=200_000 will be used by default"):
-        kbd.fit(X)
+    with pytest.warns(FutureWarning, match="Xt was renamed X in version 1.5"):
+        kbd.inverse_transform(Xt=X)
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_encoders.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_encoders.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 import numpy as np
 import pytest
 from scipy import sparse
 
 from sklearn.exceptions import NotFittedError
 from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
-from sklearn.utils import is_scalar_nan
+from sklearn.utils._missing import is_scalar_nan
 from sklearn.utils._testing import (
     _convert_container,
     assert_allclose,
     assert_array_equal,
 )
 from sklearn.utils.fixes import CSR_CONTAINERS
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_function_transformer.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_function_transformer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,34 +1,22 @@
 import warnings
 
 import numpy as np
 import pytest
 
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import FunctionTransformer, StandardScaler
-from sklearn.preprocessing._function_transformer import _get_adapter_from_container
 from sklearn.utils._testing import (
     _convert_container,
     assert_allclose_dense_sparse,
     assert_array_equal,
 )
 from sklearn.utils.fixes import CSC_CONTAINERS, CSR_CONTAINERS
 
 
-def test_get_adapter_from_container():
-    """Check the behavior fo `_get_adapter_from_container`."""
-    pd = pytest.importorskip("pandas")
-    X = pd.DataFrame({"a": [1, 2, 3], "b": [10, 20, 100]})
-    adapter = _get_adapter_from_container(X)
-    assert adapter.container_lib == "pandas"
-    err_msg = "The container does not have a registered adapter in scikit-learn."
-    with pytest.raises(ValueError, match=err_msg):
-        _get_adapter_from_container(X.to_numpy())
-
-
 def _make_func(args_store, kwargs_store, func=lambda X, *a, **k: X):
     def _func(X, *args, **kwargs):
         args_store.append(X)
         args_store.extend(args)
         kwargs_store.update(kwargs)
         return func(X)
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_label.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_label.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,24 +7,24 @@
     LabelBinarizer,
     LabelEncoder,
     MultiLabelBinarizer,
     _inverse_binarize_multiclass,
     _inverse_binarize_thresholding,
     label_binarize,
 )
-from sklearn.utils import _to_object_array
 from sklearn.utils._testing import assert_array_equal, ignore_warnings
 from sklearn.utils.fixes import (
     COO_CONTAINERS,
     CSC_CONTAINERS,
     CSR_CONTAINERS,
     DOK_CONTAINERS,
     LIL_CONTAINERS,
 )
 from sklearn.utils.multiclass import type_of_target
+from sklearn.utils.validation import _to_object_array
 
 iris = datasets.load_iris()
 
 
 def toarray(a):
     if hasattr(a, "toarray"):
         a = a.toarray()
```

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_polynomial.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_polynomial.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/preprocessing/tests/test_target_encoder.py` & `scikit_learn-1.5.0rc1/sklearn/preprocessing/tests/test_target_encoder.py`

 * *Files 1% similar despite different names*

```diff
@@ -582,16 +582,14 @@
     X_train_permuted_encoded = target_encoder.fit_transform(X_train_permuted, y_train)
     X_test_permuted_encoded = target_encoder.transform(X_test_permuted)
 
     assert_allclose(X_train_encoded, X_train_permuted_encoded)
     assert_allclose(X_test_encoded, X_test_permuted_encoded)
 
 
-# TODO(1.5) remove warning filter when kbd's subsample default is changed
-@pytest.mark.filterwarnings("ignore:In version 1.5 onwards, subsample=200_000")
 @pytest.mark.parametrize("smooth", [0.0, "auto"])
 def test_target_encoding_for_linear_regression(smooth, global_random_seed):
     # Check some expected statistical properties when fitting a linear
     # regression model on target encoded features depending on their relation
     # with that target.
 
     # In this test, we use the Ridge class with the "lsqr" solver and a little
```

### Comparing `scikit-learn-1.4.2/sklearn/random_projection.py` & `scikit_learn-1.5.0rc1/sklearn/random_projection.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,14 +18,15 @@
   into low-dimensional Euclidean space. The lemma states that a small set
   of points in a high-dimensional space can be embedded into a space of
   much lower dimension in such a way that distances between the points are
   nearly preserved. The map used for the embedding is at least Lipschitz,
   and can even be taken to be an orthogonal projection.
 
 """
+
 # Authors: Olivier Grisel <olivier.grisel@ensta.org>,
 #          Arnaud Joly <a.joly@ulg.ac.be>
 # License: BSD 3 clause
 
 import warnings
 from abc import ABCMeta, abstractmethod
 from numbers import Integral, Real
```

### Comparing `scikit-learn-1.4.2/sklearn/semi_supervised/_label_propagation.py` & `scikit_learn-1.5.0rc1/sklearn/semi_supervised/_label_propagation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/semi_supervised/_self_training.py` & `scikit_learn-1.5.0rc1/sklearn/semi_supervised/_self_training.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/semi_supervised/tests/test_label_propagation.py` & `scikit_learn-1.5.0rc1/sklearn/semi_supervised/tests/test_label_propagation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-""" test the label propagation module """
+"""test the label propagation module"""
 
 import warnings
 
 import numpy as np
 import pytest
 from scipy.sparse import issparse
```

### Comparing `scikit-learn-1.4.2/sklearn/semi_supervised/tests/test_self_training.py` & `scikit_learn-1.5.0rc1/sklearn/semi_supervised/tests/test_self_training.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/svm/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/_base.py` & `scikit_learn-1.5.0rc1/sklearn/svm/_base.py`

 * *Files 0% similar despite different names*

```diff
@@ -293,16 +293,15 @@
 
     def _warn_from_fit_status(self):
         assert self.fit_status_ in (0, 1)
         if self.fit_status_ == 1:
             warnings.warn(
                 "Solver terminated early (max_iter=%i)."
                 "  Consider pre-processing your data with"
-                " StandardScaler or MinMaxScaler."
-                % self.max_iter,
+                " StandardScaler or MinMaxScaler." % self.max_iter,
                 ConvergenceWarning,
             )
 
     def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):
         if callable(self.kernel):
             # you must store a reference to X to compute the kernel in predict
             # TODO: add keyword copy to copy on demand
@@ -1170,16 +1169,15 @@
         enc = LabelEncoder()
         y_ind = enc.fit_transform(y)
         classes_ = enc.classes_
         if len(classes_) < 2:
             raise ValueError(
                 "This solver needs samples of at least 2 classes"
                 " in the data, but the data contains only one"
-                " class: %r"
-                % classes_[0]
+                " class: %r" % classes_[0]
             )
 
         class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)
     else:
         class_weight_ = np.empty(0, dtype=np.float64)
         y_ind = y
     liblinear.set_verbosity_wrap(verbose)
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_bounds.py` & `scikit_learn-1.5.0rc1/sklearn/svm/_bounds.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Determination of parameter bounds"""
+
 # Author: Paolo Losi
 # License: BSD 3 clause
 
 from numbers import Real
 
 import numpy as np
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_classes.py` & `scikit_learn-1.5.0rc1/sklearn/svm/_classes.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,14 @@
-import warnings
 from numbers import Integral, Real
 
 import numpy as np
 
 from ..base import BaseEstimator, OutlierMixin, RegressorMixin, _fit_context
 from ..linear_model._base import LinearClassifierMixin, LinearModel, SparseCoefMixin
-from ..utils._param_validation import Hidden, Interval, StrOptions
+from ..utils._param_validation import Interval, StrOptions
 from ..utils.multiclass import check_classification_targets
 from ..utils.validation import _num_samples
 from ._base import BaseLibSVM, BaseSVC, _fit_liblinear, _get_liblinear_solver_type
 
 
 def _validate_dual_parameter(dual, loss, penalty, multi_class, X):
     """Helper function to assign the value of dual parameter."""
@@ -22,24 +21,14 @@
                 return False
         else:
             try:
                 _get_liblinear_solver_type(multi_class, penalty, loss, False)
                 return False
             except ValueError:  # primal not supported by the combination
                 return True
-    # TODO 1.5
-    elif dual == "warn":
-        warnings.warn(
-            (
-                "The default value of `dual` will change from `True` to `'auto'` in"
-                " 1.5. Set the value of `dual` explicitly to suppress the warning."
-            ),
-            FutureWarning,
-        )
-        return True
     else:
         return dual
 
 
 class LinearSVC(LinearClassifierMixin, SparseCoefMixin, BaseEstimator):
     """Linear Support Vector Classification.
 
@@ -66,15 +55,15 @@
 
     loss : {'hinge', 'squared_hinge'}, default='squared_hinge'
         Specifies the loss function. 'hinge' is the standard SVM loss
         (used e.g. by the SVC class) while 'squared_hinge' is the
         square of the hinge loss. The combination of ``penalty='l1'``
         and ``loss='hinge'`` is not supported.
 
-    dual : "auto" or bool, default=True
+    dual : "auto" or bool, default="auto"
         Select the algorithm to either solve the dual or primal
         optimization problem. Prefer dual=False when n_samples > n_features.
         `dual="auto"` will choose the value of the parameter automatically,
         based on the values of `n_samples`, `n_features`, `loss`, `multi_class`
         and `penalty`. If `n_samples` < `n_features` and optimizer supports
         chosen `loss`, `multi_class` and `penalty`, then dual will be set to True,
         otherwise it will be set to False.
@@ -85,14 +74,17 @@
 
     tol : float, default=1e-4
         Tolerance for stopping criteria.
 
     C : float, default=1.0
         Regularization parameter. The strength of the regularization is
         inversely proportional to C. Must be strictly positive.
+        For an intuitive visualization of the effects of scaling
+        the regularization parameter C, see
+        :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.
 
     multi_class : {'ovr', 'crammer_singer'}, default='ovr'
         Determines the multi-class strategy if `y` contains more than
         two classes.
         ``"ovr"`` trains n_classes one-vs-rest classifiers, while
         ``"crammer_singer"`` optimizes a joint objective over all classes.
         While `crammer_singer` is interesting from a theoretical perspective
@@ -220,32 +212,32 @@
     --------
     >>> from sklearn.svm import LinearSVC
     >>> from sklearn.pipeline import make_pipeline
     >>> from sklearn.preprocessing import StandardScaler
     >>> from sklearn.datasets import make_classification
     >>> X, y = make_classification(n_features=4, random_state=0)
     >>> clf = make_pipeline(StandardScaler(),
-    ...                     LinearSVC(dual="auto", random_state=0, tol=1e-5))
+    ...                     LinearSVC(random_state=0, tol=1e-5))
     >>> clf.fit(X, y)
     Pipeline(steps=[('standardscaler', StandardScaler()),
-                    ('linearsvc', LinearSVC(dual='auto', random_state=0, tol=1e-05))])
+                    ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])
 
     >>> print(clf.named_steps['linearsvc'].coef_)
     [[0.141...   0.526... 0.679... 0.493...]]
 
     >>> print(clf.named_steps['linearsvc'].intercept_)
     [0.1693...]
     >>> print(clf.predict([[0, 0, 0, 0]]))
     [1]
     """
 
     _parameter_constraints: dict = {
         "penalty": [StrOptions({"l1", "l2"})],
         "loss": [StrOptions({"hinge", "squared_hinge"})],
-        "dual": ["boolean", StrOptions({"auto"}), Hidden(StrOptions({"warn"}))],
+        "dual": ["boolean", StrOptions({"auto"})],
         "tol": [Interval(Real, 0.0, None, closed="neither")],
         "C": [Interval(Real, 0.0, None, closed="neither")],
         "multi_class": [StrOptions({"ovr", "crammer_singer"})],
         "fit_intercept": ["boolean"],
         "intercept_scaling": [Interval(Real, 0, None, closed="neither")],
         "class_weight": [None, dict, StrOptions({"balanced"})],
         "verbose": ["verbose"],
@@ -254,15 +246,15 @@
     }
 
     def __init__(
         self,
         penalty="l2",
         loss="squared_hinge",
         *,
-        dual="warn",
+        dual="auto",
         tol=1e-4,
         C=1.0,
         multi_class="ovr",
         fit_intercept=True,
         intercept_scaling=1,
         class_weight=None,
         verbose=0,
@@ -419,15 +411,15 @@
         `intercept_scaling`, the lower the impact of regularization on it.
         Then, the weights become `[w_x_1, ..., w_x_n,
         w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent
         the feature weights and the intercept weight is scaled by
         `intercept_scaling`. This scaling allows the intercept term to have a
         different regularization behavior compared to the other features.
 
-    dual : "auto" or bool, default=True
+    dual : "auto" or bool, default="auto"
         Select the algorithm to either solve the dual or primal
         optimization problem. Prefer dual=False when n_samples > n_features.
         `dual="auto"` will choose the value of the parameter automatically,
         based on the values of `n_samples`, `n_features` and `loss`. If
         `n_samples` < `n_features` and optimizer supports chosen `loss`,
         then dual will be set to True, otherwise it will be set to False.
 
@@ -494,18 +486,18 @@
     --------
     >>> from sklearn.svm import LinearSVR
     >>> from sklearn.pipeline import make_pipeline
     >>> from sklearn.preprocessing import StandardScaler
     >>> from sklearn.datasets import make_regression
     >>> X, y = make_regression(n_features=4, random_state=0)
     >>> regr = make_pipeline(StandardScaler(),
-    ...                      LinearSVR(dual="auto", random_state=0, tol=1e-5))
+    ...                      LinearSVR(random_state=0, tol=1e-5))
     >>> regr.fit(X, y)
     Pipeline(steps=[('standardscaler', StandardScaler()),
-                    ('linearsvr', LinearSVR(dual='auto', random_state=0, tol=1e-05))])
+                    ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])
 
     >>> print(regr.named_steps['linearsvr'].coef_)
     [18.582... 27.023... 44.357... 64.522...]
     >>> print(regr.named_steps['linearsvr'].intercept_)
     [-4...]
     >>> print(regr.predict([[0, 0, 0, 0]]))
     [-2.384...]
@@ -514,30 +506,30 @@
     _parameter_constraints: dict = {
         "epsilon": [Real],
         "tol": [Interval(Real, 0.0, None, closed="neither")],
         "C": [Interval(Real, 0.0, None, closed="neither")],
         "loss": [StrOptions({"epsilon_insensitive", "squared_epsilon_insensitive"})],
         "fit_intercept": ["boolean"],
         "intercept_scaling": [Interval(Real, 0, None, closed="neither")],
-        "dual": ["boolean", StrOptions({"auto"}), Hidden(StrOptions({"warn"}))],
+        "dual": ["boolean", StrOptions({"auto"})],
         "verbose": ["verbose"],
         "random_state": ["random_state"],
         "max_iter": [Interval(Integral, 0, None, closed="left")],
     }
 
     def __init__(
         self,
         *,
         epsilon=0.0,
         tol=1e-4,
         C=1.0,
         loss="epsilon_insensitive",
         fit_intercept=True,
         intercept_scaling=1.0,
-        dual="warn",
+        dual="auto",
         verbose=0,
         random_state=None,
         max_iter=1000,
     ):
         self.tol = tol
         self.C = C
         self.epsilon = epsilon
@@ -646,15 +638,17 @@
     Read more in the :ref:`User Guide <svm_classification>`.
 
     Parameters
     ----------
     C : float, default=1.0
         Regularization parameter. The strength of the regularization is
         inversely proportional to C. Must be strictly positive. The penalty
-        is a squared l2 penalty.
+        is a squared l2 penalty. For an intuitive visualization of the effects
+        of scaling the regularization parameter C, see
+        :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.
 
     kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \
         default='rbf'
         Specifies the kernel type to be used in the algorithm. If
         none is given, 'rbf' will be used. If a callable is given it is used to
         pre-compute the kernel matrix from data matrices; that matrix should be
         an array of shape ``(n_samples, n_samples)``. For an intuitive
@@ -1222,15 +1216,17 @@
 
     tol : float, default=1e-3
         Tolerance for stopping criterion.
 
     C : float, default=1.0
         Regularization parameter. The strength of the regularization is
         inversely proportional to C. Must be strictly positive.
-        The penalty is a squared l2 penalty.
+        The penalty is a squared l2. For an intuitive visualization of the
+        effects of scaling the regularization parameter C, see
+        :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.
 
     epsilon : float, default=0.1
          Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
          within which no penalty is associated in the training loss function
          with points predicted within a distance epsilon from the actual
          value. Must be non-negative.
 
@@ -1392,15 +1388,17 @@
     ----------
     nu : float, default=0.5
         An upper bound on the fraction of training errors and a lower bound of
         the fraction of support vectors. Should be in the interval (0, 1].  By
         default 0.5 will be taken.
 
     C : float, default=1.0
-        Penalty parameter C of the error term.
+        Penalty parameter C of the error term. For an intuitive visualization
+        of the effects of scaling the regularization parameter C, see
+        :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.
 
     kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \
         default='rbf'
          Specifies the kernel type to be used in the algorithm.
          If none is given, 'rbf' will be used. If a callable is given it is
          used to precompute the kernel matrix.
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_liblinear.pxi` & `scikit_learn-1.5.0rc1/sklearn/svm/_liblinear.pxi`

 * *Files 9% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+from ..utils._typedefs cimport intp_t
+
 cdef extern from "_cython_blas_helpers.h":
     ctypedef double (*dot_func)(int, const double*, int, const double*, int)
     ctypedef void (*axpy_func)(int, double, const double*, int, double*, int)
     ctypedef void (*scal_func)(int, double, const double*, int)
     ctypedef double (*nrm2_func)(int, const double*, int)
     cdef struct BlasFunctions:
         dot_func dot
@@ -29,13 +31,13 @@
 
 cdef extern from "liblinear_helper.c":
     void copy_w(void *, model *, int)
     parameter *set_parameter(int, double, double, int, char *, char *, int, int, double)
     problem *set_problem (char *, int, int, int, int, double, char *, char *)
     problem *csr_set_problem (char *, int, char *, char *, int, int, int, double, char *, char *)
 
-    model *set_model(parameter *, char *, cnp.npy_intp *, char *, double)
+    model *set_model(parameter *, char *, intp_t *, char *, double)
 
     double get_bias(model *)
     void free_problem (problem *)
     void free_parameter (parameter *)
     void set_verbosity(int)
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_liblinear.pyx` & `scikit_learn-1.5.0rc1/sklearn/svm/_liblinear.pyx`

 * *Files 10% similar despite different names*

```diff
@@ -1,48 +1,46 @@
 """
 Wrapper for liblinear
 
 Author: fabian.pedregosa@inria.fr
 """
 
 import  numpy as np
-cimport numpy as cnp
 
 from ..utils._cython_blas cimport _dot, _axpy, _scal, _nrm2
+from ..utils._typedefs cimport float32_t, float64_t, int32_t
 
 include "_liblinear.pxi"
 
-cnp.import_array()
-
 
 def train_wrap(
     object X,
-    const cnp.float64_t[::1] Y,
+    const float64_t[::1] Y,
     bint is_sparse,
     int solver_type,
     double eps,
     double bias,
     double C,
-    const cnp.float64_t[:] class_weight,
+    const float64_t[:] class_weight,
     int max_iter,
     unsigned random_seed,
     double epsilon,
-    const cnp.float64_t[::1] sample_weight
+    const float64_t[::1] sample_weight
 ):
     cdef parameter *param
     cdef problem *problem
     cdef model *model
     cdef char_const_ptr error_msg
     cdef int len_w
     cdef bint X_has_type_float64 = X.dtype == np.float64
     cdef char * X_data_bytes_ptr
-    cdef const cnp.float64_t[::1] X_data_64
-    cdef const cnp.float32_t[::1] X_data_32
-    cdef const cnp.int32_t[::1] X_indices
-    cdef const cnp.int32_t[::1] X_indptr
+    cdef const float64_t[::1] X_data_64
+    cdef const float32_t[::1] X_data_32
+    cdef const int32_t[::1] X_indices
+    cdef const int32_t[::1] X_indptr
 
     if is_sparse:
         X_indices = X.indices
         X_indptr = X.indptr
         if X_has_type_float64:
             X_data_64 = X.data
             X_data_bytes_ptr = <char *> &X_data_64[0]
@@ -51,17 +49,17 @@
             X_data_bytes_ptr = <char *> &X_data_32[0]
 
         problem = csr_set_problem(
             X_data_bytes_ptr,
             X_has_type_float64,
             <char *> &X_indices[0],
             <char *> &X_indptr[0],
-            (<cnp.int32_t>X.shape[0]),
-            (<cnp.int32_t>X.shape[1]),
-            (<cnp.int32_t>X.nnz),
+            (<int32_t>X.shape[0]),
+            (<int32_t>X.shape[1]),
+            (<int32_t>X.nnz),
             bias,
             <char *> &sample_weight[0],
             <char *> &Y[0]
         )
     else:
         X_as_1d_array = X.reshape(-1)
         if X_has_type_float64:
@@ -70,23 +68,23 @@
         else:
             X_data_32 = X_as_1d_array
             X_data_bytes_ptr = <char *> &X_data_32[0]
 
         problem = set_problem(
             X_data_bytes_ptr,
             X_has_type_float64,
-            (<cnp.int32_t>X.shape[0]),
-            (<cnp.int32_t>X.shape[1]),
-            (<cnp.int32_t>np.count_nonzero(X)),
+            (<int32_t>X.shape[0]),
+            (<int32_t>X.shape[1]),
+            (<int32_t>np.count_nonzero(X)),
             bias,
             <char *> &sample_weight[0],
             <char *> &Y[0]
         )
 
-    cdef cnp.int32_t[::1] class_weight_label = np.arange(class_weight.shape[0], dtype=np.intc)
+    cdef int32_t[::1] class_weight_label = np.arange(class_weight.shape[0], dtype=np.intc)
     param = set_parameter(
         solver_type,
         eps,
         C,
         class_weight.shape[0],
         <char *> &class_weight_label[0] if class_weight_label.size > 0 else NULL,
         <char *> &class_weight[0] if class_weight.size > 0 else NULL,
@@ -113,21 +111,21 @@
 
     # FREE
     free_problem(problem)
     free_parameter(param)
     # destroy_param(param)  don't call this or it will destroy class_weight_label and class_weight
 
     # coef matrix holder created as fortran since that's what's used in liblinear
-    cdef cnp.float64_t[::1, :] w
+    cdef float64_t[::1, :] w
     cdef int nr_class = get_nr_class(model)
 
     cdef int labels_ = nr_class
     if nr_class == 2:
         labels_ = 1
-    cdef cnp.int32_t[::1] n_iter = np.zeros(labels_, dtype=np.intc)
+    cdef int32_t[::1] n_iter = np.zeros(labels_, dtype=np.intc)
     get_n_iter(model, <int *> &n_iter[0])
 
     cdef int nr_feature = get_nr_feature(model)
     if bias > 0:
         nr_feature = nr_feature + 1
     if nr_class == 2 and solver_type != 4:  # solver is not Crammer-Singer
         w = np.empty((1, nr_feature), order='F')
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_libsvm.pxi` & `scikit_learn-1.5.0rc1/sklearn/svm/_libsvm.pxi`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 ################################################################################
 # Includes
+from ..utils._typedefs cimport intp_t
+
 cdef extern from "_svm_cython_blas_helpers.h":
     ctypedef double (*dot_func)(int, const double*, int, const double*, int)
     cdef struct BlasFunctions:
         dot_func dot
 
 
 cdef extern from "svm.h":
@@ -40,34 +42,34 @@
     svm_model *svm_train(svm_problem *, svm_parameter *, int *, BlasFunctions *) nogil
     void svm_free_and_destroy_model(svm_model** model_ptr_ptr)
     void svm_cross_validation(svm_problem *, svm_parameter *, int nr_fold, double *target, BlasFunctions *) nogil
 
 
 cdef extern from "libsvm_helper.c":
     # this file contains methods for accessing libsvm 'hidden' fields
-    svm_node **dense_to_sparse (char *, cnp.npy_intp *)
+    svm_node **dense_to_sparse (char *, intp_t *)
     void set_parameter (svm_parameter *, int , int , int , double, double ,
                         double , double , double , double,
                         double, int, int, int, char *, char *, int,
                         int)
-    void set_problem (svm_problem *, char *, char *, char *, cnp.npy_intp *, int)
+    void set_problem (svm_problem *, char *, char *, char *, intp_t *, int)
 
-    svm_model *set_model (svm_parameter *, int, char *, cnp.npy_intp *,
-                          char *, cnp.npy_intp *, cnp.npy_intp *, char *,
+    svm_model *set_model (svm_parameter *, int, char *, intp_t *,
+                          char *, intp_t *, intp_t *, char *,
                           char *, char *, char *, char *)
 
     void copy_sv_coef   (char *, svm_model *)
     void copy_n_iter  (char *, svm_model *)
-    void copy_intercept (char *, svm_model *, cnp.npy_intp *)
-    void copy_SV        (char *, svm_model *, cnp.npy_intp *)
+    void copy_intercept (char *, svm_model *, intp_t *)
+    void copy_SV        (char *, svm_model *, intp_t *)
     int copy_support (char *data, svm_model *model)
-    int copy_predict (char *, svm_model *, cnp.npy_intp *, char *, BlasFunctions *) nogil
-    int copy_predict_proba (char *, svm_model *, cnp.npy_intp *, char *, BlasFunctions *) nogil
-    int copy_predict_values(char *, svm_model *, cnp.npy_intp *, char *, int, BlasFunctions *) nogil
+    int copy_predict (char *, svm_model *, intp_t *, char *, BlasFunctions *) nogil
+    int copy_predict_proba (char *, svm_model *, intp_t *, char *, BlasFunctions *) nogil
+    int copy_predict_values(char *, svm_model *, intp_t *, char *, int, BlasFunctions *) nogil
     void copy_nSV     (char *, svm_model *)
-    void copy_probA   (char *, svm_model *, cnp.npy_intp *)
-    void copy_probB   (char *, svm_model *, cnp.npy_intp *)
-    cnp.npy_intp  get_l  (svm_model *)
-    cnp.npy_intp  get_nr (svm_model *)
+    void copy_probA   (char *, svm_model *, intp_t *)
+    void copy_probB   (char *, svm_model *, intp_t *)
+    intp_t  get_l  (svm_model *)
+    intp_t  get_nr (svm_model *)
     int  free_problem   (svm_problem *)
     int  free_model     (svm_model *)
     void set_verbosity(int)
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_libsvm.pyx` & `scikit_learn-1.5.0rc1/sklearn/svm/_libsvm.pyx`

 * *Files 5% similar despite different names*

```diff
@@ -24,49 +24,47 @@
 Authors
 -------
 2010: Fabian Pedregosa <fabian.pedregosa@inria.fr>
       Gael Varoquaux <gael.varoquaux@normalesup.org>
 """
 
 import  numpy as np
-cimport numpy as cnp
 from libc.stdlib cimport free
 from ..utils._cython_blas cimport _dot
+from ..utils._typedefs cimport float64_t, int32_t, intp_t
 
 include "_libsvm.pxi"
 
 cdef extern from *:
     ctypedef struct svm_parameter:
         pass
 
-cnp.import_array()
-
 
 ################################################################################
 # Internal variables
 LIBSVM_KERNEL_TYPES = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']
 
 
 ################################################################################
 # Wrapper functions
 
 def fit(
-    const cnp.float64_t[:, ::1] X,
-    const cnp.float64_t[::1] Y,
+    const float64_t[:, ::1] X,
+    const float64_t[::1] Y,
     int svm_type=0,
     kernel='rbf',
     int degree=3,
     double gamma=0.1,
     double coef0=0.0,
     double tol=1e-3,
     double C=1.0,
     double nu=0.5,
     double epsilon=0.1,
-    const cnp.float64_t[::1] class_weight=np.empty(0),
-    const cnp.float64_t[::1] sample_weight=np.empty(0),
+    const float64_t[::1] class_weight=np.empty(0),
+    const float64_t[::1] sample_weight=np.empty(0),
     int shrinking=1,
     int probability=0,
     double cache_size=100.,
     int max_iter=-1,
     int random_seed=0,
 ):
     """
@@ -162,15 +160,15 @@
         Number of iterations run by the optimization routine to fit the model.
     """
 
     cdef svm_parameter param
     cdef svm_problem problem
     cdef svm_model *model
     cdef const char *error_msg
-    cdef cnp.npy_intp SV_len
+    cdef intp_t SV_len
 
     if len(sample_weight) == 0:
         sample_weight = np.ones(X.shape[0], dtype=np.float64)
     else:
         assert sample_weight.shape[0] == X.shape[0], (
             f"sample_weight and X have incompatible shapes: sample_weight has "
             f"{sample_weight.shape[0]} samples while X has {X.shape[0]}"
@@ -178,20 +176,20 @@
 
     kernel_index = LIBSVM_KERNEL_TYPES.index(kernel)
     set_problem(
         &problem,
         <char*> &X[0, 0],
         <char*> &Y[0],
         <char*> &sample_weight[0],
-        <cnp.npy_intp*> X.shape,
+        <intp_t*> X.shape,
         kernel_index,
     )
     if problem.x == NULL:
         raise MemoryError("Seems we've run out of memory")
-    cdef cnp.int32_t[::1] class_weight_label = np.arange(
+    cdef int32_t[::1] class_weight_label = np.arange(
         class_weight.shape[0], dtype=np.int32
     )
     set_parameter(
         &param,
         svm_type,
         kernel_index,
         degree,
@@ -227,58 +225,58 @@
     # svm_train
     SV_len = get_l(model)
     n_class = get_nr(model)
 
     cdef int[::1] n_iter = np.empty(max(1, n_class * (n_class - 1) // 2), dtype=np.intc)
     copy_n_iter(<char*> &n_iter[0], model)
 
-    cdef cnp.float64_t[:, ::1] sv_coef = np.empty((n_class-1, SV_len), dtype=np.float64)
+    cdef float64_t[:, ::1] sv_coef = np.empty((n_class-1, SV_len), dtype=np.float64)
     copy_sv_coef(<char*> &sv_coef[0, 0] if sv_coef.size > 0 else NULL, model)
 
     # the intercept is just model.rho but with sign changed
-    cdef cnp.float64_t[::1] intercept = np.empty(
+    cdef float64_t[::1] intercept = np.empty(
         int((n_class*(n_class-1))/2), dtype=np.float64
     )
-    copy_intercept(<char*> &intercept[0], model, <cnp.npy_intp*> intercept.shape)
+    copy_intercept(<char*> &intercept[0], model, <intp_t*> intercept.shape)
 
-    cdef cnp.int32_t[::1] support = np.empty(SV_len, dtype=np.int32)
+    cdef int32_t[::1] support = np.empty(SV_len, dtype=np.int32)
     copy_support(<char*> &support[0] if support.size > 0 else NULL, model)
 
     # copy model.SV
-    cdef cnp.float64_t[:, ::1] support_vectors
+    cdef float64_t[:, ::1] support_vectors
     if kernel_index == 4:
         # precomputed kernel
         support_vectors = np.empty((0, 0), dtype=np.float64)
     else:
         support_vectors = np.empty((SV_len, X.shape[1]), dtype=np.float64)
         copy_SV(
             <char*> &support_vectors[0, 0] if support_vectors.size > 0 else NULL,
             model,
-            <cnp.npy_intp*> support_vectors.shape,
+            <intp_t*> support_vectors.shape,
         )
 
-    cdef cnp.int32_t[::1] n_class_SV
+    cdef int32_t[::1] n_class_SV
     if svm_type == 0 or svm_type == 1:
         n_class_SV = np.empty(n_class, dtype=np.int32)
         copy_nSV(<char*> &n_class_SV[0] if n_class_SV.size > 0 else NULL, model)
     else:
         # OneClass and SVR are considered to have 2 classes
         n_class_SV = np.array([SV_len, SV_len], dtype=np.int32)
 
-    cdef cnp.float64_t[::1] probA
-    cdef cnp.float64_t[::1] probB
+    cdef float64_t[::1] probA
+    cdef float64_t[::1] probB
     if probability != 0:
         if svm_type < 2:  # SVC and NuSVC
             probA = np.empty(int(n_class*(n_class-1)/2), dtype=np.float64)
             probB = np.empty(int(n_class*(n_class-1)/2), dtype=np.float64)
-            copy_probB(<char*> &probB[0], model, <cnp.npy_intp*> probB.shape)
+            copy_probB(<char*> &probB[0], model, <intp_t*> probB.shape)
         else:
             probA = np.empty(1, dtype=np.float64)
             probB = np.empty(0, dtype=np.float64)
-        copy_probA(<char*> &probA[0], model, <cnp.npy_intp*> probA.shape)
+        copy_probA(<char*> &probA[0], model, <intp_t*> probA.shape)
     else:
         probA = np.empty(0, dtype=np.float64)
         probB = np.empty(0, dtype=np.float64)
 
     svm_free_and_destroy_model(&model)
     free(problem.x)
 
@@ -340,29 +338,29 @@
         weight,
         max_iter,
         random_seed,
     )
 
 
 def predict(
-    const cnp.float64_t[:, ::1] X,
-    const cnp.int32_t[::1] support,
-    const cnp.float64_t[:, ::1] SV,
-    const cnp.int32_t[::1] nSV,
-    const cnp.float64_t[:, ::1] sv_coef,
-    const cnp.float64_t[::1] intercept,
-    const cnp.float64_t[::1] probA=np.empty(0),
-    const cnp.float64_t[::1] probB=np.empty(0),
+    const float64_t[:, ::1] X,
+    const int32_t[::1] support,
+    const float64_t[:, ::1] SV,
+    const int32_t[::1] nSV,
+    const float64_t[:, ::1] sv_coef,
+    const float64_t[::1] intercept,
+    const float64_t[::1] probA=np.empty(0),
+    const float64_t[::1] probB=np.empty(0),
     int svm_type=0,
     kernel='rbf',
     int degree=3,
     double gamma=0.1,
     double coef0=0.0,
-    const cnp.float64_t[::1] class_weight=np.empty(0),
-    const cnp.float64_t[::1] sample_weight=np.empty(0),
+    const float64_t[::1] class_weight=np.empty(0),
+    const float64_t[::1] sample_weight=np.empty(0),
     double cache_size=100.0,
 ):
     """
     Predict target values of X given a model (low-level method)
 
     Parameters
     ----------
@@ -406,20 +404,20 @@
         Independent parameter in poly/sigmoid kernel.
 
     Returns
     -------
     dec_values : array
         Predicted values.
     """
-    cdef cnp.float64_t[::1] dec_values
+    cdef float64_t[::1] dec_values
     cdef svm_parameter param
     cdef svm_model *model
     cdef int rv
 
-    cdef cnp.int32_t[::1] class_weight_label = np.arange(
+    cdef int32_t[::1] class_weight_label = np.arange(
         class_weight.shape[0], dtype=np.int32
     )
 
     set_predict_params(
         &param,
         svm_type,
         kernel,
@@ -432,18 +430,18 @@
         <char*> &class_weight_label[0] if class_weight_label.size > 0 else NULL,
         <char*> &class_weight[0] if class_weight.size > 0 else NULL,
     )
     model = set_model(
         &param,
         <int> nSV.shape[0],
         <char*> &SV[0, 0] if SV.size > 0 else NULL,
-        <cnp.npy_intp*> SV.shape,
+        <intp_t*> SV.shape,
         <char*> &support[0] if support.size > 0 else NULL,
-        <cnp.npy_intp*> support.shape,
-        <cnp.npy_intp*> sv_coef.strides,
+        <intp_t*> support.shape,
+        <intp_t*> sv_coef.strides,
         <char*> &sv_coef[0, 0] if sv_coef.size > 0 else NULL,
         <char*> &intercept[0],
         <char*> &nSV[0],
         <char*> &probA[0] if probA.size > 0 else NULL,
         <char*> &probB[0] if probB.size > 0 else NULL,
     )
     cdef BlasFunctions blas_functions
@@ -451,42 +449,42 @@
     # TODO: use check_model
     try:
         dec_values = np.empty(X.shape[0])
         with nogil:
             rv = copy_predict(
                 <char*> &X[0, 0],
                 model,
-                <cnp.npy_intp*> X.shape,
+                <intp_t*> X.shape,
                 <char*> &dec_values[0],
                 &blas_functions,
             )
         if rv < 0:
             raise MemoryError("We've run out of memory")
     finally:
         free_model(model)
 
     return dec_values.base
 
 
 def predict_proba(
-    const cnp.float64_t[:, ::1] X,
-    const cnp.int32_t[::1] support,
-    const cnp.float64_t[:, ::1] SV,
-    const cnp.int32_t[::1] nSV,
-    cnp.float64_t[:, ::1] sv_coef,
-    cnp.float64_t[::1] intercept,
-    cnp.float64_t[::1] probA=np.empty(0),
-    cnp.float64_t[::1] probB=np.empty(0),
+    const float64_t[:, ::1] X,
+    const int32_t[::1] support,
+    const float64_t[:, ::1] SV,
+    const int32_t[::1] nSV,
+    float64_t[:, ::1] sv_coef,
+    float64_t[::1] intercept,
+    float64_t[::1] probA=np.empty(0),
+    float64_t[::1] probB=np.empty(0),
     int svm_type=0,
     kernel='rbf',
     int degree=3,
     double gamma=0.1,
     double coef0=0.0,
-    cnp.float64_t[::1] class_weight=np.empty(0),
-    cnp.float64_t[::1] sample_weight=np.empty(0),
+    float64_t[::1] class_weight=np.empty(0),
+    float64_t[::1] sample_weight=np.empty(0),
     double cache_size=100.0,
 ):
     """
     Predict probabilities
 
     svm_model stores all parameters needed to predict a given value.
 
@@ -540,18 +538,18 @@
         Independent parameter in poly/sigmoid kernel.
 
     Returns
     -------
     dec_values : array
         Predicted values.
     """
-    cdef cnp.float64_t[:, ::1] dec_values
+    cdef float64_t[:, ::1] dec_values
     cdef svm_parameter param
     cdef svm_model *model
-    cdef cnp.int32_t[::1] class_weight_label = np.arange(
+    cdef int32_t[::1] class_weight_label = np.arange(
         class_weight.shape[0], dtype=np.int32
     )
     cdef int rv
 
     set_predict_params(
         &param,
         svm_type,
@@ -565,62 +563,62 @@
         <char*> &class_weight_label[0] if class_weight_label.size > 0 else NULL,
         <char*> &class_weight[0] if class_weight.size > 0 else NULL,
     )
     model = set_model(
         &param,
         <int> nSV.shape[0],
         <char*> &SV[0, 0] if SV.size > 0 else NULL,
-        <cnp.npy_intp*> SV.shape,
+        <intp_t*> SV.shape,
         <char*> &support[0],
-        <cnp.npy_intp*> support.shape,
-        <cnp.npy_intp*> sv_coef.strides,
+        <intp_t*> support.shape,
+        <intp_t*> sv_coef.strides,
         <char*> &sv_coef[0, 0],
         <char*> &intercept[0],
         <char*> &nSV[0],
         <char*> &probA[0] if probA.size > 0 else NULL,
         <char*> &probB[0] if probB.size > 0 else NULL,
     )
 
-    cdef cnp.npy_intp n_class = get_nr(model)
+    cdef intp_t n_class = get_nr(model)
     cdef BlasFunctions blas_functions
     blas_functions.dot = _dot[double]
     try:
         dec_values = np.empty((X.shape[0], n_class), dtype=np.float64)
         with nogil:
             rv = copy_predict_proba(
                 <char*> &X[0, 0],
                 model,
-                <cnp.npy_intp*> X.shape,
+                <intp_t*> X.shape,
                 <char*> &dec_values[0, 0],
                 &blas_functions,
             )
         if rv < 0:
             raise MemoryError("We've run out of memory")
     finally:
         free_model(model)
 
     return dec_values.base
 
 
 def decision_function(
-    const cnp.float64_t[:, ::1] X,
-    const cnp.int32_t[::1] support,
-    const cnp.float64_t[:, ::1] SV,
-    const cnp.int32_t[::1] nSV,
-    const cnp.float64_t[:, ::1] sv_coef,
-    const cnp.float64_t[::1] intercept,
-    const cnp.float64_t[::1] probA=np.empty(0),
-    const cnp.float64_t[::1] probB=np.empty(0),
+    const float64_t[:, ::1] X,
+    const int32_t[::1] support,
+    const float64_t[:, ::1] SV,
+    const int32_t[::1] nSV,
+    const float64_t[:, ::1] sv_coef,
+    const float64_t[::1] intercept,
+    const float64_t[::1] probA=np.empty(0),
+    const float64_t[::1] probB=np.empty(0),
     int svm_type=0,
     kernel='rbf',
     int degree=3,
     double gamma=0.1,
     double coef0=0.0,
-    const cnp.float64_t[::1] class_weight=np.empty(0),
-    const cnp.float64_t[::1] sample_weight=np.empty(0),
+    const float64_t[::1] class_weight=np.empty(0),
+    const float64_t[::1] sample_weight=np.empty(0),
     double cache_size=100.0,
 ):
     """
     Predict margin (libsvm name for this is predict_values)
 
     We have to reconstruct model and parameters to make sure we stay
     in sync with the python object.
@@ -667,20 +665,20 @@
         Independent parameter in poly/sigmoid kernel. 0 by default.
 
     Returns
     -------
     dec_values : array
         Predicted values.
     """
-    cdef cnp.float64_t[:, ::1] dec_values
+    cdef float64_t[:, ::1] dec_values
     cdef svm_parameter param
     cdef svm_model *model
-    cdef cnp.npy_intp n_class
+    cdef intp_t n_class
 
-    cdef cnp.int32_t[::1] class_weight_label = np.arange(
+    cdef int32_t[::1] class_weight_label = np.arange(
         class_weight.shape[0], dtype=np.int32
     )
 
     cdef int rv
 
     set_predict_params(
         &param,
@@ -696,18 +694,18 @@
         <char*> &class_weight[0] if class_weight.size > 0 else NULL,
     )
 
     model = set_model(
         &param,
         <int> nSV.shape[0],
         <char*> &SV[0, 0] if SV.size > 0 else NULL,
-        <cnp.npy_intp*> SV.shape,
+        <intp_t*> SV.shape,
         <char*> &support[0],
-        <cnp.npy_intp*> support.shape,
-        <cnp.npy_intp*> sv_coef.strides,
+        <intp_t*> support.shape,
+        <intp_t*> sv_coef.strides,
         <char*> &sv_coef[0, 0],
         <char*> &intercept[0],
         <char*> &nSV[0],
         <char*> &probA[0] if probA.size > 0 else NULL,
         <char*> &probB[0] if probB.size > 0 else NULL,
     )
 
@@ -720,42 +718,42 @@
     blas_functions.dot = _dot[double]
     try:
         dec_values = np.empty((X.shape[0], n_class), dtype=np.float64)
         with nogil:
             rv = copy_predict_values(
                 <char*> &X[0, 0],
                 model,
-                <cnp.npy_intp*> X.shape,
+                <intp_t*> X.shape,
                 <char*> &dec_values[0, 0],
                 n_class,
                 &blas_functions,
             )
         if rv < 0:
             raise MemoryError("We've run out of memory")
     finally:
         free_model(model)
 
     return dec_values.base
 
 
 def cross_validation(
-    const cnp.float64_t[:, ::1] X,
-    const cnp.float64_t[::1] Y,
+    const float64_t[:, ::1] X,
+    const float64_t[::1] Y,
     int n_fold,
     int svm_type=0,
     kernel='rbf',
     int degree=3,
     double gamma=0.1,
     double coef0=0.0,
     double tol=1e-3,
     double C=1.0,
     double nu=0.5,
     double epsilon=0.1,
-    cnp.float64_t[::1] class_weight=np.empty(0),
-    cnp.float64_t[::1] sample_weight=np.empty(0),
+    float64_t[::1] class_weight=np.empty(0),
+    float64_t[::1] sample_weight=np.empty(0),
     int shrinking=0,
     int probability=0,
     double cache_size=100.0,
     int max_iter=-1,
     int random_seed=0,
 ):
     """
@@ -854,20 +852,20 @@
     # set problem
     kernel_index = LIBSVM_KERNEL_TYPES.index(kernel)
     set_problem(
         &problem,
         <char*> &X[0, 0],
         <char*> &Y[0],
         <char*> &sample_weight[0] if sample_weight.size > 0 else NULL,
-        <cnp.npy_intp*> X.shape,
+        <intp_t*> X.shape,
         kernel_index,
     )
     if problem.x == NULL:
         raise MemoryError("Seems we've run out of memory")
-    cdef cnp.int32_t[::1] class_weight_label = np.arange(
+    cdef int32_t[::1] class_weight_label = np.arange(
         class_weight.shape[0], dtype=np.int32
     )
 
     # set parameters
     set_parameter(
         &param,
         svm_type,
@@ -889,15 +887,15 @@
         random_seed,
     )
 
     error_msg = svm_check_parameter(&problem, &param)
     if error_msg:
         raise ValueError(error_msg)
 
-    cdef cnp.float64_t[::1] target
+    cdef float64_t[::1] target
     cdef BlasFunctions blas_functions
     blas_functions.dot = _dot[double]
     try:
         target = np.empty((X.shape[0]), dtype=np.float64)
         with nogil:
             svm_cross_validation(
                 &problem,
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/_libsvm_sparse.pyx` & `scikit_learn-1.5.0rc1/sklearn/svm/_libsvm_sparse.pyx`

 * *Files 18% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 import  numpy as np
-cimport numpy as cnp
 from scipy import sparse
 from ..utils._cython_blas cimport _dot
-cnp.import_array()
+from ..utils._typedefs cimport float64_t, int32_t, intp_t
 
 cdef extern from *:
     ctypedef char* const_char_p "const char*"
 
 ################################################################################
 # Includes
 
@@ -23,66 +22,66 @@
     char *svm_csr_check_parameter(svm_csr_problem *, svm_parameter *)
     svm_csr_model *svm_csr_train(svm_csr_problem *, svm_parameter *, int *, BlasFunctions *) nogil
     void svm_csr_free_and_destroy_model(svm_csr_model** model_ptr_ptr)
 
 cdef extern from "libsvm_sparse_helper.c":
     # this file contains methods for accessing libsvm 'hidden' fields
     svm_csr_problem * csr_set_problem (
-        char *, cnp.npy_intp *, char *, cnp.npy_intp *, char *, char *, char *, int)
+        char *, intp_t *, char *, intp_t *, char *, char *, char *, int)
     svm_csr_model *csr_set_model(svm_parameter *param, int nr_class,
-                                 char *SV_data, cnp.npy_intp *SV_indices_dims,
-                                 char *SV_indices, cnp.npy_intp *SV_intptr_dims,
+                                 char *SV_data, intp_t *SV_indices_dims,
+                                 char *SV_indices, intp_t *SV_intptr_dims,
                                  char *SV_intptr,
                                  char *sv_coef, char *rho, char *nSV,
                                  char *probA, char *probB)
     svm_parameter *set_parameter (int , int , int , double, double ,
                                   double , double , double , double,
                                   double, int, int, int, char *, char *, int,
                                   int)
     void copy_sv_coef   (char *, svm_csr_model *)
     void copy_n_iter  (char *, svm_csr_model *)
     void copy_support   (char *, svm_csr_model *)
-    void copy_intercept (char *, svm_csr_model *, cnp.npy_intp *)
-    int copy_predict (char *, svm_csr_model *, cnp.npy_intp *, char *, BlasFunctions *)
-    int csr_copy_predict_values (cnp.npy_intp *data_size, char *data, cnp.npy_intp *index_size,
-                                 char *index, cnp.npy_intp *intptr_size, char *size,
+    void copy_intercept (char *, svm_csr_model *, intp_t *)
+    int copy_predict (char *, svm_csr_model *, intp_t *, char *, BlasFunctions *)
+    int csr_copy_predict_values (intp_t *data_size, char *data, intp_t *index_size,
+                                 char *index, intp_t *intptr_size, char *size,
                                  svm_csr_model *model, char *dec_values, int nr_class, BlasFunctions *)
-    int csr_copy_predict (cnp.npy_intp *data_size, char *data, cnp.npy_intp *index_size,
-                          char *index, cnp.npy_intp *intptr_size, char *size,
+    int csr_copy_predict (intp_t *data_size, char *data, intp_t *index_size,
+                          char *index, intp_t *intptr_size, char *size,
                           svm_csr_model *model, char *dec_values, BlasFunctions *) nogil
-    int csr_copy_predict_proba (cnp.npy_intp *data_size, char *data, cnp.npy_intp *index_size,
-                                char *index, cnp.npy_intp *intptr_size, char *size,
+    int csr_copy_predict_proba (intp_t *data_size, char *data, intp_t *index_size,
+                                char *index, intp_t *intptr_size, char *size,
                                 svm_csr_model *model, char *dec_values, BlasFunctions *) nogil
 
-    int  copy_predict_values(char *, svm_csr_model *, cnp.npy_intp *, char *, int, BlasFunctions *)
-    int  csr_copy_SV (char *values, cnp.npy_intp *n_indices,
-                      char *indices, cnp.npy_intp *n_indptr, char *indptr,
+    int  copy_predict_values(char *, svm_csr_model *, intp_t *, char *, int, BlasFunctions *)
+    int  csr_copy_SV (char *values, intp_t *n_indices,
+                      char *indices, intp_t *n_indptr, char *indptr,
                       svm_csr_model *model, int n_features)
-    cnp.npy_intp get_nonzero_SV (svm_csr_model *)
+    intp_t get_nonzero_SV (svm_csr_model *)
     void copy_nSV     (char *, svm_csr_model *)
-    void copy_probA   (char *, svm_csr_model *, cnp.npy_intp *)
-    void copy_probB   (char *, svm_csr_model *, cnp.npy_intp *)
-    cnp.npy_intp  get_l  (svm_csr_model *)
-    cnp.npy_intp  get_nr (svm_csr_model *)
+    void copy_probA   (char *, svm_csr_model *, intp_t *)
+    void copy_probB   (char *, svm_csr_model *, intp_t *)
+    intp_t  get_l  (svm_csr_model *)
+    intp_t  get_nr (svm_csr_model *)
     int  free_problem   (svm_csr_problem *)
     int  free_model     (svm_csr_model *)
     int  free_param     (svm_parameter *)
     int free_model_SV(svm_csr_model *model)
     void set_verbosity(int)
 
 
 def libsvm_sparse_train (int n_features,
-                         const cnp.float64_t[::1] values,
-                         const cnp.int32_t[::1] indices,
-                         const cnp.int32_t[::1] indptr,
-                         const cnp.float64_t[::1] Y,
+                         const float64_t[::1] values,
+                         const int32_t[::1] indices,
+                         const int32_t[::1] indptr,
+                         const float64_t[::1] Y,
                          int svm_type, int kernel_type, int degree, double gamma,
                          double coef0, double eps, double C,
-                         const cnp.float64_t[::1] class_weight,
-                         const cnp.float64_t[::1] sample_weight,
+                         const float64_t[::1] class_weight,
+                         const float64_t[::1] sample_weight,
                          double nu, double cache_size, double p, int
                          shrinking, int probability, int max_iter,
                          int random_seed):
     """
     Wrap svm_train from libsvm using a scipy.sparse.csr matrix
 
     Work in progress.
@@ -121,24 +120,24 @@
     # we should never end up here with a precomputed kernel matrix,
     # as this is always dense.
     assert(kernel_type != 4)
 
     # set libsvm problem
     problem = csr_set_problem(
         <char *> &values[0],
-        <cnp.npy_intp *> indices.shape,
+        <intp_t *> indices.shape,
         <char *> &indices[0],
-        <cnp.npy_intp *> indptr.shape,
+        <intp_t *> indptr.shape,
         <char *> &indptr[0],
         <char *> &Y[0],
         <char *> &sample_weight[0],
         kernel_type,
     )
 
-    cdef cnp.int32_t[::1] \
+    cdef int32_t[::1] \
         class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
 
     # set parameters
     param = set_parameter(
         svm_type,
         kernel_type,
         degree,
@@ -168,79 +167,79 @@
     cdef BlasFunctions blas_functions
     blas_functions.dot = _dot[double]
     # call svm_train, this does the real work
     cdef int fit_status = 0
     with nogil:
         model = svm_csr_train(problem, param, &fit_status, &blas_functions)
 
-    cdef cnp.npy_intp SV_len = get_l(model)
-    cdef cnp.npy_intp n_class = get_nr(model)
+    cdef intp_t SV_len = get_l(model)
+    cdef intp_t n_class = get_nr(model)
 
     cdef int[::1] n_iter
     n_iter = np.empty(max(1, n_class * (n_class - 1) // 2), dtype=np.intc)
     copy_n_iter(<char *> &n_iter[0], model)
 
     # copy model.sv_coef
     # we create a new array instead of resizing, otherwise
     # it would not erase previous information
-    cdef cnp.float64_t[::1] sv_coef_data
+    cdef float64_t[::1] sv_coef_data
     sv_coef_data = np.empty((n_class-1)*SV_len, dtype=np.float64)
     copy_sv_coef (<char *> &sv_coef_data[0] if sv_coef_data.size > 0 else NULL, model)
 
-    cdef cnp.int32_t[::1] support
+    cdef int32_t[::1] support
     support = np.empty(SV_len, dtype=np.int32)
     copy_support(<char *> &support[0] if support.size > 0 else NULL, model)
 
     # copy model.rho into the intercept
     # the intercept is just model.rho but with sign changed
-    cdef cnp.float64_t[::1]intercept
+    cdef float64_t[::1]intercept
     intercept = np.empty(n_class*(n_class-1)//2, dtype=np.float64)
-    copy_intercept (<char *> &intercept[0], model, <cnp.npy_intp *> intercept.shape)
+    copy_intercept (<char *> &intercept[0], model, <intp_t *> intercept.shape)
 
     # copy model.SV
     # we erase any previous information in SV
     # TODO: custom kernel
-    cdef cnp.npy_intp nonzero_SV
+    cdef intp_t nonzero_SV
     nonzero_SV = get_nonzero_SV (model)
 
-    cdef cnp.float64_t[::1] SV_data
-    cdef cnp.int32_t[::1] SV_indices, SV_indptr
+    cdef float64_t[::1] SV_data
+    cdef int32_t[::1] SV_indices, SV_indptr
     SV_data = np.empty(nonzero_SV, dtype=np.float64)
     SV_indices = np.empty(nonzero_SV, dtype=np.int32)
-    SV_indptr = np.empty(<cnp.npy_intp>SV_len + 1, dtype=np.int32)
+    SV_indptr = np.empty(<intp_t>SV_len + 1, dtype=np.int32)
     csr_copy_SV(
         <char *> &SV_data[0] if SV_data.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indices.shape,
+        <intp_t *> SV_indices.shape,
         <char *> &SV_indices[0] if SV_indices.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indptr.shape,
+        <intp_t *> SV_indptr.shape,
         <char *> &SV_indptr[0] if SV_indptr.size > 0 else NULL,
         model,
         n_features,
     )
     support_vectors_ = sparse.csr_matrix(
         (SV_data, SV_indices, SV_indptr), (SV_len, n_features)
     )
 
     # copy model.nSV
     # TODO: do only in classification
-    cdef cnp.int32_t[::1]n_class_SV
+    cdef int32_t[::1]n_class_SV
     n_class_SV = np.empty(n_class, dtype=np.int32)
     copy_nSV(<char *> &n_class_SV[0], model)
 
     # # copy probabilities
-    cdef cnp.float64_t[::1] probA, probB
+    cdef float64_t[::1] probA, probB
     if probability != 0:
         if svm_type < 2:  # SVC and NuSVC
             probA = np.empty(n_class*(n_class-1)//2, dtype=np.float64)
             probB = np.empty(n_class*(n_class-1)//2, dtype=np.float64)
-            copy_probB(<char *> &probB[0], model, <cnp.npy_intp *> probB.shape)
+            copy_probB(<char *> &probB[0], model, <intp_t *> probB.shape)
         else:
             probA = np.empty(1, dtype=np.float64)
             probB = np.empty(0, dtype=np.float64)
-        copy_probA(<char *> &probA[0], model, <cnp.npy_intp *> probA.shape)
+        copy_probA(<char *> &probA[0], model, <intp_t *> probA.shape)
     else:
         probA = np.empty(0, dtype=np.float64)
         probB = np.empty(0, dtype=np.float64)
 
     svm_csr_free_and_destroy_model (&model)
     free_problem(problem)
     free_param(param)
@@ -254,31 +253,31 @@
         probA.base,
         probB.base,
         fit_status,
         n_iter.base,
     )
 
 
-def libsvm_sparse_predict (const cnp.float64_t[::1] T_data,
-                           const cnp.int32_t[::1] T_indices,
-                           const cnp.int32_t[::1] T_indptr,
-                           const cnp.float64_t[::1] SV_data,
-                           const cnp.int32_t[::1] SV_indices,
-                           const cnp.int32_t[::1] SV_indptr,
-                           const cnp.float64_t[::1] sv_coef,
-                           const cnp.float64_t[::1]
+def libsvm_sparse_predict (const float64_t[::1] T_data,
+                           const int32_t[::1] T_indices,
+                           const int32_t[::1] T_indptr,
+                           const float64_t[::1] SV_data,
+                           const int32_t[::1] SV_indices,
+                           const int32_t[::1] SV_indptr,
+                           const float64_t[::1] sv_coef,
+                           const float64_t[::1]
                            intercept, int svm_type, int kernel_type, int
                            degree, double gamma, double coef0, double
                            eps, double C,
-                           const cnp.float64_t[:] class_weight,
+                           const float64_t[:] class_weight,
                            double nu, double p, int
                            shrinking, int probability,
-                           const cnp.int32_t[::1] nSV,
-                           const cnp.float64_t[::1] probA,
-                           const cnp.float64_t[::1] probB):
+                           const int32_t[::1] nSV,
+                           const float64_t[::1] probA,
+                           const float64_t[::1] probB):
     """
     Predict values T given a model.
 
     For speed, all real work is done at the C level in function
     copy_predict (libsvm_helper.c).
 
     We have to reconstruct model and parameters to make sure we stay
@@ -293,18 +292,18 @@
         target vector
 
     Returns
     -------
     dec_values : array
         predicted values.
     """
-    cdef cnp.float64_t[::1] dec_values
+    cdef float64_t[::1] dec_values
     cdef svm_parameter *param
     cdef svm_csr_model *model
-    cdef cnp.int32_t[::1] \
+    cdef int32_t[::1] \
         class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
     cdef int rv
     param = set_parameter(
         svm_type,
         kernel_type,
         degree,
         gamma,
@@ -322,35 +321,35 @@
         -1,
         -1,  # random seed has no effect on predict either
     )
 
     model = csr_set_model(
         param, <int> nSV.shape[0],
         <char *> &SV_data[0] if SV_data.size > 0 else NULL,
-        <cnp.npy_intp *>SV_indices.shape,
+        <intp_t *>SV_indices.shape,
         <char *> &SV_indices[0] if SV_indices.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indptr.shape,
+        <intp_t *> SV_indptr.shape,
         <char *> &SV_indptr[0] if SV_indptr.size > 0 else NULL,
         <char *> &sv_coef[0] if sv_coef.size > 0 else NULL,
         <char *> &intercept[0],
         <char *> &nSV[0],
         <char *> &probA[0] if probA.size > 0 else NULL,
         <char *> &probB[0] if probB.size > 0 else NULL,
     )
     # TODO: use check_model
     dec_values = np.empty(T_indptr.shape[0]-1)
     cdef BlasFunctions blas_functions
     blas_functions.dot = _dot[double]
     with nogil:
         rv = csr_copy_predict(
-            <cnp.npy_intp *> T_data.shape,
+            <intp_t *> T_data.shape,
             <char *> &T_data[0],
-            <cnp.npy_intp *> T_indices.shape,
+            <intp_t *> T_indices.shape,
             <char *> &T_indices[0],
-            <cnp.npy_intp *> T_indptr.shape,
+            <intp_t *> T_indptr.shape,
             <char *> &T_indptr[0],
             model,
             <char *> &dec_values[0],
             &blas_functions,
         )
     if rv < 0:
         raise MemoryError("We've run out of memory")
@@ -358,38 +357,38 @@
     free_model_SV(model)
     free_model(model)
     free_param(param)
     return dec_values.base
 
 
 def libsvm_sparse_predict_proba(
-    const cnp.float64_t[::1] T_data,
-    const cnp.int32_t[::1] T_indices,
-    const cnp.int32_t[::1] T_indptr,
-    const cnp.float64_t[::1] SV_data,
-    const cnp.int32_t[::1] SV_indices,
-    const cnp.int32_t[::1] SV_indptr,
-    const cnp.float64_t[::1] sv_coef,
-    const cnp.float64_t[::1]
+    const float64_t[::1] T_data,
+    const int32_t[::1] T_indices,
+    const int32_t[::1] T_indptr,
+    const float64_t[::1] SV_data,
+    const int32_t[::1] SV_indices,
+    const int32_t[::1] SV_indptr,
+    const float64_t[::1] sv_coef,
+    const float64_t[::1]
     intercept, int svm_type, int kernel_type, int
     degree, double gamma, double coef0, double
     eps, double C,
-    const cnp.float64_t[:] class_weight,
+    const float64_t[:] class_weight,
     double nu, double p, int shrinking, int probability,
-    const cnp.int32_t[::1] nSV,
-    const cnp.float64_t[::1] probA,
-    const cnp.float64_t[::1] probB,
+    const int32_t[::1] nSV,
+    const float64_t[::1] probA,
+    const float64_t[::1] probB,
 ):
     """
     Predict values T given a model.
     """
-    cdef cnp.float64_t[:, ::1] dec_values
+    cdef float64_t[:, ::1] dec_values
     cdef svm_parameter *param
     cdef svm_csr_model *model
-    cdef cnp.int32_t[::1] \
+    cdef int32_t[::1] \
         class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
     param = set_parameter(
         svm_type,
         kernel_type,
         degree,
         gamma,
         coef0,
@@ -407,37 +406,37 @@
         -1,  # random seed has no effect on predict either
     )
 
     model = csr_set_model(
         param,
         <int> nSV.shape[0],
         <char *> &SV_data[0] if SV_data.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indices.shape,
+        <intp_t *> SV_indices.shape,
         <char *> &SV_indices[0] if SV_indices.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indptr.shape,
+        <intp_t *> SV_indptr.shape,
         <char *> &SV_indptr[0] if SV_indptr.size > 0 else NULL,
         <char *> &sv_coef[0] if sv_coef.size > 0 else NULL,
         <char *> &intercept[0],
         <char *> &nSV[0],
         <char *> &probA[0] if probA.size > 0 else NULL,
         <char *> &probB[0] if probB.size > 0 else NULL,
     )
     # TODO: use check_model
-    cdef cnp.npy_intp n_class = get_nr(model)
+    cdef intp_t n_class = get_nr(model)
     cdef int rv
     dec_values = np.empty((T_indptr.shape[0]-1, n_class), dtype=np.float64)
     cdef BlasFunctions blas_functions
     blas_functions.dot = _dot[double]
     with nogil:
         rv = csr_copy_predict_proba(
-            <cnp.npy_intp *> T_data.shape,
+            <intp_t *> T_data.shape,
             <char *> &T_data[0],
-            <cnp.npy_intp *> T_indices.shape,
+            <intp_t *> T_indices.shape,
             <char *> &T_indices[0],
-            <cnp.npy_intp *> T_indptr.shape,
+            <intp_t *> T_indptr.shape,
             <char *> &T_indptr[0],
             model,
             <char *> &dec_values[0, 0],
             &blas_functions,
         )
     if rv < 0:
         raise MemoryError("We've run out of memory")
@@ -445,43 +444,43 @@
     free_model_SV(model)
     free_model(model)
     free_param(param)
     return dec_values.base
 
 
 def libsvm_sparse_decision_function(
-    const cnp.float64_t[::1] T_data,
-    const cnp.int32_t[::1] T_indices,
-    const cnp.int32_t[::1] T_indptr,
-    const cnp.float64_t[::1] SV_data,
-    const cnp.int32_t[::1] SV_indices,
-    const cnp.int32_t[::1] SV_indptr,
-    const cnp.float64_t[::1] sv_coef,
-    const cnp.float64_t[::1]
+    const float64_t[::1] T_data,
+    const int32_t[::1] T_indices,
+    const int32_t[::1] T_indptr,
+    const float64_t[::1] SV_data,
+    const int32_t[::1] SV_indices,
+    const int32_t[::1] SV_indptr,
+    const float64_t[::1] sv_coef,
+    const float64_t[::1]
     intercept, int svm_type, int kernel_type, int
     degree, double gamma, double coef0, double
     eps, double C,
-    const cnp.float64_t[:] class_weight,
+    const float64_t[:] class_weight,
     double nu, double p, int shrinking, int probability,
-    const cnp.int32_t[::1] nSV,
-    const cnp.float64_t[::1] probA,
-    const cnp.float64_t[::1] probB,
+    const int32_t[::1] nSV,
+    const float64_t[::1] probA,
+    const float64_t[::1] probB,
 ):
     """
     Predict margin (libsvm name for this is predict_values)
 
     We have to reconstruct model and parameters to make sure we stay
     in sync with the python object.
     """
-    cdef cnp.float64_t[:, ::1] dec_values
+    cdef float64_t[:, ::1] dec_values
     cdef svm_parameter *param
-    cdef cnp.npy_intp n_class
+    cdef intp_t n_class
 
     cdef svm_csr_model *model
-    cdef cnp.int32_t[::1] \
+    cdef int32_t[::1] \
         class_weight_label = np.arange(class_weight.shape[0], dtype=np.int32)
     param = set_parameter(
         svm_type,
         kernel_type,
         degree,
         gamma,
         coef0,
@@ -499,17 +498,17 @@
         -1,
     )
 
     model = csr_set_model(
         param,
         <int> nSV.shape[0],
         <char *> &SV_data[0] if SV_data.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indices.shape,
+        <intp_t *> SV_indices.shape,
         <char *> &SV_indices[0] if SV_indices.size > 0 else NULL,
-        <cnp.npy_intp *> SV_indptr.shape,
+        <intp_t *> SV_indptr.shape,
         <char *> &SV_indptr[0] if SV_indptr.size > 0 else NULL,
         <char *> &sv_coef[0] if sv_coef.size > 0 else NULL,
         <char *> &intercept[0],
         <char *> &nSV[0],
         <char *> &probA[0] if probA.size > 0 else NULL,
         <char *> &probB[0] if probB.size > 0 else NULL,
     )
@@ -520,19 +519,19 @@
         n_class = get_nr(model)
         n_class = n_class * (n_class - 1) // 2
 
     dec_values = np.empty((T_indptr.shape[0] - 1, n_class), dtype=np.float64)
     cdef BlasFunctions blas_functions
     blas_functions.dot = _dot[double]
     if csr_copy_predict_values(
-            <cnp.npy_intp *> T_data.shape,
+            <intp_t *> T_data.shape,
             <char *> &T_data[0],
-            <cnp.npy_intp *> T_indices.shape,
+            <intp_t *> T_indices.shape,
             <char *> &T_indices[0],
-            <cnp.npy_intp *> T_indptr.shape,
+            <intp_t *> T_indptr.shape,
             <char *> &T_indptr[0],
             model,
             <char *> &dec_values[0, 0],
             n_class,
             &blas_functions,
     ) < 0:
         raise MemoryError("We've run out of memory")
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/meson.build` & `scikit_learn-1.5.0rc1/sklearn/svm/meson.build`

 * *Files 9% similar despite different names*

```diff
@@ -1,57 +1,53 @@
 newrand_include = include_directories('src/newrand')
 libsvm_include = include_directories('src/libsvm')
 liblinear_include = include_directories('src/liblinear')
 
 _newrand = py.extension_module(
   '_newrand',
   '_newrand.pyx',
-  dependencies: [np_dep],
   override_options: ['cython_language=cpp'],
   include_directories: [newrand_include],
   cython_args: cython_args,
   subdir: 'sklearn/svm',
   install: true
 )
 
 libsvm_skl = static_library(
   'libsvm-skl',
   ['src/libsvm/libsvm_template.cpp'],
 )
 
 py.extension_module(
   '_libsvm',
-  ['_libsvm.pyx'],
-  dependencies: [np_dep],
+  ['_libsvm.pyx', utils_cython_tree],
   include_directories: [newrand_include, libsvm_include],
   link_with: libsvm_skl,
   cython_args: cython_args,
   subdir: 'sklearn/svm',
   install: true
 )
 
 py.extension_module(
   '_libsvm_sparse',
-  ['_libsvm_sparse.pyx'],
-  dependencies: [np_dep],
+  ['_libsvm_sparse.pyx', utils_cython_tree],
   include_directories: [newrand_include, libsvm_include],
   link_with: libsvm_skl,
   cython_args: cython_args,
   subdir: 'sklearn/svm',
   install: true
 )
 
 liblinear_skl = static_library(
   'liblinear-skl',
   ['src/liblinear/linear.cpp', 'src/liblinear/tron.cpp'],
 )
 
 py.extension_module(
   '_liblinear',
-  ['_liblinear.pyx'],
-  dependencies: [np_dep],
+  ['_liblinear.pyx', utils_cython_tree],
   include_directories: [newrand_include, liblinear_include],
   link_with: [liblinear_skl],
   cython_args: cython_args,
   subdir: 'sklearn/svm',
   install: true
 )
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/liblinear/COPYRIGHT` & `scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/COPYRIGHT`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/liblinear/liblinear_helper.c` & `scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/liblinear_helper.c`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 #include <stdlib.h>
-#include <numpy/arrayobject.h>
+#define PY_SSIZE_T_CLEAN
+#include <Python.h>
 #include "linear.h"
 
+
 /*
  * Convert matrix to sparse representation suitable for liblinear. x is
  * expected to be an array of length n_samples*n_features.
  *
  * Whether the matrix is densely or sparsely populated, the fastest way to
  * convert it to liblinear's sparse format is to calculate the amount of memory
  * needed and allocate a single big block.
@@ -136,15 +138,15 @@
     problem->n = n_features + (bias > 0);
     problem->y = (double *) Y;
     problem->W = (double *) sample_weight;
     problem->x = dense_to_sparse(X, double_precision_X, n_samples, n_features,
                         n_nonzero, bias);
     problem->bias = bias;
 
-    if (problem->x == NULL) { 
+    if (problem->x == NULL) {
         free(problem);
         return NULL;
     }
 
     return problem;
 }
 
@@ -170,16 +172,16 @@
 
     return problem;
 }
 
 
 /* Create a parameter struct with and return it */
 struct parameter *set_parameter(int solver_type, double eps, double C,
-                                npy_intp nr_weight, char *weight_label,
-                                char *weight, int max_iter, unsigned seed, 
+                                Py_ssize_t nr_weight, char *weight_label,
+                                char *weight, int max_iter, unsigned seed,
                                 double epsilon)
 {
     struct parameter *param = malloc(sizeof(struct parameter));
     if (param == NULL)
         return NULL;
 
     set_seed(seed);
@@ -192,15 +194,15 @@
     param->weight = (double *) weight;
     param->max_iter = max_iter;
     return param;
 }
 
 void copy_w(void *data, struct model *model, int len)
 {
-    memcpy(data, model->w, len * sizeof(double)); 
+    memcpy(data, model->w, len * sizeof(double));
 }
 
 double get_bias(struct model *model)
 {
     return model->bias;
 }
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/liblinear/linear.cpp` & `scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/linear.cpp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/liblinear/linear.h` & `scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/linear.h`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/liblinear/tron.cpp` & `scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/tron.cpp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/liblinear/tron.h` & `scikit_learn-1.5.0rc1/sklearn/svm/src/liblinear/tron.h`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/libsvm/LIBSVM_CHANGES` & `scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/LIBSVM_CHANGES`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/libsvm/libsvm_helper.c` & `scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/libsvm_helper.c`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 #include <stdlib.h>
-#include <numpy/arrayobject.h>
+#define PY_SSIZE_T_CLEAN
+#include <Python.h>
 #include "svm.h"
 #include "_svm_cython_blas_helpers.h"
 
 
 #ifndef MAX
     #define MAX(x, y) (((x) > (y)) ? (x) : (y))
 #endif
@@ -33,18 +34,18 @@
  * Special care must be taken with indinces, since libsvm indices start
  * at 1 and not at 0.
  *
  * Strictly speaking, the C standard does not require that structs are
  * contiguous, but in practice its a reasonable assumption.
  *
  */
-struct svm_node *dense_to_libsvm (double *x, npy_intp *dims)
+struct svm_node *dense_to_libsvm (double *x, Py_ssize_t *dims)
 {
     struct svm_node *node;
-    npy_intp len_row = dims[1];
+    Py_ssize_t len_row = dims[1];
     double *tx = x;
     int i;
 
     node = malloc (dims[0] * sizeof(struct svm_node));
 
     if (node == NULL) return NULL;
     for (i=0; i<dims[0]; ++i) {
@@ -85,15 +86,15 @@
     param->max_iter = max_iter;
     param->random_seed = random_seed;
 }
 
 /*
  * Fill an svm_problem struct. problem->x will be malloc'd.
  */
-void set_problem(struct svm_problem *problem, char *X, char *Y, char *sample_weight, npy_intp *dims, int kernel_type)
+void set_problem(struct svm_problem *problem, char *X, char *Y, char *sample_weight, Py_ssize_t *dims, int kernel_type)
 {
     if (problem == NULL) return;
     problem->l = (int) dims[0]; /* number of samples */
     problem->y = (double *) Y;
     problem->x = dense_to_libsvm((double *) X, dims); /* implicit call to malloc */
     problem->W = (double *) sample_weight;
 }
@@ -108,17 +109,17 @@
  * Possible issue: on 64 bits, the number of columns that numpy can
  * store is a long, but libsvm enforces this number (model->l) to be
  * an int, so we might have numpy matrices that do not fit into libsvm's
  * data structure.
  *
  */
 struct svm_model *set_model(struct svm_parameter *param, int nr_class,
-                            char *SV, npy_intp *SV_dims,
-                            char *support, npy_intp *support_dims,
-                            npy_intp *sv_coef_strides,
+                            char *SV, Py_ssize_t *SV_dims,
+                            char *support, Py_ssize_t *support_dims,
+                            Py_ssize_t *sv_coef_strides,
                             char *sv_coef, char *rho, char *nSV,
                             char *probA, char *probB)
 {
     struct svm_model *model;
     double *dsv_coef = (double *) sv_coef;
     int i, m;
 
@@ -210,26 +211,26 @@
 }
 
 
 
 /*
  * Get the number of support vectors in a model.
  */
-npy_intp get_l(struct svm_model *model)
+Py_ssize_t get_l(struct svm_model *model)
 {
-    return (npy_intp) model->l;
+    return (Py_ssize_t) model->l;
 }
 
 /*
  * Get the number of classes in a model, = 2 in regression/one class
  * svm.
  */
-npy_intp get_nr(struct svm_model *model)
+Py_ssize_t get_nr(struct svm_model *model)
 {
-    return (npy_intp) model->nr_class;
+    return (Py_ssize_t) model->nr_class;
 }
 
 /*
  * Get the number of iterations run in optimization
  */
 void copy_n_iter(char *data, struct svm_model *model)
 {
@@ -248,33 +249,33 @@
     double *temp = (double *) data;
     for(i=0; i<len; ++i) {
         memcpy(temp, model->sv_coef[i], sizeof(double) * model->l);
         temp += model->l;
     }
 }
 
-void copy_intercept(char *data, struct svm_model *model, npy_intp *dims)
+void copy_intercept(char *data, struct svm_model *model, Py_ssize_t *dims)
 {
     /* intercept = -rho */
-    npy_intp i, n = dims[0];
+    Py_ssize_t i, n = dims[0];
     double t, *ddata = (double *) data;
     for (i=0; i<n; ++i) {
         t = model->rho[i];
         /* we do this to avoid ugly -0.0 */
         *ddata = (t != 0) ? -t : 0;
         ++ddata;
     }
 }
 
 /*
  * This is a bit more complex since SV are stored as sparse
  * structures, so we have to do the conversion on the fly and also
  * iterate fast over data.
  */
-void copy_SV(char *data, struct svm_model *model, npy_intp *dims)
+void copy_SV(char *data, struct svm_model *model, Py_ssize_t *dims)
 {
     int i, n = model->l;
     double *tdata = (double *) data;
     int dim = model->SV[0].dim;
     for (i=0; i<n; ++i) {
         memcpy (tdata, model->SV[i].values, dim * sizeof(double));
         tdata += dim;
@@ -292,52 +293,52 @@
  */
 void copy_nSV(char *data, struct svm_model *model)
 {
     if (model->label == NULL) return;
     memcpy(data, model->nSV, model->nr_class * sizeof(int));
 }
 
-void copy_probA(char *data, struct svm_model *model, npy_intp * dims)
+void copy_probA(char *data, struct svm_model *model, Py_ssize_t * dims)
 {
     memcpy(data, model->probA, dims[0] * sizeof(double));
 }
 
-void copy_probB(char *data, struct svm_model *model, npy_intp * dims)
+void copy_probB(char *data, struct svm_model *model, Py_ssize_t * dims)
 {
     memcpy(data, model->probB, dims[0] * sizeof(double));
 }
 
 /*
  * Predict using model.
  *
  *  It will return -1 if we run out of memory.
  */
-int copy_predict(char *predict, struct svm_model *model, npy_intp *predict_dims,
+int copy_predict(char *predict, struct svm_model *model, Py_ssize_t *predict_dims,
                  char *dec_values, BlasFunctions *blas_functions)
 {
     double *t = (double *) dec_values;
     struct svm_node *predict_nodes;
-    npy_intp i;
+    Py_ssize_t i;
 
     predict_nodes = dense_to_libsvm((double *) predict, predict_dims);
 
     if (predict_nodes == NULL)
         return -1;
     for(i=0; i<predict_dims[0]; ++i) {
         *t = svm_predict(model, &predict_nodes[i], blas_functions);
         ++t;
     }
     free(predict_nodes);
     return 0;
 }
 
 int copy_predict_values(char *predict, struct svm_model *model,
-                        npy_intp *predict_dims, char *dec_values, int nr_class, BlasFunctions *blas_functions)
+                        Py_ssize_t *predict_dims, char *dec_values, int nr_class, BlasFunctions *blas_functions)
 {
-    npy_intp i;
+    Py_ssize_t i;
     struct svm_node *predict_nodes;
     predict_nodes = dense_to_libsvm((double *) predict, predict_dims);
     if (predict_nodes == NULL)
         return -1;
     for(i=0; i<predict_dims[0]; ++i) {
         svm_predict_values(model, &predict_nodes[i],
                                 ((double *) dec_values) + i*nr_class,
@@ -346,21 +347,21 @@
 
     free(predict_nodes);
     return 0;
 }
 
 
 
-int copy_predict_proba(char *predict, struct svm_model *model, npy_intp *predict_dims,
+int copy_predict_proba(char *predict, struct svm_model *model, Py_ssize_t *predict_dims,
                  char *dec_values, BlasFunctions *blas_functions)
 {
-    npy_intp i, n, m;
+    Py_ssize_t i, n, m;
     struct svm_node *predict_nodes;
     n = predict_dims[0];
-    m = (npy_intp) model->nr_class;
+    m = (Py_ssize_t) model->nr_class;
     predict_nodes = dense_to_libsvm((double *) predict, predict_dims);
     if (predict_nodes == NULL)
         return -1;
     for(i=0; i<n; ++i) {
         svm_predict_probability(model, &predict_nodes[i],
                                 ((double *) dec_values) + i*m,
 				blas_functions);
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/libsvm/svm.cpp` & `scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/svm.cpp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/libsvm/svm.h` & `scikit_learn-1.5.0rc1/sklearn/svm/src/libsvm/svm.h`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/src/newrand/newrand.h` & `scikit_learn-1.5.0rc1/sklearn/svm/src/newrand/newrand.h`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/tests/test_bounds.py` & `scikit_learn-1.5.0rc1/sklearn/svm/tests/test_bounds.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/svm/tests/test_sparse.py` & `scikit_learn-1.5.0rc1/sklearn/svm/tests/test_sparse.py`

 * *Files 2% similar despite different names*

```diff
@@ -238,16 +238,16 @@
     "lil_container, dok_container", zip(LIL_CONTAINERS, DOK_CONTAINERS)
 )
 def test_linearsvc(lil_container, dok_container):
     # Similar to test_SVC
     X_sp = lil_container(X)
     X2_sp = dok_container(X2)
 
-    clf = svm.LinearSVC(dual="auto", random_state=0).fit(X, Y)
-    sp_clf = svm.LinearSVC(dual="auto", random_state=0).fit(X_sp, Y)
+    clf = svm.LinearSVC(random_state=0).fit(X, Y)
+    sp_clf = svm.LinearSVC(random_state=0).fit(X_sp, Y)
 
     assert sp_clf.fit_intercept
 
     assert_array_almost_equal(clf.coef_, sp_clf.coef_, decimal=4)
     assert_array_almost_equal(clf.intercept_, sp_clf.intercept_, decimal=4)
 
     assert_allclose(clf.predict(X), sp_clf.predict(X_sp))
@@ -260,16 +260,16 @@
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_linearsvc_iris(csr_container):
     # Test the sparse LinearSVC with the iris dataset
     iris_data_sp = csr_container(iris.data)
 
-    sp_clf = svm.LinearSVC(dual="auto", random_state=0).fit(iris_data_sp, iris.target)
-    clf = svm.LinearSVC(dual="auto", random_state=0).fit(iris.data, iris.target)
+    sp_clf = svm.LinearSVC(random_state=0).fit(iris_data_sp, iris.target)
+    clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)
 
     assert clf.fit_intercept == sp_clf.fit_intercept
 
     assert_array_almost_equal(clf.coef_, sp_clf.coef_, decimal=1)
     assert_array_almost_equal(clf.intercept_, sp_clf.intercept_, decimal=1)
     assert_allclose(clf.predict(iris.data), sp_clf.predict(iris_data_sp))
 
@@ -291,15 +291,15 @@
     X_, y_ = make_classification(
         n_samples=200, n_features=100, weights=[0.833, 0.167], random_state=0
     )
 
     X_ = csr_container(X_)
     for clf in (
         linear_model.LogisticRegression(),
-        svm.LinearSVC(dual="auto", random_state=0),
+        svm.LinearSVC(random_state=0),
         svm.SVC(),
     ):
         clf.set_params(class_weight={0: 5})
         clf.fit(X_[:180], y_[:180])
         y_pred = clf.predict(X_[180:])
         assert np.sum(y_pred == y_[180:]) >= 11
```

### Comparing `scikit-learn-1.4.2/sklearn/svm/tests/test_svm.py` & `scikit_learn-1.5.0rc1/sklearn/svm/tests/test_svm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 """
 Testing for Support Vector Machine module (sklearn.svm)
 
 TODO: remove hard coded numerical results when possible
 """
-import re
 
 import numpy as np
 import pytest
 from numpy.testing import (
     assert_allclose,
     assert_almost_equal,
     assert_array_almost_equal,
@@ -222,31 +221,31 @@
     # Test Support Vector Regression
 
     diabetes = datasets.load_diabetes()
     for clf in (
         svm.NuSVR(kernel="linear", nu=0.4, C=1.0),
         svm.NuSVR(kernel="linear", nu=0.4, C=10.0),
         svm.SVR(kernel="linear", C=10.0),
-        svm.LinearSVR(dual="auto", C=10.0),
-        svm.LinearSVR(dual="auto", C=10.0),
+        svm.LinearSVR(C=10.0),
+        svm.LinearSVR(C=10.0),
     ):
         clf.fit(diabetes.data, diabetes.target)
         assert clf.score(diabetes.data, diabetes.target) > 0.02
 
     # non-regression test; previously, BaseLibSVM would check that
     # len(np.unique(y)) < 2, which must only be done for SVC
     svm.SVR().fit(diabetes.data, np.ones(len(diabetes.data)))
-    svm.LinearSVR(dual="auto").fit(diabetes.data, np.ones(len(diabetes.data)))
+    svm.LinearSVR().fit(diabetes.data, np.ones(len(diabetes.data)))
 
 
 def test_linearsvr():
     # check that SVR(kernel='linear') and LinearSVC() give
     # comparable results
     diabetes = datasets.load_diabetes()
-    lsvr = svm.LinearSVR(C=1e3, dual="auto").fit(diabetes.data, diabetes.target)
+    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)
     score1 = lsvr.score(diabetes.data, diabetes.target)
 
     svr = svm.SVR(kernel="linear", C=1e3).fit(diabetes.data, diabetes.target)
     score2 = svr.score(diabetes.data, diabetes.target)
 
     assert_allclose(np.linalg.norm(lsvr.coef_), np.linalg.norm(svr.coef_), 1, 0.0001)
     assert_almost_equal(score1, score2, 2)
@@ -255,45 +254,43 @@
 def test_linearsvr_fit_sampleweight():
     # check correct result when sample_weight is 1
     # check that SVR(kernel='linear') and LinearSVC() give
     # comparable results
     diabetes = datasets.load_diabetes()
     n_samples = len(diabetes.target)
     unit_weight = np.ones(n_samples)
-    lsvr = svm.LinearSVR(dual="auto", C=1e3, tol=1e-12, max_iter=10000).fit(
+    lsvr = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(
         diabetes.data, diabetes.target, sample_weight=unit_weight
     )
     score1 = lsvr.score(diabetes.data, diabetes.target)
 
-    lsvr_no_weight = svm.LinearSVR(dual="auto", C=1e3, tol=1e-12, max_iter=10000).fit(
+    lsvr_no_weight = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(
         diabetes.data, diabetes.target
     )
     score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)
 
     assert_allclose(
         np.linalg.norm(lsvr.coef_), np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001
     )
     assert_almost_equal(score1, score2, 2)
 
     # check that fit(X)  = fit([X1, X2, X3], sample_weight = [n1, n2, n3]) where
     # X = X1 repeated n1 times, X2 repeated n2 times and so forth
     random_state = check_random_state(0)
     random_weight = random_state.randint(0, 10, n_samples)
-    lsvr_unflat = svm.LinearSVR(dual="auto", C=1e3, tol=1e-12, max_iter=10000).fit(
+    lsvr_unflat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(
         diabetes.data, diabetes.target, sample_weight=random_weight
     )
     score3 = lsvr_unflat.score(
         diabetes.data, diabetes.target, sample_weight=random_weight
     )
 
     X_flat = np.repeat(diabetes.data, random_weight, axis=0)
     y_flat = np.repeat(diabetes.target, random_weight, axis=0)
-    lsvr_flat = svm.LinearSVR(dual="auto", C=1e3, tol=1e-12, max_iter=10000).fit(
-        X_flat, y_flat
-    )
+    lsvr_flat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(X_flat, y_flat)
     score4 = lsvr_flat.score(X_flat, y_flat)
 
     assert_almost_equal(score3, score4, 2)
 
 
 def test_svr_errors():
     X = [[0.0], [1.0]]
@@ -485,15 +482,15 @@
 
     X_, y_ = make_classification(
         n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2
     )
 
     for clf in (
         linear_model.LogisticRegression(),
-        svm.LinearSVC(dual="auto", random_state=0),
+        svm.LinearSVC(random_state=0),
         svm.SVC(),
     ):
         clf.set_params(class_weight={0: 0.1, 1: 10})
         clf.fit(X_[:100], y_[:100])
         y_pred = clf.predict(X_[100:])
         assert f1_score(y_[100:], y_pred) > 0.3
 
@@ -662,15 +659,15 @@
 
     classes = np.unique(y[unbalanced])
     class_weights = compute_class_weight("balanced", classes=classes, y=y[unbalanced])
     assert np.argmax(class_weights) == 2
 
     for clf in (
         svm.SVC(kernel="linear"),
-        svm.LinearSVC(dual="auto", random_state=0),
+        svm.LinearSVC(random_state=0),
         LogisticRegression(),
     ):
         # check that score is better when class='balanced' is set.
         y_pred = clf.fit(X[unbalanced], y[unbalanced]).predict(X)
         clf.set_params(class_weight="balanced")
         y_pred_balanced = clf.fit(
             X[unbalanced],
@@ -685,15 +682,15 @@
 def test_bad_input(lil_container):
     # Test dimensions for labels
     Y2 = Y[:-1]  # wrong dimensions for labels
     with pytest.raises(ValueError):
         svm.SVC().fit(X, Y2)
 
     # Test with arrays that are non-contiguous.
-    for clf in (svm.SVC(), svm.LinearSVC(dual="auto", random_state=0)):
+    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):
         Xf = np.asfortranarray(X)
         assert not Xf.flags["C_CONTIGUOUS"]
         yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)
         yf = yf[:, -1]
         assert not yf.flags["F_CONTIGUOUS"]
         assert not yf.flags["C_CONTIGUOUS"]
         clf.fit(Xf, yf)
@@ -785,15 +782,15 @@
             clf.fit(X, y)
     else:
         clf.fit(X, y)
 
 
 def test_linearsvc():
     # Test basic routines using LinearSVC
-    clf = svm.LinearSVC(dual="auto", random_state=0).fit(X, Y)
+    clf = svm.LinearSVC(random_state=0).fit(X, Y)
 
     # by default should have intercept
     assert clf.fit_intercept
 
     assert_array_equal(clf.predict(T), true_result)
     assert_array_almost_equal(clf.intercept_, [0], decimal=3)
 
@@ -816,16 +813,16 @@
     dec = clf.decision_function(T)
     res = (dec > 0).astype(int) + 1
     assert_array_equal(res, true_result)
 
 
 def test_linearsvc_crammer_singer():
     # Test LinearSVC with crammer_singer multi-class svm
-    ovr_clf = svm.LinearSVC(dual="auto", random_state=0).fit(iris.data, iris.target)
-    cs_clf = svm.LinearSVC(dual="auto", multi_class="crammer_singer", random_state=0)
+    ovr_clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)
+    cs_clf = svm.LinearSVC(multi_class="crammer_singer", random_state=0)
     cs_clf.fit(iris.data, iris.target)
 
     # similar prediction for ovr and crammer-singer:
     assert (ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > 0.9
 
     # classifiers shouldn't be the same
     assert (ovr_clf.coef_ != cs_clf.coef_).all()
@@ -839,68 +836,67 @@
     assert_array_almost_equal(dec_func, cs_clf.decision_function(iris.data))
 
 
 def test_linearsvc_fit_sampleweight():
     # check correct result when sample_weight is 1
     n_samples = len(X)
     unit_weight = np.ones(n_samples)
-    clf = svm.LinearSVC(dual="auto", random_state=0).fit(X, Y)
-    clf_unitweight = svm.LinearSVC(
-        dual="auto", random_state=0, tol=1e-12, max_iter=1000
-    ).fit(X, Y, sample_weight=unit_weight)
+    clf = svm.LinearSVC(random_state=0).fit(X, Y)
+    clf_unitweight = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(
+        X, Y, sample_weight=unit_weight
+    )
 
     # check if same as sample_weight=None
     assert_array_equal(clf_unitweight.predict(T), clf.predict(T))
     assert_allclose(clf.coef_, clf_unitweight.coef_, 1, 0.0001)
 
     # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where
     # X = X1 repeated n1 times, X2 repeated n2 times and so forth
 
     random_state = check_random_state(0)
     random_weight = random_state.randint(0, 10, n_samples)
-    lsvc_unflat = svm.LinearSVC(
-        dual="auto", random_state=0, tol=1e-12, max_iter=1000
-    ).fit(X, Y, sample_weight=random_weight)
+    lsvc_unflat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(
+        X, Y, sample_weight=random_weight
+    )
 
     pred1 = lsvc_unflat.predict(T)
 
     X_flat = np.repeat(X, random_weight, axis=0)
     y_flat = np.repeat(Y, random_weight, axis=0)
-    lsvc_flat = svm.LinearSVC(
-        dual="auto", random_state=0, tol=1e-12, max_iter=1000
-    ).fit(X_flat, y_flat)
+    lsvc_flat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(
+        X_flat, y_flat
+    )
     pred2 = lsvc_flat.predict(T)
 
     assert_array_equal(pred1, pred2)
     assert_allclose(lsvc_unflat.coef_, lsvc_flat.coef_, 1, 0.0001)
 
 
 def test_crammer_singer_binary():
     # Test Crammer-Singer formulation in the binary case
     X, y = make_classification(n_classes=2, random_state=0)
 
     for fit_intercept in (True, False):
         acc = (
             svm.LinearSVC(
-                dual="auto",
                 fit_intercept=fit_intercept,
                 multi_class="crammer_singer",
                 random_state=0,
             )
             .fit(X, y)
             .score(X, y)
         )
         assert acc > 0.9
 
 
 def test_linearsvc_iris():
     # Test that LinearSVC gives plausible predictions on the iris dataset
     # Also, test symbolic class names (classes_).
     target = iris.target_names[iris.target]
-    clf = svm.LinearSVC(dual="auto", random_state=0).fit(iris.data, target)
+    clf = svm.LinearSVC(random_state=0).fit(iris.data, target)
     assert set(clf.classes_) == set(iris.target_names)
     assert np.mean(clf.predict(iris.data) == target) > 0.8
 
     dec = clf.decision_function(iris.data)
     pred = iris.target_names[np.argmax(dec, 1)]
     assert_array_equal(pred, clf.predict(iris.data))
 
@@ -940,26 +936,26 @@
     clf.fit(X, y)
     intercept2 = clf.intercept_
     assert_array_almost_equal(intercept1, intercept2, decimal=2)
 
 
 def test_liblinear_set_coef():
     # multi-class case
-    clf = svm.LinearSVC(dual="auto").fit(iris.data, iris.target)
+    clf = svm.LinearSVC().fit(iris.data, iris.target)
     values = clf.decision_function(iris.data)
     clf.coef_ = clf.coef_.copy()
     clf.intercept_ = clf.intercept_.copy()
     values2 = clf.decision_function(iris.data)
     assert_array_almost_equal(values, values2)
 
     # binary-class case
     X = [[2, 1], [3, 1], [1, 3], [2, 3]]
     y = [0, 0, 1, 1]
 
-    clf = svm.LinearSVC(dual="auto").fit(X, y)
+    clf = svm.LinearSVC().fit(X, y)
     values = clf.decision_function(X)
     clf.coef_ = clf.coef_.copy()
     clf.intercept_ = clf.intercept_.copy()
     values2 = clf.decision_function(X)
     assert_array_equal(values, values2)
 
 
@@ -983,15 +979,15 @@
     # stdout: redirect
     import os
 
     stdout = os.dup(1)  # save original stdout
     os.dup2(os.pipe()[1], 1)  # replace it
 
     # actual call
-    clf = svm.LinearSVC(dual="auto", verbose=1)
+    clf = svm.LinearSVC(verbose=1)
     clf.fit(X, Y)
 
     # stdout: restore
     os.dup2(stdout, 1)  # restore original stdout
 
 
 def test_svc_clone_with_callable_kernel():
@@ -1067,24 +1063,24 @@
     proba_2 = a.fit(X, Y).predict_proba(X)
     assert_array_almost_equal(proba_1, proba_2)
 
 
 def test_linear_svm_convergence_warnings():
     # Test that warnings are raised if model does not converge
 
-    lsvc = svm.LinearSVC(dual="auto", random_state=0, max_iter=2)
+    lsvc = svm.LinearSVC(random_state=0, max_iter=2)
     warning_msg = "Liblinear failed to converge, increase the number of iterations."
     with pytest.warns(ConvergenceWarning, match=warning_msg):
         lsvc.fit(X, Y)
     # Check that we have an n_iter_ attribute with int type as opposed to a
     # numpy array or an np.int32 so as to match the docstring.
     assert isinstance(lsvc.n_iter_, int)
     assert lsvc.n_iter_ == 2
 
-    lsvr = svm.LinearSVR(dual="auto", random_state=0, max_iter=2)
+    lsvr = svm.LinearSVR(random_state=0, max_iter=2)
     with pytest.warns(ConvergenceWarning, match=warning_msg):
         lsvr.fit(iris.data, iris.target)
     assert isinstance(lsvr.n_iter_, int)
     assert lsvr.n_iter_ == 2
 
 
 def test_svr_coef_sign():
@@ -1092,26 +1088,26 @@
     # Non-regression test for #2933.
     X = np.random.RandomState(21).randn(10, 3)
     y = np.random.RandomState(12).randn(10)
 
     for svr in [
         svm.SVR(kernel="linear"),
         svm.NuSVR(kernel="linear"),
-        svm.LinearSVR(dual="auto"),
+        svm.LinearSVR(),
     ]:
         svr.fit(X, y)
         assert_array_almost_equal(
             svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_
         )
 
 
 def test_lsvc_intercept_scaling_zero():
     # Test that intercept_scaling is ignored when fit_intercept is False
 
-    lsvc = svm.LinearSVC(dual="auto", fit_intercept=False)
+    lsvc = svm.LinearSVC(fit_intercept=False)
     lsvc.fit(X, Y)
     assert lsvc.intercept_ == 0.0
 
 
 def test_hasattr_predict_proba():
     # Method must be (un)available before or after fit, switched by
     # `probability` param
@@ -1393,26 +1389,14 @@
     n_iter = estimator(kernel="linear").fit(X, y).n_iter_
     assert type(n_iter) == expected_n_iter_type
     if estimator in [svm.SVC, svm.NuSVC]:
         n_classes = len(np.unique(y))
         assert n_iter.shape == (n_classes * (n_classes - 1) // 2,)
 
 
-# TODO(1.5): Remove
-@pytest.mark.parametrize("Estimator", [LinearSVR, LinearSVC])
-def test_dual_auto_deprecation_warning(Estimator):
-    svm = Estimator()
-    msg = (
-        "The default value of `dual` will change from `True` to `'auto'` in"
-        " 1.5. Set the value of `dual` explicitly to suppress the warning."
-    )
-    with pytest.warns(FutureWarning, match=re.escape(msg)):
-        svm.fit(X, Y)
-
-
 @pytest.mark.parametrize("loss", ["squared_hinge", "squared_epsilon_insensitive"])
 def test_dual_auto(loss):
     # OvR, L2, N > M (6,2)
     dual = _validate_dual_parameter("auto", loss, "l2", "ovr", np.asarray(X))
     assert dual is False
     # OvR, L2, N < M (2,6)
     dual = _validate_dual_parameter("auto", loss, "l2", "ovr", np.asarray(X).T)
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/metadata_routing_common.py` & `scikit_learn-1.5.0rc1/sklearn/tests/metadata_routing_common.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from functools import partial
 
 import numpy as np
+from numpy.testing import assert_array_equal
 
 from sklearn.base import (
     BaseEstimator,
     ClassifierMixin,
     MetaEstimatorMixin,
     RegressorMixin,
     TransformerMixin,
@@ -14,14 +15,15 @@
 from sklearn.model_selection import BaseCrossValidator
 from sklearn.model_selection._split import GroupsConsumerMixin
 from sklearn.utils._metadata_requests import (
     SIMPLE_METHODS,
 )
 from sklearn.utils.metadata_routing import (
     MetadataRouter,
+    MethodMapping,
     process_routing,
 )
 from sklearn.utils.multiclass import _check_partial_fit_first_call
 
 
 def record_metadata(obj, method, record_default=True, **kwargs):
     """Utility function to store passed metadata to a method.
@@ -49,26 +51,33 @@
     ----------
     obj : estimator object
         sub-estimator to check routed params for
     method : str
         sub-estimator's method where metadata is routed to
     split_params : tuple, default=empty
         specifies any parameters which are to be checked as being a subset
-        of the original values.
+        of the original values
+    **kwargs : dict
+        passed metadata
     """
     records = getattr(obj, "_records", dict()).get(method, dict())
-    assert set(kwargs.keys()) == set(records.keys())
+    assert set(kwargs.keys()) == set(
+        records.keys()
+    ), f"Expected {kwargs.keys()} vs {records.keys()}"
     for key, value in kwargs.items():
         recorded_value = records[key]
         # The following condition is used to check for any specified parameters
         # being a subset of the original values
         if key in split_params and recorded_value is not None:
             assert np.isin(recorded_value, value).all()
         else:
-            assert recorded_value is value
+            if isinstance(recorded_value, np.ndarray):
+                assert_array_equal(recorded_value, value)
+            else:
+                assert recorded_value is value, f"Expected {recorded_value} vs {value}"
 
 
 record_metadata_not_default = partial(record_metadata, record_default=False)
 
 
 def assert_request_is_empty(metadata_request, exclude=None):
     """Check if a metadata request dict is empty.
@@ -151,22 +160,25 @@
             self.registry.append(self)
 
         record_metadata_not_default(
             self, "fit", sample_weight=sample_weight, metadata=metadata
         )
         return self
 
-    def predict(self, X, sample_weight="default", metadata="default"):
-        pass  # pragma: no cover
+    def predict(self, X, y=None, sample_weight="default", metadata="default"):
+        record_metadata_not_default(
+            self, "predict", sample_weight=sample_weight, metadata=metadata
+        )
+        return np.zeros(shape=(len(X),))
 
-        # when needed, uncomment the implementation
-        # record_metadata_not_default(
-        #     self, "predict", sample_weight=sample_weight, metadata=metadata
-        # )
-        # return np.zeros(shape=(len(X),))
+    def score(self, X, y, sample_weight="default", metadata="default"):
+        record_metadata_not_default(
+            self, "score", sample_weight=sample_weight, metadata=metadata
+        )
+        return 1
 
 
 class NonConsumingClassifier(ClassifierMixin, BaseEstimator):
     """A classifier which accepts no metadata on any method."""
 
     def __init__(self, alpha=0.0):
         self.alpha = alpha
@@ -233,14 +245,15 @@
     def fit(self, X, y, sample_weight="default", metadata="default"):
         if self.registry is not None:
             self.registry.append(self)
 
         record_metadata_not_default(
             self, "fit", sample_weight=sample_weight, metadata=metadata
         )
+
         self.classes_ = np.unique(y)
         return self
 
     def predict(self, X, sample_weight="default", metadata="default"):
         record_metadata_not_default(
             self, "predict", sample_weight=sample_weight, metadata=metadata
         )
@@ -266,14 +279,21 @@
 
     def decision_function(self, X, sample_weight="default", metadata="default"):
         record_metadata_not_default(
             self, "predict_proba", sample_weight=sample_weight, metadata=metadata
         )
         return np.zeros(shape=(len(X),))
 
+    # uncomment when needed
+    # def score(self, X, y, sample_weight="default", metadata="default"):
+    # record_metadata_not_default(
+    #    self, "score", sample_weight=sample_weight, metadata=metadata
+    # )
+    # return 1
+
 
 class ConsumingTransformer(TransformerMixin, BaseEstimator):
     """A transformer which accepts metadata on fit and transform.
 
     Parameters
     ----------
     registry : list, default=None
@@ -316,14 +336,37 @@
     def inverse_transform(self, X, sample_weight=None, metadata=None):
         record_metadata(
             self, "inverse_transform", sample_weight=sample_weight, metadata=metadata
         )
         return X
 
 
+class ConsumingNoFitTransformTransformer(BaseEstimator):
+    """A metadata consuming transformer that doesn't inherit from
+    TransformerMixin, and thus doesn't implement `fit_transform`. Note that
+    TransformerMixin's `fit_transform` doesn't route metadata to `transform`."""
+
+    def __init__(self, registry=None):
+        self.registry = registry
+
+    def fit(self, X, y=None, sample_weight=None, metadata=None):
+        if self.registry is not None:
+            self.registry.append(self)
+
+        record_metadata(self, "fit", sample_weight=sample_weight, metadata=metadata)
+
+        return self
+
+    def transform(self, X, sample_weight=None, metadata=None):
+        record_metadata(
+            self, "transform", sample_weight=sample_weight, metadata=metadata
+        )
+        return X
+
+
 class ConsumingScorer(_Scorer):
     def __init__(self, registry=None):
         super().__init__(
             score_func=mean_squared_error, sign=1, kwargs={}, response_method="predict"
         )
         self.registry = registry
 
@@ -333,15 +376,15 @@
 
         record_metadata_not_default(self, "score", **kwargs)
 
         sample_weight = kwargs.get("sample_weight", None)
         return super()._score(method_caller, clf, X, y, sample_weight=sample_weight)
 
 
-class ConsumingSplitter(BaseCrossValidator, GroupsConsumerMixin):
+class ConsumingSplitter(GroupsConsumerMixin, BaseCrossValidator):
     def __init__(self, registry=None):
         self.registry = registry
 
     def split(self, X, y=None, groups="default", metadata="default"):
         if self.registry is not None:
             self.registry.append(self)
 
@@ -372,15 +415,16 @@
 
     def fit(self, X, y, **fit_params):
         params = process_routing(self, "fit", **fit_params)
         self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)
 
     def get_metadata_routing(self):
         router = MetadataRouter(owner=self.__class__.__name__).add(
-            estimator=self.estimator, method_mapping="one-to-one"
+            estimator=self.estimator,
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
         return router
 
 
 class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):
     """A meta-regressor which is also a consumer."""
 
@@ -401,15 +445,20 @@
         params = process_routing(self, "predict", **predict_params)
         return self.estimator_.predict(X, **params.estimator.predict)
 
     def get_metadata_routing(self):
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
-            .add(estimator=self.estimator, method_mapping="one-to-one")
+            .add(
+                estimator=self.estimator,
+                method_mapping=MethodMapping()
+                .add(caller="fit", callee="fit")
+                .add(caller="predict", callee="predict"),
+            )
         )
         return router
 
 
 class WeightedMetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):
     """A meta-estimator which also consumes sample_weight itself in ``fit``."""
 
@@ -426,15 +475,18 @@
         self.estimator_ = clone(self.estimator).fit(X, y, **params.estimator.fit)
         return self
 
     def get_metadata_routing(self):
         router = (
             MetadataRouter(owner=self.__class__.__name__)
             .add_self_request(self)
-            .add(estimator=self.estimator, method_mapping="fit")
+            .add(
+                estimator=self.estimator,
+                method_mapping=MethodMapping().add(caller="fit", callee="fit"),
+            )
         )
         return router
 
 
 class MetaTransformer(MetaEstimatorMixin, TransformerMixin, BaseEstimator):
     """A simple meta-transformer."""
 
@@ -448,9 +500,12 @@
 
     def transform(self, X, y=None, **transform_params):
         params = process_routing(self, "transform", **transform_params)
         return self.transformer_.transform(X, **params.transformer.transform)
 
     def get_metadata_routing(self):
         return MetadataRouter(owner=self.__class__.__name__).add(
-            transformer=self.transformer, method_mapping="one-to-one"
+            transformer=self.transformer,
+            method_mapping=MethodMapping()
+            .add(caller="fit", callee="fit")
+            .add(caller="transform", callee="transform"),
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/random_seed.py` & `scikit_learn-1.5.0rc1/sklearn/tests/random_seed.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 to a specific seed value while still being deterministic by default.
 
 See the documentation for the SKLEARN_TESTS_GLOBAL_RANDOM_SEED
 variable for insrtuctions on how to use this fixture.
 
 https://scikit-learn.org/dev/computing/parallelism.html#sklearn-tests-global-random-seed
 """
+
 from os import environ
 from random import Random
 
 import pytest
 
 
 # Passes the main worker's random seeds to workers
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_base.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_base.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_build.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_build.py`

 * *Files 13% similar despite different names*

```diff
@@ -11,22 +11,24 @@
     # Check that sklearn is built with OpenMP-based parallelism enabled.
     # This test can be skipped by setting the environment variable
     # ``SKLEARN_SKIP_OPENMP_TEST``.
     if os.getenv("SKLEARN_SKIP_OPENMP_TEST"):
         pytest.skip("test explicitly skipped (SKLEARN_SKIP_OPENMP_TEST)")
 
     base_url = "dev" if __version__.endswith(".dev0") else "stable"
-    err_msg = textwrap.dedent("""
+    err_msg = textwrap.dedent(
+        """
         This test fails because scikit-learn has been built without OpenMP.
         This is not recommended since some estimators will run in sequential
         mode instead of leveraging thread-based parallelism.
 
         You can find instructions to build scikit-learn with OpenMP at this
         address:
 
             https://scikit-learn.org/{}/developers/advanced_installation.html
 
         You can skip this test by setting the environment variable
         SKLEARN_SKIP_OPENMP_TEST to any value.
-        """).format(base_url)
+        """
+    ).format(base_url)
 
     assert _openmp_parallelism_enabled(), err_msg
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_calibration.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_calibration.py`

 * *Files 1% similar despite different names*

```diff
@@ -152,15 +152,15 @@
     n_samples = N_SAMPLES // 2
     X, y = data
 
     sample_weight = np.random.RandomState(seed=42).uniform(size=len(y))
     X_train, y_train, sw_train = X[:n_samples], y[:n_samples], sample_weight[:n_samples]
     X_test = X[n_samples:]
 
-    estimator = LinearSVC(dual="auto", random_state=42)
+    estimator = LinearSVC(random_state=42)
     calibrated_clf = CalibratedClassifierCV(estimator, method=method, ensemble=ensemble)
     calibrated_clf.fit(X_train, y_train, sample_weight=sw_train)
     probs_with_sw = calibrated_clf.predict_proba(X_test)
 
     # As the weights are used for the calibration, they should still yield
     # different predictions
     calibrated_clf.fit(X_train, y_train)
@@ -173,15 +173,15 @@
 @pytest.mark.parametrize("method", ["sigmoid", "isotonic"])
 @pytest.mark.parametrize("ensemble", [True, False])
 def test_parallel_execution(data, method, ensemble):
     """Test parallel calibration"""
     X, y = data
     X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
 
-    estimator = make_pipeline(StandardScaler(), LinearSVC(dual="auto", random_state=42))
+    estimator = make_pipeline(StandardScaler(), LinearSVC(random_state=42))
 
     cal_clf_parallel = CalibratedClassifierCV(
         estimator, method=method, n_jobs=2, ensemble=ensemble
     )
     cal_clf_parallel.fit(X_train, y_train)
     probs_parallel = cal_clf_parallel.predict_proba(X_test)
 
@@ -202,15 +202,15 @@
 def test_calibration_multiclass(method, ensemble, seed):
     def multiclass_brier(y_true, proba_pred, n_classes):
         Y_onehot = np.eye(n_classes)[y_true]
         return np.sum((Y_onehot - proba_pred) ** 2) / Y_onehot.shape[0]
 
     # Test calibration for multiclass with classifier that implements
     # only decision function.
-    clf = LinearSVC(dual="auto", random_state=7)
+    clf = LinearSVC(random_state=7)
     X, y = make_blobs(
         n_samples=500, n_features=100, random_state=seed, centers=10, cluster_std=15.0
     )
 
     # Use an unbalanced dataset by collapsing 8 clusters into one class
     # to make the naive calibration based on a softmax more unlikely
     # to work.
@@ -334,15 +334,15 @@
 
 
 @pytest.mark.parametrize("method", ["sigmoid", "isotonic"])
 def test_calibration_ensemble_false(data, method):
     # Test that `ensemble=False` is the same as using predictions from
     # `cross_val_predict` to train calibrator.
     X, y = data
-    clf = LinearSVC(dual="auto", random_state=7)
+    clf = LinearSVC(random_state=7)
 
     cal_clf = CalibratedClassifierCV(clf, method=method, cv=3, ensemble=False)
     cal_clf.fit(X, y)
     cal_probas = cal_clf.predict_proba(X)
 
     # Get probas manually
     unbiased_preds = cross_val_predict(clf, X, y, cv=3, method="decision_function")
@@ -423,15 +423,15 @@
 
 @pytest.mark.parametrize("ensemble", [True, False])
 def test_calibration_prob_sum(ensemble):
     # Test that sum of probabilities is 1. A non-regression test for
     # issue #7796
     num_classes = 2
     X, y = make_classification(n_samples=10, n_features=5, n_classes=num_classes)
-    clf = LinearSVC(dual="auto", C=1.0, random_state=7)
+    clf = LinearSVC(C=1.0, random_state=7)
     clf_prob = CalibratedClassifierCV(
         clf, method="sigmoid", cv=LeaveOneOut(), ensemble=ensemble
     )
     clf_prob.fit(X, y)
 
     probs = clf_prob.predict_proba(X)
     assert_array_almost_equal(probs.sum(axis=1), np.ones(probs.shape[0]))
@@ -441,15 +441,15 @@
 def test_calibration_less_classes(ensemble):
     # Test to check calibration works fine when train set in a test-train
     # split does not contain all classes
     # Since this test uses LOO, at each iteration train set will not contain a
     # class label
     X = np.random.randn(10, 5)
     y = np.arange(10)
-    clf = LinearSVC(dual="auto", C=1.0, random_state=7)
+    clf = LinearSVC(C=1.0, random_state=7)
     cal_clf = CalibratedClassifierCV(
         clf, method="sigmoid", cv=LeaveOneOut(), ensemble=ensemble
     )
     cal_clf.fit(X, y)
 
     for i, calibrated_classifier in enumerate(cal_clf.calibrated_classifiers_):
         proba = calibrated_classifier.predict_proba(X)
@@ -538,16 +538,16 @@
     calib_clf.predict(X)
     calib_clf.predict_proba(X)
 
 
 @pytest.mark.parametrize(
     "clf, cv",
     [
-        pytest.param(LinearSVC(dual="auto", C=1), 2),
-        pytest.param(LinearSVC(dual="auto", C=1), "prefit"),
+        pytest.param(LinearSVC(C=1), 2),
+        pytest.param(LinearSVC(C=1), "prefit"),
     ],
 )
 def test_calibration_attributes(clf, cv):
     # Check that `n_features_in_` and `classes_` attributes created properly
     X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)
     if cv == "prefit":
         clf = clf.fit(X, y)
@@ -563,15 +563,15 @@
         assert calib_clf.n_features_in_ == X.shape[1]
 
 
 def test_calibration_inconsistent_prefit_n_features_in():
     # Check that `n_features_in_` from prefit base estimator
     # is consistent with training set
     X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=7)
-    clf = LinearSVC(dual="auto", C=1).fit(X, y)
+    clf = LinearSVC(C=1).fit(X, y)
     calib_clf = CalibratedClassifierCV(clf, cv="prefit")
 
     msg = "X has 3 features, but LinearSVC is expecting 5 features as input."
     with pytest.raises(ValueError, match=msg):
         calib_clf.fit(X[:, :3], y)
 
 
@@ -1084,7 +1084,18 @@
         def predict_proba(self, X):
             return super().predict_proba(X).astype(np.float32)
 
     model = DummyClassifer32()
     calibrator = CalibratedClassifierCV(model)
     # Does not raise an error
     calibrator.fit(*data)
+
+
+def test_error_less_class_samples_than_folds():
+    """Check that CalibratedClassifierCV works with string targets.
+
+    non-regression test for issue #28841.
+    """
+    X = np.random.normal(size=(20, 3))
+    y = ["a"] * 10 + ["b"] * 10
+
+    CalibratedClassifierCV(cv=3).fit(X, y)
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_common.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_common.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 from itertools import chain, product
 from pathlib import Path
 
 import numpy as np
 import pytest
 
 import sklearn
+from sklearn.base import BaseEstimator
 from sklearn.cluster import (
     OPTICS,
     AffinityPropagation,
     Birch,
     MeanShift,
     SpectralClustering,
 )
@@ -57,15 +58,15 @@
 from sklearn.preprocessing import (
     FunctionTransformer,
     MinMaxScaler,
     OneHotEncoder,
     StandardScaler,
 )
 from sklearn.semi_supervised import LabelPropagation, LabelSpreading
-from sklearn.utils import _IS_WASM, IS_PYPY, all_estimators
+from sklearn.utils import all_estimators
 from sklearn.utils._tags import _DEFAULT_TAGS, _safe_tags
 from sklearn.utils._testing import (
     SkipTest,
     ignore_warnings,
     set_random_state,
 )
 from sklearn.utils.estimator_checks import (
@@ -83,14 +84,15 @@
     check_set_output_transform,
     check_set_output_transform_pandas,
     check_set_output_transform_polars,
     check_transformer_get_feature_names_out,
     check_transformer_get_feature_names_out_pandas,
     parametrize_with_checks,
 )
+from sklearn.utils.fixes import _IS_PYPY, _IS_WASM
 
 
 def test_all_estimator_no_base_class():
     # test that all_estimators doesn't find abstract classes.
     for name, Estimator in all_estimators():
         msg = (
             "Base estimators such as {0} should not be included in all_estimators"
@@ -98,14 +100,24 @@
         assert not name.lower().startswith("base"), msg
 
 
 def _sample_func(x, y=1):
     pass
 
 
+class CallableEstimator(BaseEstimator):
+    """Dummy development stub for an estimator.
+
+    This is to make sure a callable estimator passes common tests.
+    """
+
+    def __call__(self):
+        pass  # pragma: nocover
+
+
 @pytest.mark.parametrize(
     "val, expected",
     [
         (partial(_sample_func, y=1), "_sample_func(y=1)"),
         (_sample_func, "_sample_func"),
         (partial(_sample_func, "world"), "_sample_func"),
         (LogisticRegression(C=2.0), "LogisticRegression(C=2.0)"),
@@ -117,14 +129,15 @@
                 warm_start=True,
             ),
             (
                 "LogisticRegression(class_weight='balanced',random_state=1,"
                 "solver='newton-cg',warm_start=True)"
             ),
         ),
+        (CallableEstimator(), "CallableEstimator()"),
     ],
 )
 def test_get_check_estimator_ids(val, expected):
     assert _get_check_estimator_ids(val) == expected
 
 
 def _tested_estimators(type_filter=None):
@@ -156,30 +169,26 @@
 
 
 def test_check_estimator_generate_only():
     all_instance_gen_checks = check_estimator(LogisticRegression(), generate_only=True)
     assert isgenerator(all_instance_gen_checks)
 
 
-def test_configure():
-    # Smoke test `python setup.py config` command run at the root of the
+def test_setup_py_check():
+    # Smoke test `python setup.py check` command run at the root of the
     # scikit-learn source tree.
-    # This test requires Cython which is not necessarily there when running
-    # the tests of an installed version of scikit-learn or when scikit-learn
-    # is installed in editable mode by pip build isolation enabled.
-    pytest.importorskip("Cython")
     cwd = os.getcwd()
     setup_path = Path(sklearn.__file__).parent.parent
     setup_filename = os.path.join(setup_path, "setup.py")
     if not os.path.exists(setup_filename):
         pytest.skip("setup.py not available")
     try:
         os.chdir(setup_path)
         old_argv = sys.argv
-        sys.argv = ["setup.py", "config"]
+        sys.argv = ["setup.py", "check"]
 
         with warnings.catch_warnings():
             # The configuration spits out warnings when not finding
             # Blas/Atlas development headers
             warnings.simplefilter("ignore", UserWarning)
             with open("setup.py") as f:
                 exec(f.read(), dict(__name__="__main__"))
@@ -221,15 +230,15 @@
     submods = [modname for _, modname, _ in pkgs]
     for modname in submods + ["sklearn"]:
         if ".tests." in modname:
             continue
         # Avoid test suite depending on setuptools
         if "sklearn._build_utils" in modname:
             continue
-        if IS_PYPY and (
+        if _IS_PYPY and (
             "_svmlight_format_io" in modname
             or "feature_extraction._hashing_fast" in modname
         ):
             continue
         package = __import__(modname, fromlist="dummy")
         for name in getattr(package, "__all__", ()):
             assert hasattr(package, name), "Module '{0}' has no attribute '{1}'".format(
@@ -255,19 +264,21 @@
         " https://github.com/mesonbuild/meson-python/issues/557 for more details"
     ),
 )
 def test_all_tests_are_importable():
     # Ensure that for each contentful subpackage, there is a test directory
     # within it that is also a subpackage (i.e. a directory with __init__.py)
 
-    HAS_TESTS_EXCEPTIONS = re.compile(r"""(?x)
+    HAS_TESTS_EXCEPTIONS = re.compile(
+        r"""(?x)
                                       \.externals(\.|$)|
                                       \.tests(\.|$)|
                                       \._
-                                      """)
+                                      """
+    )
     resource_modules = {
         "sklearn.datasets.data",
         "sklearn.datasets.descr",
         "sklearn.datasets.images",
     }
     sklearn_path = [os.path.dirname(sklearn.__file__)]
     lookup = {
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_config.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_config.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import time
 from concurrent.futures import ThreadPoolExecutor
 
 import pytest
 
 import sklearn
 from sklearn import config_context, get_config, set_config
-from sklearn.utils import _IS_WASM
+from sklearn.utils.fixes import _IS_WASM
 from sklearn.utils.parallel import Parallel, delayed
 
 
 def test_config_context():
     assert get_config() == {
         "assume_finite": False,
         "working_memory": 1024,
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_discriminant_analysis.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_discriminant_analysis.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,22 +7,23 @@
 from sklearn.datasets import make_blobs
 from sklearn.discriminant_analysis import (
     LinearDiscriminantAnalysis,
     QuadraticDiscriminantAnalysis,
     _cov,
 )
 from sklearn.preprocessing import StandardScaler
-from sklearn.utils import _IS_WASM, check_random_state
+from sklearn.utils import check_random_state
 from sklearn.utils._testing import (
     _convert_container,
     assert_allclose,
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
 )
+from sklearn.utils.fixes import _IS_WASM
 
 # Data is just 6 separable points in the plane
 X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]], dtype="f")
 y = np.array([1, 1, 1, 2, 2, 2])
 y3 = np.array([1, 1, 2, 2, 3, 3])
 
 # Degenerate data with only one feature (still should be separable)
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_docstring_parameters.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_docstring_parameters.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,27 +18,27 @@
 # make it possible to discover experimental estimators when calling `all_estimators`
 from sklearn.experimental import (
     enable_halving_search_cv,  # noqa
     enable_iterative_imputer,  # noqa
 )
 from sklearn.linear_model import LogisticRegression
 from sklearn.preprocessing import FunctionTransformer
-from sklearn.utils import IS_PYPY, all_estimators
+from sklearn.utils import all_estimators
 from sklearn.utils._testing import (
     _get_func_name,
     check_docstring_parameters,
     ignore_warnings,
 )
 from sklearn.utils.deprecation import _is_deprecated
 from sklearn.utils.estimator_checks import (
     _construct_instance,
     _enforce_estimator_tags_X,
     _enforce_estimator_tags_y,
 )
-from sklearn.utils.fixes import parse_version, sp_version
+from sklearn.utils.fixes import _IS_PYPY, parse_version, sp_version
 
 # walk_packages() ignores DeprecationWarnings, now we need to ignore
 # FutureWarnings
 with warnings.catch_warnings():
     warnings.simplefilter("ignore", FutureWarning)
     # mypy error: Module has no attribute "__path__"
     sklearn_path = [os.path.dirname(sklearn.__file__)]
@@ -47,14 +47,15 @@
             pckg[1]
             for pckg in walk_packages(prefix="sklearn.", path=sklearn_path)
             if not ("._" in pckg[1] or ".tests." in pckg[1])
         ]
     )
 
 # functions to ignore args / docstring of
+# TODO(1.7): remove "sklearn.utils._joblib"
 _DOCSTRING_IGNORES = [
     "sklearn.utils.deprecation.load_mlcomp",
     "sklearn.pipeline.make_pipeline",
     "sklearn.pipeline.make_union",
     "sklearn.utils.extmath.safe_sparse_dot",
     "sklearn.utils._joblib",
     "HalfBinomialLoss",
@@ -71,15 +72,15 @@
 ]
 
 
 # numpydoc 0.8.0's docscrape tool raises because of collections.abc under
 # Python 3.7
 @pytest.mark.filterwarnings("ignore::FutureWarning")
 @pytest.mark.filterwarnings("ignore::DeprecationWarning")
-@pytest.mark.skipif(IS_PYPY, reason="test segfaults on PyPy")
+@pytest.mark.skipif(_IS_PYPY, reason="test segfaults on PyPy")
 def test_docstring_parameters():
     # Test module docstring formatting
 
     # Skip test if numpydoc is not found
     pytest.importorskip(
         "numpydoc", reason="numpydoc is required to test the docstrings"
     )
@@ -221,30 +222,29 @@
     ):
         # default="auto" raises an error with the shape of `X`
         est.set_params(n_components=2)
     elif Estimator.__name__ == "TSNE":
         # default raises an error, perplexity must be less than n_samples
         est.set_params(perplexity=2)
 
-    # TODO(1.5): TO BE REMOVED for 1.5 (avoid FutureWarning)
-    if Estimator.__name__ in ("LinearSVC", "LinearSVR"):
-        est.set_params(dual="auto")
-
     # TODO(1.6): remove (avoid FutureWarning)
     if Estimator.__name__ in ("NMF", "MiniBatchNMF"):
         est.set_params(n_components="auto")
 
     if Estimator.__name__ == "QuantileRegressor":
         solver = "highs" if sp_version >= parse_version("1.6.0") else "interior-point"
         est.set_params(solver=solver)
 
     # Low max iter to speed up tests: we are only interested in checking the existence
     # of fitted attributes. This should be invariant to whether it has converged or not.
     if "max_iter" in est.get_params():
         est.set_params(max_iter=2)
+        # min value for `TSNE` is 250
+        if Estimator.__name__ == "TSNE":
+            est.set_params(max_iter=250)
 
     if "random_state" in est.get_params():
         est.set_params(random_state=0)
 
     # In case we want to deprecate some attributes in the future
     skipped_attributes = {}
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_docstrings.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_docstrings.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_dummy.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_dummy.py`

 * *Files 1% similar despite different names*

```diff
@@ -68,14 +68,31 @@
 
 
 def _check_equality_regressor(statistic, y_learn, y_pred_learn, y_test, y_pred_test):
     assert_array_almost_equal(np.tile(statistic, (y_learn.shape[0], 1)), y_pred_learn)
     assert_array_almost_equal(np.tile(statistic, (y_test.shape[0], 1)), y_pred_test)
 
 
+def test_feature_names_in_and_n_features_in_(global_random_seed, n_samples=10):
+    pd = pytest.importorskip("pandas")
+
+    random_state = np.random.RandomState(seed=global_random_seed)
+
+    X = pd.DataFrame([[0]] * n_samples, columns=["feature_1"])
+    y = random_state.rand(n_samples)
+
+    est = DummyRegressor().fit(X, y)
+    assert hasattr(est, "feature_names_in_")
+    assert hasattr(est, "n_features_in_")
+
+    est = DummyClassifier().fit(X, y)
+    assert hasattr(est, "feature_names_in_")
+    assert hasattr(est, "n_features_in_")
+
+
 def test_most_frequent_and_prior_strategy():
     X = [[0], [0], [0], [0]]  # ignored
     y = [1, 2, 1, 1]
 
     for strategy in ("most_frequent", "prior"):
         clf = DummyClassifier(strategy=strategy, random_state=0)
         clf.fit(X, y)
@@ -372,15 +389,15 @@
     )
     with pytest.raises(ValueError, match=err_msg):
         est.fit(X, y)
 
 
 def test_quantile_strategy_empty_train():
     est = DummyRegressor(strategy="quantile", quantile=0.4)
-    with pytest.raises(ValueError):
+    with pytest.raises(IndexError):
         est.fit([], [])
 
 
 def test_constant_strategy_regressor(global_random_seed):
     random_state = np.random.RandomState(seed=global_random_seed)
 
     X = [[0]] * 5  # ignored
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_isotonic.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_isotonic.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_kernel_approximation.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_kernel_approximation.py`

 * *Files 2% similar despite different names*

```diff
@@ -140,27 +140,14 @@
         sample_steps=sample_steps,
         sample_interval=sample_interval,
     )
     getattr(transformer, method)(X)
     assert transformer.sample_interval == sample_interval
 
 
-# TODO(1.5): remove
-def test_additive_chi2_sampler_future_warnings():
-    """Check that we raise a FutureWarning when accessing to `sample_interval_`."""
-    transformer = AdditiveChi2Sampler()
-    transformer.fit(X)
-    msg = re.escape(
-        "The ``sample_interval_`` attribute was deprecated in version 1.3 and "
-        "will be removed 1.5."
-    )
-    with pytest.warns(FutureWarning, match=msg):
-        assert transformer.sample_interval_ is not None
-
-
 @pytest.mark.parametrize("method", ["fit", "fit_transform", "transform"])
 def test_additive_chi2_sampler_wrong_sample_steps(method):
     """Check that we raise a ValueError on invalid sample_steps"""
     transformer = AdditiveChi2Sampler(sample_steps=4)
     msg = re.escape(
         "If sample_steps is not in [1, 2, 3], you need to provide sample_interval"
     )
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_kernel_ridge.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_kernel_ridge.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_metadata_routing.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_metadata_routing.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,17 @@
 import pytest
 
 from sklearn import config_context
 from sklearn.base import (
     BaseEstimator,
     clone,
 )
+from sklearn.exceptions import UnsetMetadataPassedError
 from sklearn.linear_model import LinearRegression
+from sklearn.pipeline import Pipeline
 from sklearn.tests.metadata_routing_common import (
     ConsumingClassifier,
     ConsumingRegressor,
     ConsumingTransformer,
     MetaRegressor,
     MetaTransformer,
     NonConsumingClassifier,
@@ -64,15 +66,21 @@
 def enable_slep006():
     """Enable SLEP006 for all tests."""
     with config_context(enable_metadata_routing=True):
         yield
 
 
 class SimplePipeline(BaseEstimator):
-    """A very simple pipeline, assuming the last step is always a predictor."""
+    """A very simple pipeline, assuming the last step is always a predictor.
+
+    Parameters
+    ----------
+    steps : iterable of objects
+        An iterable of transformers with the last step being a predictor.
+    """
 
     def __init__(self, steps):
         self.steps = steps
 
     def fit(self, X, y, **fit_params):
         self.steps_ = []
         params = process_routing(self, "fit", **fit_params)
@@ -102,19 +110,24 @@
 
     def get_metadata_routing(self):
         router = MetadataRouter(owner=self.__class__.__name__)
         for i, step in enumerate(self.steps[:-1]):
             router.add(
                 **{f"step_{i}": step},
                 method_mapping=MethodMapping()
-                .add(callee="fit", caller="fit")
-                .add(callee="transform", caller="fit")
-                .add(callee="transform", caller="predict"),
+                .add(caller="fit", callee="fit")
+                .add(caller="fit", callee="transform")
+                .add(caller="predict", callee="transform"),
             )
-        router.add(predictor=self.steps[-1], method_mapping="one-to-one")
+        router.add(
+            predictor=self.steps[-1],
+            method_mapping=MethodMapping()
+            .add(caller="fit", callee="fit")
+            .add(caller="predict", callee="predict"),
+        )
         return router
 
 
 def test_assert_request_is_empty():
     requests = MetadataRequest(owner="test")
     assert_request_is_empty(requests)
 
@@ -138,15 +151,18 @@
     # and excluding both fit and score would avoid an exception
     assert_request_is_empty(requests, exclude=["fit", "score"])
 
     # test if a router is empty
     assert_request_is_empty(
         MetadataRouter(owner="test")
         .add_self_request(WeightedMetaRegressor(estimator=None))
-        .add(method_mapping="fit", estimator=ConsumingRegressor())
+        .add(
+            estimator=ConsumingRegressor(),
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
+        )
     )
 
 
 @pytest.mark.parametrize(
     "estimator",
     [
         ConsumingClassifier(registry=_Registry()),
@@ -222,14 +238,37 @@
     assert est_request.fit.requests == {
         "sample_weight": None,
         "metadata": None,
     }
     assert_request_is_empty(est_request)
 
 
+def test_default_request_override():
+    """Test that default requests are correctly overridden regardless of the ASCII order
+    of the class names, hence testing small and capital letter class name starts.
+    Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/28430
+    """
+
+    class Base(BaseEstimator):
+        __metadata_request__split = {"groups": True}
+
+    class class_1(Base):
+        __metadata_request__split = {"groups": "sample_domain"}
+
+    class Class_1(Base):
+        __metadata_request__split = {"groups": "sample_domain"}
+
+    assert_request_equal(
+        class_1()._get_metadata_request(), {"split": {"groups": "sample_domain"}}
+    )
+    assert_request_equal(
+        Class_1()._get_metadata_request(), {"split": {"groups": "sample_domain"}}
+    )
+
+
 def test_process_routing_invalid_method():
     with pytest.raises(TypeError, match="Can only route and process input"):
         process_routing(ConsumingClassifier(), "invalid_method", groups=my_groups)
 
 
 def test_process_routing_invalid_object():
     class InvalidObject:
@@ -268,15 +307,15 @@
     clf.fit(X, y, sample_weight=my_weights)
 
     # If the estimator accepts the metadata but doesn't explicitly say it doesn't
     # need it, there's an error
     clf = WeightedMetaClassifier(estimator=ConsumingClassifier())
     err_message = (
         "[sample_weight] are passed but are not explicitly set as requested or"
-        " not for ConsumingClassifier.fit"
+        " not requested for ConsumingClassifier.fit"
     )
     with pytest.raises(ValueError, match=re.escape(err_message)):
         clf.fit(X, y, sample_weight=my_weights)
 
     # Explicitly saying the estimator doesn't need it, makes the error go away,
     # because in this case `WeightedMetaClassifier` consumes `sample_weight`. If
     # there was no consumer of sample_weight, passing it would result in an
@@ -642,58 +681,51 @@
             ),
             "{'foo': 'bar'}",
         ),
         (
             MetadataRequest(owner="test"),
             "{}",
         ),
-        (MethodMapping.from_str("score"), "[{'callee': 'score', 'caller': 'score'}]"),
         (
             MetadataRouter(owner="test").add(
-                method_mapping="predict", estimator=ConsumingRegressor()
+                estimator=ConsumingRegressor(),
+                method_mapping=MethodMapping().add(caller="predict", callee="predict"),
             ),
             (
-                "{'estimator': {'mapping': [{'callee': 'predict', 'caller':"
+                "{'estimator': {'mapping': [{'caller': 'predict', 'callee':"
                 " 'predict'}], 'router': {'fit': {'sample_weight': None, 'metadata':"
                 " None}, 'partial_fit': {'sample_weight': None, 'metadata': None},"
                 " 'predict': {'sample_weight': None, 'metadata': None}, 'score':"
-                " {'sample_weight': None}}}}"
+                " {'sample_weight': None, 'metadata': None}}}}"
             ),
         ),
     ],
 )
 def test_string_representations(obj, string):
     assert str(obj) == string
 
 
 @pytest.mark.parametrize(
     "obj, method, inputs, err_cls, err_msg",
     [
         (
             MethodMapping(),
             "add",
-            {"callee": "invalid", "caller": "fit"},
+            {"caller": "fit", "callee": "invalid"},
             ValueError,
             "Given callee",
         ),
         (
             MethodMapping(),
             "add",
-            {"callee": "fit", "caller": "invalid"},
+            {"caller": "invalid", "callee": "fit"},
             ValueError,
             "Given caller",
         ),
         (
-            MethodMapping,
-            "from_str",
-            {"route": "invalid"},
-            ValueError,
-            "route should be 'one-to-one' or a single method!",
-        ),
-        (
             MetadataRouter(owner="test"),
             "add_self_request",
             {"obj": MetadataRouter(owner="test")},
             ValueError,
             "Given `obj` is neither a `MetadataRequest` nor does it implement",
         ),
         (
@@ -714,24 +746,25 @@
     mm = (
         MethodMapping()
         .add(caller="fit", callee="transform")
         .add(caller="fit", callee="fit")
     )
 
     mm_list = list(mm)
-    assert mm_list[0] == ("transform", "fit")
+    assert mm_list[0] == ("fit", "transform")
     assert mm_list[1] == ("fit", "fit")
 
-    mm = MethodMapping.from_str("one-to-one")
+    mm = MethodMapping()
     for method in METHODS:
+        mm.add(caller=method, callee=method)
         assert MethodPair(method, method) in mm._routes
     assert len(mm._routes) == len(METHODS)
 
-    mm = MethodMapping.from_str("score")
-    assert repr(mm) == "[{'callee': 'score', 'caller': 'score'}]"
+    mm = MethodMapping().add(caller="score", callee="score")
+    assert repr(mm) == "[{'caller': 'score', 'callee': 'score'}]"
 
 
 def test_metadatarouter_add_self_request():
     # adding a MetadataRequest as `self` adds a copy
     request = MetadataRequest(owner="nested")
     request.fit.add_request(param="param", alias=True)
     router = MetadataRouter(owner="test").add_self_request(request)
@@ -758,59 +791,61 @@
     # it should be a copy, not the same object
     assert router._self_request is not est._get_metadata_request()
 
 
 def test_metadata_routing_add():
     # adding one with a string `method_mapping`
     router = MetadataRouter(owner="test").add(
-        method_mapping="fit",
         est=ConsumingRegressor().set_fit_request(sample_weight="weights"),
+        method_mapping=MethodMapping().add(caller="fit", callee="fit"),
     )
     assert (
         str(router)
-        == "{'est': {'mapping': [{'callee': 'fit', 'caller': 'fit'}], 'router': {'fit':"
+        == "{'est': {'mapping': [{'caller': 'fit', 'callee': 'fit'}], 'router': {'fit':"
         " {'sample_weight': 'weights', 'metadata': None}, 'partial_fit':"
         " {'sample_weight': None, 'metadata': None}, 'predict': {'sample_weight':"
-        " None, 'metadata': None}, 'score': {'sample_weight': None}}}}"
+        " None, 'metadata': None}, 'score': {'sample_weight': None, 'metadata':"
+        " None}}}}"
     )
 
     # adding one with an instance of MethodMapping
     router = MetadataRouter(owner="test").add(
-        method_mapping=MethodMapping().add(callee="score", caller="fit"),
+        method_mapping=MethodMapping().add(caller="fit", callee="score"),
         est=ConsumingRegressor().set_score_request(sample_weight=True),
     )
     assert (
         str(router)
-        == "{'est': {'mapping': [{'callee': 'score', 'caller': 'fit'}], 'router':"
+        == "{'est': {'mapping': [{'caller': 'fit', 'callee': 'score'}], 'router':"
         " {'fit': {'sample_weight': None, 'metadata': None}, 'partial_fit':"
         " {'sample_weight': None, 'metadata': None}, 'predict': {'sample_weight':"
-        " None, 'metadata': None}, 'score': {'sample_weight': True}}}}"
+        " None, 'metadata': None}, 'score': {'sample_weight': True, 'metadata':"
+        " None}}}}"
     )
 
 
 def test_metadata_routing_get_param_names():
     router = (
         MetadataRouter(owner="test")
         .add_self_request(
             WeightedMetaRegressor(estimator=ConsumingRegressor()).set_fit_request(
                 sample_weight="self_weights"
             )
         )
         .add(
-            method_mapping="fit",
             trs=ConsumingTransformer().set_fit_request(
                 sample_weight="transform_weights"
             ),
+            method_mapping=MethodMapping().add(caller="fit", callee="fit"),
         )
     )
 
     assert (
         str(router)
         == "{'$self_request': {'fit': {'sample_weight': 'self_weights'}, 'score':"
-        " {'sample_weight': None}}, 'trs': {'mapping': [{'callee': 'fit', 'caller':"
+        " {'sample_weight': None}}, 'trs': {'mapping': [{'caller': 'fit', 'callee':"
         " 'fit'}], 'router': {'fit': {'sample_weight': 'transform_weights',"
         " 'metadata': None}, 'transform': {'sample_weight': None, 'metadata': None},"
         " 'inverse_transform': {'sample_weight': None, 'metadata': None}}}}"
     )
 
     assert router._get_param_names(
         method="fit", return_alias=True, ignore_self_request=False
@@ -1002,7 +1037,75 @@
     # This passes since no metadata is passed.
     MetaRegressor(estimator=Estimator()).fit(X, y)
     # This fails since metadata is passed but Estimator() does not support it.
     with pytest.raises(
         NotImplementedError, match="Estimator has not implemented metadata routing yet."
     ):
         MetaRegressor(estimator=Estimator()).fit(X, y, metadata=my_groups)
+
+
+def test_unsetmetadatapassederror_correct():
+    """Test that UnsetMetadataPassedError raises the correct error message when
+    set_{method}_request is not set in nested cases."""
+    weighted_meta = WeightedMetaClassifier(estimator=ConsumingClassifier())
+    pipe = SimplePipeline([weighted_meta])
+    msg = re.escape(
+        "[metadata] are passed but are not explicitly set as requested or not requested"
+        " for ConsumingClassifier.fit, which is used within WeightedMetaClassifier.fit."
+        " Call `ConsumingClassifier.set_fit_request({metadata}=True/False)` for each"
+        " metadata you want to request/ignore."
+    )
+
+    with pytest.raises(UnsetMetadataPassedError, match=msg):
+        pipe.fit(X, y, metadata="blah")
+
+
+def test_unsetmetadatapassederror_correct_for_composite_methods():
+    """Test that UnsetMetadataPassedError raises the correct error message when
+    composite metadata request methods are not set in nested cases."""
+    consuming_transformer = ConsumingTransformer()
+    pipe = Pipeline([("consuming_transformer", consuming_transformer)])
+
+    msg = re.escape(
+        "[metadata] are passed but are not explicitly set as requested or not requested"
+        " for ConsumingTransformer.fit_transform, which is used within"
+        " Pipeline.fit_transform. Call"
+        " `ConsumingTransformer.set_fit_request({metadata}=True/False)"
+        ".set_transform_request({metadata}=True/False)`"
+        " for each metadata you want to request/ignore."
+    )
+    with pytest.raises(UnsetMetadataPassedError, match=msg):
+        pipe.fit_transform(X, y, metadata="blah")
+
+
+def test_unbound_set_methods_work():
+    """Tests that if the set_{method}_request is unbound, it still works.
+
+    Also test that passing positional arguments to the set_{method}_request fails
+    with the right TypeError message.
+
+    Non-regression test for https://github.com/scikit-learn/scikit-learn/issues/28632
+    """
+
+    class A(BaseEstimator):
+        def fit(self, X, y, sample_weight=None):
+            return self
+
+    error_message = re.escape(
+        "set_fit_request() takes 0 positional argument but 1 were given"
+    )
+
+    # Test positional arguments error before making the descriptor method unbound.
+    with pytest.raises(TypeError, match=error_message):
+        A().set_fit_request(True)
+
+    # This somehow makes the descriptor method unbound, which results in the `instance`
+    # argument being None, and instead `self` being passed as a positional argument
+    # to the descriptor method.
+    A.set_fit_request = A.set_fit_request
+
+    # This should pass as usual
+    A().set_fit_request(sample_weight=True)
+
+    # Test positional arguments error after making the descriptor method unbound.
+    with pytest.raises(TypeError, match=error_message):
+        A().set_fit_request(True)
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_metaestimators.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_metaestimators.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Common tests for metaestimators"""
+
 import functools
 from inspect import signature
 
 import numpy as np
 import pytest
 
 from sklearn.base import BaseEstimator, is_regressor
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_metaestimators_metadata_routing.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_metaestimators_metadata_routing.py`

 * *Files 11% similar despite different names*

```diff
@@ -12,16 +12,14 @@
 from sklearn.ensemble import (
     AdaBoostClassifier,
     AdaBoostRegressor,
     BaggingClassifier,
     BaggingRegressor,
     StackingClassifier,
     StackingRegressor,
-    VotingClassifier,
-    VotingRegressor,
 )
 from sklearn.exceptions import UnsetMetadataPassedError
 from sklearn.experimental import (
     enable_halving_search_cv,  # noqa
     enable_iterative_imputer,  # noqa
 )
 from sklearn.feature_selection import (
@@ -57,15 +55,14 @@
 )
 from sklearn.multioutput import (
     ClassifierChain,
     MultiOutputClassifier,
     MultiOutputRegressor,
     RegressorChain,
 )
-from sklearn.pipeline import FeatureUnion
 from sklearn.semi_supervised import SelfTrainingClassifier
 from sklearn.tests.metadata_routing_common import (
     ConsumingClassifier,
     ConsumingRegressor,
     ConsumingScorer,
     ConsumingSplitter,
     NonConsumingClassifier,
@@ -116,15 +113,15 @@
     {
         "metaestimator": CalibratedClassifierCV,
         "estimator_name": "estimator",
         "estimator": "classifier",
         "X": X,
         "y": y,
         "estimator_routing_methods": ["fit"],
-        "preserves_metadata": False,
+        "preserves_metadata": "subset",
     },
     {
         "metaestimator": ClassifierChain,
         "estimator_name": "base_estimator",
         "estimator": "classifier",
         "X": X,
         "y": y_multi,
@@ -285,20 +282,97 @@
     {
         "metaestimator": LassoLarsCV,
         "X": X,
         "y": y,
         "cv_name": "cv",
         "cv_routing_methods": ["fit"],
     },
+    {
+        "metaestimator": RANSACRegressor,
+        "estimator_name": "estimator",
+        "estimator": "regressor",
+        "init_args": {"min_samples": 0.5},
+        "X": X,
+        "y": y,
+        "preserves_metadata": "subset",
+        "estimator_routing_methods": ["fit", "predict", "score"],
+        "method_mapping": {"fit": ["fit", "score"]},
+    },
+    {
+        "metaestimator": IterativeImputer,
+        "estimator_name": "estimator",
+        "estimator": "regressor",
+        "init_args": {"skip_complete": False},
+        "X": X,
+        "y": y,
+        "estimator_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": BaggingClassifier,
+        "estimator_name": "estimator",
+        "estimator": "classifier",
+        "X": X,
+        "y": y,
+        "preserves_metadata": False,
+        "estimator_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": BaggingRegressor,
+        "estimator_name": "estimator",
+        "estimator": "regressor",
+        "X": X,
+        "y": y,
+        "preserves_metadata": False,
+        "estimator_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": RidgeCV,
+        "X": X,
+        "y": y,
+        "scorer_name": "scoring",
+        "scorer_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": RidgeClassifierCV,
+        "X": X,
+        "y": y,
+        "scorer_name": "scoring",
+        "scorer_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": RidgeCV,
+        "X": X,
+        "y": y,
+        "scorer_name": "scoring",
+        "scorer_routing_methods": ["fit"],
+        "cv_name": "cv",
+        "cv_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": RidgeClassifierCV,
+        "X": X,
+        "y": y,
+        "scorer_name": "scoring",
+        "scorer_routing_methods": ["fit"],
+        "cv_name": "cv",
+        "cv_routing_methods": ["fit"],
+    },
+    {
+        "metaestimator": GraphicalLassoCV,
+        "X": X,
+        "y": y,
+        "cv_name": "cv",
+        "cv_routing_methods": ["fit"],
+    },
 ]
 """List containing all metaestimators to be tested and their settings
 
 The keys are as follows:
 
-- metaestimator: The metaestmator to be tested
+- metaestimator: The metaestimator to be tested
 - estimator_name: The name of the argument for the sub-estimator
 - estimator: The sub-estimator type, either "regressor" or "classifier"
 - init_args: The arguments to be passed to the metaestimator's constructor
 - X: X-data to fit and predict
 - y: y-data to fit
 - estimator_routing_methods: list of all methods to check for routing metadata
   to the sub-estimator
@@ -315,39 +389,32 @@
 - scorer_routing_methods: list of all methods to check for routing metadata
   to the scorer
 - cv_name: The name of the argument for the CV splitter
 - cv_routing_methods: list of all methods to check for routing metadata
   to the splitter
 - method_args: a dict of dicts, defining extra arguments needed to be passed to
   methods, such as passing `classes` to `partial_fit`.
+- method_mapping: a dict of the form `{caller: [callee1, ...]}` which signals
+  which `.set_{method}_request` methods should be called to set request values.
+  If not present, a one-to-one mapping is assumed.
 """
 
 # IDs used by pytest to get meaningful verbose messages when running the tests
 METAESTIMATOR_IDS = [str(row["metaestimator"].__name__) for row in METAESTIMATORS]
 
 UNSUPPORTED_ESTIMATORS = [
     AdaBoostClassifier(),
     AdaBoostRegressor(),
-    BaggingClassifier(),
-    BaggingRegressor(),
-    FeatureUnion([]),
-    GraphicalLassoCV(),
-    IterativeImputer(),
-    RANSACRegressor(),
     RFE(ConsumingClassifier()),
     RFECV(ConsumingClassifier()),
-    RidgeCV(),
-    RidgeClassifierCV(),
     SelfTrainingClassifier(ConsumingClassifier()),
     SequentialFeatureSelector(ConsumingClassifier()),
     StackingClassifier(ConsumingClassifier()),
     StackingRegressor(ConsumingRegressor()),
     TransformedTargetRegressor(),
-    VotingClassifier(ConsumingClassifier()),
-    VotingRegressor(ConsumingRegressor()),
 ]
 
 
 def get_init_args(metaestimator_info, sub_estimator_consumes):
     """Get the init args for a metaestimator
 
     This is a helper function to get the init args for a metaestimator from
@@ -383,21 +450,25 @@
     if "estimator" in metaestimator_info:
         estimator_name = metaestimator_info["estimator_name"]
         estimator_registry = _Registry()
         sub_estimator_type = metaestimator_info["estimator"]
         if sub_estimator_consumes:
             if sub_estimator_type == "regressor":
                 estimator = ConsumingRegressor(estimator_registry)
-            else:
+            elif sub_estimator_type == "classifier":
                 estimator = ConsumingClassifier(estimator_registry)
+            else:
+                raise ValueError("Unpermitted `sub_estimator_type`.")  # pragma: nocover
         else:
             if sub_estimator_type == "regressor":
                 estimator = NonConsumingRegressor()
-            else:
+            elif sub_estimator_type == "classifier":
                 estimator = NonConsumingClassifier()
+            else:
+                raise ValueError("Unpermitted `sub_estimator_type`.")  # pragma: nocover
         kwargs[estimator_name] = estimator
     if "scorer_name" in metaestimator_info:
         scorer_name = metaestimator_info["scorer_name"]
         scorer_registry = _Registry()
         scorer = ConsumingScorer(registry=scorer_registry)
         kwargs[scorer_name] = scorer
     if "cv_name" in metaestimator_info:
@@ -410,14 +481,46 @@
         kwargs,
         (estimator, estimator_registry),
         (scorer, scorer_registry),
         (cv, cv_registry),
     )
 
 
+def set_requests(estimator, *, method_mapping, methods, metadata_name, value=True):
+    """Call `set_{method}_request` on a list of methods from the sub-estimator.
+
+    Parameters
+    ----------
+    estimator : BaseEstimator
+        The estimator for which `set_{method}_request` methods are called.
+
+    method_mapping : dict
+        The method mapping in the form of `{caller: [callee, ...]}`.
+        If a "caller" is not present in the method mapping, a one-to-one mapping is
+        assumed.
+
+    methods : list of str
+        The list of methods as "caller"s for which the request for the child should
+        be set.
+
+    metadata_name : str
+        The name of the metadata to be routed, usually either `"metadata"` or
+        `"sample_weight"` in our tests.
+
+    value : None, bool, or str
+        The request value to be set, by default it's `True`
+    """
+    for caller in methods:
+        for callee in method_mapping.get(caller, [caller]):
+            set_request_for_method = getattr(estimator, f"set_{callee}_request")
+            set_request_for_method(**{metadata_name: value})
+            if is_classifier(estimator) and callee == "partial_fit":
+                set_request_for_method(classes=True)
+
+
 @pytest.mark.parametrize("estimator", UNSUPPORTED_ESTIMATORS)
 def test_unsupported_estimators_get_metadata_routing(estimator):
     """Test that get_metadata_routing is not implemented on meta-estimators for
     which we haven't implemented routing yet."""
     with pytest.raises(NotImplementedError):
         estimator.get_metadata_routing()
 
@@ -480,66 +583,108 @@
             kwargs, (estimator, _), (scorer, _), *_ = get_init_args(
                 metaestimator, sub_estimator_consumes=True
             )
             if scorer:
                 scorer.set_score_request(**{key: True})
             val = {"sample_weight": sample_weight, "metadata": metadata}[key]
             method_kwargs = {key: val}
+            instance = cls(**kwargs)
             msg = (
                 f"[{key}] are passed but are not explicitly set as requested or not"
-                f" for {estimator.__class__.__name__}.{method_name}"
+                f" requested for {estimator.__class__.__name__}.{method_name}"
             )
-
-            instance = cls(**kwargs)
             with pytest.raises(UnsetMetadataPassedError, match=re.escape(msg)):
                 method = getattr(instance, method_name)
-                method(X, y, **method_kwargs)
+                if "fit" not in method_name:
+                    # set request on fit
+                    set_requests(
+                        estimator,
+                        method_mapping=metaestimator.get("method_mapping", {}),
+                        methods=["fit"],
+                        metadata_name=key,
+                    )
+                    instance.fit(X, y, **method_kwargs)
+                # making sure the requests are unset, in case they were set as a
+                # side effect of setting them for fit. For instance, if method
+                # mapping for fit is: `"fit": ["fit", "score"]`, that would mean
+                # calling `.score` here would not raise, because we have already
+                # set request value for child estimator's `score`.
+                set_requests(
+                    estimator,
+                    method_mapping=metaestimator.get("method_mapping", {}),
+                    methods=["fit"],
+                    metadata_name=key,
+                    value=None,
+                )
+                try:
+                    # `fit` and `partial_fit` accept y, others don't.
+                    method(X, y, **method_kwargs)
+                except TypeError:
+                    method(X, **method_kwargs)
 
 
 @pytest.mark.parametrize("metaestimator", METAESTIMATORS, ids=METAESTIMATOR_IDS)
 def test_setting_request_on_sub_estimator_removes_error(metaestimator):
     # When the metadata is explicitly requested on the sub-estimator, there
     # should be no errors.
     if "estimator" not in metaestimator:
         # This test only makes sense for metaestimators which have a
         # sub-estimator, e.g. MyMetaEstimator(estimator=MySubEstimator())
         return
 
-    def set_request(estimator, method_name):
-        # e.g. call set_fit_request on estimator
-        set_request_for_method = getattr(estimator, f"set_{method_name}_request")
-        set_request_for_method(sample_weight=True, metadata=True)
-        if is_classifier(estimator) and method_name == "partial_fit":
-            set_request_for_method(classes=True)
-
     cls = metaestimator["metaestimator"]
     X = metaestimator["X"]
     y = metaestimator["y"]
     routing_methods = metaestimator["estimator_routing_methods"]
+    method_mapping = metaestimator.get("method_mapping", {})
     preserves_metadata = metaestimator.get("preserves_metadata", True)
 
     for method_name in routing_methods:
         for key in ["sample_weight", "metadata"]:
             val = {"sample_weight": sample_weight, "metadata": metadata}[key]
             method_kwargs = {key: val}
 
             kwargs, (estimator, registry), (scorer, _), (cv, _) = get_init_args(
                 metaestimator, sub_estimator_consumes=True
             )
             if scorer:
-                set_request(scorer, "score")
+                set_requests(
+                    scorer, method_mapping={}, methods=["score"], metadata_name=key
+                )
             if cv:
                 cv.set_split_request(groups=True, metadata=True)
-            set_request(estimator, method_name)
+
+            # `set_{method}_request({metadata}==True)` on the underlying objects
+            set_requests(
+                estimator,
+                method_mapping=method_mapping,
+                methods=[method_name],
+                metadata_name=key,
+            )
+
             instance = cls(**kwargs)
             method = getattr(instance, method_name)
             extra_method_args = metaestimator.get("method_args", {}).get(
                 method_name, {}
             )
-            method(X, y, **method_kwargs, **extra_method_args)
+            if "fit" not in method_name:
+                # fit before calling method
+                set_requests(
+                    estimator,
+                    method_mapping=metaestimator.get("method_mapping", {}),
+                    methods=["fit"],
+                    metadata_name=key,
+                )
+                instance.fit(X, y, **method_kwargs, **extra_method_args)
+            try:
+                # `fit` and `partial_fit` accept y, others don't.
+                method(X, y, **method_kwargs, **extra_method_args)
+            except TypeError:
+                method(X, **method_kwargs, **extra_method_args)
+
             # sanity check that registry is not empty, or else the test passes
             # trivially
             assert registry
             if preserves_metadata is True:
                 for estimator in registry:
                     check_recorded_metadata(estimator, method_name, **method_kwargs)
             elif preserves_metadata == "subset":
@@ -576,16 +721,22 @@
         kwargs, (estimator, _), (_, _), (_, _) = get_init_args(
             metaestimator, sub_estimator_consumes=False
         )
         instance = cls(**kwargs)
         set_request(estimator, method_name)
         method = getattr(instance, method_name)
         extra_method_args = metaestimator.get("method_args", {}).get(method_name, {})
-        # This following line should pass w/o raising a routing error.
-        method(X, y, **extra_method_args)
+        if "fit" not in method_name:
+            instance.fit(X, y, **extra_method_args)
+        # The following should pass w/o raising a routing error.
+        try:
+            # `fit` and `partial_fit` accept y, others don't.
+            method(X, y, **extra_method_args)
+        except TypeError:
+            method(X, **extra_method_args)
 
 
 @pytest.mark.parametrize("metaestimator", METAESTIMATORS, ids=METAESTIMATOR_IDS)
 def test_metadata_is_routed_correctly_to_scorer(metaestimator):
     """Test that any requested metadata is correctly routed to the underlying
     scorers in CV estimators.
     """
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_multiclass.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_multiclass.py`

 * *Files 3% similar despite different names*

```diff
@@ -53,15 +53,15 @@
 perm = rng.permutation(iris.target.size)
 iris.data = iris.data[perm]
 iris.target = iris.target[perm]
 n_classes = 3
 
 
 def test_ovr_exceptions():
-    ovr = OneVsRestClassifier(LinearSVC(dual="auto", random_state=0))
+    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
 
     # test predicting without fitting
     with pytest.raises(NotFittedError):
         ovr.predict([])
 
     # Fail on multioutput data
     msg = "Multioutput target data is not supported with label binarization"
@@ -82,19 +82,19 @@
     msg = type_of_target(y)
     with pytest.raises(ValueError, match=msg):
         check_classification_targets(y)
 
 
 def test_ovr_fit_predict():
     # A classifier which implements decision_function.
-    ovr = OneVsRestClassifier(LinearSVC(dual="auto", random_state=0))
+    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
     pred = ovr.fit(iris.data, iris.target).predict(iris.data)
     assert len(ovr.estimators_) == n_classes
 
-    clf = LinearSVC(dual="auto", random_state=0)
+    clf = LinearSVC(random_state=0)
     pred2 = clf.fit(iris.data, iris.target).predict(iris.data)
     assert np.mean(iris.target == pred) == np.mean(iris.target == pred2)
 
     # A classifier which implements predict_proba.
     ovr = OneVsRestClassifier(MultinomialNB())
     pred = ovr.fit(iris.data, iris.target).predict(iris.data)
     assert np.mean(iris.target == pred) > 0.65
@@ -254,15 +254,15 @@
     y = ["eggs", "spam", "ham", "eggs", "ham"]
     Y = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0]])
 
     classes = set("ham eggs spam".split())
 
     for base_clf in (
         MultinomialNB(),
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         LinearRegression(),
         Ridge(),
         ElasticNet(),
     ):
         clf = OneVsRestClassifier(base_clf).fit(X, y)
         assert set(clf.classes_) == classes
         y_pred = clf.predict(np.array([[0, 0, 4]]))[0]
@@ -299,15 +299,15 @@
 
         # test input as label indicator matrix
         clf = OneVsRestClassifier(base_clf).fit(X, Y)
         y_pred = clf.predict([[3, 0, 0]])[0]
         assert y_pred == 1
 
     for base_clf in (
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         LinearRegression(),
         Ridge(),
         ElasticNet(),
     ):
         conduct_test(base_clf)
 
     for base_clf in (MultinomialNB(), SVC(probability=True), LogisticRegression()):
@@ -317,15 +317,15 @@
 def test_ovr_multilabel():
     # Toy dataset where features correspond directly to labels.
     X = np.array([[0, 4, 5], [0, 5, 0], [3, 3, 3], [4, 0, 6], [6, 0, 0]])
     y = np.array([[0, 1, 1], [0, 1, 0], [1, 1, 1], [1, 0, 1], [1, 0, 0]])
 
     for base_clf in (
         MultinomialNB(),
-        LinearSVC(dual="auto", random_state=0),
+        LinearSVC(random_state=0),
         LinearRegression(),
         Ridge(),
         ElasticNet(),
         Lasso(alpha=0.5),
     ):
         clf = OneVsRestClassifier(base_clf).fit(X, y)
         y_pred = clf.predict([[0, 4, 4]])[0]
@@ -455,15 +455,15 @@
     X_train, Y_train = X[:80], Y[:80]
     X_test = X[80:]
     clf = OneVsRestClassifier(svm.SVC()).fit(X_train, Y_train)
     assert_array_equal(clf.decision_function(X_test).ravel() > 0, clf.predict(X_test))
 
 
 def test_ovr_gridsearch():
-    ovr = OneVsRestClassifier(LinearSVC(dual="auto", random_state=0))
+    ovr = OneVsRestClassifier(LinearSVC(random_state=0))
     Cs = [0.1, 0.5, 0.8]
     cv = GridSearchCV(ovr, {"estimator__C": Cs})
     cv.fit(iris.data, iris.target)
     best_C = cv.best_estimator_.estimators_[0].C
     assert best_C in Cs
 
 
@@ -476,34 +476,34 @@
     ovr_pipe.fit(iris.data, iris.target)
     ovr = OneVsRestClassifier(DecisionTreeClassifier())
     ovr.fit(iris.data, iris.target)
     assert_array_equal(ovr.predict(iris.data), ovr_pipe.predict(iris.data))
 
 
 def test_ovo_exceptions():
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto", random_state=0))
+    ovo = OneVsOneClassifier(LinearSVC(random_state=0))
     with pytest.raises(NotFittedError):
         ovo.predict([])
 
 
 def test_ovo_fit_on_list():
     # Test that OneVsOne fitting works with a list of targets and yields the
     # same output as predict from an array
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto", random_state=0))
+    ovo = OneVsOneClassifier(LinearSVC(random_state=0))
     prediction_from_array = ovo.fit(iris.data, iris.target).predict(iris.data)
     iris_data_list = [list(a) for a in iris.data]
     prediction_from_list = ovo.fit(iris_data_list, list(iris.target)).predict(
         iris_data_list
     )
     assert_array_equal(prediction_from_array, prediction_from_list)
 
 
 def test_ovo_fit_predict():
     # A classifier which implements decision_function.
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto", random_state=0))
+    ovo = OneVsOneClassifier(LinearSVC(random_state=0))
     ovo.fit(iris.data, iris.target).predict(iris.data)
     assert len(ovo.estimators_) == n_classes * (n_classes - 1) / 2
 
     # A classifier which implements predict_proba.
     ovo = OneVsOneClassifier(MultinomialNB())
     ovo.fit(iris.data, iris.target).predict(iris.data)
     assert len(ovo.estimators_) == n_classes * (n_classes - 1) / 2
@@ -561,15 +561,15 @@
     ovr = OneVsOneClassifier(SVC())
     assert not hasattr(ovr, "partial_fit")
 
 
 def test_ovo_decision_function():
     n_samples = iris.data.shape[0]
 
-    ovo_clf = OneVsOneClassifier(LinearSVC(dual="auto", random_state=0))
+    ovo_clf = OneVsOneClassifier(LinearSVC(random_state=0))
     # first binary
     ovo_clf.fit(iris.data, iris.target == 0)
     decisions = ovo_clf.decision_function(iris.data)
     assert decisions.shape == (n_samples,)
 
     # then multi-class
     ovo_clf.fit(iris.data, iris.target)
@@ -606,15 +606,15 @@
         # to compute the aggregate decision function. The iris dataset
         # has 150 samples with a couple of duplicates. The OvO decisions
         # can resolve most of the ties:
         assert len(np.unique(decisions[:, class_idx])) > 146
 
 
 def test_ovo_gridsearch():
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto", random_state=0))
+    ovo = OneVsOneClassifier(LinearSVC(random_state=0))
     Cs = [0.1, 0.5, 0.8]
     cv = GridSearchCV(ovo, {"estimator__C": Cs})
     cv.fit(iris.data, iris.target)
     best_C = cv.best_estimator_.estimators_[0].C
     assert best_C in Cs
 
 
@@ -656,76 +656,74 @@
 
 
 def test_ovo_string_y():
     # Test that the OvO doesn't mess up the encoding of string labels
     X = np.eye(4)
     y = np.array(["a", "b", "c", "d"])
 
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto"))
+    ovo = OneVsOneClassifier(LinearSVC())
     ovo.fit(X, y)
     assert_array_equal(y, ovo.predict(X))
 
 
 def test_ovo_one_class():
     # Test error for OvO with one class
     X = np.eye(4)
     y = np.array(["a"] * 4)
 
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto"))
+    ovo = OneVsOneClassifier(LinearSVC())
     msg = "when only one class"
     with pytest.raises(ValueError, match=msg):
         ovo.fit(X, y)
 
 
 def test_ovo_float_y():
     # Test that the OvO errors on float targets
     X = iris.data
     y = iris.data[:, 0]
 
-    ovo = OneVsOneClassifier(LinearSVC(dual="auto"))
+    ovo = OneVsOneClassifier(LinearSVC())
     msg = "Unknown label type"
     with pytest.raises(ValueError, match=msg):
         ovo.fit(X, y)
 
 
 def test_ecoc_exceptions():
-    ecoc = OutputCodeClassifier(LinearSVC(dual="auto", random_state=0))
+    ecoc = OutputCodeClassifier(LinearSVC(random_state=0))
     with pytest.raises(NotFittedError):
         ecoc.predict([])
 
 
 def test_ecoc_fit_predict():
     # A classifier which implements decision_function.
-    ecoc = OutputCodeClassifier(
-        LinearSVC(dual="auto", random_state=0), code_size=2, random_state=0
-    )
+    ecoc = OutputCodeClassifier(LinearSVC(random_state=0), code_size=2, random_state=0)
     ecoc.fit(iris.data, iris.target).predict(iris.data)
     assert len(ecoc.estimators_) == n_classes * 2
 
     # A classifier which implements predict_proba.
     ecoc = OutputCodeClassifier(MultinomialNB(), code_size=2, random_state=0)
     ecoc.fit(iris.data, iris.target).predict(iris.data)
     assert len(ecoc.estimators_) == n_classes * 2
 
 
 def test_ecoc_gridsearch():
-    ecoc = OutputCodeClassifier(LinearSVC(dual="auto", random_state=0), random_state=0)
+    ecoc = OutputCodeClassifier(LinearSVC(random_state=0), random_state=0)
     Cs = [0.1, 0.5, 0.8]
     cv = GridSearchCV(ecoc, {"estimator__C": Cs})
     cv.fit(iris.data, iris.target)
     best_C = cv.best_estimator_.estimators_[0].C
     assert best_C in Cs
 
 
 def test_ecoc_float_y():
     # Test that the OCC errors on float targets
     X = iris.data
     y = iris.data[:, 0]
 
-    ovo = OutputCodeClassifier(LinearSVC(dual="auto"))
+    ovo = OutputCodeClassifier(LinearSVC())
     msg = "Unknown label type"
     with pytest.raises(ValueError, match=msg):
         ovo.fit(X, y)
 
 
 @pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
 def test_ecoc_delegate_sparse_base_estimator(csc_container):
@@ -745,15 +743,15 @@
         ecoc.fit(X_sp, y)
 
     ecoc.fit(X, y)
     with pytest.raises(TypeError, match="Sparse data was passed"):
         ecoc.predict(X_sp)
 
     # smoke test to check when sparse input should be supported
-    ecoc = OutputCodeClassifier(LinearSVC(dual="auto", random_state=0))
+    ecoc = OutputCodeClassifier(LinearSVC(random_state=0))
     ecoc.fit(X_sp, y).predict(X_sp)
     assert len(ecoc.estimators_) == 4
 
 
 def test_pairwise_indices():
     clf_precomputed = svm.SVC(kernel="precomputed")
     X, y = iris.data, iris.target
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_multioutput.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_multioutput.py`

 * *Files 3% similar despite different names*

```diff
@@ -333,15 +333,15 @@
         forest_.fit(X, y[:, i])
         assert list(forest_.predict(X)) == list(predictions[:, i])
         assert_array_equal(list(forest_.predict_proba(X)), list(predict_proba[i]))
 
 
 def test_multiclass_multioutput_estimator():
     # test to check meta of meta estimators
-    svc = LinearSVC(dual="auto", random_state=0)
+    svc = LinearSVC(random_state=0)
     multi_class_svc = OneVsRestClassifier(svc)
     multi_target_svc = MultiOutputClassifier(multi_class_svc)
 
     multi_target_svc.fit(X, y)
 
     predictions = multi_target_svc.predict(X)
     assert (n_samples, n_outputs) == predictions.shape
@@ -438,15 +438,15 @@
     X_test = [[1.5, 2.5, 3.5]]
     assert_array_almost_equal(clf.predict(X_test), clf_w.predict(X_test))
 
 
 def test_multi_output_exceptions():
     # NotFittedError when fit is not done but score, predict and
     # and predict_proba are called
-    moc = MultiOutputClassifier(LinearSVC(dual="auto", random_state=0))
+    moc = MultiOutputClassifier(LinearSVC(random_state=0))
     with pytest.raises(NotFittedError):
         moc.score(X, y)
 
     # ValueError when number of outputs is different
     # for fit and score
     y_new = np.column_stack((y1, y2))
     moc.fit(X, y)
@@ -474,15 +474,15 @@
     # A base estimator with `predict_proba`should expose the method even before fit
     moc = MultiOutputClassifier(LogisticRegression())
     assert hasattr(moc, "predict_proba")
     moc.fit(X, y)
     assert hasattr(moc, "predict_proba")
 
     # A base estimator without `predict_proba` should raise an AttributeError
-    moc = MultiOutputClassifier(LinearSVC(dual="auto"))
+    moc = MultiOutputClassifier(LinearSVC())
     assert not hasattr(moc, "predict_proba")
 
     outer_msg = "'MultiOutputClassifier' has no attribute 'predict_proba'"
     inner_msg = "'LinearSVC' object has no attribute 'predict_proba'"
     with pytest.raises(AttributeError, match=outer_msg) as exec_info:
         moc.predict_proba(X)
     assert isinstance(exec_info.value.__cause__, AttributeError)
@@ -504,19 +504,22 @@
         n_samples=1000, n_features=100, n_classes=16, n_informative=10, random_state=0
     )
 
     Y_multi = np.array([[int(yyy) for yyy in format(yy, "#06b")[2:]] for yy in y])
     return X, Y_multi
 
 
-def test_classifier_chain_fit_and_predict_with_linear_svc():
+@pytest.mark.parametrize("chain_method", ["predict", "decision_function"])
+def test_classifier_chain_fit_and_predict_with_linear_svc(chain_method):
     # Fit classifier chain and verify predict performance using LinearSVC
     X, Y = generate_multilabel_dataset_with_correlations()
-    classifier_chain = ClassifierChain(LinearSVC(dual="auto"))
-    classifier_chain.fit(X, Y)
+    classifier_chain = ClassifierChain(
+        LinearSVC(),
+        chain_method=chain_method,
+    ).fit(X, Y)
 
     Y_pred = classifier_chain.predict(X)
     assert Y_pred.shape == Y.shape
 
     Y_decision = classifier_chain.decision_function(X)
 
     Y_binary = Y_decision >= 0
@@ -526,20 +529,18 @@
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_classifier_chain_fit_and_predict_with_sparse_data(csr_container):
     # Fit classifier chain with sparse data
     X, Y = generate_multilabel_dataset_with_correlations()
     X_sparse = csr_container(X)
 
-    classifier_chain = ClassifierChain(LogisticRegression())
-    classifier_chain.fit(X_sparse, Y)
+    classifier_chain = ClassifierChain(LogisticRegression()).fit(X_sparse, Y)
     Y_pred_sparse = classifier_chain.predict(X_sparse)
 
-    classifier_chain = ClassifierChain(LogisticRegression())
-    classifier_chain.fit(X, Y)
+    classifier_chain = ClassifierChain(LogisticRegression()).fit(X, Y)
     Y_pred_dense = classifier_chain.predict(X)
 
     assert_array_equal(Y_pred_sparse, Y_pred_dense)
 
 
 def test_classifier_chain_vs_independent_models():
     # Verify that an ensemble of classifier chains (each of length
@@ -560,34 +561,49 @@
     Y_pred_chain = chain.predict(X_test)
 
     assert jaccard_score(Y_test, Y_pred_chain, average="samples") > jaccard_score(
         Y_test, Y_pred_ovr, average="samples"
     )
 
 
+@pytest.mark.parametrize(
+    "chain_method",
+    ["predict", "predict_proba", "predict_log_proba", "decision_function"],
+)
 @pytest.mark.parametrize("response_method", ["predict_proba", "predict_log_proba"])
-def test_base_chain_fit_and_predict(response_method):
-    # Fit base chain and verify predict performance
+def test_classifier_chain_fit_and_predict(chain_method, response_method):
+    # Fit classifier chain and verify predict performance
     X, Y = generate_multilabel_dataset_with_correlations()
-    chains = [RegressorChain(Ridge()), ClassifierChain(LogisticRegression())]
-    for chain in chains:
-        chain.fit(X, Y)
-        Y_pred = chain.predict(X)
-        assert Y_pred.shape == Y.shape
-        assert [c.coef_.size for c in chain.estimators_] == list(
-            range(X.shape[1], X.shape[1] + Y.shape[1])
-        )
+    chain = ClassifierChain(LogisticRegression(), chain_method=chain_method)
+    chain.fit(X, Y)
+    Y_pred = chain.predict(X)
+    assert Y_pred.shape == Y.shape
+    assert [c.coef_.size for c in chain.estimators_] == list(
+        range(X.shape[1], X.shape[1] + Y.shape[1])
+    )
 
-    Y_prob = getattr(chains[1], response_method)(X)
+    Y_prob = getattr(chain, response_method)(X)
     if response_method == "predict_log_proba":
         Y_prob = np.exp(Y_prob)
     Y_binary = Y_prob >= 0.5
     assert_array_equal(Y_binary, Y_pred)
 
-    assert isinstance(chains[1], ClassifierMixin)
+    assert isinstance(chain, ClassifierMixin)
+
+
+def test_regressor_chain_fit_and_predict():
+    # Fit regressor chain and verify Y and estimator coefficients shape
+    X, Y = generate_multilabel_dataset_with_correlations()
+    chain = RegressorChain(Ridge())
+    chain.fit(X, Y)
+    Y_pred = chain.predict(X)
+    assert Y_pred.shape == Y.shape
+    assert [c.coef_.size for c in chain.estimators_] == list(
+        range(X.shape[1], X.shape[1] + Y.shape[1])
+    )
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_base_chain_fit_and_predict_with_sparse_data_and_cv(csr_container):
     # Fit base chain with sparse data cross_val_predict
     X, Y = generate_multilabel_dataset_with_correlations()
     X_sparse = csr_container(X)
@@ -615,32 +631,45 @@
         assert len(set(chain_random.order_)) == 4
         # Randomly ordered chain should behave identically to a fixed order
         # chain with the same order.
         for est1, est2 in zip(chain_random.estimators_, chain_fixed.estimators_):
             assert_array_almost_equal(est1.coef_, est2.coef_)
 
 
-def test_base_chain_crossval_fit_and_predict():
+@pytest.mark.parametrize(
+    "chain_type, chain_method",
+    [
+        ("classifier", "predict"),
+        ("classifier", "predict_proba"),
+        ("classifier", "predict_log_proba"),
+        ("classifier", "decision_function"),
+        ("regressor", ""),
+    ],
+)
+def test_base_chain_crossval_fit_and_predict(chain_type, chain_method):
     # Fit chain with cross_val_predict and verify predict
     # performance
     X, Y = generate_multilabel_dataset_with_correlations()
 
-    for chain in [ClassifierChain(LogisticRegression()), RegressorChain(Ridge())]:
-        chain.fit(X, Y)
-        chain_cv = clone(chain).set_params(cv=3)
-        chain_cv.fit(X, Y)
-        Y_pred_cv = chain_cv.predict(X)
-        Y_pred = chain.predict(X)
-
-        assert Y_pred_cv.shape == Y_pred.shape
-        assert not np.all(Y_pred == Y_pred_cv)
-        if isinstance(chain, ClassifierChain):
-            assert jaccard_score(Y, Y_pred_cv, average="samples") > 0.4
-        else:
-            assert mean_squared_error(Y, Y_pred_cv) < 0.25
+    if chain_type == "classifier":
+        chain = ClassifierChain(LogisticRegression(), chain_method=chain_method)
+    else:
+        chain = RegressorChain(Ridge())
+    chain.fit(X, Y)
+    chain_cv = clone(chain).set_params(cv=3)
+    chain_cv.fit(X, Y)
+    Y_pred_cv = chain_cv.predict(X)
+    Y_pred = chain.predict(X)
+
+    assert Y_pred_cv.shape == Y_pred.shape
+    assert not np.all(Y_pred == Y_pred_cv)
+    if isinstance(chain, ClassifierChain):
+        assert jaccard_score(Y, Y_pred_cv, average="samples") > 0.4
+    else:
+        assert mean_squared_error(Y, Y_pred_cv) < 0.25
 
 
 @pytest.mark.parametrize(
     "estimator",
     [
         RandomForestClassifier(n_estimators=2),
         MultiOutputClassifier(RandomForestClassifier(n_estimators=2)),
@@ -733,15 +762,17 @@
 
 @pytest.mark.parametrize("order_type", [list, np.array, tuple])
 def test_classifier_chain_tuple_order(order_type):
     X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]
     y = [[3, 2], [2, 3], [3, 2]]
     order = order_type([1, 0])
 
-    chain = ClassifierChain(RandomForestClassifier(), order=order)
+    chain = ClassifierChain(
+        RandomForestClassifier(n_estimators=2, random_state=0), order=order
+    )
 
     chain.fit(X, y)
     X_test = [[1.5, 2.5, 3.5]]
     y_test = [[3, 2]]
     assert_array_almost_equal(chain.predict(X_test), y_test)
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_naive_bayes.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_naive_bayes.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_pipeline.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_pipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """
 Test the pipeline module.
 """
+
 import itertools
 import re
 import shutil
 import time
+import warnings
 from tempfile import mkdtemp
 
 import joblib
 import numpy as np
 import pytest
 
 from sklearn.base import BaseEstimator, TransformerMixin, clone, is_classifier
@@ -17,27 +19,29 @@
 from sklearn.decomposition import PCA, TruncatedSVD
 from sklearn.dummy import DummyRegressor
 from sklearn.ensemble import (
     HistGradientBoostingClassifier,
     RandomForestClassifier,
     RandomTreesEmbedding,
 )
-from sklearn.exceptions import NotFittedError
+from sklearn.exceptions import NotFittedError, UnsetMetadataPassedError
 from sklearn.feature_extraction.text import CountVectorizer
 from sklearn.feature_selection import SelectKBest, f_classif
 from sklearn.impute import SimpleImputer
 from sklearn.linear_model import Lasso, LinearRegression, LogisticRegression
 from sklearn.metrics import accuracy_score, r2_score
 from sklearn.model_selection import train_test_split
 from sklearn.neighbors import LocalOutlierFactor
 from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline, make_union
 from sklearn.preprocessing import FunctionTransformer, StandardScaler
 from sklearn.svm import SVC
 from sklearn.tests.metadata_routing_common import (
+    ConsumingNoFitTransformTransformer,
     ConsumingTransformer,
+    _Registry,
     check_recorded_metadata,
 )
 from sklearn.utils._metadata_requests import COMPOSITE_METHODS, METHODS
 from sklearn.utils._testing import (
     MinimalClassifier,
     MinimalRegressor,
     MinimalTransformer,
@@ -1134,18 +1138,16 @@
             "pca__pca0",
             "pca__pca1",
         ],
         ft.get_feature_names_out(["f0", "f1", "f2", "f3"]),
     )
 
 
-def test_feature_union_passthrough_get_feature_names_out():
-    """Check that get_feature_names_out works with passthrough without
-    passing input_features.
-    """
+def test_feature_union_passthrough_get_feature_names_out_true():
+    """Check feature_names_out for verbose_feature_names_out=True (default)"""
     X = iris.data
     pca = PCA(n_components=2, svd_solver="randomized", random_state=0)
 
     ft = FeatureUnion([("pca", pca), ("passthrough", "passthrough")])
     ft.fit(X)
     assert_array_equal(
         [
@@ -1156,14 +1158,81 @@
             "passthrough__x2",
             "passthrough__x3",
         ],
         ft.get_feature_names_out(),
     )
 
 
+def test_feature_union_passthrough_get_feature_names_out_false():
+    """Check feature_names_out for verbose_feature_names_out=False"""
+    X = iris.data
+    pca = PCA(n_components=2, svd_solver="randomized", random_state=0)
+
+    ft = FeatureUnion(
+        [("pca", pca), ("passthrough", "passthrough")], verbose_feature_names_out=False
+    )
+    ft.fit(X)
+    assert_array_equal(
+        [
+            "pca0",
+            "pca1",
+            "x0",
+            "x1",
+            "x2",
+            "x3",
+        ],
+        ft.get_feature_names_out(),
+    )
+
+
+def test_feature_union_passthrough_get_feature_names_out_false_errors():
+    """Check get_feature_names_out and non-verbose names and colliding names."""
+    pd = pytest.importorskip("pandas")
+    X = pd.DataFrame([[1, 2], [2, 3]], columns=["a", "b"])
+
+    select_a = FunctionTransformer(
+        lambda X: X[["a"]], feature_names_out=lambda self, _: np.asarray(["a"])
+    )
+    union = FeatureUnion(
+        [("t1", StandardScaler()), ("t2", select_a)],
+        verbose_feature_names_out=False,
+    )
+    union.fit(X)
+
+    msg = re.escape(
+        "Output feature names: ['a'] are not unique. "
+        "Please set verbose_feature_names_out=True to add prefixes to feature names"
+    )
+
+    with pytest.raises(ValueError, match=msg):
+        union.get_feature_names_out()
+
+
+def test_feature_union_passthrough_get_feature_names_out_false_errors_overlap_over_5():
+    """Check get_feature_names_out with non-verbose names and >= 5 colliding names."""
+    pd = pytest.importorskip("pandas")
+    X = pd.DataFrame([list(range(10))], columns=[f"f{i}" for i in range(10)])
+
+    union = FeatureUnion(
+        [("t1", "passthrough"), ("t2", "passthrough")],
+        verbose_feature_names_out=False,
+    )
+
+    union.fit(X)
+
+    msg = re.escape(
+        "Output feature names: ['f0', 'f1', 'f2', 'f3', 'f4', ...] "
+        "are not unique. Please set verbose_feature_names_out=True to add prefixes to"
+        " feature names"
+    )
+
+    with pytest.raises(ValueError, match=msg):
+        union.get_feature_names_out()
+
+
 def test_step_name_validation():
     error_message_1 = r"Estimator names must not contain __: got \['a__q'\]"
     error_message_2 = r"Names provided are not unique: \['a', 'a'\]"
     error_message_3 = r"Estimator names conflict with constructor arguments: \['%s'\]"
     bad_steps1 = [("a__q", Mult(2)), ("b", Mult(3))]
     bad_steps2 = [("a", Mult(2)), ("a", Mult(3))]
     for cls, param in [(Pipeline, "steps"), (FeatureUnion, "transformer_list")]:
@@ -1462,35 +1531,59 @@
     fu = make_union(ss)
     ss.fit(X, y)
     assert fu.n_features_in_ == ss.n_features_in_ == 2
 
 
 def test_feature_union_fit_params():
     # Regression test for issue: #15117
-    class Dummy(TransformerMixin, BaseEstimator):
+    class DummyTransformer(TransformerMixin, BaseEstimator):
         def fit(self, X, y=None, **fit_params):
             if fit_params != {"a": 0}:
                 raise ValueError
             return self
 
         def transform(self, X, y=None):
             return X
 
     X, y = iris.data, iris.target
-    t = FeatureUnion([("dummy0", Dummy()), ("dummy1", Dummy())])
+    t = FeatureUnion([("dummy0", DummyTransformer()), ("dummy1", DummyTransformer())])
     with pytest.raises(ValueError):
         t.fit(X, y)
 
     with pytest.raises(ValueError):
         t.fit_transform(X, y)
 
     t.fit(X, y, a=0)
     t.fit_transform(X, y, a=0)
 
 
+def test_feature_union_fit_params_without_fit_transform():
+    # Test that metadata is passed correctly to underlying transformers that don't
+    # implement a `fit_transform` method when SLEP6 is not enabled.
+
+    class DummyTransformer(ConsumingNoFitTransformTransformer):
+        def fit(self, X, y=None, **fit_params):
+            if fit_params != {"metadata": 1}:
+                raise ValueError
+            return self
+
+    X, y = iris.data, iris.target
+    t = FeatureUnion(
+        [
+            ("nofittransform0", DummyTransformer()),
+            ("nofittransform1", DummyTransformer()),
+        ]
+    )
+
+    with pytest.raises(ValueError):
+        t.fit_transform(X, y, metadata=0)
+
+    t.fit_transform(X, y, metadata=1)
+
+
 def test_pipeline_missing_values_leniency():
     # check that pipeline let the missing values validation to
     # the underlying transformers and predictors.
     X, y = iris.data, iris.target
     mask = np.random.choice([1, 0], X.shape, p=[0.1, 0.9]).astype(bool)
     X[mask] = np.nan
     pipe = make_pipeline(SimpleImputer(), LogisticRegression())
@@ -1696,16 +1789,36 @@
     # fit with numpy array
     X_array = X.to_numpy()
     union = FeatureUnion([("pass", "passthrough")])
     union.fit(X_array)
     assert not hasattr(union, "feature_names_in_")
 
 
-# Test that metadata is routed correctly for pipelines
-# ====================================================
+# TODO(1.7): remove this test
+def test_pipeline_inverse_transform_Xt_deprecation():
+    X = np.random.RandomState(0).normal(size=(10, 5))
+    pipe = Pipeline([("pca", PCA(n_components=2))])
+    X = pipe.fit_transform(X)
+
+    with pytest.raises(TypeError, match="Missing required positional argument"):
+        pipe.inverse_transform()
+
+    with pytest.raises(TypeError, match="Cannot use both X and Xt. Use X only"):
+        pipe.inverse_transform(X=X, Xt=X)
+
+    with warnings.catch_warnings(record=True):
+        warnings.simplefilter("error")
+        pipe.inverse_transform(X)
+
+    with pytest.warns(FutureWarning, match="Xt was renamed X in version 1.5"):
+        pipe.inverse_transform(Xt=X)
+
+
+# Test that metadata is routed correctly for pipelines and FeatureUnion
+# =====================================================================
 
 
 class SimpleEstimator(BaseEstimator):
     # This class is used in this section for testing routing in the pipeline.
     # This class should have every set_{method}_request
     def fit(self, X, y, sample_weight=None, prop=None):
         assert sample_weight is not None
@@ -1820,15 +1933,15 @@
     X, y = [[1]], [1]
     sample_weight, prop = [1], "a"
     est = SimpleEstimator()
     # here not setting sample_weight request and leaving it as None
     pipeline = Pipeline([("estimator", est)])
     error_message = (
         "[sample_weight, prop] are passed but are not explicitly set as requested"
-        f" or not for SimpleEstimator.{method}"
+        f" or not requested for SimpleEstimator.{method}"
     )
     with pytest.raises(ValueError, match=re.escape(error_message)):
         try:
             # passing X, y positional as the first two arguments
             getattr(pipeline, method)(X, y, sample_weight=sample_weight, prop=prop)
         except TypeError:
             # not all methods accept y (like `predict`), so here we only
@@ -1868,9 +1981,102 @@
 
     It should just ignore and pass through the data on transform.
     """
     pipe = Pipeline([("trs", FunctionTransformer()), ("estimator", last_step)])
     assert pipe.fit([[1]], [1]).transform([[1], [2], [3]]) == [[1], [2], [3]]
 
 
+@pytest.mark.usefixtures("enable_slep006")
+def test_feature_union_metadata_routing_error():
+    """Test that the right error is raised when metadata is not requested."""
+    X = np.array([[0, 1], [2, 2], [4, 6]])
+    y = [1, 2, 3]
+    sample_weight, metadata = [1, 1, 1], "a"
+
+    # test lacking set_fit_request
+    feature_union = FeatureUnion([("sub_transformer", ConsumingTransformer())])
+
+    error_message = (
+        "[sample_weight, metadata] are passed but are not explicitly set as requested"
+        f" or not requested for {ConsumingTransformer.__name__}.fit"
+    )
+
+    with pytest.raises(UnsetMetadataPassedError, match=re.escape(error_message)):
+        feature_union.fit(X, y, sample_weight=sample_weight, metadata=metadata)
+
+    # test lacking set_transform_request
+    feature_union = FeatureUnion(
+        [
+            (
+                "sub_transformer",
+                ConsumingTransformer().set_fit_request(
+                    sample_weight=True, metadata=True
+                ),
+            )
+        ]
+    )
+
+    error_message = (
+        "[sample_weight, metadata] are passed but are not explicitly set as requested "
+        f"or not requested for {ConsumingTransformer.__name__}.transform"
+    )
+
+    with pytest.raises(UnsetMetadataPassedError, match=re.escape(error_message)):
+        feature_union.fit(
+            X, y, sample_weight=sample_weight, metadata=metadata
+        ).transform(X, sample_weight=sample_weight, metadata=metadata)
+
+
+@pytest.mark.usefixtures("enable_slep006")
+def test_feature_union_get_metadata_routing_without_fit():
+    """Test that get_metadata_routing() works regardless of the Child's
+    consumption of any metadata."""
+    feature_union = FeatureUnion([("sub_transformer", ConsumingTransformer())])
+    feature_union.get_metadata_routing()
+
+
+@pytest.mark.usefixtures("enable_slep006")
+@pytest.mark.parametrize(
+    "transformer", [ConsumingTransformer, ConsumingNoFitTransformTransformer]
+)
+def test_feature_union_metadata_routing(transformer):
+    """Test that metadata is routed correctly for FeatureUnion."""
+    X = np.array([[0, 1], [2, 2], [4, 6]])
+    y = [1, 2, 3]
+    sample_weight, metadata = [1, 1, 1], "a"
+
+    feature_union = FeatureUnion(
+        [
+            (
+                "sub_trans1",
+                transformer(registry=_Registry())
+                .set_fit_request(sample_weight=True, metadata=True)
+                .set_transform_request(sample_weight=True, metadata=True),
+            ),
+            (
+                "sub_trans2",
+                transformer(registry=_Registry())
+                .set_fit_request(sample_weight=True, metadata=True)
+                .set_transform_request(sample_weight=True, metadata=True),
+            ),
+        ]
+    )
+
+    kwargs = {"sample_weight": sample_weight, "metadata": metadata}
+    feature_union.fit(X, y, **kwargs)
+    feature_union.fit_transform(X, y, **kwargs)
+    feature_union.fit(X, y, **kwargs).transform(X, **kwargs)
+
+    for transformer in feature_union.transformer_list:
+        # access sub-transformer in (name, trans) with transformer[1]
+        registry = transformer[1].registry
+        assert len(registry)
+        for sub_trans in registry:
+            check_recorded_metadata(
+                obj=sub_trans,
+                method="fit",
+                **kwargs,
+            )
+
+
 # End of routing tests
 # ====================
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_public_functions.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_public_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -203,17 +203,19 @@
     "sklearn.linear_model.enet_path",
     "sklearn.linear_model.lars_path",
     "sklearn.linear_model.lars_path_gram",
     "sklearn.linear_model.lasso_path",
     "sklearn.linear_model.orthogonal_mp",
     "sklearn.linear_model.orthogonal_mp_gram",
     "sklearn.linear_model.ridge_regression",
+    "sklearn.manifold.locally_linear_embedding",
+    "sklearn.manifold.smacof",
+    "sklearn.manifold.spectral_embedding",
     "sklearn.manifold.trustworthiness",
     "sklearn.metrics.accuracy_score",
-    "sklearn.manifold.smacof",
     "sklearn.metrics.auc",
     "sklearn.metrics.average_precision_score",
     "sklearn.metrics.balanced_accuracy_score",
     "sklearn.metrics.brier_score_loss",
     "sklearn.metrics.calinski_harabasz_score",
     "sklearn.metrics.check_scoring",
     "sklearn.metrics.completeness_score",
@@ -351,14 +353,18 @@
     ("sklearn.cluster.k_means", "sklearn.cluster.KMeans"),
     ("sklearn.cluster.mean_shift", "sklearn.cluster.MeanShift"),
     ("sklearn.cluster.spectral_clustering", "sklearn.cluster.SpectralClustering"),
     ("sklearn.covariance.graphical_lasso", "sklearn.covariance.GraphicalLasso"),
     ("sklearn.covariance.ledoit_wolf", "sklearn.covariance.LedoitWolf"),
     ("sklearn.covariance.oas", "sklearn.covariance.OAS"),
     ("sklearn.decomposition.dict_learning", "sklearn.decomposition.DictionaryLearning"),
+    (
+        "sklearn.decomposition.dict_learning_online",
+        "sklearn.decomposition.MiniBatchDictionaryLearning",
+    ),
     ("sklearn.decomposition.fastica", "sklearn.decomposition.FastICA"),
     ("sklearn.decomposition.non_negative_factorization", "sklearn.decomposition.NMF"),
     ("sklearn.preprocessing.maxabs_scale", "sklearn.preprocessing.MaxAbsScaler"),
     ("sklearn.preprocessing.minmax_scale", "sklearn.preprocessing.MinMaxScaler"),
     ("sklearn.preprocessing.power_transform", "sklearn.preprocessing.PowerTransformer"),
     (
         "sklearn.preprocessing.quantile_transform",
```

### Comparing `scikit-learn-1.4.2/sklearn/tests/test_random_projection.py` & `scikit_learn-1.5.0rc1/sklearn/tests/test_random_projection.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/tree/__init__.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/_classes.py` & `scikit_learn-1.5.0rc1/sklearn/tree/_classes.py`

 * *Files 0% similar despite different names*

```diff
@@ -1383,30 +1383,31 @@
         return self
 
     def _compute_partial_dependence_recursion(self, grid, target_features):
         """Fast partial dependence computation.
 
         Parameters
         ----------
-        grid : ndarray of shape (n_samples, n_target_features)
+        grid : ndarray of shape (n_samples, n_target_features), dtype=np.float32
             The grid points on which the partial dependence should be
             evaluated.
-        target_features : ndarray of shape (n_target_features)
+        target_features : ndarray of shape (n_target_features), dtype=np.intp
             The set of target features for which the partial dependence
             should be evaluated.
 
         Returns
         -------
-        averaged_predictions : ndarray of shape (n_samples,)
+        averaged_predictions : ndarray of shape (n_samples,), dtype=np.float64
             The value of the partial dependence function on each grid point.
         """
         grid = np.asarray(grid, dtype=DTYPE, order="C")
         averaged_predictions = np.zeros(
             shape=grid.shape[0], dtype=np.float64, order="C"
         )
+        target_features = np.asarray(target_features, dtype=np.intp, order="C")
 
         self.tree_.compute_partial_dependence(
             grid, target_features, averaged_predictions
         )
         return averaged_predictions
 
     def _more_tags(self):
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_criterion.pxd` & `scikit_learn-1.5.0rc1/sklearn/tree/_criterion.pxd`

 * *Files 2% similar despite different names*

```diff
@@ -4,17 +4,15 @@
 #          Joel Nothman <joel.nothman@gmail.com>
 #          Arnaud Joly <arnaud.v.joly@gmail.com>
 #          Jacob Schreiber <jmschreiber91@gmail.com>
 #
 # License: BSD 3 clause
 
 # See _criterion.pyx for implementation details.
-cimport numpy as cnp
-
-from ..utils._typedefs cimport float64_t, intp_t
+from ..utils._typedefs cimport float64_t, int8_t, intp_t
 
 
 cdef class Criterion:
     # The criterion computes the impurity of a node and the reduction of
     # impurity of a split on that node. It also computes the output statistics
     # such as the mean in regression and class probabilities in classification.
 
@@ -78,21 +76,21 @@
         float64_t impurity_parent,
         float64_t impurity_left,
         float64_t impurity_right
     ) noexcept nogil
     cdef float64_t proxy_impurity_improvement(self) noexcept nogil
     cdef bint check_monotonicity(
             self,
-            cnp.int8_t monotonic_cst,
+            int8_t monotonic_cst,
             float64_t lower_bound,
             float64_t upper_bound,
     ) noexcept nogil
     cdef inline bint _check_monotonicity(
             self,
-            cnp.int8_t monotonic_cst,
+            int8_t monotonic_cst,
             float64_t lower_bound,
             float64_t upper_bound,
             float64_t sum_left,
             float64_t sum_right,
     ) noexcept nogil
 
 cdef class ClassificationCriterion(Criterion):
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_criterion.pyx` & `scikit_learn-1.5.0rc1/sklearn/tree/_criterion.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/_export.py` & `scikit_learn-1.5.0rc1/sklearn/tree/_export.py`

 * *Files 2% similar despite different names*

```diff
@@ -664,42 +664,51 @@
         for ann in anns:
             ann.update_bbox_position_size(renderer)
 
         if self.fontsize is None:
             # get figure to data transform
             # adjust fontsize to avoid overlap
             # get max box width and height
-            extents = [ann.get_bbox_patch().get_window_extent() for ann in anns]
+            extents = [
+                bbox_patch.get_window_extent()
+                for ann in anns
+                if (bbox_patch := ann.get_bbox_patch()) is not None
+            ]
             max_width = max([extent.width for extent in extents])
             max_height = max([extent.height for extent in extents])
             # width should be around scale_x in axis coordinates
             size = anns[0].get_fontsize() * min(
                 scale_x / max_width, scale_y / max_height
             )
             for ann in anns:
                 ann.set_fontsize(size)
 
         return anns
 
     def recurse(self, node, tree, ax, max_x, max_y, depth=0):
         import matplotlib.pyplot as plt
 
+        # kwargs for annotations without a bounding box
+        common_kwargs = dict(
+            zorder=100 - 10 * depth,
+            xycoords="axes fraction",
+        )
+        if self.fontsize is not None:
+            common_kwargs["fontsize"] = self.fontsize
+
+        # kwargs for annotations with a bounding box
         kwargs = dict(
-            bbox=self.bbox_args.copy(),
             ha="center",
             va="center",
-            zorder=100 - 10 * depth,
-            xycoords="axes fraction",
+            bbox=self.bbox_args.copy(),
             arrowprops=self.arrow_args.copy(),
+            **common_kwargs,
         )
         kwargs["arrowprops"]["edgecolor"] = plt.rcParams["text.color"]
 
-        if self.fontsize is not None:
-            kwargs["fontsize"] = self.fontsize
-
         # offset things by .5 to center them in plot
         xy = ((node.x + 0.5) / max_x, (max_y - node.y - 0.5) / max_y)
 
         if self.max_depth is None or depth <= self.max_depth:
             if self.filled:
                 kwargs["bbox"]["fc"] = self.get_fill_color(tree, node.tree.node_id)
             else:
@@ -710,14 +719,29 @@
                 ax.annotate(node.tree.label, xy, **kwargs)
             else:
                 xy_parent = (
                     (node.parent.x + 0.5) / max_x,
                     (max_y - node.parent.y - 0.5) / max_y,
                 )
                 ax.annotate(node.tree.label, xy_parent, xy, **kwargs)
+
+                # Draw True/False labels if parent is root node
+                if node.parent.parent is None:
+                    # Adjust the position for the text to be slightly above the arrow
+                    text_pos = (
+                        (xy_parent[0] + xy[0]) / 2,
+                        (xy_parent[1] + xy[1]) / 2,
+                    )
+                    # Annotate the arrow with the edge label to indicate the child
+                    # where the sample-split condition is satisfied
+                    if node.parent.left() == node:
+                        label_text, label_ha = ("True  ", "right")
+                    else:
+                        label_text, label_ha = ("  False", "left")
+                    ax.annotate(label_text, text_pos, ha=label_ha, **common_kwargs)
             for child in node.children:
                 self.recurse(child, tree, ax, max_x, max_y, depth=depth + 1)
 
         else:
             xy_parent = (
                 (node.parent.x + 0.5) / max_x,
                 (max_y - node.parent.y - 0.5) / max_y,
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_reingold_tilford.py` & `scikit_learn-1.5.0rc1/sklearn/tree/_reingold_tilford.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/_splitter.pxd` & `scikit_learn-1.5.0rc1/sklearn/tree/_splitter.pxd`

 * *Files 6% similar despite different names*

```diff
@@ -4,19 +4,18 @@
 #          Joel Nothman <joel.nothman@gmail.com>
 #          Arnaud Joly <arnaud.v.joly@gmail.com>
 #          Jacob Schreiber <jmschreiber91@gmail.com>
 #
 # License: BSD 3 clause
 
 # See _splitter.pyx for details.
-cimport numpy as cnp
-
 from ._criterion cimport Criterion
+from ._tree cimport ParentInfo
 
-from ..utils._typedefs cimport float32_t, float64_t, intp_t, int32_t, uint32_t
+from ..utils._typedefs cimport float32_t, float64_t, intp_t, int8_t, int32_t, uint32_t
 
 
 cdef struct SplitRecord:
     # Data to track sample split
     intp_t feature         # Which feature to split on.
     intp_t pos             # Split samples array at the given position,
     #                      # i.e. count of samples below threshold for feature.
@@ -24,15 +23,15 @@
     float64_t threshold       # Threshold to split at.
     float64_t improvement     # Impurity improvement given parent node.
     float64_t impurity_left   # Impurity of the left split.
     float64_t impurity_right  # Impurity of the right split.
     float64_t lower_bound     # Lower bound on value of both children for monotonicity
     float64_t upper_bound     # Upper bound on value of both children for monotonicity
     unsigned char missing_go_to_left  # Controls if missing values go to the left node.
-    intp_t n_missing       # Number of missing values for the feature being split on
+    intp_t n_missing            # Number of missing values for the feature being split on
 
 cdef class Splitter:
     # The splitter searches in the input space for a feature and a threshold
     # to split the samples samples[start:end].
     #
     # The impurity computations are delegated to a criterion object.
 
@@ -58,15 +57,15 @@
 
     cdef const float64_t[:, ::1] y
     # Monotonicity constraints for each feature.
     # The encoding is as follows:
     #   -1: monotonic decrease
     #    0: no constraint
     #   +1: monotonic increase
-    cdef const cnp.int8_t[:] monotonic_cst
+    cdef const int8_t[:] monotonic_cst
     cdef bint with_monotonic_cst
     cdef const float64_t[:] sample_weight
 
     # The samples vector `samples` is maintained by the Splitter object such
     # that the samples contained in a node are contiguous. With this setting,
     # `node_split` reorganizes the node samples `samples[start:end]` in two
     # subsets `samples[start:pos]` and `samples[pos:end]`.
@@ -96,19 +95,16 @@
         intp_t start,
         intp_t end,
         float64_t* weighted_n_node_samples
     ) except -1 nogil
 
     cdef int node_split(
         self,
-        float64_t impurity,   # Impurity of the node
+        ParentInfo* parent,
         SplitRecord* split,
-        intp_t* n_constant_features,
-        float64_t lower_bound,
-        float64_t upper_bound,
     ) except -1 nogil
 
     cdef void node_value(self, float64_t* dest) noexcept nogil
 
     cdef void clip_node_value(self, float64_t* dest, float64_t lower_bound, float64_t upper_bound) noexcept nogil
 
     cdef float64_t node_impurity(self) noexcept nogil
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_splitter.pyx` & `scikit_learn-1.5.0rc1/sklearn/tree/_splitter.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -7,33 +7,29 @@
 #          Arnaud Joly <arnaud.v.joly@gmail.com>
 #          Joel Nothman <joel.nothman@gmail.com>
 #          Fares Hedayati <fares.hedayati@gmail.com>
 #          Jacob Schreiber <jmschreiber91@gmail.com>
 #
 # License: BSD 3 clause
 
-cimport numpy as cnp
-
-from ._criterion cimport Criterion
-
+from cython cimport final
+from libc.math cimport isnan
 from libc.stdlib cimport qsort
 from libc.string cimport memcpy
-from libc.math cimport isnan
-from cython cimport final
-
-import numpy as np
-
-from scipy.sparse import issparse
 
+from ._criterion cimport Criterion
 from ._utils cimport log
 from ._utils cimport rand_int
 from ._utils cimport rand_uniform
 from ._utils cimport RAND_R_MAX
+from ..utils._typedefs cimport int8_t
+
+import numpy as np
+from scipy.sparse import issparse
 
-cnp.import_array()
 
 cdef float64_t INFINITY = np.inf
 
 # Mitigate precision differences between 32 bit and 64 bit
 cdef float32_t FEATURE_THRESHOLD = 1e-7
 
 # Constant to switch between algorithm non zero value extract algorithm
@@ -60,15 +56,15 @@
     def __cinit__(
         self,
         Criterion criterion,
         intp_t max_features,
         intp_t min_samples_leaf,
         float64_t min_weight_leaf,
         object random_state,
-        const cnp.int8_t[:] monotonic_cst,
+        const int8_t[:] monotonic_cst,
     ):
         """
         Parameters
         ----------
         criterion : Criterion
             The criterion to measure the quality of a split.
 
@@ -84,15 +80,15 @@
         min_weight_leaf : float64_t
             The minimal weight each leaf can have, where the weight is the sum
             of the weights of each sample in it.
 
         random_state : object
             The user inputted random state to be used for pseudo-randomness
 
-        monotonic_cst : const cnp.int8_t[:]
+        monotonic_cst : const int8_t[:]
             Monotonicity constraints
 
         """
 
         self.criterion = criterion
 
         self.n_samples = 0
@@ -189,16 +185,20 @@
         self.y = y
 
         self.sample_weight = sample_weight
         if missing_values_in_feature_mask is not None:
             self.criterion.init_sum_missing()
         return 0
 
-    cdef int node_reset(self, intp_t start, intp_t end,
-                        float64_t* weighted_n_node_samples) except -1 nogil:
+    cdef int node_reset(
+        self,
+        intp_t start,
+        intp_t end,
+        float64_t* weighted_n_node_samples
+    ) except -1 nogil:
         """Reset splitter on node samples[start:end].
 
         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
 
         Parameters
         ----------
@@ -223,19 +223,16 @@
         )
 
         weighted_n_node_samples[0] = self.criterion.weighted_n_node_samples
         return 0
 
     cdef int node_split(
         self,
-        float64_t impurity,
+        ParentInfo* parent_record,
         SplitRecord* split,
-        intp_t* n_constant_features,
-        float64_t lower_bound,
-        float64_t upper_bound,
     ) except -1 nogil:
 
         """Find the best split on node samples[start:end].
 
         This is a placeholder method. The majority of computation will be done
         here.
 
@@ -293,21 +290,18 @@
     DensePartitioner
     SparsePartitioner
 
 cdef inline int node_split_best(
     Splitter splitter,
     Partitioner partitioner,
     Criterion criterion,
-    float64_t impurity,
     SplitRecord* split,
-    intp_t* n_constant_features,
+    ParentInfo* parent_record,
     bint with_monotonic_cst,
-    const cnp.int8_t[:] monotonic_cst,
-    float64_t lower_bound,
-    float64_t upper_bound,
+    const int8_t[:] monotonic_cst,
 ) except -1 nogil:
     """Find the best split on node samples[start:end]
 
     Returns -1 in case of failure to allocate memory (and raise MemoryError)
     or 0 otherwise.
     """
     # Find the best split
@@ -331,25 +325,29 @@
     cdef float64_t min_weight_leaf = splitter.min_weight_leaf
     cdef uint32_t* random_state = &splitter.rand_r_state
 
     cdef SplitRecord best_split, current_split
     cdef float64_t current_proxy_improvement = -INFINITY
     cdef float64_t best_proxy_improvement = -INFINITY
 
+    cdef float64_t impurity = parent_record.impurity
+    cdef float64_t lower_bound = parent_record.lower_bound
+    cdef float64_t upper_bound = parent_record.upper_bound
+
     cdef intp_t f_i = n_features
     cdef intp_t f_j
     cdef intp_t p
     cdef intp_t p_prev
 
     cdef intp_t n_visited_features = 0
     # Number of features discovered to be constant during the split search
     cdef intp_t n_found_constants = 0
     # Number of features known to be constant and drawn without replacement
     cdef intp_t n_drawn_constants = 0
-    cdef intp_t n_known_constants = n_constant_features[0]
+    cdef intp_t n_known_constants = parent_record.n_constant_features
     # n_total_constants = n_known_constants + n_found_constants
     cdef intp_t n_total_constants = n_known_constants
 
     _init_split(&best_split, end)
 
     partitioner.init_node_split(start, end)
 
@@ -551,25 +549,25 @@
 
     # Copy newly found constant features
     memcpy(&constant_features[n_known_constants],
            &features[n_known_constants],
            sizeof(intp_t) * n_found_constants)
 
     # Return values
+    parent_record.n_constant_features = n_total_constants
     split[0] = best_split
-    n_constant_features[0] = n_total_constants
     return 0
 
 
 # Sort n-element arrays pointed to by feature_values and samples, simultaneously,
 # by the values in feature_values. Algorithm: Introsort (Musser, SP&E, 1997).
 cdef inline void sort(float32_t* feature_values, intp_t* samples, intp_t n) noexcept nogil:
     if n == 0:
         return
-    cdef int maxd = 2 * <int>log(n)
+    cdef intp_t maxd = 2 * <intp_t>log(n)
     introsort(feature_values, samples, n, maxd)
 
 
 cdef inline void swap(float32_t* feature_values, intp_t* samples,
                       intp_t i, intp_t j) noexcept nogil:
     # Helper for sort
     feature_values[i], feature_values[j] = feature_values[j], feature_values[i]
@@ -595,15 +593,15 @@
     else:
         return b
 
 
 # Introsort with median of 3 pivot selection and 3-way partition function
 # (robust to repeated elements, e.g. lots of zero features).
 cdef void introsort(float32_t* feature_values, intp_t *samples,
-                    intp_t n, int maxd) noexcept nogil:
+                    intp_t n, intp_t maxd) noexcept nogil:
     cdef float32_t pivot
     cdef intp_t i, l, r
 
     while n > 1:
         if maxd <= 0:   # max depth limit exceeded ("gone quadratic")
             heapsort(feature_values, samples, n)
             return
@@ -673,21 +671,18 @@
         sift_down(feature_values, samples, 0, end)
         end = end - 1
 
 cdef inline int node_split_random(
     Splitter splitter,
     Partitioner partitioner,
     Criterion criterion,
-    float64_t impurity,
     SplitRecord* split,
-    intp_t* n_constant_features,
+    ParentInfo* parent_record,
     bint with_monotonic_cst,
-    const cnp.int8_t[:] monotonic_cst,
-    float64_t lower_bound,
-    float64_t upper_bound,
+    const int8_t[:] monotonic_cst,
 ) except -1 nogil:
     """Find the best random split on node samples[start:end]
 
     Returns -1 in case of failure to allocate memory (and raise MemoryError)
     or 0 otherwise.
     """
     # Draw random splits and pick the best
@@ -703,21 +698,25 @@
     cdef float64_t min_weight_leaf = splitter.min_weight_leaf
     cdef uint32_t* random_state = &splitter.rand_r_state
 
     cdef SplitRecord best_split, current_split
     cdef float64_t current_proxy_improvement = - INFINITY
     cdef float64_t best_proxy_improvement = - INFINITY
 
+    cdef float64_t impurity = parent_record.impurity
+    cdef float64_t lower_bound = parent_record.lower_bound
+    cdef float64_t upper_bound = parent_record.upper_bound
+
     cdef intp_t f_i = n_features
     cdef intp_t f_j
     # Number of features discovered to be constant during the split search
     cdef intp_t n_found_constants = 0
     # Number of features known to be constant and drawn without replacement
     cdef intp_t n_drawn_constants = 0
-    cdef intp_t n_known_constants = n_constant_features[0]
+    cdef intp_t n_known_constants = parent_record.n_constant_features
     # n_total_constants = n_known_constants + n_found_constants
     cdef intp_t n_total_constants = n_known_constants
     cdef intp_t n_visited_features = 0
     cdef float32_t min_feature_value
     cdef float32_t max_feature_value
 
     _init_split(&best_split, end)
@@ -853,16 +852,16 @@
 
     # Copy newly found constant features
     memcpy(&constant_features[n_known_constants],
            &features[n_known_constants],
            sizeof(intp_t) * n_found_constants)
 
     # Return values
+    parent_record.n_constant_features = n_total_constants
     split[0] = best_split
-    n_constant_features[0] = n_total_constants
     return 0
 
 
 @final
 cdef class DensePartitioner:
     """Partitioner specialized for dense data.
 
@@ -1342,15 +1341,19 @@
                                          samples, self.start, self.end,
                                          index_to_samples,
                                          feature_values,
                                          &self.end_negative, &self.start_positive)
 
 
 cdef int compare_SIZE_t(const void* a, const void* b) noexcept nogil:
-    """Comparison function for sort."""
+    """Comparison function for sort.
+
+    This must return an `int` as it is used by stdlib's qsort, which expects
+    an `int` return value.
+    """
     return <int>((<intp_t*>a)[0] - (<intp_t*>b)[0])
 
 
 cdef inline void binary_search(const int32_t[::1] sorted_array,
                                int32_t start, int32_t end,
                                intp_t value, intp_t* index,
                                int32_t* new_start) noexcept nogil:
@@ -1506,31 +1509,25 @@
         Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)
         self.partitioner = DensePartitioner(
             X, self.samples, self.feature_values, missing_values_in_feature_mask
         )
 
     cdef int node_split(
             self,
-            float64_t impurity,
+            ParentInfo* parent_record,
             SplitRecord* split,
-            intp_t* n_constant_features,
-            float64_t lower_bound,
-            float64_t upper_bound
     ) except -1 nogil:
         return node_split_best(
             self,
             self.partitioner,
             self.criterion,
-            impurity,
             split,
-            n_constant_features,
+            parent_record,
             self.with_monotonic_cst,
             self.monotonic_cst,
-            lower_bound,
-            upper_bound
         )
 
 cdef class BestSparseSplitter(Splitter):
     """Splitter for finding the best split, using the sparse data."""
     cdef SparsePartitioner partitioner
     cdef int init(
         self,
@@ -1542,31 +1539,25 @@
         Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)
         self.partitioner = SparsePartitioner(
             X, self.samples, self.n_samples, self.feature_values, missing_values_in_feature_mask
         )
 
     cdef int node_split(
             self,
-            float64_t impurity,
+            ParentInfo* parent_record,
             SplitRecord* split,
-            intp_t* n_constant_features,
-            float64_t lower_bound,
-            float64_t upper_bound
     ) except -1 nogil:
         return node_split_best(
             self,
             self.partitioner,
             self.criterion,
-            impurity,
             split,
-            n_constant_features,
+            parent_record,
             self.with_monotonic_cst,
             self.monotonic_cst,
-            lower_bound,
-            upper_bound
         )
 
 cdef class RandomSplitter(Splitter):
     """Splitter for finding the best random split on dense data."""
     cdef DensePartitioner partitioner
     cdef int init(
         self,
@@ -1578,31 +1569,25 @@
         Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)
         self.partitioner = DensePartitioner(
             X, self.samples, self.feature_values, missing_values_in_feature_mask
         )
 
     cdef int node_split(
             self,
-            float64_t impurity,
+            ParentInfo* parent_record,
             SplitRecord* split,
-            intp_t* n_constant_features,
-            float64_t lower_bound,
-            float64_t upper_bound
     ) except -1 nogil:
         return node_split_random(
             self,
             self.partitioner,
             self.criterion,
-            impurity,
             split,
-            n_constant_features,
+            parent_record,
             self.with_monotonic_cst,
             self.monotonic_cst,
-            lower_bound,
-            upper_bound
         )
 
 cdef class RandomSparseSplitter(Splitter):
     """Splitter for finding the best random split, using the sparse data."""
     cdef SparsePartitioner partitioner
     cdef int init(
         self,
@@ -1613,25 +1598,19 @@
     ) except -1:
         Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)
         self.partitioner = SparsePartitioner(
             X, self.samples, self.n_samples, self.feature_values, missing_values_in_feature_mask
         )
     cdef int node_split(
             self,
-            float64_t impurity,
+            ParentInfo* parent_record,
             SplitRecord* split,
-            intp_t* n_constant_features,
-            float64_t lower_bound,
-            float64_t upper_bound
     ) except -1 nogil:
         return node_split_random(
             self,
             self.partitioner,
             self.criterion,
-            impurity,
             split,
-            n_constant_features,
+            parent_record,
             self.with_monotonic_cst,
             self.monotonic_cst,
-            lower_bound,
-            upper_bound
         )
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_tree.pxd` & `scikit_learn-1.5.0rc1/sklearn/tree/_tree.pxd`

 * *Files 8% similar despite different names*

```diff
@@ -27,14 +27,23 @@
     float64_t threshold                  # Threshold value at the node
     float64_t impurity                   # Impurity of the node (i.e., the value of the criterion)
     intp_t n_node_samples                # Number of samples at the node
     float64_t weighted_n_node_samples    # Weighted number of samples at the node
     unsigned char missing_go_to_left     # Whether features have missing values
 
 
+cdef struct ParentInfo:
+    # Structure to store information about the parent of a node
+    # This is passed to the splitter, to provide information about the previous split
+
+    float64_t lower_bound           # the lower bound of the parent's impurity
+    float64_t upper_bound           # the upper bound of the parent's impurity
+    float64_t impurity              # the impurity of the parent
+    intp_t n_constant_features      # the number of constant features found in parent
+
 cdef class Tree:
     # The Tree object is a binary tree structure constructed by the
     # TreeBuilder. The tree structure is used for predictions and
     # feature importances.
 
     # Input/Output layout
     cdef public intp_t n_features        # Number of features in X
@@ -44,15 +53,15 @@
 
     # Inner structures: values are stored separately from node structure,
     # since size is determined at runtime.
     cdef public intp_t max_depth         # Max depth of the tree
     cdef public intp_t node_count        # Counter for node IDs
     cdef public intp_t capacity          # Capacity of tree, in terms of nodes
     cdef Node* nodes                     # Array of nodes
-    cdef float64_t* value                   # (capacity, n_outputs, max_n_classes) array of values
+    cdef float64_t* value                # (capacity, n_outputs, max_n_classes) array of values
     cdef intp_t value_stride             # = n_outputs * max_n_classes
 
     # Methods
     cdef intp_t _add_node(self, intp_t parent, bint is_left, bint is_leaf,
                           intp_t feature, float64_t threshold, float64_t impurity,
                           intp_t n_node_samples,
                           float64_t weighted_n_node_samples,
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_tree.pyx` & `scikit_learn-1.5.0rc1/sklearn/tree/_tree.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -59,31 +59,37 @@
 from numpy import float32 as DTYPE
 from numpy import float64 as DOUBLE
 
 cdef float64_t INFINITY = np.inf
 cdef float64_t EPSILON = np.finfo('double').eps
 
 # Some handy constants (BestFirstTreeBuilder)
-cdef int IS_FIRST = 1
-cdef int IS_NOT_FIRST = 0
-cdef int IS_LEFT = 1
-cdef int IS_NOT_LEFT = 0
+cdef bint IS_FIRST = 1
+cdef bint IS_NOT_FIRST = 0
+cdef bint IS_LEFT = 1
+cdef bint IS_NOT_LEFT = 0
 
 TREE_LEAF = -1
 TREE_UNDEFINED = -2
 cdef intp_t _TREE_LEAF = TREE_LEAF
 cdef intp_t _TREE_UNDEFINED = TREE_UNDEFINED
 
 # Build the corresponding numpy dtype for Node.
 # This works by casting `dummy` to an array of Node of length 1, which numpy
 # can construct a `dtype`-object for. See https://stackoverflow.com/q/62448946
 # for a more detailed explanation.
 cdef Node dummy
 NODE_DTYPE = np.asarray(<Node[:1]>(&dummy)).dtype
 
+cdef inline void _init_parent_record(ParentInfo* record) noexcept nogil:
+    record.n_constant_features = 0
+    record.impurity = INFINITY
+    record.lower_bound = -INFINITY
+    record.upper_bound = INFINITY
+
 # =============================================================================
 # TreeBuilder
 # =============================================================================
 
 cdef class TreeBuilder:
     """Interface for different tree building strategies."""
 
@@ -173,18 +179,18 @@
     ):
         """Build a decision tree from the training set (X, y)."""
 
         # check input
         X, y, sample_weight = self._check_input(X, y, sample_weight)
 
         # Initial capacity
-        cdef int init_capacity
+        cdef intp_t init_capacity
 
         if tree.max_depth <= 10:
-            init_capacity = <int> (2 ** (tree.max_depth + 1)) - 1
+            init_capacity = <intp_t> (2 ** (tree.max_depth + 1)) - 1
         else:
             init_capacity = 2047
 
         tree._resize(init_capacity)
 
         # Parameters
         cdef Splitter splitter = self.splitter
@@ -203,31 +209,30 @@
         cdef intp_t parent
         cdef bint is_left
         cdef intp_t n_node_samples = splitter.n_samples
         cdef float64_t weighted_n_node_samples
         cdef SplitRecord split
         cdef intp_t node_id
 
-        cdef float64_t impurity = INFINITY
-        cdef float64_t lower_bound
-        cdef float64_t upper_bound
         cdef float64_t middle_value
         cdef float64_t left_child_min
         cdef float64_t left_child_max
         cdef float64_t right_child_min
         cdef float64_t right_child_max
-        cdef intp_t n_constant_features
         cdef bint is_leaf
         cdef bint first = 1
         cdef intp_t max_depth_seen = -1
         cdef int rc = 0
 
         cdef stack[StackRecord] builder_stack
         cdef StackRecord stack_record
 
+        cdef ParentInfo parent_record
+        _init_parent_record(&parent_record)
+
         with nogil:
             # push root node onto stack
             builder_stack.push({
                 "start": 0,
                 "end": n_node_samples,
                 "depth": 0,
                 "parent": _TREE_UNDEFINED,
@@ -243,90 +248,87 @@
                 builder_stack.pop()
 
                 start = stack_record.start
                 end = stack_record.end
                 depth = stack_record.depth
                 parent = stack_record.parent
                 is_left = stack_record.is_left
-                impurity = stack_record.impurity
-                n_constant_features = stack_record.n_constant_features
-                lower_bound = stack_record.lower_bound
-                upper_bound = stack_record.upper_bound
+                parent_record.impurity = stack_record.impurity
+                parent_record.n_constant_features = stack_record.n_constant_features
+                parent_record.lower_bound = stack_record.lower_bound
+                parent_record.upper_bound = stack_record.upper_bound
 
                 n_node_samples = end - start
                 splitter.node_reset(start, end, &weighted_n_node_samples)
 
                 is_leaf = (depth >= max_depth or
                            n_node_samples < min_samples_split or
                            n_node_samples < 2 * min_samples_leaf or
                            weighted_n_node_samples < 2 * min_weight_leaf)
 
                 if first:
-                    impurity = splitter.node_impurity()
+                    parent_record.impurity = splitter.node_impurity()
                     first = 0
 
                 # impurity == 0 with tolerance due to rounding errors
-                is_leaf = is_leaf or impurity <= EPSILON
+                is_leaf = is_leaf or parent_record.impurity <= EPSILON
 
                 if not is_leaf:
                     splitter.node_split(
-                        impurity,
+                        &parent_record,
                         &split,
-                        &n_constant_features,
-                        lower_bound,
-                        upper_bound
                     )
                     # If EPSILON=0 in the below comparison, float precision
                     # issues stop splitting, producing trees that are
                     # dissimilar to v0.18
                     is_leaf = (is_leaf or split.pos >= end or
                                (split.improvement + EPSILON <
                                 min_impurity_decrease))
 
                 node_id = tree._add_node(parent, is_left, is_leaf, split.feature,
-                                         split.threshold, impurity, n_node_samples,
-                                         weighted_n_node_samples,
+                                         split.threshold, parent_record.impurity,
+                                         n_node_samples, weighted_n_node_samples,
                                          split.missing_go_to_left)
 
                 if node_id == INTPTR_MAX:
                     rc = -1
                     break
 
                 # Store value for all nodes, to facilitate tree/model
                 # inspection and interpretation
                 splitter.node_value(tree.value + node_id * tree.value_stride)
                 if splitter.with_monotonic_cst:
-                    splitter.clip_node_value(tree.value + node_id * tree.value_stride, lower_bound, upper_bound)
+                    splitter.clip_node_value(tree.value + node_id * tree.value_stride, parent_record.lower_bound, parent_record.upper_bound)
 
                 if not is_leaf:
                     if (
                         not splitter.with_monotonic_cst or
                         splitter.monotonic_cst[split.feature] == 0
                     ):
                         # Split on a feature with no monotonicity constraint
 
                         # Current bounds must always be propagated to both children.
                         # If a monotonic constraint is active, bounds are used in
                         # node value clipping.
-                        left_child_min = right_child_min = lower_bound
-                        left_child_max = right_child_max = upper_bound
+                        left_child_min = right_child_min = parent_record.lower_bound
+                        left_child_max = right_child_max = parent_record.upper_bound
                     elif splitter.monotonic_cst[split.feature] == 1:
                         # Split on a feature with monotonic increase constraint
-                        left_child_min = lower_bound
-                        right_child_max = upper_bound
+                        left_child_min = parent_record.lower_bound
+                        right_child_max = parent_record.upper_bound
 
                         # Lower bound for right child and upper bound for left child
                         # are set to the same value.
                         middle_value = splitter.criterion.middle_value()
                         right_child_min = middle_value
                         left_child_max = middle_value
                     else:  # i.e. splitter.monotonic_cst[split.feature] == -1
                         # Split on a feature with monotonic decrease constraint
-                        right_child_min = lower_bound
-                        left_child_max = upper_bound
+                        right_child_min = parent_record.lower_bound
+                        left_child_max = parent_record.upper_bound
 
                         # Lower bound for left child and upper bound for right child
                         # are set to the same value.
                         middle_value = splitter.criterion.middle_value()
                         left_child_min = middle_value
                         right_child_max = middle_value
 
@@ -334,28 +336,28 @@
                     builder_stack.push({
                         "start": split.pos,
                         "end": end,
                         "depth": depth + 1,
                         "parent": node_id,
                         "is_left": 0,
                         "impurity": split.impurity_right,
-                        "n_constant_features": n_constant_features,
+                        "n_constant_features": parent_record.n_constant_features,
                         "lower_bound": right_child_min,
                         "upper_bound": right_child_max,
                     })
 
                     # Push left child on stack
                     builder_stack.push({
                         "start": start,
                         "end": split.pos,
                         "depth": depth + 1,
                         "parent": node_id,
                         "is_left": 1,
                         "impurity": split.impurity_left,
-                        "n_constant_features": n_constant_features,
+                        "n_constant_features": parent_record.n_constant_features,
                         "lower_bound": left_child_min,
                         "upper_bound": left_child_max,
                     })
 
                 if depth > max_depth_seen:
                     max_depth_seen = depth
 
@@ -454,32 +456,33 @@
         cdef intp_t n_node_samples = splitter.n_samples
         cdef intp_t max_split_nodes = max_leaf_nodes - 1
         cdef bint is_leaf
         cdef intp_t max_depth_seen = -1
         cdef int rc = 0
         cdef Node* node
 
+        cdef ParentInfo parent_record
+        _init_parent_record(&parent_record)
+
         # Initial capacity
         cdef intp_t init_capacity = max_split_nodes + max_leaf_nodes
         tree._resize(init_capacity)
 
         with nogil:
             # add root to frontier
             rc = self._add_split_node(
                 splitter=splitter,
                 tree=tree,
                 start=0,
                 end=n_node_samples,
-                impurity=INFINITY,
                 is_first=IS_FIRST,
                 is_left=IS_LEFT,
                 parent=NULL,
                 depth=0,
-                lower_bound=-INFINITY,
-                upper_bound=INFINITY,
+                parent_record=&parent_record,
                 res=&split_node_left,
             )
             if rc >= 0:
                 _add_to_frontier(split_node_left, frontier)
 
             while not frontier.empty():
                 pop_heap(frontier.begin(), frontier.end(), &_compare_records)
@@ -529,47 +532,49 @@
                         left_child_min = record.middle_value
                         right_child_max = record.middle_value
 
                     # Decrement number of split nodes available
                     max_split_nodes -= 1
 
                     # Compute left split node
+                    parent_record.lower_bound = left_child_min
+                    parent_record.upper_bound = left_child_max
+                    parent_record.impurity = record.impurity_left
                     rc = self._add_split_node(
                         splitter=splitter,
                         tree=tree,
                         start=record.start,
                         end=record.pos,
-                        impurity=record.impurity_left,
                         is_first=IS_NOT_FIRST,
                         is_left=IS_LEFT,
                         parent=node,
                         depth=record.depth + 1,
-                        lower_bound=left_child_min,
-                        upper_bound=left_child_max,
+                        parent_record=&parent_record,
                         res=&split_node_left,
                     )
                     if rc == -1:
                         break
 
                     # tree.nodes may have changed
                     node = &tree.nodes[record.node_id]
 
                     # Compute right split node
+                    parent_record.lower_bound = right_child_min
+                    parent_record.upper_bound = right_child_max
+                    parent_record.impurity = record.impurity_right
                     rc = self._add_split_node(
                         splitter=splitter,
                         tree=tree,
                         start=record.pos,
                         end=record.end,
-                        impurity=record.impurity_right,
                         is_first=IS_NOT_FIRST,
                         is_left=IS_NOT_LEFT,
                         parent=node,
                         depth=record.depth + 1,
-                        lower_bound=right_child_min,
-                        upper_bound=right_child_max,
+                        parent_record=&parent_record,
                         res=&split_node_right,
                     )
                     if rc == -1:
                         break
 
                     # Add nodes to queue
                     _add_to_frontier(split_node_left, frontier)
@@ -589,80 +594,77 @@
 
     cdef inline int _add_split_node(
         self,
         Splitter splitter,
         Tree tree,
         intp_t start,
         intp_t end,
-        float64_t impurity,
         bint is_first,
         bint is_left,
         Node* parent,
         intp_t depth,
-        float64_t lower_bound,
-        float64_t upper_bound,
+        ParentInfo* parent_record,
         FrontierRecord* res
     ) except -1 nogil:
         """Adds node w/ partition ``[start, end)`` to the frontier. """
         cdef SplitRecord split
         cdef intp_t node_id
         cdef intp_t n_node_samples
-        cdef intp_t n_constant_features = 0
         cdef float64_t min_impurity_decrease = self.min_impurity_decrease
         cdef float64_t weighted_n_node_samples
         cdef bint is_leaf
 
         splitter.node_reset(start, end, &weighted_n_node_samples)
 
+        # reset n_constant_features for this specific split before beginning split search
+        parent_record.n_constant_features = 0
+
         if is_first:
-            impurity = splitter.node_impurity()
+            parent_record.impurity = splitter.node_impurity()
 
         n_node_samples = end - start
         is_leaf = (depth >= self.max_depth or
                    n_node_samples < self.min_samples_split or
                    n_node_samples < 2 * self.min_samples_leaf or
                    weighted_n_node_samples < 2 * self.min_weight_leaf or
-                   impurity <= EPSILON  # impurity == 0 with tolerance
+                   parent_record.impurity <= EPSILON  # impurity == 0 with tolerance
                    )
 
         if not is_leaf:
             splitter.node_split(
-                impurity,
+                parent_record,
                 &split,
-                &n_constant_features,
-                lower_bound,
-                upper_bound
             )
             # If EPSILON=0 in the below comparison, float precision issues stop
             # splitting early, producing trees that are dissimilar to v0.18
             is_leaf = (is_leaf or split.pos >= end or
                        split.improvement + EPSILON < min_impurity_decrease)
 
         node_id = tree._add_node(parent - tree.nodes
                                  if parent != NULL
                                  else _TREE_UNDEFINED,
                                  is_left, is_leaf,
-                                 split.feature, split.threshold, impurity, n_node_samples,
-                                 weighted_n_node_samples,
+                                 split.feature, split.threshold, parent_record.impurity,
+                                 n_node_samples, weighted_n_node_samples,
                                  split.missing_go_to_left)
         if node_id == INTPTR_MAX:
             return -1
 
         # compute values also for split nodes (might become leafs later).
         splitter.node_value(tree.value + node_id * tree.value_stride)
         if splitter.with_monotonic_cst:
-            splitter.clip_node_value(tree.value + node_id * tree.value_stride, lower_bound, upper_bound)
+            splitter.clip_node_value(tree.value + node_id * tree.value_stride, parent_record.lower_bound, parent_record.upper_bound)
 
         res.node_id = node_id
         res.start = start
         res.end = end
         res.depth = depth
-        res.impurity = impurity
-        res.lower_bound = lower_bound
-        res.upper_bound = upper_bound
+        res.impurity = parent_record.impurity
+        res.lower_bound = parent_record.lower_bound
+        res.upper_bound = parent_record.upper_bound
         res.middle_value = splitter.criterion.middle_value()
 
         if not is_leaf:
             # is split node
             res.pos = split.pos
             res.is_leaf = 0
             res.improvement = split.improvement
@@ -670,16 +672,16 @@
             res.impurity_right = split.impurity_right
 
         else:
             # is leaf => 0 improvement
             res.pos = end
             res.is_leaf = 1
             res.improvement = 0.0
-            res.impurity_left = impurity
-            res.impurity_right = impurity
+            res.impurity_left = parent_record.impurity
+            res.impurity_right = parent_record.impurity
 
         return 0
 
 
 # =============================================================================
 # Tree
 # =============================================================================
@@ -692,53 +694,53 @@
     tree's root. You can find a detailed description of all arrays in
     `_tree.pxd`. NOTE: Some of the arrays only apply to either leaves or split
     nodes, resp. In this case the values of nodes of the other type are
     arbitrary!
 
     Attributes
     ----------
-    node_count : int
+    node_count : intp_t
         The number of nodes (internal nodes + leaves) in the tree.
 
-    capacity : int
+    capacity : intp_t
         The current capacity (i.e., size) of the arrays, which is at least as
         great as `node_count`.
 
-    max_depth : int
+    max_depth : intp_t
         The depth of the tree, i.e. the maximum depth of its leaves.
 
-    children_left : array of int, shape [node_count]
+    children_left : array of intp_t, shape [node_count]
         children_left[i] holds the node id of the left child of node i.
         For leaves, children_left[i] == TREE_LEAF. Otherwise,
         children_left[i] > i. This child handles the case where
         X[:, feature[i]] <= threshold[i].
 
-    children_right : array of int, shape [node_count]
+    children_right : array of intp_t, shape [node_count]
         children_right[i] holds the node id of the right child of node i.
         For leaves, children_right[i] == TREE_LEAF. Otherwise,
         children_right[i] > i. This child handles the case where
         X[:, feature[i]] > threshold[i].
 
-    n_leaves : int
+    n_leaves : intp_t
         Number of leaves in the tree.
 
-    feature : array of int, shape [node_count]
+    feature : array of intp_t, shape [node_count]
         feature[i] holds the feature to split on, for the internal node i.
 
     threshold : array of float64_t, shape [node_count]
         threshold[i] holds the threshold for the internal node i.
 
     value : array of float64_t, shape [node_count, n_outputs, max_n_classes]
         Contains the constant prediction value of each node.
 
     impurity : array of float64_t, shape [node_count]
         impurity[i] holds the impurity (i.e., the value of the splitting
         criterion) at node i.
 
-    n_node_samples : array of int, shape [node_count]
+    n_node_samples : array of intp_t, shape [node_count]
         n_node_samples[i] holds the number of training samples reaching node i.
 
     weighted_n_node_samples : array of float64_t, shape [node_count]
         weighted_n_node_samples[i] holds the weighted number of training samples
         reaching node i.
 
     missing_go_to_left : array of bool, shape [node_count]
@@ -793,15 +795,15 @@
 
     @property
     def value(self):
         return self._get_value_ndarray()[:self.node_count]
 
     # TODO: Convert n_classes to cython.integral memory view once
     #  https://github.com/cython/cython/issues/5243 is fixed
-    def __cinit__(self, int n_features, cnp.ndarray n_classes, int n_outputs):
+    def __cinit__(self, intp_t n_features, cnp.ndarray n_classes, intp_t n_outputs):
         """Constructor."""
         cdef intp_t dummy = 0
         size_t_dtype = np.array(dummy).dtype
 
         n_classes = _check_n_classes(n_classes, size_t_dtype)
 
         # Input/Output layout
@@ -1339,15 +1341,15 @@
                                    cnp.NPY_ARRAY_DEFAULT, None)
         Py_INCREF(self)
         if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:
             raise ValueError("Can't initialize array.")
         return arr
 
     def compute_partial_dependence(self, float32_t[:, ::1] X,
-                                   int[::1] target_features,
+                                   const intp_t[::1] target_features,
                                    float64_t[::1] out):
         """Partial dependence of the response on the ``target_feature`` set.
 
         For each sample in ``X`` a tree traversal is performed.
         Each traversal starts from the root with weight 1.0.
 
         At each non-leaf node that splits on a target feature, either
@@ -1375,15 +1377,15 @@
         cdef:
             float64_t[::1] weight_stack = np.zeros(self.node_count,
                                                    dtype=np.float64)
             intp_t[::1] node_idx_stack = np.zeros(self.node_count,
                                                   dtype=np.intp)
             intp_t sample_idx
             intp_t feature_idx
-            int stack_size
+            intp_t stack_size
             float64_t left_sample_frac
             float64_t current_weight
             float64_t total_weight  # used for sanity check only
             Node *current_node  # use a pointer to avoid copying attributes
             intp_t current_node_idx
             bint is_target_feature
             intp_t _TREE_LEAF = TREE_LEAF  # to avoid python interactions
@@ -1623,15 +1625,15 @@
 
 cdef class _PathFinder(_CCPPruneController):
     """Record metrics used to return the cost complexity path."""
     cdef float64_t[:] ccp_alphas
     cdef float64_t[:] impurities
     cdef uint32_t count
 
-    def __cinit__(self,  int node_count):
+    def __cinit__(self,  intp_t node_count):
         self.ccp_alphas = np.zeros(shape=(node_count), dtype=np.float64)
         self.impurities = np.zeros(shape=(node_count), dtype=np.float64)
         self.count = 0
 
     cdef void save_metrics(self,
                            float64_t effective_alpha,
                            float64_t subtree_impurities) noexcept nogil:
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/_utils.pxd` & `scikit_learn-1.5.0rc1/sklearn/tree/_utils.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/_utils.pyx` & `scikit_learn-1.5.0rc1/sklearn/tree/_utils.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -446,17 +446,17 @@
             # whole median
             return self.samples.get_value_from_index(self.k-1)
 
 
 def _any_isnan_axis0(const float32_t[:, :] X):
     """Same as np.any(np.isnan(X), axis=0)"""
     cdef:
-        int i, j
-        int n_samples = X.shape[0]
-        int n_features = X.shape[1]
+        intp_t i, j
+        intp_t n_samples = X.shape[0]
+        intp_t n_features = X.shape[1]
         unsigned char[::1] isnan_out = np.zeros(X.shape[1], dtype=np.bool_)
 
     with nogil:
         for i in range(n_samples):
             for j in range(n_features):
                 if isnan_out[j]:
                     continue
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/meson.build` & `scikit_learn-1.5.0rc1/sklearn/tree/meson.build`

 * *Files 14% similar despite different names*

```diff
@@ -12,15 +12,15 @@
     {'sources': ['_utils.pyx'],
      'override_options': ['optimization=3']},
 }
 
 foreach ext_name, ext_dict : tree_extension_metadata
   py.extension_module(
     ext_name,
-    ext_dict.get('sources'),
+    [ext_dict.get('sources'), utils_cython_tree],
     dependencies: [np_dep],
     override_options : ext_dict.get('override_options', []),
     cython_args: cython_args,
     subdir: 'sklearn/tree',
     install: true
   )
 endforeach
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/tests/test_export.py` & `scikit_learn-1.5.0rc1/sklearn/tree/tests/test_export.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """
 Testing for export functions of decision trees (sklearn.tree.export).
 """
+
 from io import StringIO
 from re import finditer, search
 from textwrap import dedent
 
 import numpy as np
 import pytest
 from numpy.random import RandomState
@@ -371,149 +372,175 @@
         export_text(clf, class_names=["a"])
 
 
 def test_export_text():
     clf = DecisionTreeClassifier(max_depth=2, random_state=0)
     clf.fit(X, y)
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- feature_1 <= 0.00
     |   |--- class: -1
     |--- feature_1 >  0.00
     |   |--- class: 1
-    """).lstrip()
+    """
+    ).lstrip()
 
     assert export_text(clf) == expected_report
     # testing that leaves at level 1 are not truncated
     assert export_text(clf, max_depth=0) == expected_report
     # testing that the rest of the tree is truncated
     assert export_text(clf, max_depth=10) == expected_report
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- feature_1 <= 0.00
     |   |--- weights: [3.00, 0.00] class: -1
     |--- feature_1 >  0.00
     |   |--- weights: [0.00, 3.00] class: 1
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(clf, show_weights=True) == expected_report
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |- feature_1 <= 0.00
     | |- class: -1
     |- feature_1 >  0.00
     | |- class: 1
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(clf, spacing=1) == expected_report
 
     X_l = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [-1, 1]]
     y_l = [-1, -1, -1, 1, 1, 1, 2]
     clf = DecisionTreeClassifier(max_depth=4, random_state=0)
     clf.fit(X_l, y_l)
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- feature_1 <= 0.00
     |   |--- class: -1
     |--- feature_1 >  0.00
     |   |--- truncated branch of depth 2
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(clf, max_depth=0) == expected_report
 
     X_mo = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
     y_mo = [[-1, -1], [-1, -1], [-1, -1], [1, 1], [1, 1], [1, 1]]
 
     reg = DecisionTreeRegressor(max_depth=2, random_state=0)
     reg.fit(X_mo, y_mo)
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- feature_1 <= 0.0
     |   |--- value: [-1.0, -1.0]
     |--- feature_1 >  0.0
     |   |--- value: [1.0, 1.0]
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(reg, decimals=1) == expected_report
     assert export_text(reg, decimals=1, show_weights=True) == expected_report
 
     X_single = [[-2], [-1], [-1], [1], [1], [2]]
     reg = DecisionTreeRegressor(max_depth=2, random_state=0)
     reg.fit(X_single, y_mo)
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- first <= 0.0
     |   |--- value: [-1.0, -1.0]
     |--- first >  0.0
     |   |--- value: [1.0, 1.0]
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(reg, decimals=1, feature_names=["first"]) == expected_report
     assert (
         export_text(reg, decimals=1, show_weights=True, feature_names=["first"])
         == expected_report
     )
 
 
 @pytest.mark.parametrize("constructor", [list, np.array])
 def test_export_text_feature_class_names_array_support(constructor):
     # Check that export_graphviz treats feature names
     # and class names correctly and supports arrays
     clf = DecisionTreeClassifier(max_depth=2, random_state=0)
     clf.fit(X, y)
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- b <= 0.00
     |   |--- class: -1
     |--- b >  0.00
     |   |--- class: 1
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(clf, feature_names=constructor(["a", "b"])) == expected_report
 
-    expected_report = dedent("""
+    expected_report = dedent(
+        """
     |--- feature_1 <= 0.00
     |   |--- class: cat
     |--- feature_1 >  0.00
     |   |--- class: dog
-    """).lstrip()
+    """
+    ).lstrip()
     assert export_text(clf, class_names=constructor(["cat", "dog"])) == expected_report
 
 
 def test_plot_tree_entropy(pyplot):
     # mostly smoke tests
     # Check correctness of export_graphviz for criterion = entropy
     clf = DecisionTreeClassifier(
         max_depth=3, min_samples_split=2, criterion="entropy", random_state=2
     )
     clf.fit(X, y)
 
     # Test export code
     feature_names = ["first feat", "sepal_width"]
     nodes = plot_tree(clf, feature_names=feature_names)
-    assert len(nodes) == 3
+    assert len(nodes) == 5
     assert (
         nodes[0].get_text()
         == "first feat <= 0.0\nentropy = 1.0\nsamples = 6\nvalue = [3, 3]"
     )
     assert nodes[1].get_text() == "entropy = 0.0\nsamples = 3\nvalue = [3, 0]"
-    assert nodes[2].get_text() == "entropy = 0.0\nsamples = 3\nvalue = [0, 3]"
+    assert nodes[2].get_text() == "True  "
+    assert nodes[3].get_text() == "entropy = 0.0\nsamples = 3\nvalue = [0, 3]"
+    assert nodes[4].get_text() == "  False"
 
 
-def test_plot_tree_gini(pyplot):
+@pytest.mark.parametrize("fontsize", [None, 10, 20])
+def test_plot_tree_gini(pyplot, fontsize):
     # mostly smoke tests
     # Check correctness of export_graphviz for criterion = gini
     clf = DecisionTreeClassifier(
-        max_depth=3, min_samples_split=2, criterion="gini", random_state=2
+        max_depth=3,
+        min_samples_split=2,
+        criterion="gini",
+        random_state=2,
     )
     clf.fit(X, y)
 
     # Test export code
     feature_names = ["first feat", "sepal_width"]
-    nodes = plot_tree(clf, feature_names=feature_names)
-    assert len(nodes) == 3
+    nodes = plot_tree(clf, feature_names=feature_names, fontsize=fontsize)
+    assert len(nodes) == 5
+    if fontsize is not None:
+        assert all(node.get_fontsize() == fontsize for node in nodes)
     assert (
         nodes[0].get_text()
         == "first feat <= 0.0\ngini = 0.5\nsamples = 6\nvalue = [3, 3]"
     )
     assert nodes[1].get_text() == "gini = 0.0\nsamples = 3\nvalue = [3, 0]"
-    assert nodes[2].get_text() == "gini = 0.0\nsamples = 3\nvalue = [0, 3]"
+    assert nodes[2].get_text() == "True  "
+    assert nodes[3].get_text() == "gini = 0.0\nsamples = 3\nvalue = [0, 3]"
+    assert nodes[4].get_text() == "  False"
 
 
 def test_not_fitted_tree(pyplot):
     # Testing if not fitted tree throws the correct error
     clf = DecisionTreeRegressor()
     with pytest.raises(NotFittedError):
         plot_tree(clf)
```

### Comparing `scikit-learn-1.4.2/sklearn/tree/tests/test_monotonic_tree.py` & `scikit_learn-1.5.0rc1/sklearn/tree/tests/test_monotonic_tree.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/tests/test_reingold_tilford.py` & `scikit_learn-1.5.0rc1/sklearn/tree/tests/test_reingold_tilford.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/tree/tests/test_tree.py` & `scikit_learn-1.5.0rc1/sklearn/tree/tests/test_tree.py`

 * *Files 0% similar despite different names*

```diff
@@ -40,25 +40,30 @@
     TREE_LEAF,
     TREE_UNDEFINED,
     _check_n_classes,
     _check_node_ndarray,
     _check_value_ndarray,
 )
 from sklearn.tree._tree import Tree as CythonTree
-from sklearn.utils import _IS_32BIT, compute_sample_weight
+from sklearn.utils import compute_sample_weight
 from sklearn.utils._testing import (
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
     create_memmap_backed_data,
     ignore_warnings,
     skip_if_32bit,
 )
 from sklearn.utils.estimator_checks import check_sample_weights_invariance
-from sklearn.utils.fixes import COO_CONTAINERS, CSC_CONTAINERS, CSR_CONTAINERS
+from sklearn.utils.fixes import (
+    _IS_32BIT,
+    COO_CONTAINERS,
+    CSC_CONTAINERS,
+    CSR_CONTAINERS,
+)
 from sklearn.utils.validation import check_random_state
 
 CLF_CRITERIONS = ("gini", "log_loss")
 REG_CRITERIONS = ("squared_error", "absolute_error", "friedman_mse", "poisson")
 
 CLF_TREES = {
     "DecisionTreeClassifier": DecisionTreeClassifier,
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/__init__.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_testing.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1299 +1,1194 @@
-"""
-The :mod:`sklearn.utils` module includes various utilities.
-"""
-
-import math
-import numbers
-import platform
-import struct
-import timeit
+"""Testing utilities."""
+
+# Copyright (c) 2011, 2012
+# Authors: Pietro Berkes,
+#          Andreas Muller
+#          Mathieu Blondel
+#          Olivier Grisel
+#          Arnaud Joly
+#          Denis Engemann
+#          Giorgio Patrini
+#          Thierry Guillemot
+# License: BSD 3 clause
+import atexit
+import contextlib
+import functools
+import importlib
+import inspect
+import os
+import os.path as op
+import re
+import shutil
+import sys
+import tempfile
+import unittest
 import warnings
-from collections.abc import Sequence
-from contextlib import contextmanager, suppress
-from itertools import compress, islice
+from collections.abc import Iterable
+from dataclasses import dataclass
+from functools import wraps
+from inspect import signature
+from subprocess import STDOUT, CalledProcessError, TimeoutExpired, check_output
+from unittest import TestCase
 
+import joblib
 import numpy as np
-from scipy.sparse import issparse
+import scipy as sp
+from numpy.testing import assert_allclose as np_assert_allclose
+from numpy.testing import (
+    assert_almost_equal,
+    assert_approx_equal,
+    assert_array_almost_equal,
+    assert_array_equal,
+    assert_array_less,
+    assert_no_warnings,
+)
 
-from .. import get_config
-from ..exceptions import DataConversionWarning
-from . import _joblib, metadata_routing
-from ._bunch import Bunch
-from ._estimator_html_repr import estimator_html_repr
-from ._param_validation import Integral, Interval, validate_params
-from .class_weight import compute_class_weight, compute_sample_weight
-from .deprecation import deprecated
-from .discovery import all_estimators
-from .fixes import parse_version, threadpool_info
-from .murmurhash import murmurhash3_32
-from .validation import (
-    _is_arraylike_not_scalar,
-    _is_pandas_df,
-    _is_polars_df,
-    _use_interchange_protocol,
-    as_float_array,
-    assert_all_finite,
+import sklearn
+from sklearn.utils._array_api import _check_array_api_dispatch
+from sklearn.utils.fixes import (
+    _IS_32BIT,
+    _IS_PYPY,
+    VisibleDeprecationWarning,
+    _in_unstable_openblas_configuration,
+    parse_version,
+    sp_version,
+)
+from sklearn.utils.multiclass import check_classification_targets
+from sklearn.utils.validation import (
     check_array,
-    check_consistent_length,
-    check_random_state,
-    check_scalar,
-    check_symmetric,
+    check_is_fitted,
     check_X_y,
-    column_or_1d,
-    indexable,
 )
 
-# Do not deprecate parallel_backend and register_parallel_backend as they are
-# needed to tune `scikit-learn` behavior and have different effect if called
-# from the vendored version or or the site-package version. The other are
-# utilities that are independent of scikit-learn so they are not part of
-# scikit-learn public API.
-parallel_backend = _joblib.parallel_backend
-register_parallel_backend = _joblib.register_parallel_backend
-
 __all__ = [
-    "murmurhash3_32",
-    "as_float_array",
-    "assert_all_finite",
-    "check_array",
-    "check_random_state",
-    "compute_class_weight",
-    "compute_sample_weight",
-    "column_or_1d",
-    "check_consistent_length",
-    "check_X_y",
-    "check_scalar",
-    "indexable",
-    "check_symmetric",
-    "indices_to_mask",
-    "deprecated",
-    "parallel_backend",
-    "register_parallel_backend",
-    "resample",
-    "shuffle",
-    "check_matplotlib_support",
-    "all_estimators",
-    "DataConversionWarning",
-    "estimator_html_repr",
-    "Bunch",
-    "metadata_routing",
+    "assert_raises",
+    "assert_raises_regexp",
+    "assert_array_equal",
+    "assert_almost_equal",
+    "assert_array_almost_equal",
+    "assert_array_less",
+    "assert_approx_equal",
+    "assert_allclose",
+    "assert_run_python_script_without_output",
+    "assert_no_warnings",
+    "SkipTest",
 ]
 
-IS_PYPY = platform.python_implementation() == "PyPy"
-_IS_32BIT = 8 * struct.calcsize("P") == 32
-_IS_WASM = platform.machine() in ["wasm32", "wasm64"]
-
-
-def _in_unstable_openblas_configuration():
-    """Return True if in an unstable configuration for OpenBLAS"""
-
-    # Import libraries which might load OpenBLAS.
-    import numpy  # noqa
-    import scipy  # noqa
-
-    modules_info = threadpool_info()
-
-    open_blas_used = any(info["internal_api"] == "openblas" for info in modules_info)
-    if not open_blas_used:
-        return False
-
-    # OpenBLAS 0.3.16 fixed instability for arm64, see:
-    # https://github.com/xianyi/OpenBLAS/blob/1b6db3dbba672b4f8af935bd43a1ff6cff4d20b7/Changelog.txt#L56-L58 # noqa
-    openblas_arm64_stable_version = parse_version("0.3.16")
-    for info in modules_info:
-        if info["internal_api"] != "openblas":
-            continue
-        openblas_version = info.get("version")
-        openblas_architecture = info.get("architecture")
-        if openblas_version is None or openblas_architecture is None:
-            # Cannot be sure that OpenBLAS is good enough. Assume unstable:
-            return True
-        if (
-            openblas_architecture == "neoversen1"
-            and parse_version(openblas_version) < openblas_arm64_stable_version
-        ):
-            # See discussions in https://github.com/numpy/numpy/issues/19411
-            return True
-    return False
+_dummy = TestCase("__init__")
+assert_raises = _dummy.assertRaises
+SkipTest = unittest.case.SkipTest
+assert_dict_equal = _dummy.assertDictEqual
 
+assert_raises_regex = _dummy.assertRaisesRegex
+# assert_raises_regexp is deprecated in Python 3.4 in favor of
+# assert_raises_regex but lets keep the backward compat in scikit-learn with
+# the old name for now
+assert_raises_regexp = assert_raises_regex
 
-@validate_params(
-    {
-        "X": ["array-like", "sparse matrix"],
-        "mask": ["array-like"],
-    },
-    prefer_skip_nested_validation=True,
-)
-def safe_mask(X, mask):
-    """Return a mask which is safe to use on X.
 
-    Parameters
-    ----------
-    X : {array-like, sparse matrix}
-        Data on which to apply mask.
+def ignore_warnings(obj=None, category=Warning):
+    """Context manager and decorator to ignore warnings.
 
-    mask : array-like
-        Mask to be used on X.
-
-    Returns
-    -------
-    mask : ndarray
-        Array that is safe to use on X.
-
-    Examples
-    --------
-    >>> from sklearn.utils import safe_mask
-    >>> from scipy.sparse import csr_matrix
-    >>> data = csr_matrix([[1], [2], [3], [4], [5]])
-    >>> condition = [False, True, True, False, True]
-    >>> mask = safe_mask(data, condition)
-    >>> data[mask].toarray()
-    array([[2],
-           [3],
-           [5]])
-    """
-    mask = np.asarray(mask)
-    if np.issubdtype(mask.dtype, np.signedinteger):
-        return mask
-
-    if hasattr(X, "toarray"):
-        ind = np.arange(mask.shape[0])
-        mask = ind[mask]
-    return mask
-
-
-def axis0_safe_slice(X, mask, len_mask):
-    """Return a mask which is safer to use on X than safe_mask.
-
-    This mask is safer than safe_mask since it returns an
-    empty array, when a sparse matrix is sliced with a boolean mask
-    with all False, instead of raising an unhelpful error in older
-    versions of SciPy.
-
-    See: https://github.com/scipy/scipy/issues/5361
-
-    Also note that we can avoid doing the dot product by checking if
-    the len_mask is not zero in _huber_loss_and_gradient but this
-    is not going to be the bottleneck, since the number of outliers
-    and non_outliers are typically non-zero and it makes the code
-    tougher to follow.
+    Note: Using this (in both variants) will clear all warnings
+    from all python modules loaded. In case you need to test
+    cross-module-warning-logging, this is not your tool of choice.
 
     Parameters
     ----------
-    X : {array-like, sparse matrix}
-        Data on which to apply mask.
-
-    mask : ndarray
-        Mask to be used on X.
-
-    len_mask : int
-        The length of the mask.
+    obj : callable, default=None
+        callable where you want to ignore the warnings.
+    category : warning class, default=Warning
+        The category to filter. If Warning, all categories will be muted.
 
-    Returns
-    -------
-    mask : ndarray
-        Array that is safe to use on X.
-    """
-    if len_mask != 0:
-        return X[safe_mask(X, mask), :]
-    return np.zeros(shape=(0, X.shape[1]))
-
-
-def _array_indexing(array, key, key_dtype, axis):
-    """Index an array or scipy.sparse consistently across NumPy version."""
-    if issparse(array) and key_dtype == "bool":
-        key = np.asarray(key)
-    if isinstance(key, tuple):
-        key = list(key)
-    return array[key, ...] if axis == 0 else array[:, key]
-
-
-def _pandas_indexing(X, key, key_dtype, axis):
-    """Index a pandas dataframe or a series."""
-    if _is_arraylike_not_scalar(key):
-        key = np.asarray(key)
-
-    if key_dtype == "int" and not (isinstance(key, slice) or np.isscalar(key)):
-        # using take() instead of iloc[] ensures the return value is a "proper"
-        # copy that will not raise SettingWithCopyWarning
-        return X.take(key, axis=axis)
+    Examples
+    --------
+    >>> import warnings
+    >>> from sklearn.utils._testing import ignore_warnings
+    >>> with ignore_warnings():
+    ...     warnings.warn('buhuhuhu')
+
+    >>> def nasty_warn():
+    ...     warnings.warn('buhuhuhu')
+    ...     print(42)
+
+    >>> ignore_warnings(nasty_warn)()
+    42
+    """
+    if isinstance(obj, type) and issubclass(obj, Warning):
+        # Avoid common pitfall of passing category as the first positional
+        # argument which result in the test not being run
+        warning_name = obj.__name__
+        raise ValueError(
+            "'obj' should be a callable where you want to ignore warnings. "
+            "You passed a warning class instead: 'obj={warning_name}'. "
+            "If you want to pass a warning class to ignore_warnings, "
+            "you should use 'category={warning_name}'".format(warning_name=warning_name)
+        )
+    elif callable(obj):
+        return _IgnoreWarnings(category=category)(obj)
     else:
-        # check whether we should index with loc or iloc
-        indexer = X.iloc if key_dtype == "int" else X.loc
-        return indexer[:, key] if axis else indexer[key]
-
-
-def _list_indexing(X, key, key_dtype):
-    """Index a Python list."""
-    if np.isscalar(key) or isinstance(key, slice):
-        # key is a slice or a scalar
-        return X[key]
-    if key_dtype == "bool":
-        # key is a boolean array-like
-        return list(compress(X, key))
-    # key is a integer array-like of key
-    return [X[idx] for idx in key]
-
-
-def _polars_indexing(X, key, key_dtype, axis):
-    """Indexing X with polars interchange protocol."""
-    # Polars behavior is more consistent with lists
-    if isinstance(key, np.ndarray):
-        key = key.tolist()
+        return _IgnoreWarnings(category=category)
 
-    if axis == 1:
-        return X[:, key]
-    else:
-        return X[key]
 
+class _IgnoreWarnings:
+    """Improved and simplified Python warnings context manager and decorator.
 
-def _determine_key_type(key, accept_slice=True):
-    """Determine the data type of key.
+    This class allows the user to ignore the warnings raised by a function.
+    Copied from Python 2.7.5 and modified as required.
 
     Parameters
     ----------
-    key : scalar, slice or array-like
-        The key from which we want to infer the data type.
+    category : tuple of warning class, default=Warning
+        The category to filter. By default, all the categories will be muted.
 
-    accept_slice : bool, default=True
-        Whether or not to raise an error if the key is a slice.
-
-    Returns
-    -------
-    dtype : {'int', 'str', 'bool', None}
-        Returns the data type of key.
     """
-    err_msg = (
-        "No valid specification of the columns. Only a scalar, list or "
-        "slice of all integers or all strings, or boolean mask is "
-        "allowed"
-    )
-
-    dtype_to_str = {int: "int", str: "str", bool: "bool", np.bool_: "bool"}
-    array_dtype_to_str = {
-        "i": "int",
-        "u": "int",
-        "b": "bool",
-        "O": "str",
-        "U": "str",
-        "S": "str",
-    }
-
-    if key is None:
-        return None
-    if isinstance(key, tuple(dtype_to_str.keys())):
-        try:
-            return dtype_to_str[type(key)]
-        except KeyError:
-            raise ValueError(err_msg)
-    if isinstance(key, slice):
-        if not accept_slice:
-            raise TypeError(
-                "Only array-like or scalar are supported. A Python slice was given."
-            )
-        if key.start is None and key.stop is None:
-            return None
-        key_start_type = _determine_key_type(key.start)
-        key_stop_type = _determine_key_type(key.stop)
-        if key_start_type is not None and key_stop_type is not None:
-            if key_start_type != key_stop_type:
-                raise ValueError(err_msg)
-        if key_start_type is not None:
-            return key_start_type
-        return key_stop_type
-    if isinstance(key, (list, tuple)):
-        unique_key = set(key)
-        key_type = {_determine_key_type(elt) for elt in unique_key}
-        if not key_type:
-            return None
-        if len(key_type) != 1:
-            raise ValueError(err_msg)
-        return key_type.pop()
-    if hasattr(key, "dtype"):
-        try:
-            return array_dtype_to_str[key.dtype.kind]
-        except KeyError:
-            raise ValueError(err_msg)
-    raise ValueError(err_msg)
 
+    def __init__(self, category):
+        self._record = True
+        self._module = sys.modules["warnings"]
+        self._entered = False
+        self.log = []
+        self.category = category
 
-def _safe_indexing(X, indices, *, axis=0):
-    """Return rows, items or columns of X using indices.
+    def __call__(self, fn):
+        """Decorator to catch and hide warnings without visual nesting."""
 
-    .. warning::
+        @wraps(fn)
+        def wrapper(*args, **kwargs):
+            with warnings.catch_warnings():
+                warnings.simplefilter("ignore", self.category)
+                return fn(*args, **kwargs)
 
-        This utility is documented, but **private**. This means that
-        backward compatibility might be broken without any deprecation
-        cycle.
+        return wrapper
 
-    Parameters
-    ----------
-    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series
-        Data from which to sample rows, items or columns. `list` are only
-        supported when `axis=0`.
-    indices : bool, int, str, slice, array-like
-        - If `axis=0`, boolean and integer array-like, integer slice,
-          and scalar integer are supported.
-        - If `axis=1`:
-            - to select a single column, `indices` can be of `int` type for
-              all `X` types and `str` only for dataframe. The selected subset
-              will be 1D, unless `X` is a sparse matrix in which case it will
-              be 2D.
-            - to select multiples columns, `indices` can be one of the
-              following: `list`, `array`, `slice`. The type used in
-              these containers can be one of the following: `int`, 'bool' and
-              `str`. However, `str` is only supported when `X` is a dataframe.
-              The selected subset will be 2D.
-    axis : int, default=0
-        The axis along which `X` will be subsampled. `axis=0` will select
-        rows while `axis=1` will select columns.
+    def __repr__(self):
+        args = []
+        if self._record:
+            args.append("record=True")
+        if self._module is not sys.modules["warnings"]:
+            args.append("module=%r" % self._module)
+        name = type(self).__name__
+        return "%s(%s)" % (name, ", ".join(args))
 
-    Returns
-    -------
-    subset
-        Subset of X on axis 0 or 1.
+    def __enter__(self):
+        if self._entered:
+            raise RuntimeError("Cannot enter %r twice" % self)
+        self._entered = True
+        self._filters = self._module.filters
+        self._module.filters = self._filters[:]
+        self._showwarning = self._module.showwarning
+        warnings.simplefilter("ignore", self.category)
 
-    Notes
-    -----
-    CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are
-    not supported.
+    def __exit__(self, *exc_info):
+        if not self._entered:
+            raise RuntimeError("Cannot exit %r without entering first" % self)
+        self._module.filters = self._filters
+        self._module.showwarning = self._showwarning
+        self.log[:] = []
 
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.utils import _safe_indexing
-    >>> data = np.array([[1, 2], [3, 4], [5, 6]])
-    >>> _safe_indexing(data, 0, axis=0)  # select the first row
-    array([1, 2])
-    >>> _safe_indexing(data, 0, axis=1)  # select the first column
-    array([1, 3, 5])
-    """
-    if indices is None:
-        return X
-
-    if axis not in (0, 1):
-        raise ValueError(
-            "'axis' should be either 0 (to index rows) or 1 (to index "
-            " column). Got {} instead.".format(axis)
-        )
-
-    indices_dtype = _determine_key_type(indices)
 
-    if axis == 0 and indices_dtype == "str":
-        raise ValueError("String indexing is not supported with 'axis=0'")
+def assert_raise_message(exceptions, message, function, *args, **kwargs):
+    """Helper function to test the message raised in an exception.
 
-    if axis == 1 and isinstance(X, list):
-        raise ValueError("axis=1 is not supported for lists")
-
-    if axis == 1 and hasattr(X, "ndim") and X.ndim != 2:
-        raise ValueError(
-            "'X' should be a 2D NumPy array, 2D sparse matrix or pandas "
-            "dataframe when indexing the columns (i.e. 'axis=1'). "
-            "Got {} instead with {} dimension(s).".format(type(X), X.ndim)
-        )
-
-    if (
-        axis == 1
-        and indices_dtype == "str"
-        and not (_is_pandas_df(X) or _use_interchange_protocol(X))
-    ):
-        raise ValueError(
-            "Specifying the columns using strings is only supported for dataframes."
-        )
-
-    if hasattr(X, "iloc"):
-        # TODO: we should probably use _is_pandas_df(X) instead but this would
-        # require updating some tests such as test_train_test_split_mock_pandas.
-        return _pandas_indexing(X, indices, indices_dtype, axis=axis)
-    elif _is_polars_df(X):
-        return _polars_indexing(X, indices, indices_dtype, axis=axis)
-    elif hasattr(X, "shape"):
-        return _array_indexing(X, indices, indices_dtype, axis=axis)
-    else:
-        return _list_indexing(X, indices, indices_dtype)
-
-
-def _safe_assign(X, values, *, row_indexer=None, column_indexer=None):
-    """Safe assignment to a numpy array, sparse matrix, or pandas dataframe.
+    Given an exception, a callable to raise the exception, and
+    a message string, tests that the correct exception is raised and
+    that the message is a substring of the error thrown. Used to test
+    that the specific message thrown during an exception is correct.
 
     Parameters
     ----------
-    X : {ndarray, sparse-matrix, dataframe}
-        Array to be modified. It is expected to be 2-dimensional.
-
-    values : ndarray
-        The values to be assigned to `X`.
-
-    row_indexer : array-like, dtype={int, bool}, default=None
-        A 1-dimensional array to select the rows of interest. If `None`, all
-        rows are selected.
-
-    column_indexer : array-like, dtype={int, bool}, default=None
-        A 1-dimensional array to select the columns of interest. If `None`, all
-        columns are selected.
-    """
-    row_indexer = slice(None, None, None) if row_indexer is None else row_indexer
-    column_indexer = (
-        slice(None, None, None) if column_indexer is None else column_indexer
-    )
-
-    if hasattr(X, "iloc"):  # pandas dataframe
-        with warnings.catch_warnings():
-            # pandas >= 1.5 raises a warning when using iloc to set values in a column
-            # that does not have the same type as the column being set. It happens
-            # for instance when setting a categorical column with a string.
-            # In the future the behavior won't change and the warning should disappear.
-            # TODO(1.3): check if the warning is still raised or remove the filter.
-            warnings.simplefilter("ignore", FutureWarning)
-            X.iloc[row_indexer, column_indexer] = values
-    else:  # numpy array or sparse matrix
-        X[row_indexer, column_indexer] = values
-
+    exceptions : exception or tuple of exception
+        An Exception object.
 
-def _get_column_indices_for_bool_or_int(key, n_columns):
-    # Convert key into list of positive integer indexes
-    try:
-        idx = _safe_indexing(np.arange(n_columns), key)
-    except IndexError as e:
-        raise ValueError(
-            f"all features must be in [0, {n_columns - 1}] or [-{n_columns}, 0]"
-        ) from e
-    return np.atleast_1d(idx).tolist()
+    message : str
+        The error message or a substring of the error message.
 
+    function : callable
+        Callable object to raise error.
 
-def _get_column_indices(X, key):
-    """Get feature column indices for input data X and key.
+    *args : the positional arguments to `function`.
 
-    For accepted values of `key`, see the docstring of
-    :func:`_safe_indexing`.
+    **kwargs : the keyword arguments to `function`.
     """
-    key_dtype = _determine_key_type(key)
-    if _use_interchange_protocol(X):
-        return _get_column_indices_interchange(X.__dataframe__(), key, key_dtype)
-
-    n_columns = X.shape[1]
-    if isinstance(key, (list, tuple)) and not key:
-        # we get an empty list
-        return []
-    elif key_dtype in ("bool", "int"):
-        return _get_column_indices_for_bool_or_int(key, n_columns)
-    else:
-        try:
-            all_columns = X.columns
-        except AttributeError:
-            raise ValueError(
-                "Specifying the columns using strings is only supported for dataframes."
+    try:
+        function(*args, **kwargs)
+    except exceptions as e:
+        error_message = str(e)
+        if message not in error_message:
+            raise AssertionError(
+                "Error message does not include the expected"
+                " string: %r. Observed error message: %r" % (message, error_message)
             )
-        if isinstance(key, str):
-            columns = [key]
-        elif isinstance(key, slice):
-            start, stop = key.start, key.stop
-            if start is not None:
-                start = all_columns.get_loc(start)
-            if stop is not None:
-                # pandas indexing with strings is endpoint included
-                stop = all_columns.get_loc(stop) + 1
-            else:
-                stop = n_columns + 1
-            return list(islice(range(n_columns), start, stop))
-        else:
-            columns = list(key)
-
-        try:
-            column_indices = []
-            for col in columns:
-                col_idx = all_columns.get_loc(col)
-                if not isinstance(col_idx, numbers.Integral):
-                    raise ValueError(
-                        f"Selected columns, {columns}, are not unique in dataframe"
-                    )
-                column_indices.append(col_idx)
-
-        except KeyError as e:
-            raise ValueError("A given column is not a column of the dataframe") from e
-
-        return column_indices
-
-
-def _get_column_indices_interchange(X_interchange, key, key_dtype):
-    """Same as _get_column_indices but for X with __dataframe__ protocol."""
-
-    n_columns = X_interchange.num_columns()
-
-    if isinstance(key, (list, tuple)) and not key:
-        # we get an empty list
-        return []
-    elif key_dtype in ("bool", "int"):
-        return _get_column_indices_for_bool_or_int(key, n_columns)
     else:
-        column_names = list(X_interchange.column_names())
-
-        if isinstance(key, slice):
-            if key.step not in [1, None]:
-                raise NotImplementedError("key.step must be 1 or None")
-            start, stop = key.start, key.stop
-            if start is not None:
-                start = column_names.index(start)
-
-            if stop is not None:
-                stop = column_names.index(stop) + 1
-            else:
-                stop = n_columns + 1
-            return list(islice(range(n_columns), start, stop))
-
-        selected_columns = [key] if np.isscalar(key) else key
-
-        try:
-            return [column_names.index(col) for col in selected_columns]
-        except ValueError as e:
-            raise ValueError("A given column is not a column of the dataframe") from e
+        # concatenate exception names
+        if isinstance(exceptions, tuple):
+            names = " or ".join(e.__name__ for e in exceptions)
+        else:
+            names = exceptions.__name__
 
+        raise AssertionError("%s not raised by %s" % (names, function.__name__))
 
-@validate_params(
-    {
-        "replace": ["boolean"],
-        "n_samples": [Interval(numbers.Integral, 1, None, closed="left"), None],
-        "random_state": ["random_state"],
-        "stratify": ["array-like", None],
-    },
-    prefer_skip_nested_validation=True,
-)
-def resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):
-    """Resample arrays or sparse matrices in a consistent way.
 
-    The default strategy implements one step of the bootstrapping
-    procedure.
+def assert_allclose(
+    actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg="", verbose=True
+):
+    """dtype-aware variant of numpy.testing.assert_allclose
+
+    This variant introspects the least precise floating point dtype
+    in the input argument and automatically sets the relative tolerance
+    parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64
+    in scikit-learn).
+
+    `atol` is always left to 0. by default. It should be adjusted manually
+    to an assertion-specific value in case there are null values expected
+    in `desired`.
+
+    The aggregate tolerance is `atol + rtol * abs(desired)`.
 
     Parameters
     ----------
-    *arrays : sequence of array-like of shape (n_samples,) or \
-            (n_samples, n_outputs)
-        Indexable data-structures can be arrays, lists, dataframes or scipy
-        sparse matrices with consistent first dimension.
-
-    replace : bool, default=True
-        Implements resampling with replacement. If False, this will implement
-        (sliced) random permutations.
-
-    n_samples : int, default=None
-        Number of samples to generate. If left to None this is
-        automatically set to the first dimension of the arrays.
-        If replace is False it should not be larger than the length of
-        arrays.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines random number generation for shuffling
-        the data.
-        Pass an int for reproducible results across multiple function calls.
-        See :term:`Glossary <random_state>`.
-
-    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \
-            default=None
-        If not None, data is split in a stratified fashion, using this as
-        the class labels.
+    actual : array_like
+        Array obtained.
+    desired : array_like
+        Array desired.
+    rtol : float, optional, default=None
+        Relative tolerance.
+        If None, it is set based on the provided arrays' dtypes.
+    atol : float, optional, default=0.
+        Absolute tolerance.
+    equal_nan : bool, optional, default=True
+        If True, NaNs will compare equal.
+    err_msg : str, optional, default=''
+        The error message to be printed in case of failure.
+    verbose : bool, optional, default=True
+        If True, the conflicting values are appended to the error message.
 
-    Returns
-    -------
-    resampled_arrays : sequence of array-like of shape (n_samples,) or \
-            (n_samples, n_outputs)
-        Sequence of resampled copies of the collections. The original arrays
-        are not impacted.
+    Raises
+    ------
+    AssertionError
+        If actual and desired are not equal up to specified precision.
 
     See Also
     --------
-    shuffle : Shuffle arrays or sparse matrices in a consistent way.
+    numpy.testing.assert_allclose
 
     Examples
     --------
-    It is possible to mix sparse and dense arrays in the same run::
-
-      >>> import numpy as np
-      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
-      >>> y = np.array([0, 1, 2])
-
-      >>> from scipy.sparse import coo_matrix
-      >>> X_sparse = coo_matrix(X)
-
-      >>> from sklearn.utils import resample
-      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)
-      >>> X
-      array([[1., 0.],
-             [2., 1.],
-             [1., 0.]])
-
-      >>> X_sparse
-      <3x2 sparse matrix of type '<... 'numpy.float64'>'
-          with 4 stored elements in Compressed Sparse Row format>
-
-      >>> X_sparse.toarray()
-      array([[1., 0.],
-             [2., 1.],
-             [1., 0.]])
-
-      >>> y
-      array([0, 1, 0])
-
-      >>> resample(y, n_samples=2, random_state=0)
-      array([0, 1])
-
-    Example using stratification::
-
-      >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]
-      >>> resample(y, n_samples=5, replace=False, stratify=y,
-      ...          random_state=0)
-      [1, 1, 1, 0, 1]
-    """
-    max_n_samples = n_samples
-    random_state = check_random_state(random_state)
-
-    if len(arrays) == 0:
-        return None
+    >>> import numpy as np
+    >>> from sklearn.utils._testing import assert_allclose
+    >>> x = [1e-5, 1e-3, 1e-1]
+    >>> y = np.arccos(np.cos(x))
+    >>> assert_allclose(x, y, rtol=1e-5, atol=0)
+    >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)
+    >>> assert_allclose(a, 1e-5)
+    """
+    dtypes = []
+
+    actual, desired = np.asanyarray(actual), np.asanyarray(desired)
+    dtypes = [actual.dtype, desired.dtype]
+
+    if rtol is None:
+        rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]
+        rtol = max(rtols)
+
+    np_assert_allclose(
+        actual,
+        desired,
+        rtol=rtol,
+        atol=atol,
+        equal_nan=equal_nan,
+        err_msg=err_msg,
+        verbose=verbose,
+    )
 
-    first = arrays[0]
-    n_samples = first.shape[0] if hasattr(first, "shape") else len(first)
 
-    if max_n_samples is None:
-        max_n_samples = n_samples
-    elif (max_n_samples > n_samples) and (not replace):
-        raise ValueError(
-            "Cannot sample %d out of arrays with dim %d when replace is False"
-            % (max_n_samples, n_samples)
-        )
+def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=""):
+    """Assert allclose for sparse and dense data.
 
-    check_consistent_length(*arrays)
+    Both x and y need to be either sparse or dense, they
+    can't be mixed.
 
-    if stratify is None:
-        if replace:
-            indices = random_state.randint(0, n_samples, size=(max_n_samples,))
-        else:
-            indices = np.arange(n_samples)
-            random_state.shuffle(indices)
-            indices = indices[:max_n_samples]
+    Parameters
+    ----------
+    x : {array-like, sparse matrix}
+        First array to compare.
+
+    y : {array-like, sparse matrix}
+        Second array to compare.
+
+    rtol : float, default=1e-07
+        relative tolerance; see numpy.allclose.
+
+    atol : float, default=1e-9
+        absolute tolerance; see numpy.allclose. Note that the default here is
+        more tolerant than the default for numpy.testing.assert_allclose, where
+        atol=0.
+
+    err_msg : str, default=''
+        Error message to raise.
+    """
+    if sp.sparse.issparse(x) and sp.sparse.issparse(y):
+        x = x.tocsr()
+        y = y.tocsr()
+        x.sum_duplicates()
+        y.sum_duplicates()
+        assert_array_equal(x.indices, y.indices, err_msg=err_msg)
+        assert_array_equal(x.indptr, y.indptr, err_msg=err_msg)
+        assert_allclose(x.data, y.data, rtol=rtol, atol=atol, err_msg=err_msg)
+    elif not sp.sparse.issparse(x) and not sp.sparse.issparse(y):
+        # both dense
+        assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
     else:
-        # Code adapted from StratifiedShuffleSplit()
-        y = check_array(stratify, ensure_2d=False, dtype=None)
-        if y.ndim == 2:
-            # for multi-label y, map each distinct row to a string repr
-            # using join because str(row) uses an ellipsis if len(row) > 1000
-            y = np.array([" ".join(row.astype("str")) for row in y])
-
-        classes, y_indices = np.unique(y, return_inverse=True)
-        n_classes = classes.shape[0]
-
-        class_counts = np.bincount(y_indices)
-
-        # Find the sorted list of instances for each class:
-        # (np.unique above performs a sort, so code is O(n logn) already)
-        class_indices = np.split(
-            np.argsort(y_indices, kind="mergesort"), np.cumsum(class_counts)[:-1]
+        raise ValueError(
+            "Can only compare two sparse matrices, not a sparse matrix and an array."
         )
 
-        n_i = _approximate_mode(class_counts, max_n_samples, random_state)
-
-        indices = []
-
-        for i in range(n_classes):
-            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)
-            indices.extend(indices_i)
-
-        indices = random_state.permutation(indices)
-
-    # convert sparse matrices to CSR for row-based indexing
-    arrays = [a.tocsr() if issparse(a) else a for a in arrays]
-    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]
-    if len(resampled_arrays) == 1:
-        # syntactic sugar for the unit argument case
-        return resampled_arrays[0]
-    else:
-        return resampled_arrays
-
-
-def shuffle(*arrays, random_state=None, n_samples=None):
-    """Shuffle arrays or sparse matrices in a consistent way.
 
-    This is a convenience alias to ``resample(*arrays, replace=False)`` to do
-    random permutations of the collections.
+def set_random_state(estimator, random_state=0):
+    """Set random state of an estimator if it has the `random_state` param.
 
     Parameters
     ----------
-    *arrays : sequence of indexable data-structures
-        Indexable data-structures can be arrays, lists, dataframes or scipy
-        sparse matrices with consistent first dimension.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines random number generation for shuffling
-        the data.
+    estimator : object
+        The estimator.
+    random_state : int, RandomState instance or None, default=0
+        Pseudo random number generator state.
         Pass an int for reproducible results across multiple function calls.
         See :term:`Glossary <random_state>`.
+    """
+    if "random_state" in estimator.get_params():
+        estimator.set_params(random_state=random_state)
 
-    n_samples : int, default=None
-        Number of samples to generate. If left to None this is
-        automatically set to the first dimension of the arrays.  It should
-        not be larger than the length of arrays.
 
-    Returns
-    -------
-    shuffled_arrays : sequence of indexable data-structures
-        Sequence of shuffled copies of the collections. The original arrays
-        are not impacted.
+try:
+    _check_array_api_dispatch(True)
+    ARRAY_API_COMPAT_FUNCTIONAL = True
+except ImportError:
+    ARRAY_API_COMPAT_FUNCTIONAL = False
+
+try:
+    import pytest
+
+    skip_if_32bit = pytest.mark.skipif(_IS_32BIT, reason="skipped on 32bit platforms")
+    fails_if_pypy = pytest.mark.xfail(_IS_PYPY, reason="not compatible with PyPy")
+    fails_if_unstable_openblas = pytest.mark.xfail(
+        _in_unstable_openblas_configuration(),
+        reason="OpenBLAS is unstable for this configuration",
+    )
+    skip_if_no_parallel = pytest.mark.skipif(
+        not joblib.parallel.mp, reason="joblib is in serial mode"
+    )
+    skip_if_array_api_compat_not_configured = pytest.mark.skipif(
+        not ARRAY_API_COMPAT_FUNCTIONAL,
+        reason="requires array_api_compat installed and a new enough version of NumPy",
+    )
 
-    See Also
-    --------
-    resample : Resample arrays or sparse matrices in a consistent way.
+    #  Decorator for tests involving both BLAS calls and multiprocessing.
+    #
+    #  Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction
+    #  with some implementation of BLAS (or other libraries that manage an
+    #  internal posix thread pool) can cause a crash or a freeze of the Python
+    #  process.
+    #
+    #  In practice all known packaged distributions (from Linux distros or
+    #  Anaconda) of BLAS under Linux seems to be safe. So we this problem seems
+    #  to only impact OSX users.
+    #
+    #  This wrapper makes it possible to skip tests that can possibly cause
+    #  this crash under OS X with.
+    #
+    #  Under Python 3.4+ it is possible to use the `forkserver` start method
+    #  for multiprocessing to avoid this issue. However it can cause pickling
+    #  errors on interactively defined functions. It therefore not enabled by
+    #  default.
 
-    Examples
-    --------
-    It is possible to mix sparse and dense arrays in the same run::
+    if_safe_multiprocessing_with_blas = pytest.mark.skipif(
+        sys.platform == "darwin", reason="Possible multi-process bug with some BLAS"
+    )
+except ImportError:
+    pass
 
-      >>> import numpy as np
-      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
-      >>> y = np.array([0, 1, 2])
-
-      >>> from scipy.sparse import coo_matrix
-      >>> X_sparse = coo_matrix(X)
-
-      >>> from sklearn.utils import shuffle
-      >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0)
-      >>> X
-      array([[0., 0.],
-             [2., 1.],
-             [1., 0.]])
-
-      >>> X_sparse
-      <3x2 sparse matrix of type '<... 'numpy.float64'>'
-          with 3 stored elements in Compressed Sparse Row format>
-
-      >>> X_sparse.toarray()
-      array([[0., 0.],
-             [2., 1.],
-             [1., 0.]])
 
-      >>> y
-      array([2, 1, 0])
+def check_skip_network():
+    if int(os.environ.get("SKLEARN_SKIP_NETWORK_TESTS", 0)):
+        raise SkipTest("Text tutorial requires large dataset download")
 
-      >>> shuffle(y, n_samples=2, random_state=0)
-      array([0, 1])
-    """
-    return resample(
-        *arrays, replace=False, n_samples=n_samples, random_state=random_state
-    )
 
+def _delete_folder(folder_path, warn=False):
+    """Utility function to cleanup a temporary folder if still existing.
+
+    Copy from joblib.pool (for independence).
+    """
+    try:
+        if os.path.exists(folder_path):
+            # This can fail under windows,
+            #  but will succeed when called by atexit
+            shutil.rmtree(folder_path)
+    except OSError:
+        if warn:
+            warnings.warn("Could not delete temporary folder %s" % folder_path)
 
-def safe_sqr(X, *, copy=True):
-    """Element wise squaring of array-likes and sparse matrices.
 
+class TempMemmap:
+    """
     Parameters
     ----------
-    X : {array-like, ndarray, sparse matrix}
-
-    copy : bool, default=True
-        Whether to create a copy of X and operate on it or to perform
-        inplace computation (default behaviour).
-
-    Returns
-    -------
-    X ** 2 : element wise square
-         Return the element-wise square of the input.
-
-    Examples
-    --------
-    >>> from sklearn.utils import safe_sqr
-    >>> safe_sqr([1, 2, 3])
-    array([1, 4, 9])
+    data
+    mmap_mode : str, default='r'
     """
-    X = check_array(X, accept_sparse=["csr", "csc", "coo"], ensure_2d=False)
-    if issparse(X):
-        if copy:
-            X = X.copy()
-        X.data **= 2
-    else:
-        if copy:
-            X = X**2
-        else:
-            X **= 2
-    return X
-
 
-def _chunk_generator(gen, chunksize):
-    """Chunk generator, ``gen`` into lists of length ``chunksize``. The last
-    chunk may have a length less than ``chunksize``."""
-    while True:
-        chunk = list(islice(gen, chunksize))
-        if chunk:
-            yield chunk
-        else:
-            return
+    def __init__(self, data, mmap_mode="r"):
+        self.mmap_mode = mmap_mode
+        self.data = data
 
+    def __enter__(self):
+        data_read_only, self.temp_folder = create_memmap_backed_data(
+            self.data, mmap_mode=self.mmap_mode, return_folder=True
+        )
+        return data_read_only
 
-@validate_params(
-    {
-        "n": [Interval(numbers.Integral, 1, None, closed="left")],
-        "batch_size": [Interval(numbers.Integral, 1, None, closed="left")],
-        "min_batch_size": [Interval(numbers.Integral, 0, None, closed="left")],
-    },
-    prefer_skip_nested_validation=True,
-)
-def gen_batches(n, batch_size, *, min_batch_size=0):
-    """Generator to create slices containing `batch_size` elements from 0 to `n`.
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        _delete_folder(self.temp_folder)
 
-    The last slice may contain less than `batch_size` elements, when
-    `batch_size` does not divide `n`.
 
+def create_memmap_backed_data(data, mmap_mode="r", return_folder=False):
+    """
     Parameters
     ----------
-    n : int
-        Size of the sequence.
-    batch_size : int
-        Number of elements in each batch.
-    min_batch_size : int, default=0
-        Minimum number of elements in each batch.
-
-    Yields
-    ------
-    slice of `batch_size` elements
-
-    See Also
-    --------
-    gen_even_slices: Generator to create n_packs slices going up to n.
-
-    Examples
-    --------
-    >>> from sklearn.utils import gen_batches
-    >>> list(gen_batches(7, 3))
-    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]
-    >>> list(gen_batches(6, 3))
-    [slice(0, 3, None), slice(3, 6, None)]
-    >>> list(gen_batches(2, 3))
-    [slice(0, 2, None)]
-    >>> list(gen_batches(7, 3, min_batch_size=0))
-    [slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]
-    >>> list(gen_batches(7, 3, min_batch_size=2))
-    [slice(0, 3, None), slice(3, 7, None)]
+    data
+    mmap_mode : str, default='r'
+    return_folder :  bool, default=False
     """
-    start = 0
-    for _ in range(int(n // batch_size)):
-        end = start + batch_size
-        if end + min_batch_size > n:
-            continue
-        yield slice(start, end)
-        start = end
-    if start < n:
-        yield slice(start, n)
-
-
-@validate_params(
-    {
-        "n": [Interval(Integral, 1, None, closed="left")],
-        "n_packs": [Interval(Integral, 1, None, closed="left")],
-        "n_samples": [Interval(Integral, 1, None, closed="left"), None],
-    },
-    prefer_skip_nested_validation=True,
-)
-def gen_even_slices(n, n_packs, *, n_samples=None):
-    """Generator to create `n_packs` evenly spaced slices going up to `n`.
+    temp_folder = tempfile.mkdtemp(prefix="sklearn_testing_")
+    atexit.register(functools.partial(_delete_folder, temp_folder, warn=True))
+    filename = op.join(temp_folder, "data.pkl")
+    joblib.dump(data, filename)
+    memmap_backed_data = joblib.load(filename, mmap_mode=mmap_mode)
+    result = (
+        memmap_backed_data if not return_folder else (memmap_backed_data, temp_folder)
+    )
+    return result
 
-    If `n_packs` does not divide `n`, except for the first `n % n_packs`
-    slices, remaining slices may contain fewer elements.
 
-    Parameters
-    ----------
-    n : int
-        Size of the sequence.
-    n_packs : int
-        Number of slices to generate.
-    n_samples : int, default=None
-        Number of samples. Pass `n_samples` when the slices are to be used for
-        sparse matrix indexing; slicing off-the-end raises an exception, while
-        it works for NumPy arrays.
+# Utils to test docstrings
 
-    Yields
-    ------
-    `slice` representing a set of indices from 0 to n.
 
-    See Also
-    --------
-    gen_batches: Generator to create slices containing batch_size elements
-        from 0 to n.
+def _get_args(function, varargs=False):
+    """Helper to get function arguments."""
 
-    Examples
-    --------
-    >>> from sklearn.utils import gen_even_slices
-    >>> list(gen_even_slices(10, 1))
-    [slice(0, 10, None)]
-    >>> list(gen_even_slices(10, 10))
-    [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]
-    >>> list(gen_even_slices(10, 5))
-    [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]
-    >>> list(gen_even_slices(10, 3))
-    [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]
-    """
-    start = 0
-    for pack_num in range(n_packs):
-        this_n = n // n_packs
-        if pack_num < n % n_packs:
-            this_n += 1
-        if this_n > 0:
-            end = start + this_n
-            if n_samples is not None:
-                end = min(n_samples, end)
-            yield slice(start, end, None)
-            start = end
+    try:
+        params = signature(function).parameters
+    except ValueError:
+        # Error on builtin C function
+        return []
+    args = [
+        key
+        for key, param in params.items()
+        if param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)
+    ]
+    if varargs:
+        varargs = [
+            param.name
+            for param in params.values()
+            if param.kind == param.VAR_POSITIONAL
+        ]
+        if len(varargs) == 0:
+            varargs = None
+        return args, varargs
+    else:
+        return args
 
 
-def tosequence(x):
-    """Cast iterable x to a Sequence, avoiding a copy if possible.
+def _get_func_name(func):
+    """Get function full name.
 
     Parameters
     ----------
-    x : iterable
-        The iterable to be converted.
+    func : callable
+        The function object.
 
     Returns
     -------
-    x : Sequence
-        If `x` is a NumPy array, it returns it as a `ndarray`. If `x`
-        is a `Sequence`, `x` is returned as-is. If `x` is from any other
-        type, `x` is returned casted as a list.
+    name : str
+        The function name.
     """
-    if isinstance(x, np.ndarray):
-        return np.asarray(x)
-    elif isinstance(x, Sequence):
-        return x
-    else:
-        return list(x)
+    parts = []
+    module = inspect.getmodule(func)
+    if module:
+        parts.append(module.__name__)
+
+    qualname = func.__qualname__
+    if qualname != func.__name__:
+        parts.append(qualname[: qualname.find(".")])
 
+    parts.append(func.__name__)
+    return ".".join(parts)
 
-def _to_object_array(sequence):
-    """Convert sequence to a 1-D NumPy array of object dtype.
 
-    numpy.array constructor has a similar use but it's output
-    is ambiguous. It can be 1-D NumPy array of object dtype if
-    the input is a ragged array, but if the input is a list of
-    equal length arrays, then the output is a 2D numpy.array.
-    _to_object_array solves this ambiguity by guarantying that
-    the output is a 1-D NumPy array of objects for any input.
+def check_docstring_parameters(func, doc=None, ignore=None):
+    """Helper to check docstring.
 
     Parameters
     ----------
-    sequence : array-like of shape (n_elements,)
-        The sequence to be converted.
+    func : callable
+        The function object to test.
+    doc : str, default=None
+        Docstring if it is passed manually to the test.
+    ignore : list, default=None
+        Parameters to ignore.
 
     Returns
     -------
-    out : ndarray of shape (n_elements,), dtype=object
-        The converted sequence into a 1-D NumPy array of object dtype.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.utils import _to_object_array
-    >>> _to_object_array([np.array([0]), np.array([1])])
-    array([array([0]), array([1])], dtype=object)
-    >>> _to_object_array([np.array([0]), np.array([1, 2])])
-    array([array([0]), array([1, 2])], dtype=object)
-    >>> _to_object_array([np.array([0]), np.array([1, 2])])
-    array([array([0]), array([1, 2])], dtype=object)
+    incorrect : list
+        A list of string describing the incorrect results.
     """
-    out = np.empty(len(sequence), dtype=object)
-    out[:] = sequence
-    return out
+    from numpydoc import docscrape
 
+    incorrect = []
+    ignore = [] if ignore is None else ignore
 
-def indices_to_mask(indices, mask_length):
-    """Convert list of indices to boolean mask.
+    func_name = _get_func_name(func)
+    if not func_name.startswith("sklearn.") or func_name.startswith(
+        "sklearn.externals"
+    ):
+        return incorrect
+    # Don't check docstring for property-functions
+    if inspect.isdatadescriptor(func):
+        return incorrect
+    # Don't check docstring for setup / teardown pytest functions
+    if func_name.split(".")[-1] in ("setup_module", "teardown_module"):
+        return incorrect
+    # Dont check estimator_checks module
+    if func_name.split(".")[2] == "estimator_checks":
+        return incorrect
+    # Get the arguments from the function signature
+    param_signature = list(filter(lambda x: x not in ignore, _get_args(func)))
+    # drop self
+    if len(param_signature) > 0 and param_signature[0] == "self":
+        param_signature.remove("self")
+
+    # Analyze function's docstring
+    if doc is None:
+        records = []
+        with warnings.catch_warnings(record=True):
+            warnings.simplefilter("error", UserWarning)
+            try:
+                doc = docscrape.FunctionDoc(func)
+            except UserWarning as exp:
+                if "potentially wrong underline length" in str(exp):
+                    # Catch warning raised as of numpydoc 1.2 when
+                    # the underline length for a section of a docstring
+                    # is not consistent.
+                    message = str(exp).split("\n")[:3]
+                    incorrect += [f"In function: {func_name}"] + message
+                    return incorrect
+                records.append(str(exp))
+            except Exception as exp:
+                incorrect += [func_name + " parsing error: " + str(exp)]
+                return incorrect
+        if len(records):
+            raise RuntimeError("Error for %s:\n%s" % (func_name, records[0]))
+
+    param_docs = []
+    for name, type_definition, param_doc in doc["Parameters"]:
+        # Type hints are empty only if parameter name ended with :
+        if not type_definition.strip():
+            if ":" in name and name[: name.index(":")][-1:].strip():
+                incorrect += [
+                    func_name
+                    + " There was no space between the param name and colon (%r)" % name
+                ]
+            elif name.rstrip().endswith(":"):
+                incorrect += [
+                    func_name
+                    + " Parameter %r has an empty type spec. Remove the colon"
+                    % (name.lstrip())
+                ]
+
+        # Create a list of parameters to compare with the parameters gotten
+        # from the func signature
+        if "*" not in name:
+            param_docs.append(name.split(":")[0].strip("` "))
+
+    # If one of the docstring's parameters had an error then return that
+    # incorrect message
+    if len(incorrect) > 0:
+        return incorrect
+
+    # Remove the parameters that should be ignored from list
+    param_docs = list(filter(lambda x: x not in ignore, param_docs))
+
+    # The following is derived from pytest, Copyright (c) 2004-2017 Holger
+    # Krekel and others, Licensed under MIT License. See
+    # https://github.com/pytest-dev/pytest
+
+    message = []
+    for i in range(min(len(param_docs), len(param_signature))):
+        if param_signature[i] != param_docs[i]:
+            message += [
+                "There's a parameter name mismatch in function"
+                " docstring w.r.t. function signature, at index %s"
+                " diff: %r != %r" % (i, param_signature[i], param_docs[i])
+            ]
+            break
+    if len(param_signature) > len(param_docs):
+        message += [
+            "Parameters in function docstring have less items w.r.t."
+            " function signature, first missing item: %s"
+            % param_signature[len(param_docs)]
+        ]
+
+    elif len(param_signature) < len(param_docs):
+        message += [
+            "Parameters in function docstring have more items w.r.t."
+            " function signature, first extra item: %s"
+            % param_docs[len(param_signature)]
+        ]
+
+    # If there wasn't any difference in the parameters themselves between
+    # docstring and signature including having the same length then return
+    # empty list
+    if len(message) == 0:
+        return []
 
-    Parameters
-    ----------
-    indices : list-like
-        List of integers treated as indices.
-    mask_length : int
-        Length of boolean mask to be generated.
-        This parameter must be greater than max(indices).
+    import difflib
+    import pprint
 
-    Returns
-    -------
-    mask : 1d boolean nd-array
-        Boolean array that is True where indices are present, else False.
+    param_docs_formatted = pprint.pformat(param_docs).splitlines()
+    param_signature_formatted = pprint.pformat(param_signature).splitlines()
 
-    Examples
-    --------
-    >>> from sklearn.utils import indices_to_mask
-    >>> indices = [1, 2 , 3, 4]
-    >>> indices_to_mask(indices, 5)
-    array([False,  True,  True,  True,  True])
-    """
-    if mask_length <= np.max(indices):
-        raise ValueError("mask_length must be greater than max(indices)")
+    message += ["Full diff:"]
 
-    mask = np.zeros(mask_length, dtype=bool)
-    mask[indices] = True
+    message.extend(
+        line.strip()
+        for line in difflib.ndiff(param_signature_formatted, param_docs_formatted)
+    )
 
-    return mask
+    incorrect.extend(message)
 
+    # Prepend function name
+    incorrect = ["In function: " + func_name] + incorrect
 
-def _message_with_time(source, message, time):
-    """Create one line message for logging purposes.
+    return incorrect
 
-    Parameters
-    ----------
-    source : str
-        String indicating the source or the reference of the message.
 
-    message : str
-        Short message.
+def assert_run_python_script_without_output(source_code, pattern=".+", timeout=60):
+    """Utility to check assertions in an independent Python subprocess.
+
+    The script provided in the source code should return 0 and the stdtout +
+    stderr should not match the pattern `pattern`.
 
-    time : int
-        Time in seconds.
+    This is a port from cloudpickle https://github.com/cloudpipe/cloudpickle
+
+    Parameters
+    ----------
+    source_code : str
+        The Python source code to execute.
+    pattern : str
+        Pattern that the stdout + stderr should not match. By default, unless
+        stdout + stderr are both empty, an error will be raised.
+    timeout : int, default=60
+        Time in seconds before timeout.
     """
-    start_message = "[%s] " % source
+    fd, source_file = tempfile.mkstemp(suffix="_src_test_sklearn.py")
+    os.close(fd)
+    try:
+        with open(source_file, "wb") as f:
+            f.write(source_code.encode("utf-8"))
+        cmd = [sys.executable, source_file]
+        cwd = op.normpath(op.join(op.dirname(sklearn.__file__), ".."))
+        env = os.environ.copy()
+        try:
+            env["PYTHONPATH"] = os.pathsep.join([cwd, env["PYTHONPATH"]])
+        except KeyError:
+            env["PYTHONPATH"] = cwd
+        kwargs = {"cwd": cwd, "stderr": STDOUT, "env": env}
+        # If coverage is running, pass the config file to the subprocess
+        coverage_rc = os.environ.get("COVERAGE_PROCESS_START")
+        if coverage_rc:
+            kwargs["env"]["COVERAGE_PROCESS_START"] = coverage_rc
 
-    # adapted from joblib.logger.short_format_time without the Windows -.1s
-    # adjustment
-    if time > 60:
-        time_str = "%4.1fmin" % (time / 60)
-    else:
-        time_str = " %5.1fs" % time
-    end_message = " %s, total=%s" % (message, time_str)
-    dots_len = 70 - len(start_message) - len(end_message)
-    return "%s%s%s" % (start_message, dots_len * ".", end_message)
+        kwargs["timeout"] = timeout
+        try:
+            try:
+                out = check_output(cmd, **kwargs)
+            except CalledProcessError as e:
+                raise RuntimeError(
+                    "script errored with output:\n%s" % e.output.decode("utf-8")
+                )
+
+            out = out.decode("utf-8")
+            if re.search(pattern, out):
+                if pattern == ".+":
+                    expectation = "Expected no output"
+                else:
+                    expectation = f"The output was not supposed to match {pattern!r}"
+
+                message = f"{expectation}, got the following output instead: {out!r}"
+                raise AssertionError(message)
+        except TimeoutExpired as e:
+            raise RuntimeError(
+                "script timeout, output so far:\n%s" % e.output.decode("utf-8")
+            )
+    finally:
+        os.unlink(source_file)
 
 
-@contextmanager
-def _print_elapsed_time(source, message=None):
-    """Log elapsed time to stdout when the context is exited.
+def _convert_container(
+    container,
+    constructor_name,
+    columns_name=None,
+    dtype=None,
+    minversion=None,
+    categorical_feature_names=None,
+):
+    """Convert a given container to a specific array-like with a dtype.
 
     Parameters
     ----------
-    source : str
-        String indicating the source or the reference of the message.
-
-    message : str, default=None
-        Short message. If None, nothing will be printed.
+    container : array-like
+        The container to convert.
+    constructor_name : {"list", "tuple", "array", "sparse", "dataframe", \
+            "series", "index", "slice", "sparse_csr", "sparse_csc", \
+            "sparse_csr_array", "sparse_csc_array", "pyarrow", "polars", \
+            "polars_series"}
+        The type of the returned container.
+    columns_name : index or array-like, default=None
+        For pandas container supporting `columns_names`, it will affect
+        specific names.
+    dtype : dtype, default=None
+        Force the dtype of the container. Does not apply to `"slice"`
+        container.
+    minversion : str, default=None
+        Minimum version for package to install.
+    categorical_feature_names : list of str, default=None
+        List of column names to cast to categorical dtype.
 
     Returns
     -------
-    context_manager
-        Prints elapsed time upon exit if verbose.
+    converted_container
     """
-    if message is None:
-        yield
-    else:
-        start = timeit.default_timer()
-        yield
-        print(_message_with_time(source, message, timeit.default_timer() - start))
-
+    if constructor_name == "list":
+        if dtype is None:
+            return list(container)
+        else:
+            return np.asarray(container, dtype=dtype).tolist()
+    elif constructor_name == "tuple":
+        if dtype is None:
+            return tuple(container)
+        else:
+            return tuple(np.asarray(container, dtype=dtype).tolist())
+    elif constructor_name == "array":
+        return np.asarray(container, dtype=dtype)
+    elif constructor_name in ("pandas", "dataframe"):
+        pd = pytest.importorskip("pandas", minversion=minversion)
+        result = pd.DataFrame(container, columns=columns_name, dtype=dtype, copy=False)
+        if categorical_feature_names is not None:
+            for col_name in categorical_feature_names:
+                result[col_name] = result[col_name].astype("category")
+        return result
+    elif constructor_name == "pyarrow":
+        pa = pytest.importorskip("pyarrow", minversion=minversion)
+        array = np.asarray(container)
+        if columns_name is None:
+            columns_name = [f"col{i}" for i in range(array.shape[1])]
+        data = {name: array[:, i] for i, name in enumerate(columns_name)}
+        result = pa.Table.from_pydict(data)
+        if categorical_feature_names is not None:
+            for col_idx, col_name in enumerate(result.column_names):
+                if col_name in categorical_feature_names:
+                    result = result.set_column(
+                        col_idx, col_name, result.column(col_name).dictionary_encode()
+                    )
+        return result
+    elif constructor_name == "polars":
+        pl = pytest.importorskip("polars", minversion=minversion)
+        result = pl.DataFrame(container, schema=columns_name, orient="row")
+        if categorical_feature_names is not None:
+            for col_name in categorical_feature_names:
+                result = result.with_columns(pl.col(col_name).cast(pl.Categorical))
+        return result
+    elif constructor_name == "series":
+        pd = pytest.importorskip("pandas", minversion=minversion)
+        return pd.Series(container, dtype=dtype)
+    elif constructor_name == "polars_series":
+        pl = pytest.importorskip("polars", minversion=minversion)
+        return pl.Series(values=container)
+    elif constructor_name == "index":
+        pd = pytest.importorskip("pandas", minversion=minversion)
+        return pd.Index(container, dtype=dtype)
+    elif constructor_name == "slice":
+        return slice(container[0], container[1])
+    elif "sparse" in constructor_name:
+        if not sp.sparse.issparse(container):
+            # For scipy >= 1.13, sparse array constructed from 1d array may be
+            # 1d or raise an exception. To avoid this, we make sure that the
+            # input container is 2d. For more details, see
+            # https://github.com/scipy/scipy/pull/18530#issuecomment-1878005149
+            container = np.atleast_2d(container)
 
-def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):
-    """Calculate how many rows can be processed within `working_memory`.
+        if "array" in constructor_name and sp_version < parse_version("1.8"):
+            raise ValueError(
+                f"{constructor_name} is only available with scipy>=1.8.0, got "
+                f"{sp_version}"
+            )
+        if constructor_name in ("sparse", "sparse_csr"):
+            # sparse and sparse_csr are equivalent for legacy reasons
+            return sp.sparse.csr_matrix(container, dtype=dtype)
+        elif constructor_name == "sparse_csr_array":
+            return sp.sparse.csr_array(container, dtype=dtype)
+        elif constructor_name == "sparse_csc":
+            return sp.sparse.csc_matrix(container, dtype=dtype)
+        elif constructor_name == "sparse_csc_array":
+            return sp.sparse.csc_array(container, dtype=dtype)
+
+
+def raises(expected_exc_type, match=None, may_pass=False, err_msg=None):
+    """Context manager to ensure exceptions are raised within a code block.
+
+    This is similar to and inspired from pytest.raises, but supports a few
+    other cases.
+
+    This is only intended to be used in estimator_checks.py where we don't
+    want to use pytest. In the rest of the code base, just use pytest.raises
+    instead.
 
     Parameters
     ----------
-    row_bytes : int
-        The expected number of bytes of memory that will be consumed
-        during the processing of each row.
-    max_n_rows : int, default=None
-        The maximum return value.
-    working_memory : int or float, default=None
-        The number of rows to fit inside this number of MiB will be
-        returned. When None (default), the value of
-        ``sklearn.get_config()['working_memory']`` is used.
+    excepted_exc_type : Exception or list of Exception
+        The exception that should be raised by the block. If a list, the block
+        should raise one of the exceptions.
+    match : str or list of str, default=None
+        A regex that the exception message should match. If a list, one of
+        the entries must match. If None, match isn't enforced.
+    may_pass : bool, default=False
+        If True, the block is allowed to not raise an exception. Useful in
+        cases where some estimators may support a feature but others must
+        fail with an appropriate error message. By default, the context
+        manager will raise an exception if the block does not raise an
+        exception.
+    err_msg : str, default=None
+        If the context manager fails (e.g. the block fails to raise the
+        proper exception, or fails to match), then an AssertionError is
+        raised with this message. By default, an AssertionError is raised
+        with a default error message (depends on the kind of failure). Use
+        this to indicate how users should fix their estimators to pass the
+        checks.
+
+    Attributes
+    ----------
+    raised_and_matched : bool
+        True if an exception was raised and a match was found, False otherwise.
+    """
+    return _Raises(expected_exc_type, match, may_pass, err_msg)
+
+
+class _Raises(contextlib.AbstractContextManager):
+    # see raises() for parameters
+    def __init__(self, expected_exc_type, match, may_pass, err_msg):
+        self.expected_exc_types = (
+            expected_exc_type
+            if isinstance(expected_exc_type, Iterable)
+            else [expected_exc_type]
+        )
+        self.matches = [match] if isinstance(match, str) else match
+        self.may_pass = may_pass
+        self.err_msg = err_msg
+        self.raised_and_matched = False
+
+    def __exit__(self, exc_type, exc_value, _):
+        # see
+        # https://docs.python.org/2.5/whatsnew/pep-343.html#SECTION000910000000000000000
+
+        if exc_type is None:  # No exception was raised in the block
+            if self.may_pass:
+                return True  # CM is happy
+            else:
+                err_msg = self.err_msg or f"Did not raise: {self.expected_exc_types}"
+                raise AssertionError(err_msg)
 
-    Returns
-    -------
-    int
-        The number of rows which can be processed within `working_memory`.
+        if not any(
+            issubclass(exc_type, expected_type)
+            for expected_type in self.expected_exc_types
+        ):
+            if self.err_msg is not None:
+                raise AssertionError(self.err_msg) from exc_value
+            else:
+                return False  # will re-raise the original exception
 
-    Warns
-    -----
-    Issues a UserWarning if `row_bytes exceeds `working_memory` MiB.
-    """
+        if self.matches is not None:
+            err_msg = self.err_msg or (
+                "The error message should contain one of the following "
+                "patterns:\n{}\nGot {}".format("\n".join(self.matches), str(exc_value))
+            )
+            if not any(re.search(match, str(exc_value)) for match in self.matches):
+                raise AssertionError(err_msg) from exc_value
+            self.raised_and_matched = True
 
-    if working_memory is None:
-        working_memory = get_config()["working_memory"]
+        return True
 
-    chunk_n_rows = int(working_memory * (2**20) // row_bytes)
-    if max_n_rows is not None:
-        chunk_n_rows = min(chunk_n_rows, max_n_rows)
-    if chunk_n_rows < 1:
-        warnings.warn(
-            "Could not adhere to working_memory config. "
-            "Currently %.0fMiB, %.0fMiB required."
-            % (working_memory, np.ceil(row_bytes * 2**-20))
-        )
-        chunk_n_rows = 1
-    return chunk_n_rows
 
+class MinimalClassifier:
+    """Minimal classifier implementation without inheriting from BaseEstimator.
 
-def _is_pandas_na(x):
-    """Test if x is pandas.NA.
+    This estimator should be tested with:
 
-    We intentionally do not use this function to return `True` for `pd.NA` in
-    `is_scalar_nan`, because estimators that support `pd.NA` are the exception
-    rather than the rule at the moment. When `pd.NA` is more universally
-    supported, we may reconsider this decision.
+    * `check_estimator` in `test_estimator_checks.py`;
+    * within a `Pipeline` in `test_pipeline.py`;
+    * within a `SearchCV` in `test_search.py`.
+    """
 
-    Parameters
-    ----------
-    x : any type
+    _estimator_type = "classifier"
 
-    Returns
-    -------
-    boolean
-    """
-    with suppress(ImportError):
-        from pandas import NA
+    def __init__(self, param=None):
+        self.param = param
 
-        return x is NA
+    def get_params(self, deep=True):
+        return {"param": self.param}
 
-    return False
+    def set_params(self, **params):
+        for key, value in params.items():
+            setattr(self, key, value)
+        return self
 
+    def fit(self, X, y):
+        X, y = check_X_y(X, y)
+        check_classification_targets(y)
+        self.classes_, counts = np.unique(y, return_counts=True)
+        self._most_frequent_class_idx = counts.argmax()
+        return self
 
-def is_scalar_nan(x):
-    """Test if x is NaN.
+    def predict_proba(self, X):
+        check_is_fitted(self)
+        X = check_array(X)
+        proba_shape = (X.shape[0], self.classes_.size)
+        y_proba = np.zeros(shape=proba_shape, dtype=np.float64)
+        y_proba[:, self._most_frequent_class_idx] = 1.0
+        return y_proba
 
-    This function is meant to overcome the issue that np.isnan does not allow
-    non-numerical types as input, and that np.nan is not float('nan').
+    def predict(self, X):
+        y_proba = self.predict_proba(X)
+        y_pred = y_proba.argmax(axis=1)
+        return self.classes_[y_pred]
 
-    Parameters
-    ----------
-    x : any type
-        Any scalar value.
+    def score(self, X, y):
+        from sklearn.metrics import accuracy_score
 
-    Returns
-    -------
-    bool
-        Returns true if x is NaN, and false otherwise.
+        return accuracy_score(y, self.predict(X))
 
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.utils import is_scalar_nan
-    >>> is_scalar_nan(np.nan)
-    True
-    >>> is_scalar_nan(float("nan"))
-    True
-    >>> is_scalar_nan(None)
-    False
-    >>> is_scalar_nan("")
-    False
-    >>> is_scalar_nan([np.nan])
-    False
+
+class MinimalRegressor:
+    """Minimal regressor implementation without inheriting from BaseEstimator.
+
+    This estimator should be tested with:
+
+    * `check_estimator` in `test_estimator_checks.py`;
+    * within a `Pipeline` in `test_pipeline.py`;
+    * within a `SearchCV` in `test_search.py`.
     """
-    return (
-        not isinstance(x, numbers.Integral)
-        and isinstance(x, numbers.Real)
-        and math.isnan(x)
-    )
 
+    _estimator_type = "regressor"
 
-def _approximate_mode(class_counts, n_draws, rng):
-    """Computes approximate mode of multivariate hypergeometric.
+    def __init__(self, param=None):
+        self.param = param
 
-    This is an approximation to the mode of the multivariate
-    hypergeometric given by class_counts and n_draws.
-    It shouldn't be off by more than one.
+    def get_params(self, deep=True):
+        return {"param": self.param}
 
-    It is the mostly likely outcome of drawing n_draws many
-    samples from the population given by class_counts.
+    def set_params(self, **params):
+        for key, value in params.items():
+            setattr(self, key, value)
+        return self
 
-    Parameters
-    ----------
-    class_counts : ndarray of int
-        Population per class.
-    n_draws : int
-        Number of draws (samples to draw) from the overall population.
-    rng : random state
-        Used to break ties.
+    def fit(self, X, y):
+        X, y = check_X_y(X, y)
+        self.is_fitted_ = True
+        self._mean = np.mean(y)
+        return self
 
-    Returns
-    -------
-    sampled_classes : ndarray of int
-        Number of samples drawn from each class.
-        np.sum(sampled_classes) == n_draws
+    def predict(self, X):
+        check_is_fitted(self)
+        X = check_array(X)
+        return np.ones(shape=(X.shape[0],)) * self._mean
 
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.utils import _approximate_mode
-    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)
-    array([2, 1])
-    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)
-    array([3, 1])
-    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),
-    ...                   n_draws=2, rng=0)
-    array([0, 1, 1, 0])
-    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),
-    ...                   n_draws=2, rng=42)
-    array([1, 1, 0, 0])
-    """
-    rng = check_random_state(rng)
-    # this computes a bad approximation to the mode of the
-    # multivariate hypergeometric given by class_counts and n_draws
-    continuous = class_counts / class_counts.sum() * n_draws
-    # floored means we don't overshoot n_samples, but probably undershoot
-    floored = np.floor(continuous)
-    # we add samples according to how much "left over" probability
-    # they had, until we arrive at n_samples
-    need_to_add = int(n_draws - floored.sum())
-    if need_to_add > 0:
-        remainder = continuous - floored
-        values = np.sort(np.unique(remainder))[::-1]
-        # add according to remainder, but break ties
-        # randomly to avoid biases
-        for value in values:
-            (inds,) = np.where(remainder == value)
-            # if we need_to_add less than what's in inds
-            # we draw randomly from them.
-            # if we need to add more, we add them all and
-            # go to the next value
-            add_now = min(len(inds), need_to_add)
-            inds = rng.choice(inds, size=add_now, replace=False)
-            floored[inds] += 1
-            need_to_add -= add_now
-            if need_to_add == 0:
-                break
-    return floored.astype(int)
+    def score(self, X, y):
+        from sklearn.metrics import r2_score
 
+        return r2_score(y, self.predict(X))
 
-def check_matplotlib_support(caller_name):
-    """Raise ImportError with detailed error message if mpl is not installed.
 
-    Plot utilities like any of the Display's plotting functions should lazily import
-    matplotlib and call this helper before any computation.
+class MinimalTransformer:
+    """Minimal transformer implementation without inheriting from
+    BaseEstimator.
 
-    Parameters
-    ----------
-    caller_name : str
-        The name of the caller that requires matplotlib.
+    This estimator should be tested with:
+
+    * `check_estimator` in `test_estimator_checks.py`;
+    * within a `Pipeline` in `test_pipeline.py`;
+    * within a `SearchCV` in `test_search.py`.
     """
-    try:
-        import matplotlib  # noqa
-    except ImportError as e:
-        raise ImportError(
-            "{} requires matplotlib. You can install matplotlib with "
-            "`pip install matplotlib`".format(caller_name)
-        ) from e
 
+    def __init__(self, param=None):
+        self.param = param
 
-def check_pandas_support(caller_name):
-    """Raise ImportError with detailed error message if pandas is not installed.
+    def get_params(self, deep=True):
+        return {"param": self.param}
 
-    Plot utilities like :func:`fetch_openml` should lazily import
-    pandas and call this helper before any computation.
+    def set_params(self, **params):
+        for key, value in params.items():
+            setattr(self, key, value)
+        return self
 
-    Parameters
-    ----------
-    caller_name : str
-        The name of the caller that requires pandas.
+    def fit(self, X, y=None):
+        check_array(X)
+        self.is_fitted_ = True
+        return self
 
-    Returns
-    -------
-    pandas
-        The pandas package.
-    """
+    def transform(self, X, y=None):
+        check_is_fitted(self)
+        X = check_array(X)
+        return X
+
+    def fit_transform(self, X, y=None):
+        return self.fit(X, y).transform(X, y)
+
+
+def _array_api_for_tests(array_namespace, device):
     try:
-        import pandas  # noqa
+        array_mod = importlib.import_module(array_namespace)
+    except ModuleNotFoundError:
+        raise SkipTest(
+            f"{array_namespace} is not installed: not checking array_api input"
+        )
+    try:
+        import array_api_compat  # noqa
+    except ImportError:
+        raise SkipTest(
+            "array_api_compat is not installed: not checking array_api input"
+        )
+
+    # First create an array using the chosen array module and then get the
+    # corresponding (compatibility wrapped) array namespace based on it.
+    # This is because `cupy` is not the same as the compatibility wrapped
+    # namespace of a CuPy array.
+    xp = array_api_compat.get_namespace(array_mod.asarray(1))
+    if (
+        array_namespace == "torch"
+        and device == "cuda"
+        and not xp.backends.cuda.is_built()
+    ):
+        raise SkipTest("PyTorch test requires cuda, which is not available")
+    elif array_namespace == "torch" and device == "mps":
+        if os.getenv("PYTORCH_ENABLE_MPS_FALLBACK") != "1":
+            # For now we need PYTORCH_ENABLE_MPS_FALLBACK=1 for all estimators to work
+            # when using the MPS device.
+            raise SkipTest(
+                "Skipping MPS device test because PYTORCH_ENABLE_MPS_FALLBACK is not "
+                "set."
+            )
+        if not xp.backends.mps.is_built():
+            raise SkipTest(
+                "MPS is not available because the current PyTorch install was not "
+                "built with MPS enabled."
+            )
+    elif array_namespace in {"cupy", "cupy.array_api"}:  # pragma: nocover
+        import cupy
 
-        return pandas
-    except ImportError as e:
-        raise ImportError("{} requires pandas.".format(caller_name)) from e
+        if cupy.cuda.runtime.getDeviceCount() == 0:
+            raise SkipTest("CuPy test requires cuda, which is not available")
+    return xp
+
+
+def _get_warnings_filters_info_list():
+    @dataclass
+    class WarningInfo:
+        action: "warnings._ActionKind"
+        message: str = ""
+        category: type[Warning] = Warning
+
+        def to_filterwarning_str(self):
+            if self.category.__module__ == "builtins":
+                category = self.category.__name__
+            else:
+                category = f"{self.category.__module__}.{self.category.__name__}"
+
+            return f"{self.action}:{self.message}:{category}"
+
+    return [
+        WarningInfo("error", category=DeprecationWarning),
+        WarningInfo("error", category=FutureWarning),
+        WarningInfo("error", category=VisibleDeprecationWarning),
+        # TODO: remove when pyamg > 5.0.1
+        # Avoid a deprecation warning due pkg_resources usage in pyamg.
+        WarningInfo(
+            "ignore",
+            message="pkg_resources is deprecated as an API",
+            category=DeprecationWarning,
+        ),
+        WarningInfo(
+            "ignore",
+            message="Deprecated call to `pkg_resources",
+            category=DeprecationWarning,
+        ),
+        # pytest-cov issue https://github.com/pytest-dev/pytest-cov/issues/557 not
+        # fixed although it has been closed. https://github.com/pytest-dev/pytest-cov/pull/623
+        # would probably fix it.
+        WarningInfo(
+            "ignore",
+            message=(
+                "The --rsyncdir command line argument and rsyncdirs config variable are"
+                " deprecated"
+            ),
+            category=DeprecationWarning,
+        ),
+        # XXX: Easiest way to ignore pandas Pyarrow DeprecationWarning in the
+        # short-term. See https://github.com/pandas-dev/pandas/issues/54466 for
+        # more details.
+        WarningInfo(
+            "ignore",
+            message=r"\s*Pyarrow will become a required dependency",
+            category=DeprecationWarning,
+        ),
+        # warnings has been fixed from dateutil main but not released yet, see
+        # https://github.com/dateutil/dateutil/issues/1314
+        WarningInfo(
+            "ignore",
+            message="datetime.datetime.utcfromtimestamp",
+            category=DeprecationWarning,
+        ),
+        # Python 3.12 warnings from joblib fixed in master but not released yet,
+        # see https://github.com/joblib/joblib/pull/1518
+        WarningInfo(
+            "ignore", message="ast.Num is deprecated", category=DeprecationWarning
+        ),
+        WarningInfo(
+            "ignore", message="Attribute n is deprecated", category=DeprecationWarning
+        ),
+        # Python 3.12 warnings from sphinx-gallery fixed in master but not
+        # released yet, see
+        # https://github.com/sphinx-gallery/sphinx-gallery/pull/1242
+        WarningInfo(
+            "ignore", message="ast.Str is deprecated", category=DeprecationWarning
+        ),
+        WarningInfo(
+            "ignore", message="Attribute s is deprecated", category=DeprecationWarning
+        ),
+    ]
+
+
+def get_pytest_filterwarning_lines():
+    warning_filters_info_list = _get_warnings_filters_info_list()
+    return [
+        warning_info.to_filterwarning_str()
+        for warning_info in warning_filters_info_list
+    ]
+
+
+def turn_warnings_into_errors():
+    warnings_filters_info_list = _get_warnings_filters_info_list()
+    for warning_info in warnings_filters_info_list:
+        warnings.filterwarnings(
+            warning_info.action,
+            message=warning_info.message,
+            category=warning_info.category,
+        )
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_arpack.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_arpack.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_array_api.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_array_api.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,49 +1,80 @@
 """Tools to support array_api."""
+
 import itertools
 import math
 from functools import wraps
 
 import numpy
 import scipy.special as special
 
 from .._config import get_config
 from .fixes import parse_version
 
+_NUMPY_NAMESPACE_NAMES = {"numpy", "array_api_compat.numpy"}
 
-def yield_namespace_device_dtype_combinations():
-    """Yield supported namespace, device, dtype tuples for testing.
 
-    Use this to test that an estimator works with all combinations.
+def yield_namespaces(include_numpy_namespaces=True):
+    """Yield supported namespace.
+
+    This is meant to be used for testing purposes only.
+
+    Parameters
+    ----------
+    include_numpy_namespaces : bool, default=True
+        If True, also yield numpy namespaces.
 
     Returns
     -------
     array_namespace : str
         The name of the Array API namespace.
-
-    device : str
-        The name of the device on which to allocate the arrays. Can be None to
-        indicate that the default value should be used.
-
-    dtype_name : str
-        The name of the data type to use for arrays. Can be None to indicate
-        that the default value should be used.
     """
     for array_namespace in [
         # The following is used to test the array_api_compat wrapper when
         # array_api_dispatch is enabled: in particular, the arrays used in the
         # tests are regular numpy arrays without any "device" attribute.
         "numpy",
         # Stricter NumPy-based Array API implementation. The
-        # numpy.array_api.Array instances always a dummy "device" attribute.
-        "numpy.array_api",
+        # array_api_strict.Array instances always have a dummy "device" attribute.
+        "array_api_strict",
         "cupy",
         "cupy.array_api",
         "torch",
     ]:
+        if not include_numpy_namespaces and array_namespace in _NUMPY_NAMESPACE_NAMES:
+            continue
+        yield array_namespace
+
+
+def yield_namespace_device_dtype_combinations(include_numpy_namespaces=True):
+    """Yield supported namespace, device, dtype tuples for testing.
+
+    Use this to test that an estimator works with all combinations.
+
+    Parameters
+    ----------
+    include_numpy_namespaces : bool, default=True
+        If True, also yield numpy namespaces.
+
+    Returns
+    -------
+    array_namespace : str
+        The name of the Array API namespace.
+
+    device : str
+        The name of the device on which to allocate the arrays. Can be None to
+        indicate that the default value should be used.
+
+    dtype_name : str
+        The name of the data type to use for arrays. Can be None to indicate
+        that the default value should be used.
+    """
+    for array_namespace in yield_namespaces(
+        include_numpy_namespaces=include_numpy_namespaces
+    ):
         if array_namespace == "torch":
             for device, dtype in itertools.product(
                 ("cpu", "cuda"), ("float64", "float32")
             ):
                 yield array_namespace, device, dtype
             yield array_namespace, "mps", "float32"
         else:
@@ -70,30 +101,65 @@
         if numpy_version < parse_version(min_numpy_version):
             raise ImportError(
                 f"NumPy must be {min_numpy_version} or newer to dispatch array using"
                 " the API specification"
             )
 
 
-def device(x):
-    """Hardware device the array data resides on.
+def _single_array_device(array):
+    """Hardware device where the array data resides on."""
+    if isinstance(array, (numpy.ndarray, numpy.generic)) or not hasattr(
+        array, "device"
+    ):
+        return "cpu"
+    else:
+        return array.device
+
+
+def device(*array_list, remove_none=True, remove_types=(str,)):
+    """Hardware device where the array data resides on.
+
+    If the hardware device is not the same for all arrays, an error is raised.
 
     Parameters
     ----------
-    x : array
-        Array instance from NumPy or an array API compatible library.
+    *array_list : arrays
+        List of array instances from NumPy or an array API compatible library.
+
+    remove_none : bool, default=True
+        Whether to ignore None objects passed in array_list.
+
+    remove_types : tuple or list, default=(str,)
+        Types to ignore in array_list.
 
     Returns
     -------
     out : device
         `device` object (see the "Device Support" section of the array API spec).
     """
-    if isinstance(x, (numpy.ndarray, numpy.generic)):
-        return "cpu"
-    return x.device
+    array_list = _remove_non_arrays(
+        *array_list, remove_none=remove_none, remove_types=remove_types
+    )
+
+    # Note that _remove_non_arrays ensures that array_list is not empty.
+    device_ = _single_array_device(array_list[0])
+
+    # Note: here we cannot simply use a Python `set` as it requires
+    # hashable members which is not guaranteed for Array API device
+    # objects. In particular, CuPy devices are not hashable at the
+    # time of writing.
+    for array in array_list[1:]:
+        device_other = _single_array_device(array)
+        if device_ != device_other:
+            raise ValueError(
+                f"Input arrays use different devices: {str(device_)}, "
+                f"{str(device_other)}"
+            )
+
+    return device_
 
 
 def size(x):
     """Return the total number of elements of x.
 
     Parameters
     ----------
@@ -106,15 +172,15 @@
         Total number of elements.
     """
     return math.prod(x.shape)
 
 
 def _is_numpy_namespace(xp):
     """Return True if xp is backed by NumPy."""
-    return xp.__name__ in {"numpy", "array_api_compat.numpy", "numpy.array_api"}
+    return xp.__name__ in _NUMPY_NAMESPACE_NAMES
 
 
 def _union1d(a, b, xp):
     if _is_numpy_namespace(xp):
         return xp.asarray(numpy.union1d(a, b))
     assert a.ndim == b.ndim == 1
     return xp.unique_values(xp.concat([xp.unique_values(a), xp.unique_values(b)]))
@@ -145,15 +211,14 @@
                 _isdtype_single(dtype, k, xp=xp)
                 for k in ("signed integer", "unsigned integer")
             )
         elif kind == "real floating":
             return dtype in supported_float_dtypes(xp)
         elif kind == "complex floating":
             # Some name spaces do not have complex, such as cupy.array_api
-            # and numpy.array_api
             complex_dtypes = set()
             if hasattr(xp, "complex64"):
                 complex_dtypes.add(xp.complex64)
             if hasattr(xp, "complex128"):
                 complex_dtypes.add(xp.complex128)
             return dtype in complex_dtypes
         elif kind == "numeric":
@@ -164,28 +229,57 @@
         else:
             raise ValueError(f"Unrecognized data type kind: {kind!r}")
     else:
         return dtype == kind
 
 
 def supported_float_dtypes(xp):
-    """Supported floating point types for the namespace
+    """Supported floating point types for the namespace.
 
     Note: float16 is not officially part of the Array API spec at the
     time of writing but scikit-learn estimators and functions can choose
     to accept it when xp.float16 is defined.
 
     https://data-apis.org/array-api/latest/API_specification/data_types.html
     """
     if hasattr(xp, "float16"):
         return (xp.float64, xp.float32, xp.float16)
     else:
         return (xp.float64, xp.float32)
 
 
+def ensure_common_namespace_device(reference, *arrays):
+    """Ensure that all arrays use the same namespace and device as reference.
+
+    If neccessary the arrays are moved to the same namespace and device as
+    the reference array.
+
+    Parameters
+    ----------
+    reference : array
+        Reference array.
+
+    *arrays : array
+        Arrays to check.
+
+    Returns
+    -------
+    arrays : list
+        Arrays with the same namespace and device as reference.
+    """
+    xp, is_array_api = get_namespace(reference)
+
+    if is_array_api:
+        device_ = device(reference)
+        # Move arrays to the same namespace and device as the reference array.
+        return [xp.asarray(a, device=device_) for a in arrays]
+    else:
+        return arrays
+
+
 class _ArrayAPIWrapper:
     """sklearn specific Array API compatibility wrapper
 
     This wrapper makes it possible for scikit-learn maintainers to
     deal with discrepancies between different implementations of the
     Python Array API standard and its evolution over time.
 
@@ -222,22 +316,28 @@
 
     return wrapped_func
 
 
 class _NumPyAPIWrapper:
     """Array API compat wrapper for any numpy version
 
-    NumPy < 1.22 does not expose the numpy.array_api namespace. This
-    wrapper makes it possible to write code that uses the standard
-    Array API while working with any version of NumPy supported by
-    scikit-learn.
+    NumPy < 2 does not implement the namespace. NumPy 2 and later should
+    progressively implement more an more of the latest Array API spec but this
+    is still work in progress at this time.
+
+    This wrapper makes it possible to write code that uses the standard Array
+    API while working with any version of NumPy supported by scikit-learn.
 
     See the `get_namespace()` public function for more details.
     """
 
+    # TODO: once scikit-learn drops support for NumPy < 2, this class can be
+    # removed, assuming Array API compliance of NumPy 2 is actually sufficient
+    # for scikit-learn's needs.
+
     # Creation functions in spec:
     # https://data-apis.org/array-api/latest/API_specification/creation_functions.html
     _CREATION_FUNCS = {
         "arange",
         "empty",
         "empty_like",
         "eye",
@@ -327,81 +427,151 @@
     def isdtype(self, dtype, kind):
         return isdtype(dtype, kind, xp=self)
 
 
 _NUMPY_API_WRAPPER_INSTANCE = _NumPyAPIWrapper()
 
 
-def get_namespace(*arrays):
+def _remove_non_arrays(*arrays, remove_none=True, remove_types=(str,)):
+    """Filter arrays to exclude None and/or specific types.
+
+    Raise ValueError if no arrays are left after filtering.
+
+    Parameters
+    ----------
+    *arrays : array objects
+        Array objects.
+
+    remove_none : bool, default=True
+        Whether to ignore None objects passed in arrays.
+
+    remove_types : tuple or list, default=(str,)
+        Types to ignore in the arrays.
+
+    Returns
+    -------
+    filtered_arrays : list
+        List of arrays with None and typoe
+    """
+    filtered_arrays = []
+    remove_types = tuple(remove_types)
+    for array in arrays:
+        if remove_none and array is None:
+            continue
+        if isinstance(array, remove_types):
+            continue
+        filtered_arrays.append(array)
+
+    if not filtered_arrays:
+        raise ValueError(
+            f"At least one input array expected after filtering with {remove_none=}, "
+            f"remove_types=[{', '.join(t.__name__ for t in remove_types)}]. Got none. "
+            f"Original types: [{', '.join(type(a).__name__ for a in arrays)}]."
+        )
+    return filtered_arrays
+
+
+def get_namespace(*arrays, remove_none=True, remove_types=(str,), xp=None):
     """Get namespace of arrays.
 
-    Introspect `arrays` arguments and return their common Array API
-    compatible namespace object, if any. NumPy 1.22 and later can
-    construct such containers using the `numpy.array_api` namespace
-    for instance.
+    Introspect `arrays` arguments and return their common Array API compatible
+    namespace object, if any.
 
     See: https://numpy.org/neps/nep-0047-array-api-standard.html
 
-    If `arrays` are regular numpy arrays, an instance of the
-    `_NumPyAPIWrapper` compatibility wrapper is returned instead.
+    If `arrays` are regular numpy arrays, an instance of the `_NumPyAPIWrapper`
+    compatibility wrapper is returned instead.
 
-    Namespace support is not enabled by default. To enabled it
-    call:
+    Namespace support is not enabled by default. To enabled it call:
 
       sklearn.set_config(array_api_dispatch=True)
 
     or:
 
       with sklearn.config_context(array_api_dispatch=True):
           # your code here
 
-    Otherwise an instance of the `_NumPyAPIWrapper`
-    compatibility wrapper is always returned irrespective of
-    the fact that arrays implement the `__array_namespace__`
-    protocol or not.
+    Otherwise an instance of the `_NumPyAPIWrapper` compatibility wrapper is
+    always returned irrespective of the fact that arrays implement the
+    `__array_namespace__` protocol or not.
 
     Parameters
     ----------
     *arrays : array objects
         Array objects.
 
+    remove_none : bool, default=True
+        Whether to ignore None objects passed in arrays.
+
+    remove_types : tuple or list, default=(str,)
+        Types to ignore in the arrays.
+
+    xp : module, default=None
+        Precomputed array namespace module. When passed, typically from a caller
+        that has already performed inspection of its own inputs, skips array
+        namespace inspection.
+
     Returns
     -------
     namespace : module
         Namespace shared by array objects. If any of the `arrays` are not arrays,
         the namespace defaults to NumPy.
 
     is_array_api_compliant : bool
         True if the arrays are containers that implement the Array API spec.
         Always False when array_api_dispatch=False.
     """
     array_api_dispatch = get_config()["array_api_dispatch"]
     if not array_api_dispatch:
-        return _NUMPY_API_WRAPPER_INSTANCE, False
+        if xp is not None:
+            return xp, False
+        else:
+            return _NUMPY_API_WRAPPER_INSTANCE, False
+
+    if xp is not None:
+        return xp, True
+
+    arrays = _remove_non_arrays(
+        *arrays, remove_none=remove_none, remove_types=remove_types
+    )
 
     _check_array_api_dispatch(array_api_dispatch)
 
     # array-api-compat is a required dependency of scikit-learn only when
     # configuring `array_api_dispatch=True`. Its import should therefore be
     # protected by _check_array_api_dispatch to display an informative error
     # message in case it is missing.
     import array_api_compat
 
     namespace, is_array_api_compliant = array_api_compat.get_namespace(*arrays), True
 
     # These namespaces need additional wrapping to smooth out small differences
     # between implementations
-    if namespace.__name__ in {"numpy.array_api", "cupy.array_api"}:
+    if namespace.__name__ in {"cupy.array_api"}:
         namespace = _ArrayAPIWrapper(namespace)
 
     return namespace, is_array_api_compliant
 
 
-def _expit(X):
-    xp, _ = get_namespace(X)
+def get_namespace_and_device(*array_list, remove_none=True, remove_types=(str,)):
+    """Combination into one single function of `get_namespace` and `device`."""
+    array_list = _remove_non_arrays(
+        *array_list, remove_none=remove_none, remove_types=remove_types
+    )
+
+    skip_remove_kwargs = dict(remove_none=False, remove_types=[])
+
+    return (
+        *get_namespace(*array_list, **skip_remove_kwargs),
+        device(*array_list, **skip_remove_kwargs),
+    )
+
+
+def _expit(X, xp=None):
+    xp, _ = get_namespace(X, xp=xp)
     if _is_numpy_namespace(xp):
         return xp.asarray(special.expit(numpy.asarray(X)))
 
     return 1.0 / (1.0 + xp.exp(-X))
 
 
 def _add_to_diagonal(array, value, xp):
@@ -417,118 +587,187 @@
             array[i, i] += value[i]
     else:
         # scalar value
         for i in range(array.shape[0]):
             array[i, i] += value
 
 
-def _weighted_sum(sample_score, sample_weight, normalize=False, xp=None):
-    # XXX: this function accepts Array API input but returns a Python scalar
-    # float. The call to float() is convenient because it removes the need to
-    # move back results from device to host memory (e.g. calling `.cpu()` on a
-    # torch tensor). However, this might interact in unexpected ways (break?)
-    # with lazy Array API implementations. See:
-    # https://github.com/data-apis/array-api/issues/642
-    if xp is None:
-        xp, _ = get_namespace(sample_score)
-    if normalize and _is_numpy_namespace(xp):
-        sample_score_np = numpy.asarray(sample_score)
-        if sample_weight is not None:
-            sample_weight_np = numpy.asarray(sample_weight)
-        else:
-            sample_weight_np = None
-        return float(numpy.average(sample_score_np, weights=sample_weight_np))
+def _find_matching_floating_dtype(*arrays, xp):
+    """Find a suitable floating point dtype when computing with arrays.
+
+    If any of the arrays are floating point, return the dtype with the highest
+    precision by following official type promotion rules:
+
+    https://data-apis.org/array-api/latest/API_specification/type_promotion.html
+
+    If there are no floating point input arrays (all integral inputs for
+    instance), return the default floating point dtype for the namespace.
+    """
+    dtyped_arrays = [a for a in arrays if hasattr(a, "dtype")]
+    floating_dtypes = [
+        a.dtype for a in dtyped_arrays if xp.isdtype(a.dtype, "real floating")
+    ]
+    if floating_dtypes:
+        # Return the floating dtype with the highest precision:
+        return xp.result_type(*floating_dtypes)
+
+    # If none of the input arrays have a floating point dtype, they must be all
+    # integer arrays or containers of Python scalars: return the default
+    # floating point dtype for the namespace (implementation specific).
+    return xp.asarray(0.0).dtype
+
 
-    if not xp.isdtype(sample_score.dtype, "real floating"):
-        # We move to cpu device ahead of time since certain devices may not support
-        # float64, but we want the same precision for all devices and namespaces.
-        sample_score = xp.astype(xp.asarray(sample_score, device="cpu"), xp.float64)
-
-    if sample_weight is not None:
-        sample_weight = xp.asarray(
-            sample_weight, dtype=sample_score.dtype, device=device(sample_score)
+def _average(a, axis=None, weights=None, normalize=True, xp=None):
+    """Partial port of np.average to support the Array API.
+
+    It does a best effort at mimicking the return dtype rule described at
+    https://numpy.org/doc/stable/reference/generated/numpy.average.html but
+    only for the common cases needed in scikit-learn.
+    """
+    xp, _, device_ = get_namespace_and_device(a, weights)
+
+    if _is_numpy_namespace(xp):
+        if normalize:
+            return xp.asarray(numpy.average(a, axis=axis, weights=weights))
+        elif axis is None and weights is not None:
+            return xp.asarray(numpy.dot(a, weights))
+
+    a = xp.asarray(a, device=device_)
+    if weights is not None:
+        weights = xp.asarray(weights, device=device_)
+
+    if weights is not None and a.shape != weights.shape:
+        if axis is None:
+            raise TypeError(
+                f"Axis must be specified when the shape of a {tuple(a.shape)} and "
+                f"weights {tuple(weights.shape)} differ."
+            )
+
+        if weights.ndim != 1:
+            raise TypeError(
+                f"1D weights expected when a.shape={tuple(a.shape)} and "
+                f"weights.shape={tuple(weights.shape)} differ."
+            )
+
+        if size(weights) != a.shape[axis]:
+            raise ValueError(
+                f"Length of weights {size(weights)} not compatible with "
+                f" a.shape={tuple(a.shape)} and {axis=}."
+            )
+
+        # If weights are 1D, add singleton dimensions for broadcasting
+        shape = [1] * a.ndim
+        shape[axis] = a.shape[axis]
+        weights = xp.reshape(weights, shape)
+
+    if xp.isdtype(a.dtype, "complex floating"):
+        raise NotImplementedError(
+            "Complex floating point values are not supported by average."
+        )
+    if weights is not None and xp.isdtype(weights.dtype, "complex floating"):
+        raise NotImplementedError(
+            "Complex floating point values are not supported by average."
         )
-        if not xp.isdtype(sample_weight.dtype, "real floating"):
-            sample_weight = xp.astype(sample_weight, xp.float64)
 
-    if normalize:
-        if sample_weight is not None:
-            scale = xp.sum(sample_weight)
-        else:
-            scale = sample_score.shape[0]
-        if scale != 0:
-            sample_score = sample_score / scale
+    output_dtype = _find_matching_floating_dtype(a, weights, xp=xp)
+    a = xp.astype(a, output_dtype)
 
-    if sample_weight is not None:
-        return float(sample_score @ sample_weight)
-    else:
-        return float(xp.sum(sample_score))
+    if weights is None:
+        return (xp.mean if normalize else xp.sum)(a, axis=axis)
 
+    weights = xp.astype(weights, output_dtype)
 
-def _nanmin(X, axis=None):
+    sum_ = xp.sum(xp.multiply(a, weights), axis=axis)
+
+    if not normalize:
+        return sum_
+
+    scale = xp.sum(weights, axis=axis)
+    if xp.any(scale == 0.0):
+        raise ZeroDivisionError("Weights sum to zero, can't be normalized")
+
+    return sum_ / scale
+
+
+def _nanmin(X, axis=None, xp=None):
     # TODO: refactor once nan-aware reductions are standardized:
     # https://github.com/data-apis/array-api/issues/621
-    xp, _ = get_namespace(X)
+    xp, _ = get_namespace(X, xp=xp)
     if _is_numpy_namespace(xp):
         return xp.asarray(numpy.nanmin(X, axis=axis))
 
     else:
         mask = xp.isnan(X)
         X = xp.min(xp.where(mask, xp.asarray(+xp.inf, device=device(X)), X), axis=axis)
         # Replace Infs from all NaN slices with NaN again
         mask = xp.all(mask, axis=axis)
         if xp.any(mask):
             X = xp.where(mask, xp.asarray(xp.nan), X)
         return X
 
 
-def _nanmax(X, axis=None):
+def _nanmax(X, axis=None, xp=None):
     # TODO: refactor once nan-aware reductions are standardized:
     # https://github.com/data-apis/array-api/issues/621
-    xp, _ = get_namespace(X)
+    xp, _ = get_namespace(X, xp=xp)
     if _is_numpy_namespace(xp):
         return xp.asarray(numpy.nanmax(X, axis=axis))
 
     else:
         mask = xp.isnan(X)
         X = xp.max(xp.where(mask, xp.asarray(-xp.inf, device=device(X)), X), axis=axis)
         # Replace Infs from all NaN slices with NaN again
         mask = xp.all(mask, axis=axis)
         if xp.any(mask):
             X = xp.where(mask, xp.asarray(xp.nan), X)
         return X
 
 
-def _asarray_with_order(array, dtype=None, order=None, copy=None, *, xp=None):
+def _asarray_with_order(
+    array, dtype=None, order=None, copy=None, *, xp=None, device=None
+):
     """Helper to support the order kwarg only for NumPy-backed arrays
 
     Memory layout parameter `order` is not exposed in the Array API standard,
     however some input validation code in scikit-learn needs to work both
     for classes and functions that will leverage Array API only operations
     and for code that inherently relies on NumPy backed data containers with
     specific memory layout constraints (e.g. our own Cython code). The
     purpose of this helper is to make it possible to share code for data
     container validation without memory copies for both downstream use cases:
     the `order` parameter is only enforced if the input array implementation
     is NumPy based, otherwise `order` is just silently ignored.
     """
-    if xp is None:
-        xp, _ = get_namespace(array)
+    xp, _ = get_namespace(array, xp=xp)
     if _is_numpy_namespace(xp):
         # Use NumPy API to support order
         if copy is True:
             array = numpy.array(array, order=order, dtype=dtype)
         else:
             array = numpy.asarray(array, order=order, dtype=dtype)
 
         # At this point array is a NumPy ndarray. We convert it to an array
         # container that is consistent with the input's namespace.
         return xp.asarray(array)
     else:
-        return xp.asarray(array, dtype=dtype, copy=copy)
+        return xp.asarray(array, dtype=dtype, copy=copy, device=device)
+
+
+def _ravel(array, xp=None):
+    """Array API compliant version of np.ravel.
+
+    For non numpy namespaces, it just returns a flattened array, that might
+    be or not be a copy.
+    """
+    xp, _ = get_namespace(array, xp=xp)
+    if _is_numpy_namespace(xp):
+        array = numpy.asarray(array)
+        return xp.asarray(numpy.ravel(array, order="C"))
+
+    return xp.reshape(array, shape=(-1,))
 
 
 def _convert_to_numpy(array, xp):
     """Convert X into a NumPy ndarray on the CPU."""
     xp_name = xp.__name__
 
     if xp_name in {"array_api_compat.torch", "torch"}:
@@ -567,9 +806,33 @@
         if hasattr(attribute, "__dlpack__") or isinstance(attribute, numpy.ndarray):
             attribute = converter(attribute)
         setattr(new_estimator, key, attribute)
     return new_estimator
 
 
 def _atol_for_type(dtype):
-    """Return the absolute tolerance for a given dtype."""
+    """Return the absolute tolerance for a given numpy dtype."""
     return numpy.finfo(dtype).eps * 100
+
+
+def indexing_dtype(xp):
+    """Return a platform-specific integer dtype suitable for indexing.
+
+    On 32-bit platforms, this will typically return int32 and int64 otherwise.
+
+    Note: using dtype is recommended for indexing transient array
+    datastructures. For long-lived arrays, such as the fitted attributes of
+    estimators, it is instead recommended to use platform-independent int32 if
+    we do not expect to index more 2B elements. Using fixed dtypes simplifies
+    the handling of serialized models, e.g. to deploy a model fit on a 64-bit
+    platform to a target 32-bit platform such as WASM/pyodide.
+    """
+    # Currently this is implemented with simple hack that assumes that
+    # following "may be" statements in the Array API spec always hold:
+    # > The default integer data type should be the same across platforms, but
+    # > the default may vary depending on whether Python is 32-bit or 64-bit.
+    # > The default array index data type may be int32 on 32-bit platforms, but
+    # > the default should be int64 otherwise.
+    # https://data-apis.org/array-api/latest/API_specification/data_types.html#default-data-types
+    # TODO: once sufficiently adopted, we might want to instead rely on the
+    # newer inspection API: https://github.com/data-apis/array-api/issues/640
+    return xp.asarray(0).dtype
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_available_if.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_available_if.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_bunch.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_bunch.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_cython_blas.pxd` & `scikit_learn-1.5.0rc1/sklearn/utils/_cython_blas.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_cython_blas.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_cython_blas.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_encode.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_encode.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from collections import Counter
 from contextlib import suppress
 from typing import NamedTuple
 
 import numpy as np
 
-from . import is_scalar_nan
+from ._missing import is_scalar_nan
 
 
 def _unique(values, *, return_inverse=False, return_counts=False):
     """Helper function to find unique values with support for python objects.
 
     Uses pure python method for object dtype, and numpy method for
     all other dtypes.
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_estimator_html_repr.css` & `scikit_learn-1.5.0rc1/sklearn/utils/_estimator_html_repr.css`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_estimator_html_repr.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_estimator_html_repr.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_fast_dict.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_fast_dict.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_heap.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_heap.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_isfinite.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_isfinite.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_metadata_requests.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_metadata_requests.py`

 * *Files 3% similar despite different names*

```diff
@@ -395,29 +395,37 @@
             for prop, alias in self._requests.items()
             if alias == WARN and prop in params
         }
         for param in warn_params:
             warn(
                 f"Support for {param} has recently been added to this class. "
                 "To maintain backward compatibility, it is ignored now. "
-                "You can set the request value to False to silence this "
-                "warning, or to True to consume and use the metadata."
+                f"Using `set_{self.method}_request({param}={{True, False}})` "
+                "on this method of the class, you can set the request value "
+                "to False to silence this warning, or to True to consume and "
+                "use the metadata."
             )
 
-    def _route_params(self, params):
+    def _route_params(self, params, parent, caller):
         """Prepare the given parameters to be passed to the method.
 
         The output of this method can be used directly as the input to the
         corresponding method as extra props.
 
         Parameters
         ----------
         params : dict
             A dictionary of provided metadata.
 
+        parent : object
+            Parent class object, that routes the metadata.
+
+        caller : str
+            Method from the parent class object, where the metadata is routed from.
+
         Returns
         -------
         params : Bunch
             A :class:`~sklearn.utils.Bunch` of {prop: value} which can be given to the
             corresponding method.
         """
         self._check_warnings(params=params)
@@ -430,20 +438,34 @@
             elif alias is True and prop in args:
                 res[prop] = args[prop]
             elif alias is None and prop in args:
                 unrequested[prop] = args[prop]
             elif alias in args:
                 res[prop] = args[alias]
         if unrequested:
+            if self.method in COMPOSITE_METHODS:
+                callee_methods = COMPOSITE_METHODS[self.method]
+            else:
+                callee_methods = [self.method]
+            set_requests_on = "".join(
+                [
+                    f".set_{method}_request({{metadata}}=True/False)"
+                    for method in callee_methods
+                ]
+            )
+            message = (
+                f"[{', '.join([key for key in unrequested])}] are passed but are not"
+                " explicitly set as requested or not requested for"
+                f" {self.owner}.{self.method}, which is used within"
+                f" {parent}.{caller}. Call `{self.owner}"
+                + set_requests_on
+                + "` for each metadata you want to request/ignore."
+            )
             raise UnsetMetadataPassedError(
-                message=(
-                    f"[{', '.join([key for key in unrequested])}] are passed but are"
-                    " not explicitly set as requested or not for"
-                    f" {self.owner}.{self.method}"
-                ),
+                message=message,
                 unrequested_params=unrequested,
                 routed_params=res,
             )
         return res
 
     def _consumes(self, params):
         """Check whether the given parameters are consumed by this method.
@@ -587,36 +609,44 @@
         Returns
         -------
         names : set of str
             A set of strings with the names of all parameters.
         """
         return getattr(self, method)._get_param_names(return_alias=return_alias)
 
-    def _route_params(self, *, method, params):
+    def _route_params(self, *, params, method, parent, caller):
         """Prepare the given parameters to be passed to the method.
 
         The output of this method can be used directly as the input to the
         corresponding method as extra keyword arguments to pass metadata.
 
         Parameters
         ----------
+        params : dict
+            A dictionary of provided metadata.
+
         method : str
             The name of the method for which the parameters are requested and
             routed.
 
-        params : dict
-            A dictionary of provided metadata.
+        parent : object
+            Parent class object, that routes the metadata.
+
+        caller : str
+            Method from the parent class object, where the metadata is routed from.
 
         Returns
         -------
         params : Bunch
             A :class:`~sklearn.utils.Bunch` of {prop: value} which can be given to the
             corresponding method.
         """
-        return getattr(self, method)._route_params(params=params)
+        return getattr(self, method)._route_params(
+            params=params, parent=parent, caller=caller
+        )
 
     def _check_warnings(self, *, method, params):
         """Check whether metadata is passed which is marked as WARN.
 
         If any metadata is passed which is marked as WARN, a warning is raised.
 
         Parameters
@@ -659,108 +689,79 @@
 # This namedtuple is used to store a (mapping, routing) pair. Mapping is a
 # MethodMapping object, and routing is the output of `get_metadata_routing`.
 # MetadataRouter stores a collection of these namedtuples.
 RouterMappingPair = namedtuple("RouterMappingPair", ["mapping", "router"])
 
 # A namedtuple storing a single method route. A collection of these namedtuples
 # is stored in a MetadataRouter.
-MethodPair = namedtuple("MethodPair", ["callee", "caller"])
+MethodPair = namedtuple("MethodPair", ["caller", "callee"])
 
 
 class MethodMapping:
-    """Stores the mapping between callee and caller methods for a router.
+    """Stores the mapping between caller and callee methods for a router.
 
     This class is primarily used in a ``get_metadata_routing()`` of a router
     object when defining the mapping between a sub-object (a sub-estimator or a
-    scorer) to the router's methods. It stores a collection of ``Route``
-    namedtuples.
+    scorer) to the router's methods. It stores a collection of namedtuples.
 
     Iterating through an instance of this class will yield named
-    ``MethodPair(callee, caller)`` tuples.
+    ``MethodPair(caller, callee)`` tuples.
 
     .. versionadded:: 1.3
     """
 
     def __init__(self):
         self._routes = []
 
     def __iter__(self):
         return iter(self._routes)
 
-    def add(self, *, callee, caller):
+    def add(self, *, caller, callee):
         """Add a method mapping.
 
         Parameters
         ----------
-        callee : str
-            Child object's method name. This method is called in ``caller``.
 
         caller : str
             Parent estimator's method name in which the ``callee`` is called.
 
+        callee : str
+            Child object's method name. This method is called in ``caller``.
+
         Returns
         -------
         self : MethodMapping
             Returns self.
         """
-        if callee not in METHODS:
+        if caller not in METHODS:
             raise ValueError(
-                f"Given callee:{callee} is not a valid method. Valid methods are:"
+                f"Given caller:{caller} is not a valid method. Valid methods are:"
                 f" {METHODS}"
             )
-        if caller not in METHODS:
+        if callee not in METHODS:
             raise ValueError(
-                f"Given caller:{caller} is not a valid method. Valid methods are:"
+                f"Given callee:{callee} is not a valid method. Valid methods are:"
                 f" {METHODS}"
             )
-        self._routes.append(MethodPair(callee=callee, caller=caller))
+        self._routes.append(MethodPair(caller=caller, callee=callee))
         return self
 
     def _serialize(self):
         """Serialize the object.
 
         Returns
         -------
         obj : list
             A serialized version of the instance in the form of a list.
         """
         result = list()
         for route in self._routes:
-            result.append({"callee": route.callee, "caller": route.caller})
+            result.append({"caller": route.caller, "callee": route.callee})
         return result
 
-    @classmethod
-    def from_str(cls, route):
-        """Construct an instance from a string.
-
-        Parameters
-        ----------
-        route : str
-            A string representing the mapping, it can be:
-
-              - `"one-to-one"`: a one to one mapping for all methods.
-              - `"method"`: the name of a single method, such as ``fit``,
-                ``transform``, ``score``, etc.
-
-        Returns
-        -------
-        obj : MethodMapping
-            A :class:`~sklearn.utils.metadata_routing.MethodMapping` instance
-            constructed from the given string.
-        """
-        routing = cls()
-        if route == "one-to-one":
-            for method in METHODS:
-                routing.add(callee=method, caller=method)
-        elif route in METHODS:
-            routing.add(callee=route, caller=route)
-        else:
-            raise ValueError("route should be 'one-to-one' or a single method!")
-        return routing
-
     def __repr__(self):
         return str(self._serialize())
 
     def __str__(self):
         return str(repr(self))
 
 
@@ -834,32 +835,27 @@
         return self
 
     def add(self, *, method_mapping, **objs):
         """Add named objects with their corresponding method mapping.
 
         Parameters
         ----------
-        method_mapping : MethodMapping or str
-            The mapping between the child and the parent's methods. If str, the
-            output of :func:`~sklearn.utils.metadata_routing.MethodMapping.from_str`
-            is used.
+        method_mapping : MethodMapping
+            The mapping between the child and the parent's methods.
 
         **objs : dict
             A dictionary of objects from which metadata is extracted by calling
             :func:`~sklearn.utils.metadata_routing.get_routing_for_object` on them.
 
         Returns
         -------
         self : MetadataRouter
             Returns `self`.
         """
-        if isinstance(method_mapping, str):
-            method_mapping = MethodMapping.from_str(method_mapping)
-        else:
-            method_mapping = deepcopy(method_mapping)
+        method_mapping = deepcopy(method_mapping)
 
         for name, obj in objs.items():
             self._route_mappings[name] = RouterMappingPair(
                 mapping=method_mapping, router=get_routing_for_object(obj)
             )
         return self
 
@@ -882,15 +878,15 @@
             A set of parameters which are consumed by the given method.
         """
         res = set()
         if self._self_request:
             res = res | self._self_request.consumes(method=method, params=params)
 
         for _, route_mapping in self._route_mappings.items():
-            for callee, caller in route_mapping.mapping:
+            for caller, callee in route_mapping.mapping:
                 if caller == method:
                     res = res | route_mapping.router.consumes(
                         method=callee, params=params
                     )
 
         return res
 
@@ -925,51 +921,64 @@
             res = res.union(
                 self._self_request._get_param_names(
                     method=method, return_alias=return_alias
                 )
             )
 
         for name, route_mapping in self._route_mappings.items():
-            for callee, caller in route_mapping.mapping:
+            for caller, callee in route_mapping.mapping:
                 if caller == method:
                     res = res.union(
                         route_mapping.router._get_param_names(
                             method=callee, return_alias=True, ignore_self_request=False
                         )
                     )
         return res
 
-    def _route_params(self, *, params, method):
+    def _route_params(self, *, params, method, parent, caller):
         """Prepare the given parameters to be passed to the method.
 
         This is used when a router is used as a child object of another router.
         The parent router then passes all parameters understood by the child
         object to it and delegates their validation to the child.
 
         The output of this method can be used directly as the input to the
         corresponding method as extra props.
 
         Parameters
         ----------
+        params : dict
+            A dictionary of provided metadata.
+
         method : str
             The name of the method for which the parameters are requested and
             routed.
 
-        params : dict
-            A dictionary of provided metadata.
+        parent : object
+            Parent class object, that routes the metadata.
+
+        caller : str
+            Method from the parent class object, where the metadata is routed from.
 
         Returns
         -------
         params : Bunch
             A :class:`~sklearn.utils.Bunch` of {prop: value} which can be given to the
             corresponding method.
         """
         res = Bunch()
         if self._self_request:
-            res.update(self._self_request._route_params(params=params, method=method))
+            res.update(
+                self._self_request._route_params(
+                    params=params,
+                    method=method,
+                    parent=parent,
+                    caller=caller,
+                )
+            )
 
         param_names = self._get_param_names(
             method=method, return_alias=True, ignore_self_request=True
         )
         child_params = {
             key: value for key, value in params.items() if key in param_names
         }
@@ -986,17 +995,16 @@
 
         res.update(child_params)
         return res
 
     def route_params(self, *, caller, params):
         """Return the input parameters requested by child objects.
 
-        The output of this method is a bunch, which includes the inputs for all
-        methods of each child object that are used in the router's `caller`
-        method.
+        The output of this method is a bunch, which includes the metadata for all
+        methods of each child object that is used in the router's `caller` method.
 
         If the router is also a consumer, it also checks for warnings of
         `self`'s/consumer's requested metadata.
 
         Parameters
         ----------
         caller : str
@@ -1007,30 +1015,33 @@
         params : dict
             A dictionary of provided metadata.
 
         Returns
         -------
         params : Bunch
             A :class:`~sklearn.utils.Bunch` of the form
-            ``{"object_name": {"method_name": {prop: value}}}`` which can be
+            ``{"object_name": {"method_name": {params: value}}}`` which can be
             used to pass the required metadata to corresponding methods or
             corresponding child objects.
         """
         if self._self_request:
             self._self_request._check_warnings(params=params, method=caller)
 
         res = Bunch()
         for name, route_mapping in self._route_mappings.items():
             router, mapping = route_mapping.router, route_mapping.mapping
 
             res[name] = Bunch()
-            for _callee, _caller in mapping:
+            for _caller, _callee in mapping:
                 if _caller == caller:
                     res[name][_callee] = router._route_params(
-                        params=params, method=_callee
+                        params=params,
+                        method=_callee,
+                        parent=self.owner,
+                        caller=caller,
                     )
         return res
 
     def validate_metadata(self, *, method, params):
         """Validate given metadata for a method.
 
         This raises a ``TypeError`` if some of the passed metadata are not
@@ -1055,15 +1066,15 @@
             )
         else:
             self_params = set()
         extra_keys = set(params.keys()) - param_names - self_params
         if extra_keys:
             raise TypeError(
                 f"{self.owner}.{method} got unexpected argument(s) {extra_keys}, which"
-                " are not requested metadata in any object."
+                " are not routed to any object."
             )
 
     def _serialize(self):
         """Serialize the object.
 
         Returns
         -------
@@ -1078,20 +1089,19 @@
             res[name]["mapping"] = route_mapping.mapping._serialize()
             res[name]["router"] = route_mapping.router._serialize()
 
         return res
 
     def __iter__(self):
         if self._self_request:
-            yield (
-                "$self_request",
-                RouterMappingPair(
-                    mapping=MethodMapping.from_str("one-to-one"),
-                    router=self._self_request,
-                ),
+            method_mapping = MethodMapping()
+            for method in METHODS:
+                method_mapping.add(caller=method, callee=method)
+            yield "$self_request", RouterMappingPair(
+                mapping=method_mapping, router=self._self_request
             )
         for name, route_mapping in self._route_mappings.items():
             yield (name, route_mapping)
 
     def __repr__(self):
         return str(self._serialize())
 
@@ -1111,20 +1121,20 @@
     original object.
 
     .. versionadded:: 1.3
 
     Parameters
     ----------
     obj : object
+        - If the object provides a `get_metadata_routing` method, return a copy
+            of the output of that method.
         - If the object is already a
             :class:`~sklearn.utils.metadata_routing.MetadataRequest` or a
             :class:`~sklearn.utils.metadata_routing.MetadataRouter`, return a copy
             of that.
-        - If the object provides a `get_metadata_routing` method, return a copy
-            of the output of that method.
         - Returns an empty :class:`~sklearn.utils.metadata_routing.MetadataRequest`
             otherwise.
 
     Returns
     -------
     obj : MetadataRequest or MetadataRouting
         A ``MetadataRequest`` or a ``MetadataRouting`` taken or created from
@@ -1232,42 +1242,59 @@
     def __init__(self, name, keys, validate_keys=True):
         self.name = name
         self.keys = keys
         self.validate_keys = validate_keys
 
     def __get__(self, instance, owner):
         # we would want to have a method which accepts only the expected args
-        def func(**kw):
+        def func(*args, **kw):
             """Updates the request for provided parameters
 
             This docstring is overwritten below.
             See REQUESTER_DOC for expected functionality
             """
             if not _routing_enabled():
                 raise RuntimeError(
                     "This method is only available when metadata routing is enabled."
                     " You can enable it using"
                     " sklearn.set_config(enable_metadata_routing=True)."
                 )
 
             if self.validate_keys and (set(kw) - set(self.keys)):
                 raise TypeError(
-                    f"Unexpected args: {set(kw) - set(self.keys)}. Accepted arguments"
-                    f" are: {set(self.keys)}"
+                    f"Unexpected args: {set(kw) - set(self.keys)} in {self.name}. "
+                    f"Accepted arguments are: {set(self.keys)}"
+                )
+
+            # This makes it possible to use the decorated method as an unbound method,
+            # for instance when monkeypatching.
+            # https://github.com/scikit-learn/scikit-learn/issues/28632
+            if instance is None:
+                _instance = args[0]
+                args = args[1:]
+            else:
+                _instance = instance
+
+            # Replicating python's behavior when positional args are given other than
+            # `self`, and `self` is only allowed if this method is unbound.
+            if args:
+                raise TypeError(
+                    f"set_{self.name}_request() takes 0 positional argument but"
+                    f" {len(args)} were given"
                 )
 
-            requests = instance._get_metadata_request()
+            requests = _instance._get_metadata_request()
             method_metadata_request = getattr(requests, self.name)
 
             for prop, alias in kw.items():
                 if alias is not UNCHANGED:
                     method_metadata_request.add_request(param=prop, alias=alias)
-            instance._metadata_request = requests
+            _instance._metadata_request = requests
 
-            return instance
+            return _instance
 
         # Now we set the relevant attributes of the function so that it seems
         # like a normal method to the end user, with known expected arguments.
         func.__name__ = f"set_{self.name}_request"
         params = [
             inspect.Parameter(
                 name="self",
@@ -1425,31 +1452,29 @@
         # __metadata_request__* attributes take precedence over signature
         # sniffing.
 
         # need to go through the MRO since this is a class attribute and
         # ``vars`` doesn't report the parent class attributes. We go through
         # the reverse of the MRO so that child classes have precedence over
         # their parents.
-        defaults = dict()
+        substr = "__metadata_request__"
         for base_class in reversed(inspect.getmro(cls)):
-            base_defaults = {
-                attr: value
-                for attr, value in vars(base_class).items()
-                if "__metadata_request__" in attr
-            }
-            defaults.update(base_defaults)
-        defaults = dict(sorted(defaults.items()))
-
-        for attr, value in defaults.items():
-            # we don't check for attr.startswith() since python prefixes attrs
-            # starting with __ with the `_ClassName`.
-            substr = "__metadata_request__"
-            method = attr[attr.index(substr) + len(substr) :]
-            for prop, alias in value.items():
-                getattr(requests, method).add_request(param=prop, alias=alias)
+            for attr, value in vars(base_class).items():
+                if substr not in attr:
+                    continue
+                # we don't check for attr.startswith() since python prefixes attrs
+                # starting with __ with the `_ClassName`.
+                method = attr[attr.index(substr) + len(substr) :]
+                for prop, alias in value.items():
+                    # Here we add request values specified via those class attributes
+                    # to the `MetadataRequest` object. Adding a request which already
+                    # exists will override the previous one. Since we go through the
+                    # MRO in reverse order, the one specified by the lowest most classes
+                    # in the inheritance tree are the ones which take effect.
+                    getattr(requests, method).add_request(param=prop, alias=alias)
 
         return requests
 
     def _get_metadata_request(self):
         """Get requested data properties.
 
         Please check :ref:`User Guide <metadata_routing>` on how the routing
@@ -1494,17 +1519,18 @@
 # since they're positional only, users will never type those underscores.
 def process_routing(_obj, _method, /, **kwargs):
     """Validate and route input parameters.
 
     This function is used inside a router's method, e.g. :term:`fit`,
     to validate the metadata and handle the routing.
 
-    Assuming this signature: ``fit(self, X, y, sample_weight=None, **fit_params)``,
+    Assuming this signature of a router's fit method:
+    ``fit(self, X, y, sample_weight=None, **fit_params)``,
     a call to this function would be:
-    ``process_routing(self, sample_weight=sample_weight, **fit_params)``.
+    ``process_routing(self, "fit", sample_weight=sample_weight, **fit_params)``.
 
     Note that if routing is not enabled and ``kwargs`` is empty, then it
     returns an empty routing where ``process_routing(...).ANYTHING.ANY_METHOD``
     is always an empty dictionary.
 
     .. versionadded:: 1.3
 
@@ -1519,16 +1545,18 @@
 
     **kwargs : dict
         Metadata to be routed.
 
     Returns
     -------
     routed_params : Bunch
+        A :class:`~utils.Bunch` of the form ``{"object_name": {"method_name":
+        {params: value}}}`` which can be used to pass the required metadata to
         A :class:`~sklearn.utils.Bunch` of the form ``{"object_name": {"method_name":
-        {prop: value}}}`` which can be used to pass the required metadata to
+        {params: value}}}`` which can be used to pass the required metadata to
         corresponding methods or corresponding child objects. The object names
         are those defined in `obj.get_metadata_routing()`.
     """
     if not kwargs:
         # If routing is not enabled and kwargs are empty, then we don't have to
         # try doing any routing, we can simply return a structure which returns
         # an empty dict on routed_params.ANYTHING.ANY_METHOD.
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_mocking.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_mocking.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,19 @@
 import numpy as np
 
 from ..base import BaseEstimator, ClassifierMixin
 from ..utils._metadata_requests import RequestMethod
 from .metaestimators import available_if
-from .validation import _check_sample_weight, _num_samples, check_array, check_is_fitted
+from .validation import (
+    _check_sample_weight,
+    _num_samples,
+    check_array,
+    check_is_fitted,
+    check_random_state,
+)
 
 
 class ArraySlicingWrapper:
     """
     Parameters
     ----------
     array
@@ -129,23 +135,25 @@
         check_y_params=None,
         check_X=None,
         check_X_params=None,
         methods_to_check="all",
         foo_param=0,
         expected_sample_weight=None,
         expected_fit_params=None,
+        random_state=None,
     ):
         self.check_y = check_y
         self.check_y_params = check_y_params
         self.check_X = check_X
         self.check_X_params = check_X_params
         self.methods_to_check = methods_to_check
         self.foo_param = foo_param
         self.expected_sample_weight = expected_sample_weight
         self.expected_fit_params = expected_fit_params
+        self.random_state = random_state
 
     def _check_X_y(self, X, y=None, should_be_fitted=True):
         """Validate X and y and make extra check.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
@@ -239,15 +247,16 @@
         Returns
         -------
         preds : ndarray of shape (n_samples,)
             Predictions of the first class seens in `classes_`.
         """
         if self.methods_to_check == "all" or "predict" in self.methods_to_check:
             X, y = self._check_X_y(X)
-        return self.classes_[np.zeros(_num_samples(X), dtype=int)]
+        rng = check_random_state(self.random_state)
+        return rng.choice(self.classes_, size=_num_samples(X))
 
     def predict_proba(self, X):
         """Predict probabilities for each class.
 
         Here, the dummy classifier will provide a probability of 1 for the
         first class of `classes_` and 0 otherwise.
 
@@ -259,16 +268,18 @@
         Returns
         -------
         proba : ndarray of shape (n_samples, n_classes)
             The probabilities for each sample and class.
         """
         if self.methods_to_check == "all" or "predict_proba" in self.methods_to_check:
             X, y = self._check_X_y(X)
-        proba = np.zeros((_num_samples(X), len(self.classes_)))
-        proba[:, 0] = 1
+        rng = check_random_state(self.random_state)
+        proba = rng.randn(_num_samples(X), len(self.classes_))
+        proba = np.abs(proba, out=proba)
+        proba /= np.sum(proba, axis=1)[:, np.newaxis]
         return proba
 
     def decision_function(self, X):
         """Confidence score.
 
         Parameters
         ----------
@@ -282,22 +293,21 @@
             Confidence score.
         """
         if (
             self.methods_to_check == "all"
             or "decision_function" in self.methods_to_check
         ):
             X, y = self._check_X_y(X)
+        rng = check_random_state(self.random_state)
         if len(self.classes_) == 2:
             # for binary classifier, the confidence score is related to
             # classes_[1] and therefore should be null.
-            return np.zeros(_num_samples(X))
+            return rng.randn(_num_samples(X))
         else:
-            decision = np.zeros((_num_samples(X), len(self.classes_)))
-            decision[:, 0] = 1
-            return decision
+            return rng.randn(_num_samples(X), len(self.classes_))
 
     def score(self, X=None, Y=None):
         """Fake score.
 
         Parameters
         ----------
         X : array-like of shape (n_samples, n_features)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_openmp_helpers.pxd` & `scikit_learn-1.5.0rc1/sklearn/utils/_openmp_helpers.pxd`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_openmp_helpers.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_openmp_helpers.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_param_validation.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_param_validation.py`

 * *Files 1% similar despite different names*

```diff
@@ -574,15 +574,15 @@
         )
 
 
 class _Booleans(_Constraint):
     """Constraint representing boolean likes.
 
     Convenience class for
-    [bool, np.bool_, Integral (deprecated)]
+    [bool, np.bool_]
     """
 
     def __init__(self):
         super().__init__()
         self._constraints = [
             _InstancesOf(bool),
             _InstancesOf(np.bool_),
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_plotting.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_plotting.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import numpy as np
 
-from . import check_consistent_length, check_matplotlib_support
+from . import check_consistent_length
+from ._optional_dependencies import check_matplotlib_support
 from ._response import _get_response_values_binary
 from .multiclass import type_of_target
 from .validation import _check_pos_label_consistency
 
 
 class _BinaryClassifierCurveDisplayMixin:
     """Mixin class to be used in Displays requiring a binary classifier.
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_pprint.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_pprint.py`

 * *Files 0% similar despite different names*

```diff
@@ -65,15 +65,15 @@
 
 import inspect
 import pprint
 from collections import OrderedDict
 
 from .._config import get_config
 from ..base import BaseEstimator
-from . import is_scalar_nan
+from ._missing import is_scalar_nan
 
 
 class KeyValTuple(tuple):
     """Dummy class for correctly rendering key-value tuples from dicts."""
 
     def __repr__(self):
         # needed for _dispatch[tuple.__repr__] not to be overridden
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_random.pxd` & `scikit_learn-1.5.0rc1/sklearn/utils/_random.pxd`

 * *Files 14% similar despite different names*

```diff
@@ -1,36 +1,35 @@
 # Authors: Arnaud Joly
 #
 # License: BSD 3 clause
 
+from ._typedefs cimport uint32_t
 
-cimport numpy as cnp
-ctypedef cnp.npy_uint32 UINT32_t
 
-cdef inline UINT32_t DEFAULT_SEED = 1
+cdef inline uint32_t DEFAULT_SEED = 1
 
 cdef enum:
     # Max value for our rand_r replacement (near the bottom).
     # We don't use RAND_MAX because it's different across platforms and
     # particularly tiny on Windows/MSVC.
     # It corresponds to the maximum representable value for
     # 32-bit signed integers (i.e. 2^31 - 1).
     RAND_R_MAX = 2147483647
 
 
 # rand_r replacement using a 32bit XorShift generator
 # See http://www.jstatsoft.org/v08/i14/paper for details
-cdef inline UINT32_t our_rand_r(UINT32_t* seed) nogil:
+cdef inline uint32_t our_rand_r(uint32_t* seed) nogil:
     """Generate a pseudo-random np.uint32 from a np.uint32 seed"""
     # seed shouldn't ever be 0.
     if (seed[0] == 0):
         seed[0] = DEFAULT_SEED
 
-    seed[0] ^= <UINT32_t>(seed[0] << 13)
-    seed[0] ^= <UINT32_t>(seed[0] >> 17)
-    seed[0] ^= <UINT32_t>(seed[0] << 5)
+    seed[0] ^= <uint32_t>(seed[0] << 13)
+    seed[0] ^= <uint32_t>(seed[0] >> 17)
+    seed[0] ^= <uint32_t>(seed[0] << 5)
 
     # Use the modulo to make sure that we don't return a values greater than the
     # maximum representable value for signed 32bit integers (i.e. 2^31 - 1).
     # Note that the parenthesis are needed to avoid overflow: here
-    # RAND_R_MAX is cast to UINT32_t before 1 is added.
-    return seed[0] % ((<UINT32_t>RAND_R_MAX) + 1)
+    # RAND_R_MAX is cast to uint32_t before 1 is added.
+    return seed[0] % ((<uint32_t>RAND_R_MAX) + 1)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_random.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_random.pyx`

 * *Files 1% similar despite different names*

```diff
@@ -7,28 +7,28 @@
 This module complements missing features of ``numpy.random``.
 
 The module contains:
     * Several algorithms to sample integers without replacement.
     * Fast rand_r alternative based on xor shifts
 """
 import numpy as np
-cimport numpy as cnp
-cnp.import_array()
-
 from . import check_random_state
 
-cdef UINT32_t DEFAULT_SEED = 1
+from ._typedefs cimport intp_t
+
+
+cdef uint32_t DEFAULT_SEED = 1
 
 
 # Compatibility type to always accept the default int type used by NumPy, both
-# before and after NumPy 2. On Windows, `long` does not always match `cnp.inp_t`.
+# before and after NumPy 2. On Windows, `long` does not always match `inp_t`.
 # See the comments in the `sample_without_replacement` Python function for more
 # details.
 ctypedef fused default_int:
-    cnp.intp_t
+    intp_t
     long
 
 
 cpdef _sample_without_replacement_check_input(default_int n_population,
                                               default_int n_samples):
     """ Check that input are consistent for sample_without_replacement"""
     if n_population < 0:
@@ -322,15 +322,15 @@
     Examples
     --------
     >>> from sklearn.utils.random import sample_without_replacement
     >>> sample_without_replacement(10, 5, random_state=42)
     array([8, 1, 5, 0, 7])
     """
     cdef:
-        cnp.intp_t n_pop_intp, n_samples_intp
+        intp_t n_pop_intp, n_samples_intp
         long n_pop_long, n_samples_long
 
     # On most platforms `np.int_ is np.intp`.  However, before NumPy 2 the
     # default integer `np.int_` was a long which is 32bit on 64bit windows
     # while `intp` is 64bit on 64bit platforms and 32bit on 32bit ones.
     if np.int_ is np.intp:
         # Branch always taken on NumPy >=2 (or when not on 64bit windows).
@@ -347,9 +347,9 @@
         n_samples_long = n_samples
         return _sample_without_replacement(
                 n_pop_long, n_samples_long, method, random_state)
 
 
 def _our_rand_r_py(seed):
     """Python utils to test the our_rand_r function"""
-    cdef UINT32_t my_seed = seed
+    cdef uint32_t my_seed = seed
     return our_rand_r(&my_seed)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_response.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_response.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """Utilities to get the response values of a classifier or a regressor.
 
 It allows to make uniform checks and validation.
 """
+
 import numpy as np
 
 from ..base import is_classifier
 from .multiclass import type_of_target
 from .validation import _check_response_method, check_is_fitted
 
 
@@ -238,15 +239,17 @@
         y_pred, pos_label = prediction_method(X), None
 
     if return_response_method_used:
         return y_pred, pos_label, prediction_method.__name__
     return y_pred, pos_label
 
 
-def _get_response_values_binary(estimator, X, response_method, pos_label=None):
+def _get_response_values_binary(
+    estimator, X, response_method, pos_label=None, return_response_method_used=False
+):
     """Compute the response values of a binary classifier.
 
     Parameters
     ----------
     estimator : estimator instance
         Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`
         in which the last estimator is a binary classifier.
@@ -261,23 +264,35 @@
         :term:`decision_function` is tried next.
 
     pos_label : int, float, bool or str, default=None
         The class considered as the positive class when computing
         the metrics. By default, `estimators.classes_[1]` is
         considered as the positive class.
 
+    return_response_method_used : bool, default=False
+        Whether to return the response method used to compute the response
+        values.
+
+        .. versionadded:: 1.5
+
     Returns
     -------
     y_pred : ndarray of shape (n_samples,)
         Target scores calculated from the provided response_method
         and pos_label.
 
     pos_label : int, float, bool or str
         The class considered as the positive class when computing
         the metrics.
+
+    response_method_used : str
+        The response method used to compute the response values. Only returned
+        if `return_response_method_used` is `True`.
+
+        .. versionadded:: 1.5
     """
     classification_error = "Expected 'estimator' to be a binary classifier."
 
     check_is_fitted(estimator)
     if not is_classifier(estimator):
         raise ValueError(
             classification_error + f" Got {estimator.__class__.__name__} instead."
@@ -291,8 +306,9 @@
         response_method = ["predict_proba", "decision_function"]
 
     return _get_response_values(
         estimator,
         X,
         response_method,
         pos_label=pos_label,
+        return_response_method_used=return_response_method_used,
     )
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_seq_dataset.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/utils/_seq_dataset.pxd.tp`

 * *Files 4% similar despite different names*

```diff
@@ -9,37 +9,37 @@
 Generated file: _seq_dataset.pxd
 
 Each class is duplicated for all dtypes (float and double). The keywords
 between double braces are substituted in setup.py.
 """
 
 # name_suffix, c_type
-dtypes = [('64', 'double'),
-          ('32', 'float')]
+dtypes = [('64', 'float64_t'),
+          ('32', 'float32_t')]
 
 }}
 """Dataset abstractions for sequential data access."""
 
-cimport numpy as cnp
+from ._typedefs cimport float32_t, float64_t, intp_t, uint32_t
 
 # SequentialDataset and its two concrete subclasses are (optionally randomized)
 # iterators over the rows of a matrix X and corresponding target values y.
 
 {{for name_suffix, c_type in dtypes}}
 
 #------------------------------------------------------------------------------
 
 cdef class SequentialDataset{{name_suffix}}:
     cdef int current_index
     cdef int[::1] index
     cdef int *index_data_ptr
     cdef Py_ssize_t n_samples
-    cdef cnp.uint32_t seed
+    cdef uint32_t seed
 
-    cdef void shuffle(self, cnp.uint32_t seed) noexcept nogil
+    cdef void shuffle(self, uint32_t seed) noexcept nogil
     cdef int _get_next_index(self) noexcept nogil
     cdef int _get_random_index(self) noexcept nogil
 
     cdef void _sample(self, {{c_type}} **x_data_ptr, int **x_ind_ptr,
                       int *nnz, {{c_type}} *y, {{c_type}} *sample_weight,
                       int current_index) noexcept nogil
     cdef void next(self, {{c_type}} **x_data_ptr, int **x_ind_ptr,
@@ -49,15 +49,15 @@
 
 
 cdef class ArrayDataset{{name_suffix}}(SequentialDataset{{name_suffix}}):
     cdef const {{c_type}}[:, ::1] X
     cdef const {{c_type}}[::1] Y
     cdef const {{c_type}}[::1] sample_weights
     cdef Py_ssize_t n_features
-    cdef cnp.npy_intp X_stride
+    cdef intp_t X_stride
     cdef {{c_type}} *X_data_ptr
     cdef {{c_type}} *Y_data_ptr
     cdef const int[::1] feature_indices
     cdef int *feature_indices_ptr
     cdef {{c_type}} *sample_weight_data
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_seq_dataset.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/utils/_seq_dataset.pyx.tp`

 * *Files 6% similar despite different names*

```diff
@@ -14,28 +14,27 @@
         Arthur Imbert <arthurimbert05@gmail.com>
         Joan Massich <mailsik@gmail.com>
 
 License: BSD 3 clause
 """
 
 # name_suffix, c_type, np_type
-dtypes = [('64', 'double', 'np.float64'),
-          ('32', 'float', 'np.float32')]
+dtypes = [('64', 'float64_t', 'np.float64'),
+          ('32', 'float32_t', 'np.float32')]
 
 }}
 """Dataset abstractions for sequential data access."""
 
-cimport cython
-from libc.limits cimport INT_MAX
-cimport numpy as cnp
 import numpy as np
 
-cnp.import_array()
+cimport cython
+from libc.limits cimport INT_MAX
 
 from ._random cimport our_rand_r
+from ._typedefs cimport float32_t, float64_t, uint32_t
 
 {{for name_suffix, c_type, np_type in dtypes}}
 
 #------------------------------------------------------------------------------
 
 cdef class SequentialDataset{{name_suffix}}:
     """Base class for datasets with sequential data access.
@@ -58,15 +57,15 @@
         Index of current sample in ``index``.
         The index of current sample in the data is given by
         index_data_ptr[current_index].
 
     n_samples : Py_ssize_t
         Number of samples in the dataset.
 
-    seed : cnp.uint32_t
+    seed : uint32_t
         Seed used for random sampling. This attribute is modified at each call to the
         `random` method.
     """
 
     cdef void next(self, {{c_type}} **x_data_ptr, int **x_ind_ptr,
                    int *nnz, {{c_type}} *y, {{c_type}} *sample_weight) noexcept nogil:
         """Get the next example ``x`` from the dataset.
@@ -134,15 +133,15 @@
             Index of current sample.
         """
         cdef int current_index = self._get_random_index()
         self._sample(x_data_ptr, x_ind_ptr, nnz, y, sample_weight,
                      current_index)
         return current_index
 
-    cdef void shuffle(self, cnp.uint32_t seed) noexcept nogil:
+    cdef void shuffle(self, uint32_t seed) noexcept nogil:
         """Permutes the ordering of examples."""
         # Fisher-Yates shuffle
         cdef int *ind = self.index_data_ptr
         cdef int n = self.n_samples
         cdef unsigned i, j
         for i in range(n - 1):
             j = i + our_rand_r(&seed) % (n - i)
@@ -164,15 +163,15 @@
         return current_index
 
     cdef void _sample(self, {{c_type}} **x_data_ptr, int **x_ind_ptr,
                       int *nnz, {{c_type}} *y, {{c_type}} *sample_weight,
                       int current_index) noexcept nogil:
         pass
 
-    def _shuffle_py(self, cnp.uint32_t seed):
+    def _shuffle_py(self, uint32_t seed):
         """python function used for easy testing"""
         self.shuffle(seed)
 
     def _next_py(self):
         """python function used for easy testing"""
         cdef int current_index = self._get_next_index()
         return self._sample_py(current_index)
@@ -220,15 +219,15 @@
     """
 
     def __cinit__(
         self,
         const {{c_type}}[:, ::1] X,
         const {{c_type}}[::1] Y,
         const {{c_type}}[::1] sample_weights,
-        cnp.uint32_t seed=1,
+        uint32_t seed=1,
     ):
         """A ``SequentialDataset`` backed by a two-dimensional numpy array.
 
         Parameters
         ----------
         X : ndarray, dtype={{c_type}}, ndim=2, mode='c'
             The sample array, of shape(n_samples, n_features)
@@ -286,15 +285,15 @@
     def __cinit__(
         self,
         const {{c_type}}[::1] X_data,
         const int[::1] X_indptr,
         const int[::1] X_indices,
         const {{c_type}}[::1] Y,
         const {{c_type}}[::1] sample_weights,
-        cnp.uint32_t seed=1,
+        uint32_t seed=1,
     ):
         """Dataset backed by a scipy sparse CSR matrix.
 
         The feature indices of ``x`` are given by x_ind_ptr[0:nnz].
         The corresponding feature values are given by
         x_data_ptr[0:nnz].
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_set_output.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_set_output.py`

 * *Files 2% similar despite different names*

```diff
@@ -193,14 +193,32 @@
 
 
 ADAPTERS_MANAGER = ContainerAdaptersManager()
 ADAPTERS_MANAGER.register(PandasAdapter())
 ADAPTERS_MANAGER.register(PolarsAdapter())
 
 
+def _get_adapter_from_container(container):
+    """Get the adapter that knows how to handle such container.
+
+    See :class:`sklearn.utils._set_output.ContainerAdapterProtocol` for more
+    details.
+    """
+    module_name = container.__class__.__module__.split(".")[0]
+    try:
+        return ADAPTERS_MANAGER.adapters[module_name]
+    except KeyError as exc:
+        available_adapters = list(ADAPTERS_MANAGER.adapters.keys())
+        raise ValueError(
+            "The container does not have a registered adapter in scikit-learn. "
+            f"Available adapters are: {available_adapters} while the container "
+            f"provided is: {container!r}."
+        ) from exc
+
+
 def _get_container_adapter(method, estimator=None):
     """Get container adapter."""
     dense_config = _get_output_config(method, estimator)["dense"]
     try:
         return ADAPTERS_MANAGER.adapters[dense_config]
     except KeyError:
         return None
@@ -370,15 +388,15 @@
         """Set output container.
 
         See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
         for an example on how to use the API.
 
         Parameters
         ----------
-        transform : {"default", "pandas"}, default=None
+        transform : {"default", "pandas", "polars"}, default=None
             Configure output of `transform` and `fit_transform`.
 
             - `"default"`: Default output format of a transformer
             - `"pandas"`: DataFrame output
             - `"polars"`: Polars output
             - `None`: Transform configuration is unchanged
 
@@ -406,15 +424,15 @@
     This is used by meta-estimators to set the output for child estimators.
 
     Parameters
     ----------
     estimator : estimator instance
         Estimator instance.
 
-    transform : {"default", "pandas"}, default=None
+    transform : {"default", "pandas", "polars"}, default=None
         Configure output of the following estimator's methods:
 
         - `"transform"`
         - `"fit_transform"`
 
         If `None`, this operation is a no-op.
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_show_versions.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_show_versions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 """
 Utility methods to print system info for debugging
 
 adapted from :func:`pandas.show_versions`
 """
+
 # License: BSD 3 clause
 
 import platform
 import sys
 
+from threadpoolctl import threadpool_info
+
 from .. import __version__
-from ..utils.fixes import threadpool_info
 from ._openmp_helpers import _openmp_parallelism_enabled
 
 
 def _get_sys_info():
     """System information
 
     Returns
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_sorting.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_sorting.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_tags.py` & `scikit_learn-1.5.0rc1/sklearn/utils/_tags.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_testing.py` & `scikit_learn-1.5.0rc1/sklearn/model_selection/_classification_threshold.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1169 +1,1000 @@
-"""Testing utilities."""
+from collections.abc import MutableMapping
+from numbers import Integral, Real
 
-# Copyright (c) 2011, 2012
-# Authors: Pietro Berkes,
-#          Andreas Muller
-#          Mathieu Blondel
-#          Olivier Grisel
-#          Arnaud Joly
-#          Denis Engemann
-#          Giorgio Patrini
-#          Thierry Guillemot
-# License: BSD 3 clause
-import atexit
-import contextlib
-import functools
-import importlib
-import inspect
-import os
-import os.path as op
-import re
-import shutil
-import sys
-import tempfile
-import unittest
-import warnings
-from collections.abc import Iterable
-from dataclasses import dataclass
-from functools import wraps
-from inspect import signature
-from subprocess import STDOUT, CalledProcessError, TimeoutExpired, check_output
-from unittest import TestCase
-
-import joblib
 import numpy as np
-import scipy as sp
-from numpy.testing import assert_allclose as np_assert_allclose
-from numpy.testing import (
-    assert_almost_equal,
-    assert_approx_equal,
-    assert_array_almost_equal,
-    assert_array_equal,
-    assert_array_less,
-    assert_no_warnings,
-)
 
-import sklearn
-from sklearn.utils import (
-    _IS_32BIT,
-    IS_PYPY,
-    _in_unstable_openblas_configuration,
+from ..base import (
+    BaseEstimator,
+    ClassifierMixin,
+    MetaEstimatorMixin,
+    _fit_context,
+    clone,
+)
+from ..exceptions import NotFittedError
+from ..metrics import (
+    check_scoring,
+    get_scorer_names,
 )
-from sklearn.utils._array_api import _check_array_api_dispatch
-from sklearn.utils.fixes import VisibleDeprecationWarning, parse_version, sp_version
-from sklearn.utils.multiclass import check_classification_targets
-from sklearn.utils.validation import (
-    check_array,
+from ..metrics._scorer import _BaseScorer
+from ..utils import _safe_indexing
+from ..utils._param_validation import HasMethods, Interval, RealNotInt, StrOptions
+from ..utils._response import _get_response_values_binary
+from ..utils.metadata_routing import (
+    MetadataRouter,
+    MethodMapping,
+    _raise_for_params,
+    process_routing,
+)
+from ..utils.metaestimators import available_if
+from ..utils.multiclass import type_of_target
+from ..utils.parallel import Parallel, delayed
+from ..utils.validation import (
+    _check_method_params,
+    _num_samples,
     check_is_fitted,
-    check_X_y,
+    indexable,
 )
+from ._split import StratifiedShuffleSplit, check_cv
 
-__all__ = [
-    "assert_raises",
-    "assert_raises_regexp",
-    "assert_array_equal",
-    "assert_almost_equal",
-    "assert_array_almost_equal",
-    "assert_array_less",
-    "assert_approx_equal",
-    "assert_allclose",
-    "assert_run_python_script_without_output",
-    "assert_no_warnings",
-    "SkipTest",
-]
-
-_dummy = TestCase("__init__")
-assert_raises = _dummy.assertRaises
-SkipTest = unittest.case.SkipTest
-assert_dict_equal = _dummy.assertDictEqual
-
-assert_raises_regex = _dummy.assertRaisesRegex
-# assert_raises_regexp is deprecated in Python 3.4 in favor of
-# assert_raises_regex but lets keep the backward compat in scikit-learn with
-# the old name for now
-assert_raises_regexp = assert_raises_regex
-
-
-def ignore_warnings(obj=None, category=Warning):
-    """Context manager and decorator to ignore warnings.
-
-    Note: Using this (in both variants) will clear all warnings
-    from all python modules loaded. In case you need to test
-    cross-module-warning-logging, this is not your tool of choice.
-
-    Parameters
-    ----------
-    obj : callable, default=None
-        callable where you want to ignore the warnings.
-    category : warning class, default=Warning
-        The category to filter. If Warning, all categories will be muted.
 
-    Examples
-    --------
-    >>> import warnings
-    >>> from sklearn.utils._testing import ignore_warnings
-    >>> with ignore_warnings():
-    ...     warnings.warn('buhuhuhu')
-
-    >>> def nasty_warn():
-    ...     warnings.warn('buhuhuhu')
-    ...     print(42)
+def _estimator_has(attr):
+    """Check if we can delegate a method to the underlying estimator.
 
-    >>> ignore_warnings(nasty_warn)()
-    42
+    First, we check the fitted estimator if available, otherwise we
+    check the unfitted estimator.
     """
-    if isinstance(obj, type) and issubclass(obj, Warning):
-        # Avoid common pitfall of passing category as the first positional
-        # argument which result in the test not being run
-        warning_name = obj.__name__
-        raise ValueError(
-            "'obj' should be a callable where you want to ignore warnings. "
-            "You passed a warning class instead: 'obj={warning_name}'. "
-            "If you want to pass a warning class to ignore_warnings, "
-            "you should use 'category={warning_name}'".format(warning_name=warning_name)
-        )
-    elif callable(obj):
-        return _IgnoreWarnings(category=category)(obj)
+
+    def check(self):
+        if hasattr(self, "estimator_"):
+            getattr(self.estimator_, attr)
+        else:
+            getattr(self.estimator, attr)
+        return True
+
+    return check
+
+
+def _threshold_scores_to_class_labels(y_score, threshold, classes, pos_label):
+    """Threshold `y_score` and return the associated class labels."""
+    if pos_label is None:
+        map_thresholded_score_to_label = np.array([0, 1])
     else:
-        return _IgnoreWarnings(category=category)
+        pos_label_idx = np.flatnonzero(classes == pos_label)[0]
+        neg_label_idx = np.flatnonzero(classes != pos_label)[0]
+        map_thresholded_score_to_label = np.array([neg_label_idx, pos_label_idx])
+
+    return classes[map_thresholded_score_to_label[(y_score >= threshold).astype(int)]]
+
 
+class BaseThresholdClassifier(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):
+    """Base class for binary classifiers that set a non-default decision threshold.
 
-class _IgnoreWarnings:
-    """Improved and simplified Python warnings context manager and decorator.
+    In this base class, we define the following interface:
 
-    This class allows the user to ignore the warnings raised by a function.
-    Copied from Python 2.7.5 and modified as required.
+    - the validation of common parameters in `fit`;
+    - the different prediction methods that can be used with the classifier.
+
+    .. versionadded:: 1.5
 
     Parameters
     ----------
-    category : tuple of warning class, default=Warning
-        The category to filter. By default, all the categories will be muted.
+    estimator : estimator instance
+        The binary classifier, fitted or not, for which we want to optimize
+        the decision threshold used during `predict`.
+
+    response_method : {"auto", "decision_function", "predict_proba"}, default="auto"
+        Methods by the classifier `estimator` corresponding to the
+        decision function for which we want to find a threshold. It can be:
 
+        * if `"auto"`, it will try to invoke, for each classifier,
+          `"predict_proba"` or `"decision_function"` in that order.
+        * otherwise, one of `"predict_proba"` or `"decision_function"`.
+          If the method is not implemented by the classifier, it will raise an
+          error.
     """
 
-    def __init__(self, category):
-        self._record = True
-        self._module = sys.modules["warnings"]
-        self._entered = False
-        self.log = []
-        self.category = category
-
-    def __call__(self, fn):
-        """Decorator to catch and hide warnings without visual nesting."""
-
-        @wraps(fn)
-        def wrapper(*args, **kwargs):
-            with warnings.catch_warnings():
-                warnings.simplefilter("ignore", self.category)
-                return fn(*args, **kwargs)
-
-        return wrapper
-
-    def __repr__(self):
-        args = []
-        if self._record:
-            args.append("record=True")
-        if self._module is not sys.modules["warnings"]:
-            args.append("module=%r" % self._module)
-        name = type(self).__name__
-        return "%s(%s)" % (name, ", ".join(args))
-
-    def __enter__(self):
-        if self._entered:
-            raise RuntimeError("Cannot enter %r twice" % self)
-        self._entered = True
-        self._filters = self._module.filters
-        self._module.filters = self._filters[:]
-        self._showwarning = self._module.showwarning
-        warnings.simplefilter("ignore", self.category)
-
-    def __exit__(self, *exc_info):
-        if not self._entered:
-            raise RuntimeError("Cannot exit %r without entering first" % self)
-        self._module.filters = self._filters
-        self._module.showwarning = self._showwarning
-        self.log[:] = []
-
-
-def assert_raise_message(exceptions, message, function, *args, **kwargs):
-    """Helper function to test the message raised in an exception.
-
-    Given an exception, a callable to raise the exception, and
-    a message string, tests that the correct exception is raised and
-    that the message is a substring of the error thrown. Used to test
-    that the specific message thrown during an exception is correct.
+    _required_parameters = ["estimator"]
+    _parameter_constraints: dict = {
+        "estimator": [
+            HasMethods(["fit", "predict_proba"]),
+            HasMethods(["fit", "decision_function"]),
+        ],
+        "response_method": [StrOptions({"auto", "predict_proba", "decision_function"})],
+    }
 
-    Parameters
-    ----------
-    exceptions : exception or tuple of exception
-        An Exception object.
+    def __init__(self, estimator, *, response_method="auto"):
+        self.estimator = estimator
+        self.response_method = response_method
 
-    message : str
-        The error message or a substring of the error message.
+    @_fit_context(
+        # *ThresholdClassifier*.estimator is not validated yet
+        prefer_skip_nested_validation=False
+    )
+    def fit(self, X, y, **params):
+        """Fit the classifier.
 
-    function : callable
-        Callable object to raise error.
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training data.
+
+        y : array-like of shape (n_samples,)
+            Target values.
+
+        **params : dict
+            Parameters to pass to the `fit` method of the underlying
+            classifier.
+
+        Returns
+        -------
+        self : object
+            Returns an instance of self.
+        """
+        _raise_for_params(params, self, None)
 
-    *args : the positional arguments to `function`.
+        X, y = indexable(X, y)
 
-    **kwargs : the keyword arguments to `function`.
-    """
-    try:
-        function(*args, **kwargs)
-    except exceptions as e:
-        error_message = str(e)
-        if message not in error_message:
-            raise AssertionError(
-                "Error message does not include the expected"
-                " string: %r. Observed error message: %r" % (message, error_message)
+        y_type = type_of_target(y, input_name="y")
+        if y_type != "binary":
+            raise ValueError(
+                f"Only binary classification is supported. Unknown label type: {y_type}"
             )
-    else:
-        # concatenate exception names
-        if isinstance(exceptions, tuple):
-            names = " or ".join(e.__name__ for e in exceptions)
+
+        if self.response_method == "auto":
+            self._response_method = ["predict_proba", "decision_function"]
         else:
-            names = exceptions.__name__
+            self._response_method = self.response_method
 
-        raise AssertionError("%s not raised by %s" % (names, function.__name__))
+        self._fit(X, y, **params)
 
+        if hasattr(self.estimator_, "n_features_in_"):
+            self.n_features_in_ = self.estimator_.n_features_in_
+        if hasattr(self.estimator_, "feature_names_in_"):
+            self.feature_names_in_ = self.estimator_.feature_names_in_
 
-def assert_allclose(
-    actual, desired, rtol=None, atol=0.0, equal_nan=True, err_msg="", verbose=True
-):
-    """dtype-aware variant of numpy.testing.assert_allclose
+        return self
+
+    @property
+    def classes_(self):
+        """Classes labels."""
+        return self.estimator_.classes_
 
-    This variant introspects the least precise floating point dtype
-    in the input argument and automatically sets the relative tolerance
-    parameter to 1e-4 float32 and use 1e-7 otherwise (typically float64
-    in scikit-learn).
-
-    `atol` is always left to 0. by default. It should be adjusted manually
-    to an assertion-specific value in case there are null values expected
-    in `desired`.
+    @available_if(_estimator_has("predict_proba"))
+    def predict_proba(self, X):
+        """Predict class probabilities for `X` using the fitted estimator.
 
-    The aggregate tolerance is `atol + rtol * abs(desired)`.
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        Returns
+        -------
+        probabilities : ndarray of shape (n_samples, n_classes)
+            The class probabilities of the input samples.
+        """
+        check_is_fitted(self, "estimator_")
+        return self.estimator_.predict_proba(X)
+
+    @available_if(_estimator_has("predict_log_proba"))
+    def predict_log_proba(self, X):
+        """Predict logarithm class probabilities for `X` using the fitted estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        Returns
+        -------
+        log_probabilities : ndarray of shape (n_samples, n_classes)
+            The logarithm class probabilities of the input samples.
+        """
+        check_is_fitted(self, "estimator_")
+        return self.estimator_.predict_log_proba(X)
+
+    @available_if(_estimator_has("decision_function"))
+    def decision_function(self, X):
+        """Decision function for samples in `X` using the fitted estimator.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        Returns
+        -------
+        decisions : ndarray of shape (n_samples,)
+            The decision function computed the fitted estimator.
+        """
+        check_is_fitted(self, "estimator_")
+        return self.estimator_.decision_function(X)
+
+    def _more_tags(self):
+        return {
+            "binary_only": True,
+            "_xfail_checks": {
+                "check_classifiers_train": "Threshold at probability 0.5 does not hold",
+                "check_sample_weights_invariance": (
+                    "Due to the cross-validation and sample ordering, removing a sample"
+                    " is not strictly equal to putting is weight to zero. Specific unit"
+                    " tests are added for TunedThresholdClassifierCV specifically."
+                ),
+            },
+        }
+
+
+class FixedThresholdClassifier(BaseThresholdClassifier):
+    """Binary classifier that manually sets the decision threshold.
+
+    This classifier allows to change the default decision threshold used for
+    converting posterior probability estimates (i.e. output of `predict_proba`) or
+    decision scores (i.e. output of `decision_function`) into a class label.
+
+    Here, the threshold is not optimized and is set to a constant value.
+
+    Read more in the :ref:`User Guide <FixedThresholdClassifier>`.
+
+    .. versionadded:: 1.5
+
+    Parameters
+    ----------
+    estimator : estimator instance
+        The binary classifier, fitted or not, for which we want to optimize
+        the decision threshold used during `predict`.
+
+    threshold : {"auto"} or float, default="auto"
+        The decision threshold to use when converting posterior probability estimates
+        (i.e. output of `predict_proba`) or decision scores (i.e. output of
+        `decision_function`) into a class label. When `"auto"`, the threshold is set
+        to 0.5 if `predict_proba` is used as `response_method`, otherwise it is set to
+        0 (i.e. the default threshold for `decision_function`).
+
+    pos_label : int, float, bool or str, default=None
+        The label of the positive class. Used to process the output of the
+        `response_method` method. When `pos_label=None`, if `y_true` is in `{-1, 1}` or
+        `{0, 1}`, `pos_label` is set to 1, otherwise an error will be raised.
+
+    response_method : {"auto", "decision_function", "predict_proba"}, default="auto"
+        Methods by the classifier `estimator` corresponding to the
+        decision function for which we want to find a threshold. It can be:
+
+        * if `"auto"`, it will try to invoke `"predict_proba"` or `"decision_function"`
+          in that order.
+        * otherwise, one of `"predict_proba"` or `"decision_function"`.
+          If the method is not implemented by the classifier, it will raise an
+          error.
 
-    Parameters
+    Attributes
     ----------
-    actual : array_like
-        Array obtained.
-    desired : array_like
-        Array desired.
-    rtol : float, optional, default=None
-        Relative tolerance.
-        If None, it is set based on the provided arrays' dtypes.
-    atol : float, optional, default=0.
-        Absolute tolerance.
-    equal_nan : bool, optional, default=True
-        If True, NaNs will compare equal.
-    err_msg : str, optional, default=''
-        The error message to be printed in case of failure.
-    verbose : bool, optional, default=True
-        If True, the conflicting values are appended to the error message.
-
-    Raises
-    ------
-    AssertionError
-        If actual and desired are not equal up to specified precision.
+    estimator_ : estimator instance
+        The fitted classifier used when predicting.
+
+    classes_ : ndarray of shape (n_classes,)
+        The class labels.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
 
     See Also
     --------
-    numpy.testing.assert_allclose
+    sklearn.model_selection.TunedThresholdClassifierCV : Classifier that post-tunes
+        the decision threshold based on some metrics and using cross-validation.
+    sklearn.calibration.CalibratedClassifierCV : Estimator that calibrates
+        probabilities.
 
     Examples
     --------
-    >>> import numpy as np
-    >>> from sklearn.utils._testing import assert_allclose
-    >>> x = [1e-5, 1e-3, 1e-1]
-    >>> y = np.arccos(np.cos(x))
-    >>> assert_allclose(x, y, rtol=1e-5, atol=0)
-    >>> a = np.full(shape=10, fill_value=1e-5, dtype=np.float32)
-    >>> assert_allclose(a, 1e-5)
-    """
-    dtypes = []
-
-    actual, desired = np.asanyarray(actual), np.asanyarray(desired)
-    dtypes = [actual.dtype, desired.dtype]
-
-    if rtol is None:
-        rtols = [1e-4 if dtype == np.float32 else 1e-7 for dtype in dtypes]
-        rtol = max(rtols)
-
-    np_assert_allclose(
-        actual,
-        desired,
-        rtol=rtol,
-        atol=atol,
-        equal_nan=equal_nan,
-        err_msg=err_msg,
-        verbose=verbose,
-    )
-
+    >>> from sklearn.datasets import make_classification
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> from sklearn.metrics import confusion_matrix
+    >>> from sklearn.model_selection import FixedThresholdClassifier, train_test_split
+    >>> X, y = make_classification(
+    ...     n_samples=1_000, weights=[0.9, 0.1], class_sep=0.8, random_state=42
+    ... )
+    >>> X_train, X_test, y_train, y_test = train_test_split(
+    ...     X, y, stratify=y, random_state=42
+    ... )
+    >>> classifier = LogisticRegression(random_state=0).fit(X_train, y_train)
+    >>> print(confusion_matrix(y_test, classifier.predict(X_test)))
+    [[217   7]
+     [ 19   7]]
+    >>> classifier_other_threshold = FixedThresholdClassifier(
+    ...     classifier, threshold=0.1, response_method="predict_proba"
+    ... ).fit(X_train, y_train)
+    >>> print(confusion_matrix(y_test, classifier_other_threshold.predict(X_test)))
+    [[184  40]
+     [  6  20]]
+    """
+
+    _parameter_constraints: dict = {
+        **BaseThresholdClassifier._parameter_constraints,
+        "threshold": [StrOptions({"auto"}), Real],
+        "pos_label": [Real, str, "boolean", None],
+    }
+
+    def __init__(
+        self,
+        estimator,
+        *,
+        threshold="auto",
+        pos_label=None,
+        response_method="auto",
+    ):
+        super().__init__(estimator=estimator, response_method=response_method)
+        self.pos_label = pos_label
+        self.threshold = threshold
+
+    def _fit(self, X, y, **params):
+        """Fit the classifier.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training data.
+
+        y : array-like of shape (n_samples,)
+            Target values.
+
+        **params : dict
+            Parameters to pass to the `fit` method of the underlying
+            classifier.
+
+        Returns
+        -------
+        self : object
+            Returns an instance of self.
+        """
+        routed_params = process_routing(self, "fit", **params)
+        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)
+        return self
 
-def assert_allclose_dense_sparse(x, y, rtol=1e-07, atol=1e-9, err_msg=""):
-    """Assert allclose for sparse and dense data.
+    def predict(self, X):
+        """Predict the target of new samples.
 
-    Both x and y need to be either sparse or dense, they
-    can't be mixed.
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The samples, as accepted by `estimator.predict`.
+
+        Returns
+        -------
+        class_labels : ndarray of shape (n_samples,)
+            The predicted class.
+        """
+        check_is_fitted(self, "estimator_")
+        y_score, _, response_method_used = _get_response_values_binary(
+            self.estimator_,
+            X,
+            self._response_method,
+            pos_label=self.pos_label,
+            return_response_method_used=True,
+        )
 
-    Parameters
-    ----------
-    x : {array-like, sparse matrix}
-        First array to compare.
+        if self.threshold == "auto":
+            decision_threshold = 0.5 if response_method_used == "predict_proba" else 0.0
+        else:
+            decision_threshold = self.threshold
 
-    y : {array-like, sparse matrix}
-        Second array to compare.
+        return _threshold_scores_to_class_labels(
+            y_score, decision_threshold, self.classes_, self.pos_label
+        )
 
-    rtol : float, default=1e-07
-        relative tolerance; see numpy.allclose.
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
 
-    atol : float, default=1e-9
-        absolute tolerance; see numpy.allclose. Note that the default here is
-        more tolerant than the default for numpy.testing.assert_allclose, where
-        atol=0.
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
 
-    err_msg : str, default=''
-        Error message to raise.
-    """
-    if sp.sparse.issparse(x) and sp.sparse.issparse(y):
-        x = x.tocsr()
-        y = y.tocsr()
-        x.sum_duplicates()
-        y.sum_duplicates()
-        assert_array_equal(x.indices, y.indices, err_msg=err_msg)
-        assert_array_equal(x.indptr, y.indptr, err_msg=err_msg)
-        assert_allclose(x.data, y.data, rtol=rtol, atol=atol, err_msg=err_msg)
-    elif not sp.sparse.issparse(x) and not sp.sparse.issparse(y):
-        # both dense
-        assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
-    else:
-        raise ValueError(
-            "Can only compare two sparse matrices, not a sparse matrix and an array."
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = MetadataRouter(owner=self.__class__.__name__).add(
+            estimator=self.estimator,
+            method_mapping=MethodMapping().add(callee="fit", caller="fit"),
         )
+        return router
 
 
-def set_random_state(estimator, random_state=0):
-    """Set random state of an estimator if it has the `random_state` param.
+class _CurveScorer(_BaseScorer):
+    """Scorer taking a continuous response and output a score for each threshold.
 
     Parameters
     ----------
-    estimator : object
-        The estimator.
-    random_state : int, RandomState instance or None, default=0
-        Pseudo random number generator state.
-        Pass an int for reproducible results across multiple function calls.
-        See :term:`Glossary <random_state>`.
-    """
-    if "random_state" in estimator.get_params():
-        estimator.set_params(random_state=random_state)
-
-
-try:
-    _check_array_api_dispatch(True)
-    ARRAY_API_COMPAT_FUNCTIONAL = True
-except ImportError:
-    ARRAY_API_COMPAT_FUNCTIONAL = False
-
-try:
-    import pytest
-
-    skip_if_32bit = pytest.mark.skipif(_IS_32BIT, reason="skipped on 32bit platforms")
-    fails_if_pypy = pytest.mark.xfail(IS_PYPY, reason="not compatible with PyPy")
-    fails_if_unstable_openblas = pytest.mark.xfail(
-        _in_unstable_openblas_configuration(),
-        reason="OpenBLAS is unstable for this configuration",
-    )
-    skip_if_no_parallel = pytest.mark.skipif(
-        not joblib.parallel.mp, reason="joblib is in serial mode"
-    )
-    skip_if_array_api_compat_not_configured = pytest.mark.skipif(
-        not ARRAY_API_COMPAT_FUNCTIONAL,
-        reason="requires array_api_compat installed and a new enough version of NumPy",
-    )
-
-    #  Decorator for tests involving both BLAS calls and multiprocessing.
-    #
-    #  Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction
-    #  with some implementation of BLAS (or other libraries that manage an
-    #  internal posix thread pool) can cause a crash or a freeze of the Python
-    #  process.
-    #
-    #  In practice all known packaged distributions (from Linux distros or
-    #  Anaconda) of BLAS under Linux seems to be safe. So we this problem seems
-    #  to only impact OSX users.
-    #
-    #  This wrapper makes it possible to skip tests that can possibly cause
-    #  this crash under OS X with.
-    #
-    #  Under Python 3.4+ it is possible to use the `forkserver` start method
-    #  for multiprocessing to avoid this issue. However it can cause pickling
-    #  errors on interactively defined functions. It therefore not enabled by
-    #  default.
-
-    if_safe_multiprocessing_with_blas = pytest.mark.skipif(
-        sys.platform == "darwin", reason="Possible multi-process bug with some BLAS"
-    )
-except ImportError:
-    pass
-
+    score_func : callable
+        The score function to use. It will be called as
+        `score_func(y_true, y_pred, **kwargs)`.
 
-def check_skip_network():
-    if int(os.environ.get("SKLEARN_SKIP_NETWORK_TESTS", 0)):
-        raise SkipTest("Text tutorial requires large dataset download")
+    sign : int
+        Either 1 or -1 to returns the score with `sign * score_func(estimator, X, y)`.
+        Thus, `sign` defined if higher scores are better or worse.
 
+    kwargs : dict
+        Additional parameters to pass to the score function.
 
-def _delete_folder(folder_path, warn=False):
-    """Utility function to cleanup a temporary folder if still existing.
+    thresholds : int or array-like
+        Related to the number of decision thresholds for which we want to compute the
+        score. If an integer, it will be used to generate `thresholds` thresholds
+        uniformly distributed between the minimum and maximum predicted scores. If an
+        array-like, it will be used as the thresholds.
 
-    Copy from joblib.pool (for independence).
+    response_method : str
+        The method to call on the estimator to get the response values.
     """
-    try:
-        if os.path.exists(folder_path):
-            # This can fail under windows,
-            #  but will succeed when called by atexit
-            shutil.rmtree(folder_path)
-    except OSError:
-        if warn:
-            warnings.warn("Could not delete temporary folder %s" % folder_path)
-
 
-class TempMemmap:
-    """
-    Parameters
-    ----------
-    data
-    mmap_mode : str, default='r'
-    """
+    def __init__(self, score_func, sign, kwargs, thresholds, response_method):
+        super().__init__(
+            score_func=score_func,
+            sign=sign,
+            kwargs=kwargs,
+            response_method=response_method,
+        )
+        self._thresholds = thresholds
 
-    def __init__(self, data, mmap_mode="r"):
-        self.mmap_mode = mmap_mode
-        self.data = data
-
-    def __enter__(self):
-        data_read_only, self.temp_folder = create_memmap_backed_data(
-            self.data, mmap_mode=self.mmap_mode, return_folder=True
+    @classmethod
+    def from_scorer(cls, scorer, response_method, thresholds):
+        """Create a continuous scorer from a normal scorer."""
+        instance = cls(
+            score_func=scorer._score_func,
+            sign=scorer._sign,
+            response_method=response_method,
+            thresholds=thresholds,
+            kwargs=scorer._kwargs,
+        )
+        # transfer the metadata request
+        instance._metadata_request = scorer._get_metadata_request()
+        return instance
+
+    def _score(self, method_caller, estimator, X, y_true, **kwargs):
+        """Evaluate predicted target values for X relative to y_true.
+
+        Parameters
+        ----------
+        method_caller : callable
+            Returns predictions given an estimator, method name, and other
+            arguments, potentially caching results.
+
+        estimator : object
+            Trained estimator to use for scoring.
+
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Test data that will be fed to estimator.predict.
+
+        y_true : array-like of shape (n_samples,)
+            Gold standard target values for X.
+
+        **kwargs : dict
+            Other parameters passed to the scorer. Refer to
+            :func:`set_score_request` for more details.
+
+        Returns
+        -------
+        scores : ndarray of shape (thresholds,)
+            The scores associated to each threshold.
+
+        potential_thresholds : ndarray of shape (thresholds,)
+            The potential thresholds used to compute the scores.
+        """
+        pos_label = self._get_pos_label()
+        y_score = method_caller(
+            estimator, self._response_method, X, pos_label=pos_label
         )
-        return data_read_only
 
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        _delete_folder(self.temp_folder)
+        scoring_kwargs = {**self._kwargs, **kwargs}
+        if isinstance(self._thresholds, Integral):
+            potential_thresholds = np.linspace(
+                np.min(y_score), np.max(y_score), self._thresholds
+            )
+        else:
+            potential_thresholds = np.asarray(self._thresholds)
+        score_thresholds = [
+            self._sign
+            * self._score_func(
+                y_true,
+                _threshold_scores_to_class_labels(
+                    y_score, th, estimator.classes_, pos_label
+                ),
+                **scoring_kwargs,
+            )
+            for th in potential_thresholds
+        ]
+        return np.array(score_thresholds), potential_thresholds
 
 
-def create_memmap_backed_data(data, mmap_mode="r", return_folder=False):
-    """
+def _fit_and_score_over_thresholds(
+    classifier,
+    X,
+    y,
+    *,
+    fit_params,
+    train_idx,
+    val_idx,
+    curve_scorer,
+    score_params,
+):
+    """Fit a classifier and compute the scores for different decision thresholds.
+
     Parameters
     ----------
-    data
-    mmap_mode : str, default='r'
-    return_folder :  bool, default=False
-    """
-    temp_folder = tempfile.mkdtemp(prefix="sklearn_testing_")
-    atexit.register(functools.partial(_delete_folder, temp_folder, warn=True))
-    filename = op.join(temp_folder, "data.pkl")
-    joblib.dump(data, filename)
-    memmap_backed_data = joblib.load(filename, mmap_mode=mmap_mode)
-    result = (
-        memmap_backed_data if not return_folder else (memmap_backed_data, temp_folder)
-    )
-    return result
+    classifier : estimator instance
+        The classifier to fit and use for scoring. If `classifier` is already fitted,
+        it will be used as is.
 
+    X : {array-like, sparse matrix} of shape (n_samples, n_features)
+        The entire dataset.
 
-# Utils to test docstrings
+    y : array-like of shape (n_samples,)
+        The entire target vector.
 
+    fit_params : dict
+        Parameters to pass to the `fit` method of the underlying classifier.
 
-def _get_args(function, varargs=False):
-    """Helper to get function arguments."""
+    train_idx : ndarray of shape (n_train_samples,) or None
+        The indices of the training set. If `None`, `classifier` is expected to be
+        already fitted.
 
-    try:
-        params = signature(function).parameters
-    except ValueError:
-        # Error on builtin C function
-        return []
-    args = [
-        key
-        for key, param in params.items()
-        if param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)
-    ]
-    if varargs:
-        varargs = [
-            param.name
-            for param in params.values()
-            if param.kind == param.VAR_POSITIONAL
-        ]
-        if len(varargs) == 0:
-            varargs = None
-        return args, varargs
-    else:
-        return args
+    val_idx : ndarray of shape (n_val_samples,)
+        The indices of the validation set used to score `classifier`. If `train_idx`,
+        the entire set will be used.
 
+    curve_scorer : scorer instance
+        The scorer taking `classifier` and the validation set as input and outputting
+        decision thresholds and scores as a curve. Note that this is different from
+        the usual scorer that output a single score value:
 
-def _get_func_name(func):
-    """Get function full name.
+        * when `score_method` is one of the four constraint metrics, the curve scorer
+          will output a curve of two scores parametrized by the decision threshold, e.g.
+          TPR/TNR or precision/recall curves for each threshold;
+        * otherwise, the curve scorer will output a single score value for each
+          threshold.
 
-    Parameters
-    ----------
-    func : callable
-        The function object.
+    score_params : dict
+        Parameters to pass to the `score` method of the underlying scorer.
 
     Returns
     -------
-    name : str
-        The function name.
+    scores : ndarray of shape (thresholds,) or tuple of such arrays
+        The scores computed for each decision threshold. When TPR/TNR or precision/
+        recall are computed, `scores` is a tuple of two arrays.
+
+    potential_thresholds : ndarray of shape (thresholds,)
+        The decision thresholds used to compute the scores. They are returned in
+        ascending order.
     """
-    parts = []
-    module = inspect.getmodule(func)
-    if module:
-        parts.append(module.__name__)
 
-    qualname = func.__qualname__
-    if qualname != func.__name__:
-        parts.append(qualname[: qualname.find(".")])
+    if train_idx is not None:
+        X_train, X_val = _safe_indexing(X, train_idx), _safe_indexing(X, val_idx)
+        y_train, y_val = _safe_indexing(y, train_idx), _safe_indexing(y, val_idx)
+        fit_params_train = _check_method_params(X, fit_params, indices=train_idx)
+        score_params_val = _check_method_params(X, score_params, indices=val_idx)
+        classifier.fit(X_train, y_train, **fit_params_train)
+    else:  # prefit estimator, only a validation set is provided
+        X_val, y_val, score_params_val = X, y, score_params
 
-    parts.append(func.__name__)
-    return ".".join(parts)
+    return curve_scorer(classifier, X_val, y_val, **score_params_val)
 
 
-def check_docstring_parameters(func, doc=None, ignore=None):
-    """Helper to check docstring.
+def _mean_interpolated_score(target_thresholds, cv_thresholds, cv_scores):
+    """Compute the mean interpolated score across folds by defining common thresholds.
 
     Parameters
     ----------
-    func : callable
-        The function object to test.
-    doc : str, default=None
-        Docstring if it is passed manually to the test.
-    ignore : list, default=None
-        Parameters to ignore.
+    target_thresholds : ndarray of shape (thresholds,)
+        The thresholds to use to compute the mean score.
+
+    cv_thresholds : ndarray of shape (n_folds, thresholds_fold)
+        The thresholds used to compute the scores for each fold.
+
+    cv_scores : ndarray of shape (n_folds, thresholds_fold)
+        The scores computed for each threshold for each fold.
 
     Returns
     -------
-    incorrect : list
-        A list of string describing the incorrect results.
+    mean_score : ndarray of shape (thresholds,)
+        The mean score across all folds for each target threshold.
     """
-    from numpydoc import docscrape
-
-    incorrect = []
-    ignore = [] if ignore is None else ignore
-
-    func_name = _get_func_name(func)
-    if not func_name.startswith("sklearn.") or func_name.startswith(
-        "sklearn.externals"
-    ):
-        return incorrect
-    # Don't check docstring for property-functions
-    if inspect.isdatadescriptor(func):
-        return incorrect
-    # Don't check docstring for setup / teardown pytest functions
-    if func_name.split(".")[-1] in ("setup_module", "teardown_module"):
-        return incorrect
-    # Dont check estimator_checks module
-    if func_name.split(".")[2] == "estimator_checks":
-        return incorrect
-    # Get the arguments from the function signature
-    param_signature = list(filter(lambda x: x not in ignore, _get_args(func)))
-    # drop self
-    if len(param_signature) > 0 and param_signature[0] == "self":
-        param_signature.remove("self")
-
-    # Analyze function's docstring
-    if doc is None:
-        records = []
-        with warnings.catch_warnings(record=True):
-            warnings.simplefilter("error", UserWarning)
-            try:
-                doc = docscrape.FunctionDoc(func)
-            except UserWarning as exp:
-                if "potentially wrong underline length" in str(exp):
-                    # Catch warning raised as of numpydoc 1.2 when
-                    # the underline length for a section of a docstring
-                    # is not consistent.
-                    message = str(exp).split("\n")[:3]
-                    incorrect += [f"In function: {func_name}"] + message
-                    return incorrect
-                records.append(str(exp))
-            except Exception as exp:
-                incorrect += [func_name + " parsing error: " + str(exp)]
-                return incorrect
-        if len(records):
-            raise RuntimeError("Error for %s:\n%s" % (func_name, records[0]))
-
-    param_docs = []
-    for name, type_definition, param_doc in doc["Parameters"]:
-        # Type hints are empty only if parameter name ended with :
-        if not type_definition.strip():
-            if ":" in name and name[: name.index(":")][-1:].strip():
-                incorrect += [
-                    func_name
-                    + " There was no space between the param name and colon (%r)" % name
-                ]
-            elif name.rstrip().endswith(":"):
-                incorrect += [
-                    func_name
-                    + " Parameter %r has an empty type spec. Remove the colon"
-                    % (name.lstrip())
-                ]
-
-        # Create a list of parameters to compare with the parameters gotten
-        # from the func signature
-        if "*" not in name:
-            param_docs.append(name.split(":")[0].strip("` "))
-
-    # If one of the docstring's parameters had an error then return that
-    # incorrect message
-    if len(incorrect) > 0:
-        return incorrect
-
-    # Remove the parameters that should be ignored from list
-    param_docs = list(filter(lambda x: x not in ignore, param_docs))
-
-    # The following is derived from pytest, Copyright (c) 2004-2017 Holger
-    # Krekel and others, Licensed under MIT License. See
-    # https://github.com/pytest-dev/pytest
-
-    message = []
-    for i in range(min(len(param_docs), len(param_signature))):
-        if param_signature[i] != param_docs[i]:
-            message += [
-                "There's a parameter name mismatch in function"
-                " docstring w.r.t. function signature, at index %s"
-                " diff: %r != %r" % (i, param_signature[i], param_docs[i])
-            ]
-            break
-    if len(param_signature) > len(param_docs):
-        message += [
-            "Parameters in function docstring have less items w.r.t."
-            " function signature, first missing item: %s"
-            % param_signature[len(param_docs)]
-        ]
-
-    elif len(param_signature) < len(param_docs):
-        message += [
-            "Parameters in function docstring have more items w.r.t."
-            " function signature, first extra item: %s"
-            % param_docs[len(param_signature)]
-        ]
-
-    # If there wasn't any difference in the parameters themselves between
-    # docstring and signature including having the same length then return
-    # empty list
-    if len(message) == 0:
-        return []
-
-    import difflib
-    import pprint
-
-    param_docs_formatted = pprint.pformat(param_docs).splitlines()
-    param_signature_formatted = pprint.pformat(param_signature).splitlines()
-
-    message += ["Full diff:"]
-
-    message.extend(
-        line.strip()
-        for line in difflib.ndiff(param_signature_formatted, param_docs_formatted)
+    return np.mean(
+        [
+            np.interp(target_thresholds, split_thresholds, split_score)
+            for split_thresholds, split_score in zip(cv_thresholds, cv_scores)
+        ],
+        axis=0,
     )
 
-    incorrect.extend(message)
-
-    # Prepend function name
-    incorrect = ["In function: " + func_name] + incorrect
-
-    return incorrect
 
+class TunedThresholdClassifierCV(BaseThresholdClassifier):
+    """Classifier that post-tunes the decision threshold using cross-validation.
 
-def assert_run_python_script_without_output(source_code, pattern=".+", timeout=60):
-    """Utility to check assertions in an independent Python subprocess.
+    This estimator post-tunes the decision threshold (cut-off point) that is
+    used for converting posterior probability estimates (i.e. output of
+    `predict_proba`) or decision scores (i.e. output of `decision_function`)
+    into a class label. The tuning is done by optimizing a binary metric,
+    potentially constrained by a another metric.
+
+    Read more in the :ref:`User Guide <TunedThresholdClassifierCV>`.
+
+    .. versionadded:: 1.5
+
+    Parameters
+    ----------
+    estimator : estimator instance
+        The classifier, fitted or not, for which we want to optimize
+        the decision threshold used during `predict`.
+
+    scoring : str or callable, default="balanced_accuracy"
+        The objective metric to be optimized. Can be one of:
+
+        * a string associated to a scoring function for binary classification
+          (see model evaluation documentation);
+        * a scorer callable object created with :func:`~sklearn.metrics.make_scorer`;
+
+    response_method : {"auto", "decision_function", "predict_proba"}, default="auto"
+        Methods by the classifier `estimator` corresponding to the
+        decision function for which we want to find a threshold. It can be:
+
+        * if `"auto"`, it will try to invoke, for each classifier,
+          `"predict_proba"` or `"decision_function"` in that order.
+        * otherwise, one of `"predict_proba"` or `"decision_function"`.
+          If the method is not implemented by the classifier, it will raise an
+          error.
+
+    thresholds : int or array-like, default=100
+        The number of decision threshold to use when discretizing the output of the
+        classifier `method`. Pass an array-like to manually specify the thresholds
+        to use.
+
+    cv : int, float, cross-validation generator, iterable or "prefit", default=None
+        Determines the cross-validation splitting strategy to train classifier.
+        Possible inputs for cv are:
+
+        * `None`, to use the default 5-fold stratified K-fold cross validation;
+        * An integer number, to specify the number of folds in a stratified k-fold;
+        * A float number, to specify a single shuffle split. The floating number should
+          be in (0, 1) and represent the size of the validation set;
+        * An object to be used as a cross-validation generator;
+        * An iterable yielding train, test splits;
+        * `"prefit"`, to bypass the cross-validation.
+
+        Refer :ref:`User Guide <cross_validation>` for the various
+        cross-validation strategies that can be used here.
+
+        .. warning::
+            Using `cv="prefit"` and passing the same dataset for fitting `estimator`
+            and tuning the cut-off point is subject to undesired overfitting. You can
+            refer to :ref:`TunedThresholdClassifierCV_no_cv` for an example.
+
+            This option should only be used when the set used to fit `estimator` is
+            different from the one used to tune the cut-off point (by calling
+            :meth:`TunedThresholdClassifierCV.fit`).
+
+    refit : bool, default=True
+        Whether or not to refit the classifier on the entire training set once
+        the decision threshold has been found.
+        Note that forcing `refit=False` on cross-validation having more
+        than a single split will raise an error. Similarly, `refit=True` in
+        conjunction with `cv="prefit"` will raise an error.
+
+    n_jobs : int, default=None
+        The number of jobs to run in parallel. When `cv` represents a
+        cross-validation strategy, the fitting and scoring on each data split
+        is done in parallel. ``None`` means 1 unless in a
+        :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors. See :term:`Glossary <n_jobs>` for more details.
 
-    The script provided in the source code should return 0 and the stdtout +
-    stderr should not match the pattern `pattern`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of cross-validation when `cv` is a float.
+        See :term:`Glossary <random_state>`.
 
-    This is a port from cloudpickle https://github.com/cloudpipe/cloudpickle
+    store_cv_results : bool, default=False
+        Whether to store all scores and thresholds computed during the cross-validation
+        process.
 
-    Parameters
+    Attributes
     ----------
-    source_code : str
-        The Python source code to execute.
-    pattern : str
-        Pattern that the stdout + stderr should not match. By default, unless
-        stdout + stderr are both empty, an error will be raised.
-    timeout : int, default=60
-        Time in seconds before timeout.
-    """
-    fd, source_file = tempfile.mkstemp(suffix="_src_test_sklearn.py")
-    os.close(fd)
-    try:
-        with open(source_file, "wb") as f:
-            f.write(source_code.encode("utf-8"))
-        cmd = [sys.executable, source_file]
-        cwd = op.normpath(op.join(op.dirname(sklearn.__file__), ".."))
-        env = os.environ.copy()
-        try:
-            env["PYTHONPATH"] = os.pathsep.join([cwd, env["PYTHONPATH"]])
-        except KeyError:
-            env["PYTHONPATH"] = cwd
-        kwargs = {"cwd": cwd, "stderr": STDOUT, "env": env}
-        # If coverage is running, pass the config file to the subprocess
-        coverage_rc = os.environ.get("COVERAGE_PROCESS_START")
-        if coverage_rc:
-            kwargs["env"]["COVERAGE_PROCESS_START"] = coverage_rc
-
-        kwargs["timeout"] = timeout
-        try:
-            try:
-                out = check_output(cmd, **kwargs)
-            except CalledProcessError as e:
-                raise RuntimeError(
-                    "script errored with output:\n%s" % e.output.decode("utf-8")
-                )
+    estimator_ : estimator instance
+        The fitted classifier used when predicting.
 
-            out = out.decode("utf-8")
-            if re.search(pattern, out):
-                if pattern == ".+":
-                    expectation = "Expected no output"
-                else:
-                    expectation = f"The output was not supposed to match {pattern!r}"
-
-                message = f"{expectation}, got the following output instead: {out!r}"
-                raise AssertionError(message)
-        except TimeoutExpired as e:
-            raise RuntimeError(
-                "script timeout, output so far:\n%s" % e.output.decode("utf-8")
-            )
-    finally:
-        os.unlink(source_file)
+    best_threshold_ : float
+        The new decision threshold.
 
+    best_score_ : float or None
+        The optimal score of the objective metric, evaluated at `best_threshold_`.
 
-def _convert_container(
-    container,
-    constructor_name,
-    columns_name=None,
-    dtype=None,
-    minversion=None,
-    categorical_feature_names=None,
-):
-    """Convert a given container to a specific array-like with a dtype.
+    cv_results_ : dict or None
+        A dictionary containing the scores and thresholds computed during the
+        cross-validation process. Only exist if `store_cv_results=True`. The
+        keys are `"thresholds"` and `"scores"`.
+
+    classes_ : ndarray of shape (n_classes,)
+        The class labels.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
 
-    Parameters
-    ----------
-    container : array-like
-        The container to convert.
-    constructor_name : {"list", "tuple", "array", "sparse", "dataframe", \
-            "series", "index", "slice", "sparse_csr", "sparse_csc"}
-        The type of the returned container.
-    columns_name : index or array-like, default=None
-        For pandas container supporting `columns_names`, it will affect
-        specific names.
-    dtype : dtype, default=None
-        Force the dtype of the container. Does not apply to `"slice"`
-        container.
-    minversion : str, default=None
-        Minimum version for package to install.
-    categorical_feature_names : list of str, default=None
-        List of column names to cast to categorical dtype.
+    See Also
+    --------
+    sklearn.model_selection.FixedThresholdClassifier : Classifier that uses a
+        constant threshold.
+    sklearn.calibration.CalibratedClassifierCV : Estimator that calibrates
+        probabilities.
 
-    Returns
-    -------
-    converted_container
-    """
-    if constructor_name == "list":
-        if dtype is None:
-            return list(container)
+    Examples
+    --------
+    >>> from sklearn.datasets import make_classification
+    >>> from sklearn.ensemble import RandomForestClassifier
+    >>> from sklearn.metrics import classification_report
+    >>> from sklearn.model_selection import TunedThresholdClassifierCV, train_test_split
+    >>> X, y = make_classification(
+    ...     n_samples=1_000, weights=[0.9, 0.1], class_sep=0.8, random_state=42
+    ... )
+    >>> X_train, X_test, y_train, y_test = train_test_split(
+    ...     X, y, stratify=y, random_state=42
+    ... )
+    >>> classifier = RandomForestClassifier(random_state=0).fit(X_train, y_train)
+    >>> print(classification_report(y_test, classifier.predict(X_test)))
+                  precision    recall  f1-score   support
+    <BLANKLINE>
+               0       0.94      0.99      0.96       224
+               1       0.80      0.46      0.59        26
+    <BLANKLINE>
+        accuracy                           0.93       250
+       macro avg       0.87      0.72      0.77       250
+    weighted avg       0.93      0.93      0.92       250
+    <BLANKLINE>
+    >>> classifier_tuned = TunedThresholdClassifierCV(
+    ...     classifier, scoring="balanced_accuracy"
+    ... ).fit(X_train, y_train)
+    >>> print(
+    ...     f"Cut-off point found at {classifier_tuned.best_threshold_:.3f}"
+    ... )
+    Cut-off point found at 0.342
+    >>> print(classification_report(y_test, classifier_tuned.predict(X_test)))
+                  precision    recall  f1-score   support
+    <BLANKLINE>
+               0       0.96      0.95      0.96       224
+               1       0.61      0.65      0.63        26
+    <BLANKLINE>
+        accuracy                           0.92       250
+       macro avg       0.78      0.80      0.79       250
+    weighted avg       0.92      0.92      0.92       250
+    <BLANKLINE>
+    """
+
+    _parameter_constraints: dict = {
+        **BaseThresholdClassifier._parameter_constraints,
+        "scoring": [
+            StrOptions(set(get_scorer_names())),
+            callable,
+            MutableMapping,
+        ],
+        "thresholds": [Interval(Integral, 1, None, closed="left"), "array-like"],
+        "cv": [
+            "cv_object",
+            StrOptions({"prefit"}),
+            Interval(RealNotInt, 0.0, 1.0, closed="neither"),
+        ],
+        "refit": ["boolean"],
+        "n_jobs": [Integral, None],
+        "random_state": ["random_state"],
+        "store_cv_results": ["boolean"],
+    }
+
+    def __init__(
+        self,
+        estimator,
+        *,
+        scoring="balanced_accuracy",
+        response_method="auto",
+        thresholds=100,
+        cv=None,
+        refit=True,
+        n_jobs=None,
+        random_state=None,
+        store_cv_results=False,
+    ):
+        super().__init__(estimator=estimator, response_method=response_method)
+        self.scoring = scoring
+        self.thresholds = thresholds
+        self.cv = cv
+        self.refit = refit
+        self.n_jobs = n_jobs
+        self.random_state = random_state
+        self.store_cv_results = store_cv_results
+
+    def _fit(self, X, y, **params):
+        """Fit the classifier and post-tune the decision threshold.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training data.
+
+        y : array-like of shape (n_samples,)
+            Target values.
+
+        **params : dict
+            Parameters to pass to the `fit` method of the underlying
+            classifier and to the `scoring` scorer.
+
+        Returns
+        -------
+        self : object
+            Returns an instance of self.
+        """
+        if isinstance(self.cv, Real) and 0 < self.cv < 1:
+            cv = StratifiedShuffleSplit(
+                n_splits=1, test_size=self.cv, random_state=self.random_state
+            )
+        elif self.cv == "prefit":
+            if self.refit is True:
+                raise ValueError("When cv='prefit', refit cannot be True.")
+            try:
+                check_is_fitted(self.estimator, "classes_")
+            except NotFittedError as exc:
+                raise NotFittedError(
+                    """When cv='prefit', `estimator` must be fitted."""
+                ) from exc
+            cv = self.cv
         else:
-            return np.asarray(container, dtype=dtype).tolist()
-    elif constructor_name == "tuple":
-        if dtype is None:
-            return tuple(container)
+            cv = check_cv(self.cv, y=y, classifier=True)
+            if self.refit is False and cv.get_n_splits() > 1:
+                raise ValueError("When cv has several folds, refit cannot be False.")
+
+        routed_params = process_routing(self, "fit", **params)
+        self._curve_scorer = self._get_curve_scorer()
+
+        # in the following block, we:
+        # - define the final classifier `self.estimator_` and train it if necessary
+        # - define `classifier` to be used to post-tune the decision threshold
+        # - define `split` to be used to fit/score `classifier`
+        if cv == "prefit":
+            self.estimator_ = self.estimator
+            classifier = self.estimator_
+            splits = [(None, range(_num_samples(X)))]
         else:
-            return tuple(np.asarray(container, dtype=dtype).tolist())
-    elif constructor_name == "array":
-        return np.asarray(container, dtype=dtype)
-    elif constructor_name in ("pandas", "dataframe"):
-        pd = pytest.importorskip("pandas", minversion=minversion)
-        result = pd.DataFrame(container, columns=columns_name, dtype=dtype, copy=False)
-        if categorical_feature_names is not None:
-            for col_name in categorical_feature_names:
-                result[col_name] = result[col_name].astype("category")
-        return result
-    elif constructor_name == "pyarrow":
-        pa = pytest.importorskip("pyarrow", minversion=minversion)
-        array = np.asarray(container)
-        if columns_name is None:
-            columns_name = [f"col{i}" for i in range(array.shape[1])]
-        data = {name: array[:, i] for i, name in enumerate(columns_name)}
-        result = pa.Table.from_pydict(data)
-        if categorical_feature_names is not None:
-            for col_idx, col_name in enumerate(result.column_names):
-                if col_name in categorical_feature_names:
-                    result = result.set_column(
-                        col_idx, col_name, result.column(col_name).dictionary_encode()
-                    )
-        return result
-    elif constructor_name == "polars":
-        pl = pytest.importorskip("polars", minversion=minversion)
-        result = pl.DataFrame(container, schema=columns_name, orient="row")
-        if categorical_feature_names is not None:
-            for col_name in categorical_feature_names:
-                result = result.with_columns(pl.col(col_name).cast(pl.Categorical))
-        return result
-    elif constructor_name == "series":
-        pd = pytest.importorskip("pandas", minversion=minversion)
-        return pd.Series(container, dtype=dtype)
-    elif constructor_name == "index":
-        pd = pytest.importorskip("pandas", minversion=minversion)
-        return pd.Index(container, dtype=dtype)
-    elif constructor_name == "slice":
-        return slice(container[0], container[1])
-    elif "sparse" in constructor_name:
-        if not sp.sparse.issparse(container):
-            # For scipy >= 1.13, sparse array constructed from 1d array may be
-            # 1d or raise an exception. To avoid this, we make sure that the
-            # input container is 2d. For more details, see
-            # https://github.com/scipy/scipy/pull/18530#issuecomment-1878005149
-            container = np.atleast_2d(container)
-
-        if "array" in constructor_name and sp_version < parse_version("1.8"):
-            raise ValueError(
-                f"{constructor_name} is only available with scipy>=1.8.0, got "
-                f"{sp_version}"
-            )
-        if constructor_name in ("sparse", "sparse_csr"):
-            # sparse and sparse_csr are equivalent for legacy reasons
-            return sp.sparse.csr_matrix(container, dtype=dtype)
-        elif constructor_name == "sparse_csr_array":
-            return sp.sparse.csr_array(container, dtype=dtype)
-        elif constructor_name == "sparse_csc":
-            return sp.sparse.csc_matrix(container, dtype=dtype)
-        elif constructor_name == "sparse_csc_array":
-            return sp.sparse.csc_array(container, dtype=dtype)
-
-
-def raises(expected_exc_type, match=None, may_pass=False, err_msg=None):
-    """Context manager to ensure exceptions are raised within a code block.
-
-    This is similar to and inspired from pytest.raises, but supports a few
-    other cases.
-
-    This is only intended to be used in estimator_checks.py where we don't
-    want to use pytest. In the rest of the code base, just use pytest.raises
-    instead.
-
-    Parameters
-    ----------
-    excepted_exc_type : Exception or list of Exception
-        The exception that should be raised by the block. If a list, the block
-        should raise one of the exceptions.
-    match : str or list of str, default=None
-        A regex that the exception message should match. If a list, one of
-        the entries must match. If None, match isn't enforced.
-    may_pass : bool, default=False
-        If True, the block is allowed to not raise an exception. Useful in
-        cases where some estimators may support a feature but others must
-        fail with an appropriate error message. By default, the context
-        manager will raise an exception if the block does not raise an
-        exception.
-    err_msg : str, default=None
-        If the context manager fails (e.g. the block fails to raise the
-        proper exception, or fails to match), then an AssertionError is
-        raised with this message. By default, an AssertionError is raised
-        with a default error message (depends on the kind of failure). Use
-        this to indicate how users should fix their estimators to pass the
-        checks.
-
-    Attributes
-    ----------
-    raised_and_matched : bool
-        True if an exception was raised and a match was found, False otherwise.
-    """
-    return _Raises(expected_exc_type, match, may_pass, err_msg)
-
-
-class _Raises(contextlib.AbstractContextManager):
-    # see raises() for parameters
-    def __init__(self, expected_exc_type, match, may_pass, err_msg):
-        self.expected_exc_types = (
-            expected_exc_type
-            if isinstance(expected_exc_type, Iterable)
-            else [expected_exc_type]
-        )
-        self.matches = [match] if isinstance(match, str) else match
-        self.may_pass = may_pass
-        self.err_msg = err_msg
-        self.raised_and_matched = False
-
-    def __exit__(self, exc_type, exc_value, _):
-        # see
-        # https://docs.python.org/2.5/whatsnew/pep-343.html#SECTION000910000000000000000
-
-        if exc_type is None:  # No exception was raised in the block
-            if self.may_pass:
-                return True  # CM is happy
+            self.estimator_ = clone(self.estimator)
+            classifier = clone(self.estimator)
+            splits = cv.split(X, y, **routed_params.splitter.split)
+
+            if self.refit:
+                # train on the whole dataset
+                X_train, y_train, fit_params_train = X, y, routed_params.estimator.fit
             else:
-                err_msg = self.err_msg or f"Did not raise: {self.expected_exc_types}"
-                raise AssertionError(err_msg)
+                # single split cross-validation
+                train_idx, _ = next(cv.split(X, y, **routed_params.splitter.split))
+                X_train = _safe_indexing(X, train_idx)
+                y_train = _safe_indexing(y, train_idx)
+                fit_params_train = _check_method_params(
+                    X, routed_params.estimator.fit, indices=train_idx
+                )
 
-        if not any(
-            issubclass(exc_type, expected_type)
-            for expected_type in self.expected_exc_types
-        ):
-            if self.err_msg is not None:
-                raise AssertionError(self.err_msg) from exc_value
-            else:
-                return False  # will re-raise the original exception
+            self.estimator_.fit(X_train, y_train, **fit_params_train)
 
-        if self.matches is not None:
-            err_msg = self.err_msg or (
-                "The error message should contain one of the following "
-                "patterns:\n{}\nGot {}".format("\n".join(self.matches), str(exc_value))
+        cv_scores, cv_thresholds = zip(
+            *Parallel(n_jobs=self.n_jobs)(
+                delayed(_fit_and_score_over_thresholds)(
+                    clone(classifier) if cv != "prefit" else classifier,
+                    X,
+                    y,
+                    fit_params=routed_params.estimator.fit,
+                    train_idx=train_idx,
+                    val_idx=val_idx,
+                    curve_scorer=self._curve_scorer,
+                    score_params=routed_params.scorer.score,
+                )
+                for train_idx, val_idx in splits
             )
-            if not any(re.search(match, str(exc_value)) for match in self.matches):
-                raise AssertionError(err_msg) from exc_value
-            self.raised_and_matched = True
-
-        return True
-
-
-class MinimalClassifier:
-    """Minimal classifier implementation with inheriting from BaseEstimator.
-
-    This estimator should be tested with:
-
-    * `check_estimator` in `test_estimator_checks.py`;
-    * within a `Pipeline` in `test_pipeline.py`;
-    * within a `SearchCV` in `test_search.py`.
-    """
-
-    _estimator_type = "classifier"
-
-    def __init__(self, param=None):
-        self.param = param
-
-    def get_params(self, deep=True):
-        return {"param": self.param}
-
-    def set_params(self, **params):
-        for key, value in params.items():
-            setattr(self, key, value)
-        return self
-
-    def fit(self, X, y):
-        X, y = check_X_y(X, y)
-        check_classification_targets(y)
-        self.classes_, counts = np.unique(y, return_counts=True)
-        self._most_frequent_class_idx = counts.argmax()
-        return self
-
-    def predict_proba(self, X):
-        check_is_fitted(self)
-        X = check_array(X)
-        proba_shape = (X.shape[0], self.classes_.size)
-        y_proba = np.zeros(shape=proba_shape, dtype=np.float64)
-        y_proba[:, self._most_frequent_class_idx] = 1.0
-        return y_proba
-
-    def predict(self, X):
-        y_proba = self.predict_proba(X)
-        y_pred = y_proba.argmax(axis=1)
-        return self.classes_[y_pred]
-
-    def score(self, X, y):
-        from sklearn.metrics import accuracy_score
-
-        return accuracy_score(y, self.predict(X))
-
-
-class MinimalRegressor:
-    """Minimal regressor implementation with inheriting from BaseEstimator.
-
-    This estimator should be tested with:
-
-    * `check_estimator` in `test_estimator_checks.py`;
-    * within a `Pipeline` in `test_pipeline.py`;
-    * within a `SearchCV` in `test_search.py`.
-    """
-
-    _estimator_type = "regressor"
+        )
 
-    def __init__(self, param=None):
-        self.param = param
+        if any(np.isclose(th[0], th[-1]) for th in cv_thresholds):
+            raise ValueError(
+                "The provided estimator makes constant predictions. Therefore, it is "
+                "impossible to optimize the decision threshold."
+            )
 
-    def get_params(self, deep=True):
-        return {"param": self.param}
+        # find the global min and max thresholds across all folds
+        min_threshold = min(
+            split_thresholds.min() for split_thresholds in cv_thresholds
+        )
+        max_threshold = max(
+            split_thresholds.max() for split_thresholds in cv_thresholds
+        )
+        if isinstance(self.thresholds, Integral):
+            decision_thresholds = np.linspace(
+                min_threshold, max_threshold, num=self.thresholds
+            )
+        else:
+            decision_thresholds = np.asarray(self.thresholds)
 
-    def set_params(self, **params):
-        for key, value in params.items():
-            setattr(self, key, value)
-        return self
+        objective_scores = _mean_interpolated_score(
+            decision_thresholds, cv_thresholds, cv_scores
+        )
+        best_idx = objective_scores.argmax()
+        self.best_score_ = objective_scores[best_idx]
+        self.best_threshold_ = decision_thresholds[best_idx]
+        if self.store_cv_results:
+            self.cv_results_ = {
+                "thresholds": decision_thresholds,
+                "scores": objective_scores,
+            }
 
-    def fit(self, X, y):
-        X, y = check_X_y(X, y)
-        self.is_fitted_ = True
-        self._mean = np.mean(y)
         return self
 
     def predict(self, X):
-        check_is_fitted(self)
-        X = check_array(X)
-        return np.ones(shape=(X.shape[0],)) * self._mean
-
-    def score(self, X, y):
-        from sklearn.metrics import r2_score
-
-        return r2_score(y, self.predict(X))
-
-
-class MinimalTransformer:
-    """Minimal transformer implementation with inheriting from
-    BaseEstimator.
+        """Predict the target of new samples.
 
-    This estimator should be tested with:
-
-    * `check_estimator` in `test_estimator_checks.py`;
-    * within a `Pipeline` in `test_pipeline.py`;
-    * within a `SearchCV` in `test_search.py`.
-    """
-
-    def __init__(self, param=None):
-        self.param = param
-
-    def get_params(self, deep=True):
-        return {"param": self.param}
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The samples, as accepted by `estimator.predict`.
+
+        Returns
+        -------
+        class_labels : ndarray of shape (n_samples,)
+            The predicted class.
+        """
+        check_is_fitted(self, "estimator_")
+        pos_label = self._curve_scorer._get_pos_label()
+        y_score, _ = _get_response_values_binary(
+            self.estimator_,
+            X,
+            self._response_method,
+            pos_label=pos_label,
+        )
 
-    def set_params(self, **params):
-        for key, value in params.items():
-            setattr(self, key, value)
-        return self
+        return _threshold_scores_to_class_labels(
+            y_score, self.best_threshold_, self.classes_, pos_label
+        )
 
-    def fit(self, X, y=None):
-        check_array(X)
-        self.is_fitted_ = True
-        return self
+    def get_metadata_routing(self):
+        """Get metadata routing of this object.
 
-    def transform(self, X, y=None):
-        check_is_fitted(self)
-        X = check_array(X)
-        return X
-
-    def fit_transform(self, X, y=None):
-        return self.fit(X, y).transform(X, y)
-
-
-def _array_api_for_tests(array_namespace, device):
-    try:
-        if array_namespace == "numpy.array_api":
-            # FIXME: once it is not experimental anymore
-            with ignore_warnings(category=UserWarning):
-                # UserWarning: numpy.array_api submodule is still experimental.
-                array_mod = importlib.import_module(array_namespace)
-        else:
-            array_mod = importlib.import_module(array_namespace)
-    except ModuleNotFoundError:
-        raise SkipTest(
-            f"{array_namespace} is not installed: not checking array_api input"
-        )
-    try:
-        import array_api_compat  # noqa
-    except ImportError:
-        raise SkipTest(
-            "array_api_compat is not installed: not checking array_api input"
-        )
+        Please check :ref:`User Guide <metadata_routing>` on how the routing
+        mechanism works.
 
-    # First create an array using the chosen array module and then get the
-    # corresponding (compatibility wrapped) array namespace based on it.
-    # This is because `cupy` is not the same as the compatibility wrapped
-    # namespace of a CuPy array.
-    xp = array_api_compat.get_namespace(array_mod.asarray(1))
-    if (
-        array_namespace == "torch"
-        and device == "cuda"
-        and not xp.backends.cuda.is_built()
-    ):
-        raise SkipTest("PyTorch test requires cuda, which is not available")
-    elif array_namespace == "torch" and device == "mps":
-        if os.getenv("PYTORCH_ENABLE_MPS_FALLBACK") != "1":
-            # For now we need PYTORCH_ENABLE_MPS_FALLBACK=1 for all estimators to work
-            # when using the MPS device.
-            raise SkipTest(
-                "Skipping MPS device test because PYTORCH_ENABLE_MPS_FALLBACK is not "
-                "set."
+        Returns
+        -------
+        routing : MetadataRouter
+            A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating
+            routing information.
+        """
+        router = (
+            MetadataRouter(owner=self.__class__.__name__)
+            .add(
+                estimator=self.estimator,
+                method_mapping=MethodMapping().add(callee="fit", caller="fit"),
             )
-        if not xp.backends.mps.is_built():
-            raise SkipTest(
-                "MPS is not available because the current PyTorch install was not "
-                "built with MPS enabled."
+            .add(
+                splitter=self.cv,
+                method_mapping=MethodMapping().add(callee="split", caller="fit"),
             )
-    elif array_namespace in {"cupy", "cupy.array_api"}:  # pragma: nocover
-        import cupy
-
-        if cupy.cuda.runtime.getDeviceCount() == 0:
-            raise SkipTest("CuPy test requires cuda, which is not available")
-    return xp
-
-
-def _get_warnings_filters_info_list():
-    @dataclass
-    class WarningInfo:
-        action: "warnings._ActionKind"
-        message: str = ""
-        category: type[Warning] = Warning
-
-        def to_filterwarning_str(self):
-            if self.category.__module__ == "builtins":
-                category = self.category.__name__
-            else:
-                category = f"{self.category.__module__}.{self.category.__name__}"
-
-            return f"{self.action}:{self.message}:{category}"
+            .add(
+                scorer=self._get_curve_scorer(),
+                method_mapping=MethodMapping().add(callee="score", caller="fit"),
+            )
+        )
+        return router
 
-    return [
-        WarningInfo("error", category=DeprecationWarning),
-        WarningInfo("error", category=FutureWarning),
-        WarningInfo("error", category=VisibleDeprecationWarning),
-        # TODO: remove when pyamg > 5.0.1
-        # Avoid a deprecation warning due pkg_resources usage in pyamg.
-        WarningInfo(
-            "ignore",
-            message="pkg_resources is deprecated as an API",
-            category=DeprecationWarning,
-        ),
-        WarningInfo(
-            "ignore",
-            message="Deprecated call to `pkg_resources",
-            category=DeprecationWarning,
-        ),
-        # pytest-cov issue https://github.com/pytest-dev/pytest-cov/issues/557 not
-        # fixed although it has been closed. https://github.com/pytest-dev/pytest-cov/pull/623
-        # would probably fix it.
-        WarningInfo(
-            "ignore",
-            message=(
-                "The --rsyncdir command line argument and rsyncdirs config variable are"
-                " deprecated"
-            ),
-            category=DeprecationWarning,
-        ),
-        # XXX: Easiest way to ignore pandas Pyarrow DeprecationWarning in the
-        # short-term. See https://github.com/pandas-dev/pandas/issues/54466 for
-        # more details.
-        WarningInfo(
-            "ignore",
-            message=r"\s*Pyarrow will become a required dependency",
-            category=DeprecationWarning,
-        ),
-    ]
-
-
-def get_pytest_filterwarning_lines():
-    warning_filters_info_list = _get_warnings_filters_info_list()
-    return [
-        warning_info.to_filterwarning_str()
-        for warning_info in warning_filters_info_list
-    ]
-
-
-def turn_warnings_into_errors():
-    warnings_filters_info_list = _get_warnings_filters_info_list()
-    for warning_info in warnings_filters_info_list:
-        warnings.filterwarnings(
-            warning_info.action,
-            message=warning_info.message,
-            category=warning_info.category,
+    def _get_curve_scorer(self):
+        """Get the curve scorer based on the objective metric used."""
+        scoring = check_scoring(self.estimator, scoring=self.scoring)
+        curve_scorer = _CurveScorer.from_scorer(
+            scoring, self._response_method, self.thresholds
         )
+        return curve_scorer
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_typedefs.pxd` & `scikit_learn-1.5.0rc1/sklearn/utils/_typedefs.pxd`

 * *Files 24% similar despite different names*

```diff
@@ -1,29 +1,41 @@
 # Commonly used types
 # These are redefinitions of the ones defined by numpy in
-# https://github.com/numpy/numpy/blob/main/numpy/__init__.pxd
-# and exposed by cython in
-# https://github.com/cython/cython/blob/master/Cython/Includes/numpy/__init__.pxd.
+# https://github.com/numpy/numpy/blob/main/numpy/__init__.pxd.
 # It will eventually avoid having to always include the numpy headers even when we
 # would only use it for the types.
 #
 # When used to declare variables that will receive values from numpy arrays, it
 # should match the dtype of the array. For example, to declare a variable that will
 # receive values from a numpy array of dtype np.float64, the type float64_t must be
 # used.
 #
 # TODO: Stop defining custom types locally or globally like DTYPE_t and friends and
 # use these consistently throughout the codebase.
 # NOTE: Extend this list as needed when converting more cython extensions.
 ctypedef unsigned char uint8_t
 ctypedef unsigned int uint32_t
 ctypedef unsigned long long uint64_t
+# Note: In NumPy 2, indexing always happens with npy_intp which is an alias for
+# the Py_ssize_t type, see PEP 353.
+#
+# Note that on most platforms Py_ssize_t is equivalent to C99's intptr_t,
+# but they can differ on architecture with segmented memory (none
+# supported by scikit-learn at the time of writing).
+#
+# intp_t/np.intp should be used to index arrays in a platform dependent way.
+# Storing arrays with platform dependent dtypes as attribute on picklable
+# objects is not recommended as it requires special care when loading and
+# using such datastructures on a host with different bitness. Instead one
+# should rather use fixed width integer types such as int32 or uint32 when we know
+# that the number of elements to index is not larger to 2 or 4 billions.
 ctypedef Py_ssize_t intp_t
 ctypedef float float32_t
 ctypedef double float64_t
 # Sparse matrices indices and indices' pointers arrays must use int32_t over
 # intp_t because intp_t is platform dependent.
 # When large sparse matrices are supported, indexing must use int64_t.
 # See https://github.com/scikit-learn/scikit-learn/issues/23653 which tracks the
 # ongoing work to support large sparse matrices.
+ctypedef signed char int8_t
 ctypedef signed int int32_t
 ctypedef signed long long int64_t
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/_vector_sentinel.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/_vector_sentinel.pyx`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_weight_vector.pxd.tp` & `scikit_learn-1.5.0rc1/sklearn/utils/_weight_vector.pxd.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/_weight_vector.pyx.tp` & `scikit_learn-1.5.0rc1/sklearn/utils/_weight_vector.pyx.tp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/arrayfuncs.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/arrayfuncs.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 """
 The :mod:`sklearn.utils.arrayfuncs` module includes a small collection of auxiliary
 functions that operate on arrays.
 """
 
 from cython cimport floating
+from cython.parallel cimport prange
 from libc.math cimport fabs
 from libc.float cimport DBL_MAX, FLT_MAX
 
 from ._cython_blas cimport _copy, _rotg, _rot
+from ._typedefs cimport float64_t
 
 
 ctypedef fused real_numeric:
     short
     int
     long
     float
@@ -114,7 +116,21 @@
             c = -c
             s = -s
 
         L1[i + 1] = 0.  # just for cleanup
         L1 += m
 
         _rot(n - i - 2, L1 + i, m, L1 + i + 1, m, c, s)
+
+
+def sum_parallel(const floating [:] array, int n_threads):
+    """Parallel sum, always using float64 internally."""
+    cdef:
+        float64_t out = 0.
+        int i = 0
+
+    for i in prange(
+        array.shape[0], schedule='static', nogil=True, num_threads=n_threads
+    ):
+        out += array[i]
+
+    return out
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/class_weight.py` & `scikit_learn-1.5.0rc1/sklearn/utils/class_weight.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/deprecation.py` & `scikit_learn-1.5.0rc1/sklearn/utils/deprecation.py`

 * *Files 22% similar despite different names*

```diff
@@ -40,16 +40,16 @@
         Parameters
         ----------
         obj : object
         """
         if isinstance(obj, type):
             return self._decorate_class(obj)
         elif isinstance(obj, property):
-            # Note that this is only triggered properly if the `property`
-            # decorator comes before the `deprecated` decorator, like so:
+            # Note that this is only triggered properly if the `deprecated`
+            # decorator is placed before the `property` decorator, like so:
             #
             # @deprecated(msg)
             # @property
             # def deprecated_attribute_(self):
             #     ...
             return self._decorate_property(obj)
         else:
@@ -110,7 +110,26 @@
     closures = getattr(func, "__closure__", [])
     if closures is None:
         closures = []
     is_deprecated = "deprecated" in "".join(
         [c.cell_contents for c in closures if isinstance(c.cell_contents, str)]
     )
     return is_deprecated
+
+
+# TODO: remove in 1.7
+def _deprecate_Xt_in_inverse_transform(X, Xt):
+    """Helper to deprecate the `Xt` argument in favor of `X` in inverse_transform."""
+    if X is not None and Xt is not None:
+        raise TypeError("Cannot use both X and Xt. Use X only.")
+
+    if X is None and Xt is None:
+        raise TypeError("Missing required positional argument: X.")
+
+    if Xt is not None:
+        warnings.warn(
+            "Xt was renamed X in version 1.5 and will be removed in 1.7.",
+            FutureWarning,
+        )
+        return Xt
+
+    return X
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/discovery.py` & `scikit_learn-1.5.0rc1/sklearn/utils/discovery.py`

 * *Files 0% similar despite different names*

```diff
@@ -74,16 +74,16 @@
     from ..base import (
         BaseEstimator,
         ClassifierMixin,
         ClusterMixin,
         RegressorMixin,
         TransformerMixin,
     )
-    from . import IS_PYPY
     from ._testing import ignore_warnings
+    from .fixes import _IS_PYPY
 
     def is_abstract(c):
         if not (hasattr(c, "__abstractmethods__")):
             return False
         if not len(c.__abstractmethods__):
             return False
         return True
@@ -104,15 +104,15 @@
             classes = inspect.getmembers(module, inspect.isclass)
             classes = [
                 (name, est_cls) for name, est_cls in classes if not name.startswith("_")
             ]
 
             # TODO: Remove when FeatureHasher is implemented in PYPY
             # Skips FeatureHasher for PYPY
-            if IS_PYPY and "feature_extraction" in module_name:
+            if _IS_PYPY and "feature_extraction" in module_name:
                 classes = [
                     (name, est_cls)
                     for name, est_cls in classes
                     if name == "FeatureHasher"
                 ]
 
             all_classes.extend(classes)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/estimator_checks.py` & `scikit_learn-1.5.0rc1/sklearn/utils/estimator_checks.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 
 import pickle
 import re
 import warnings
 from contextlib import nullcontext
 from copy import deepcopy
 from functools import partial, wraps
-from inspect import signature
+from inspect import isfunction, signature
 from numbers import Integral, Real
 
 import joblib
 import numpy as np
 from scipy import sparse
 from scipy.stats import rankdata
 
@@ -47,29 +47,27 @@
 from ..model_selection import ShuffleSplit, train_test_split
 from ..model_selection._validation import _safe_split
 from ..pipeline import make_pipeline
 from ..preprocessing import StandardScaler, scale
 from ..random_projection import BaseRandomProjection
 from ..tree import DecisionTreeClassifier, DecisionTreeRegressor
 from ..utils._array_api import (
+    _atol_for_type,
     _convert_to_numpy,
     get_namespace,
     yield_namespace_device_dtype_combinations,
 )
-from ..utils._array_api import (
-    device as array_device,
-)
+from ..utils._array_api import device as array_device
 from ..utils._param_validation import (
     InvalidParameterError,
     generate_invalid_param_val,
     make_constraint,
 )
-from ..utils.fixes import parse_version, sp_version
-from ..utils.validation import check_is_fitted
-from . import IS_PYPY, is_scalar_nan, shuffle
+from . import shuffle
+from ._missing import is_scalar_nan
 from ._param_validation import Interval
 from ._tags import (
     _DEFAULT_TAGS,
     _safe_tags,
 )
 from ._testing import (
     SkipTest,
@@ -82,15 +80,16 @@
     assert_array_less,
     assert_raise_message,
     create_memmap_backed_data,
     ignore_warnings,
     raises,
     set_random_state,
 )
-from .validation import _num_samples, has_fit_parameter
+from .fixes import _IS_PYPY, SPARSE_ARRAY_PRESENT, parse_version, sp_version
+from .validation import _num_samples, check_is_fitted, has_fit_parameter
 
 REGRESSION_DATASET = None
 CROSS_DECOMPOSITION = ["PLSCanonical", "PLSRegression", "CCA", "PLSSVD"]
 
 
 def _yield_checks(estimator):
     name = estimator.__class__.__name__
@@ -131,15 +130,16 @@
         # Check that pairwise estimator throws error on non-square input
         yield check_nonsquare_error
 
     yield check_estimators_overwrite_params
     if hasattr(estimator, "sparsify"):
         yield check_sparsify_coefficients
 
-    yield check_estimator_sparse_data
+    yield check_estimator_sparse_array
+    yield check_estimator_sparse_matrix
 
     # Test that estimators can be pickled, and once pickled
     # give the same answer as before.
     yield check_estimators_pickle
     yield partial(check_estimators_pickle, readonly_memmap=True)
 
     yield check_estimator_get_tags_default_keys
@@ -401,21 +401,19 @@
     -------
     id : str or None
 
     See Also
     --------
     check_estimator
     """
-    if callable(obj):
-        if not isinstance(obj, partial):
-            return obj.__name__
-
+    if isfunction(obj):
+        return obj.__name__
+    if isinstance(obj, partial):
         if not obj.keywords:
             return obj.func.__name__
-
         kwstring = ",".join(["{}={}".format(k, v) for k, v in obj.keywords.items()])
         return "{}({})".format(obj.func.__name__, kwstring)
     if hasattr(obj, "get_params"):
         with config_context(print_changed_only=True):
             return re.sub(r"\s", "", str(obj))
 
 
@@ -806,15 +804,15 @@
     data : array-like
         The data.
     """
 
     def __init__(self, data):
         self.data = np.asarray(data)
 
-    def __array__(self, dtype=None):
+    def __array__(self, dtype=None, copy=None):
         return self.data
 
     def __array_function__(self, func, types, args, kwargs):
         if func.__name__ == "may_share_memory":
             return True
         raise TypeError("Don't want to call array_function {}!".format(func.__name__))
 
@@ -833,25 +831,25 @@
         True if _pairwise is set to True and False otherwise.
     """
     metric = getattr(estimator, "metric", None)
 
     return bool(metric == "precomputed")
 
 
-def _generate_sparse_matrix(X_csr):
-    """Generate sparse matrices with {32,64}bit indices of diverse format.
+def _generate_sparse_data(X_csr):
+    """Generate sparse matrices or arrays with {32,64}bit indices of diverse format.
 
     Parameters
     ----------
-    X_csr: CSR Matrix
-        Input matrix in CSR format.
+    X_csr: scipy.sparse.csr_matrix or scipy.sparse.csr_array
+        Input in CSR format.
 
     Returns
     -------
-    out: iter(Matrices)
+    out: iter(Matrices) or iter(Arrays)
         In format['dok', 'lil', 'dia', 'bsr', 'csr', 'csc', 'coo',
         'coo_64', 'csc_64', 'csr_64']
     """
 
     assert X_csr.format == "csr"
     yield "csr", X_csr.copy()
     for sparse_format in ["dok", "lil", "dia", "bsr", "csc", "coo"]:
@@ -925,15 +923,15 @@
 
         est_xp_param_np = _convert_to_numpy(est_xp_param, xp=xp)
         if check_values:
             assert_allclose(
                 attribute,
                 est_xp_param_np,
                 err_msg=f"{key} not the same",
-                atol=np.finfo(X.dtype).eps * 100,
+                atol=_atol_for_type(X.dtype),
             )
         else:
             assert attribute.shape == est_xp_param_np.shape
             assert attribute.dtype == est_xp_param_np.dtype
 
     # Check estimator methods, if supported, give the same results
     methods = (
@@ -955,15 +953,15 @@
             result = method(X, y)
             with config_context(array_api_dispatch=True):
                 result_xp = getattr(est_xp, method_name)(X_xp, y_xp)
             # score typically returns a Python float
             assert isinstance(result, float)
             assert isinstance(result_xp, float)
             if check_values:
-                assert abs(result - result_xp) < np.finfo(X.dtype).eps * 100
+                assert abs(result - result_xp) < _atol_for_type(X.dtype)
             continue
         else:
             result = method(X)
             with config_context(array_api_dispatch=True):
                 result_xp = getattr(est_xp, method_name)(X_xp)
 
         with config_context(array_api_dispatch=True):
@@ -977,15 +975,15 @@
         result_xp_np = _convert_to_numpy(result_xp, xp=xp)
 
         if check_values:
             assert_allclose(
                 result,
                 result_xp_np,
                 err_msg=f"{method} did not the return the same result",
-                atol=np.finfo(X.dtype).eps * 100,
+                atol=_atol_for_type(X.dtype),
             )
         else:
             if hasattr(result, "shape"):
                 assert result.shape == result_xp_np.shape
                 assert result.dtype == result_xp_np.dtype
 
         if method_name == "transform" and hasattr(est, "inverse_transform"):
@@ -1002,15 +1000,15 @@
 
             invese_result_xp_np = _convert_to_numpy(invese_result_xp, xp=xp)
             if check_values:
                 assert_allclose(
                     inverse_result,
                     invese_result_xp_np,
                     err_msg="inverse_transform did not the return the same result",
-                    atol=np.finfo(X.dtype).eps * 100,
+                    atol=_atol_for_type(X.dtype),
                 )
             else:
                 assert inverse_result.shape == invese_result_xp_np.shape
                 assert inverse_result.dtype == invese_result_xp_np.dtype
 
 
 def check_array_api_input_and_values(
@@ -1026,44 +1024,44 @@
         array_namespace=array_namespace,
         device=device,
         dtype_name=dtype_name,
         check_values=True,
     )
 
 
-def check_estimator_sparse_data(name, estimator_orig):
+def _check_estimator_sparse_container(name, estimator_orig, sparse_type):
     rng = np.random.RandomState(0)
     X = rng.uniform(size=(40, 3))
     X[X < 0.8] = 0
     X = _enforce_estimator_tags_X(estimator_orig, X)
-    X_csr = sparse.csr_matrix(X)
     y = (4 * rng.uniform(size=40)).astype(int)
     # catch deprecation warnings
     with ignore_warnings(category=FutureWarning):
         estimator = clone(estimator_orig)
     y = _enforce_estimator_tags_y(estimator, y)
     tags = _safe_tags(estimator_orig)
-    for matrix_format, X in _generate_sparse_matrix(X_csr):
+    for matrix_format, X in _generate_sparse_data(sparse_type(X)):
         # catch deprecation warnings
         with ignore_warnings(category=FutureWarning):
             estimator = clone(estimator_orig)
             if name in ["Scaler", "StandardScaler"]:
                 estimator.set_params(with_mean=False)
         # fit and predict
         if "64" in matrix_format:
             err_msg = (
                 f"Estimator {name} doesn't seem to support {matrix_format} "
                 "matrix, and is not failing gracefully, e.g. by using "
-                "check_array(X, accept_large_sparse=False)"
+                "check_array(X, accept_large_sparse=False)."
             )
         else:
             err_msg = (
                 f"Estimator {name} doesn't seem to fail gracefully on sparse "
                 "data: error message should state explicitly that sparse "
-                "input is not supported if this is not the case."
+                "input is not supported if this is not the case, e.g. by using "
+                "check_array(X, accept_sparse=False)."
             )
         with raises(
             (TypeError, ValueError),
             match=["sparse", "Sparse"],
             may_pass=True,
             err_msg=err_msg,
         ):
@@ -1080,14 +1078,23 @@
                 if tags["binary_only"]:
                     expected_probs_shape = (X.shape[0], 2)
                 else:
                     expected_probs_shape = (X.shape[0], 4)
                 assert probs.shape == expected_probs_shape
 
 
+def check_estimator_sparse_matrix(name, estimator_orig):
+    _check_estimator_sparse_container(name, estimator_orig, sparse.csr_matrix)
+
+
+def check_estimator_sparse_array(name, estimator_orig):
+    if SPARSE_ARRAY_PRESENT:
+        _check_estimator_sparse_container(name, estimator_orig, sparse.csr_array)
+
+
 @ignore_warnings(category=FutureWarning)
 def check_sample_weights_pandas_series(name, estimator_orig):
     # check that estimators will accept a 'sample_weight' parameter of
     # type pandas.Series in the 'fit' function.
     estimator = clone(estimator_orig)
     try:
         import pandas as pd
@@ -1447,32 +1454,30 @@
 
     # check that fit doesn't add any public attribute
     assert not attrs_added_by_fit, (
         "Estimator adds public attribute(s) during"
         " the fit method."
         " Estimators are only allowed to add private attributes"
         " either started with _ or ended"
-        " with _ but %s added"
-        % ", ".join(attrs_added_by_fit)
+        " with _ but %s added" % ", ".join(attrs_added_by_fit)
     )
 
     # check that fit doesn't change any public attribute
     attrs_changed_by_fit = [
         key
         for key in public_keys_after_fit
         if (dict_before_fit[key] is not dict_after_fit[key])
     ]
 
     assert not attrs_changed_by_fit, (
         "Estimator changes public attribute(s) during"
         " the fit method. Estimators are only allowed"
         " to change attributes started"
         " or ended with _, but"
-        " %s changed"
-        % ", ".join(attrs_changed_by_fit)
+        " %s changed" % ", ".join(attrs_changed_by_fit)
     )
 
 
 @ignore_warnings(category=FutureWarning)
 def check_fit2d_predict1d(name, estimator_orig):
     # check by fitting a 2d array and predicting with a 1d array
     rnd = np.random.RandomState(0)
@@ -2913,16 +2918,15 @@
         [str(w_x) for w_x in w]
     )
     if not tags["multioutput"]:
         # check that we warned if we don't support multi-output
         assert len(w) > 0, msg
         assert (
             "DataConversionWarning('A column-vector y"
-            " was passed when a 1d array was expected"
-            in msg
+            " was passed when a 1d array was expected" in msg
         )
     assert_allclose(y_pred.ravel(), y_pred_2d.ravel())
 
 
 @ignore_warnings
 def check_classifiers_predictions(X, y, name, classifier_orig):
     classes = np.unique(y)
@@ -3282,15 +3286,15 @@
             f"Estimator {name} should store all parameters as an attribute during init."
         )
 
     if hasattr(type(estimator).__init__, "deprecated_original"):
         return
 
     init_params = _get_args(type(estimator).__init__)
-    if IS_PYPY:
+    if _IS_PYPY:
         # __init__ signature has additional objects in PyPy
         for key in ["obj"]:
             if key in init_params:
                 init_params.remove(key)
     parents_init_params = [
         param
         for params_parent in (_get_args(parent) for parent in type(estimator).__mro__)
@@ -3989,16 +3993,16 @@
     rng = np.random.RandomState(0)
 
     estimator = clone(estimator_orig)
     set_random_state(estimator)
     if "warm_start" in estimator.get_params():
         estimator.set_params(warm_start=False)
 
-    n_samples = 150
-    X = rng.normal(size=(n_samples, 8))
+    n_samples = 10
+    X = rng.normal(size=(n_samples, 4))
     X = _enforce_estimator_tags_X(estimator, X)
 
     if is_regressor(estimator):
         y = rng.normal(size=n_samples)
     else:
         y = rng.randint(low=0, high=2, size=n_samples)
     y = _enforce_estimator_tags_y(estimator, y)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/extmath.py` & `scikit_learn-1.5.0rc1/sklearn/utils/extmath.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """
 The :mod:`sklearn.utils.extmath` module includes utilities to perform
 optimal mathematical operations in scikit-learn that are not available in SciPy.
 """
+
 # Authors: Gael Varoquaux
 #          Alexandre Gramfort
 #          Alexandre T. Passos
 #          Olivier Grisel
 #          Lars Buitinck
 #          Stefan van der Walt
 #          Kyle Kastner
@@ -15,20 +16,19 @@
 import warnings
 from functools import partial
 from numbers import Integral
 
 import numpy as np
 from scipy import linalg, sparse
 
-from ..utils import deprecated
 from ..utils._param_validation import Interval, StrOptions, validate_params
-from . import check_random_state
+from ..utils.deprecation import deprecated
 from ._array_api import _is_numpy_namespace, device, get_namespace
 from .sparsefuncs_fast import csr_row_norms
-from .validation import check_array
+from .validation import check_array, check_random_state
 
 
 def squared_norm(x):
     """Squared Euclidean or Frobenius norm of x.
 
     Faster than norm(x) ** 2.
 
@@ -321,20 +321,20 @@
         )
 
     if is_array_api_compliant:
         qr_normalizer = partial(xp.linalg.qr, mode="reduced")
     else:
         # Use scipy.linalg instead of numpy.linalg when not explicitly
         # using the Array API.
-        qr_normalizer = partial(linalg.qr, mode="economic")
+        qr_normalizer = partial(linalg.qr, mode="economic", check_finite=False)
 
     if power_iteration_normalizer == "QR":
         normalizer = qr_normalizer
     elif power_iteration_normalizer == "LU":
-        normalizer = partial(linalg.lu, permute_l=True)
+        normalizer = partial(linalg.lu, permute_l=True, check_finite=False)
     else:
         normalizer = lambda x: (x, None)
 
     # Perform power iterations with Q to further 'imprint' the top
     # singular vectors of A in Q
     for _ in range(n_iter):
         Q, _ = normalizer(A @ Q)
@@ -860,52 +860,55 @@
 
     Parameters
     ----------
     u : ndarray
         Parameters u and v are the output of `linalg.svd` or
         :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner
         dimensions so one can compute `np.dot(u * s, v)`.
+        u can be None if `u_based_decision` is False.
 
     v : ndarray
         Parameters u and v are the output of `linalg.svd` or
         :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner
         dimensions so one can compute `np.dot(u * s, v)`. The input v should
         really be called vt to be consistent with scipy's output.
+        v can be None if `u_based_decision` is True.
 
     u_based_decision : bool, default=True
         If True, use the columns of u as the basis for sign flipping.
         Otherwise, use the rows of v. The choice of which variable to base the
         decision on is generally algorithm dependent.
 
     Returns
     -------
     u_adjusted : ndarray
         Array u with adjusted columns and the same dimensions as u.
 
     v_adjusted : ndarray
         Array v with adjusted rows and the same dimensions as v.
     """
-    xp, _ = get_namespace(u, v)
-    device = getattr(u, "device", None)
+    xp, _ = get_namespace(*[a for a in [u, v] if a is not None])
 
     if u_based_decision:
         # columns of u, rows of v, or equivalently rows of u.T and v
         max_abs_u_cols = xp.argmax(xp.abs(u.T), axis=1)
-        shift = xp.arange(u.T.shape[0], device=device)
+        shift = xp.arange(u.T.shape[0], device=device(u))
         indices = max_abs_u_cols + shift * u.T.shape[1]
         signs = xp.sign(xp.take(xp.reshape(u.T, (-1,)), indices, axis=0))
         u *= signs[np.newaxis, :]
-        v *= signs[:, np.newaxis]
+        if v is not None:
+            v *= signs[:, np.newaxis]
     else:
         # rows of v, columns of u
         max_abs_v_rows = xp.argmax(xp.abs(v), axis=1)
-        shift = xp.arange(v.shape[0], device=device)
+        shift = xp.arange(v.shape[0], device=device(v))
         indices = max_abs_v_rows + shift * v.shape[1]
-        signs = xp.sign(xp.take(xp.reshape(v, (-1,)), indices))
-        u *= signs[np.newaxis, :]
+        signs = xp.sign(xp.take(xp.reshape(v, (-1,)), indices, axis=0))
+        if u is not None:
+            u *= signs[np.newaxis, :]
         v *= signs[:, np.newaxis]
     return u, v
 
 
 # TODO(1.6): remove
 @deprecated(  # type: ignore
     "The function `log_logistic` is deprecated and will be removed in 1.6. "
@@ -1278,7 +1281,111 @@
     weights = np.asarray(weights)
     a, weights = a[~mask], weights[~mask]
     try:
         return np.average(a, weights=weights)
     except ZeroDivisionError:
         # this is when all weights are zero, then ignore them
         return np.average(a)
+
+
+def safe_sqr(X, *, copy=True):
+    """Element wise squaring of array-likes and sparse matrices.
+
+    Parameters
+    ----------
+    X : {array-like, ndarray, sparse matrix}
+
+    copy : bool, default=True
+        Whether to create a copy of X and operate on it or to perform
+        inplace computation (default behaviour).
+
+    Returns
+    -------
+    X ** 2 : element wise square
+         Return the element-wise square of the input.
+
+    Examples
+    --------
+    >>> from sklearn.utils import safe_sqr
+    >>> safe_sqr([1, 2, 3])
+    array([1, 4, 9])
+    """
+    X = check_array(X, accept_sparse=["csr", "csc", "coo"], ensure_2d=False)
+    if sparse.issparse(X):
+        if copy:
+            X = X.copy()
+        X.data **= 2
+    else:
+        if copy:
+            X = X**2
+        else:
+            X **= 2
+    return X
+
+
+def _approximate_mode(class_counts, n_draws, rng):
+    """Computes approximate mode of multivariate hypergeometric.
+
+    This is an approximation to the mode of the multivariate
+    hypergeometric given by class_counts and n_draws.
+    It shouldn't be off by more than one.
+
+    It is the mostly likely outcome of drawing n_draws many
+    samples from the population given by class_counts.
+
+    Parameters
+    ----------
+    class_counts : ndarray of int
+        Population per class.
+    n_draws : int
+        Number of draws (samples to draw) from the overall population.
+    rng : random state
+        Used to break ties.
+
+    Returns
+    -------
+    sampled_classes : ndarray of int
+        Number of samples drawn from each class.
+        np.sum(sampled_classes) == n_draws
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.utils.extmath import _approximate_mode
+    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)
+    array([2, 1])
+    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)
+    array([3, 1])
+    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),
+    ...                   n_draws=2, rng=0)
+    array([0, 1, 1, 0])
+    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),
+    ...                   n_draws=2, rng=42)
+    array([1, 1, 0, 0])
+    """
+    rng = check_random_state(rng)
+    # this computes a bad approximation to the mode of the
+    # multivariate hypergeometric given by class_counts and n_draws
+    continuous = class_counts / class_counts.sum() * n_draws
+    # floored means we don't overshoot n_samples, but probably undershoot
+    floored = np.floor(continuous)
+    # we add samples according to how much "left over" probability
+    # they had, until we arrive at n_samples
+    need_to_add = int(n_draws - floored.sum())
+    if need_to_add > 0:
+        remainder = continuous - floored
+        values = np.sort(np.unique(remainder))[::-1]
+        # add according to remainder, but break ties
+        # randomly to avoid biases
+        for value in values:
+            (inds,) = np.where(remainder == value)
+            # if we need_to_add less than what's in inds
+            # we draw randomly from them.
+            # if we need to add more, we add them all and
+            # go to the next value
+            add_now = min(len(inds), need_to_add)
+            inds = rng.choice(inds, size=add_now, replace=False)
+            floored[inds] += 1
+            need_to_add -= add_now
+            if need_to_add == 0:
+                break
+    return floored.astype(int)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/fixes.py` & `scikit_learn-1.5.0rc1/sklearn/utils/fixes.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,30 +1,35 @@
 """Compatibility fixes for older version of python, numpy and scipy
 
 If you add content to this file, please give the version of the package
 at which the fix is no longer needed.
 """
+
 # Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>
 #          Gael Varoquaux <gael.varoquaux@normalesup.org>
 #          Fabian Pedregosa <fpedregosa@acm.org>
 #          Lars Buitinck
 #
 # License: BSD 3 clause
 
+import platform
+import struct
 
 import numpy as np
 import scipy
 import scipy.sparse.linalg
 import scipy.stats
-import threadpoolctl
 
 import sklearn
 
 from ..externals._packaging.version import parse as parse_version
-from .deprecation import deprecated
+
+_IS_PYPY = platform.python_implementation() == "PyPy"
+_IS_32BIT = 8 * struct.calcsize("P") == 32
+_IS_WASM = platform.machine() in ["wasm32", "wasm64"]
 
 np_version = parse_version(np.__version__)
 np_base_version = parse_version(np_version.base_version)
 sp_version = parse_version(scipy.__version__)
 sp_base_version = parse_version(sp_version.base_version)
 
 # TODO: We can consider removing the containers and importing
@@ -46,14 +51,33 @@
     CSC_CONTAINERS.append(scipy.sparse.csc_array)
     COO_CONTAINERS.append(scipy.sparse.coo_array)
     LIL_CONTAINERS.append(scipy.sparse.lil_array)
     DOK_CONTAINERS.append(scipy.sparse.dok_array)
     BSR_CONTAINERS.append(scipy.sparse.bsr_array)
     DIA_CONTAINERS.append(scipy.sparse.dia_array)
 
+
+# Remove when minimum scipy version is 1.11.0
+try:
+    from scipy.sparse import sparray  # noqa
+
+    SPARRAY_PRESENT = True
+except ImportError:
+    SPARRAY_PRESENT = False
+
+
+# Remove when minimum scipy version is 1.8
+try:
+    from scipy.sparse import csr_array  # noqa
+
+    SPARSE_ARRAY_PRESENT = True
+except ImportError:
+    SPARSE_ARRAY_PRESENT = False
+
+
 try:
     from scipy.optimize._linesearch import line_search_wolfe1, line_search_wolfe2
 except ImportError:  # SciPy < 1.8
     from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1  # type: ignore  # noqa
 
 
 def _object_dtype_isnan(X):
@@ -68,60 +92,14 @@
 
 if np_version < parse_version("1.22"):
     percentile = _percentile
 else:  # >= 1.22
     from numpy import percentile  # type: ignore  # noqa
 
 
-# compatibility fix for threadpoolctl >= 3.0.0
-# since version 3 it's possible to setup a global threadpool controller to avoid
-# looping through all loaded shared libraries each time.
-# the global controller is created during the first call to threadpoolctl.
-def _get_threadpool_controller():
-    if not hasattr(threadpoolctl, "ThreadpoolController"):
-        return None
-
-    if not hasattr(sklearn, "_sklearn_threadpool_controller"):
-        sklearn._sklearn_threadpool_controller = threadpoolctl.ThreadpoolController()
-
-    return sklearn._sklearn_threadpool_controller
-
-
-def threadpool_limits(limits=None, user_api=None):
-    controller = _get_threadpool_controller()
-    if controller is not None:
-        return controller.limit(limits=limits, user_api=user_api)
-    else:
-        return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)
-
-
-threadpool_limits.__doc__ = threadpoolctl.threadpool_limits.__doc__
-
-
-def threadpool_info():
-    controller = _get_threadpool_controller()
-    if controller is not None:
-        return controller.info()
-    else:
-        return threadpoolctl.threadpool_info()
-
-
-threadpool_info.__doc__ = threadpoolctl.threadpool_info.__doc__
-
-
-@deprecated(
-    "The function `delayed` has been moved from `sklearn.utils.fixes` to "
-    "`sklearn.utils.parallel`. This import path will be removed in 1.5."
-)
-def delayed(function):
-    from sklearn.utils.parallel import delayed
-
-    return delayed(function)
-
-
 # TODO: Remove when SciPy 1.11 is the minimum supported version
 def _mode(a, axis=0):
     if sp_version >= parse_version("1.9.0"):
         mode = scipy.stats.mode(a, axis=axis, keepdims=True)
         if sp_version >= parse_version("1.10.999"):
             # scipy.stats.mode has changed returned array shape with axis=None
             # and keepdims=True, see https://github.com/scipy/scipy/pull/17561
@@ -388,7 +366,52 @@
 
 
 # TODO: Remove when Scipy 1.12 is the minimum supported version
 if sp_version < parse_version("1.12"):
     from ..externals._scipy.sparse.csgraph import laplacian  # type: ignore  # noqa
 else:
     from scipy.sparse.csgraph import laplacian  # type: ignore  # noqa  # pragma: no cover
+
+
+# TODO: Remove when we drop support for Python 3.9. Note the filter argument has
+# been back-ported in 3.9.17 but we can not assume anything about the micro
+# version, see
+# https://docs.python.org/3.9/library/tarfile.html#tarfile.TarFile.extractall
+# for more details
+def tarfile_extractall(tarfile, path):
+    try:
+        tarfile.extractall(path, filter="data")
+    except TypeError:
+        tarfile.extractall(path)
+
+
+def _in_unstable_openblas_configuration():
+    """Return True if in an unstable configuration for OpenBLAS"""
+
+    # Import libraries which might load OpenBLAS.
+    import numpy  # noqa
+    import scipy  # noqa
+
+    modules_info = sklearn._threadpool_controller.info()
+
+    open_blas_used = any(info["internal_api"] == "openblas" for info in modules_info)
+    if not open_blas_used:
+        return False
+
+    # OpenBLAS 0.3.16 fixed instability for arm64, see:
+    # https://github.com/xianyi/OpenBLAS/blob/1b6db3dbba672b4f8af935bd43a1ff6cff4d20b7/Changelog.txt#L56-L58 # noqa
+    openblas_arm64_stable_version = parse_version("0.3.16")
+    for info in modules_info:
+        if info["internal_api"] != "openblas":
+            continue
+        openblas_version = info.get("version")
+        openblas_architecture = info.get("architecture")
+        if openblas_version is None or openblas_architecture is None:
+            # Cannot be sure that OpenBLAS is good enough. Assume unstable:
+            return True  # pragma: no cover
+        if (
+            openblas_architecture == "neoversen1"
+            and parse_version(openblas_version) < openblas_arm64_stable_version
+        ):
+            # See discussions in https://github.com/numpy/numpy/issues/19411
+            return True  # pragma: no cover
+    return False
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/graph.py` & `scikit_learn-1.5.0rc1/sklearn/utils/graph.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/meson.build` & `scikit_learn-1.5.0rc1/sklearn/utils/meson.build`

 * *Files 6% similar despite different names*

```diff
@@ -1,46 +1,48 @@
 # utils is cimported from other subpackages so this is needed for the cimport
 # to work
 utils_cython_tree = [
+  # We add sklearn_root_cython_tree to make sure sklearn/__init__.py is copied
+  # early in the build
+  sklearn_root_cython_tree,
   fs.copyfile('__init__.py'),
   fs.copyfile('_cython_blas.pxd'),
   fs.copyfile('_heap.pxd'),
   fs.copyfile('_openmp_helpers.pxd'),
   fs.copyfile('_random.pxd'),
   fs.copyfile('_sorting.pxd'),
   fs.copyfile('_typedefs.pxd'),
   fs.copyfile('_vector_sentinel.pxd'),
 ]
 
 utils_extension_metadata = {
   'sparsefuncs_fast':
-    {'sources': ['sparsefuncs_fast.pyx'], 'dependencies': [np_dep]},
+    {'sources': ['sparsefuncs_fast.pyx']},
   '_cython_blas': {'sources': ['_cython_blas.pyx']},
   'arrayfuncs': {'sources': ['arrayfuncs.pyx']},
   'murmurhash': {
       'sources': ['murmurhash.pyx', 'src' / 'MurmurHash3.cpp'],
-      'dependencies': [np_dep]
   },
   '_fast_dict':
     {'sources': ['_fast_dict.pyx'], 'override_options': ['cython_language=cpp']},
   '_openmp_helpers': {'sources': ['_openmp_helpers.pyx'], 'dependencies': [openmp_dep]},
-  '_random': {'sources': ['_random.pyx'], 'dependencies': [np_dep]},
+  '_random': {'sources': ['_random.pyx']},
   '_typedefs': {'sources': ['_typedefs.pyx']},
   '_heap': {'sources': ['_heap.pyx']},
   '_sorting': {'sources': ['_sorting.pyx']},
   '_vector_sentinel':
     {'sources': ['_vector_sentinel.pyx'], 'override_options': ['cython_language=cpp'],
      'dependencies': [np_dep]},
   '_isfinite': {'sources': ['_isfinite.pyx']},
 }
 
 foreach ext_name, ext_dict : utils_extension_metadata
   py.extension_module(
     ext_name,
-    ext_dict.get('sources') + utils_cython_tree,
+    [ext_dict.get('sources'), utils_cython_tree],
     dependencies: ext_dict.get('dependencies', []),
     override_options : ext_dict.get('override_options', []),
     cython_args: cython_args,
     subdir: 'sklearn/utils',
     install: true
   )
 endforeach
@@ -60,14 +62,13 @@
     name + '_pyx',
     output: name + '.pyx',
     input: name + '.pyx.tp',
     command: [py, tempita, '@INPUT@', '-o', '@OUTDIR@']
   )
   py.extension_module(
     name,
-    [pxd, pyx] + utils_cython_tree,
-    dependencies: [np_dep],
+    [pxd, pyx, utils_cython_tree],
     cython_args: cython_args,
     subdir: 'sklearn/utils',
     install: true
    )
 endforeach
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/metadata_routing.py` & `scikit_learn-1.5.0rc1/sklearn/utils/metadata_routing.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/metaestimators.py` & `scikit_learn-1.5.0rc1/sklearn/utils/metaestimators.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/multiclass.py` & `scikit_learn-1.5.0rc1/sklearn/utils/multiclass.py`

 * *Files 3% similar despite different names*

```diff
@@ -338,21 +338,32 @@
                 if str(e).startswith("Complex data not supported"):
                     raise
 
                 # dtype=object should be provided explicitly for ragged arrays,
                 # see NEP 34
                 y = check_array(y, dtype=object, **check_y_kwargs)
 
-    # The old sequence of sequences format
     try:
-        first_row = y[[0], :] if issparse(y) else y[0]
+        # TODO(1.7): Change to ValueError when byte labels is deprecated.
+        # labels in bytes format
+        first_row_or_val = y[[0], :] if issparse(y) else y[0]
+        if isinstance(first_row_or_val, bytes):
+            warnings.warn(
+                (
+                    "Support for labels represented as bytes is deprecated in v1.5 and"
+                    " will error in v1.7. Convert the labels to a string or integer"
+                    " format."
+                ),
+                FutureWarning,
+            )
+        # The old sequence of sequences format
         if (
-            not hasattr(first_row, "__array__")
-            and isinstance(first_row, Sequence)
-            and not isinstance(first_row, str)
+            not hasattr(first_row_or_val, "__array__")
+            and isinstance(first_row_or_val, Sequence)
+            and not isinstance(first_row_or_val, str)
         ):
             raise ValueError(
                 "You appear to be using a legacy multi-label data"
                 " representation. Sequence of sequences are no"
                 " longer supported; use a binary array or sparse"
                 " matrix instead - the MultiLabelBinarizer"
                 " transformer can convert to this format."
@@ -386,17 +397,17 @@
         # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]
         data = y.data if issparse(y) else y
         if xp.any(data != xp.astype(data, int)):
             _assert_all_finite(data, input_name=input_name)
             return "continuous" + suffix
 
     # Check multiclass
-    if issparse(first_row):
-        first_row = first_row.data
-    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):
+    if issparse(first_row_or_val):
+        first_row_or_val = first_row_or_val.data
+    if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row_or_val) > 1):
         # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]
         return "multiclass" + suffix
     else:
         return "binary"  # [1, 2] or [["a"], ["b"]]
 
 
 def _check_partial_fit_first_call(clf, classes=None):
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/murmurhash.pxd` & `scikit_learn-1.5.0rc1/sklearn/utils/murmurhash.pxd`

 * *Files 18% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 """Export fast murmurhash C/C++ routines + cython wrappers"""
 
-cimport numpy as cnp
+from ..utils._typedefs cimport int32_t, uint32_t
 
 # The C API is disabled for now, since it requires -I flags to get
 # compilation to work even when these functions are not used.
 # cdef extern from "MurmurHash3.h":
 #     void MurmurHash3_x86_32(void* key, int len, unsigned int seed,
 #                             void* out)
 #
 #     void MurmurHash3_x86_128(void* key, int len, unsigned int seed,
 #                              void* out)
 #
 #     void MurmurHash3_x64_128(void* key, int len, unsigned int seed,
 #                              void* out)
 
 
-cpdef cnp.uint32_t murmurhash3_int_u32(int key, unsigned int seed)
-cpdef cnp.int32_t murmurhash3_int_s32(int key, unsigned int seed)
-cpdef cnp.uint32_t murmurhash3_bytes_u32(bytes key, unsigned int seed)
-cpdef cnp.int32_t murmurhash3_bytes_s32(bytes key, unsigned int seed)
+cpdef uint32_t murmurhash3_int_u32(int key, unsigned int seed)
+cpdef int32_t murmurhash3_int_s32(int key, unsigned int seed)
+cpdef uint32_t murmurhash3_bytes_u32(bytes key, unsigned int seed)
+cpdef int32_t murmurhash3_bytes_s32(bytes key, unsigned int seed)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/murmurhash.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/murmurhash.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -10,76 +10,74 @@
   https://code.google.com/p/smhasher/
 
 """
 # Author: Olivier Grisel <olivier.grisel@ensta.org>
 #
 # License: BSD 3 clause
 
+from ..utils._typedefs cimport int32_t, uint32_t
 
-cimport numpy as cnp
 import numpy as np
 
 cdef extern from "src/MurmurHash3.h":
-    void MurmurHash3_x86_32(void *key, int len, cnp.uint32_t seed, void *out)
-    void MurmurHash3_x86_128(void *key, int len, cnp.uint32_t seed, void *out)
-    void MurmurHash3_x64_128 (void *key, int len, cnp.uint32_t seed, void *out)
+    void MurmurHash3_x86_32(void *key, int len, uint32_t seed, void *out)
+    void MurmurHash3_x86_128(void *key, int len, uint32_t seed, void *out)
+    void MurmurHash3_x64_128 (void *key, int len, uint32_t seed, void *out)
 
-cnp.import_array()
 
-
-cpdef cnp.uint32_t murmurhash3_int_u32(int key, unsigned int seed):
+cpdef uint32_t murmurhash3_int_u32(int key, unsigned int seed):
     """Compute the 32bit murmurhash3 of a int key at seed."""
-    cdef cnp.uint32_t out
+    cdef uint32_t out
     MurmurHash3_x86_32(&key, sizeof(int), seed, &out)
     return out
 
 
-cpdef cnp.int32_t murmurhash3_int_s32(int key, unsigned int seed):
+cpdef int32_t murmurhash3_int_s32(int key, unsigned int seed):
     """Compute the 32bit murmurhash3 of a int key at seed."""
-    cdef cnp.int32_t out
+    cdef int32_t out
     MurmurHash3_x86_32(&key, sizeof(int), seed, &out)
     return out
 
 
-cpdef cnp.uint32_t murmurhash3_bytes_u32(bytes key, unsigned int seed):
+cpdef uint32_t murmurhash3_bytes_u32(bytes key, unsigned int seed):
     """Compute the 32bit murmurhash3 of a bytes key at seed."""
-    cdef cnp.uint32_t out
+    cdef uint32_t out
     MurmurHash3_x86_32(<char*> key, len(key), seed, &out)
     return out
 
 
-cpdef cnp.int32_t murmurhash3_bytes_s32(bytes key, unsigned int seed):
+cpdef int32_t murmurhash3_bytes_s32(bytes key, unsigned int seed):
     """Compute the 32bit murmurhash3 of a bytes key at seed."""
-    cdef cnp.int32_t out
+    cdef int32_t out
     MurmurHash3_x86_32(<char*> key, len(key), seed, &out)
     return out
 
 
 def _murmurhash3_bytes_array_u32(
-    const cnp.int32_t[:] key,
+    const int32_t[:] key,
     unsigned int seed,
 ):
     """Compute 32bit murmurhash3 hashes of a key int array at seed."""
     # TODO make it possible to pass preallocated output array
     cdef:
-        cnp.uint32_t[:] out = np.zeros(key.size, np.uint32)
+        uint32_t[:] out = np.zeros(key.size, np.uint32)
         Py_ssize_t i
     for i in range(key.shape[0]):
         out[i] = murmurhash3_int_u32(key[i], seed)
     return np.asarray(out)
 
 
 def _murmurhash3_bytes_array_s32(
-    const cnp.int32_t[:] key,
+    const int32_t[:] key,
     unsigned int seed,
 ):
     """Compute 32bit murmurhash3 hashes of a key int array at seed."""
     # TODO make it possible to pass preallocated output array
     cdef:
-        cnp.int32_t[:] out = np.zeros(key.size, np.int32)
+        int32_t[:] out = np.zeros(key.size, np.int32)
         Py_ssize_t i
     for i in range(key.shape[0]):
         out[i] = murmurhash3_int_s32(key[i], seed)
     return np.asarray(out)
 
 
 def murmurhash3_32(key, seed=0, positive=False):
@@ -117,17 +115,17 @@
     elif isinstance(key, unicode):
         if positive:
             return murmurhash3_bytes_u32(key.encode('utf-8'), seed)
         else:
             return murmurhash3_bytes_s32(key.encode('utf-8'), seed)
     elif isinstance(key, int) or isinstance(key, np.int32):
         if positive:
-            return murmurhash3_int_u32(<cnp.int32_t>key, seed)
+            return murmurhash3_int_u32(<int32_t>key, seed)
         else:
-            return murmurhash3_int_s32(<cnp.int32_t>key, seed)
+            return murmurhash3_int_s32(<int32_t>key, seed)
     elif isinstance(key, np.ndarray):
         if key.dtype != np.int32:
             raise TypeError(
                 "key.dtype should be int32, got %s" % key.dtype)
         if positive:
             return _murmurhash3_bytes_array_u32(key.ravel(), seed).reshape(key.shape)
         else:
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/optimize.py` & `scikit_learn-1.5.0rc1/sklearn/utils/optimize.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 Unlike the scipy.optimize version, this version of the Newton conjugate
 gradient solver uses only one function call to retrieve the
 func value, the gradient value and a callable for the Hessian matvec
 product. If the function call is very expensive (e.g. for logistic
 regression with large design matrix), this approach gives very
 significant speedups.
 """
+
 # This is a modified file from scipy.optimize
 # Original authors: Travis Oliphant, Eric Jones
 # Modifications by Gael Varoquaux, Mathieu Blondel and Tom Dupre la Tour
 # License: BSD
 
 import warnings
 
@@ -22,44 +23,66 @@
 from .fixes import line_search_wolfe1, line_search_wolfe2
 
 
 class _LineSearchError(RuntimeError):
     pass
 
 
-def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs):
+def _line_search_wolfe12(
+    f, fprime, xk, pk, gfk, old_fval, old_old_fval, verbose=0, **kwargs
+):
     """
     Same as line_search_wolfe1, but fall back to line_search_wolfe2 if
     suitable step length is not found, and raise an exception if a
     suitable step length is not found.
 
     Raises
     ------
     _LineSearchError
         If no suitable step size is found.
 
     """
+    is_verbose = verbose >= 2
+    eps = 16 * np.finfo(np.asarray(old_fval).dtype).eps
+    if is_verbose:
+        print("  Line Search")
+        print(f"    eps=16 * finfo.eps={eps}")
+        print("    try line search wolfe1")
+
     ret = line_search_wolfe1(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)
 
+    if is_verbose:
+        _not_ = "not " if ret[0] is None else ""
+        print("    wolfe1 line search was " + _not_ + "successful")
+
     if ret[0] is None:
         # Have a look at the line_search method of our NewtonSolver class. We borrow
         # the logic from there
         # Deal with relative loss differences around machine precision.
         args = kwargs.get("args", tuple())
         fval = f(xk + pk, *args)
-        eps = 16 * np.finfo(np.asarray(old_fval).dtype).eps
         tiny_loss = np.abs(old_fval * eps)
         loss_improvement = fval - old_fval
         check = np.abs(loss_improvement) <= tiny_loss
+        if is_verbose:
+            print(
+                "    check loss |improvement| <= eps * |loss_old|:"
+                f" {np.abs(loss_improvement)} <= {tiny_loss} {check}"
+            )
         if check:
             # 2.1 Check sum of absolute gradients as alternative condition.
             sum_abs_grad_old = scipy.linalg.norm(gfk, ord=1)
             grad = fprime(xk + pk, *args)
             sum_abs_grad = scipy.linalg.norm(grad, ord=1)
             check = sum_abs_grad < sum_abs_grad_old
+            if is_verbose:
+                print(
+                    "    check sum(|gradient|) < sum(|gradient_old|): "
+                    f"{sum_abs_grad} < {sum_abs_grad_old} {check}"
+                )
             if check:
                 ret = (
                     1.0,  # step size
                     ret[1] + 1,  # number of function evaluations
                     ret[2] + 1,  # number of gradient evaluations
                     fval,
                     old_fval,
@@ -67,25 +90,30 @@
                 )
 
     if ret[0] is None:
         # line search failed: try different one.
         # TODO: It seems that the new check for the sum of absolute gradients above
         # catches all cases that, earlier, ended up here. In fact, our tests never
         # trigger this "if branch" here and we can consider to remove it.
+        if is_verbose:
+            print("    last resort: try line search wolfe2")
         ret = line_search_wolfe2(
             f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs
         )
+        if is_verbose:
+            _not_ = "not " if ret[0] is None else ""
+            print("    wolfe2 line search was " + _not_ + "successful")
 
     if ret[0] is None:
         raise _LineSearchError()
 
     return ret
 
 
-def _cg(fhess_p, fgrad, maxiter, tol):
+def _cg(fhess_p, fgrad, maxiter, tol, verbose=0):
     """
     Solve iteratively the linear system 'fhess_p . xsupi = fgrad'
     with a conjugate gradient descent.
 
     Parameters
     ----------
     fhess_p : callable
@@ -102,64 +130,90 @@
         Stopping criterion.
 
     Returns
     -------
     xsupi : ndarray of shape (n_features,) or (n_features + 1,)
         Estimated solution.
     """
+    eps = 16 * np.finfo(np.float64).eps
     xsupi = np.zeros(len(fgrad), dtype=fgrad.dtype)
-    ri = np.copy(fgrad)
+    ri = np.copy(fgrad)  # residual = fgrad - fhess_p @ xsupi
     psupi = -ri
     i = 0
     dri0 = np.dot(ri, ri)
-    # We also track of |p_i|^2.
+    # We also keep track of |p_i|^2.
     psupi_norm2 = dri0
+    is_verbose = verbose >= 2
 
     while i <= maxiter:
         if np.sum(np.abs(ri)) <= tol:
+            if is_verbose:
+                print(
+                    f"  Inner CG solver iteration {i} stopped with\n"
+                    f"    sum(|residuals|) <= tol: {np.sum(np.abs(ri))} <= {tol}"
+                )
             break
 
         Ap = fhess_p(psupi)
         # check curvature
         curv = np.dot(psupi, Ap)
-        if 0 <= curv <= 16 * np.finfo(np.float64).eps * psupi_norm2:
+        if 0 <= curv <= eps * psupi_norm2:
             # See https://arxiv.org/abs/1803.02924, Algo 1 Capped Conjugate Gradient.
+            if is_verbose:
+                print(
+                    f"  Inner CG solver iteration {i} stopped with\n"
+                    f"    tiny_|p| = eps * ||p||^2, eps = {eps}, "
+                    f"squred L2 norm ||p||^2 = {psupi_norm2}\n"
+                    f"    curvature <= tiny_|p|: {curv} <= {eps * psupi_norm2}"
+                )
             break
         elif curv < 0:
             if i > 0:
+                if is_verbose:
+                    print(
+                        f"  Inner CG solver iteration {i} stopped with negative "
+                        f"curvature, curvature = {curv}"
+                    )
                 break
             else:
                 # fall back to steepest descent direction
                 xsupi += dri0 / curv * psupi
+                if is_verbose:
+                    print("  Inner CG solver iteration 0 fell back to steepest descent")
                 break
         alphai = dri0 / curv
         xsupi += alphai * psupi
         ri += alphai * Ap
         dri1 = np.dot(ri, ri)
         betai = dri1 / dri0
         psupi = -ri + betai * psupi
         # We use  |p_i|^2 = |r_i|^2 + beta_i^2 |p_{i-1}|^2
         psupi_norm2 = dri1 + betai**2 * psupi_norm2
         i = i + 1
         dri0 = dri1  # update np.dot(ri,ri) for next time.
-
+    if is_verbose and i > maxiter:
+        print(
+            f"  Inner CG solver stopped reaching maxiter={i - 1} with "
+            f"sum(|residuals|) = {np.sum(np.abs(ri))}"
+        )
     return xsupi
 
 
 def _newton_cg(
     grad_hess,
     func,
     grad,
     x0,
     args=(),
     tol=1e-4,
     maxiter=100,
     maxinner=200,
     line_search=True,
     warn=True,
+    verbose=0,
 ):
     """
     Minimization of scalar function of one or more variables using the
     Newton-CG algorithm.
 
     Parameters
     ----------
@@ -205,52 +259,75 @@
     x0 = np.asarray(x0).flatten()
     xk = np.copy(x0)
     k = 0
 
     if line_search:
         old_fval = func(x0, *args)
         old_old_fval = None
+    else:
+        old_fval = 0
+
+    is_verbose = verbose > 0
 
     # Outer loop: our Newton iteration
     while k < maxiter:
         # Compute a search direction pk by applying the CG method to
         #  del2 f(xk) p = - fgrad f(xk) starting from 0.
         fgrad, fhess_p = grad_hess(xk, *args)
 
         absgrad = np.abs(fgrad)
-        if np.max(absgrad) <= tol:
+        max_absgrad = np.max(absgrad)
+        check = max_absgrad <= tol
+        if is_verbose:
+            print(f"Newton-CG iter = {k}")
+            print("  Check Convergence")
+            print(f"    max |gradient| <= tol: {max_absgrad} <= {tol} {check}")
+        if check:
             break
 
         maggrad = np.sum(absgrad)
         eta = min([0.5, np.sqrt(maggrad)])
         termcond = eta * maggrad
 
         # Inner loop: solve the Newton update by conjugate gradient, to
         # avoid inverting the Hessian
-        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
+        xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond, verbose=verbose)
 
         alphak = 1.0
 
         if line_search:
             try:
                 alphak, fc, gc, old_fval, old_old_fval, gfkp1 = _line_search_wolfe12(
-                    func, grad, xk, xsupi, fgrad, old_fval, old_old_fval, args=args
+                    func,
+                    grad,
+                    xk,
+                    xsupi,
+                    fgrad,
+                    old_fval,
+                    old_old_fval,
+                    verbose=verbose,
+                    args=args,
                 )
             except _LineSearchError:
                 warnings.warn("Line Search failed")
                 break
 
         xk += alphak * xsupi  # upcast if necessary
         k += 1
 
     if warn and k >= maxiter:
         warnings.warn(
-            "newton-cg failed to converge. Increase the number of iterations.",
+            (
+                f"newton-cg failed to converge at loss = {old_fval}. Increase the"
+                " number of iterations."
+            ),
             ConvergenceWarning,
         )
+    elif is_verbose:
+        print(f"  Solver did converge at loss = {old_fval}.")
     return xk, k
 
 
 def _check_optimize_result(solver, result, max_iter=None, extra_warning_msg=None):
     """Check the OptimizeResult for successful convergence
 
     Parameters
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/parallel.py` & `scikit_learn-1.5.0rc1/sklearn/utils/parallel.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/random.py` & `scikit_learn-1.5.0rc1/sklearn/utils/random.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/sparsefuncs.py` & `scikit_learn-1.5.0rc1/sklearn/utils/sparsefuncs.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/sparsefuncs_fast.pyx` & `scikit_learn-1.5.0rc1/sklearn/utils/sparsefuncs_fast.pyx`

 * *Files 4% similar despite different names*

```diff
@@ -10,29 +10,22 @@
 #          Giorgio Patrini
 #
 # License: BSD 3 clause
 
 from libc.math cimport fabs, sqrt, isnan
 from libc.stdint cimport intptr_t
 
-cimport numpy as cnp
 import numpy as np
 from cython cimport floating
-from ..utils._typedefs cimport float64_t, int32_t, intp_t, uint64_t
+from ..utils._typedefs cimport float64_t, int32_t, int64_t, intp_t, uint64_t
 
-cnp.import_array()
 
-
-# This `integral` fused type is defined to be used over `cython.integral`
-# to only generate implementations for `int{32,64}_t`.
-# TODO: use `{int,float}{32,64}_t` when cython#5230 is resolved:
-# https://github.com/cython/cython/issues/5230
 ctypedef fused integral:
-    int  # int32_t
-    long long  # int64_t
+    int32_t
+    int64_t
 
 
 def csr_row_norms(X):
     """Squared L2 norm of each row in CSR matrix X."""
     if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
     return _sqeuclidean_row_norms_sparse(X.data, X.indptr)
@@ -488,15 +481,37 @@
         np.asarray(updated_mean),
         np.asarray(updated_var),
         np.asarray(updated_n),
     )
 
 
 def inplace_csr_row_normalize_l1(X):
-    """Inplace row normalize using the l1 norm"""
+    """Normalize inplace the rows of a CSR matrix or array by their L1 norm.
+
+    Parameters
+    ----------
+    X : scipy.sparse.csr_matrix and scipy.sparse.csr_array, \
+            shape=(n_samples, n_features)
+        The input matrix or array to be modified inplace.
+
+    Examples
+    --------
+    >>> from scipy.sparse import csr_matrix
+    >>> from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l1
+    >>> X = csr_matrix(([1.0, 2.0, 3.0], [0, 2, 3], [0, 3, 4]), shape=(3, 4))
+    >>> X.toarray()
+    array([[1., 2., 0., 0.],
+           [0., 0., 3., 0.],
+           [0., 0., 0., 4.]])
+    >>> inplace_csr_row_normalize_l1(X)
+    >>> X.toarray()
+    array([[0.33...   , 0.66...   , 0.        , 0.        ],
+           [0.        , 0.        , 1.        , 0.        ],
+           [0.        , 0.        , 0.        , 1.        ]])
+    """
     _inplace_csr_row_normalize_l1(X.data, X.shape, X.indices, X.indptr)
 
 
 def _inplace_csr_row_normalize_l1(
     floating[:] X_data,
     shape,
     const integral[:] X_indices,
@@ -525,15 +540,36 @@
             continue
 
         for j in range(X_indptr[i], X_indptr[i + 1]):
             X_data[j] /= sum_
 
 
 def inplace_csr_row_normalize_l2(X):
-    """Inplace row normalize using the l2 norm"""
+    """Normalize inplace the rows of a CSR matrix or array by their L2 norm.
+
+    Parameters
+    ----------
+    X : scipy.sparse.csr_matrix, shape=(n_samples, n_features)
+        The input matrix or array to be modified inplace.
+
+    Examples
+    --------
+    >>> from scipy.sparse import csr_matrix
+    >>> from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2
+    >>> X = csr_matrix(([1.0, 2.0, 3.0], [0, 2, 3], [0, 3, 4]), shape=(3, 4))
+    >>> X.toarray()
+    array([[1., 2., 0., 0.],
+           [0., 0., 3., 0.],
+           [0., 0., 0., 4.]])
+    >>> inplace_csr_row_normalize_l2(X)
+    >>> X.toarray()
+    array([[0.44...   , 0.89...   , 0.        , 0.        ],
+           [0.        , 0.        , 1.        , 0.        ],
+           [0.        , 0.        , 0.        , 1.        ]])
+    """
     _inplace_csr_row_normalize_l2(X.data, X.shape, X.indices, X.indptr)
 
 
 def _inplace_csr_row_normalize_l2(
     floating[:] X_data,
     shape,
     const integral[:] X_indices,
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/src/MurmurHash3.cpp` & `scikit_learn-1.5.0rc1/sklearn/utils/src/MurmurHash3.cpp`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/src/MurmurHash3.h` & `scikit_learn-1.5.0rc1/sklearn/utils/src/MurmurHash3.h`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/stats.py` & `scikit_learn-1.5.0rc1/sklearn/utils/stats.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_arrayfuncs.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_arrayfuncs.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_bunch.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_bunch.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_class_weight.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_class_weight.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_cython_blas.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_cython_blas.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_cython_templating.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_cython_templating.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_deprecation.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_deprecation.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_encode.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_encode.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_estimator_checks.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_estimator_checks.py`

 * *Files 2% similar despite different names*

```diff
@@ -60,15 +60,15 @@
     check_no_attributes_set_in_init,
     check_outlier_contamination,
     check_outlier_corruption,
     check_regressor_data_not_an_array,
     check_requires_y_none,
     set_random_state,
 )
-from sklearn.utils.fixes import CSR_CONTAINERS
+from sklearn.utils.fixes import CSR_CONTAINERS, SPARRAY_PRESENT
 from sklearn.utils.metaestimators import available_if
 from sklearn.utils.validation import check_array, check_is_fitted, check_X_y
 
 
 class CorrectNotFittedError(ValueError):
     """Exception class to raise if estimator is used before fitting.
 
@@ -203,17 +203,25 @@
 class NoCheckinPredict(BaseBadClassifier):
     def fit(self, X, y):
         X, y = self._validate_data(X, y)
         return self
 
 
 class NoSparseClassifier(BaseBadClassifier):
+    def __init__(self, raise_for_type=None):
+        # raise_for_type : str, expects "sparse_array" or "sparse_matrix"
+        self.raise_for_type = raise_for_type
+
     def fit(self, X, y):
         X, y = self._validate_data(X, y, accept_sparse=["csr", "csc"])
-        if sp.issparse(X):
+        if self.raise_for_type == "sparse_array":
+            correct_type = isinstance(X, sp.sparray)
+        elif self.raise_for_type == "sparse_matrix":
+            correct_type = isinstance(X, sp.spmatrix)
+        if correct_type:
             raise ValueError("Nonsensical Error")
         return self
 
     def predict(self, X):
         X = check_array(X)
         return np.ones(X.shape[0])
 
@@ -353,28 +361,39 @@
         X = check_array(X)
         if self.has_single_class_:
             return np.zeros(X.shape[0])
         return np.ones(X.shape[0])
 
 
 class LargeSparseNotSupportedClassifier(BaseEstimator):
+    """Estimator that claims to support large sparse data
+    (accept_large_sparse=True), but doesn't"""
+
+    def __init__(self, raise_for_type=None):
+        # raise_for_type : str, expects "sparse_array" or "sparse_matrix"
+        self.raise_for_type = raise_for_type
+
     def fit(self, X, y):
         X, y = self._validate_data(
             X,
             y,
             accept_sparse=("csr", "csc", "coo"),
             accept_large_sparse=True,
             multi_output=True,
             y_numeric=True,
         )
-        if sp.issparse(X):
-            if X.getformat() == "coo":
+        if self.raise_for_type == "sparse_array":
+            correct_type = isinstance(X, sp.sparray)
+        elif self.raise_for_type == "sparse_matrix":
+            correct_type = isinstance(X, sp.spmatrix)
+        if correct_type:
+            if X.format == "coo":
                 if X.row.dtype == "int64" or X.col.dtype == "int64":
                     raise ValueError("Estimator doesn't support 64-bit indices")
-            elif X.getformat() in ["csc", "csr"]:
+            elif X.format in ["csc", "csr"]:
                 assert "int64" not in (
                     X.indices.dtype,
                     X.indptr.dtype,
                 ), "Estimator doesn't support 64-bit indices"
 
         return self
 
@@ -506,23 +525,23 @@
 
 def test_check_array_api_input():
     try:
         importlib.import_module("array_api_compat")
     except ModuleNotFoundError:
         raise SkipTest("array_api_compat is required to run this test")
     try:
-        importlib.import_module("numpy.array_api")
+        importlib.import_module("array_api_strict")
     except ModuleNotFoundError:  # pragma: nocover
-        raise SkipTest("numpy.array_api is required to run this test")
+        raise SkipTest("array-api-strict is required to run this test")
 
     with raises(AssertionError, match="Not equal to tolerance"):
         check_array_api_input(
             "BrokenArrayAPI",
             BrokenArrayAPI(),
-            array_namespace="numpy.array_api",
+            array_namespace="array_api_strict",
             check_values=True,
         )
 
 
 def test_not_an_array_array_function():
     not_array = _NotAnArray(np.ones(10))
     msg = "Don't want to call array_function sum!"
@@ -630,19 +649,23 @@
     name = NotInvariantPredict.__name__
     method = "predict"
     msg = ("{method} of {name} is not invariant when applied to a subset.").format(
         method=method, name=name
     )
     with raises(AssertionError, match=msg):
         check_estimator(NotInvariantPredict())
-    # check for sparse matrix input handling
+    # check for sparse data input handling
     name = NoSparseClassifier.__name__
     msg = "Estimator %s doesn't seem to fail gracefully on sparse data" % name
     with raises(AssertionError, match=msg):
-        check_estimator(NoSparseClassifier())
+        check_estimator(NoSparseClassifier("sparse_matrix"))
+
+    if SPARRAY_PRESENT:
+        with raises(AssertionError, match=msg):
+            check_estimator(NoSparseClassifier("sparse_array"))
 
     # check for classifiers reducing to less than two classes via sample weights
     name = OneClassSampleErrorClassifier.__name__
     msg = (
         f"{name} failed when fitted on one label after sample_weight "
         "trimming. Error message is not explicit, it should have "
         "'class'."
@@ -652,15 +675,19 @@
 
     # Large indices test on bad estimator
     msg = (
         "Estimator LargeSparseNotSupportedClassifier doesn't seem to "
         r"support \S{3}_64 matrix, and is not failing gracefully.*"
     )
     with raises(AssertionError, match=msg):
-        check_estimator(LargeSparseNotSupportedClassifier())
+        check_estimator(LargeSparseNotSupportedClassifier("sparse_matrix"))
+
+    if SPARRAY_PRESENT:
+        with raises(AssertionError, match=msg):
+            check_estimator(LargeSparseNotSupportedClassifier("sparse_array"))
 
     # does error on binary_only untagged estimator
     msg = "Only 2 classes are supported"
     with raises(ValueError, match=msg):
         check_estimator(UntaggedBinaryClassifier())
 
     for csr_container in CSR_CONTAINERS:
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_estimator_html_repr.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_estimator_html_repr.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_extmath.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_extmath.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,14 +18,15 @@
     assert_allclose_dense_sparse,
     assert_almost_equal,
     assert_array_almost_equal,
     assert_array_equal,
     skip_if_32bit,
 )
 from sklearn.utils.extmath import (
+    _approximate_mode,
     _deterministic_vector_sign_flip,
     _incremental_mean_and_var,
     _randomized_eigsh,
     _safe_accumulator_op,
     cartesian,
     density,
     log_logistic,
@@ -698,17 +699,15 @@
 def test_incremental_weighted_mean_and_variance_simple(rng, dtype):
     mult = 10
     X = rng.rand(1000, 20).astype(dtype) * mult
     sample_weight = rng.rand(X.shape[0]) * mult
     mean, var, _ = _incremental_mean_and_var(X, 0, 0, 0, sample_weight=sample_weight)
 
     expected_mean = np.average(X, weights=sample_weight, axis=0)
-    expected_var = (
-        np.average(X**2, weights=sample_weight, axis=0) - expected_mean**2
-    )
+    expected_var = np.average(X**2, weights=sample_weight, axis=0) - expected_mean**2
     assert_almost_equal(mean, expected_mean)
     assert_almost_equal(var, expected_var)
 
 
 @pytest.mark.parametrize("mean", [0, 1e7, -1e7])
 @pytest.mark.parametrize("var", [1, 1e-8, 1e5])
 @pytest.mark.parametrize(
@@ -1058,7 +1057,24 @@
     actual = safe_sparse_dot(A, B, dense_output=dense_output)
 
     assert sparse.issparse(actual) == (not dense_output)
 
     if dense_output:
         expected = expected.toarray()
     assert_allclose_dense_sparse(actual, expected)
+
+
+def test_approximate_mode():
+    """Make sure sklearn.utils.extmath._approximate_mode returns valid
+    results for cases where "class_counts * n_draws" is enough
+    to overflow 32-bit signed integer.
+
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/20774
+    """
+    X = np.array([99000, 1000], dtype=np.int32)
+    ret = _approximate_mode(class_counts=X, n_draws=25000, rng=0)
+
+    # Draws 25% of the total population, so in this case a fair draw means:
+    # 25% * 99.000 = 24.750
+    # 25% *  1.000 =    250
+    assert_array_equal(ret, [24750, 250])
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_fast_dict.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_fast_dict.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-""" Test fast_dict.
-"""
+"""Test fast_dict."""
+
 import numpy as np
 from numpy.testing import assert_allclose, assert_array_equal
 
 from sklearn.utils._fast_dict import IntFloatDict, argmin
 
 
 def test_int_float_dict():
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_fixes.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_fixes.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,43 +3,28 @@
 #          Lars Buitinck
 # License: BSD 3 clause
 
 import numpy as np
 import pytest
 
 from sklearn.utils._testing import assert_array_equal
-from sklearn.utils.fixes import (
-    _object_dtype_isnan,
-    _smallest_admissible_index_dtype,
-    delayed,
-)
+from sklearn.utils.fixes import _object_dtype_isnan, _smallest_admissible_index_dtype
 
 
 @pytest.mark.parametrize("dtype, val", ([object, 1], [object, "a"], [float, 1]))
 def test_object_dtype_isnan(dtype, val):
     X = np.array([[val, np.nan], [np.nan, val]], dtype=dtype)
 
     expected_mask = np.array([[False, True], [True, False]])
 
     mask = _object_dtype_isnan(X)
 
     assert_array_equal(mask, expected_mask)
 
 
-def test_delayed_deprecation():
-    """Check that we issue the FutureWarning regarding the deprecation of delayed."""
-
-    def func(x):
-        return x
-
-    warn_msg = "The function `delayed` has been moved from `sklearn.utils.fixes`"
-    with pytest.warns(FutureWarning, match=warn_msg):
-        delayed(func)
-
-
 @pytest.mark.parametrize(
     "params, expected_dtype",
     [
         ({}, np.int32),  # default behaviour
         ({"maxval": np.iinfo(np.int32).max}, np.int32),
         ({"maxval": np.iinfo(np.int32).max + 1}, np.int64),
     ],
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_graph.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_graph.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_metaestimators.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_metaestimators.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_mocking.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_mocking.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import numpy as np
 import pytest
-from numpy.testing import assert_allclose, assert_array_equal
+from numpy.testing import assert_array_equal
 from scipy import sparse
 
 from sklearn.datasets import load_iris
 from sklearn.utils import _safe_indexing, check_array
 from sklearn.utils._mocking import (
     CheckingClassifier,
     _MockEstimatorOnOffPrediction,
@@ -86,44 +86,39 @@
     clf.fit(X, y)
 
     assert_array_equal(clf.classes_, np.unique(y))
     assert len(clf.classes_) == 3
     assert clf.n_features_in_ == 4
 
     y_pred = clf.predict(X)
-    assert_array_equal(y_pred, np.zeros(y_pred.size, dtype=int))
+    assert all(pred in clf.classes_ for pred in y_pred)
 
     assert clf.score(X) == pytest.approx(0)
     clf.set_params(foo_param=10)
     assert clf.fit(X, y).score(X) == pytest.approx(1)
 
     y_proba = clf.predict_proba(X)
     assert y_proba.shape == (150, 3)
-    assert_allclose(y_proba[:, 0], 1)
-    assert_allclose(y_proba[:, 1:], 0)
+    assert np.logical_and(y_proba >= 0, y_proba <= 1).all()
 
     y_decision = clf.decision_function(X)
     assert y_decision.shape == (150, 3)
-    assert_allclose(y_decision[:, 0], 1)
-    assert_allclose(y_decision[:, 1:], 0)
 
     # check the shape in case of binary classification
     first_2_classes = np.logical_or(y == 0, y == 1)
     X = _safe_indexing(X, first_2_classes)
     y = _safe_indexing(y, first_2_classes)
     clf.fit(X, y)
 
     y_proba = clf.predict_proba(X)
     assert y_proba.shape == (100, 2)
-    assert_allclose(y_proba[:, 0], 1)
-    assert_allclose(y_proba[:, 1], 0)
+    assert np.logical_and(y_proba >= 0, y_proba <= 1).all()
 
     y_decision = clf.decision_function(X)
     assert y_decision.shape == (100,)
-    assert_allclose(y_decision, 0)
 
 
 @pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
 def test_checking_classifier_with_params(iris, csr_container):
     X, y = iris
     X_sparse = csr_container(X)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_multiclass.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_multiclass.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 
 from sklearn import config_context, datasets
 from sklearn.model_selection import ShuffleSplit
 from sklearn.svm import SVC
 from sklearn.utils._array_api import yield_namespace_device_dtype_combinations
 from sklearn.utils._testing import (
     _array_api_for_tests,
+    _convert_container,
     assert_allclose,
     assert_array_almost_equal,
     assert_array_equal,
 )
 from sklearn.utils.estimator_checks import _NotAnArray
 from sklearn.utils.fixes import (
     COO_CONTAINERS,
@@ -591,7 +592,22 @@
         _ovr_decision_function(
             np.array([predictions[i]]), np.array([confidences[i]]), n_classes
         )[0]
         for i in range(4)
     ]
 
     assert_allclose(dec_values, dec_values_one, atol=1e-6)
+
+
+# TODO(1.7): Change to ValueError when byte labels is deprecated.
+@pytest.mark.parametrize("input_type", ["list", "array"])
+def test_labels_in_bytes_format(input_type):
+    # check that we raise an error with bytes encoded labels
+    # non-regression test for:
+    # https://github.com/scikit-learn/scikit-learn/issues/16980
+    target = _convert_container([b"a", b"b"], input_type)
+    err_msg = (
+        "Support for labels represented as bytes is deprecated in v1.5 and will"
+        " error in v1.7. Convert the labels to a string or integer format."
+    )
+    with pytest.warns(FutureWarning, match=err_msg):
+        type_of_target(target)
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_murmurhash.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_murmurhash.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_parallel.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_parallel.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_param_validation.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_param_validation.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,14 +30,15 @@
     _SparseMatrices,
     _VerboseHelper,
     generate_invalid_param_val,
     generate_valid_param,
     make_constraint,
     validate_params,
 )
+from sklearn.utils.fixes import CSR_CONTAINERS
 
 
 # Some helpers for the tests
 @validate_params(
     {"a": [Real], "b": [Real], "c": [Real], "d": [Real]},
     prefer_skip_nested_validation=True,
 )
@@ -401,14 +402,18 @@
         (StrOptions({"a", "b", "c"}), "b"),
         (Options(type, {np.float32, np.float64}), np.float64),
         (callable, lambda x: x + 1),
         (None, None),
         ("array-like", [[1, 2], [3, 4]]),
         ("array-like", np.array([[1, 2], [3, 4]])),
         ("sparse matrix", csr_matrix([[1, 2], [3, 4]])),
+        *[
+            ("sparse matrix", container([[1, 2], [3, 4]]))
+            for container in CSR_CONTAINERS
+        ],
         ("random_state", 0),
         ("random_state", np.random.RandomState(0)),
         ("random_state", None),
         (_Class, _Class()),
         (int, 1),
         (Real, 0.5),
         ("boolean", False),
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_plotting.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_plotting.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_pprint.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_pprint.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_random.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_random.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_response.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_response.py`

 * *Files 16% similar despite different names*

```diff
@@ -236,44 +236,68 @@
     """Check that we raise the proper error messages in _get_response_values_binary."""
 
     estimator.fit(X, y)
     with pytest.raises(ValueError, match=err_msg):
         _get_response_values_binary(estimator, X, **params)
 
 
-def test_get_response_predict_proba():
+@pytest.mark.parametrize("return_response_method_used", [True, False])
+def test_get_response_predict_proba(return_response_method_used):
     """Check the behaviour of `_get_response_values_binary` using `predict_proba`."""
     classifier = DecisionTreeClassifier().fit(X_binary, y_binary)
-    y_proba, pos_label = _get_response_values_binary(
-        classifier, X_binary, response_method="predict_proba"
+    results = _get_response_values_binary(
+        classifier,
+        X_binary,
+        response_method="predict_proba",
+        return_response_method_used=return_response_method_used,
     )
-    assert_allclose(y_proba, classifier.predict_proba(X_binary)[:, 1])
-    assert pos_label == 1
-
-    y_proba, pos_label = _get_response_values_binary(
-        classifier, X_binary, response_method="predict_proba", pos_label=0
+    assert_allclose(results[0], classifier.predict_proba(X_binary)[:, 1])
+    assert results[1] == 1
+    if return_response_method_used:
+        assert results[2] == "predict_proba"
+
+    results = _get_response_values_binary(
+        classifier,
+        X_binary,
+        response_method="predict_proba",
+        pos_label=0,
+        return_response_method_used=return_response_method_used,
     )
-    assert_allclose(y_proba, classifier.predict_proba(X_binary)[:, 0])
-    assert pos_label == 0
+    assert_allclose(results[0], classifier.predict_proba(X_binary)[:, 0])
+    assert results[1] == 0
+    if return_response_method_used:
+        assert results[2] == "predict_proba"
 
 
-def test_get_response_decision_function():
+@pytest.mark.parametrize("return_response_method_used", [True, False])
+def test_get_response_decision_function(return_response_method_used):
     """Check the behaviour of `_get_response_values_binary` using decision_function."""
     classifier = LogisticRegression().fit(X_binary, y_binary)
-    y_score, pos_label = _get_response_values_binary(
-        classifier, X_binary, response_method="decision_function"
+    results = _get_response_values_binary(
+        classifier,
+        X_binary,
+        response_method="decision_function",
+        return_response_method_used=return_response_method_used,
     )
-    assert_allclose(y_score, classifier.decision_function(X_binary))
-    assert pos_label == 1
-
-    y_score, pos_label = _get_response_values_binary(
-        classifier, X_binary, response_method="decision_function", pos_label=0
+    assert_allclose(results[0], classifier.decision_function(X_binary))
+    assert results[1] == 1
+    if return_response_method_used:
+        assert results[2] == "decision_function"
+
+    results = _get_response_values_binary(
+        classifier,
+        X_binary,
+        response_method="decision_function",
+        pos_label=0,
+        return_response_method_used=return_response_method_used,
     )
-    assert_allclose(y_score, classifier.decision_function(X_binary) * -1)
-    assert pos_label == 0
+    assert_allclose(results[0], classifier.decision_function(X_binary) * -1)
+    assert results[1] == 0
+    if return_response_method_used:
+        assert results[2] == "decision_function"
 
 
 @pytest.mark.parametrize(
     "estimator, response_method",
     [
         (DecisionTreeClassifier(max_depth=2, random_state=0), "predict_proba"),
         (DecisionTreeClassifier(max_depth=2, random_state=0), "predict_log_proba"),
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_seq_dataset.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_seq_dataset.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_set_output.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_set_output.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 from numpy.testing import assert_array_equal
 
 from sklearn._config import config_context, get_config
 from sklearn.preprocessing import StandardScaler
 from sklearn.utils._set_output import (
     ADAPTERS_MANAGER,
     ContainerAdapterProtocol,
+    _get_adapter_from_container,
     _get_output_config,
     _safe_set_output,
     _SetOutputMixin,
     _wrap_data_with_container,
     check_library_installed,
 )
 from sklearn.utils.fixes import CSR_CONTAINERS
@@ -446,7 +447,18 @@
         orig_import_module(name, package=None)
 
     monkeypatch.setattr(importlib, "import_module", patched_import_module)
 
     msg = "Setting output container to 'pandas' requires"
     with pytest.raises(ImportError, match=msg):
         check_library_installed("pandas")
+
+
+def test_get_adapter_from_container():
+    """Check the behavior fo `_get_adapter_from_container`."""
+    pd = pytest.importorskip("pandas")
+    X = pd.DataFrame({"a": [1, 2, 3], "b": [10, 20, 100]})
+    adapter = _get_adapter_from_container(X)
+    assert adapter.container_lib == "pandas"
+    err_msg = "The container does not have a registered adapter in scikit-learn."
+    with pytest.raises(ValueError, match=err_msg):
+        _get_adapter_from_container(X.to_numpy())
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_shortest_path.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_shortest_path.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_show_versions.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_show_versions.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,11 @@
+from threadpoolctl import threadpool_info
+
 from sklearn.utils._show_versions import _get_deps_info, _get_sys_info, show_versions
 from sklearn.utils._testing import ignore_warnings
-from sklearn.utils.fixes import threadpool_info
 
 
 def test_get_sys_info():
     sys_info = _get_sys_info()
 
     assert "python" in sys_info
     assert "executable" in sys_info
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_sparsefuncs.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_sparsefuncs.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_stats.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_stats.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_tags.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_tags.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_testing.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_testing.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,14 @@
 
 import numpy as np
 import pytest
 from scipy import sparse
 
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 from sklearn.tree import DecisionTreeClassifier
-from sklearn.utils import _IS_WASM
 from sklearn.utils._testing import (
     TempMemmap,
     _convert_container,
     _delete_folder,
     _get_warnings_filters_info_list,
     assert_allclose,
     assert_allclose_dense_sparse,
@@ -27,14 +26,15 @@
     ignore_warnings,
     raises,
     set_random_state,
     turn_warnings_into_errors,
 )
 from sklearn.utils.deprecation import deprecated
 from sklearn.utils.fixes import (
+    _IS_WASM,
     CSC_CONTAINERS,
     CSR_CONTAINERS,
     parse_version,
     sp_version,
 )
 from sklearn.utils.metaestimators import available_if
 
@@ -680,16 +680,16 @@
     constructor_name,
     container_type,
     dtype,
     superdtype,
 ):
     """Check that we convert the container to the right type of array with the
     right data type."""
-    if constructor_name in ("dataframe", "series", "index"):
-        # delay the import of pandas within the function to only skip this test
+    if constructor_name in ("dataframe", "polars", "series", "polars_series", "index"):
+        # delay the import of pandas/polars within the function to only skip this test
         # instead of the whole file
         container_type = container_type()
     container = [0, 1]
 
     container_converted = _convert_container(
         container,
         constructor_name,
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_utils.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_indexing.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,220 +1,68 @@
-import string
-import timeit
 import warnings
 from copy import copy
-from itertools import chain
 from unittest import SkipTest
 
 import numpy as np
 import pytest
 
-from sklearn import config_context
+import sklearn
 from sklearn.externals._packaging.version import parse as parse_version
-from sklearn.utils import (
-    _approximate_mode,
+from sklearn.utils import _safe_indexing, resample, shuffle
+from sklearn.utils._array_api import yield_namespace_device_dtype_combinations
+from sklearn.utils._indexing import (
     _determine_key_type,
     _get_column_indices,
-    _is_polars_df,
-    _message_with_time,
-    _print_elapsed_time,
     _safe_assign,
-    _safe_indexing,
-    _to_object_array,
-    check_random_state,
-    column_or_1d,
-    deprecated,
-    gen_even_slices,
-    get_chunk_n_rows,
-    is_scalar_nan,
-    resample,
-    safe_mask,
-    shuffle,
 )
 from sklearn.utils._mocking import MockDataFrame
 from sklearn.utils._testing import (
+    _array_api_for_tests,
     _convert_container,
     assert_allclose_dense_sparse,
     assert_array_equal,
-    assert_no_warnings,
+    skip_if_array_api_compat_not_configured,
 )
 from sklearn.utils.fixes import CSC_CONTAINERS, CSR_CONTAINERS
 
 # toy array
 X_toy = np.arange(9).reshape((3, 3))
 
 
-def test_make_rng():
-    # Check the check_random_state utility function behavior
-    assert check_random_state(None) is np.random.mtrand._rand
-    assert check_random_state(np.random) is np.random.mtrand._rand
-
-    rng_42 = np.random.RandomState(42)
-    assert check_random_state(42).randint(100) == rng_42.randint(100)
-
-    rng_42 = np.random.RandomState(42)
-    assert check_random_state(rng_42) is rng_42
-
-    rng_42 = np.random.RandomState(42)
-    assert check_random_state(43).randint(100) != rng_42.randint(100)
-
-    with pytest.raises(ValueError):
-        check_random_state("some invalid seed")
-
-
-def test_deprecated():
-    # Test whether the deprecated decorator issues appropriate warnings
-    # Copied almost verbatim from https://docs.python.org/library/warnings.html
-
-    # First a function...
-    with warnings.catch_warnings(record=True) as w:
-        warnings.simplefilter("always")
-
-        @deprecated()
-        def ham():
-            return "spam"
-
-        spam = ham()
-
-        assert spam == "spam"  # function must remain usable
-
-        assert len(w) == 1
-        assert issubclass(w[0].category, FutureWarning)
-        assert "deprecated" in str(w[0].message).lower()
-
-    # ... then a class.
-    with warnings.catch_warnings(record=True) as w:
-        warnings.simplefilter("always")
-
-        @deprecated("don't use this")
-        class Ham:
-            SPAM = 1
-
-        ham = Ham()
-
-        assert hasattr(ham, "SPAM")
-
-        assert len(w) == 1
-        assert issubclass(w[0].category, FutureWarning)
-        assert "deprecated" in str(w[0].message).lower()
-
-
-def test_resample():
-    # Border case not worth mentioning in doctests
-    assert resample() is None
-
-    # Check that invalid arguments yield ValueError
-    with pytest.raises(ValueError):
-        resample([0], [0, 1])
-    with pytest.raises(ValueError):
-        resample([0, 1], [0, 1], replace=False, n_samples=3)
-
-    # Issue:6581, n_samples can be more when replace is True (default).
-    assert len(resample([1, 2], n_samples=5)) == 5
-
-
-def test_resample_stratified():
-    # Make sure resample can stratify
-    rng = np.random.RandomState(0)
-    n_samples = 100
-    p = 0.9
-    X = rng.normal(size=(n_samples, 1))
-    y = rng.binomial(1, p, size=n_samples)
-
-    _, y_not_stratified = resample(X, y, n_samples=10, random_state=0, stratify=None)
-    assert np.all(y_not_stratified == 1)
-
-    _, y_stratified = resample(X, y, n_samples=10, random_state=0, stratify=y)
-    assert not np.all(y_stratified == 1)
-    assert np.sum(y_stratified) == 9  # all 1s, one 0
-
-
-def test_resample_stratified_replace():
-    # Make sure stratified resampling supports the replace parameter
-    rng = np.random.RandomState(0)
-    n_samples = 100
-    X = rng.normal(size=(n_samples, 1))
-    y = rng.randint(0, 2, size=n_samples)
-
-    X_replace, _ = resample(
-        X, y, replace=True, n_samples=50, random_state=rng, stratify=y
-    )
-    X_no_replace, _ = resample(
-        X, y, replace=False, n_samples=50, random_state=rng, stratify=y
+def test_polars_indexing():
+    """Check _safe_indexing for polars as expected."""
+    pl = pytest.importorskip("polars", minversion="0.18.2")
+    df = pl.DataFrame(
+        {"a": [1, 2, 3, 4], "b": [4, 5, 6, 8], "c": [1, 4, 1, 10]}, orient="row"
     )
-    assert np.unique(X_replace).shape[0] < 50
-    assert np.unique(X_no_replace).shape[0] == 50
 
-    # make sure n_samples can be greater than X.shape[0] if we sample with
-    # replacement
-    X_replace, _ = resample(
-        X, y, replace=True, n_samples=1000, random_state=rng, stratify=y
-    )
-    assert X_replace.shape[0] == 1000
-    assert np.unique(X_replace).shape[0] == 100
+    from polars.testing import assert_frame_equal
 
+    str_keys = [["b"], ["a", "b"], ["b", "a", "c"], ["c"], ["a"]]
 
-def test_resample_stratify_2dy():
-    # Make sure y can be 2d when stratifying
-    rng = np.random.RandomState(0)
-    n_samples = 100
-    X = rng.normal(size=(n_samples, 1))
-    y = rng.randint(0, 2, size=(n_samples, 2))
-    X, y = resample(X, y, n_samples=50, random_state=rng, stratify=y)
-    assert y.ndim == 2
+    for key in str_keys:
+        out = _safe_indexing(df, key, axis=1)
+        assert_frame_equal(df[key], out)
 
+    bool_keys = [([True, False, True], ["a", "c"]), ([False, False, True], ["c"])]
 
-@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
-def test_resample_stratify_sparse_error(csr_container):
-    # resample must be ndarray
-    rng = np.random.RandomState(0)
-    n_samples = 100
-    X = rng.normal(size=(n_samples, 2))
-    y = rng.randint(0, 2, size=n_samples)
-    stratify = csr_container(y.reshape(-1, 1))
-    with pytest.raises(TypeError, match="Sparse data was passed"):
-        X, y = resample(X, y, n_samples=50, random_state=rng, stratify=stratify)
+    for bool_key, str_key in bool_keys:
+        out = _safe_indexing(df, bool_key, axis=1)
+        assert_frame_equal(df[:, str_key], out)
 
+    int_keys = [([0, 1], ["a", "b"]), ([2], ["c"])]
 
-@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
-def test_safe_mask(csr_container):
-    random_state = check_random_state(0)
-    X = random_state.rand(5, 4)
-    X_csr = csr_container(X)
-    mask = [False, False, True, True, True]
-
-    mask = safe_mask(X, mask)
-    assert X[mask].shape[0] == 3
-
-    mask = safe_mask(X_csr, mask)
-    assert X_csr[mask].shape[0] == 3
-
-
-def test_column_or_1d():
-    EXAMPLES = [
-        ("binary", ["spam", "egg", "spam"]),
-        ("binary", [0, 1, 0, 1]),
-        ("continuous", np.arange(10) / 20.0),
-        ("multiclass", [1, 2, 3]),
-        ("multiclass", [0, 1, 2, 2, 0]),
-        ("multiclass", [[1], [2], [3]]),
-        ("multilabel-indicator", [[0, 1, 0], [0, 0, 1]]),
-        ("multiclass-multioutput", [[1, 2, 3]]),
-        ("multiclass-multioutput", [[1, 1], [2, 2], [3, 1]]),
-        ("multiclass-multioutput", [[5, 1], [4, 2], [3, 1]]),
-        ("multiclass-multioutput", [[1, 2, 3]]),
-        ("continuous-multioutput", np.arange(30).reshape((-1, 3))),
-    ]
+    for int_key, str_key in int_keys:
+        out = _safe_indexing(df, int_key, axis=1)
+        assert_frame_equal(df[:, str_key], out)
 
-    for y_type, y in EXAMPLES:
-        if y_type in ["binary", "multiclass", "continuous"]:
-            assert_array_equal(column_or_1d(y), np.ravel(y))
-        else:
-            with pytest.raises(ValueError):
-                column_or_1d(y)
+    axis_0_keys = [[0, 1], [1, 3], [3, 2]]
+    for key in axis_0_keys:
+        out = _safe_indexing(df, key, axis=0)
+        assert_frame_equal(df[key], out)
 
 
 @pytest.mark.parametrize(
     "key, dtype",
     [
         (0, "int"),
         ("0", "str"),
@@ -250,41 +98,68 @@
 
 
 def test_determine_key_type_slice_error():
     with pytest.raises(TypeError, match="Only array-like or scalar are"):
         _determine_key_type(slice(0, 2, 1), accept_slice=False)
 
 
-@pytest.mark.parametrize("array_type", ["list", "array", "sparse", "dataframe"])
+@skip_if_array_api_compat_not_configured
+@pytest.mark.parametrize(
+    "array_namespace, device, dtype_name", yield_namespace_device_dtype_combinations()
+)
+def test_determine_key_type_array_api(array_namespace, device, dtype_name):
+    xp = _array_api_for_tests(array_namespace, device)
+
+    with sklearn.config_context(array_api_dispatch=True):
+        int_array_key = xp.asarray([1, 2, 3])
+        assert _determine_key_type(int_array_key) == "int"
+
+        bool_array_key = xp.asarray([True, False, True])
+        assert _determine_key_type(bool_array_key) == "bool"
+
+        try:
+            complex_array_key = xp.asarray([1 + 1j, 2 + 2j, 3 + 3j])
+        except TypeError:
+            # Complex numbers are not supported by all Array API libraries.
+            complex_array_key = None
+
+        if complex_array_key is not None:
+            with pytest.raises(ValueError, match="No valid specification of the"):
+                _determine_key_type(complex_array_key)
+
+
+@pytest.mark.parametrize(
+    "array_type", ["list", "array", "sparse", "dataframe", "polars"]
+)
 @pytest.mark.parametrize("indices_type", ["list", "tuple", "array", "series", "slice"])
 def test_safe_indexing_2d_container_axis_0(array_type, indices_type):
     indices = [1, 2]
     if indices_type == "slice" and isinstance(indices[1], int):
         indices[1] += 1
     array = _convert_container([[1, 2, 3], [4, 5, 6], [7, 8, 9]], array_type)
     indices = _convert_container(indices, indices_type)
     subset = _safe_indexing(array, indices, axis=0)
     assert_allclose_dense_sparse(
         subset, _convert_container([[4, 5, 6], [7, 8, 9]], array_type)
     )
 
 
-@pytest.mark.parametrize("array_type", ["list", "array", "series"])
+@pytest.mark.parametrize("array_type", ["list", "array", "series", "polars_series"])
 @pytest.mark.parametrize("indices_type", ["list", "tuple", "array", "series", "slice"])
 def test_safe_indexing_1d_container(array_type, indices_type):
     indices = [1, 2]
     if indices_type == "slice" and isinstance(indices[1], int):
         indices[1] += 1
     array = _convert_container([1, 2, 3, 4, 5, 6, 7, 8, 9], array_type)
     indices = _convert_container(indices, indices_type)
     subset = _safe_indexing(array, indices, axis=0)
     assert_allclose_dense_sparse(subset, _convert_container([2, 3], array_type))
 
 
-@pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe"])
+@pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe", "polars"])
 @pytest.mark.parametrize("indices_type", ["list", "tuple", "array", "series", "slice"])
 @pytest.mark.parametrize("indices", [[1, 2], ["col_1", "col_2"]])
 def test_safe_indexing_2d_container_axis_1(array_type, indices_type, indices):
     # validation of the indices
     # we make a copy because indices is mutable and shared between tests
     indices_converted = copy(indices)
     if indices_type == "slice" and isinstance(indices[1], int):
@@ -292,30 +167,30 @@
 
     columns_name = ["col_0", "col_1", "col_2"]
     array = _convert_container(
         [[1, 2, 3], [4, 5, 6], [7, 8, 9]], array_type, columns_name
     )
     indices_converted = _convert_container(indices_converted, indices_type)
 
-    if isinstance(indices[0], str) and array_type != "dataframe":
+    if isinstance(indices[0], str) and array_type not in ("dataframe", "polars"):
         err_msg = (
             "Specifying the columns using strings is only supported for dataframes"
         )
         with pytest.raises(ValueError, match=err_msg):
             _safe_indexing(array, indices_converted, axis=1)
     else:
         subset = _safe_indexing(array, indices_converted, axis=1)
         assert_allclose_dense_sparse(
             subset, _convert_container([[2, 3], [5, 6], [8, 9]], array_type)
         )
 
 
 @pytest.mark.parametrize("array_read_only", [True, False])
 @pytest.mark.parametrize("indices_read_only", [True, False])
-@pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe"])
+@pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe", "polars"])
 @pytest.mark.parametrize("indices_type", ["array", "series"])
 @pytest.mark.parametrize(
     "axis, expected_array", [(0, [[4, 5, 6], [7, 8, 9]]), (1, [[2, 3], [5, 6], [8, 9]])]
 )
 def test_safe_indexing_2d_read_only_axis_1(
     array_read_only, indices_read_only, array_type, indices_type, axis, expected_array
 ):
@@ -327,25 +202,25 @@
     if indices_read_only:
         indices.setflags(write=False)
     indices = _convert_container(indices, indices_type)
     subset = _safe_indexing(array, indices, axis=axis)
     assert_allclose_dense_sparse(subset, _convert_container(expected_array, array_type))
 
 
-@pytest.mark.parametrize("array_type", ["list", "array", "series"])
+@pytest.mark.parametrize("array_type", ["list", "array", "series", "polars_series"])
 @pytest.mark.parametrize("indices_type", ["list", "tuple", "array", "series"])
 def test_safe_indexing_1d_container_mask(array_type, indices_type):
     indices = [False] + [True] * 2 + [False] * 6
     array = _convert_container([1, 2, 3, 4, 5, 6, 7, 8, 9], array_type)
     indices = _convert_container(indices, indices_type)
     subset = _safe_indexing(array, indices, axis=0)
     assert_allclose_dense_sparse(subset, _convert_container([2, 3], array_type))
 
 
-@pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe"])
+@pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe", "polars"])
 @pytest.mark.parametrize("indices_type", ["list", "tuple", "array", "series"])
 @pytest.mark.parametrize(
     "axis, expected_subset",
     [(0, [[4, 5, 6], [7, 8, 9]]), (1, [[2, 3], [5, 6], [8, 9]])],
 )
 def test_safe_indexing_2d_mask(array_type, indices_type, axis, expected_subset):
     columns_name = ["col_0", "col_1", "col_2"]
@@ -364,44 +239,50 @@
 @pytest.mark.parametrize(
     "array_type, expected_output_type",
     [
         ("list", "list"),
         ("array", "array"),
         ("sparse", "sparse"),
         ("dataframe", "series"),
+        ("polars", "polars_series"),
     ],
 )
 def test_safe_indexing_2d_scalar_axis_0(array_type, expected_output_type):
     array = _convert_container([[1, 2, 3], [4, 5, 6], [7, 8, 9]], array_type)
     indices = 2
     subset = _safe_indexing(array, indices, axis=0)
     expected_array = _convert_container([7, 8, 9], expected_output_type)
     assert_allclose_dense_sparse(subset, expected_array)
 
 
-@pytest.mark.parametrize("array_type", ["list", "array", "series"])
+@pytest.mark.parametrize("array_type", ["list", "array", "series", "polars_series"])
 def test_safe_indexing_1d_scalar(array_type):
     array = _convert_container([1, 2, 3, 4, 5, 6, 7, 8, 9], array_type)
     indices = 2
     subset = _safe_indexing(array, indices, axis=0)
     assert subset == 3
 
 
 @pytest.mark.parametrize(
     "array_type, expected_output_type",
-    [("array", "array"), ("sparse", "sparse"), ("dataframe", "series")],
+    [
+        ("array", "array"),
+        ("sparse", "sparse"),
+        ("dataframe", "series"),
+        ("polars", "polars_series"),
+    ],
 )
 @pytest.mark.parametrize("indices", [2, "col_2"])
 def test_safe_indexing_2d_scalar_axis_1(array_type, expected_output_type, indices):
     columns_name = ["col_0", "col_1", "col_2"]
     array = _convert_container(
         [[1, 2, 3], [4, 5, 6], [7, 8, 9]], array_type, columns_name
     )
 
-    if isinstance(indices, str) and array_type != "dataframe":
+    if isinstance(indices, str) and array_type not in ("dataframe", "polars"):
         err_msg = (
             "Specifying the columns using strings is only supported for dataframes"
         )
         with pytest.raises(ValueError, match=err_msg):
             _safe_indexing(array, indices, axis=1)
     else:
         subset = _safe_indexing(array, indices, axis=1)
@@ -430,26 +311,29 @@
 
 @pytest.mark.parametrize("axis", [None, 3])
 def test_safe_indexing_error_axis(axis):
     with pytest.raises(ValueError, match="'axis' should be either 0"):
         _safe_indexing(X_toy, [0, 1], axis=axis)
 
 
-@pytest.mark.parametrize("X_constructor", ["array", "series"])
+@pytest.mark.parametrize("X_constructor", ["array", "series", "polars_series"])
 def test_safe_indexing_1d_array_error(X_constructor):
     # check that we are raising an error if the array-like passed is 1D and
     # we try to index on the 2nd dimension
     X = list(range(5))
     if X_constructor == "array":
         X_constructor = np.asarray(X)
     elif X_constructor == "series":
         pd = pytest.importorskip("pandas")
         X_constructor = pd.Series(X)
+    elif X_constructor == "polars_series":
+        pl = pytest.importorskip("polars")
+        X_constructor = pl.Series(values=X)
 
-    err_msg = "'X' should be a 2D NumPy array, 2D sparse matrix or pandas"
+    err_msg = "'X' should be a 2D NumPy array, 2D sparse matrix or dataframe"
     with pytest.raises(ValueError, match=err_msg):
         _safe_indexing(X_constructor, [0, 1], axis=1)
 
 
 def test_safe_indexing_container_axis_0_unsupported_type():
     indices = ["col_1", "col_2"]
     array = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
@@ -488,270 +372,14 @@
     """Check that we raise a ValueError when axis=1 with input as list."""
     X = [[1, 2], [4, 5], [7, 8]]
     err_msg = "axis=1 is not supported for lists"
     with pytest.raises(ValueError, match=err_msg):
         _safe_indexing(X, indices, axis=1)
 
 
-@pytest.mark.parametrize(
-    "key, err_msg",
-    [
-        (10, r"all features must be in \[0, 2\]"),
-        ("whatever", "A given column is not a column of the dataframe"),
-        (object(), "No valid specification of the columns"),
-    ],
-)
-def test_get_column_indices_error(key, err_msg):
-    pd = pytest.importorskip("pandas")
-    X_df = pd.DataFrame(X_toy, columns=["col_0", "col_1", "col_2"])
-
-    with pytest.raises(ValueError, match=err_msg):
-        _get_column_indices(X_df, key)
-
-
-@pytest.mark.parametrize(
-    "key", [["col1"], ["col2"], ["col1", "col2"], ["col1", "col3"], ["col2", "col3"]]
-)
-def test_get_column_indices_pandas_nonunique_columns_error(key):
-    pd = pytest.importorskip("pandas")
-    toy = np.zeros((1, 5), dtype=int)
-    columns = ["col1", "col1", "col2", "col3", "col2"]
-    X = pd.DataFrame(toy, columns=columns)
-
-    err_msg = "Selected columns, {}, are not unique in dataframe".format(key)
-    with pytest.raises(ValueError) as exc_info:
-        _get_column_indices(X, key)
-    assert str(exc_info.value) == err_msg
-
-
-def test_shuffle_on_ndim_equals_three():
-    def to_tuple(A):  # to make the inner arrays hashable
-        return tuple(tuple(tuple(C) for C in B) for B in A)
-
-    A = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # A.shape = (2,2,2)
-    S = set(to_tuple(A))
-    shuffle(A)  # shouldn't raise a ValueError for dim = 3
-    assert set(to_tuple(A)) == S
-
-
-@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
-def test_shuffle_dont_convert_to_array(csc_container):
-    # Check that shuffle does not try to convert to numpy arrays with float
-    # dtypes can let any indexable datastructure pass-through.
-    a = ["a", "b", "c"]
-    b = np.array(["a", "b", "c"], dtype=object)
-    c = [1, 2, 3]
-    d = MockDataFrame(np.array([["a", 0], ["b", 1], ["c", 2]], dtype=object))
-    e = csc_container(np.arange(6).reshape(3, 2))
-    a_s, b_s, c_s, d_s, e_s = shuffle(a, b, c, d, e, random_state=0)
-
-    assert a_s == ["c", "b", "a"]
-    assert type(a_s) == list  # noqa: E721
-
-    assert_array_equal(b_s, ["c", "b", "a"])
-    assert b_s.dtype == object
-
-    assert c_s == [3, 2, 1]
-    assert type(c_s) == list  # noqa: E721
-
-    assert_array_equal(d_s, np.array([["c", 2], ["b", 1], ["a", 0]], dtype=object))
-    assert type(d_s) == MockDataFrame  # noqa: E721
-
-    assert_array_equal(e_s.toarray(), np.array([[4, 5], [2, 3], [0, 1]]))
-
-
-def test_gen_even_slices():
-    # check that gen_even_slices contains all samples
-    some_range = range(10)
-    joined_range = list(chain(*[some_range[slice] for slice in gen_even_slices(10, 3)]))
-    assert_array_equal(some_range, joined_range)
-
-
-@pytest.mark.parametrize(
-    ("row_bytes", "max_n_rows", "working_memory", "expected"),
-    [
-        (1024, None, 1, 1024),
-        (1024, None, 0.99999999, 1023),
-        (1023, None, 1, 1025),
-        (1025, None, 1, 1023),
-        (1024, None, 2, 2048),
-        (1024, 7, 1, 7),
-        (1024 * 1024, None, 1, 1),
-    ],
-)
-def test_get_chunk_n_rows(row_bytes, max_n_rows, working_memory, expected):
-    with warnings.catch_warnings():
-        warnings.simplefilter("error", UserWarning)
-        actual = get_chunk_n_rows(
-            row_bytes=row_bytes,
-            max_n_rows=max_n_rows,
-            working_memory=working_memory,
-        )
-
-    assert actual == expected
-    assert type(actual) is type(expected)
-    with config_context(working_memory=working_memory):
-        with warnings.catch_warnings():
-            warnings.simplefilter("error", UserWarning)
-            actual = get_chunk_n_rows(row_bytes=row_bytes, max_n_rows=max_n_rows)
-        assert actual == expected
-        assert type(actual) is type(expected)
-
-
-def test_get_chunk_n_rows_warns():
-    """Check that warning is raised when working_memory is too low."""
-    row_bytes = 1024 * 1024 + 1
-    max_n_rows = None
-    working_memory = 1
-    expected = 1
-
-    warn_msg = (
-        "Could not adhere to working_memory config. Currently 1MiB, 2MiB required."
-    )
-    with pytest.warns(UserWarning, match=warn_msg):
-        actual = get_chunk_n_rows(
-            row_bytes=row_bytes,
-            max_n_rows=max_n_rows,
-            working_memory=working_memory,
-        )
-
-    assert actual == expected
-    assert type(actual) is type(expected)
-
-    with config_context(working_memory=working_memory):
-        with pytest.warns(UserWarning, match=warn_msg):
-            actual = get_chunk_n_rows(row_bytes=row_bytes, max_n_rows=max_n_rows)
-        assert actual == expected
-        assert type(actual) is type(expected)
-
-
-@pytest.mark.parametrize(
-    ["source", "message", "is_long"],
-    [
-        ("ABC", string.ascii_lowercase, False),
-        ("ABCDEF", string.ascii_lowercase, False),
-        ("ABC", string.ascii_lowercase * 3, True),
-        ("ABC" * 10, string.ascii_lowercase, True),
-        ("ABC", string.ascii_lowercase + "\u1048", False),
-    ],
-)
-@pytest.mark.parametrize(
-    ["time", "time_str"],
-    [
-        (0.2, "   0.2s"),
-        (20, "  20.0s"),
-        (2000, "33.3min"),
-        (20000, "333.3min"),
-    ],
-)
-def test_message_with_time(source, message, is_long, time, time_str):
-    out = _message_with_time(source, message, time)
-    if is_long:
-        assert len(out) > 70
-    else:
-        assert len(out) == 70
-
-    assert out.startswith("[" + source + "] ")
-    out = out[len(source) + 3 :]
-
-    assert out.endswith(time_str)
-    out = out[: -len(time_str)]
-    assert out.endswith(", total=")
-    out = out[: -len(", total=")]
-    assert out.endswith(message)
-    out = out[: -len(message)]
-    assert out.endswith(" ")
-    out = out[:-1]
-
-    if is_long:
-        assert not out
-    else:
-        assert list(set(out)) == ["."]
-
-
-@pytest.mark.parametrize(
-    ["message", "expected"],
-    [
-        ("hello", _message_with_time("ABC", "hello", 0.1) + "\n"),
-        ("", _message_with_time("ABC", "", 0.1) + "\n"),
-        (None, ""),
-    ],
-)
-def test_print_elapsed_time(message, expected, capsys, monkeypatch):
-    monkeypatch.setattr(timeit, "default_timer", lambda: 0)
-    with _print_elapsed_time("ABC", message):
-        monkeypatch.setattr(timeit, "default_timer", lambda: 0.1)
-    assert capsys.readouterr().out == expected
-
-
-@pytest.mark.parametrize(
-    "value, result",
-    [
-        (float("nan"), True),
-        (np.nan, True),
-        (float(np.nan), True),
-        (np.float32(np.nan), True),
-        (np.float64(np.nan), True),
-        (0, False),
-        (0.0, False),
-        (None, False),
-        ("", False),
-        ("nan", False),
-        ([np.nan], False),
-        (9867966753463435747313673, False),  # Python int that overflows with C type
-    ],
-)
-def test_is_scalar_nan(value, result):
-    assert is_scalar_nan(value) is result
-    # make sure that we are returning a Python bool
-    assert isinstance(is_scalar_nan(value), bool)
-
-
-def test_approximate_mode():
-    """Make sure sklearn.utils._approximate_mode returns valid
-    results for cases where "class_counts * n_draws" is enough
-    to overflow 32-bit signed integer.
-
-    Non-regression test for:
-    https://github.com/scikit-learn/scikit-learn/issues/20774
-    """
-    X = np.array([99000, 1000], dtype=np.int32)
-    ret = _approximate_mode(class_counts=X, n_draws=25000, rng=0)
-
-    # Draws 25% of the total population, so in this case a fair draw means:
-    # 25% * 99.000 = 24.750
-    # 25% *  1.000 =    250
-    assert_array_equal(ret, [24750, 250])
-
-
-def dummy_func():
-    pass
-
-
-def test_deprecation_joblib_api(tmpdir):
-    # Only parallel_backend and register_parallel_backend are not deprecated in
-    # sklearn.utils
-    from sklearn.utils import parallel_backend, register_parallel_backend
-
-    assert_no_warnings(parallel_backend, "loky", None)
-    assert_no_warnings(register_parallel_backend, "failing", None)
-
-    from sklearn.utils._joblib import joblib
-
-    del joblib.parallel.BACKENDS["failing"]
-
-
-@pytest.mark.parametrize("sequence", [[np.array(1), np.array(2)], [[1, 2], [3, 4]]])
-def test_to_object_array(sequence):
-    out = _to_object_array(sequence)
-    assert isinstance(out, np.ndarray)
-    assert out.dtype.kind == "O"
-    assert out.ndim == 1
-
-
 @pytest.mark.parametrize("array_type", ["array", "sparse", "dataframe"])
 def test_safe_assign(array_type):
     """Check that `_safe_assign` works as expected."""
     rng = np.random.RandomState(0)
     X_array = rng.randn(10, 5)
 
     row_indexer = [1, 2]
@@ -778,14 +406,45 @@
     values = rng.randn(*X.shape)
     X = _convert_container(X_array, array_type)
     _safe_assign(X, values, column_indexer=column_indexer)
 
     assert_allclose_dense_sparse(X, _convert_container(values, array_type))
 
 
+@pytest.mark.parametrize(
+    "key, err_msg",
+    [
+        (10, r"all features must be in \[0, 2\]"),
+        ("whatever", "A given column is not a column of the dataframe"),
+        (object(), "No valid specification of the columns"),
+    ],
+)
+def test_get_column_indices_error(key, err_msg):
+    pd = pytest.importorskip("pandas")
+    X_df = pd.DataFrame(X_toy, columns=["col_0", "col_1", "col_2"])
+
+    with pytest.raises(ValueError, match=err_msg):
+        _get_column_indices(X_df, key)
+
+
+@pytest.mark.parametrize(
+    "key", [["col1"], ["col2"], ["col1", "col2"], ["col1", "col3"], ["col2", "col3"]]
+)
+def test_get_column_indices_pandas_nonunique_columns_error(key):
+    pd = pytest.importorskip("pandas")
+    toy = np.zeros((1, 5), dtype=int)
+    columns = ["col1", "col1", "col2", "col3", "col2"]
+    X = pd.DataFrame(toy, columns=columns)
+
+    err_msg = "Selected columns, {}, are not unique in dataframe".format(key)
+    with pytest.raises(ValueError) as exc_info:
+        _get_column_indices(X, key)
+    assert str(exc_info.value) == err_msg
+
+
 def test_get_column_indices_interchange():
     """Check _get_column_indices for edge cases with the interchange"""
     pd = pytest.importorskip("pandas", minversion="1.5")
 
     df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=["a", "b", "c"])
 
     # Hide the fact that this is a pandas dataframe to trigger the dataframe protocol
@@ -818,49 +477,118 @@
         _get_column_indices(df_mocked, ["not_a_column"])
 
     msg = "key.step must be 1 or None"
     with pytest.raises(NotImplementedError, match=msg):
         _get_column_indices(df_mocked, slice("a", None, 2))
 
 
-def test_polars_indexing():
-    """Check _safe_indexing for polars as expected."""
-    pl = pytest.importorskip("polars", minversion="0.18.2")
-    df = pl.DataFrame(
-        {"a": [1, 2, 3, 4], "b": [4, 5, 6, 8], "c": [1, 4, 1, 10]}, orient="row"
+def test_resample():
+    # Border case not worth mentioning in doctests
+    assert resample() is None
+
+    # Check that invalid arguments yield ValueError
+    with pytest.raises(ValueError):
+        resample([0], [0, 1])
+    with pytest.raises(ValueError):
+        resample([0, 1], [0, 1], replace=False, n_samples=3)
+
+    # Issue:6581, n_samples can be more when replace is True (default).
+    assert len(resample([1, 2], n_samples=5)) == 5
+
+
+def test_resample_stratified():
+    # Make sure resample can stratify
+    rng = np.random.RandomState(0)
+    n_samples = 100
+    p = 0.9
+    X = rng.normal(size=(n_samples, 1))
+    y = rng.binomial(1, p, size=n_samples)
+
+    _, y_not_stratified = resample(X, y, n_samples=10, random_state=0, stratify=None)
+    assert np.all(y_not_stratified == 1)
+
+    _, y_stratified = resample(X, y, n_samples=10, random_state=0, stratify=y)
+    assert not np.all(y_stratified == 1)
+    assert np.sum(y_stratified) == 9  # all 1s, one 0
+
+
+def test_resample_stratified_replace():
+    # Make sure stratified resampling supports the replace parameter
+    rng = np.random.RandomState(0)
+    n_samples = 100
+    X = rng.normal(size=(n_samples, 1))
+    y = rng.randint(0, 2, size=n_samples)
+
+    X_replace, _ = resample(
+        X, y, replace=True, n_samples=50, random_state=rng, stratify=y
+    )
+    X_no_replace, _ = resample(
+        X, y, replace=False, n_samples=50, random_state=rng, stratify=y
     )
+    assert np.unique(X_replace).shape[0] < 50
+    assert np.unique(X_no_replace).shape[0] == 50
 
-    from polars.testing import assert_frame_equal
+    # make sure n_samples can be greater than X.shape[0] if we sample with
+    # replacement
+    X_replace, _ = resample(
+        X, y, replace=True, n_samples=1000, random_state=rng, stratify=y
+    )
+    assert X_replace.shape[0] == 1000
+    assert np.unique(X_replace).shape[0] == 100
 
-    str_keys = [["b"], ["a", "b"], ["b", "a", "c"], ["c"], ["a"]]
 
-    for key in str_keys:
-        out = _safe_indexing(df, key, axis=1)
-        assert_frame_equal(df[key], out)
+def test_resample_stratify_2dy():
+    # Make sure y can be 2d when stratifying
+    rng = np.random.RandomState(0)
+    n_samples = 100
+    X = rng.normal(size=(n_samples, 1))
+    y = rng.randint(0, 2, size=(n_samples, 2))
+    X, y = resample(X, y, n_samples=50, random_state=rng, stratify=y)
+    assert y.ndim == 2
 
-    bool_keys = [([True, False, True], ["a", "c"]), ([False, False, True], ["c"])]
 
-    for bool_key, str_key in bool_keys:
-        out = _safe_indexing(df, bool_key, axis=1)
-        assert_frame_equal(df[:, str_key], out)
+@pytest.mark.parametrize("csr_container", CSR_CONTAINERS)
+def test_resample_stratify_sparse_error(csr_container):
+    # resample must be ndarray
+    rng = np.random.RandomState(0)
+    n_samples = 100
+    X = rng.normal(size=(n_samples, 2))
+    y = rng.randint(0, 2, size=n_samples)
+    stratify = csr_container(y.reshape(-1, 1))
+    with pytest.raises(TypeError, match="Sparse data was passed"):
+        X, y = resample(X, y, n_samples=50, random_state=rng, stratify=stratify)
 
-    int_keys = [([0, 1], ["a", "b"]), ([2], ["c"])]
 
-    for int_key, str_key in int_keys:
-        out = _safe_indexing(df, int_key, axis=1)
-        assert_frame_equal(df[:, str_key], out)
+def test_shuffle_on_ndim_equals_three():
+    def to_tuple(A):  # to make the inner arrays hashable
+        return tuple(tuple(tuple(C) for C in B) for B in A)
 
-    axis_0_keys = [[0, 1], [1, 3], [3, 2]]
-    for key in axis_0_keys:
-        out = _safe_indexing(df, key, axis=0)
-        assert_frame_equal(df[key], out)
+    A = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # A.shape = (2,2,2)
+    S = set(to_tuple(A))
+    shuffle(A)  # shouldn't raise a ValueError for dim = 3
+    assert set(to_tuple(A)) == S
 
 
-def test__is_polars_df():
-    """Check that _is_polars_df return False for non-dataframe objects."""
+@pytest.mark.parametrize("csc_container", CSC_CONTAINERS)
+def test_shuffle_dont_convert_to_array(csc_container):
+    # Check that shuffle does not try to convert to numpy arrays with float
+    # dtypes can let any indexable datastructure pass-through.
+    a = ["a", "b", "c"]
+    b = np.array(["a", "b", "c"], dtype=object)
+    c = [1, 2, 3]
+    d = MockDataFrame(np.array([["a", 0], ["b", 1], ["c", 2]], dtype=object))
+    e = csc_container(np.arange(6).reshape(3, 2))
+    a_s, b_s, c_s, d_s, e_s = shuffle(a, b, c, d, e, random_state=0)
 
-    class LooksLikePolars:
-        def __init__(self):
-            self.columns = ["a", "b"]
-            self.schema = ["a", "b"]
+    assert a_s == ["c", "b", "a"]
+    assert type(a_s) == list  # noqa: E721
+
+    assert_array_equal(b_s, ["c", "b", "a"])
+    assert b_s.dtype == object
+
+    assert c_s == [3, 2, 1]
+    assert type(c_s) == list  # noqa: E721
+
+    assert_array_equal(d_s, np.array([["c", 2], ["b", 1], ["a", 0]], dtype=object))
+    assert type(d_s) == MockDataFrame  # noqa: E721
 
-    assert not _is_polars_df(LooksLikePolars())
+    assert_array_equal(e_s.toarray(), np.array([[4, 5], [2, 3], [0, 1]]))
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_validation.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_validation.py`

 * *Files 1% similar despite different names*

```diff
@@ -70,24 +70,45 @@
     _deprecate_positional_args,
     _get_feature_names,
     _is_fitted,
     _is_pandas_df,
     _is_polars_df,
     _num_features,
     _num_samples,
+    _to_object_array,
     assert_all_finite,
     check_consistent_length,
     check_is_fitted,
     check_memory,
     check_non_negative,
+    check_random_state,
     check_scalar,
+    column_or_1d,
     has_fit_parameter,
 )
 
 
+def test_make_rng():
+    # Check the check_random_state utility function behavior
+    assert check_random_state(None) is np.random.mtrand._rand
+    assert check_random_state(np.random) is np.random.mtrand._rand
+
+    rng_42 = np.random.RandomState(42)
+    assert check_random_state(42).randint(100) == rng_42.randint(100)
+
+    rng_42 = np.random.RandomState(42)
+    assert check_random_state(rng_42) is rng_42
+
+    rng_42 = np.random.RandomState(42)
+    assert check_random_state(43).randint(100) != rng_42.randint(100)
+
+    with pytest.raises(ValueError):
+        check_random_state("some invalid seed")
+
+
 def test_as_float_array():
     # Test function for as_float_array
     X = np.ones((3, 10), dtype=np.int32)
     X = X + np.arange(10, dtype=np.int32)
     X2 = as_float_array(X, copy=False)
     assert X2.dtype == np.float32
     # Another test
@@ -1968,15 +1989,15 @@
         force_all_finite=False,
     )
     assert np.issubdtype(result.dtype.kind, np.floating)
     assert_allclose(result, input_values)
 
 
 @skip_if_array_api_compat_not_configured
-@pytest.mark.parametrize("array_namespace", ["numpy.array_api", "cupy.array_api"])
+@pytest.mark.parametrize("array_namespace", ["array_api_strict", "cupy.array_api"])
 def test_check_array_array_api_has_non_finite(array_namespace):
     """Checks that Array API arrays checks non-finite correctly."""
     xp = pytest.importorskip(array_namespace)
 
     X_nan = xp.asarray([[xp.nan, 1, 0], [0, xp.nan, 3]], dtype=xp.float32)
     with config_context(array_api_dispatch=True):
         with pytest.raises(ValueError, match="Input contains NaN."):
@@ -2048,7 +2069,50 @@
     X_checked = check_array(X, accept_sparse=output_format)
     if output_format == "coo":
         assert X_checked.row.dtype == np.int32
         assert X_checked.col.dtype == np.int32
     else:  # output_format in ["csr", "csc"]
         assert X_checked.indices.dtype == np.int32
         assert X_checked.indptr.dtype == np.int32
+
+
+@pytest.mark.parametrize("sequence", [[np.array(1), np.array(2)], [[1, 2], [3, 4]]])
+def test_to_object_array(sequence):
+    out = _to_object_array(sequence)
+    assert isinstance(out, np.ndarray)
+    assert out.dtype.kind == "O"
+    assert out.ndim == 1
+
+
+def test_column_or_1d():
+    EXAMPLES = [
+        ("binary", ["spam", "egg", "spam"]),
+        ("binary", [0, 1, 0, 1]),
+        ("continuous", np.arange(10) / 20.0),
+        ("multiclass", [1, 2, 3]),
+        ("multiclass", [0, 1, 2, 2, 0]),
+        ("multiclass", [[1], [2], [3]]),
+        ("multilabel-indicator", [[0, 1, 0], [0, 0, 1]]),
+        ("multiclass-multioutput", [[1, 2, 3]]),
+        ("multiclass-multioutput", [[1, 1], [2, 2], [3, 1]]),
+        ("multiclass-multioutput", [[5, 1], [4, 2], [3, 1]]),
+        ("multiclass-multioutput", [[1, 2, 3]]),
+        ("continuous-multioutput", np.arange(30).reshape((-1, 3))),
+    ]
+
+    for y_type, y in EXAMPLES:
+        if y_type in ["binary", "multiclass", "continuous"]:
+            assert_array_equal(column_or_1d(y), np.ravel(y))
+        else:
+            with pytest.raises(ValueError):
+                column_or_1d(y)
+
+
+def test__is_polars_df():
+    """Check that _is_polars_df return False for non-dataframe objects."""
+
+    class LooksLikePolars:
+        def __init__(self):
+            self.columns = ["a", "b"]
+            self.schema = ["a", "b"]
+
+    assert not _is_polars_df(LooksLikePolars())
```

### Comparing `scikit-learn-1.4.2/sklearn/utils/tests/test_weight_vector.py` & `scikit_learn-1.5.0rc1/sklearn/utils/tests/test_weight_vector.py`

 * *Files identical despite different names*

### Comparing `scikit-learn-1.4.2/sklearn/utils/validation.py` & `scikit_learn-1.5.0rc1/sklearn/utils/validation.py`

 * *Files 2% similar despite different names*

```diff
@@ -286,14 +286,17 @@
         else:
             return_dtype = np.float64
         return X.astype(return_dtype)
 
 
 def _is_arraylike(x):
     """Returns whether the input is array-like."""
+    if sp.issparse(x):
+        return False
+
     return hasattr(x, "__len__") or hasattr(x, "shape") or hasattr(x, "__array__")
 
 
 def _is_arraylike_not_scalar(array):
     """Return True if array is array-like and not a scalar"""
     return _is_arraylike(array) and not np.isscalar(array)
 
@@ -1296,15 +1299,15 @@
             estimator=estimator,
         )
     else:
         estimator_name = _check_estimator_name(estimator)
         y = column_or_1d(y, warn=True)
         _assert_all_finite(y, input_name="y", estimator_name=estimator_name)
         _ensure_no_complex_data(y)
-    if y_numeric and y.dtype.kind == "O":
+    if y_numeric and hasattr(y.dtype, "kind") and y.dtype.kind == "O":
         y = y.astype(np.float64)
 
     return y
 
 
 def column_or_1d(y, *, dtype=None, warn=False):
     """Ravel column or 1d numpy array, else raises an error.
@@ -2131,16 +2134,18 @@
     method_params_validated : dict
         Validated parameters. We ensure that the values support indexing.
     """
     from . import _safe_indexing
 
     method_params_validated = {}
     for param_key, param_value in params.items():
-        if not _is_arraylike(param_value) or _num_samples(param_value) != _num_samples(
-            X
+        if (
+            not _is_arraylike(param_value)
+            and not sp.issparse(param_value)
+            or _num_samples(param_value) != _num_samples(X)
         ):
             # Non-indexable pass-through (for now for backward-compatibility).
             # https://github.com/scikit-learn/scikit-learn/issues/15805
             method_params_validated[param_key] = param_value
         else:
             # Any other method_params should support indexing
             # (e.g. for cross-validation).
@@ -2166,14 +2171,23 @@
     try:
         pd = sys.modules["pandas"]
     except KeyError:
         return False
     return isinstance(X, pd.DataFrame)
 
 
+def _is_polars_df_or_series(X):
+    """Return True if the X is a polars dataframe or series."""
+    try:
+        pl = sys.modules["polars"]
+    except KeyError:
+        return False
+    return isinstance(X, (pl.DataFrame, pl.Series))
+
+
 def _is_polars_df(X):
     """Return True if the X is a polars dataframe."""
     try:
         pl = sys.modules["polars"]
     except KeyError:
         return False
     return isinstance(X, pl.DataFrame)
@@ -2435,28 +2449,62 @@
         In the case that `y_true` does not have label in {-1, 1} or {0, 1},
         it will raise a `ValueError`.
     """
     # ensure binary classification if pos_label is not specified
     # classes.dtype.kind in ('O', 'U', 'S') is required to avoid
     # triggering a FutureWarning by calling np.array_equal(a, b)
     # when elements in the two arrays are not comparable.
-    classes = np.unique(y_true)
-    if pos_label is None and (
-        classes.dtype.kind in "OUS"
-        or not (
+    if pos_label is None:
+        # Compute classes only if pos_label is not specified:
+        classes = np.unique(y_true)
+        if classes.dtype.kind in "OUS" or not (
             np.array_equal(classes, [0, 1])
             or np.array_equal(classes, [-1, 1])
             or np.array_equal(classes, [0])
             or np.array_equal(classes, [-1])
             or np.array_equal(classes, [1])
-        )
-    ):
-        classes_repr = ", ".join([repr(c) for c in classes.tolist()])
-        raise ValueError(
-            f"y_true takes value in {{{classes_repr}}} and pos_label is not "
-            "specified: either make y_true take value in {0, 1} or "
-            "{-1, 1} or pass pos_label explicitly."
-        )
-    elif pos_label is None:
+        ):
+            classes_repr = ", ".join([repr(c) for c in classes.tolist()])
+            raise ValueError(
+                f"y_true takes value in {{{classes_repr}}} and pos_label is not "
+                "specified: either make y_true take value in {0, 1} or "
+                "{-1, 1} or pass pos_label explicitly."
+            )
         pos_label = 1
 
     return pos_label
+
+
+def _to_object_array(sequence):
+    """Convert sequence to a 1-D NumPy array of object dtype.
+
+    numpy.array constructor has a similar use but it's output
+    is ambiguous. It can be 1-D NumPy array of object dtype if
+    the input is a ragged array, but if the input is a list of
+    equal length arrays, then the output is a 2D numpy.array.
+    _to_object_array solves this ambiguity by guarantying that
+    the output is a 1-D NumPy array of objects for any input.
+
+    Parameters
+    ----------
+    sequence : array-like of shape (n_elements,)
+        The sequence to be converted.
+
+    Returns
+    -------
+    out : ndarray of shape (n_elements,), dtype=object
+        The converted sequence into a 1-D NumPy array of object dtype.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.utils.validation import _to_object_array
+    >>> _to_object_array([np.array([0]), np.array([1])])
+    array([array([0]), array([1])], dtype=object)
+    >>> _to_object_array([np.array([0]), np.array([1, 2])])
+    array([array([0]), array([1, 2])], dtype=object)
+    >>> _to_object_array([np.array([0]), np.array([1, 2])])
+    array([array([0]), array([1, 2])], dtype=object)
+    """
+    out = np.empty(len(sequence), dtype=object)
+    out[:] = sequence
+    return out
```

